# Benchmarking Vector databases on Code embeddings

_topics: information retrieval, databases, measurement & performance analysis_

so far, relational databases have served us well for information retrieval on sparse representation vectors using inverted indexes.

but the recent popularity in learning encoders called “transformers” and their dense representation vectors called "embeddings" which can capture some "latent semantic space"

- 

??? why is this significant

??? what did we accomplish

??? what's the gist of it all

## introduction

the goal is to search the top-k passage embeddings with the largest dot products for a given query embedding.





inverted indexes for sparse representation vectors
