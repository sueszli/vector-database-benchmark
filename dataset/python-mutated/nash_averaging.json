[
    {
        "func_name": "_max_entropy_symmetric_nash",
        "original": "def _max_entropy_symmetric_nash(p_mat, eps=1e-09):\n    \"\"\"Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\n\n    Using convex programming:\n      min p^Tlog(p)\n      s.t.\n      p_mat.dot(p) <= 0, since game value must be 0\n      p >= 0\n      1^T * p = 1\n\n  Args:\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\n    eps: minimum probability threshold\n\n  Returns:\n    p*: a maxent symmetric nash\n  \"\"\"\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    x = cp.Variable(shape=n)\n    obj = cp.Maximize(cp.sum(cp.entr(x)))\n    constraints = [p_mat @ x <= 0, x >= eps * np.ones(n)]\n    constraints.append(cp.sum(x) == 1)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return x.value.reshape((-1, 1))",
        "mutated": [
            "def _max_entropy_symmetric_nash(p_mat, eps=1e-09):\n    if False:\n        i = 10\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    Using convex programming:\\n      min p^Tlog(p)\\n      s.t.\\n      p_mat.dot(p) <= 0, since game value must be 0\\n      p >= 0\\n      1^T * p = 1\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    p*: a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    x = cp.Variable(shape=n)\n    obj = cp.Maximize(cp.sum(cp.entr(x)))\n    constraints = [p_mat @ x <= 0, x >= eps * np.ones(n)]\n    constraints.append(cp.sum(x) == 1)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return x.value.reshape((-1, 1))",
            "def _max_entropy_symmetric_nash(p_mat, eps=1e-09):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    Using convex programming:\\n      min p^Tlog(p)\\n      s.t.\\n      p_mat.dot(p) <= 0, since game value must be 0\\n      p >= 0\\n      1^T * p = 1\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    p*: a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    x = cp.Variable(shape=n)\n    obj = cp.Maximize(cp.sum(cp.entr(x)))\n    constraints = [p_mat @ x <= 0, x >= eps * np.ones(n)]\n    constraints.append(cp.sum(x) == 1)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return x.value.reshape((-1, 1))",
            "def _max_entropy_symmetric_nash(p_mat, eps=1e-09):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    Using convex programming:\\n      min p^Tlog(p)\\n      s.t.\\n      p_mat.dot(p) <= 0, since game value must be 0\\n      p >= 0\\n      1^T * p = 1\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    p*: a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    x = cp.Variable(shape=n)\n    obj = cp.Maximize(cp.sum(cp.entr(x)))\n    constraints = [p_mat @ x <= 0, x >= eps * np.ones(n)]\n    constraints.append(cp.sum(x) == 1)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return x.value.reshape((-1, 1))",
            "def _max_entropy_symmetric_nash(p_mat, eps=1e-09):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    Using convex programming:\\n      min p^Tlog(p)\\n      s.t.\\n      p_mat.dot(p) <= 0, since game value must be 0\\n      p >= 0\\n      1^T * p = 1\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    p*: a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    x = cp.Variable(shape=n)\n    obj = cp.Maximize(cp.sum(cp.entr(x)))\n    constraints = [p_mat @ x <= 0, x >= eps * np.ones(n)]\n    constraints.append(cp.sum(x) == 1)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return x.value.reshape((-1, 1))",
            "def _max_entropy_symmetric_nash(p_mat, eps=1e-09):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    Using convex programming:\\n      min p^Tlog(p)\\n      s.t.\\n      p_mat.dot(p) <= 0, since game value must be 0\\n      p >= 0\\n      1^T * p = 1\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    p*: a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    x = cp.Variable(shape=n)\n    obj = cp.Maximize(cp.sum(cp.entr(x)))\n    constraints = [p_mat @ x <= 0, x >= eps * np.ones(n)]\n    constraints.append(cp.sum(x) == 1)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return x.value.reshape((-1, 1))"
        ]
    },
    {
        "func_name": "_max_entropy_symmetric_nash_avt",
        "original": "def _max_entropy_symmetric_nash_avt(p_mat, num_agents, num_tasks, eps=1e-09):\n    \"\"\"Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\n\n    This covers the agent-vs-task cases.\n\n    Using convex programming:\n      min x^Tlog(x) + y^Tlog(y)\n      s.t.\n      x >= 0\n      1^T * x = 1\n      y >= 0\n      1^T * y = 1\n      forall s, such that s has exactly one unit mass on an agent strategy\n      and one unit mass on a task strategy,\n      s^T*p_mat*z <= 0, where z = [x, y], since game-value is 0.\n\n  Args:\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\n    num_agents: number of agents\n    num_tasks: number of tasks\n    eps: minimum probability threshold\n\n  Returns:\n    (x*, y*): a maxent symmetric nash\n  \"\"\"\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    assert n == num_agents + num_tasks\n    x = cp.Variable(shape=num_agents)\n    y = cp.Variable(shape=num_tasks)\n    z = cp.hstack([x, y])\n    obj = cp.Maximize(cp.sum(cp.entr(z)))\n    constraints = [x >= eps * np.ones(num_agents), cp.sum(x) == 1, y >= eps * np.ones(num_tasks), cp.sum(y) == 1]\n    dev_payoffs = p_mat @ z\n    for a_idx in range(num_agents):\n        for t_idx in range(num_tasks):\n            pure_strategy = np.zeros(n)\n            pure_strategy[a_idx] = 1\n            pure_strategy[num_agents + t_idx] = 1\n            pure_strategy = pure_strategy.reshape((1, -1))\n            constraints.append(pure_strategy @ dev_payoffs <= 0)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return (x.value.reshape((-1, 1)), y.value.reshape((-1, 1)))",
        "mutated": [
            "def _max_entropy_symmetric_nash_avt(p_mat, num_agents, num_tasks, eps=1e-09):\n    if False:\n        i = 10\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    This covers the agent-vs-task cases.\\n\\n    Using convex programming:\\n      min x^Tlog(x) + y^Tlog(y)\\n      s.t.\\n      x >= 0\\n      1^T * x = 1\\n      y >= 0\\n      1^T * y = 1\\n      forall s, such that s has exactly one unit mass on an agent strategy\\n      and one unit mass on a task strategy,\\n      s^T*p_mat*z <= 0, where z = [x, y], since game-value is 0.\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    num_agents: number of agents\\n    num_tasks: number of tasks\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    (x*, y*): a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    assert n == num_agents + num_tasks\n    x = cp.Variable(shape=num_agents)\n    y = cp.Variable(shape=num_tasks)\n    z = cp.hstack([x, y])\n    obj = cp.Maximize(cp.sum(cp.entr(z)))\n    constraints = [x >= eps * np.ones(num_agents), cp.sum(x) == 1, y >= eps * np.ones(num_tasks), cp.sum(y) == 1]\n    dev_payoffs = p_mat @ z\n    for a_idx in range(num_agents):\n        for t_idx in range(num_tasks):\n            pure_strategy = np.zeros(n)\n            pure_strategy[a_idx] = 1\n            pure_strategy[num_agents + t_idx] = 1\n            pure_strategy = pure_strategy.reshape((1, -1))\n            constraints.append(pure_strategy @ dev_payoffs <= 0)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return (x.value.reshape((-1, 1)), y.value.reshape((-1, 1)))",
            "def _max_entropy_symmetric_nash_avt(p_mat, num_agents, num_tasks, eps=1e-09):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    This covers the agent-vs-task cases.\\n\\n    Using convex programming:\\n      min x^Tlog(x) + y^Tlog(y)\\n      s.t.\\n      x >= 0\\n      1^T * x = 1\\n      y >= 0\\n      1^T * y = 1\\n      forall s, such that s has exactly one unit mass on an agent strategy\\n      and one unit mass on a task strategy,\\n      s^T*p_mat*z <= 0, where z = [x, y], since game-value is 0.\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    num_agents: number of agents\\n    num_tasks: number of tasks\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    (x*, y*): a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    assert n == num_agents + num_tasks\n    x = cp.Variable(shape=num_agents)\n    y = cp.Variable(shape=num_tasks)\n    z = cp.hstack([x, y])\n    obj = cp.Maximize(cp.sum(cp.entr(z)))\n    constraints = [x >= eps * np.ones(num_agents), cp.sum(x) == 1, y >= eps * np.ones(num_tasks), cp.sum(y) == 1]\n    dev_payoffs = p_mat @ z\n    for a_idx in range(num_agents):\n        for t_idx in range(num_tasks):\n            pure_strategy = np.zeros(n)\n            pure_strategy[a_idx] = 1\n            pure_strategy[num_agents + t_idx] = 1\n            pure_strategy = pure_strategy.reshape((1, -1))\n            constraints.append(pure_strategy @ dev_payoffs <= 0)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return (x.value.reshape((-1, 1)), y.value.reshape((-1, 1)))",
            "def _max_entropy_symmetric_nash_avt(p_mat, num_agents, num_tasks, eps=1e-09):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    This covers the agent-vs-task cases.\\n\\n    Using convex programming:\\n      min x^Tlog(x) + y^Tlog(y)\\n      s.t.\\n      x >= 0\\n      1^T * x = 1\\n      y >= 0\\n      1^T * y = 1\\n      forall s, such that s has exactly one unit mass on an agent strategy\\n      and one unit mass on a task strategy,\\n      s^T*p_mat*z <= 0, where z = [x, y], since game-value is 0.\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    num_agents: number of agents\\n    num_tasks: number of tasks\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    (x*, y*): a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    assert n == num_agents + num_tasks\n    x = cp.Variable(shape=num_agents)\n    y = cp.Variable(shape=num_tasks)\n    z = cp.hstack([x, y])\n    obj = cp.Maximize(cp.sum(cp.entr(z)))\n    constraints = [x >= eps * np.ones(num_agents), cp.sum(x) == 1, y >= eps * np.ones(num_tasks), cp.sum(y) == 1]\n    dev_payoffs = p_mat @ z\n    for a_idx in range(num_agents):\n        for t_idx in range(num_tasks):\n            pure_strategy = np.zeros(n)\n            pure_strategy[a_idx] = 1\n            pure_strategy[num_agents + t_idx] = 1\n            pure_strategy = pure_strategy.reshape((1, -1))\n            constraints.append(pure_strategy @ dev_payoffs <= 0)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return (x.value.reshape((-1, 1)), y.value.reshape((-1, 1)))",
            "def _max_entropy_symmetric_nash_avt(p_mat, num_agents, num_tasks, eps=1e-09):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    This covers the agent-vs-task cases.\\n\\n    Using convex programming:\\n      min x^Tlog(x) + y^Tlog(y)\\n      s.t.\\n      x >= 0\\n      1^T * x = 1\\n      y >= 0\\n      1^T * y = 1\\n      forall s, such that s has exactly one unit mass on an agent strategy\\n      and one unit mass on a task strategy,\\n      s^T*p_mat*z <= 0, where z = [x, y], since game-value is 0.\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    num_agents: number of agents\\n    num_tasks: number of tasks\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    (x*, y*): a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    assert n == num_agents + num_tasks\n    x = cp.Variable(shape=num_agents)\n    y = cp.Variable(shape=num_tasks)\n    z = cp.hstack([x, y])\n    obj = cp.Maximize(cp.sum(cp.entr(z)))\n    constraints = [x >= eps * np.ones(num_agents), cp.sum(x) == 1, y >= eps * np.ones(num_tasks), cp.sum(y) == 1]\n    dev_payoffs = p_mat @ z\n    for a_idx in range(num_agents):\n        for t_idx in range(num_tasks):\n            pure_strategy = np.zeros(n)\n            pure_strategy[a_idx] = 1\n            pure_strategy[num_agents + t_idx] = 1\n            pure_strategy = pure_strategy.reshape((1, -1))\n            constraints.append(pure_strategy @ dev_payoffs <= 0)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return (x.value.reshape((-1, 1)), y.value.reshape((-1, 1)))",
            "def _max_entropy_symmetric_nash_avt(p_mat, num_agents, num_tasks, eps=1e-09):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Solves for the maxent symmetric nash for symmetric 2P zero-sum games.\\n\\n    This covers the agent-vs-task cases.\\n\\n    Using convex programming:\\n      min x^Tlog(x) + y^Tlog(y)\\n      s.t.\\n      x >= 0\\n      1^T * x = 1\\n      y >= 0\\n      1^T * y = 1\\n      forall s, such that s has exactly one unit mass on an agent strategy\\n      and one unit mass on a task strategy,\\n      s^T*p_mat*z <= 0, where z = [x, y], since game-value is 0.\\n\\n  Args:\\n    p_mat: an N*N anti-symmetric payoff matrix for the row player\\n    num_agents: number of agents\\n    num_tasks: number of tasks\\n    eps: minimum probability threshold\\n\\n  Returns:\\n    (x*, y*): a maxent symmetric nash\\n  '\n    assert np.array_equal(p_mat, -p_mat.T) and eps >= 0 and (eps <= 0.5)\n    n = len(p_mat)\n    assert n == num_agents + num_tasks\n    x = cp.Variable(shape=num_agents)\n    y = cp.Variable(shape=num_tasks)\n    z = cp.hstack([x, y])\n    obj = cp.Maximize(cp.sum(cp.entr(z)))\n    constraints = [x >= eps * np.ones(num_agents), cp.sum(x) == 1, y >= eps * np.ones(num_tasks), cp.sum(y) == 1]\n    dev_payoffs = p_mat @ z\n    for a_idx in range(num_agents):\n        for t_idx in range(num_tasks):\n            pure_strategy = np.zeros(n)\n            pure_strategy[a_idx] = 1\n            pure_strategy[num_agents + t_idx] = 1\n            pure_strategy = pure_strategy.reshape((1, -1))\n            constraints.append(pure_strategy @ dev_payoffs <= 0)\n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    return (x.value.reshape((-1, 1)), y.value.reshape((-1, 1)))"
        ]
    },
    {
        "func_name": "nash_averaging_avt_matrix",
        "original": "def nash_averaging_avt_matrix(s_mat, eps=0.0):\n    \"\"\"Apply the agent-vs-task Nash Averaging from Appendix D, from a matrix.\n\n  Args:\n    s_mat: The S matrix from the paper, representing m rows (agents) and n\n      columns (tasks), with scores for the agent on the task. Note that the\n      values need not be normalized, but will be normalized across tasks before\n      being processed.\n    eps: minimum probability threshold.\n\n  Returns:\n    maxent_nash: nash mixture for row player and column player\n    nash_avg_score: the expected payoff under maxent_nash\n  \"\"\"\n    (m, n) = s_mat.shape\n    min_payoffs = np.min(s_mat, axis=0)\n    max_payoffs = np.max(s_mat, axis=0)\n    std_p_mat = (s_mat - min_payoffs) / (max_payoffs - min_payoffs)\n    a_mat = np.block([[np.zeros(shape=(m, m)), std_p_mat], [-std_p_mat.T, np.zeros(shape=(n, n))]])\n    (pa_sol, pe_sol) = _max_entropy_symmetric_nash_avt(a_mat, num_agents=m, num_tasks=n, eps=eps)\n    (pa, pe) = (np.asarray(pa_sol), np.asarray(pe_sol))\n    return ((pa, pe), (std_p_mat.dot(pe), -std_p_mat.T.dot(pa)))",
        "mutated": [
            "def nash_averaging_avt_matrix(s_mat, eps=0.0):\n    if False:\n        i = 10\n    'Apply the agent-vs-task Nash Averaging from Appendix D, from a matrix.\\n\\n  Args:\\n    s_mat: The S matrix from the paper, representing m rows (agents) and n\\n      columns (tasks), with scores for the agent on the task. Note that the\\n      values need not be normalized, but will be normalized across tasks before\\n      being processed.\\n    eps: minimum probability threshold.\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    (m, n) = s_mat.shape\n    min_payoffs = np.min(s_mat, axis=0)\n    max_payoffs = np.max(s_mat, axis=0)\n    std_p_mat = (s_mat - min_payoffs) / (max_payoffs - min_payoffs)\n    a_mat = np.block([[np.zeros(shape=(m, m)), std_p_mat], [-std_p_mat.T, np.zeros(shape=(n, n))]])\n    (pa_sol, pe_sol) = _max_entropy_symmetric_nash_avt(a_mat, num_agents=m, num_tasks=n, eps=eps)\n    (pa, pe) = (np.asarray(pa_sol), np.asarray(pe_sol))\n    return ((pa, pe), (std_p_mat.dot(pe), -std_p_mat.T.dot(pa)))",
            "def nash_averaging_avt_matrix(s_mat, eps=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Apply the agent-vs-task Nash Averaging from Appendix D, from a matrix.\\n\\n  Args:\\n    s_mat: The S matrix from the paper, representing m rows (agents) and n\\n      columns (tasks), with scores for the agent on the task. Note that the\\n      values need not be normalized, but will be normalized across tasks before\\n      being processed.\\n    eps: minimum probability threshold.\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    (m, n) = s_mat.shape\n    min_payoffs = np.min(s_mat, axis=0)\n    max_payoffs = np.max(s_mat, axis=0)\n    std_p_mat = (s_mat - min_payoffs) / (max_payoffs - min_payoffs)\n    a_mat = np.block([[np.zeros(shape=(m, m)), std_p_mat], [-std_p_mat.T, np.zeros(shape=(n, n))]])\n    (pa_sol, pe_sol) = _max_entropy_symmetric_nash_avt(a_mat, num_agents=m, num_tasks=n, eps=eps)\n    (pa, pe) = (np.asarray(pa_sol), np.asarray(pe_sol))\n    return ((pa, pe), (std_p_mat.dot(pe), -std_p_mat.T.dot(pa)))",
            "def nash_averaging_avt_matrix(s_mat, eps=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Apply the agent-vs-task Nash Averaging from Appendix D, from a matrix.\\n\\n  Args:\\n    s_mat: The S matrix from the paper, representing m rows (agents) and n\\n      columns (tasks), with scores for the agent on the task. Note that the\\n      values need not be normalized, but will be normalized across tasks before\\n      being processed.\\n    eps: minimum probability threshold.\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    (m, n) = s_mat.shape\n    min_payoffs = np.min(s_mat, axis=0)\n    max_payoffs = np.max(s_mat, axis=0)\n    std_p_mat = (s_mat - min_payoffs) / (max_payoffs - min_payoffs)\n    a_mat = np.block([[np.zeros(shape=(m, m)), std_p_mat], [-std_p_mat.T, np.zeros(shape=(n, n))]])\n    (pa_sol, pe_sol) = _max_entropy_symmetric_nash_avt(a_mat, num_agents=m, num_tasks=n, eps=eps)\n    (pa, pe) = (np.asarray(pa_sol), np.asarray(pe_sol))\n    return ((pa, pe), (std_p_mat.dot(pe), -std_p_mat.T.dot(pa)))",
            "def nash_averaging_avt_matrix(s_mat, eps=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Apply the agent-vs-task Nash Averaging from Appendix D, from a matrix.\\n\\n  Args:\\n    s_mat: The S matrix from the paper, representing m rows (agents) and n\\n      columns (tasks), with scores for the agent on the task. Note that the\\n      values need not be normalized, but will be normalized across tasks before\\n      being processed.\\n    eps: minimum probability threshold.\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    (m, n) = s_mat.shape\n    min_payoffs = np.min(s_mat, axis=0)\n    max_payoffs = np.max(s_mat, axis=0)\n    std_p_mat = (s_mat - min_payoffs) / (max_payoffs - min_payoffs)\n    a_mat = np.block([[np.zeros(shape=(m, m)), std_p_mat], [-std_p_mat.T, np.zeros(shape=(n, n))]])\n    (pa_sol, pe_sol) = _max_entropy_symmetric_nash_avt(a_mat, num_agents=m, num_tasks=n, eps=eps)\n    (pa, pe) = (np.asarray(pa_sol), np.asarray(pe_sol))\n    return ((pa, pe), (std_p_mat.dot(pe), -std_p_mat.T.dot(pa)))",
            "def nash_averaging_avt_matrix(s_mat, eps=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Apply the agent-vs-task Nash Averaging from Appendix D, from a matrix.\\n\\n  Args:\\n    s_mat: The S matrix from the paper, representing m rows (agents) and n\\n      columns (tasks), with scores for the agent on the task. Note that the\\n      values need not be normalized, but will be normalized across tasks before\\n      being processed.\\n    eps: minimum probability threshold.\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    (m, n) = s_mat.shape\n    min_payoffs = np.min(s_mat, axis=0)\n    max_payoffs = np.max(s_mat, axis=0)\n    std_p_mat = (s_mat - min_payoffs) / (max_payoffs - min_payoffs)\n    a_mat = np.block([[np.zeros(shape=(m, m)), std_p_mat], [-std_p_mat.T, np.zeros(shape=(n, n))]])\n    (pa_sol, pe_sol) = _max_entropy_symmetric_nash_avt(a_mat, num_agents=m, num_tasks=n, eps=eps)\n    (pa, pe) = (np.asarray(pa_sol), np.asarray(pe_sol))\n    return ((pa, pe), (std_p_mat.dot(pe), -std_p_mat.T.dot(pa)))"
        ]
    },
    {
        "func_name": "nash_averaging",
        "original": "def nash_averaging(game, eps=0.0, a_v_a=True):\n    \"\"\"Nash averaging, see https://arxiv.org/abs/1806.02643.\n\n  Args:\n    game: a pyspiel game\n    eps: minimum probability mass for maxent nash\n    a_v_a: whether it is Agent-vs-Agent or Agent-vs-Task\n\n  Returns:\n    maxent_nash: nash mixture for row player and column player\n    nash_avg_score: the expected payoff under maxent_nash\n  \"\"\"\n    p_mat = game_payoffs_array(game)\n    if len(p_mat) != 2:\n        raise ValueError('Nash Averaging works only for two players.')\n    if np.max(np.abs(p_mat[0] + p_mat[1])) > 0:\n        raise ValueError('Must be zero-sum')\n    if a_v_a:\n        if not np.array_equal(p_mat[0], -p_mat[0].T):\n            raise ValueError('AvA only works for symmetric two-player zero-sum games.')\n        maxent_nash = np.array(_max_entropy_symmetric_nash(p_mat[0], eps=eps))\n        return (maxent_nash, p_mat[0].dot(maxent_nash))\n    return nash_averaging_avt_matrix(p_mat[0], eps=eps)",
        "mutated": [
            "def nash_averaging(game, eps=0.0, a_v_a=True):\n    if False:\n        i = 10\n    'Nash averaging, see https://arxiv.org/abs/1806.02643.\\n\\n  Args:\\n    game: a pyspiel game\\n    eps: minimum probability mass for maxent nash\\n    a_v_a: whether it is Agent-vs-Agent or Agent-vs-Task\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    p_mat = game_payoffs_array(game)\n    if len(p_mat) != 2:\n        raise ValueError('Nash Averaging works only for two players.')\n    if np.max(np.abs(p_mat[0] + p_mat[1])) > 0:\n        raise ValueError('Must be zero-sum')\n    if a_v_a:\n        if not np.array_equal(p_mat[0], -p_mat[0].T):\n            raise ValueError('AvA only works for symmetric two-player zero-sum games.')\n        maxent_nash = np.array(_max_entropy_symmetric_nash(p_mat[0], eps=eps))\n        return (maxent_nash, p_mat[0].dot(maxent_nash))\n    return nash_averaging_avt_matrix(p_mat[0], eps=eps)",
            "def nash_averaging(game, eps=0.0, a_v_a=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Nash averaging, see https://arxiv.org/abs/1806.02643.\\n\\n  Args:\\n    game: a pyspiel game\\n    eps: minimum probability mass for maxent nash\\n    a_v_a: whether it is Agent-vs-Agent or Agent-vs-Task\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    p_mat = game_payoffs_array(game)\n    if len(p_mat) != 2:\n        raise ValueError('Nash Averaging works only for two players.')\n    if np.max(np.abs(p_mat[0] + p_mat[1])) > 0:\n        raise ValueError('Must be zero-sum')\n    if a_v_a:\n        if not np.array_equal(p_mat[0], -p_mat[0].T):\n            raise ValueError('AvA only works for symmetric two-player zero-sum games.')\n        maxent_nash = np.array(_max_entropy_symmetric_nash(p_mat[0], eps=eps))\n        return (maxent_nash, p_mat[0].dot(maxent_nash))\n    return nash_averaging_avt_matrix(p_mat[0], eps=eps)",
            "def nash_averaging(game, eps=0.0, a_v_a=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Nash averaging, see https://arxiv.org/abs/1806.02643.\\n\\n  Args:\\n    game: a pyspiel game\\n    eps: minimum probability mass for maxent nash\\n    a_v_a: whether it is Agent-vs-Agent or Agent-vs-Task\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    p_mat = game_payoffs_array(game)\n    if len(p_mat) != 2:\n        raise ValueError('Nash Averaging works only for two players.')\n    if np.max(np.abs(p_mat[0] + p_mat[1])) > 0:\n        raise ValueError('Must be zero-sum')\n    if a_v_a:\n        if not np.array_equal(p_mat[0], -p_mat[0].T):\n            raise ValueError('AvA only works for symmetric two-player zero-sum games.')\n        maxent_nash = np.array(_max_entropy_symmetric_nash(p_mat[0], eps=eps))\n        return (maxent_nash, p_mat[0].dot(maxent_nash))\n    return nash_averaging_avt_matrix(p_mat[0], eps=eps)",
            "def nash_averaging(game, eps=0.0, a_v_a=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Nash averaging, see https://arxiv.org/abs/1806.02643.\\n\\n  Args:\\n    game: a pyspiel game\\n    eps: minimum probability mass for maxent nash\\n    a_v_a: whether it is Agent-vs-Agent or Agent-vs-Task\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    p_mat = game_payoffs_array(game)\n    if len(p_mat) != 2:\n        raise ValueError('Nash Averaging works only for two players.')\n    if np.max(np.abs(p_mat[0] + p_mat[1])) > 0:\n        raise ValueError('Must be zero-sum')\n    if a_v_a:\n        if not np.array_equal(p_mat[0], -p_mat[0].T):\n            raise ValueError('AvA only works for symmetric two-player zero-sum games.')\n        maxent_nash = np.array(_max_entropy_symmetric_nash(p_mat[0], eps=eps))\n        return (maxent_nash, p_mat[0].dot(maxent_nash))\n    return nash_averaging_avt_matrix(p_mat[0], eps=eps)",
            "def nash_averaging(game, eps=0.0, a_v_a=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Nash averaging, see https://arxiv.org/abs/1806.02643.\\n\\n  Args:\\n    game: a pyspiel game\\n    eps: minimum probability mass for maxent nash\\n    a_v_a: whether it is Agent-vs-Agent or Agent-vs-Task\\n\\n  Returns:\\n    maxent_nash: nash mixture for row player and column player\\n    nash_avg_score: the expected payoff under maxent_nash\\n  '\n    p_mat = game_payoffs_array(game)\n    if len(p_mat) != 2:\n        raise ValueError('Nash Averaging works only for two players.')\n    if np.max(np.abs(p_mat[0] + p_mat[1])) > 0:\n        raise ValueError('Must be zero-sum')\n    if a_v_a:\n        if not np.array_equal(p_mat[0], -p_mat[0].T):\n            raise ValueError('AvA only works for symmetric two-player zero-sum games.')\n        maxent_nash = np.array(_max_entropy_symmetric_nash(p_mat[0], eps=eps))\n        return (maxent_nash, p_mat[0].dot(maxent_nash))\n    return nash_averaging_avt_matrix(p_mat[0], eps=eps)"
        ]
    }
]