[
    {
        "func_name": "lowess",
        "original": "def lowess(endog, exog, frac=2.0 / 3, it=3):\n    \"\"\"\n    LOWESS (Locally Weighted Scatterplot Smoothing)\n\n    A lowess function that outs smoothed estimates of endog\n    at the given exog values from points (exog, endog)\n\n    Parameters\n    ----------\n    endog : 1-D numpy array\n        The y-values of the observed points\n    exog : 1-D numpy array\n        The x-values of the observed points\n    frac : float\n        Between 0 and 1. The fraction of the data used\n        when estimating each y-value.\n    it : int\n        The number of residual-based reweightings\n        to perform.\n\n    Returns\n    -------\n    out: numpy array\n        A numpy array with two columns. The first column\n        is the sorted x values and the second column the\n        associated estimated y-values.\n\n    Notes\n    -----\n    This lowess function implements the algorithm given in the\n    reference below using local linear estimates.\n\n    Suppose the input data has N points. The algorithm works by\n    estimating the true ``y_i`` by taking the frac*N closest points\n    to ``(x_i,y_i)`` based on their x values and estimating ``y_i``\n    using a weighted linear regression. The weight for ``(x_j,y_j)``\n    is `_lowess_tricube` function applied to ``|x_i-x_j|``.\n\n    If ``iter > 0``, then further weighted local linear regressions\n    are performed, where the weights are the same as above\n    times the `_lowess_bisquare` function of the residuals. Each iteration\n    takes approximately the same amount of time as the original fit,\n    so these iterations are expensive. They are most useful when\n    the noise has extremely heavy tails, such as Cauchy noise.\n    Noise with less heavy-tails, such as t-distributions with ``df > 2``,\n    are less problematic. The weights downgrade the influence of\n    points with large residuals. In the extreme case, points whose\n    residuals are larger than 6 times the median absolute residual\n    are given weight 0.\n\n    Some experimentation is likely required to find a good\n    choice of frac and iter for a particular dataset.\n\n    References\n    ----------\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\n    and Smoothing Scatterplots\". Journal of the American Statistical\n    Association 74 (368): 829-836.\n\n    Examples\n    --------\n    The below allows a comparison between how different the fits from\n    `lowess` for different values of frac can be.\n\n    >>> import numpy as np\n    >>> import statsmodels.api as sm\n    >>> lowess = sm.nonparametric.lowess\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\n    >>> z = lowess(y, x)\n    >>> w = lowess(y, x, frac=1./3)\n\n    This gives a similar comparison for when it is 0 vs not.\n\n    >>> import scipy.stats as stats\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\n    >>> z = lowess(y, x, frac= 1./3, it=0)\n    >>> w = lowess(y, x, frac=1./3)\n    \"\"\"\n    x = exog\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != x.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    n = exog.shape[0]\n    fitted = np.zeros(n)\n    k = int(frac * n)\n    index_array = np.argsort(exog)\n    x_copy = np.array(exog[index_array])\n    y_copy = endog[index_array]\n    (fitted, weights) = _lowess_initial_fit(x_copy, y_copy, k, n)\n    for i in range(it):\n        _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n)\n    out = np.array([x_copy, fitted]).T\n    out.shape = (n, 2)\n    return out",
        "mutated": [
            "def lowess(endog, exog, frac=2.0 / 3, it=3):\n    if False:\n        i = 10\n    '\\n    LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n\\n    Returns\\n    -------\\n    out: numpy array\\n        A numpy array with two columns. The first column\\n        is the sorted x values and the second column the\\n        associated estimated y-values.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the true ``y_i`` by taking the frac*N closest points\\n    to ``(x_i,y_i)`` based on their x values and estimating ``y_i``\\n    using a weighted linear regression. The weight for ``(x_j,y_j)``\\n    is `_lowess_tricube` function applied to ``|x_i-x_j|``.\\n\\n    If ``iter > 0``, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the `_lowess_bisquare` function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with ``df > 2``,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    Some experimentation is likely required to find a good\\n    choice of frac and iter for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    `lowess` for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import scipy.stats as stats\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n    '\n    x = exog\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != x.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    n = exog.shape[0]\n    fitted = np.zeros(n)\n    k = int(frac * n)\n    index_array = np.argsort(exog)\n    x_copy = np.array(exog[index_array])\n    y_copy = endog[index_array]\n    (fitted, weights) = _lowess_initial_fit(x_copy, y_copy, k, n)\n    for i in range(it):\n        _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n)\n    out = np.array([x_copy, fitted]).T\n    out.shape = (n, 2)\n    return out",
            "def lowess(endog, exog, frac=2.0 / 3, it=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n\\n    Returns\\n    -------\\n    out: numpy array\\n        A numpy array with two columns. The first column\\n        is the sorted x values and the second column the\\n        associated estimated y-values.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the true ``y_i`` by taking the frac*N closest points\\n    to ``(x_i,y_i)`` based on their x values and estimating ``y_i``\\n    using a weighted linear regression. The weight for ``(x_j,y_j)``\\n    is `_lowess_tricube` function applied to ``|x_i-x_j|``.\\n\\n    If ``iter > 0``, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the `_lowess_bisquare` function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with ``df > 2``,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    Some experimentation is likely required to find a good\\n    choice of frac and iter for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    `lowess` for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import scipy.stats as stats\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n    '\n    x = exog\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != x.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    n = exog.shape[0]\n    fitted = np.zeros(n)\n    k = int(frac * n)\n    index_array = np.argsort(exog)\n    x_copy = np.array(exog[index_array])\n    y_copy = endog[index_array]\n    (fitted, weights) = _lowess_initial_fit(x_copy, y_copy, k, n)\n    for i in range(it):\n        _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n)\n    out = np.array([x_copy, fitted]).T\n    out.shape = (n, 2)\n    return out",
            "def lowess(endog, exog, frac=2.0 / 3, it=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n\\n    Returns\\n    -------\\n    out: numpy array\\n        A numpy array with two columns. The first column\\n        is the sorted x values and the second column the\\n        associated estimated y-values.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the true ``y_i`` by taking the frac*N closest points\\n    to ``(x_i,y_i)`` based on their x values and estimating ``y_i``\\n    using a weighted linear regression. The weight for ``(x_j,y_j)``\\n    is `_lowess_tricube` function applied to ``|x_i-x_j|``.\\n\\n    If ``iter > 0``, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the `_lowess_bisquare` function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with ``df > 2``,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    Some experimentation is likely required to find a good\\n    choice of frac and iter for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    `lowess` for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import scipy.stats as stats\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n    '\n    x = exog\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != x.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    n = exog.shape[0]\n    fitted = np.zeros(n)\n    k = int(frac * n)\n    index_array = np.argsort(exog)\n    x_copy = np.array(exog[index_array])\n    y_copy = endog[index_array]\n    (fitted, weights) = _lowess_initial_fit(x_copy, y_copy, k, n)\n    for i in range(it):\n        _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n)\n    out = np.array([x_copy, fitted]).T\n    out.shape = (n, 2)\n    return out",
            "def lowess(endog, exog, frac=2.0 / 3, it=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n\\n    Returns\\n    -------\\n    out: numpy array\\n        A numpy array with two columns. The first column\\n        is the sorted x values and the second column the\\n        associated estimated y-values.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the true ``y_i`` by taking the frac*N closest points\\n    to ``(x_i,y_i)`` based on their x values and estimating ``y_i``\\n    using a weighted linear regression. The weight for ``(x_j,y_j)``\\n    is `_lowess_tricube` function applied to ``|x_i-x_j|``.\\n\\n    If ``iter > 0``, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the `_lowess_bisquare` function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with ``df > 2``,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    Some experimentation is likely required to find a good\\n    choice of frac and iter for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    `lowess` for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import scipy.stats as stats\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n    '\n    x = exog\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != x.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    n = exog.shape[0]\n    fitted = np.zeros(n)\n    k = int(frac * n)\n    index_array = np.argsort(exog)\n    x_copy = np.array(exog[index_array])\n    y_copy = endog[index_array]\n    (fitted, weights) = _lowess_initial_fit(x_copy, y_copy, k, n)\n    for i in range(it):\n        _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n)\n    out = np.array([x_copy, fitted]).T\n    out.shape = (n, 2)\n    return out",
            "def lowess(endog, exog, frac=2.0 / 3, it=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    LOWESS (Locally Weighted Scatterplot Smoothing)\\n\\n    A lowess function that outs smoothed estimates of endog\\n    at the given exog values from points (exog, endog)\\n\\n    Parameters\\n    ----------\\n    endog : 1-D numpy array\\n        The y-values of the observed points\\n    exog : 1-D numpy array\\n        The x-values of the observed points\\n    frac : float\\n        Between 0 and 1. The fraction of the data used\\n        when estimating each y-value.\\n    it : int\\n        The number of residual-based reweightings\\n        to perform.\\n\\n    Returns\\n    -------\\n    out: numpy array\\n        A numpy array with two columns. The first column\\n        is the sorted x values and the second column the\\n        associated estimated y-values.\\n\\n    Notes\\n    -----\\n    This lowess function implements the algorithm given in the\\n    reference below using local linear estimates.\\n\\n    Suppose the input data has N points. The algorithm works by\\n    estimating the true ``y_i`` by taking the frac*N closest points\\n    to ``(x_i,y_i)`` based on their x values and estimating ``y_i``\\n    using a weighted linear regression. The weight for ``(x_j,y_j)``\\n    is `_lowess_tricube` function applied to ``|x_i-x_j|``.\\n\\n    If ``iter > 0``, then further weighted local linear regressions\\n    are performed, where the weights are the same as above\\n    times the `_lowess_bisquare` function of the residuals. Each iteration\\n    takes approximately the same amount of time as the original fit,\\n    so these iterations are expensive. They are most useful when\\n    the noise has extremely heavy tails, such as Cauchy noise.\\n    Noise with less heavy-tails, such as t-distributions with ``df > 2``,\\n    are less problematic. The weights downgrade the influence of\\n    points with large residuals. In the extreme case, points whose\\n    residuals are larger than 6 times the median absolute residual\\n    are given weight 0.\\n\\n    Some experimentation is likely required to find a good\\n    choice of frac and iter for a particular dataset.\\n\\n    References\\n    ----------\\n    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\\n    and Smoothing Scatterplots\". Journal of the American Statistical\\n    Association 74 (368): 829-836.\\n\\n    Examples\\n    --------\\n    The below allows a comparison between how different the fits from\\n    `lowess` for different values of frac can be.\\n\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n    >>> lowess = sm.nonparametric.lowess\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + np.random.normal(size=len(x))\\n    >>> z = lowess(y, x)\\n    >>> w = lowess(y, x, frac=1./3)\\n\\n    This gives a similar comparison for when it is 0 vs not.\\n\\n    >>> import scipy.stats as stats\\n    >>> x = np.random.uniform(low=-2*np.pi, high=2*np.pi, size=500)\\n    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\\n    >>> z = lowess(y, x, frac= 1./3, it=0)\\n    >>> w = lowess(y, x, frac=1./3)\\n    '\n    x = exog\n    if exog.ndim != 1:\n        raise ValueError('exog must be a vector')\n    if endog.ndim != 1:\n        raise ValueError('endog must be a vector')\n    if endog.shape[0] != x.shape[0]:\n        raise ValueError('exog and endog must have same length')\n    n = exog.shape[0]\n    fitted = np.zeros(n)\n    k = int(frac * n)\n    index_array = np.argsort(exog)\n    x_copy = np.array(exog[index_array])\n    y_copy = endog[index_array]\n    (fitted, weights) = _lowess_initial_fit(x_copy, y_copy, k, n)\n    for i in range(it):\n        _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n)\n    out = np.array([x_copy, fitted]).T\n    out.shape = (n, 2)\n    return out"
        ]
    },
    {
        "func_name": "_lowess_initial_fit",
        "original": "def _lowess_initial_fit(x_copy, y_copy, k, n):\n    \"\"\"\n    The initial weighted local linear regression for lowess.\n\n    Parameters\n    ----------\n    x_copy : 1-d ndarray\n        The x-values/exogenous part of the data being smoothed\n    y_copy : 1-d ndarray\n        The y-values/ endogenous part of the data being smoothed\n   k : int\n        The number of data points which affect the linear fit for\n        each estimated point\n    n : int\n        The total number of points\n\n    Returns\n    -------\n    fitted : 1-d ndarray\n        The fitted y-values\n    weights : 2-d ndarray\n        An n by k array. The contribution to the weights in the\n        local linear fit coming from the distances between the\n        x-values\n\n   \"\"\"\n    weights = np.zeros((n, k), dtype=x_copy.dtype)\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    fitted = np.zeros(n)\n    for i in range(n):\n        left_width = x_copy[i] - x_copy[nn_indices[0]]\n        right_width = x_copy[nn_indices[1] - 1] - x_copy[i]\n        width = max(left_width, right_width)\n        _lowess_wt_standardize(weights[i, :], x_copy[nn_indices[0]:nn_indices[1]], x_copy[i], width)\n        _lowess_tricube(weights[i, :])\n        weights[i, :] = np.sqrt(weights[i, :])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = weights[i, :] * y_copy[nn_indices[0]:nn_indices[1]]\n        beta = lstsq(weights[i, :].reshape(k, 1) * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)\n    return (fitted, weights)",
        "mutated": [
            "def _lowess_initial_fit(x_copy, y_copy, k, n):\n    if False:\n        i = 10\n    '\\n    The initial weighted local linear regression for lowess.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n   k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n    Returns\\n    -------\\n    fitted : 1-d ndarray\\n        The fitted y-values\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n\\n   '\n    weights = np.zeros((n, k), dtype=x_copy.dtype)\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    fitted = np.zeros(n)\n    for i in range(n):\n        left_width = x_copy[i] - x_copy[nn_indices[0]]\n        right_width = x_copy[nn_indices[1] - 1] - x_copy[i]\n        width = max(left_width, right_width)\n        _lowess_wt_standardize(weights[i, :], x_copy[nn_indices[0]:nn_indices[1]], x_copy[i], width)\n        _lowess_tricube(weights[i, :])\n        weights[i, :] = np.sqrt(weights[i, :])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = weights[i, :] * y_copy[nn_indices[0]:nn_indices[1]]\n        beta = lstsq(weights[i, :].reshape(k, 1) * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)\n    return (fitted, weights)",
            "def _lowess_initial_fit(x_copy, y_copy, k, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    The initial weighted local linear regression for lowess.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n   k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n    Returns\\n    -------\\n    fitted : 1-d ndarray\\n        The fitted y-values\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n\\n   '\n    weights = np.zeros((n, k), dtype=x_copy.dtype)\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    fitted = np.zeros(n)\n    for i in range(n):\n        left_width = x_copy[i] - x_copy[nn_indices[0]]\n        right_width = x_copy[nn_indices[1] - 1] - x_copy[i]\n        width = max(left_width, right_width)\n        _lowess_wt_standardize(weights[i, :], x_copy[nn_indices[0]:nn_indices[1]], x_copy[i], width)\n        _lowess_tricube(weights[i, :])\n        weights[i, :] = np.sqrt(weights[i, :])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = weights[i, :] * y_copy[nn_indices[0]:nn_indices[1]]\n        beta = lstsq(weights[i, :].reshape(k, 1) * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)\n    return (fitted, weights)",
            "def _lowess_initial_fit(x_copy, y_copy, k, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    The initial weighted local linear regression for lowess.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n   k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n    Returns\\n    -------\\n    fitted : 1-d ndarray\\n        The fitted y-values\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n\\n   '\n    weights = np.zeros((n, k), dtype=x_copy.dtype)\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    fitted = np.zeros(n)\n    for i in range(n):\n        left_width = x_copy[i] - x_copy[nn_indices[0]]\n        right_width = x_copy[nn_indices[1] - 1] - x_copy[i]\n        width = max(left_width, right_width)\n        _lowess_wt_standardize(weights[i, :], x_copy[nn_indices[0]:nn_indices[1]], x_copy[i], width)\n        _lowess_tricube(weights[i, :])\n        weights[i, :] = np.sqrt(weights[i, :])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = weights[i, :] * y_copy[nn_indices[0]:nn_indices[1]]\n        beta = lstsq(weights[i, :].reshape(k, 1) * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)\n    return (fitted, weights)",
            "def _lowess_initial_fit(x_copy, y_copy, k, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    The initial weighted local linear regression for lowess.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n   k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n    Returns\\n    -------\\n    fitted : 1-d ndarray\\n        The fitted y-values\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n\\n   '\n    weights = np.zeros((n, k), dtype=x_copy.dtype)\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    fitted = np.zeros(n)\n    for i in range(n):\n        left_width = x_copy[i] - x_copy[nn_indices[0]]\n        right_width = x_copy[nn_indices[1] - 1] - x_copy[i]\n        width = max(left_width, right_width)\n        _lowess_wt_standardize(weights[i, :], x_copy[nn_indices[0]:nn_indices[1]], x_copy[i], width)\n        _lowess_tricube(weights[i, :])\n        weights[i, :] = np.sqrt(weights[i, :])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = weights[i, :] * y_copy[nn_indices[0]:nn_indices[1]]\n        beta = lstsq(weights[i, :].reshape(k, 1) * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)\n    return (fitted, weights)",
            "def _lowess_initial_fit(x_copy, y_copy, k, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    The initial weighted local linear regression for lowess.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n   k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n    Returns\\n    -------\\n    fitted : 1-d ndarray\\n        The fitted y-values\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n\\n   '\n    weights = np.zeros((n, k), dtype=x_copy.dtype)\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    fitted = np.zeros(n)\n    for i in range(n):\n        left_width = x_copy[i] - x_copy[nn_indices[0]]\n        right_width = x_copy[nn_indices[1] - 1] - x_copy[i]\n        width = max(left_width, right_width)\n        _lowess_wt_standardize(weights[i, :], x_copy[nn_indices[0]:nn_indices[1]], x_copy[i], width)\n        _lowess_tricube(weights[i, :])\n        weights[i, :] = np.sqrt(weights[i, :])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = weights[i, :] * y_copy[nn_indices[0]:nn_indices[1]]\n        beta = lstsq(weights[i, :].reshape(k, 1) * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)\n    return (fitted, weights)"
        ]
    },
    {
        "func_name": "_lowess_wt_standardize",
        "original": "def _lowess_wt_standardize(weights, new_entries, x_copy_i, width):\n    \"\"\"\n    The initial phase of creating the weights.\n    Subtract the current x_i and divide by the width.\n\n    Parameters\n    ----------\n    weights : ndarray\n        The memory where (new_entries - x_copy_i)/width will be placed\n    new_entries : ndarray\n        The x-values of the k closest points to x[i]\n    x_copy_i : float\n        x[i], the i'th point in the (sorted) x values\n    width : float\n        The maximum distance between x[i] and any point in new_entries\n\n    Returns\n    -------\n    Nothing. The modifications are made to weight in place.\n    \"\"\"\n    weights[:] = new_entries\n    weights -= x_copy_i\n    weights /= width",
        "mutated": [
            "def _lowess_wt_standardize(weights, new_entries, x_copy_i, width):\n    if False:\n        i = 10\n    \"\\n    The initial phase of creating the weights.\\n    Subtract the current x_i and divide by the width.\\n\\n    Parameters\\n    ----------\\n    weights : ndarray\\n        The memory where (new_entries - x_copy_i)/width will be placed\\n    new_entries : ndarray\\n        The x-values of the k closest points to x[i]\\n    x_copy_i : float\\n        x[i], the i'th point in the (sorted) x values\\n    width : float\\n        The maximum distance between x[i] and any point in new_entries\\n\\n    Returns\\n    -------\\n    Nothing. The modifications are made to weight in place.\\n    \"\n    weights[:] = new_entries\n    weights -= x_copy_i\n    weights /= width",
            "def _lowess_wt_standardize(weights, new_entries, x_copy_i, width):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    The initial phase of creating the weights.\\n    Subtract the current x_i and divide by the width.\\n\\n    Parameters\\n    ----------\\n    weights : ndarray\\n        The memory where (new_entries - x_copy_i)/width will be placed\\n    new_entries : ndarray\\n        The x-values of the k closest points to x[i]\\n    x_copy_i : float\\n        x[i], the i'th point in the (sorted) x values\\n    width : float\\n        The maximum distance between x[i] and any point in new_entries\\n\\n    Returns\\n    -------\\n    Nothing. The modifications are made to weight in place.\\n    \"\n    weights[:] = new_entries\n    weights -= x_copy_i\n    weights /= width",
            "def _lowess_wt_standardize(weights, new_entries, x_copy_i, width):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    The initial phase of creating the weights.\\n    Subtract the current x_i and divide by the width.\\n\\n    Parameters\\n    ----------\\n    weights : ndarray\\n        The memory where (new_entries - x_copy_i)/width will be placed\\n    new_entries : ndarray\\n        The x-values of the k closest points to x[i]\\n    x_copy_i : float\\n        x[i], the i'th point in the (sorted) x values\\n    width : float\\n        The maximum distance between x[i] and any point in new_entries\\n\\n    Returns\\n    -------\\n    Nothing. The modifications are made to weight in place.\\n    \"\n    weights[:] = new_entries\n    weights -= x_copy_i\n    weights /= width",
            "def _lowess_wt_standardize(weights, new_entries, x_copy_i, width):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    The initial phase of creating the weights.\\n    Subtract the current x_i and divide by the width.\\n\\n    Parameters\\n    ----------\\n    weights : ndarray\\n        The memory where (new_entries - x_copy_i)/width will be placed\\n    new_entries : ndarray\\n        The x-values of the k closest points to x[i]\\n    x_copy_i : float\\n        x[i], the i'th point in the (sorted) x values\\n    width : float\\n        The maximum distance between x[i] and any point in new_entries\\n\\n    Returns\\n    -------\\n    Nothing. The modifications are made to weight in place.\\n    \"\n    weights[:] = new_entries\n    weights -= x_copy_i\n    weights /= width",
            "def _lowess_wt_standardize(weights, new_entries, x_copy_i, width):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    The initial phase of creating the weights.\\n    Subtract the current x_i and divide by the width.\\n\\n    Parameters\\n    ----------\\n    weights : ndarray\\n        The memory where (new_entries - x_copy_i)/width will be placed\\n    new_entries : ndarray\\n        The x-values of the k closest points to x[i]\\n    x_copy_i : float\\n        x[i], the i'th point in the (sorted) x values\\n    width : float\\n        The maximum distance between x[i] and any point in new_entries\\n\\n    Returns\\n    -------\\n    Nothing. The modifications are made to weight in place.\\n    \"\n    weights[:] = new_entries\n    weights -= x_copy_i\n    weights /= width"
        ]
    },
    {
        "func_name": "_lowess_robustify_fit",
        "original": "def _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n):\n    \"\"\"\n    Additional weighted local linear regressions, performed if\n    iter>0. They take into account the sizes of the residuals,\n    to eliminate the effect of extreme outliers.\n\n    Parameters\n    ----------\n    x_copy : 1-d ndarray\n        The x-values/exogenous part of the data being smoothed\n    y_copy : 1-d ndarray\n        The y-values/ endogenous part of the data being smoothed\n    fitted : 1-d ndarray\n        The fitted y-values from the previous iteration\n    weights : 2-d ndarray\n        An n by k array. The contribution to the weights in the\n        local linear fit coming from the distances between the\n        x-values\n    k : int\n        The number of data points which affect the linear fit for\n        each estimated point\n    n : int\n        The total number of points\n\n   Returns\n    -------\n    Nothing. The fitted values are modified in place.\n    \"\"\"\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    residual_weights = np.copy(y_copy)\n    residual_weights.shape = (n,)\n    residual_weights -= fitted\n    residual_weights = np.absolute(residual_weights)\n    s = np.median(residual_weights)\n    residual_weights /= 6 * s\n    too_big = residual_weights >= 1\n    _lowess_bisquare(residual_weights)\n    residual_weights[too_big] = 0\n    for i in range(n):\n        total_weights = weights[i, :] * np.sqrt(residual_weights[nn_indices[0]:nn_indices[1]])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = total_weights * y_copy[nn_indices[0]:nn_indices[1]]\n        total_weights.shape = (k, 1)\n        beta = lstsq(total_weights * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)",
        "mutated": [
            "def _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n):\n    if False:\n        i = 10\n    '\\n    Additional weighted local linear regressions, performed if\\n    iter>0. They take into account the sizes of the residuals,\\n    to eliminate the effect of extreme outliers.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n    fitted : 1-d ndarray\\n        The fitted y-values from the previous iteration\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n    k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n   Returns\\n    -------\\n    Nothing. The fitted values are modified in place.\\n    '\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    residual_weights = np.copy(y_copy)\n    residual_weights.shape = (n,)\n    residual_weights -= fitted\n    residual_weights = np.absolute(residual_weights)\n    s = np.median(residual_weights)\n    residual_weights /= 6 * s\n    too_big = residual_weights >= 1\n    _lowess_bisquare(residual_weights)\n    residual_weights[too_big] = 0\n    for i in range(n):\n        total_weights = weights[i, :] * np.sqrt(residual_weights[nn_indices[0]:nn_indices[1]])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = total_weights * y_copy[nn_indices[0]:nn_indices[1]]\n        total_weights.shape = (k, 1)\n        beta = lstsq(total_weights * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)",
            "def _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Additional weighted local linear regressions, performed if\\n    iter>0. They take into account the sizes of the residuals,\\n    to eliminate the effect of extreme outliers.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n    fitted : 1-d ndarray\\n        The fitted y-values from the previous iteration\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n    k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n   Returns\\n    -------\\n    Nothing. The fitted values are modified in place.\\n    '\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    residual_weights = np.copy(y_copy)\n    residual_weights.shape = (n,)\n    residual_weights -= fitted\n    residual_weights = np.absolute(residual_weights)\n    s = np.median(residual_weights)\n    residual_weights /= 6 * s\n    too_big = residual_weights >= 1\n    _lowess_bisquare(residual_weights)\n    residual_weights[too_big] = 0\n    for i in range(n):\n        total_weights = weights[i, :] * np.sqrt(residual_weights[nn_indices[0]:nn_indices[1]])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = total_weights * y_copy[nn_indices[0]:nn_indices[1]]\n        total_weights.shape = (k, 1)\n        beta = lstsq(total_weights * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)",
            "def _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Additional weighted local linear regressions, performed if\\n    iter>0. They take into account the sizes of the residuals,\\n    to eliminate the effect of extreme outliers.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n    fitted : 1-d ndarray\\n        The fitted y-values from the previous iteration\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n    k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n   Returns\\n    -------\\n    Nothing. The fitted values are modified in place.\\n    '\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    residual_weights = np.copy(y_copy)\n    residual_weights.shape = (n,)\n    residual_weights -= fitted\n    residual_weights = np.absolute(residual_weights)\n    s = np.median(residual_weights)\n    residual_weights /= 6 * s\n    too_big = residual_weights >= 1\n    _lowess_bisquare(residual_weights)\n    residual_weights[too_big] = 0\n    for i in range(n):\n        total_weights = weights[i, :] * np.sqrt(residual_weights[nn_indices[0]:nn_indices[1]])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = total_weights * y_copy[nn_indices[0]:nn_indices[1]]\n        total_weights.shape = (k, 1)\n        beta = lstsq(total_weights * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)",
            "def _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Additional weighted local linear regressions, performed if\\n    iter>0. They take into account the sizes of the residuals,\\n    to eliminate the effect of extreme outliers.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n    fitted : 1-d ndarray\\n        The fitted y-values from the previous iteration\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n    k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n   Returns\\n    -------\\n    Nothing. The fitted values are modified in place.\\n    '\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    residual_weights = np.copy(y_copy)\n    residual_weights.shape = (n,)\n    residual_weights -= fitted\n    residual_weights = np.absolute(residual_weights)\n    s = np.median(residual_weights)\n    residual_weights /= 6 * s\n    too_big = residual_weights >= 1\n    _lowess_bisquare(residual_weights)\n    residual_weights[too_big] = 0\n    for i in range(n):\n        total_weights = weights[i, :] * np.sqrt(residual_weights[nn_indices[0]:nn_indices[1]])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = total_weights * y_copy[nn_indices[0]:nn_indices[1]]\n        total_weights.shape = (k, 1)\n        beta = lstsq(total_weights * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)",
            "def _lowess_robustify_fit(x_copy, y_copy, fitted, weights, k, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Additional weighted local linear regressions, performed if\\n    iter>0. They take into account the sizes of the residuals,\\n    to eliminate the effect of extreme outliers.\\n\\n    Parameters\\n    ----------\\n    x_copy : 1-d ndarray\\n        The x-values/exogenous part of the data being smoothed\\n    y_copy : 1-d ndarray\\n        The y-values/ endogenous part of the data being smoothed\\n    fitted : 1-d ndarray\\n        The fitted y-values from the previous iteration\\n    weights : 2-d ndarray\\n        An n by k array. The contribution to the weights in the\\n        local linear fit coming from the distances between the\\n        x-values\\n    k : int\\n        The number of data points which affect the linear fit for\\n        each estimated point\\n    n : int\\n        The total number of points\\n\\n   Returns\\n    -------\\n    Nothing. The fitted values are modified in place.\\n    '\n    nn_indices = [0, k]\n    X = np.ones((k, 2))\n    residual_weights = np.copy(y_copy)\n    residual_weights.shape = (n,)\n    residual_weights -= fitted\n    residual_weights = np.absolute(residual_weights)\n    s = np.median(residual_weights)\n    residual_weights /= 6 * s\n    too_big = residual_weights >= 1\n    _lowess_bisquare(residual_weights)\n    residual_weights[too_big] = 0\n    for i in range(n):\n        total_weights = weights[i, :] * np.sqrt(residual_weights[nn_indices[0]:nn_indices[1]])\n        X[:, 1] = x_copy[nn_indices[0]:nn_indices[1]]\n        y_i = total_weights * y_copy[nn_indices[0]:nn_indices[1]]\n        total_weights.shape = (k, 1)\n        beta = lstsq(total_weights * X, y_i, rcond=-1)[0]\n        fitted[i] = beta[0] + beta[1] * x_copy[i]\n        _lowess_update_nn(x_copy, nn_indices, i + 1)"
        ]
    },
    {
        "func_name": "_lowess_update_nn",
        "original": "def _lowess_update_nn(x, cur_nn, i):\n    \"\"\"\n    Update the endpoints of the nearest neighbors to\n    the ith point.\n\n    Parameters\n    ----------\n    x : iterable\n        The sorted points of x-values\n    cur_nn : list of length 2\n        The two current indices between which are the\n        k closest points to x[i]. (The actual value of\n        k is irrelevant for the algorithm.\n    i : int\n        The index of the current value in x for which\n        the k closest points are desired.\n\n    Returns\n    -------\n    Nothing. It modifies cur_nn in place.\n    \"\"\"\n    while True:\n        if cur_nn[1] < x.size:\n            left_dist = x[i] - x[cur_nn[0]]\n            new_right_dist = x[cur_nn[1]] - x[i]\n            if new_right_dist < left_dist:\n                cur_nn[0] = cur_nn[0] + 1\n                cur_nn[1] = cur_nn[1] + 1\n            else:\n                break\n        else:\n            break",
        "mutated": [
            "def _lowess_update_nn(x, cur_nn, i):\n    if False:\n        i = 10\n    '\\n    Update the endpoints of the nearest neighbors to\\n    the ith point.\\n\\n    Parameters\\n    ----------\\n    x : iterable\\n        The sorted points of x-values\\n    cur_nn : list of length 2\\n        The two current indices between which are the\\n        k closest points to x[i]. (The actual value of\\n        k is irrelevant for the algorithm.\\n    i : int\\n        The index of the current value in x for which\\n        the k closest points are desired.\\n\\n    Returns\\n    -------\\n    Nothing. It modifies cur_nn in place.\\n    '\n    while True:\n        if cur_nn[1] < x.size:\n            left_dist = x[i] - x[cur_nn[0]]\n            new_right_dist = x[cur_nn[1]] - x[i]\n            if new_right_dist < left_dist:\n                cur_nn[0] = cur_nn[0] + 1\n                cur_nn[1] = cur_nn[1] + 1\n            else:\n                break\n        else:\n            break",
            "def _lowess_update_nn(x, cur_nn, i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Update the endpoints of the nearest neighbors to\\n    the ith point.\\n\\n    Parameters\\n    ----------\\n    x : iterable\\n        The sorted points of x-values\\n    cur_nn : list of length 2\\n        The two current indices between which are the\\n        k closest points to x[i]. (The actual value of\\n        k is irrelevant for the algorithm.\\n    i : int\\n        The index of the current value in x for which\\n        the k closest points are desired.\\n\\n    Returns\\n    -------\\n    Nothing. It modifies cur_nn in place.\\n    '\n    while True:\n        if cur_nn[1] < x.size:\n            left_dist = x[i] - x[cur_nn[0]]\n            new_right_dist = x[cur_nn[1]] - x[i]\n            if new_right_dist < left_dist:\n                cur_nn[0] = cur_nn[0] + 1\n                cur_nn[1] = cur_nn[1] + 1\n            else:\n                break\n        else:\n            break",
            "def _lowess_update_nn(x, cur_nn, i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Update the endpoints of the nearest neighbors to\\n    the ith point.\\n\\n    Parameters\\n    ----------\\n    x : iterable\\n        The sorted points of x-values\\n    cur_nn : list of length 2\\n        The two current indices between which are the\\n        k closest points to x[i]. (The actual value of\\n        k is irrelevant for the algorithm.\\n    i : int\\n        The index of the current value in x for which\\n        the k closest points are desired.\\n\\n    Returns\\n    -------\\n    Nothing. It modifies cur_nn in place.\\n    '\n    while True:\n        if cur_nn[1] < x.size:\n            left_dist = x[i] - x[cur_nn[0]]\n            new_right_dist = x[cur_nn[1]] - x[i]\n            if new_right_dist < left_dist:\n                cur_nn[0] = cur_nn[0] + 1\n                cur_nn[1] = cur_nn[1] + 1\n            else:\n                break\n        else:\n            break",
            "def _lowess_update_nn(x, cur_nn, i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Update the endpoints of the nearest neighbors to\\n    the ith point.\\n\\n    Parameters\\n    ----------\\n    x : iterable\\n        The sorted points of x-values\\n    cur_nn : list of length 2\\n        The two current indices between which are the\\n        k closest points to x[i]. (The actual value of\\n        k is irrelevant for the algorithm.\\n    i : int\\n        The index of the current value in x for which\\n        the k closest points are desired.\\n\\n    Returns\\n    -------\\n    Nothing. It modifies cur_nn in place.\\n    '\n    while True:\n        if cur_nn[1] < x.size:\n            left_dist = x[i] - x[cur_nn[0]]\n            new_right_dist = x[cur_nn[1]] - x[i]\n            if new_right_dist < left_dist:\n                cur_nn[0] = cur_nn[0] + 1\n                cur_nn[1] = cur_nn[1] + 1\n            else:\n                break\n        else:\n            break",
            "def _lowess_update_nn(x, cur_nn, i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Update the endpoints of the nearest neighbors to\\n    the ith point.\\n\\n    Parameters\\n    ----------\\n    x : iterable\\n        The sorted points of x-values\\n    cur_nn : list of length 2\\n        The two current indices between which are the\\n        k closest points to x[i]. (The actual value of\\n        k is irrelevant for the algorithm.\\n    i : int\\n        The index of the current value in x for which\\n        the k closest points are desired.\\n\\n    Returns\\n    -------\\n    Nothing. It modifies cur_nn in place.\\n    '\n    while True:\n        if cur_nn[1] < x.size:\n            left_dist = x[i] - x[cur_nn[0]]\n            new_right_dist = x[cur_nn[1]] - x[i]\n            if new_right_dist < left_dist:\n                cur_nn[0] = cur_nn[0] + 1\n                cur_nn[1] = cur_nn[1] + 1\n            else:\n                break\n        else:\n            break"
        ]
    },
    {
        "func_name": "_lowess_tricube",
        "original": "def _lowess_tricube(t):\n    \"\"\"\n    The _tricube function applied to a numpy array.\n    The tricube function is (1-abs(t)**3)**3.\n\n    Parameters\n    ----------\n    t : ndarray\n        Array the tricube function is applied to elementwise and\n        in-place.\n\n    Returns\n    -------\n    Nothing\n    \"\"\"\n    t[:] = np.absolute(t)\n    _lowess_mycube(t)\n    t[:] = np.negative(t)\n    t += 1\n    _lowess_mycube(t)",
        "mutated": [
            "def _lowess_tricube(t):\n    if False:\n        i = 10\n    '\\n    The _tricube function applied to a numpy array.\\n    The tricube function is (1-abs(t)**3)**3.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array the tricube function is applied to elementwise and\\n        in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t[:] = np.absolute(t)\n    _lowess_mycube(t)\n    t[:] = np.negative(t)\n    t += 1\n    _lowess_mycube(t)",
            "def _lowess_tricube(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    The _tricube function applied to a numpy array.\\n    The tricube function is (1-abs(t)**3)**3.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array the tricube function is applied to elementwise and\\n        in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t[:] = np.absolute(t)\n    _lowess_mycube(t)\n    t[:] = np.negative(t)\n    t += 1\n    _lowess_mycube(t)",
            "def _lowess_tricube(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    The _tricube function applied to a numpy array.\\n    The tricube function is (1-abs(t)**3)**3.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array the tricube function is applied to elementwise and\\n        in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t[:] = np.absolute(t)\n    _lowess_mycube(t)\n    t[:] = np.negative(t)\n    t += 1\n    _lowess_mycube(t)",
            "def _lowess_tricube(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    The _tricube function applied to a numpy array.\\n    The tricube function is (1-abs(t)**3)**3.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array the tricube function is applied to elementwise and\\n        in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t[:] = np.absolute(t)\n    _lowess_mycube(t)\n    t[:] = np.negative(t)\n    t += 1\n    _lowess_mycube(t)",
            "def _lowess_tricube(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    The _tricube function applied to a numpy array.\\n    The tricube function is (1-abs(t)**3)**3.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array the tricube function is applied to elementwise and\\n        in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t[:] = np.absolute(t)\n    _lowess_mycube(t)\n    t[:] = np.negative(t)\n    t += 1\n    _lowess_mycube(t)"
        ]
    },
    {
        "func_name": "_lowess_mycube",
        "original": "def _lowess_mycube(t):\n    \"\"\"\n    Fast matrix cube\n\n    Parameters\n    ----------\n    t : ndarray\n        Array that is cubed, elementwise and in-place\n\n    Returns\n    -------\n    Nothing\n    \"\"\"\n    t2 = t * t\n    t *= t2",
        "mutated": [
            "def _lowess_mycube(t):\n    if False:\n        i = 10\n    '\\n    Fast matrix cube\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array that is cubed, elementwise and in-place\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t2 = t * t\n    t *= t2",
            "def _lowess_mycube(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Fast matrix cube\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array that is cubed, elementwise and in-place\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t2 = t * t\n    t *= t2",
            "def _lowess_mycube(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Fast matrix cube\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array that is cubed, elementwise and in-place\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t2 = t * t\n    t *= t2",
            "def _lowess_mycube(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Fast matrix cube\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array that is cubed, elementwise and in-place\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t2 = t * t\n    t *= t2",
            "def _lowess_mycube(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Fast matrix cube\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        Array that is cubed, elementwise and in-place\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t2 = t * t\n    t *= t2"
        ]
    },
    {
        "func_name": "_lowess_bisquare",
        "original": "def _lowess_bisquare(t):\n    \"\"\"\n    The bisquare function applied to a numpy array.\n    The bisquare function is (1-t**2)**2.\n\n    Parameters\n    ----------\n    t : ndarray\n        array bisquare function is applied to, element-wise and in-place.\n\n    Returns\n    -------\n    Nothing\n    \"\"\"\n    t *= t\n    t[:] = np.negative(t)\n    t += 1\n    t *= t",
        "mutated": [
            "def _lowess_bisquare(t):\n    if False:\n        i = 10\n    '\\n    The bisquare function applied to a numpy array.\\n    The bisquare function is (1-t**2)**2.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        array bisquare function is applied to, element-wise and in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t *= t\n    t[:] = np.negative(t)\n    t += 1\n    t *= t",
            "def _lowess_bisquare(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    The bisquare function applied to a numpy array.\\n    The bisquare function is (1-t**2)**2.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        array bisquare function is applied to, element-wise and in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t *= t\n    t[:] = np.negative(t)\n    t += 1\n    t *= t",
            "def _lowess_bisquare(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    The bisquare function applied to a numpy array.\\n    The bisquare function is (1-t**2)**2.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        array bisquare function is applied to, element-wise and in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t *= t\n    t[:] = np.negative(t)\n    t += 1\n    t *= t",
            "def _lowess_bisquare(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    The bisquare function applied to a numpy array.\\n    The bisquare function is (1-t**2)**2.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        array bisquare function is applied to, element-wise and in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t *= t\n    t[:] = np.negative(t)\n    t += 1\n    t *= t",
            "def _lowess_bisquare(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    The bisquare function applied to a numpy array.\\n    The bisquare function is (1-t**2)**2.\\n\\n    Parameters\\n    ----------\\n    t : ndarray\\n        array bisquare function is applied to, element-wise and in-place.\\n\\n    Returns\\n    -------\\n    Nothing\\n    '\n    t *= t\n    t[:] = np.negative(t)\n    t += 1\n    t *= t"
        ]
    }
]