[
    {
        "func_name": "column_imbalance_score",
        "original": "def column_imbalance_score(value_counts: pd.Series, n_classes: int) -> Union[float, int]:\n    \"\"\"column_imbalance_score\n\n    The class balance score for categorical and boolean variables uses entropy to calculate a  bounded score between 0 and 1.\n    A perfectly uniform distribution would return a score of 0, and a perfectly imbalanced distribution would return a score of 1.\n\n    When dealing with probabilities with finite values (e.g categorical), entropy is maximised the \u2018flatter\u2019 the distribution is. (Jaynes: Probability Theory, The Logic of Science)\n    To calculate the class imbalance, we calculate the entropy of that distribution and the maximum possible entropy for that number of classes.\n    To calculate the entropy of the 'distribution' we use value counts (e.g frequency of classes) and we can determine the maximum entropy as log2(number of classes).\n    We then divide the entropy by the maximum possible entropy to get a value between 0 and 1 which we then subtract from 1.\n\n    Args:\n        value_counts (pd.Series): frequency of each category\n        n_classes (int): number of classes\n\n    Returns:\n        Union[float, int]: float or integer bounded between 0 and 1 inclusively\n    \"\"\"\n    if n_classes > 1:\n        value_counts = value_counts.to_numpy(dtype=float)\n        return 1 - entropy(value_counts, base=2) / log2(n_classes)\n    return 0",
        "mutated": [
            "def column_imbalance_score(value_counts: pd.Series, n_classes: int) -> Union[float, int]:\n    if False:\n        i = 10\n    \"column_imbalance_score\\n\\n    The class balance score for categorical and boolean variables uses entropy to calculate a  bounded score between 0 and 1.\\n    A perfectly uniform distribution would return a score of 0, and a perfectly imbalanced distribution would return a score of 1.\\n\\n    When dealing with probabilities with finite values (e.g categorical), entropy is maximised the \u2018flatter\u2019 the distribution is. (Jaynes: Probability Theory, The Logic of Science)\\n    To calculate the class imbalance, we calculate the entropy of that distribution and the maximum possible entropy for that number of classes.\\n    To calculate the entropy of the 'distribution' we use value counts (e.g frequency of classes) and we can determine the maximum entropy as log2(number of classes).\\n    We then divide the entropy by the maximum possible entropy to get a value between 0 and 1 which we then subtract from 1.\\n\\n    Args:\\n        value_counts (pd.Series): frequency of each category\\n        n_classes (int): number of classes\\n\\n    Returns:\\n        Union[float, int]: float or integer bounded between 0 and 1 inclusively\\n    \"\n    if n_classes > 1:\n        value_counts = value_counts.to_numpy(dtype=float)\n        return 1 - entropy(value_counts, base=2) / log2(n_classes)\n    return 0",
            "def column_imbalance_score(value_counts: pd.Series, n_classes: int) -> Union[float, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"column_imbalance_score\\n\\n    The class balance score for categorical and boolean variables uses entropy to calculate a  bounded score between 0 and 1.\\n    A perfectly uniform distribution would return a score of 0, and a perfectly imbalanced distribution would return a score of 1.\\n\\n    When dealing with probabilities with finite values (e.g categorical), entropy is maximised the \u2018flatter\u2019 the distribution is. (Jaynes: Probability Theory, The Logic of Science)\\n    To calculate the class imbalance, we calculate the entropy of that distribution and the maximum possible entropy for that number of classes.\\n    To calculate the entropy of the 'distribution' we use value counts (e.g frequency of classes) and we can determine the maximum entropy as log2(number of classes).\\n    We then divide the entropy by the maximum possible entropy to get a value between 0 and 1 which we then subtract from 1.\\n\\n    Args:\\n        value_counts (pd.Series): frequency of each category\\n        n_classes (int): number of classes\\n\\n    Returns:\\n        Union[float, int]: float or integer bounded between 0 and 1 inclusively\\n    \"\n    if n_classes > 1:\n        value_counts = value_counts.to_numpy(dtype=float)\n        return 1 - entropy(value_counts, base=2) / log2(n_classes)\n    return 0",
            "def column_imbalance_score(value_counts: pd.Series, n_classes: int) -> Union[float, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"column_imbalance_score\\n\\n    The class balance score for categorical and boolean variables uses entropy to calculate a  bounded score between 0 and 1.\\n    A perfectly uniform distribution would return a score of 0, and a perfectly imbalanced distribution would return a score of 1.\\n\\n    When dealing with probabilities with finite values (e.g categorical), entropy is maximised the \u2018flatter\u2019 the distribution is. (Jaynes: Probability Theory, The Logic of Science)\\n    To calculate the class imbalance, we calculate the entropy of that distribution and the maximum possible entropy for that number of classes.\\n    To calculate the entropy of the 'distribution' we use value counts (e.g frequency of classes) and we can determine the maximum entropy as log2(number of classes).\\n    We then divide the entropy by the maximum possible entropy to get a value between 0 and 1 which we then subtract from 1.\\n\\n    Args:\\n        value_counts (pd.Series): frequency of each category\\n        n_classes (int): number of classes\\n\\n    Returns:\\n        Union[float, int]: float or integer bounded between 0 and 1 inclusively\\n    \"\n    if n_classes > 1:\n        value_counts = value_counts.to_numpy(dtype=float)\n        return 1 - entropy(value_counts, base=2) / log2(n_classes)\n    return 0",
            "def column_imbalance_score(value_counts: pd.Series, n_classes: int) -> Union[float, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"column_imbalance_score\\n\\n    The class balance score for categorical and boolean variables uses entropy to calculate a  bounded score between 0 and 1.\\n    A perfectly uniform distribution would return a score of 0, and a perfectly imbalanced distribution would return a score of 1.\\n\\n    When dealing with probabilities with finite values (e.g categorical), entropy is maximised the \u2018flatter\u2019 the distribution is. (Jaynes: Probability Theory, The Logic of Science)\\n    To calculate the class imbalance, we calculate the entropy of that distribution and the maximum possible entropy for that number of classes.\\n    To calculate the entropy of the 'distribution' we use value counts (e.g frequency of classes) and we can determine the maximum entropy as log2(number of classes).\\n    We then divide the entropy by the maximum possible entropy to get a value between 0 and 1 which we then subtract from 1.\\n\\n    Args:\\n        value_counts (pd.Series): frequency of each category\\n        n_classes (int): number of classes\\n\\n    Returns:\\n        Union[float, int]: float or integer bounded between 0 and 1 inclusively\\n    \"\n    if n_classes > 1:\n        value_counts = value_counts.to_numpy(dtype=float)\n        return 1 - entropy(value_counts, base=2) / log2(n_classes)\n    return 0",
            "def column_imbalance_score(value_counts: pd.Series, n_classes: int) -> Union[float, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"column_imbalance_score\\n\\n    The class balance score for categorical and boolean variables uses entropy to calculate a  bounded score between 0 and 1.\\n    A perfectly uniform distribution would return a score of 0, and a perfectly imbalanced distribution would return a score of 1.\\n\\n    When dealing with probabilities with finite values (e.g categorical), entropy is maximised the \u2018flatter\u2019 the distribution is. (Jaynes: Probability Theory, The Logic of Science)\\n    To calculate the class imbalance, we calculate the entropy of that distribution and the maximum possible entropy for that number of classes.\\n    To calculate the entropy of the 'distribution' we use value counts (e.g frequency of classes) and we can determine the maximum entropy as log2(number of classes).\\n    We then divide the entropy by the maximum possible entropy to get a value between 0 and 1 which we then subtract from 1.\\n\\n    Args:\\n        value_counts (pd.Series): frequency of each category\\n        n_classes (int): number of classes\\n\\n    Returns:\\n        Union[float, int]: float or integer bounded between 0 and 1 inclusively\\n    \"\n    if n_classes > 1:\n        value_counts = value_counts.to_numpy(dtype=float)\n        return 1 - entropy(value_counts, base=2) / log2(n_classes)\n    return 0"
        ]
    }
]