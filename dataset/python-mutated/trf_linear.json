[
    {
        "func_name": "regularized_lsq_with_qr",
        "original": "def regularized_lsq_with_qr(m, n, R, QTb, perm, diag, copy_R=True):\n    \"\"\"Solve regularized least squares using information from QR-decomposition.\n\n    The initial problem is to solve the following system in a least-squares\n    sense::\n\n        A x = b\n        D x = 0\n\n    where D is diagonal matrix. The method is based on QR decomposition\n    of the form A P = Q R, where P is a column permutation matrix, Q is an\n    orthogonal matrix and R is an upper triangular matrix.\n\n    Parameters\n    ----------\n    m, n : int\n        Initial shape of A.\n    R : ndarray, shape (n, n)\n        Upper triangular matrix from QR decomposition of A.\n    QTb : ndarray, shape (n,)\n        First n components of Q^T b.\n    perm : ndarray, shape (n,)\n        Array defining column permutation of A, such that ith column of\n        P is perm[i]-th column of identity matrix.\n    diag : ndarray, shape (n,)\n        Array containing diagonal elements of D.\n\n    Returns\n    -------\n    x : ndarray, shape (n,)\n        Found least-squares solution.\n    \"\"\"\n    if copy_R:\n        R = R.copy()\n    v = QTb.copy()\n    givens_elimination(R, v, diag[perm])\n    abs_diag_R = np.abs(np.diag(R))\n    threshold = EPS * max(m, n) * np.max(abs_diag_R)\n    (nns,) = np.nonzero(abs_diag_R > threshold)\n    R = R[np.ix_(nns, nns)]\n    v = v[nns]\n    x = np.zeros(n)\n    x[perm[nns]] = solve_triangular(R, v)\n    return x",
        "mutated": [
            "def regularized_lsq_with_qr(m, n, R, QTb, perm, diag, copy_R=True):\n    if False:\n        i = 10\n    'Solve regularized least squares using information from QR-decomposition.\\n\\n    The initial problem is to solve the following system in a least-squares\\n    sense::\\n\\n        A x = b\\n        D x = 0\\n\\n    where D is diagonal matrix. The method is based on QR decomposition\\n    of the form A P = Q R, where P is a column permutation matrix, Q is an\\n    orthogonal matrix and R is an upper triangular matrix.\\n\\n    Parameters\\n    ----------\\n    m, n : int\\n        Initial shape of A.\\n    R : ndarray, shape (n, n)\\n        Upper triangular matrix from QR decomposition of A.\\n    QTb : ndarray, shape (n,)\\n        First n components of Q^T b.\\n    perm : ndarray, shape (n,)\\n        Array defining column permutation of A, such that ith column of\\n        P is perm[i]-th column of identity matrix.\\n    diag : ndarray, shape (n,)\\n        Array containing diagonal elements of D.\\n\\n    Returns\\n    -------\\n    x : ndarray, shape (n,)\\n        Found least-squares solution.\\n    '\n    if copy_R:\n        R = R.copy()\n    v = QTb.copy()\n    givens_elimination(R, v, diag[perm])\n    abs_diag_R = np.abs(np.diag(R))\n    threshold = EPS * max(m, n) * np.max(abs_diag_R)\n    (nns,) = np.nonzero(abs_diag_R > threshold)\n    R = R[np.ix_(nns, nns)]\n    v = v[nns]\n    x = np.zeros(n)\n    x[perm[nns]] = solve_triangular(R, v)\n    return x",
            "def regularized_lsq_with_qr(m, n, R, QTb, perm, diag, copy_R=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Solve regularized least squares using information from QR-decomposition.\\n\\n    The initial problem is to solve the following system in a least-squares\\n    sense::\\n\\n        A x = b\\n        D x = 0\\n\\n    where D is diagonal matrix. The method is based on QR decomposition\\n    of the form A P = Q R, where P is a column permutation matrix, Q is an\\n    orthogonal matrix and R is an upper triangular matrix.\\n\\n    Parameters\\n    ----------\\n    m, n : int\\n        Initial shape of A.\\n    R : ndarray, shape (n, n)\\n        Upper triangular matrix from QR decomposition of A.\\n    QTb : ndarray, shape (n,)\\n        First n components of Q^T b.\\n    perm : ndarray, shape (n,)\\n        Array defining column permutation of A, such that ith column of\\n        P is perm[i]-th column of identity matrix.\\n    diag : ndarray, shape (n,)\\n        Array containing diagonal elements of D.\\n\\n    Returns\\n    -------\\n    x : ndarray, shape (n,)\\n        Found least-squares solution.\\n    '\n    if copy_R:\n        R = R.copy()\n    v = QTb.copy()\n    givens_elimination(R, v, diag[perm])\n    abs_diag_R = np.abs(np.diag(R))\n    threshold = EPS * max(m, n) * np.max(abs_diag_R)\n    (nns,) = np.nonzero(abs_diag_R > threshold)\n    R = R[np.ix_(nns, nns)]\n    v = v[nns]\n    x = np.zeros(n)\n    x[perm[nns]] = solve_triangular(R, v)\n    return x",
            "def regularized_lsq_with_qr(m, n, R, QTb, perm, diag, copy_R=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Solve regularized least squares using information from QR-decomposition.\\n\\n    The initial problem is to solve the following system in a least-squares\\n    sense::\\n\\n        A x = b\\n        D x = 0\\n\\n    where D is diagonal matrix. The method is based on QR decomposition\\n    of the form A P = Q R, where P is a column permutation matrix, Q is an\\n    orthogonal matrix and R is an upper triangular matrix.\\n\\n    Parameters\\n    ----------\\n    m, n : int\\n        Initial shape of A.\\n    R : ndarray, shape (n, n)\\n        Upper triangular matrix from QR decomposition of A.\\n    QTb : ndarray, shape (n,)\\n        First n components of Q^T b.\\n    perm : ndarray, shape (n,)\\n        Array defining column permutation of A, such that ith column of\\n        P is perm[i]-th column of identity matrix.\\n    diag : ndarray, shape (n,)\\n        Array containing diagonal elements of D.\\n\\n    Returns\\n    -------\\n    x : ndarray, shape (n,)\\n        Found least-squares solution.\\n    '\n    if copy_R:\n        R = R.copy()\n    v = QTb.copy()\n    givens_elimination(R, v, diag[perm])\n    abs_diag_R = np.abs(np.diag(R))\n    threshold = EPS * max(m, n) * np.max(abs_diag_R)\n    (nns,) = np.nonzero(abs_diag_R > threshold)\n    R = R[np.ix_(nns, nns)]\n    v = v[nns]\n    x = np.zeros(n)\n    x[perm[nns]] = solve_triangular(R, v)\n    return x",
            "def regularized_lsq_with_qr(m, n, R, QTb, perm, diag, copy_R=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Solve regularized least squares using information from QR-decomposition.\\n\\n    The initial problem is to solve the following system in a least-squares\\n    sense::\\n\\n        A x = b\\n        D x = 0\\n\\n    where D is diagonal matrix. The method is based on QR decomposition\\n    of the form A P = Q R, where P is a column permutation matrix, Q is an\\n    orthogonal matrix and R is an upper triangular matrix.\\n\\n    Parameters\\n    ----------\\n    m, n : int\\n        Initial shape of A.\\n    R : ndarray, shape (n, n)\\n        Upper triangular matrix from QR decomposition of A.\\n    QTb : ndarray, shape (n,)\\n        First n components of Q^T b.\\n    perm : ndarray, shape (n,)\\n        Array defining column permutation of A, such that ith column of\\n        P is perm[i]-th column of identity matrix.\\n    diag : ndarray, shape (n,)\\n        Array containing diagonal elements of D.\\n\\n    Returns\\n    -------\\n    x : ndarray, shape (n,)\\n        Found least-squares solution.\\n    '\n    if copy_R:\n        R = R.copy()\n    v = QTb.copy()\n    givens_elimination(R, v, diag[perm])\n    abs_diag_R = np.abs(np.diag(R))\n    threshold = EPS * max(m, n) * np.max(abs_diag_R)\n    (nns,) = np.nonzero(abs_diag_R > threshold)\n    R = R[np.ix_(nns, nns)]\n    v = v[nns]\n    x = np.zeros(n)\n    x[perm[nns]] = solve_triangular(R, v)\n    return x",
            "def regularized_lsq_with_qr(m, n, R, QTb, perm, diag, copy_R=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Solve regularized least squares using information from QR-decomposition.\\n\\n    The initial problem is to solve the following system in a least-squares\\n    sense::\\n\\n        A x = b\\n        D x = 0\\n\\n    where D is diagonal matrix. The method is based on QR decomposition\\n    of the form A P = Q R, where P is a column permutation matrix, Q is an\\n    orthogonal matrix and R is an upper triangular matrix.\\n\\n    Parameters\\n    ----------\\n    m, n : int\\n        Initial shape of A.\\n    R : ndarray, shape (n, n)\\n        Upper triangular matrix from QR decomposition of A.\\n    QTb : ndarray, shape (n,)\\n        First n components of Q^T b.\\n    perm : ndarray, shape (n,)\\n        Array defining column permutation of A, such that ith column of\\n        P is perm[i]-th column of identity matrix.\\n    diag : ndarray, shape (n,)\\n        Array containing diagonal elements of D.\\n\\n    Returns\\n    -------\\n    x : ndarray, shape (n,)\\n        Found least-squares solution.\\n    '\n    if copy_R:\n        R = R.copy()\n    v = QTb.copy()\n    givens_elimination(R, v, diag[perm])\n    abs_diag_R = np.abs(np.diag(R))\n    threshold = EPS * max(m, n) * np.max(abs_diag_R)\n    (nns,) = np.nonzero(abs_diag_R > threshold)\n    R = R[np.ix_(nns, nns)]\n    v = v[nns]\n    x = np.zeros(n)\n    x[perm[nns]] = solve_triangular(R, v)\n    return x"
        ]
    },
    {
        "func_name": "backtracking",
        "original": "def backtracking(A, g, x, p, theta, p_dot_g, lb, ub):\n    \"\"\"Find an appropriate step size using backtracking line search.\"\"\"\n    alpha = 1\n    while True:\n        (x_new, _) = reflective_transformation(x + alpha * p, lb, ub)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change > -0.1 * alpha * p_dot_g:\n            break\n        alpha *= 0.5\n    active = find_active_constraints(x_new, lb, ub)\n    if np.any(active != 0):\n        (x_new, _) = reflective_transformation(x + theta * alpha * p, lb, ub)\n        x_new = make_strictly_feasible(x_new, lb, ub, rstep=0)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n    return (x, step, cost_change)",
        "mutated": [
            "def backtracking(A, g, x, p, theta, p_dot_g, lb, ub):\n    if False:\n        i = 10\n    'Find an appropriate step size using backtracking line search.'\n    alpha = 1\n    while True:\n        (x_new, _) = reflective_transformation(x + alpha * p, lb, ub)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change > -0.1 * alpha * p_dot_g:\n            break\n        alpha *= 0.5\n    active = find_active_constraints(x_new, lb, ub)\n    if np.any(active != 0):\n        (x_new, _) = reflective_transformation(x + theta * alpha * p, lb, ub)\n        x_new = make_strictly_feasible(x_new, lb, ub, rstep=0)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n    return (x, step, cost_change)",
            "def backtracking(A, g, x, p, theta, p_dot_g, lb, ub):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Find an appropriate step size using backtracking line search.'\n    alpha = 1\n    while True:\n        (x_new, _) = reflective_transformation(x + alpha * p, lb, ub)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change > -0.1 * alpha * p_dot_g:\n            break\n        alpha *= 0.5\n    active = find_active_constraints(x_new, lb, ub)\n    if np.any(active != 0):\n        (x_new, _) = reflective_transformation(x + theta * alpha * p, lb, ub)\n        x_new = make_strictly_feasible(x_new, lb, ub, rstep=0)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n    return (x, step, cost_change)",
            "def backtracking(A, g, x, p, theta, p_dot_g, lb, ub):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Find an appropriate step size using backtracking line search.'\n    alpha = 1\n    while True:\n        (x_new, _) = reflective_transformation(x + alpha * p, lb, ub)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change > -0.1 * alpha * p_dot_g:\n            break\n        alpha *= 0.5\n    active = find_active_constraints(x_new, lb, ub)\n    if np.any(active != 0):\n        (x_new, _) = reflective_transformation(x + theta * alpha * p, lb, ub)\n        x_new = make_strictly_feasible(x_new, lb, ub, rstep=0)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n    return (x, step, cost_change)",
            "def backtracking(A, g, x, p, theta, p_dot_g, lb, ub):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Find an appropriate step size using backtracking line search.'\n    alpha = 1\n    while True:\n        (x_new, _) = reflective_transformation(x + alpha * p, lb, ub)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change > -0.1 * alpha * p_dot_g:\n            break\n        alpha *= 0.5\n    active = find_active_constraints(x_new, lb, ub)\n    if np.any(active != 0):\n        (x_new, _) = reflective_transformation(x + theta * alpha * p, lb, ub)\n        x_new = make_strictly_feasible(x_new, lb, ub, rstep=0)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n    return (x, step, cost_change)",
            "def backtracking(A, g, x, p, theta, p_dot_g, lb, ub):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Find an appropriate step size using backtracking line search.'\n    alpha = 1\n    while True:\n        (x_new, _) = reflective_transformation(x + alpha * p, lb, ub)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change > -0.1 * alpha * p_dot_g:\n            break\n        alpha *= 0.5\n    active = find_active_constraints(x_new, lb, ub)\n    if np.any(active != 0):\n        (x_new, _) = reflective_transformation(x + theta * alpha * p, lb, ub)\n        x_new = make_strictly_feasible(x_new, lb, ub, rstep=0)\n        step = x_new - x\n        cost_change = -evaluate_quadratic(A, g, step)\n    return (x, step, cost_change)"
        ]
    },
    {
        "func_name": "select_step",
        "original": "def select_step(x, A_h, g_h, c_h, p, p_h, d, lb, ub, theta):\n    \"\"\"Select the best step according to Trust Region Reflective algorithm.\"\"\"\n    if in_bounds(x + p, lb, ub):\n        return p\n    (p_stride, hits) = step_size_to_bound(x, p, lb, ub)\n    r_h = np.copy(p_h)\n    r_h[hits.astype(bool)] *= -1\n    r = d * r_h\n    p *= p_stride\n    p_h *= p_stride\n    x_on_bound = x + p\n    (r_stride_u, _) = step_size_to_bound(x_on_bound, r, lb, ub)\n    r_stride_l = (1 - theta) * r_stride_u\n    r_stride_u *= theta\n    if r_stride_u > 0:\n        (a, b, c) = build_quadratic_1d(A_h, g_h, r_h, s0=p_h, diag=c_h)\n        (r_stride, r_value) = minimize_quadratic_1d(a, b, r_stride_l, r_stride_u, c=c)\n        r_h = p_h + r_h * r_stride\n        r = d * r_h\n    else:\n        r_value = np.inf\n    p_h *= theta\n    p *= theta\n    p_value = evaluate_quadratic(A_h, g_h, p_h, diag=c_h)\n    ag_h = -g_h\n    ag = d * ag_h\n    (ag_stride_u, _) = step_size_to_bound(x, ag, lb, ub)\n    ag_stride_u *= theta\n    (a, b) = build_quadratic_1d(A_h, g_h, ag_h, diag=c_h)\n    (ag_stride, ag_value) = minimize_quadratic_1d(a, b, 0, ag_stride_u)\n    ag *= ag_stride\n    if p_value < r_value and p_value < ag_value:\n        return p\n    elif r_value < p_value and r_value < ag_value:\n        return r\n    else:\n        return ag",
        "mutated": [
            "def select_step(x, A_h, g_h, c_h, p, p_h, d, lb, ub, theta):\n    if False:\n        i = 10\n    'Select the best step according to Trust Region Reflective algorithm.'\n    if in_bounds(x + p, lb, ub):\n        return p\n    (p_stride, hits) = step_size_to_bound(x, p, lb, ub)\n    r_h = np.copy(p_h)\n    r_h[hits.astype(bool)] *= -1\n    r = d * r_h\n    p *= p_stride\n    p_h *= p_stride\n    x_on_bound = x + p\n    (r_stride_u, _) = step_size_to_bound(x_on_bound, r, lb, ub)\n    r_stride_l = (1 - theta) * r_stride_u\n    r_stride_u *= theta\n    if r_stride_u > 0:\n        (a, b, c) = build_quadratic_1d(A_h, g_h, r_h, s0=p_h, diag=c_h)\n        (r_stride, r_value) = minimize_quadratic_1d(a, b, r_stride_l, r_stride_u, c=c)\n        r_h = p_h + r_h * r_stride\n        r = d * r_h\n    else:\n        r_value = np.inf\n    p_h *= theta\n    p *= theta\n    p_value = evaluate_quadratic(A_h, g_h, p_h, diag=c_h)\n    ag_h = -g_h\n    ag = d * ag_h\n    (ag_stride_u, _) = step_size_to_bound(x, ag, lb, ub)\n    ag_stride_u *= theta\n    (a, b) = build_quadratic_1d(A_h, g_h, ag_h, diag=c_h)\n    (ag_stride, ag_value) = minimize_quadratic_1d(a, b, 0, ag_stride_u)\n    ag *= ag_stride\n    if p_value < r_value and p_value < ag_value:\n        return p\n    elif r_value < p_value and r_value < ag_value:\n        return r\n    else:\n        return ag",
            "def select_step(x, A_h, g_h, c_h, p, p_h, d, lb, ub, theta):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Select the best step according to Trust Region Reflective algorithm.'\n    if in_bounds(x + p, lb, ub):\n        return p\n    (p_stride, hits) = step_size_to_bound(x, p, lb, ub)\n    r_h = np.copy(p_h)\n    r_h[hits.astype(bool)] *= -1\n    r = d * r_h\n    p *= p_stride\n    p_h *= p_stride\n    x_on_bound = x + p\n    (r_stride_u, _) = step_size_to_bound(x_on_bound, r, lb, ub)\n    r_stride_l = (1 - theta) * r_stride_u\n    r_stride_u *= theta\n    if r_stride_u > 0:\n        (a, b, c) = build_quadratic_1d(A_h, g_h, r_h, s0=p_h, diag=c_h)\n        (r_stride, r_value) = minimize_quadratic_1d(a, b, r_stride_l, r_stride_u, c=c)\n        r_h = p_h + r_h * r_stride\n        r = d * r_h\n    else:\n        r_value = np.inf\n    p_h *= theta\n    p *= theta\n    p_value = evaluate_quadratic(A_h, g_h, p_h, diag=c_h)\n    ag_h = -g_h\n    ag = d * ag_h\n    (ag_stride_u, _) = step_size_to_bound(x, ag, lb, ub)\n    ag_stride_u *= theta\n    (a, b) = build_quadratic_1d(A_h, g_h, ag_h, diag=c_h)\n    (ag_stride, ag_value) = minimize_quadratic_1d(a, b, 0, ag_stride_u)\n    ag *= ag_stride\n    if p_value < r_value and p_value < ag_value:\n        return p\n    elif r_value < p_value and r_value < ag_value:\n        return r\n    else:\n        return ag",
            "def select_step(x, A_h, g_h, c_h, p, p_h, d, lb, ub, theta):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Select the best step according to Trust Region Reflective algorithm.'\n    if in_bounds(x + p, lb, ub):\n        return p\n    (p_stride, hits) = step_size_to_bound(x, p, lb, ub)\n    r_h = np.copy(p_h)\n    r_h[hits.astype(bool)] *= -1\n    r = d * r_h\n    p *= p_stride\n    p_h *= p_stride\n    x_on_bound = x + p\n    (r_stride_u, _) = step_size_to_bound(x_on_bound, r, lb, ub)\n    r_stride_l = (1 - theta) * r_stride_u\n    r_stride_u *= theta\n    if r_stride_u > 0:\n        (a, b, c) = build_quadratic_1d(A_h, g_h, r_h, s0=p_h, diag=c_h)\n        (r_stride, r_value) = minimize_quadratic_1d(a, b, r_stride_l, r_stride_u, c=c)\n        r_h = p_h + r_h * r_stride\n        r = d * r_h\n    else:\n        r_value = np.inf\n    p_h *= theta\n    p *= theta\n    p_value = evaluate_quadratic(A_h, g_h, p_h, diag=c_h)\n    ag_h = -g_h\n    ag = d * ag_h\n    (ag_stride_u, _) = step_size_to_bound(x, ag, lb, ub)\n    ag_stride_u *= theta\n    (a, b) = build_quadratic_1d(A_h, g_h, ag_h, diag=c_h)\n    (ag_stride, ag_value) = minimize_quadratic_1d(a, b, 0, ag_stride_u)\n    ag *= ag_stride\n    if p_value < r_value and p_value < ag_value:\n        return p\n    elif r_value < p_value and r_value < ag_value:\n        return r\n    else:\n        return ag",
            "def select_step(x, A_h, g_h, c_h, p, p_h, d, lb, ub, theta):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Select the best step according to Trust Region Reflective algorithm.'\n    if in_bounds(x + p, lb, ub):\n        return p\n    (p_stride, hits) = step_size_to_bound(x, p, lb, ub)\n    r_h = np.copy(p_h)\n    r_h[hits.astype(bool)] *= -1\n    r = d * r_h\n    p *= p_stride\n    p_h *= p_stride\n    x_on_bound = x + p\n    (r_stride_u, _) = step_size_to_bound(x_on_bound, r, lb, ub)\n    r_stride_l = (1 - theta) * r_stride_u\n    r_stride_u *= theta\n    if r_stride_u > 0:\n        (a, b, c) = build_quadratic_1d(A_h, g_h, r_h, s0=p_h, diag=c_h)\n        (r_stride, r_value) = minimize_quadratic_1d(a, b, r_stride_l, r_stride_u, c=c)\n        r_h = p_h + r_h * r_stride\n        r = d * r_h\n    else:\n        r_value = np.inf\n    p_h *= theta\n    p *= theta\n    p_value = evaluate_quadratic(A_h, g_h, p_h, diag=c_h)\n    ag_h = -g_h\n    ag = d * ag_h\n    (ag_stride_u, _) = step_size_to_bound(x, ag, lb, ub)\n    ag_stride_u *= theta\n    (a, b) = build_quadratic_1d(A_h, g_h, ag_h, diag=c_h)\n    (ag_stride, ag_value) = minimize_quadratic_1d(a, b, 0, ag_stride_u)\n    ag *= ag_stride\n    if p_value < r_value and p_value < ag_value:\n        return p\n    elif r_value < p_value and r_value < ag_value:\n        return r\n    else:\n        return ag",
            "def select_step(x, A_h, g_h, c_h, p, p_h, d, lb, ub, theta):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Select the best step according to Trust Region Reflective algorithm.'\n    if in_bounds(x + p, lb, ub):\n        return p\n    (p_stride, hits) = step_size_to_bound(x, p, lb, ub)\n    r_h = np.copy(p_h)\n    r_h[hits.astype(bool)] *= -1\n    r = d * r_h\n    p *= p_stride\n    p_h *= p_stride\n    x_on_bound = x + p\n    (r_stride_u, _) = step_size_to_bound(x_on_bound, r, lb, ub)\n    r_stride_l = (1 - theta) * r_stride_u\n    r_stride_u *= theta\n    if r_stride_u > 0:\n        (a, b, c) = build_quadratic_1d(A_h, g_h, r_h, s0=p_h, diag=c_h)\n        (r_stride, r_value) = minimize_quadratic_1d(a, b, r_stride_l, r_stride_u, c=c)\n        r_h = p_h + r_h * r_stride\n        r = d * r_h\n    else:\n        r_value = np.inf\n    p_h *= theta\n    p *= theta\n    p_value = evaluate_quadratic(A_h, g_h, p_h, diag=c_h)\n    ag_h = -g_h\n    ag = d * ag_h\n    (ag_stride_u, _) = step_size_to_bound(x, ag, lb, ub)\n    ag_stride_u *= theta\n    (a, b) = build_quadratic_1d(A_h, g_h, ag_h, diag=c_h)\n    (ag_stride, ag_value) = minimize_quadratic_1d(a, b, 0, ag_stride_u)\n    ag *= ag_stride\n    if p_value < r_value and p_value < ag_value:\n        return p\n    elif r_value < p_value and r_value < ag_value:\n        return r\n    else:\n        return ag"
        ]
    },
    {
        "func_name": "trf_linear",
        "original": "def trf_linear(A, b, x_lsq, lb, ub, tol, lsq_solver, lsmr_tol, max_iter, verbose, *, lsmr_maxiter=None):\n    (m, n) = A.shape\n    (x, _) = reflective_transformation(x_lsq, lb, ub)\n    x = make_strictly_feasible(x, lb, ub, rstep=0.1)\n    if lsq_solver == 'exact':\n        (QT, R, perm) = qr(A, mode='economic', pivoting=True)\n        QT = QT.T\n        if m < n:\n            R = np.vstack((R, np.zeros((n - m, n))))\n        QTr = np.zeros(n)\n        k = min(m, n)\n    elif lsq_solver == 'lsmr':\n        r_aug = np.zeros(m + n)\n        auto_lsmr_tol = False\n        if lsmr_tol is None:\n            lsmr_tol = 0.01 * tol\n        elif lsmr_tol == 'auto':\n            auto_lsmr_tol = True\n    r = A.dot(x) - b\n    g = compute_grad(A, r)\n    cost = 0.5 * np.dot(r, r)\n    initial_cost = cost\n    termination_status = None\n    step_norm = None\n    cost_change = None\n    if max_iter is None:\n        max_iter = 100\n    if verbose == 2:\n        print_header_linear()\n    for iteration in range(max_iter):\n        (v, dv) = CL_scaling_vector(x, g, lb, ub)\n        g_scaled = g * v\n        g_norm = norm(g_scaled, ord=np.inf)\n        if g_norm < tol:\n            termination_status = 1\n        if verbose == 2:\n            print_iteration_linear(iteration, cost, cost_change, step_norm, g_norm)\n        if termination_status is not None:\n            break\n        diag_h = g * dv\n        diag_root_h = diag_h ** 0.5\n        d = v ** 0.5\n        g_h = d * g\n        A_h = right_multiplied_operator(A, d)\n        if lsq_solver == 'exact':\n            QTr[:k] = QT.dot(r)\n            p_h = -regularized_lsq_with_qr(m, n, R * d[perm], QTr, perm, diag_root_h, copy_R=False)\n        elif lsq_solver == 'lsmr':\n            lsmr_op = regularized_lsq_operator(A_h, diag_root_h)\n            r_aug[:m] = r\n            if auto_lsmr_tol:\n                eta = 0.01 * min(0.5, g_norm)\n                lsmr_tol = max(EPS, min(0.1, eta * g_norm))\n            p_h = -lsmr(lsmr_op, r_aug, maxiter=lsmr_maxiter, atol=lsmr_tol, btol=lsmr_tol)[0]\n        p = d * p_h\n        p_dot_g = np.dot(p, g)\n        if p_dot_g > 0:\n            termination_status = -1\n        theta = 1 - min(0.005, g_norm)\n        step = select_step(x, A_h, g_h, diag_h, p, p_h, d, lb, ub, theta)\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change < 0:\n            (x, step, cost_change) = backtracking(A, g, x, p, theta, p_dot_g, lb, ub)\n        else:\n            x = make_strictly_feasible(x + step, lb, ub, rstep=0)\n        step_norm = norm(step)\n        r = A.dot(x) - b\n        g = compute_grad(A, r)\n        if cost_change < tol * cost:\n            termination_status = 2\n        cost = 0.5 * np.dot(r, r)\n    if termination_status is None:\n        termination_status = 0\n    active_mask = find_active_constraints(x, lb, ub, rtol=tol)\n    return OptimizeResult(x=x, fun=r, cost=cost, optimality=g_norm, active_mask=active_mask, nit=iteration + 1, status=termination_status, initial_cost=initial_cost)",
        "mutated": [
            "def trf_linear(A, b, x_lsq, lb, ub, tol, lsq_solver, lsmr_tol, max_iter, verbose, *, lsmr_maxiter=None):\n    if False:\n        i = 10\n    (m, n) = A.shape\n    (x, _) = reflective_transformation(x_lsq, lb, ub)\n    x = make_strictly_feasible(x, lb, ub, rstep=0.1)\n    if lsq_solver == 'exact':\n        (QT, R, perm) = qr(A, mode='economic', pivoting=True)\n        QT = QT.T\n        if m < n:\n            R = np.vstack((R, np.zeros((n - m, n))))\n        QTr = np.zeros(n)\n        k = min(m, n)\n    elif lsq_solver == 'lsmr':\n        r_aug = np.zeros(m + n)\n        auto_lsmr_tol = False\n        if lsmr_tol is None:\n            lsmr_tol = 0.01 * tol\n        elif lsmr_tol == 'auto':\n            auto_lsmr_tol = True\n    r = A.dot(x) - b\n    g = compute_grad(A, r)\n    cost = 0.5 * np.dot(r, r)\n    initial_cost = cost\n    termination_status = None\n    step_norm = None\n    cost_change = None\n    if max_iter is None:\n        max_iter = 100\n    if verbose == 2:\n        print_header_linear()\n    for iteration in range(max_iter):\n        (v, dv) = CL_scaling_vector(x, g, lb, ub)\n        g_scaled = g * v\n        g_norm = norm(g_scaled, ord=np.inf)\n        if g_norm < tol:\n            termination_status = 1\n        if verbose == 2:\n            print_iteration_linear(iteration, cost, cost_change, step_norm, g_norm)\n        if termination_status is not None:\n            break\n        diag_h = g * dv\n        diag_root_h = diag_h ** 0.5\n        d = v ** 0.5\n        g_h = d * g\n        A_h = right_multiplied_operator(A, d)\n        if lsq_solver == 'exact':\n            QTr[:k] = QT.dot(r)\n            p_h = -regularized_lsq_with_qr(m, n, R * d[perm], QTr, perm, diag_root_h, copy_R=False)\n        elif lsq_solver == 'lsmr':\n            lsmr_op = regularized_lsq_operator(A_h, diag_root_h)\n            r_aug[:m] = r\n            if auto_lsmr_tol:\n                eta = 0.01 * min(0.5, g_norm)\n                lsmr_tol = max(EPS, min(0.1, eta * g_norm))\n            p_h = -lsmr(lsmr_op, r_aug, maxiter=lsmr_maxiter, atol=lsmr_tol, btol=lsmr_tol)[0]\n        p = d * p_h\n        p_dot_g = np.dot(p, g)\n        if p_dot_g > 0:\n            termination_status = -1\n        theta = 1 - min(0.005, g_norm)\n        step = select_step(x, A_h, g_h, diag_h, p, p_h, d, lb, ub, theta)\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change < 0:\n            (x, step, cost_change) = backtracking(A, g, x, p, theta, p_dot_g, lb, ub)\n        else:\n            x = make_strictly_feasible(x + step, lb, ub, rstep=0)\n        step_norm = norm(step)\n        r = A.dot(x) - b\n        g = compute_grad(A, r)\n        if cost_change < tol * cost:\n            termination_status = 2\n        cost = 0.5 * np.dot(r, r)\n    if termination_status is None:\n        termination_status = 0\n    active_mask = find_active_constraints(x, lb, ub, rtol=tol)\n    return OptimizeResult(x=x, fun=r, cost=cost, optimality=g_norm, active_mask=active_mask, nit=iteration + 1, status=termination_status, initial_cost=initial_cost)",
            "def trf_linear(A, b, x_lsq, lb, ub, tol, lsq_solver, lsmr_tol, max_iter, verbose, *, lsmr_maxiter=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (m, n) = A.shape\n    (x, _) = reflective_transformation(x_lsq, lb, ub)\n    x = make_strictly_feasible(x, lb, ub, rstep=0.1)\n    if lsq_solver == 'exact':\n        (QT, R, perm) = qr(A, mode='economic', pivoting=True)\n        QT = QT.T\n        if m < n:\n            R = np.vstack((R, np.zeros((n - m, n))))\n        QTr = np.zeros(n)\n        k = min(m, n)\n    elif lsq_solver == 'lsmr':\n        r_aug = np.zeros(m + n)\n        auto_lsmr_tol = False\n        if lsmr_tol is None:\n            lsmr_tol = 0.01 * tol\n        elif lsmr_tol == 'auto':\n            auto_lsmr_tol = True\n    r = A.dot(x) - b\n    g = compute_grad(A, r)\n    cost = 0.5 * np.dot(r, r)\n    initial_cost = cost\n    termination_status = None\n    step_norm = None\n    cost_change = None\n    if max_iter is None:\n        max_iter = 100\n    if verbose == 2:\n        print_header_linear()\n    for iteration in range(max_iter):\n        (v, dv) = CL_scaling_vector(x, g, lb, ub)\n        g_scaled = g * v\n        g_norm = norm(g_scaled, ord=np.inf)\n        if g_norm < tol:\n            termination_status = 1\n        if verbose == 2:\n            print_iteration_linear(iteration, cost, cost_change, step_norm, g_norm)\n        if termination_status is not None:\n            break\n        diag_h = g * dv\n        diag_root_h = diag_h ** 0.5\n        d = v ** 0.5\n        g_h = d * g\n        A_h = right_multiplied_operator(A, d)\n        if lsq_solver == 'exact':\n            QTr[:k] = QT.dot(r)\n            p_h = -regularized_lsq_with_qr(m, n, R * d[perm], QTr, perm, diag_root_h, copy_R=False)\n        elif lsq_solver == 'lsmr':\n            lsmr_op = regularized_lsq_operator(A_h, diag_root_h)\n            r_aug[:m] = r\n            if auto_lsmr_tol:\n                eta = 0.01 * min(0.5, g_norm)\n                lsmr_tol = max(EPS, min(0.1, eta * g_norm))\n            p_h = -lsmr(lsmr_op, r_aug, maxiter=lsmr_maxiter, atol=lsmr_tol, btol=lsmr_tol)[0]\n        p = d * p_h\n        p_dot_g = np.dot(p, g)\n        if p_dot_g > 0:\n            termination_status = -1\n        theta = 1 - min(0.005, g_norm)\n        step = select_step(x, A_h, g_h, diag_h, p, p_h, d, lb, ub, theta)\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change < 0:\n            (x, step, cost_change) = backtracking(A, g, x, p, theta, p_dot_g, lb, ub)\n        else:\n            x = make_strictly_feasible(x + step, lb, ub, rstep=0)\n        step_norm = norm(step)\n        r = A.dot(x) - b\n        g = compute_grad(A, r)\n        if cost_change < tol * cost:\n            termination_status = 2\n        cost = 0.5 * np.dot(r, r)\n    if termination_status is None:\n        termination_status = 0\n    active_mask = find_active_constraints(x, lb, ub, rtol=tol)\n    return OptimizeResult(x=x, fun=r, cost=cost, optimality=g_norm, active_mask=active_mask, nit=iteration + 1, status=termination_status, initial_cost=initial_cost)",
            "def trf_linear(A, b, x_lsq, lb, ub, tol, lsq_solver, lsmr_tol, max_iter, verbose, *, lsmr_maxiter=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (m, n) = A.shape\n    (x, _) = reflective_transformation(x_lsq, lb, ub)\n    x = make_strictly_feasible(x, lb, ub, rstep=0.1)\n    if lsq_solver == 'exact':\n        (QT, R, perm) = qr(A, mode='economic', pivoting=True)\n        QT = QT.T\n        if m < n:\n            R = np.vstack((R, np.zeros((n - m, n))))\n        QTr = np.zeros(n)\n        k = min(m, n)\n    elif lsq_solver == 'lsmr':\n        r_aug = np.zeros(m + n)\n        auto_lsmr_tol = False\n        if lsmr_tol is None:\n            lsmr_tol = 0.01 * tol\n        elif lsmr_tol == 'auto':\n            auto_lsmr_tol = True\n    r = A.dot(x) - b\n    g = compute_grad(A, r)\n    cost = 0.5 * np.dot(r, r)\n    initial_cost = cost\n    termination_status = None\n    step_norm = None\n    cost_change = None\n    if max_iter is None:\n        max_iter = 100\n    if verbose == 2:\n        print_header_linear()\n    for iteration in range(max_iter):\n        (v, dv) = CL_scaling_vector(x, g, lb, ub)\n        g_scaled = g * v\n        g_norm = norm(g_scaled, ord=np.inf)\n        if g_norm < tol:\n            termination_status = 1\n        if verbose == 2:\n            print_iteration_linear(iteration, cost, cost_change, step_norm, g_norm)\n        if termination_status is not None:\n            break\n        diag_h = g * dv\n        diag_root_h = diag_h ** 0.5\n        d = v ** 0.5\n        g_h = d * g\n        A_h = right_multiplied_operator(A, d)\n        if lsq_solver == 'exact':\n            QTr[:k] = QT.dot(r)\n            p_h = -regularized_lsq_with_qr(m, n, R * d[perm], QTr, perm, diag_root_h, copy_R=False)\n        elif lsq_solver == 'lsmr':\n            lsmr_op = regularized_lsq_operator(A_h, diag_root_h)\n            r_aug[:m] = r\n            if auto_lsmr_tol:\n                eta = 0.01 * min(0.5, g_norm)\n                lsmr_tol = max(EPS, min(0.1, eta * g_norm))\n            p_h = -lsmr(lsmr_op, r_aug, maxiter=lsmr_maxiter, atol=lsmr_tol, btol=lsmr_tol)[0]\n        p = d * p_h\n        p_dot_g = np.dot(p, g)\n        if p_dot_g > 0:\n            termination_status = -1\n        theta = 1 - min(0.005, g_norm)\n        step = select_step(x, A_h, g_h, diag_h, p, p_h, d, lb, ub, theta)\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change < 0:\n            (x, step, cost_change) = backtracking(A, g, x, p, theta, p_dot_g, lb, ub)\n        else:\n            x = make_strictly_feasible(x + step, lb, ub, rstep=0)\n        step_norm = norm(step)\n        r = A.dot(x) - b\n        g = compute_grad(A, r)\n        if cost_change < tol * cost:\n            termination_status = 2\n        cost = 0.5 * np.dot(r, r)\n    if termination_status is None:\n        termination_status = 0\n    active_mask = find_active_constraints(x, lb, ub, rtol=tol)\n    return OptimizeResult(x=x, fun=r, cost=cost, optimality=g_norm, active_mask=active_mask, nit=iteration + 1, status=termination_status, initial_cost=initial_cost)",
            "def trf_linear(A, b, x_lsq, lb, ub, tol, lsq_solver, lsmr_tol, max_iter, verbose, *, lsmr_maxiter=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (m, n) = A.shape\n    (x, _) = reflective_transformation(x_lsq, lb, ub)\n    x = make_strictly_feasible(x, lb, ub, rstep=0.1)\n    if lsq_solver == 'exact':\n        (QT, R, perm) = qr(A, mode='economic', pivoting=True)\n        QT = QT.T\n        if m < n:\n            R = np.vstack((R, np.zeros((n - m, n))))\n        QTr = np.zeros(n)\n        k = min(m, n)\n    elif lsq_solver == 'lsmr':\n        r_aug = np.zeros(m + n)\n        auto_lsmr_tol = False\n        if lsmr_tol is None:\n            lsmr_tol = 0.01 * tol\n        elif lsmr_tol == 'auto':\n            auto_lsmr_tol = True\n    r = A.dot(x) - b\n    g = compute_grad(A, r)\n    cost = 0.5 * np.dot(r, r)\n    initial_cost = cost\n    termination_status = None\n    step_norm = None\n    cost_change = None\n    if max_iter is None:\n        max_iter = 100\n    if verbose == 2:\n        print_header_linear()\n    for iteration in range(max_iter):\n        (v, dv) = CL_scaling_vector(x, g, lb, ub)\n        g_scaled = g * v\n        g_norm = norm(g_scaled, ord=np.inf)\n        if g_norm < tol:\n            termination_status = 1\n        if verbose == 2:\n            print_iteration_linear(iteration, cost, cost_change, step_norm, g_norm)\n        if termination_status is not None:\n            break\n        diag_h = g * dv\n        diag_root_h = diag_h ** 0.5\n        d = v ** 0.5\n        g_h = d * g\n        A_h = right_multiplied_operator(A, d)\n        if lsq_solver == 'exact':\n            QTr[:k] = QT.dot(r)\n            p_h = -regularized_lsq_with_qr(m, n, R * d[perm], QTr, perm, diag_root_h, copy_R=False)\n        elif lsq_solver == 'lsmr':\n            lsmr_op = regularized_lsq_operator(A_h, diag_root_h)\n            r_aug[:m] = r\n            if auto_lsmr_tol:\n                eta = 0.01 * min(0.5, g_norm)\n                lsmr_tol = max(EPS, min(0.1, eta * g_norm))\n            p_h = -lsmr(lsmr_op, r_aug, maxiter=lsmr_maxiter, atol=lsmr_tol, btol=lsmr_tol)[0]\n        p = d * p_h\n        p_dot_g = np.dot(p, g)\n        if p_dot_g > 0:\n            termination_status = -1\n        theta = 1 - min(0.005, g_norm)\n        step = select_step(x, A_h, g_h, diag_h, p, p_h, d, lb, ub, theta)\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change < 0:\n            (x, step, cost_change) = backtracking(A, g, x, p, theta, p_dot_g, lb, ub)\n        else:\n            x = make_strictly_feasible(x + step, lb, ub, rstep=0)\n        step_norm = norm(step)\n        r = A.dot(x) - b\n        g = compute_grad(A, r)\n        if cost_change < tol * cost:\n            termination_status = 2\n        cost = 0.5 * np.dot(r, r)\n    if termination_status is None:\n        termination_status = 0\n    active_mask = find_active_constraints(x, lb, ub, rtol=tol)\n    return OptimizeResult(x=x, fun=r, cost=cost, optimality=g_norm, active_mask=active_mask, nit=iteration + 1, status=termination_status, initial_cost=initial_cost)",
            "def trf_linear(A, b, x_lsq, lb, ub, tol, lsq_solver, lsmr_tol, max_iter, verbose, *, lsmr_maxiter=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (m, n) = A.shape\n    (x, _) = reflective_transformation(x_lsq, lb, ub)\n    x = make_strictly_feasible(x, lb, ub, rstep=0.1)\n    if lsq_solver == 'exact':\n        (QT, R, perm) = qr(A, mode='economic', pivoting=True)\n        QT = QT.T\n        if m < n:\n            R = np.vstack((R, np.zeros((n - m, n))))\n        QTr = np.zeros(n)\n        k = min(m, n)\n    elif lsq_solver == 'lsmr':\n        r_aug = np.zeros(m + n)\n        auto_lsmr_tol = False\n        if lsmr_tol is None:\n            lsmr_tol = 0.01 * tol\n        elif lsmr_tol == 'auto':\n            auto_lsmr_tol = True\n    r = A.dot(x) - b\n    g = compute_grad(A, r)\n    cost = 0.5 * np.dot(r, r)\n    initial_cost = cost\n    termination_status = None\n    step_norm = None\n    cost_change = None\n    if max_iter is None:\n        max_iter = 100\n    if verbose == 2:\n        print_header_linear()\n    for iteration in range(max_iter):\n        (v, dv) = CL_scaling_vector(x, g, lb, ub)\n        g_scaled = g * v\n        g_norm = norm(g_scaled, ord=np.inf)\n        if g_norm < tol:\n            termination_status = 1\n        if verbose == 2:\n            print_iteration_linear(iteration, cost, cost_change, step_norm, g_norm)\n        if termination_status is not None:\n            break\n        diag_h = g * dv\n        diag_root_h = diag_h ** 0.5\n        d = v ** 0.5\n        g_h = d * g\n        A_h = right_multiplied_operator(A, d)\n        if lsq_solver == 'exact':\n            QTr[:k] = QT.dot(r)\n            p_h = -regularized_lsq_with_qr(m, n, R * d[perm], QTr, perm, diag_root_h, copy_R=False)\n        elif lsq_solver == 'lsmr':\n            lsmr_op = regularized_lsq_operator(A_h, diag_root_h)\n            r_aug[:m] = r\n            if auto_lsmr_tol:\n                eta = 0.01 * min(0.5, g_norm)\n                lsmr_tol = max(EPS, min(0.1, eta * g_norm))\n            p_h = -lsmr(lsmr_op, r_aug, maxiter=lsmr_maxiter, atol=lsmr_tol, btol=lsmr_tol)[0]\n        p = d * p_h\n        p_dot_g = np.dot(p, g)\n        if p_dot_g > 0:\n            termination_status = -1\n        theta = 1 - min(0.005, g_norm)\n        step = select_step(x, A_h, g_h, diag_h, p, p_h, d, lb, ub, theta)\n        cost_change = -evaluate_quadratic(A, g, step)\n        if cost_change < 0:\n            (x, step, cost_change) = backtracking(A, g, x, p, theta, p_dot_g, lb, ub)\n        else:\n            x = make_strictly_feasible(x + step, lb, ub, rstep=0)\n        step_norm = norm(step)\n        r = A.dot(x) - b\n        g = compute_grad(A, r)\n        if cost_change < tol * cost:\n            termination_status = 2\n        cost = 0.5 * np.dot(r, r)\n    if termination_status is None:\n        termination_status = 0\n    active_mask = find_active_constraints(x, lb, ub, rtol=tol)\n    return OptimizeResult(x=x, fun=r, cost=cost, optimality=g_norm, active_mask=active_mask, nit=iteration + 1, status=termination_status, initial_cost=initial_cost)"
        ]
    }
]