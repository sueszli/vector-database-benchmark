[
    {
        "func_name": "rename_state_dict_key",
        "original": "def rename_state_dict_key(k):\n    for (pegasus_name, hf_name) in PATTERNS:\n        k = k.replace(pegasus_name, hf_name)\n    return k",
        "mutated": [
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n    for (pegasus_name, hf_name) in PATTERNS:\n        k = k.replace(pegasus_name, hf_name)\n    return k",
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for (pegasus_name, hf_name) in PATTERNS:\n        k = k.replace(pegasus_name, hf_name)\n    return k",
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for (pegasus_name, hf_name) in PATTERNS:\n        k = k.replace(pegasus_name, hf_name)\n    return k",
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for (pegasus_name, hf_name) in PATTERNS:\n        k = k.replace(pegasus_name, hf_name)\n    return k",
            "def rename_state_dict_key(k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for (pegasus_name, hf_name) in PATTERNS:\n        k = k.replace(pegasus_name, hf_name)\n    return k"
        ]
    },
    {
        "func_name": "convert_pegasus",
        "original": "def convert_pegasus(tf_weights: dict, cfg_updates: dict) -> PegasusForConditionalGeneration:\n    cfg_kwargs = DEFAULTS.copy()\n    cfg_kwargs.update(cfg_updates)\n    cfg = PegasusConfig(**cfg_kwargs)\n    torch_model = PegasusForConditionalGeneration(cfg)\n    sd = torch_model.model.state_dict()\n    mapping = {}\n    for (k, v) in tf_weights.items():\n        new_k = rename_state_dict_key(k)\n        if new_k not in sd:\n            raise ValueError(f'could not find new key {new_k} in state dict. (converted from {k})')\n        if 'dense' in k or 'proj' in new_k:\n            v = v.T\n        mapping[new_k] = torch.tensor(v, dtype=sd[new_k].dtype)\n        assert v.shape == sd[new_k].shape, f'{new_k}, {k}, {v.shape}, {sd[new_k].shape}'\n    mapping['shared.weight'][cfg.pad_token_id] = torch.zeros_like(mapping['shared.weight'][cfg.pad_token_id + 1])\n    mapping['encoder.embed_tokens.weight'] = mapping['shared.weight']\n    mapping['decoder.embed_tokens.weight'] = mapping['shared.weight']\n    empty_biases = {k: torch.zeros_like(v) for (k, v) in sd.items() if k.endswith('bias') and k not in mapping}\n    mapping.update(**empty_biases)\n    (missing, extra) = torch_model.model.load_state_dict(mapping, strict=False)\n    unexpected_missing = [k for k in missing if k not in ['encoder.embed_positions.weight', 'decoder.embed_positions.weight']]\n    assert unexpected_missing == [], f'no matches found for the following torch keys {unexpected_missing}'\n    assert extra == [], f'no matches found for the following tf keys {extra}'\n    return torch_model",
        "mutated": [
            "def convert_pegasus(tf_weights: dict, cfg_updates: dict) -> PegasusForConditionalGeneration:\n    if False:\n        i = 10\n    cfg_kwargs = DEFAULTS.copy()\n    cfg_kwargs.update(cfg_updates)\n    cfg = PegasusConfig(**cfg_kwargs)\n    torch_model = PegasusForConditionalGeneration(cfg)\n    sd = torch_model.model.state_dict()\n    mapping = {}\n    for (k, v) in tf_weights.items():\n        new_k = rename_state_dict_key(k)\n        if new_k not in sd:\n            raise ValueError(f'could not find new key {new_k} in state dict. (converted from {k})')\n        if 'dense' in k or 'proj' in new_k:\n            v = v.T\n        mapping[new_k] = torch.tensor(v, dtype=sd[new_k].dtype)\n        assert v.shape == sd[new_k].shape, f'{new_k}, {k}, {v.shape}, {sd[new_k].shape}'\n    mapping['shared.weight'][cfg.pad_token_id] = torch.zeros_like(mapping['shared.weight'][cfg.pad_token_id + 1])\n    mapping['encoder.embed_tokens.weight'] = mapping['shared.weight']\n    mapping['decoder.embed_tokens.weight'] = mapping['shared.weight']\n    empty_biases = {k: torch.zeros_like(v) for (k, v) in sd.items() if k.endswith('bias') and k not in mapping}\n    mapping.update(**empty_biases)\n    (missing, extra) = torch_model.model.load_state_dict(mapping, strict=False)\n    unexpected_missing = [k for k in missing if k not in ['encoder.embed_positions.weight', 'decoder.embed_positions.weight']]\n    assert unexpected_missing == [], f'no matches found for the following torch keys {unexpected_missing}'\n    assert extra == [], f'no matches found for the following tf keys {extra}'\n    return torch_model",
            "def convert_pegasus(tf_weights: dict, cfg_updates: dict) -> PegasusForConditionalGeneration:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cfg_kwargs = DEFAULTS.copy()\n    cfg_kwargs.update(cfg_updates)\n    cfg = PegasusConfig(**cfg_kwargs)\n    torch_model = PegasusForConditionalGeneration(cfg)\n    sd = torch_model.model.state_dict()\n    mapping = {}\n    for (k, v) in tf_weights.items():\n        new_k = rename_state_dict_key(k)\n        if new_k not in sd:\n            raise ValueError(f'could not find new key {new_k} in state dict. (converted from {k})')\n        if 'dense' in k or 'proj' in new_k:\n            v = v.T\n        mapping[new_k] = torch.tensor(v, dtype=sd[new_k].dtype)\n        assert v.shape == sd[new_k].shape, f'{new_k}, {k}, {v.shape}, {sd[new_k].shape}'\n    mapping['shared.weight'][cfg.pad_token_id] = torch.zeros_like(mapping['shared.weight'][cfg.pad_token_id + 1])\n    mapping['encoder.embed_tokens.weight'] = mapping['shared.weight']\n    mapping['decoder.embed_tokens.weight'] = mapping['shared.weight']\n    empty_biases = {k: torch.zeros_like(v) for (k, v) in sd.items() if k.endswith('bias') and k not in mapping}\n    mapping.update(**empty_biases)\n    (missing, extra) = torch_model.model.load_state_dict(mapping, strict=False)\n    unexpected_missing = [k for k in missing if k not in ['encoder.embed_positions.weight', 'decoder.embed_positions.weight']]\n    assert unexpected_missing == [], f'no matches found for the following torch keys {unexpected_missing}'\n    assert extra == [], f'no matches found for the following tf keys {extra}'\n    return torch_model",
            "def convert_pegasus(tf_weights: dict, cfg_updates: dict) -> PegasusForConditionalGeneration:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cfg_kwargs = DEFAULTS.copy()\n    cfg_kwargs.update(cfg_updates)\n    cfg = PegasusConfig(**cfg_kwargs)\n    torch_model = PegasusForConditionalGeneration(cfg)\n    sd = torch_model.model.state_dict()\n    mapping = {}\n    for (k, v) in tf_weights.items():\n        new_k = rename_state_dict_key(k)\n        if new_k not in sd:\n            raise ValueError(f'could not find new key {new_k} in state dict. (converted from {k})')\n        if 'dense' in k or 'proj' in new_k:\n            v = v.T\n        mapping[new_k] = torch.tensor(v, dtype=sd[new_k].dtype)\n        assert v.shape == sd[new_k].shape, f'{new_k}, {k}, {v.shape}, {sd[new_k].shape}'\n    mapping['shared.weight'][cfg.pad_token_id] = torch.zeros_like(mapping['shared.weight'][cfg.pad_token_id + 1])\n    mapping['encoder.embed_tokens.weight'] = mapping['shared.weight']\n    mapping['decoder.embed_tokens.weight'] = mapping['shared.weight']\n    empty_biases = {k: torch.zeros_like(v) for (k, v) in sd.items() if k.endswith('bias') and k not in mapping}\n    mapping.update(**empty_biases)\n    (missing, extra) = torch_model.model.load_state_dict(mapping, strict=False)\n    unexpected_missing = [k for k in missing if k not in ['encoder.embed_positions.weight', 'decoder.embed_positions.weight']]\n    assert unexpected_missing == [], f'no matches found for the following torch keys {unexpected_missing}'\n    assert extra == [], f'no matches found for the following tf keys {extra}'\n    return torch_model",
            "def convert_pegasus(tf_weights: dict, cfg_updates: dict) -> PegasusForConditionalGeneration:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cfg_kwargs = DEFAULTS.copy()\n    cfg_kwargs.update(cfg_updates)\n    cfg = PegasusConfig(**cfg_kwargs)\n    torch_model = PegasusForConditionalGeneration(cfg)\n    sd = torch_model.model.state_dict()\n    mapping = {}\n    for (k, v) in tf_weights.items():\n        new_k = rename_state_dict_key(k)\n        if new_k not in sd:\n            raise ValueError(f'could not find new key {new_k} in state dict. (converted from {k})')\n        if 'dense' in k or 'proj' in new_k:\n            v = v.T\n        mapping[new_k] = torch.tensor(v, dtype=sd[new_k].dtype)\n        assert v.shape == sd[new_k].shape, f'{new_k}, {k}, {v.shape}, {sd[new_k].shape}'\n    mapping['shared.weight'][cfg.pad_token_id] = torch.zeros_like(mapping['shared.weight'][cfg.pad_token_id + 1])\n    mapping['encoder.embed_tokens.weight'] = mapping['shared.weight']\n    mapping['decoder.embed_tokens.weight'] = mapping['shared.weight']\n    empty_biases = {k: torch.zeros_like(v) for (k, v) in sd.items() if k.endswith('bias') and k not in mapping}\n    mapping.update(**empty_biases)\n    (missing, extra) = torch_model.model.load_state_dict(mapping, strict=False)\n    unexpected_missing = [k for k in missing if k not in ['encoder.embed_positions.weight', 'decoder.embed_positions.weight']]\n    assert unexpected_missing == [], f'no matches found for the following torch keys {unexpected_missing}'\n    assert extra == [], f'no matches found for the following tf keys {extra}'\n    return torch_model",
            "def convert_pegasus(tf_weights: dict, cfg_updates: dict) -> PegasusForConditionalGeneration:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cfg_kwargs = DEFAULTS.copy()\n    cfg_kwargs.update(cfg_updates)\n    cfg = PegasusConfig(**cfg_kwargs)\n    torch_model = PegasusForConditionalGeneration(cfg)\n    sd = torch_model.model.state_dict()\n    mapping = {}\n    for (k, v) in tf_weights.items():\n        new_k = rename_state_dict_key(k)\n        if new_k not in sd:\n            raise ValueError(f'could not find new key {new_k} in state dict. (converted from {k})')\n        if 'dense' in k or 'proj' in new_k:\n            v = v.T\n        mapping[new_k] = torch.tensor(v, dtype=sd[new_k].dtype)\n        assert v.shape == sd[new_k].shape, f'{new_k}, {k}, {v.shape}, {sd[new_k].shape}'\n    mapping['shared.weight'][cfg.pad_token_id] = torch.zeros_like(mapping['shared.weight'][cfg.pad_token_id + 1])\n    mapping['encoder.embed_tokens.weight'] = mapping['shared.weight']\n    mapping['decoder.embed_tokens.weight'] = mapping['shared.weight']\n    empty_biases = {k: torch.zeros_like(v) for (k, v) in sd.items() if k.endswith('bias') and k not in mapping}\n    mapping.update(**empty_biases)\n    (missing, extra) = torch_model.model.load_state_dict(mapping, strict=False)\n    unexpected_missing = [k for k in missing if k not in ['encoder.embed_positions.weight', 'decoder.embed_positions.weight']]\n    assert unexpected_missing == [], f'no matches found for the following torch keys {unexpected_missing}'\n    assert extra == [], f'no matches found for the following tf keys {extra}'\n    return torch_model"
        ]
    },
    {
        "func_name": "get_tf_weights_as_numpy",
        "original": "def get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000') -> Dict:\n    init_vars = tf.train.list_variables(path)\n    tf_weights = {}\n    ignore_name = ['Adafactor', 'global_step']\n    for (name, shape) in tqdm(init_vars, desc='converting tf checkpoint to dict'):\n        skip_key = any((pat in name for pat in ignore_name))\n        if skip_key:\n            continue\n        array = tf.train.load_variable(path, name)\n        tf_weights[name] = array\n    return tf_weights",
        "mutated": [
            "def get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000') -> Dict:\n    if False:\n        i = 10\n    init_vars = tf.train.list_variables(path)\n    tf_weights = {}\n    ignore_name = ['Adafactor', 'global_step']\n    for (name, shape) in tqdm(init_vars, desc='converting tf checkpoint to dict'):\n        skip_key = any((pat in name for pat in ignore_name))\n        if skip_key:\n            continue\n        array = tf.train.load_variable(path, name)\n        tf_weights[name] = array\n    return tf_weights",
            "def get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000') -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    init_vars = tf.train.list_variables(path)\n    tf_weights = {}\n    ignore_name = ['Adafactor', 'global_step']\n    for (name, shape) in tqdm(init_vars, desc='converting tf checkpoint to dict'):\n        skip_key = any((pat in name for pat in ignore_name))\n        if skip_key:\n            continue\n        array = tf.train.load_variable(path, name)\n        tf_weights[name] = array\n    return tf_weights",
            "def get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000') -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    init_vars = tf.train.list_variables(path)\n    tf_weights = {}\n    ignore_name = ['Adafactor', 'global_step']\n    for (name, shape) in tqdm(init_vars, desc='converting tf checkpoint to dict'):\n        skip_key = any((pat in name for pat in ignore_name))\n        if skip_key:\n            continue\n        array = tf.train.load_variable(path, name)\n        tf_weights[name] = array\n    return tf_weights",
            "def get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000') -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    init_vars = tf.train.list_variables(path)\n    tf_weights = {}\n    ignore_name = ['Adafactor', 'global_step']\n    for (name, shape) in tqdm(init_vars, desc='converting tf checkpoint to dict'):\n        skip_key = any((pat in name for pat in ignore_name))\n        if skip_key:\n            continue\n        array = tf.train.load_variable(path, name)\n        tf_weights[name] = array\n    return tf_weights",
            "def get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000') -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    init_vars = tf.train.list_variables(path)\n    tf_weights = {}\n    ignore_name = ['Adafactor', 'global_step']\n    for (name, shape) in tqdm(init_vars, desc='converting tf checkpoint to dict'):\n        skip_key = any((pat in name for pat in ignore_name))\n        if skip_key:\n            continue\n        array = tf.train.load_variable(path, name)\n        tf_weights[name] = array\n    return tf_weights"
        ]
    },
    {
        "func_name": "convert_pegasus_ckpt_to_pytorch",
        "original": "def convert_pegasus_ckpt_to_pytorch(ckpt_path: str, save_dir: str):\n    dataset = Path(ckpt_path).parent.name\n    desired_max_model_length = task_specific_params[f'summarization_{dataset}']['max_position_embeddings']\n    tok = PegasusTokenizer.from_pretrained('sshleifer/pegasus', model_max_length=desired_max_model_length)\n    assert tok.model_max_length == desired_max_model_length\n    tok.save_pretrained(save_dir)\n    tf_weights = get_tf_weights_as_numpy(ckpt_path)\n    cfg_updates = task_specific_params[f'summarization_{dataset}']\n    if dataset == 'large':\n        cfg_updates['task_specific_params'] = task_specific_params\n    torch_model = convert_pegasus(tf_weights, cfg_updates)\n    torch_model.save_pretrained(save_dir)\n    sd = torch_model.state_dict()\n    sd.pop('model.decoder.embed_positions.weight')\n    sd.pop('model.encoder.embed_positions.weight')\n    torch.save(sd, Path(save_dir) / 'pytorch_model.bin')",
        "mutated": [
            "def convert_pegasus_ckpt_to_pytorch(ckpt_path: str, save_dir: str):\n    if False:\n        i = 10\n    dataset = Path(ckpt_path).parent.name\n    desired_max_model_length = task_specific_params[f'summarization_{dataset}']['max_position_embeddings']\n    tok = PegasusTokenizer.from_pretrained('sshleifer/pegasus', model_max_length=desired_max_model_length)\n    assert tok.model_max_length == desired_max_model_length\n    tok.save_pretrained(save_dir)\n    tf_weights = get_tf_weights_as_numpy(ckpt_path)\n    cfg_updates = task_specific_params[f'summarization_{dataset}']\n    if dataset == 'large':\n        cfg_updates['task_specific_params'] = task_specific_params\n    torch_model = convert_pegasus(tf_weights, cfg_updates)\n    torch_model.save_pretrained(save_dir)\n    sd = torch_model.state_dict()\n    sd.pop('model.decoder.embed_positions.weight')\n    sd.pop('model.encoder.embed_positions.weight')\n    torch.save(sd, Path(save_dir) / 'pytorch_model.bin')",
            "def convert_pegasus_ckpt_to_pytorch(ckpt_path: str, save_dir: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dataset = Path(ckpt_path).parent.name\n    desired_max_model_length = task_specific_params[f'summarization_{dataset}']['max_position_embeddings']\n    tok = PegasusTokenizer.from_pretrained('sshleifer/pegasus', model_max_length=desired_max_model_length)\n    assert tok.model_max_length == desired_max_model_length\n    tok.save_pretrained(save_dir)\n    tf_weights = get_tf_weights_as_numpy(ckpt_path)\n    cfg_updates = task_specific_params[f'summarization_{dataset}']\n    if dataset == 'large':\n        cfg_updates['task_specific_params'] = task_specific_params\n    torch_model = convert_pegasus(tf_weights, cfg_updates)\n    torch_model.save_pretrained(save_dir)\n    sd = torch_model.state_dict()\n    sd.pop('model.decoder.embed_positions.weight')\n    sd.pop('model.encoder.embed_positions.weight')\n    torch.save(sd, Path(save_dir) / 'pytorch_model.bin')",
            "def convert_pegasus_ckpt_to_pytorch(ckpt_path: str, save_dir: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dataset = Path(ckpt_path).parent.name\n    desired_max_model_length = task_specific_params[f'summarization_{dataset}']['max_position_embeddings']\n    tok = PegasusTokenizer.from_pretrained('sshleifer/pegasus', model_max_length=desired_max_model_length)\n    assert tok.model_max_length == desired_max_model_length\n    tok.save_pretrained(save_dir)\n    tf_weights = get_tf_weights_as_numpy(ckpt_path)\n    cfg_updates = task_specific_params[f'summarization_{dataset}']\n    if dataset == 'large':\n        cfg_updates['task_specific_params'] = task_specific_params\n    torch_model = convert_pegasus(tf_weights, cfg_updates)\n    torch_model.save_pretrained(save_dir)\n    sd = torch_model.state_dict()\n    sd.pop('model.decoder.embed_positions.weight')\n    sd.pop('model.encoder.embed_positions.weight')\n    torch.save(sd, Path(save_dir) / 'pytorch_model.bin')",
            "def convert_pegasus_ckpt_to_pytorch(ckpt_path: str, save_dir: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dataset = Path(ckpt_path).parent.name\n    desired_max_model_length = task_specific_params[f'summarization_{dataset}']['max_position_embeddings']\n    tok = PegasusTokenizer.from_pretrained('sshleifer/pegasus', model_max_length=desired_max_model_length)\n    assert tok.model_max_length == desired_max_model_length\n    tok.save_pretrained(save_dir)\n    tf_weights = get_tf_weights_as_numpy(ckpt_path)\n    cfg_updates = task_specific_params[f'summarization_{dataset}']\n    if dataset == 'large':\n        cfg_updates['task_specific_params'] = task_specific_params\n    torch_model = convert_pegasus(tf_weights, cfg_updates)\n    torch_model.save_pretrained(save_dir)\n    sd = torch_model.state_dict()\n    sd.pop('model.decoder.embed_positions.weight')\n    sd.pop('model.encoder.embed_positions.weight')\n    torch.save(sd, Path(save_dir) / 'pytorch_model.bin')",
            "def convert_pegasus_ckpt_to_pytorch(ckpt_path: str, save_dir: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dataset = Path(ckpt_path).parent.name\n    desired_max_model_length = task_specific_params[f'summarization_{dataset}']['max_position_embeddings']\n    tok = PegasusTokenizer.from_pretrained('sshleifer/pegasus', model_max_length=desired_max_model_length)\n    assert tok.model_max_length == desired_max_model_length\n    tok.save_pretrained(save_dir)\n    tf_weights = get_tf_weights_as_numpy(ckpt_path)\n    cfg_updates = task_specific_params[f'summarization_{dataset}']\n    if dataset == 'large':\n        cfg_updates['task_specific_params'] = task_specific_params\n    torch_model = convert_pegasus(tf_weights, cfg_updates)\n    torch_model.save_pretrained(save_dir)\n    sd = torch_model.state_dict()\n    sd.pop('model.decoder.embed_positions.weight')\n    sd.pop('model.encoder.embed_positions.weight')\n    torch.save(sd, Path(save_dir) / 'pytorch_model.bin')"
        ]
    }
]