[
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('src_root', default='src', help='Root directory with all source files.  Expected structure is root dir -> language dirs -> package dirs -> text files to process')\n    parser.add_argument('tgt_root', default='tgt', help='Root directory with all target files.')\n    parser.add_argument('--langs', default='', help='A list of language codes to process.  If not set, all languages under src_root will be processed.')\n    parser.add_argument('--packages', default='', help='A list of packages to process.  If not set, all packages under the languages found will be processed.')\n    parser.add_argument('--no_xz_output', default=True, dest='xz_output', action='store_false', help='Output compressed xz files')\n    parser.add_argument('--split_size', default=50, type=int, help='How large to make each split, in MB')\n    parser.add_argument('--no_make_test_file', default=True, dest='make_test_file', action='store_false', help=\"Don't save a test file.  Honestly, we never even use it.  Best for low resource languages where every bit helps\")\n    args = parser.parse_args()\n    print('Processing files:')\n    print(f'source root: {args.src_root}')\n    print(f'target root: {args.tgt_root}')\n    print('')\n    langs = []\n    if len(args.langs) > 0:\n        langs = args.langs.split(',')\n        print('Only processing the following languages: ' + str(langs))\n    packages = []\n    if len(args.packages) > 0:\n        packages = args.packages.split(',')\n        print('Only processing the following packages: ' + str(packages))\n    src_root = Path(args.src_root)\n    tgt_root = Path(args.tgt_root)\n    lang_dirs = os.listdir(src_root)\n    lang_dirs = [l for l in lang_dirs if l not in EXCLUDED_FOLDERS]\n    lang_dirs = [l for l in lang_dirs if os.path.isdir(src_root / l)]\n    if len(langs) > 0:\n        lang_dirs = [l for l in lang_dirs if l in langs]\n    print(f'{len(lang_dirs)} total languages found:')\n    print(lang_dirs)\n    print('')\n    split_size = int(args.split_size * 1024 * 1024)\n    for lang in lang_dirs:\n        lang_root = src_root / lang\n        data_dirs = os.listdir(lang_root)\n        if len(packages) > 0:\n            data_dirs = [d for d in data_dirs if d in packages]\n        data_dirs = [d for d in data_dirs if os.path.isdir(lang_root / d)]\n        print(f'{len(data_dirs)} total corpus found for language {lang}.')\n        print(data_dirs)\n        print('')\n        for dataset_name in data_dirs:\n            src_dir = lang_root / dataset_name\n            tgt_dir = tgt_root / lang / dataset_name\n            if not os.path.exists(tgt_dir):\n                os.makedirs(tgt_dir)\n            print(f'-> Processing {lang}-{dataset_name}')\n            prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, args.xz_output, split_size, args.make_test_file)\n        print('')",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('src_root', default='src', help='Root directory with all source files.  Expected structure is root dir -> language dirs -> package dirs -> text files to process')\n    parser.add_argument('tgt_root', default='tgt', help='Root directory with all target files.')\n    parser.add_argument('--langs', default='', help='A list of language codes to process.  If not set, all languages under src_root will be processed.')\n    parser.add_argument('--packages', default='', help='A list of packages to process.  If not set, all packages under the languages found will be processed.')\n    parser.add_argument('--no_xz_output', default=True, dest='xz_output', action='store_false', help='Output compressed xz files')\n    parser.add_argument('--split_size', default=50, type=int, help='How large to make each split, in MB')\n    parser.add_argument('--no_make_test_file', default=True, dest='make_test_file', action='store_false', help=\"Don't save a test file.  Honestly, we never even use it.  Best for low resource languages where every bit helps\")\n    args = parser.parse_args()\n    print('Processing files:')\n    print(f'source root: {args.src_root}')\n    print(f'target root: {args.tgt_root}')\n    print('')\n    langs = []\n    if len(args.langs) > 0:\n        langs = args.langs.split(',')\n        print('Only processing the following languages: ' + str(langs))\n    packages = []\n    if len(args.packages) > 0:\n        packages = args.packages.split(',')\n        print('Only processing the following packages: ' + str(packages))\n    src_root = Path(args.src_root)\n    tgt_root = Path(args.tgt_root)\n    lang_dirs = os.listdir(src_root)\n    lang_dirs = [l for l in lang_dirs if l not in EXCLUDED_FOLDERS]\n    lang_dirs = [l for l in lang_dirs if os.path.isdir(src_root / l)]\n    if len(langs) > 0:\n        lang_dirs = [l for l in lang_dirs if l in langs]\n    print(f'{len(lang_dirs)} total languages found:')\n    print(lang_dirs)\n    print('')\n    split_size = int(args.split_size * 1024 * 1024)\n    for lang in lang_dirs:\n        lang_root = src_root / lang\n        data_dirs = os.listdir(lang_root)\n        if len(packages) > 0:\n            data_dirs = [d for d in data_dirs if d in packages]\n        data_dirs = [d for d in data_dirs if os.path.isdir(lang_root / d)]\n        print(f'{len(data_dirs)} total corpus found for language {lang}.')\n        print(data_dirs)\n        print('')\n        for dataset_name in data_dirs:\n            src_dir = lang_root / dataset_name\n            tgt_dir = tgt_root / lang / dataset_name\n            if not os.path.exists(tgt_dir):\n                os.makedirs(tgt_dir)\n            print(f'-> Processing {lang}-{dataset_name}')\n            prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, args.xz_output, split_size, args.make_test_file)\n        print('')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('src_root', default='src', help='Root directory with all source files.  Expected structure is root dir -> language dirs -> package dirs -> text files to process')\n    parser.add_argument('tgt_root', default='tgt', help='Root directory with all target files.')\n    parser.add_argument('--langs', default='', help='A list of language codes to process.  If not set, all languages under src_root will be processed.')\n    parser.add_argument('--packages', default='', help='A list of packages to process.  If not set, all packages under the languages found will be processed.')\n    parser.add_argument('--no_xz_output', default=True, dest='xz_output', action='store_false', help='Output compressed xz files')\n    parser.add_argument('--split_size', default=50, type=int, help='How large to make each split, in MB')\n    parser.add_argument('--no_make_test_file', default=True, dest='make_test_file', action='store_false', help=\"Don't save a test file.  Honestly, we never even use it.  Best for low resource languages where every bit helps\")\n    args = parser.parse_args()\n    print('Processing files:')\n    print(f'source root: {args.src_root}')\n    print(f'target root: {args.tgt_root}')\n    print('')\n    langs = []\n    if len(args.langs) > 0:\n        langs = args.langs.split(',')\n        print('Only processing the following languages: ' + str(langs))\n    packages = []\n    if len(args.packages) > 0:\n        packages = args.packages.split(',')\n        print('Only processing the following packages: ' + str(packages))\n    src_root = Path(args.src_root)\n    tgt_root = Path(args.tgt_root)\n    lang_dirs = os.listdir(src_root)\n    lang_dirs = [l for l in lang_dirs if l not in EXCLUDED_FOLDERS]\n    lang_dirs = [l for l in lang_dirs if os.path.isdir(src_root / l)]\n    if len(langs) > 0:\n        lang_dirs = [l for l in lang_dirs if l in langs]\n    print(f'{len(lang_dirs)} total languages found:')\n    print(lang_dirs)\n    print('')\n    split_size = int(args.split_size * 1024 * 1024)\n    for lang in lang_dirs:\n        lang_root = src_root / lang\n        data_dirs = os.listdir(lang_root)\n        if len(packages) > 0:\n            data_dirs = [d for d in data_dirs if d in packages]\n        data_dirs = [d for d in data_dirs if os.path.isdir(lang_root / d)]\n        print(f'{len(data_dirs)} total corpus found for language {lang}.')\n        print(data_dirs)\n        print('')\n        for dataset_name in data_dirs:\n            src_dir = lang_root / dataset_name\n            tgt_dir = tgt_root / lang / dataset_name\n            if not os.path.exists(tgt_dir):\n                os.makedirs(tgt_dir)\n            print(f'-> Processing {lang}-{dataset_name}')\n            prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, args.xz_output, split_size, args.make_test_file)\n        print('')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('src_root', default='src', help='Root directory with all source files.  Expected structure is root dir -> language dirs -> package dirs -> text files to process')\n    parser.add_argument('tgt_root', default='tgt', help='Root directory with all target files.')\n    parser.add_argument('--langs', default='', help='A list of language codes to process.  If not set, all languages under src_root will be processed.')\n    parser.add_argument('--packages', default='', help='A list of packages to process.  If not set, all packages under the languages found will be processed.')\n    parser.add_argument('--no_xz_output', default=True, dest='xz_output', action='store_false', help='Output compressed xz files')\n    parser.add_argument('--split_size', default=50, type=int, help='How large to make each split, in MB')\n    parser.add_argument('--no_make_test_file', default=True, dest='make_test_file', action='store_false', help=\"Don't save a test file.  Honestly, we never even use it.  Best for low resource languages where every bit helps\")\n    args = parser.parse_args()\n    print('Processing files:')\n    print(f'source root: {args.src_root}')\n    print(f'target root: {args.tgt_root}')\n    print('')\n    langs = []\n    if len(args.langs) > 0:\n        langs = args.langs.split(',')\n        print('Only processing the following languages: ' + str(langs))\n    packages = []\n    if len(args.packages) > 0:\n        packages = args.packages.split(',')\n        print('Only processing the following packages: ' + str(packages))\n    src_root = Path(args.src_root)\n    tgt_root = Path(args.tgt_root)\n    lang_dirs = os.listdir(src_root)\n    lang_dirs = [l for l in lang_dirs if l not in EXCLUDED_FOLDERS]\n    lang_dirs = [l for l in lang_dirs if os.path.isdir(src_root / l)]\n    if len(langs) > 0:\n        lang_dirs = [l for l in lang_dirs if l in langs]\n    print(f'{len(lang_dirs)} total languages found:')\n    print(lang_dirs)\n    print('')\n    split_size = int(args.split_size * 1024 * 1024)\n    for lang in lang_dirs:\n        lang_root = src_root / lang\n        data_dirs = os.listdir(lang_root)\n        if len(packages) > 0:\n            data_dirs = [d for d in data_dirs if d in packages]\n        data_dirs = [d for d in data_dirs if os.path.isdir(lang_root / d)]\n        print(f'{len(data_dirs)} total corpus found for language {lang}.')\n        print(data_dirs)\n        print('')\n        for dataset_name in data_dirs:\n            src_dir = lang_root / dataset_name\n            tgt_dir = tgt_root / lang / dataset_name\n            if not os.path.exists(tgt_dir):\n                os.makedirs(tgt_dir)\n            print(f'-> Processing {lang}-{dataset_name}')\n            prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, args.xz_output, split_size, args.make_test_file)\n        print('')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('src_root', default='src', help='Root directory with all source files.  Expected structure is root dir -> language dirs -> package dirs -> text files to process')\n    parser.add_argument('tgt_root', default='tgt', help='Root directory with all target files.')\n    parser.add_argument('--langs', default='', help='A list of language codes to process.  If not set, all languages under src_root will be processed.')\n    parser.add_argument('--packages', default='', help='A list of packages to process.  If not set, all packages under the languages found will be processed.')\n    parser.add_argument('--no_xz_output', default=True, dest='xz_output', action='store_false', help='Output compressed xz files')\n    parser.add_argument('--split_size', default=50, type=int, help='How large to make each split, in MB')\n    parser.add_argument('--no_make_test_file', default=True, dest='make_test_file', action='store_false', help=\"Don't save a test file.  Honestly, we never even use it.  Best for low resource languages where every bit helps\")\n    args = parser.parse_args()\n    print('Processing files:')\n    print(f'source root: {args.src_root}')\n    print(f'target root: {args.tgt_root}')\n    print('')\n    langs = []\n    if len(args.langs) > 0:\n        langs = args.langs.split(',')\n        print('Only processing the following languages: ' + str(langs))\n    packages = []\n    if len(args.packages) > 0:\n        packages = args.packages.split(',')\n        print('Only processing the following packages: ' + str(packages))\n    src_root = Path(args.src_root)\n    tgt_root = Path(args.tgt_root)\n    lang_dirs = os.listdir(src_root)\n    lang_dirs = [l for l in lang_dirs if l not in EXCLUDED_FOLDERS]\n    lang_dirs = [l for l in lang_dirs if os.path.isdir(src_root / l)]\n    if len(langs) > 0:\n        lang_dirs = [l for l in lang_dirs if l in langs]\n    print(f'{len(lang_dirs)} total languages found:')\n    print(lang_dirs)\n    print('')\n    split_size = int(args.split_size * 1024 * 1024)\n    for lang in lang_dirs:\n        lang_root = src_root / lang\n        data_dirs = os.listdir(lang_root)\n        if len(packages) > 0:\n            data_dirs = [d for d in data_dirs if d in packages]\n        data_dirs = [d for d in data_dirs if os.path.isdir(lang_root / d)]\n        print(f'{len(data_dirs)} total corpus found for language {lang}.')\n        print(data_dirs)\n        print('')\n        for dataset_name in data_dirs:\n            src_dir = lang_root / dataset_name\n            tgt_dir = tgt_root / lang / dataset_name\n            if not os.path.exists(tgt_dir):\n                os.makedirs(tgt_dir)\n            print(f'-> Processing {lang}-{dataset_name}')\n            prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, args.xz_output, split_size, args.make_test_file)\n        print('')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('src_root', default='src', help='Root directory with all source files.  Expected structure is root dir -> language dirs -> package dirs -> text files to process')\n    parser.add_argument('tgt_root', default='tgt', help='Root directory with all target files.')\n    parser.add_argument('--langs', default='', help='A list of language codes to process.  If not set, all languages under src_root will be processed.')\n    parser.add_argument('--packages', default='', help='A list of packages to process.  If not set, all packages under the languages found will be processed.')\n    parser.add_argument('--no_xz_output', default=True, dest='xz_output', action='store_false', help='Output compressed xz files')\n    parser.add_argument('--split_size', default=50, type=int, help='How large to make each split, in MB')\n    parser.add_argument('--no_make_test_file', default=True, dest='make_test_file', action='store_false', help=\"Don't save a test file.  Honestly, we never even use it.  Best for low resource languages where every bit helps\")\n    args = parser.parse_args()\n    print('Processing files:')\n    print(f'source root: {args.src_root}')\n    print(f'target root: {args.tgt_root}')\n    print('')\n    langs = []\n    if len(args.langs) > 0:\n        langs = args.langs.split(',')\n        print('Only processing the following languages: ' + str(langs))\n    packages = []\n    if len(args.packages) > 0:\n        packages = args.packages.split(',')\n        print('Only processing the following packages: ' + str(packages))\n    src_root = Path(args.src_root)\n    tgt_root = Path(args.tgt_root)\n    lang_dirs = os.listdir(src_root)\n    lang_dirs = [l for l in lang_dirs if l not in EXCLUDED_FOLDERS]\n    lang_dirs = [l for l in lang_dirs if os.path.isdir(src_root / l)]\n    if len(langs) > 0:\n        lang_dirs = [l for l in lang_dirs if l in langs]\n    print(f'{len(lang_dirs)} total languages found:')\n    print(lang_dirs)\n    print('')\n    split_size = int(args.split_size * 1024 * 1024)\n    for lang in lang_dirs:\n        lang_root = src_root / lang\n        data_dirs = os.listdir(lang_root)\n        if len(packages) > 0:\n            data_dirs = [d for d in data_dirs if d in packages]\n        data_dirs = [d for d in data_dirs if os.path.isdir(lang_root / d)]\n        print(f'{len(data_dirs)} total corpus found for language {lang}.')\n        print(data_dirs)\n        print('')\n        for dataset_name in data_dirs:\n            src_dir = lang_root / dataset_name\n            tgt_dir = tgt_root / lang / dataset_name\n            if not os.path.exists(tgt_dir):\n                os.makedirs(tgt_dir)\n            print(f'-> Processing {lang}-{dataset_name}')\n            prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, args.xz_output, split_size, args.make_test_file)\n        print('')"
        ]
    },
    {
        "func_name": "prepare_lm_data",
        "original": "def prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, compress, split_size, make_test_file):\n    \"\"\"\n    Combine, shuffle and split data into smaller files, following a naming convention.\n    \"\"\"\n    assert isinstance(src_dir, Path)\n    assert isinstance(tgt_dir, Path)\n    with tempfile.TemporaryDirectory(dir=tgt_dir) as tempdir:\n        tgt_tmp = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp')\n        print(f'--> Copying files into {tgt_tmp}...')\n        input_files = glob.glob(str(src_dir) + '/*.txt') + glob.glob(str(src_dir) + '/*.txt.xz') + glob.glob(str(src_dir) + '/*.txt.gz')\n        for src_fn in tqdm(input_files):\n            if src_fn.endswith('.txt'):\n                cmd = f'cat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.xz'):\n                cmd = f'xzcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.gz'):\n                cmd = f'zcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            else:\n                raise AssertionError('should not have found %s' % src_fn)\n        tgt_tmp_shuffled = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp.shuffled')\n        print(f'--> Shuffling files into {tgt_tmp_shuffled}...')\n        cmd = f'cat {tgt_tmp} | shuf > {tgt_tmp_shuffled}'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to shuffle files!')\n        size = os.path.getsize(tgt_tmp_shuffled) / 1024 / 1024 / 1024\n        print(f'--> Shuffled file size: {size:.4f} GB')\n        if size < 0.1:\n            raise RuntimeError('Not enough data found to build a charlm.  At least 100MB data expected')\n        print(f'--> Splitting into smaller files of size {split_size} ...')\n        train_dir = tgt_dir / 'train'\n        if not os.path.exists(train_dir):\n            os.makedirs(train_dir)\n        cmd = f'split -C {split_size} -a 4 -d --additional-suffix .txt {tgt_tmp_shuffled} {train_dir}/{lang}-{dataset_name}-'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to split files!')\n        total = len(glob.glob(f'{train_dir}/*.txt'))\n        print(f'--> {total} total files generated.')\n        if total < 3:\n            raise RuntimeError('Something went wrong!  %d file(s) produced by shuffle and split, expected at least 3' % total)\n        dev_file = f'{tgt_dir}/dev.txt'\n        test_file = f'{tgt_dir}/test.txt'\n        if make_test_file:\n            print('--> Creating dev and test files...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-001.txt', test_file)\n            txt_files = [dev_file, test_file] + glob.glob(f'{train_dir}/*.txt')\n        else:\n            print('--> Creating dev file...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            txt_files = [dev_file] + glob.glob(f'{train_dir}/*.txt')\n        if compress:\n            print('--> Compressing files...')\n            for txt_file in tqdm(txt_files):\n                subprocess.run(['xz', txt_file])\n        print('--> Cleaning up...')\n    print(f'--> All done for {lang}-{dataset_name}.\\n')",
        "mutated": [
            "def prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, compress, split_size, make_test_file):\n    if False:\n        i = 10\n    '\\n    Combine, shuffle and split data into smaller files, following a naming convention.\\n    '\n    assert isinstance(src_dir, Path)\n    assert isinstance(tgt_dir, Path)\n    with tempfile.TemporaryDirectory(dir=tgt_dir) as tempdir:\n        tgt_tmp = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp')\n        print(f'--> Copying files into {tgt_tmp}...')\n        input_files = glob.glob(str(src_dir) + '/*.txt') + glob.glob(str(src_dir) + '/*.txt.xz') + glob.glob(str(src_dir) + '/*.txt.gz')\n        for src_fn in tqdm(input_files):\n            if src_fn.endswith('.txt'):\n                cmd = f'cat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.xz'):\n                cmd = f'xzcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.gz'):\n                cmd = f'zcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            else:\n                raise AssertionError('should not have found %s' % src_fn)\n        tgt_tmp_shuffled = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp.shuffled')\n        print(f'--> Shuffling files into {tgt_tmp_shuffled}...')\n        cmd = f'cat {tgt_tmp} | shuf > {tgt_tmp_shuffled}'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to shuffle files!')\n        size = os.path.getsize(tgt_tmp_shuffled) / 1024 / 1024 / 1024\n        print(f'--> Shuffled file size: {size:.4f} GB')\n        if size < 0.1:\n            raise RuntimeError('Not enough data found to build a charlm.  At least 100MB data expected')\n        print(f'--> Splitting into smaller files of size {split_size} ...')\n        train_dir = tgt_dir / 'train'\n        if not os.path.exists(train_dir):\n            os.makedirs(train_dir)\n        cmd = f'split -C {split_size} -a 4 -d --additional-suffix .txt {tgt_tmp_shuffled} {train_dir}/{lang}-{dataset_name}-'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to split files!')\n        total = len(glob.glob(f'{train_dir}/*.txt'))\n        print(f'--> {total} total files generated.')\n        if total < 3:\n            raise RuntimeError('Something went wrong!  %d file(s) produced by shuffle and split, expected at least 3' % total)\n        dev_file = f'{tgt_dir}/dev.txt'\n        test_file = f'{tgt_dir}/test.txt'\n        if make_test_file:\n            print('--> Creating dev and test files...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-001.txt', test_file)\n            txt_files = [dev_file, test_file] + glob.glob(f'{train_dir}/*.txt')\n        else:\n            print('--> Creating dev file...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            txt_files = [dev_file] + glob.glob(f'{train_dir}/*.txt')\n        if compress:\n            print('--> Compressing files...')\n            for txt_file in tqdm(txt_files):\n                subprocess.run(['xz', txt_file])\n        print('--> Cleaning up...')\n    print(f'--> All done for {lang}-{dataset_name}.\\n')",
            "def prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, compress, split_size, make_test_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Combine, shuffle and split data into smaller files, following a naming convention.\\n    '\n    assert isinstance(src_dir, Path)\n    assert isinstance(tgt_dir, Path)\n    with tempfile.TemporaryDirectory(dir=tgt_dir) as tempdir:\n        tgt_tmp = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp')\n        print(f'--> Copying files into {tgt_tmp}...')\n        input_files = glob.glob(str(src_dir) + '/*.txt') + glob.glob(str(src_dir) + '/*.txt.xz') + glob.glob(str(src_dir) + '/*.txt.gz')\n        for src_fn in tqdm(input_files):\n            if src_fn.endswith('.txt'):\n                cmd = f'cat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.xz'):\n                cmd = f'xzcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.gz'):\n                cmd = f'zcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            else:\n                raise AssertionError('should not have found %s' % src_fn)\n        tgt_tmp_shuffled = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp.shuffled')\n        print(f'--> Shuffling files into {tgt_tmp_shuffled}...')\n        cmd = f'cat {tgt_tmp} | shuf > {tgt_tmp_shuffled}'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to shuffle files!')\n        size = os.path.getsize(tgt_tmp_shuffled) / 1024 / 1024 / 1024\n        print(f'--> Shuffled file size: {size:.4f} GB')\n        if size < 0.1:\n            raise RuntimeError('Not enough data found to build a charlm.  At least 100MB data expected')\n        print(f'--> Splitting into smaller files of size {split_size} ...')\n        train_dir = tgt_dir / 'train'\n        if not os.path.exists(train_dir):\n            os.makedirs(train_dir)\n        cmd = f'split -C {split_size} -a 4 -d --additional-suffix .txt {tgt_tmp_shuffled} {train_dir}/{lang}-{dataset_name}-'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to split files!')\n        total = len(glob.glob(f'{train_dir}/*.txt'))\n        print(f'--> {total} total files generated.')\n        if total < 3:\n            raise RuntimeError('Something went wrong!  %d file(s) produced by shuffle and split, expected at least 3' % total)\n        dev_file = f'{tgt_dir}/dev.txt'\n        test_file = f'{tgt_dir}/test.txt'\n        if make_test_file:\n            print('--> Creating dev and test files...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-001.txt', test_file)\n            txt_files = [dev_file, test_file] + glob.glob(f'{train_dir}/*.txt')\n        else:\n            print('--> Creating dev file...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            txt_files = [dev_file] + glob.glob(f'{train_dir}/*.txt')\n        if compress:\n            print('--> Compressing files...')\n            for txt_file in tqdm(txt_files):\n                subprocess.run(['xz', txt_file])\n        print('--> Cleaning up...')\n    print(f'--> All done for {lang}-{dataset_name}.\\n')",
            "def prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, compress, split_size, make_test_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Combine, shuffle and split data into smaller files, following a naming convention.\\n    '\n    assert isinstance(src_dir, Path)\n    assert isinstance(tgt_dir, Path)\n    with tempfile.TemporaryDirectory(dir=tgt_dir) as tempdir:\n        tgt_tmp = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp')\n        print(f'--> Copying files into {tgt_tmp}...')\n        input_files = glob.glob(str(src_dir) + '/*.txt') + glob.glob(str(src_dir) + '/*.txt.xz') + glob.glob(str(src_dir) + '/*.txt.gz')\n        for src_fn in tqdm(input_files):\n            if src_fn.endswith('.txt'):\n                cmd = f'cat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.xz'):\n                cmd = f'xzcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.gz'):\n                cmd = f'zcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            else:\n                raise AssertionError('should not have found %s' % src_fn)\n        tgt_tmp_shuffled = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp.shuffled')\n        print(f'--> Shuffling files into {tgt_tmp_shuffled}...')\n        cmd = f'cat {tgt_tmp} | shuf > {tgt_tmp_shuffled}'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to shuffle files!')\n        size = os.path.getsize(tgt_tmp_shuffled) / 1024 / 1024 / 1024\n        print(f'--> Shuffled file size: {size:.4f} GB')\n        if size < 0.1:\n            raise RuntimeError('Not enough data found to build a charlm.  At least 100MB data expected')\n        print(f'--> Splitting into smaller files of size {split_size} ...')\n        train_dir = tgt_dir / 'train'\n        if not os.path.exists(train_dir):\n            os.makedirs(train_dir)\n        cmd = f'split -C {split_size} -a 4 -d --additional-suffix .txt {tgt_tmp_shuffled} {train_dir}/{lang}-{dataset_name}-'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to split files!')\n        total = len(glob.glob(f'{train_dir}/*.txt'))\n        print(f'--> {total} total files generated.')\n        if total < 3:\n            raise RuntimeError('Something went wrong!  %d file(s) produced by shuffle and split, expected at least 3' % total)\n        dev_file = f'{tgt_dir}/dev.txt'\n        test_file = f'{tgt_dir}/test.txt'\n        if make_test_file:\n            print('--> Creating dev and test files...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-001.txt', test_file)\n            txt_files = [dev_file, test_file] + glob.glob(f'{train_dir}/*.txt')\n        else:\n            print('--> Creating dev file...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            txt_files = [dev_file] + glob.glob(f'{train_dir}/*.txt')\n        if compress:\n            print('--> Compressing files...')\n            for txt_file in tqdm(txt_files):\n                subprocess.run(['xz', txt_file])\n        print('--> Cleaning up...')\n    print(f'--> All done for {lang}-{dataset_name}.\\n')",
            "def prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, compress, split_size, make_test_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Combine, shuffle and split data into smaller files, following a naming convention.\\n    '\n    assert isinstance(src_dir, Path)\n    assert isinstance(tgt_dir, Path)\n    with tempfile.TemporaryDirectory(dir=tgt_dir) as tempdir:\n        tgt_tmp = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp')\n        print(f'--> Copying files into {tgt_tmp}...')\n        input_files = glob.glob(str(src_dir) + '/*.txt') + glob.glob(str(src_dir) + '/*.txt.xz') + glob.glob(str(src_dir) + '/*.txt.gz')\n        for src_fn in tqdm(input_files):\n            if src_fn.endswith('.txt'):\n                cmd = f'cat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.xz'):\n                cmd = f'xzcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.gz'):\n                cmd = f'zcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            else:\n                raise AssertionError('should not have found %s' % src_fn)\n        tgt_tmp_shuffled = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp.shuffled')\n        print(f'--> Shuffling files into {tgt_tmp_shuffled}...')\n        cmd = f'cat {tgt_tmp} | shuf > {tgt_tmp_shuffled}'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to shuffle files!')\n        size = os.path.getsize(tgt_tmp_shuffled) / 1024 / 1024 / 1024\n        print(f'--> Shuffled file size: {size:.4f} GB')\n        if size < 0.1:\n            raise RuntimeError('Not enough data found to build a charlm.  At least 100MB data expected')\n        print(f'--> Splitting into smaller files of size {split_size} ...')\n        train_dir = tgt_dir / 'train'\n        if not os.path.exists(train_dir):\n            os.makedirs(train_dir)\n        cmd = f'split -C {split_size} -a 4 -d --additional-suffix .txt {tgt_tmp_shuffled} {train_dir}/{lang}-{dataset_name}-'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to split files!')\n        total = len(glob.glob(f'{train_dir}/*.txt'))\n        print(f'--> {total} total files generated.')\n        if total < 3:\n            raise RuntimeError('Something went wrong!  %d file(s) produced by shuffle and split, expected at least 3' % total)\n        dev_file = f'{tgt_dir}/dev.txt'\n        test_file = f'{tgt_dir}/test.txt'\n        if make_test_file:\n            print('--> Creating dev and test files...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-001.txt', test_file)\n            txt_files = [dev_file, test_file] + glob.glob(f'{train_dir}/*.txt')\n        else:\n            print('--> Creating dev file...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            txt_files = [dev_file] + glob.glob(f'{train_dir}/*.txt')\n        if compress:\n            print('--> Compressing files...')\n            for txt_file in tqdm(txt_files):\n                subprocess.run(['xz', txt_file])\n        print('--> Cleaning up...')\n    print(f'--> All done for {lang}-{dataset_name}.\\n')",
            "def prepare_lm_data(src_dir, tgt_dir, lang, dataset_name, compress, split_size, make_test_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Combine, shuffle and split data into smaller files, following a naming convention.\\n    '\n    assert isinstance(src_dir, Path)\n    assert isinstance(tgt_dir, Path)\n    with tempfile.TemporaryDirectory(dir=tgt_dir) as tempdir:\n        tgt_tmp = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp')\n        print(f'--> Copying files into {tgt_tmp}...')\n        input_files = glob.glob(str(src_dir) + '/*.txt') + glob.glob(str(src_dir) + '/*.txt.xz') + glob.glob(str(src_dir) + '/*.txt.gz')\n        for src_fn in tqdm(input_files):\n            if src_fn.endswith('.txt'):\n                cmd = f'cat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.xz'):\n                cmd = f'xzcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            elif src_fn.endswith('.txt.gz'):\n                cmd = f'zcat {src_fn} >> {tgt_tmp}'\n                subprocess.run(cmd, shell=True)\n            else:\n                raise AssertionError('should not have found %s' % src_fn)\n        tgt_tmp_shuffled = os.path.join(tempdir, f'{lang}-{dataset_name}.tmp.shuffled')\n        print(f'--> Shuffling files into {tgt_tmp_shuffled}...')\n        cmd = f'cat {tgt_tmp} | shuf > {tgt_tmp_shuffled}'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to shuffle files!')\n        size = os.path.getsize(tgt_tmp_shuffled) / 1024 / 1024 / 1024\n        print(f'--> Shuffled file size: {size:.4f} GB')\n        if size < 0.1:\n            raise RuntimeError('Not enough data found to build a charlm.  At least 100MB data expected')\n        print(f'--> Splitting into smaller files of size {split_size} ...')\n        train_dir = tgt_dir / 'train'\n        if not os.path.exists(train_dir):\n            os.makedirs(train_dir)\n        cmd = f'split -C {split_size} -a 4 -d --additional-suffix .txt {tgt_tmp_shuffled} {train_dir}/{lang}-{dataset_name}-'\n        result = subprocess.run(cmd, shell=True)\n        if result.returncode != 0:\n            raise RuntimeError('Failed to split files!')\n        total = len(glob.glob(f'{train_dir}/*.txt'))\n        print(f'--> {total} total files generated.')\n        if total < 3:\n            raise RuntimeError('Something went wrong!  %d file(s) produced by shuffle and split, expected at least 3' % total)\n        dev_file = f'{tgt_dir}/dev.txt'\n        test_file = f'{tgt_dir}/test.txt'\n        if make_test_file:\n            print('--> Creating dev and test files...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-001.txt', test_file)\n            txt_files = [dev_file, test_file] + glob.glob(f'{train_dir}/*.txt')\n        else:\n            print('--> Creating dev file...')\n            shutil.move(f'{train_dir}/{lang}-{dataset_name}-000.txt', dev_file)\n            txt_files = [dev_file] + glob.glob(f'{train_dir}/*.txt')\n        if compress:\n            print('--> Compressing files...')\n            for txt_file in tqdm(txt_files):\n                subprocess.run(['xz', txt_file])\n        print('--> Cleaning up...')\n    print(f'--> All done for {lang}-{dataset_name}.\\n')"
        ]
    }
]