[
    {
        "func_name": "__init__",
        "original": "def __init__(self, cfg: dict, env: BaseEnvManager=None, policy: namedtuple=None, tb_logger: 'SummaryWriter'=None, exp_name: Optional[str]='default_experiment', instance_name: Optional[str]='evaluator') -> None:\n    \"\"\"\n        Overview:\n            Init method. Load config and use ``self._cfg`` setting to build common serial evaluator components,             e.g. logger helper, timer.\n        Arguments:\n            - cfg (:obj:`EasyDict`): Configuration EasyDict.\n        \"\"\"\n    self._cfg = cfg\n    self._exp_name = exp_name\n    self._instance_name = instance_name\n    if get_rank() == 0:\n        if tb_logger is not None:\n            (self._logger, _) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name, need_tb=False)\n            self._tb_logger = tb_logger\n        else:\n            (self._logger, self._tb_logger) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name)\n    else:\n        (self._logger, self._tb_logger) = (None, None)\n    self.reset(policy, env)\n    self._timer = EasyTimer()\n    self._default_n_episode = cfg.n_episode\n    self._stop_value = cfg.stop_value\n    self._render = cfg.render\n    assert self._render.mode in ('envstep', 'train_iter'), 'mode should be envstep or train_iter'",
        "mutated": [
            "def __init__(self, cfg: dict, env: BaseEnvManager=None, policy: namedtuple=None, tb_logger: 'SummaryWriter'=None, exp_name: Optional[str]='default_experiment', instance_name: Optional[str]='evaluator') -> None:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Init method. Load config and use ``self._cfg`` setting to build common serial evaluator components,             e.g. logger helper, timer.\\n        Arguments:\\n            - cfg (:obj:`EasyDict`): Configuration EasyDict.\\n        '\n    self._cfg = cfg\n    self._exp_name = exp_name\n    self._instance_name = instance_name\n    if get_rank() == 0:\n        if tb_logger is not None:\n            (self._logger, _) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name, need_tb=False)\n            self._tb_logger = tb_logger\n        else:\n            (self._logger, self._tb_logger) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name)\n    else:\n        (self._logger, self._tb_logger) = (None, None)\n    self.reset(policy, env)\n    self._timer = EasyTimer()\n    self._default_n_episode = cfg.n_episode\n    self._stop_value = cfg.stop_value\n    self._render = cfg.render\n    assert self._render.mode in ('envstep', 'train_iter'), 'mode should be envstep or train_iter'",
            "def __init__(self, cfg: dict, env: BaseEnvManager=None, policy: namedtuple=None, tb_logger: 'SummaryWriter'=None, exp_name: Optional[str]='default_experiment', instance_name: Optional[str]='evaluator') -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Init method. Load config and use ``self._cfg`` setting to build common serial evaluator components,             e.g. logger helper, timer.\\n        Arguments:\\n            - cfg (:obj:`EasyDict`): Configuration EasyDict.\\n        '\n    self._cfg = cfg\n    self._exp_name = exp_name\n    self._instance_name = instance_name\n    if get_rank() == 0:\n        if tb_logger is not None:\n            (self._logger, _) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name, need_tb=False)\n            self._tb_logger = tb_logger\n        else:\n            (self._logger, self._tb_logger) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name)\n    else:\n        (self._logger, self._tb_logger) = (None, None)\n    self.reset(policy, env)\n    self._timer = EasyTimer()\n    self._default_n_episode = cfg.n_episode\n    self._stop_value = cfg.stop_value\n    self._render = cfg.render\n    assert self._render.mode in ('envstep', 'train_iter'), 'mode should be envstep or train_iter'",
            "def __init__(self, cfg: dict, env: BaseEnvManager=None, policy: namedtuple=None, tb_logger: 'SummaryWriter'=None, exp_name: Optional[str]='default_experiment', instance_name: Optional[str]='evaluator') -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Init method. Load config and use ``self._cfg`` setting to build common serial evaluator components,             e.g. logger helper, timer.\\n        Arguments:\\n            - cfg (:obj:`EasyDict`): Configuration EasyDict.\\n        '\n    self._cfg = cfg\n    self._exp_name = exp_name\n    self._instance_name = instance_name\n    if get_rank() == 0:\n        if tb_logger is not None:\n            (self._logger, _) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name, need_tb=False)\n            self._tb_logger = tb_logger\n        else:\n            (self._logger, self._tb_logger) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name)\n    else:\n        (self._logger, self._tb_logger) = (None, None)\n    self.reset(policy, env)\n    self._timer = EasyTimer()\n    self._default_n_episode = cfg.n_episode\n    self._stop_value = cfg.stop_value\n    self._render = cfg.render\n    assert self._render.mode in ('envstep', 'train_iter'), 'mode should be envstep or train_iter'",
            "def __init__(self, cfg: dict, env: BaseEnvManager=None, policy: namedtuple=None, tb_logger: 'SummaryWriter'=None, exp_name: Optional[str]='default_experiment', instance_name: Optional[str]='evaluator') -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Init method. Load config and use ``self._cfg`` setting to build common serial evaluator components,             e.g. logger helper, timer.\\n        Arguments:\\n            - cfg (:obj:`EasyDict`): Configuration EasyDict.\\n        '\n    self._cfg = cfg\n    self._exp_name = exp_name\n    self._instance_name = instance_name\n    if get_rank() == 0:\n        if tb_logger is not None:\n            (self._logger, _) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name, need_tb=False)\n            self._tb_logger = tb_logger\n        else:\n            (self._logger, self._tb_logger) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name)\n    else:\n        (self._logger, self._tb_logger) = (None, None)\n    self.reset(policy, env)\n    self._timer = EasyTimer()\n    self._default_n_episode = cfg.n_episode\n    self._stop_value = cfg.stop_value\n    self._render = cfg.render\n    assert self._render.mode in ('envstep', 'train_iter'), 'mode should be envstep or train_iter'",
            "def __init__(self, cfg: dict, env: BaseEnvManager=None, policy: namedtuple=None, tb_logger: 'SummaryWriter'=None, exp_name: Optional[str]='default_experiment', instance_name: Optional[str]='evaluator') -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Init method. Load config and use ``self._cfg`` setting to build common serial evaluator components,             e.g. logger helper, timer.\\n        Arguments:\\n            - cfg (:obj:`EasyDict`): Configuration EasyDict.\\n        '\n    self._cfg = cfg\n    self._exp_name = exp_name\n    self._instance_name = instance_name\n    if get_rank() == 0:\n        if tb_logger is not None:\n            (self._logger, _) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name, need_tb=False)\n            self._tb_logger = tb_logger\n        else:\n            (self._logger, self._tb_logger) = build_logger('./{}/log/{}'.format(self._exp_name, self._instance_name), self._instance_name)\n    else:\n        (self._logger, self._tb_logger) = (None, None)\n    self.reset(policy, env)\n    self._timer = EasyTimer()\n    self._default_n_episode = cfg.n_episode\n    self._stop_value = cfg.stop_value\n    self._render = cfg.render\n    assert self._render.mode in ('envstep', 'train_iter'), 'mode should be envstep or train_iter'"
        ]
    },
    {
        "func_name": "reset_env",
        "original": "def reset_env(self, _env: Optional[BaseEnvManager]=None) -> None:\n    \"\"\"\n        Overview:\n            Reset evaluator's environment. In some case, we need evaluator use the same policy in different                 environments. We can use reset_env to reset the environment.\n            If _env is None, reset the old environment.\n            If _env is not None, replace the old environment in the evaluator with the                 new passed in environment and launch.\n        Arguments:\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\n        \"\"\"\n    if _env is not None:\n        self._env = _env\n        self._env.launch()\n        self._env_num = self._env.env_num\n    else:\n        self._env.reset()",
        "mutated": [
            "def reset_env(self, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n    \"\\n        Overview:\\n            Reset evaluator's environment. In some case, we need evaluator use the same policy in different                 environments. We can use reset_env to reset the environment.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the                 new passed in environment and launch.\\n        Arguments:\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self._env = _env\n        self._env.launch()\n        self._env_num = self._env.env_num\n    else:\n        self._env.reset()",
            "def reset_env(self, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Overview:\\n            Reset evaluator's environment. In some case, we need evaluator use the same policy in different                 environments. We can use reset_env to reset the environment.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the                 new passed in environment and launch.\\n        Arguments:\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self._env = _env\n        self._env.launch()\n        self._env_num = self._env.env_num\n    else:\n        self._env.reset()",
            "def reset_env(self, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Overview:\\n            Reset evaluator's environment. In some case, we need evaluator use the same policy in different                 environments. We can use reset_env to reset the environment.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the                 new passed in environment and launch.\\n        Arguments:\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self._env = _env\n        self._env.launch()\n        self._env_num = self._env.env_num\n    else:\n        self._env.reset()",
            "def reset_env(self, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Overview:\\n            Reset evaluator's environment. In some case, we need evaluator use the same policy in different                 environments. We can use reset_env to reset the environment.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the                 new passed in environment and launch.\\n        Arguments:\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self._env = _env\n        self._env.launch()\n        self._env_num = self._env.env_num\n    else:\n        self._env.reset()",
            "def reset_env(self, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Overview:\\n            Reset evaluator's environment. In some case, we need evaluator use the same policy in different                 environments. We can use reset_env to reset the environment.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the                 new passed in environment and launch.\\n        Arguments:\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self._env = _env\n        self._env.launch()\n        self._env_num = self._env.env_num\n    else:\n        self._env.reset()"
        ]
    },
    {
        "func_name": "reset_policy",
        "original": "def reset_policy(self, _policy: Optional[namedtuple]=None) -> None:\n    \"\"\"\n        Overview:\n            Reset evaluator's policy. In some case, we need evaluator work in this same environment but use                different policy. We can use reset_policy to reset the policy.\n            If _policy is None, reset the old policy.\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\n        Arguments:\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\n        \"\"\"\n    assert hasattr(self, '_env'), 'please set env first'\n    if _policy is not None:\n        self._policy = _policy\n    self._policy_cfg = self._policy.get_attribute('cfg')\n    self._policy.reset()",
        "mutated": [
            "def reset_policy(self, _policy: Optional[namedtuple]=None) -> None:\n    if False:\n        i = 10\n    \"\\n        Overview:\\n            Reset evaluator's policy. In some case, we need evaluator work in this same environment but use                different policy. We can use reset_policy to reset the policy.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n        \"\n    assert hasattr(self, '_env'), 'please set env first'\n    if _policy is not None:\n        self._policy = _policy\n    self._policy_cfg = self._policy.get_attribute('cfg')\n    self._policy.reset()",
            "def reset_policy(self, _policy: Optional[namedtuple]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Overview:\\n            Reset evaluator's policy. In some case, we need evaluator work in this same environment but use                different policy. We can use reset_policy to reset the policy.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n        \"\n    assert hasattr(self, '_env'), 'please set env first'\n    if _policy is not None:\n        self._policy = _policy\n    self._policy_cfg = self._policy.get_attribute('cfg')\n    self._policy.reset()",
            "def reset_policy(self, _policy: Optional[namedtuple]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Overview:\\n            Reset evaluator's policy. In some case, we need evaluator work in this same environment but use                different policy. We can use reset_policy to reset the policy.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n        \"\n    assert hasattr(self, '_env'), 'please set env first'\n    if _policy is not None:\n        self._policy = _policy\n    self._policy_cfg = self._policy.get_attribute('cfg')\n    self._policy.reset()",
            "def reset_policy(self, _policy: Optional[namedtuple]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Overview:\\n            Reset evaluator's policy. In some case, we need evaluator work in this same environment but use                different policy. We can use reset_policy to reset the policy.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n        \"\n    assert hasattr(self, '_env'), 'please set env first'\n    if _policy is not None:\n        self._policy = _policy\n    self._policy_cfg = self._policy.get_attribute('cfg')\n    self._policy.reset()",
            "def reset_policy(self, _policy: Optional[namedtuple]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Overview:\\n            Reset evaluator's policy. In some case, we need evaluator work in this same environment but use                different policy. We can use reset_policy to reset the policy.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n        \"\n    assert hasattr(self, '_env'), 'please set env first'\n    if _policy is not None:\n        self._policy = _policy\n    self._policy_cfg = self._policy.get_attribute('cfg')\n    self._policy.reset()"
        ]
    },
    {
        "func_name": "reset",
        "original": "def reset(self, _policy: Optional[namedtuple]=None, _env: Optional[BaseEnvManager]=None) -> None:\n    \"\"\"\n        Overview:\n            Reset evaluator's policy and environment. Use new policy and environment to collect data.\n            If _env is None, reset the old environment.\n            If _env is not None, replace the old environment in the evaluator with the new passed in                 environment and launch.\n            If _policy is None, reset the old policy.\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\n        Arguments:\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\n        \"\"\"\n    if _env is not None:\n        self.reset_env(_env)\n    if _policy is not None:\n        self.reset_policy(_policy)\n    if self._policy_cfg.type == 'dreamer_command':\n        self._states = None\n        self._resets = np.array([False for i in range(self._env_num)])\n    self._max_episode_return = float('-inf')\n    self._last_eval_iter = -1\n    self._end_flag = False\n    self._last_render_iter = -1",
        "mutated": [
            "def reset(self, _policy: Optional[namedtuple]=None, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n    \"\\n        Overview:\\n            Reset evaluator's policy and environment. Use new policy and environment to collect data.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the new passed in                 environment and launch.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self.reset_env(_env)\n    if _policy is not None:\n        self.reset_policy(_policy)\n    if self._policy_cfg.type == 'dreamer_command':\n        self._states = None\n        self._resets = np.array([False for i in range(self._env_num)])\n    self._max_episode_return = float('-inf')\n    self._last_eval_iter = -1\n    self._end_flag = False\n    self._last_render_iter = -1",
            "def reset(self, _policy: Optional[namedtuple]=None, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Overview:\\n            Reset evaluator's policy and environment. Use new policy and environment to collect data.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the new passed in                 environment and launch.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self.reset_env(_env)\n    if _policy is not None:\n        self.reset_policy(_policy)\n    if self._policy_cfg.type == 'dreamer_command':\n        self._states = None\n        self._resets = np.array([False for i in range(self._env_num)])\n    self._max_episode_return = float('-inf')\n    self._last_eval_iter = -1\n    self._end_flag = False\n    self._last_render_iter = -1",
            "def reset(self, _policy: Optional[namedtuple]=None, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Overview:\\n            Reset evaluator's policy and environment. Use new policy and environment to collect data.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the new passed in                 environment and launch.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self.reset_env(_env)\n    if _policy is not None:\n        self.reset_policy(_policy)\n    if self._policy_cfg.type == 'dreamer_command':\n        self._states = None\n        self._resets = np.array([False for i in range(self._env_num)])\n    self._max_episode_return = float('-inf')\n    self._last_eval_iter = -1\n    self._end_flag = False\n    self._last_render_iter = -1",
            "def reset(self, _policy: Optional[namedtuple]=None, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Overview:\\n            Reset evaluator's policy and environment. Use new policy and environment to collect data.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the new passed in                 environment and launch.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self.reset_env(_env)\n    if _policy is not None:\n        self.reset_policy(_policy)\n    if self._policy_cfg.type == 'dreamer_command':\n        self._states = None\n        self._resets = np.array([False for i in range(self._env_num)])\n    self._max_episode_return = float('-inf')\n    self._last_eval_iter = -1\n    self._end_flag = False\n    self._last_render_iter = -1",
            "def reset(self, _policy: Optional[namedtuple]=None, _env: Optional[BaseEnvManager]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Overview:\\n            Reset evaluator's policy and environment. Use new policy and environment to collect data.\\n            If _env is None, reset the old environment.\\n            If _env is not None, replace the old environment in the evaluator with the new passed in                 environment and launch.\\n            If _policy is None, reset the old policy.\\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\\n        Arguments:\\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of eval_mode policy\\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized                 env_manager(BaseEnvManager)\\n        \"\n    if _env is not None:\n        self.reset_env(_env)\n    if _policy is not None:\n        self.reset_policy(_policy)\n    if self._policy_cfg.type == 'dreamer_command':\n        self._states = None\n        self._resets = np.array([False for i in range(self._env_num)])\n    self._max_episode_return = float('-inf')\n    self._last_eval_iter = -1\n    self._end_flag = False\n    self._last_render_iter = -1"
        ]
    },
    {
        "func_name": "close",
        "original": "def close(self) -> None:\n    \"\"\"\n        Overview:\n            Close the evaluator. If end_flag is False, close the environment, flush the tb_logger                and close the tb_logger.\n        \"\"\"\n    if self._end_flag:\n        return\n    self._end_flag = True\n    self._env.close()\n    if self._tb_logger:\n        self._tb_logger.flush()\n        self._tb_logger.close()",
        "mutated": [
            "def close(self) -> None:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Close the evaluator. If end_flag is False, close the environment, flush the tb_logger                and close the tb_logger.\\n        '\n    if self._end_flag:\n        return\n    self._end_flag = True\n    self._env.close()\n    if self._tb_logger:\n        self._tb_logger.flush()\n        self._tb_logger.close()",
            "def close(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Close the evaluator. If end_flag is False, close the environment, flush the tb_logger                and close the tb_logger.\\n        '\n    if self._end_flag:\n        return\n    self._end_flag = True\n    self._env.close()\n    if self._tb_logger:\n        self._tb_logger.flush()\n        self._tb_logger.close()",
            "def close(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Close the evaluator. If end_flag is False, close the environment, flush the tb_logger                and close the tb_logger.\\n        '\n    if self._end_flag:\n        return\n    self._end_flag = True\n    self._env.close()\n    if self._tb_logger:\n        self._tb_logger.flush()\n        self._tb_logger.close()",
            "def close(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Close the evaluator. If end_flag is False, close the environment, flush the tb_logger                and close the tb_logger.\\n        '\n    if self._end_flag:\n        return\n    self._end_flag = True\n    self._env.close()\n    if self._tb_logger:\n        self._tb_logger.flush()\n        self._tb_logger.close()",
            "def close(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Close the evaluator. If end_flag is False, close the environment, flush the tb_logger                and close the tb_logger.\\n        '\n    if self._end_flag:\n        return\n    self._end_flag = True\n    self._env.close()\n    if self._tb_logger:\n        self._tb_logger.flush()\n        self._tb_logger.close()"
        ]
    },
    {
        "func_name": "__del__",
        "original": "def __del__(self):\n    \"\"\"\n        Overview:\n            Execute the close command and close the evaluator. __del__ is automatically called                 to destroy the evaluator instance when the evaluator finishes its work\n        \"\"\"\n    self.close()",
        "mutated": [
            "def __del__(self):\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Execute the close command and close the evaluator. __del__ is automatically called                 to destroy the evaluator instance when the evaluator finishes its work\\n        '\n    self.close()",
            "def __del__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Execute the close command and close the evaluator. __del__ is automatically called                 to destroy the evaluator instance when the evaluator finishes its work\\n        '\n    self.close()",
            "def __del__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Execute the close command and close the evaluator. __del__ is automatically called                 to destroy the evaluator instance when the evaluator finishes its work\\n        '\n    self.close()",
            "def __del__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Execute the close command and close the evaluator. __del__ is automatically called                 to destroy the evaluator instance when the evaluator finishes its work\\n        '\n    self.close()",
            "def __del__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Execute the close command and close the evaluator. __del__ is automatically called                 to destroy the evaluator instance when the evaluator finishes its work\\n        '\n    self.close()"
        ]
    },
    {
        "func_name": "should_eval",
        "original": "def should_eval(self, train_iter: int) -> bool:\n    \"\"\"\n        Overview:\n            Determine whether you need to start the evaluation mode, if the number of training has reached                the maximum number of times to start the evaluator, return True\n        \"\"\"\n    if train_iter == self._last_eval_iter:\n        return False\n    if train_iter - self._last_eval_iter < self._cfg.eval_freq and train_iter != 0:\n        return False\n    self._last_eval_iter = train_iter\n    return True",
        "mutated": [
            "def should_eval(self, train_iter: int) -> bool:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Determine whether you need to start the evaluation mode, if the number of training has reached                the maximum number of times to start the evaluator, return True\\n        '\n    if train_iter == self._last_eval_iter:\n        return False\n    if train_iter - self._last_eval_iter < self._cfg.eval_freq and train_iter != 0:\n        return False\n    self._last_eval_iter = train_iter\n    return True",
            "def should_eval(self, train_iter: int) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Determine whether you need to start the evaluation mode, if the number of training has reached                the maximum number of times to start the evaluator, return True\\n        '\n    if train_iter == self._last_eval_iter:\n        return False\n    if train_iter - self._last_eval_iter < self._cfg.eval_freq and train_iter != 0:\n        return False\n    self._last_eval_iter = train_iter\n    return True",
            "def should_eval(self, train_iter: int) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Determine whether you need to start the evaluation mode, if the number of training has reached                the maximum number of times to start the evaluator, return True\\n        '\n    if train_iter == self._last_eval_iter:\n        return False\n    if train_iter - self._last_eval_iter < self._cfg.eval_freq and train_iter != 0:\n        return False\n    self._last_eval_iter = train_iter\n    return True",
            "def should_eval(self, train_iter: int) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Determine whether you need to start the evaluation mode, if the number of training has reached                the maximum number of times to start the evaluator, return True\\n        '\n    if train_iter == self._last_eval_iter:\n        return False\n    if train_iter - self._last_eval_iter < self._cfg.eval_freq and train_iter != 0:\n        return False\n    self._last_eval_iter = train_iter\n    return True",
            "def should_eval(self, train_iter: int) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Determine whether you need to start the evaluation mode, if the number of training has reached                the maximum number of times to start the evaluator, return True\\n        '\n    if train_iter == self._last_eval_iter:\n        return False\n    if train_iter - self._last_eval_iter < self._cfg.eval_freq and train_iter != 0:\n        return False\n    self._last_eval_iter = train_iter\n    return True"
        ]
    },
    {
        "func_name": "_should_render",
        "original": "def _should_render(self, envstep, train_iter):\n    if self._render.render_freq == -1:\n        return False\n    iter = envstep if self._render.mode == 'envstep' else train_iter\n    if iter - self._last_render_iter < self._render.render_freq:\n        return False\n    self._last_render_iter = iter\n    return True",
        "mutated": [
            "def _should_render(self, envstep, train_iter):\n    if False:\n        i = 10\n    if self._render.render_freq == -1:\n        return False\n    iter = envstep if self._render.mode == 'envstep' else train_iter\n    if iter - self._last_render_iter < self._render.render_freq:\n        return False\n    self._last_render_iter = iter\n    return True",
            "def _should_render(self, envstep, train_iter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self._render.render_freq == -1:\n        return False\n    iter = envstep if self._render.mode == 'envstep' else train_iter\n    if iter - self._last_render_iter < self._render.render_freq:\n        return False\n    self._last_render_iter = iter\n    return True",
            "def _should_render(self, envstep, train_iter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self._render.render_freq == -1:\n        return False\n    iter = envstep if self._render.mode == 'envstep' else train_iter\n    if iter - self._last_render_iter < self._render.render_freq:\n        return False\n    self._last_render_iter = iter\n    return True",
            "def _should_render(self, envstep, train_iter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self._render.render_freq == -1:\n        return False\n    iter = envstep if self._render.mode == 'envstep' else train_iter\n    if iter - self._last_render_iter < self._render.render_freq:\n        return False\n    self._last_render_iter = iter\n    return True",
            "def _should_render(self, envstep, train_iter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self._render.render_freq == -1:\n        return False\n    iter = envstep if self._render.mode == 'envstep' else train_iter\n    if iter - self._last_render_iter < self._render.render_freq:\n        return False\n    self._last_render_iter = iter\n    return True"
        ]
    },
    {
        "func_name": "eval",
        "original": "def eval(self, save_ckpt_fn: Callable=None, train_iter: int=-1, envstep: int=-1, n_episode: Optional[int]=None, force_render: bool=False, policy_kwargs: Optional[Dict]={}) -> Tuple[bool, Dict[str, List]]:\n    \"\"\"\n        Overview:\n            Evaluate policy and store the best policy based on whether it reaches the highest historical reward.\n        Arguments:\n            - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward.\n            - train_iter (:obj:`int`): Current training iteration.\n            - envstep (:obj:`int`): Current env interaction step.\n            - n_episode (:obj:`int`): Number of evaluation episodes.\n        Returns:\n            - stop_flag (:obj:`bool`): Whether this training program can be ended.\n            - episode_info (:obj:`Dict[str, List]`): Current evaluation episode information.\n        \"\"\"\n    stop_flag = False\n    if get_rank() == 0:\n        if n_episode is None:\n            n_episode = self._default_n_episode\n        assert n_episode is not None, 'please indicate eval n_episode'\n        envstep_count = 0\n        info = {}\n        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)\n        self._env.reset()\n        self._policy.reset()\n        render = force_render or self._should_render(envstep, train_iter)\n        with self._timer:\n            while not eval_monitor.is_finished():\n                obs = self._env.ready_obs\n                obs = to_tensor(obs, dtype=torch.float32)\n                if render:\n                    eval_monitor.update_video(self._env.ready_imgs)\n                if self._policy_cfg.type == 'dreamer_command':\n                    policy_output = self._policy.forward(obs, **policy_kwargs, reset=self._resets, state=self._states)\n                    self._states = [output['state'] for output in policy_output.values()]\n                else:\n                    policy_output = self._policy.forward(obs, **policy_kwargs)\n                actions = {i: a['action'] for (i, a) in policy_output.items()}\n                actions = to_ndarray(actions)\n                timesteps = self._env.step(actions)\n                timesteps = to_tensor(timesteps, dtype=torch.float32)\n                for (env_id, t) in timesteps.items():\n                    if t.info.get('abnormal', False):\n                        self._policy.reset([env_id])\n                        continue\n                    if self._policy_cfg.type == 'dreamer_command':\n                        self._resets[env_id] = t.done\n                    if t.done:\n                        if 'figure_path' in self._cfg and self._cfg.figure_path is not None:\n                            self._env.enable_save_figure(env_id, self._cfg.figure_path)\n                        self._policy.reset([env_id])\n                        reward = t.info['eval_episode_return']\n                        saved_info = {'eval_episode_return': t.info['eval_episode_return']}\n                        if 'episode_info' in t.info:\n                            saved_info.update(t.info['episode_info'])\n                        eval_monitor.update_info(env_id, saved_info)\n                        eval_monitor.update_reward(env_id, reward)\n                        self._logger.info('[EVALUATOR]env {} finish episode, final reward: {:.4f}, current episode: {}'.format(env_id, eval_monitor.get_latest_reward(env_id), eval_monitor.get_current_episode()))\n                    envstep_count += 1\n        duration = self._timer.value\n        episode_return = eval_monitor.get_episode_return()\n        info = {'train_iter': train_iter, 'ckpt_name': 'iteration_{}.pth.tar'.format(train_iter), 'episode_count': n_episode, 'envstep_count': envstep_count, 'avg_envstep_per_episode': envstep_count / n_episode, 'evaluate_time': duration, 'avg_envstep_per_sec': envstep_count / duration, 'avg_time_per_episode': n_episode / duration, 'reward_mean': np.mean(episode_return), 'reward_std': np.std(episode_return), 'reward_max': np.max(episode_return), 'reward_min': np.min(episode_return)}\n        episode_info = eval_monitor.get_episode_info()\n        if episode_info is not None:\n            info.update(episode_info)\n        self._logger.info(self._logger.get_tabulate_vars_hor(info))\n        for (k, v) in info.items():\n            if k in ['train_iter', 'ckpt_name', 'each_reward']:\n                continue\n            if not np.isscalar(v):\n                continue\n            self._tb_logger.add_scalar('{}_iter/'.format(self._instance_name) + k, v, train_iter)\n            self._tb_logger.add_scalar('{}_step/'.format(self._instance_name) + k, v, envstep)\n        if render:\n            video_title = '{}_{}/'.format(self._instance_name, self._render.mode)\n            videos = eval_monitor.get_video()\n            render_iter = envstep if self._render.mode == 'envstep' else train_iter\n            from ding.utils import fps\n            self._tb_logger.add_video(video_title, videos, render_iter, fps(self._env))\n        episode_return = np.mean(episode_return)\n        if episode_return > self._max_episode_return:\n            if save_ckpt_fn:\n                save_ckpt_fn('ckpt_best.pth.tar')\n            self._max_episode_return = episode_return\n        stop_flag = episode_return >= self._stop_value and train_iter > 0\n        if stop_flag:\n            self._logger.info('[DI-engine serial pipeline] ' + 'Current episode_return: {:.4f} is greater than stop_value: {}'.format(episode_return, self._stop_value) + ', so your RL agent is converged, you can refer to ' + \"'log/evaluator/evaluator_logger.txt' for details.\")\n    if get_world_size() > 1:\n        objects = [stop_flag, episode_info]\n        broadcast_object_list(objects, src=0)\n        (stop_flag, episode_info) = objects\n    episode_info = to_item(episode_info)\n    return (stop_flag, episode_info)",
        "mutated": [
            "def eval(self, save_ckpt_fn: Callable=None, train_iter: int=-1, envstep: int=-1, n_episode: Optional[int]=None, force_render: bool=False, policy_kwargs: Optional[Dict]={}) -> Tuple[bool, Dict[str, List]]:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Evaluate policy and store the best policy based on whether it reaches the highest historical reward.\\n        Arguments:\\n            - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward.\\n            - train_iter (:obj:`int`): Current training iteration.\\n            - envstep (:obj:`int`): Current env interaction step.\\n            - n_episode (:obj:`int`): Number of evaluation episodes.\\n        Returns:\\n            - stop_flag (:obj:`bool`): Whether this training program can be ended.\\n            - episode_info (:obj:`Dict[str, List]`): Current evaluation episode information.\\n        '\n    stop_flag = False\n    if get_rank() == 0:\n        if n_episode is None:\n            n_episode = self._default_n_episode\n        assert n_episode is not None, 'please indicate eval n_episode'\n        envstep_count = 0\n        info = {}\n        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)\n        self._env.reset()\n        self._policy.reset()\n        render = force_render or self._should_render(envstep, train_iter)\n        with self._timer:\n            while not eval_monitor.is_finished():\n                obs = self._env.ready_obs\n                obs = to_tensor(obs, dtype=torch.float32)\n                if render:\n                    eval_monitor.update_video(self._env.ready_imgs)\n                if self._policy_cfg.type == 'dreamer_command':\n                    policy_output = self._policy.forward(obs, **policy_kwargs, reset=self._resets, state=self._states)\n                    self._states = [output['state'] for output in policy_output.values()]\n                else:\n                    policy_output = self._policy.forward(obs, **policy_kwargs)\n                actions = {i: a['action'] for (i, a) in policy_output.items()}\n                actions = to_ndarray(actions)\n                timesteps = self._env.step(actions)\n                timesteps = to_tensor(timesteps, dtype=torch.float32)\n                for (env_id, t) in timesteps.items():\n                    if t.info.get('abnormal', False):\n                        self._policy.reset([env_id])\n                        continue\n                    if self._policy_cfg.type == 'dreamer_command':\n                        self._resets[env_id] = t.done\n                    if t.done:\n                        if 'figure_path' in self._cfg and self._cfg.figure_path is not None:\n                            self._env.enable_save_figure(env_id, self._cfg.figure_path)\n                        self._policy.reset([env_id])\n                        reward = t.info['eval_episode_return']\n                        saved_info = {'eval_episode_return': t.info['eval_episode_return']}\n                        if 'episode_info' in t.info:\n                            saved_info.update(t.info['episode_info'])\n                        eval_monitor.update_info(env_id, saved_info)\n                        eval_monitor.update_reward(env_id, reward)\n                        self._logger.info('[EVALUATOR]env {} finish episode, final reward: {:.4f}, current episode: {}'.format(env_id, eval_monitor.get_latest_reward(env_id), eval_monitor.get_current_episode()))\n                    envstep_count += 1\n        duration = self._timer.value\n        episode_return = eval_monitor.get_episode_return()\n        info = {'train_iter': train_iter, 'ckpt_name': 'iteration_{}.pth.tar'.format(train_iter), 'episode_count': n_episode, 'envstep_count': envstep_count, 'avg_envstep_per_episode': envstep_count / n_episode, 'evaluate_time': duration, 'avg_envstep_per_sec': envstep_count / duration, 'avg_time_per_episode': n_episode / duration, 'reward_mean': np.mean(episode_return), 'reward_std': np.std(episode_return), 'reward_max': np.max(episode_return), 'reward_min': np.min(episode_return)}\n        episode_info = eval_monitor.get_episode_info()\n        if episode_info is not None:\n            info.update(episode_info)\n        self._logger.info(self._logger.get_tabulate_vars_hor(info))\n        for (k, v) in info.items():\n            if k in ['train_iter', 'ckpt_name', 'each_reward']:\n                continue\n            if not np.isscalar(v):\n                continue\n            self._tb_logger.add_scalar('{}_iter/'.format(self._instance_name) + k, v, train_iter)\n            self._tb_logger.add_scalar('{}_step/'.format(self._instance_name) + k, v, envstep)\n        if render:\n            video_title = '{}_{}/'.format(self._instance_name, self._render.mode)\n            videos = eval_monitor.get_video()\n            render_iter = envstep if self._render.mode == 'envstep' else train_iter\n            from ding.utils import fps\n            self._tb_logger.add_video(video_title, videos, render_iter, fps(self._env))\n        episode_return = np.mean(episode_return)\n        if episode_return > self._max_episode_return:\n            if save_ckpt_fn:\n                save_ckpt_fn('ckpt_best.pth.tar')\n            self._max_episode_return = episode_return\n        stop_flag = episode_return >= self._stop_value and train_iter > 0\n        if stop_flag:\n            self._logger.info('[DI-engine serial pipeline] ' + 'Current episode_return: {:.4f} is greater than stop_value: {}'.format(episode_return, self._stop_value) + ', so your RL agent is converged, you can refer to ' + \"'log/evaluator/evaluator_logger.txt' for details.\")\n    if get_world_size() > 1:\n        objects = [stop_flag, episode_info]\n        broadcast_object_list(objects, src=0)\n        (stop_flag, episode_info) = objects\n    episode_info = to_item(episode_info)\n    return (stop_flag, episode_info)",
            "def eval(self, save_ckpt_fn: Callable=None, train_iter: int=-1, envstep: int=-1, n_episode: Optional[int]=None, force_render: bool=False, policy_kwargs: Optional[Dict]={}) -> Tuple[bool, Dict[str, List]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Evaluate policy and store the best policy based on whether it reaches the highest historical reward.\\n        Arguments:\\n            - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward.\\n            - train_iter (:obj:`int`): Current training iteration.\\n            - envstep (:obj:`int`): Current env interaction step.\\n            - n_episode (:obj:`int`): Number of evaluation episodes.\\n        Returns:\\n            - stop_flag (:obj:`bool`): Whether this training program can be ended.\\n            - episode_info (:obj:`Dict[str, List]`): Current evaluation episode information.\\n        '\n    stop_flag = False\n    if get_rank() == 0:\n        if n_episode is None:\n            n_episode = self._default_n_episode\n        assert n_episode is not None, 'please indicate eval n_episode'\n        envstep_count = 0\n        info = {}\n        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)\n        self._env.reset()\n        self._policy.reset()\n        render = force_render or self._should_render(envstep, train_iter)\n        with self._timer:\n            while not eval_monitor.is_finished():\n                obs = self._env.ready_obs\n                obs = to_tensor(obs, dtype=torch.float32)\n                if render:\n                    eval_monitor.update_video(self._env.ready_imgs)\n                if self._policy_cfg.type == 'dreamer_command':\n                    policy_output = self._policy.forward(obs, **policy_kwargs, reset=self._resets, state=self._states)\n                    self._states = [output['state'] for output in policy_output.values()]\n                else:\n                    policy_output = self._policy.forward(obs, **policy_kwargs)\n                actions = {i: a['action'] for (i, a) in policy_output.items()}\n                actions = to_ndarray(actions)\n                timesteps = self._env.step(actions)\n                timesteps = to_tensor(timesteps, dtype=torch.float32)\n                for (env_id, t) in timesteps.items():\n                    if t.info.get('abnormal', False):\n                        self._policy.reset([env_id])\n                        continue\n                    if self._policy_cfg.type == 'dreamer_command':\n                        self._resets[env_id] = t.done\n                    if t.done:\n                        if 'figure_path' in self._cfg and self._cfg.figure_path is not None:\n                            self._env.enable_save_figure(env_id, self._cfg.figure_path)\n                        self._policy.reset([env_id])\n                        reward = t.info['eval_episode_return']\n                        saved_info = {'eval_episode_return': t.info['eval_episode_return']}\n                        if 'episode_info' in t.info:\n                            saved_info.update(t.info['episode_info'])\n                        eval_monitor.update_info(env_id, saved_info)\n                        eval_monitor.update_reward(env_id, reward)\n                        self._logger.info('[EVALUATOR]env {} finish episode, final reward: {:.4f}, current episode: {}'.format(env_id, eval_monitor.get_latest_reward(env_id), eval_monitor.get_current_episode()))\n                    envstep_count += 1\n        duration = self._timer.value\n        episode_return = eval_monitor.get_episode_return()\n        info = {'train_iter': train_iter, 'ckpt_name': 'iteration_{}.pth.tar'.format(train_iter), 'episode_count': n_episode, 'envstep_count': envstep_count, 'avg_envstep_per_episode': envstep_count / n_episode, 'evaluate_time': duration, 'avg_envstep_per_sec': envstep_count / duration, 'avg_time_per_episode': n_episode / duration, 'reward_mean': np.mean(episode_return), 'reward_std': np.std(episode_return), 'reward_max': np.max(episode_return), 'reward_min': np.min(episode_return)}\n        episode_info = eval_monitor.get_episode_info()\n        if episode_info is not None:\n            info.update(episode_info)\n        self._logger.info(self._logger.get_tabulate_vars_hor(info))\n        for (k, v) in info.items():\n            if k in ['train_iter', 'ckpt_name', 'each_reward']:\n                continue\n            if not np.isscalar(v):\n                continue\n            self._tb_logger.add_scalar('{}_iter/'.format(self._instance_name) + k, v, train_iter)\n            self._tb_logger.add_scalar('{}_step/'.format(self._instance_name) + k, v, envstep)\n        if render:\n            video_title = '{}_{}/'.format(self._instance_name, self._render.mode)\n            videos = eval_monitor.get_video()\n            render_iter = envstep if self._render.mode == 'envstep' else train_iter\n            from ding.utils import fps\n            self._tb_logger.add_video(video_title, videos, render_iter, fps(self._env))\n        episode_return = np.mean(episode_return)\n        if episode_return > self._max_episode_return:\n            if save_ckpt_fn:\n                save_ckpt_fn('ckpt_best.pth.tar')\n            self._max_episode_return = episode_return\n        stop_flag = episode_return >= self._stop_value and train_iter > 0\n        if stop_flag:\n            self._logger.info('[DI-engine serial pipeline] ' + 'Current episode_return: {:.4f} is greater than stop_value: {}'.format(episode_return, self._stop_value) + ', so your RL agent is converged, you can refer to ' + \"'log/evaluator/evaluator_logger.txt' for details.\")\n    if get_world_size() > 1:\n        objects = [stop_flag, episode_info]\n        broadcast_object_list(objects, src=0)\n        (stop_flag, episode_info) = objects\n    episode_info = to_item(episode_info)\n    return (stop_flag, episode_info)",
            "def eval(self, save_ckpt_fn: Callable=None, train_iter: int=-1, envstep: int=-1, n_episode: Optional[int]=None, force_render: bool=False, policy_kwargs: Optional[Dict]={}) -> Tuple[bool, Dict[str, List]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Evaluate policy and store the best policy based on whether it reaches the highest historical reward.\\n        Arguments:\\n            - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward.\\n            - train_iter (:obj:`int`): Current training iteration.\\n            - envstep (:obj:`int`): Current env interaction step.\\n            - n_episode (:obj:`int`): Number of evaluation episodes.\\n        Returns:\\n            - stop_flag (:obj:`bool`): Whether this training program can be ended.\\n            - episode_info (:obj:`Dict[str, List]`): Current evaluation episode information.\\n        '\n    stop_flag = False\n    if get_rank() == 0:\n        if n_episode is None:\n            n_episode = self._default_n_episode\n        assert n_episode is not None, 'please indicate eval n_episode'\n        envstep_count = 0\n        info = {}\n        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)\n        self._env.reset()\n        self._policy.reset()\n        render = force_render or self._should_render(envstep, train_iter)\n        with self._timer:\n            while not eval_monitor.is_finished():\n                obs = self._env.ready_obs\n                obs = to_tensor(obs, dtype=torch.float32)\n                if render:\n                    eval_monitor.update_video(self._env.ready_imgs)\n                if self._policy_cfg.type == 'dreamer_command':\n                    policy_output = self._policy.forward(obs, **policy_kwargs, reset=self._resets, state=self._states)\n                    self._states = [output['state'] for output in policy_output.values()]\n                else:\n                    policy_output = self._policy.forward(obs, **policy_kwargs)\n                actions = {i: a['action'] for (i, a) in policy_output.items()}\n                actions = to_ndarray(actions)\n                timesteps = self._env.step(actions)\n                timesteps = to_tensor(timesteps, dtype=torch.float32)\n                for (env_id, t) in timesteps.items():\n                    if t.info.get('abnormal', False):\n                        self._policy.reset([env_id])\n                        continue\n                    if self._policy_cfg.type == 'dreamer_command':\n                        self._resets[env_id] = t.done\n                    if t.done:\n                        if 'figure_path' in self._cfg and self._cfg.figure_path is not None:\n                            self._env.enable_save_figure(env_id, self._cfg.figure_path)\n                        self._policy.reset([env_id])\n                        reward = t.info['eval_episode_return']\n                        saved_info = {'eval_episode_return': t.info['eval_episode_return']}\n                        if 'episode_info' in t.info:\n                            saved_info.update(t.info['episode_info'])\n                        eval_monitor.update_info(env_id, saved_info)\n                        eval_monitor.update_reward(env_id, reward)\n                        self._logger.info('[EVALUATOR]env {} finish episode, final reward: {:.4f}, current episode: {}'.format(env_id, eval_monitor.get_latest_reward(env_id), eval_monitor.get_current_episode()))\n                    envstep_count += 1\n        duration = self._timer.value\n        episode_return = eval_monitor.get_episode_return()\n        info = {'train_iter': train_iter, 'ckpt_name': 'iteration_{}.pth.tar'.format(train_iter), 'episode_count': n_episode, 'envstep_count': envstep_count, 'avg_envstep_per_episode': envstep_count / n_episode, 'evaluate_time': duration, 'avg_envstep_per_sec': envstep_count / duration, 'avg_time_per_episode': n_episode / duration, 'reward_mean': np.mean(episode_return), 'reward_std': np.std(episode_return), 'reward_max': np.max(episode_return), 'reward_min': np.min(episode_return)}\n        episode_info = eval_monitor.get_episode_info()\n        if episode_info is not None:\n            info.update(episode_info)\n        self._logger.info(self._logger.get_tabulate_vars_hor(info))\n        for (k, v) in info.items():\n            if k in ['train_iter', 'ckpt_name', 'each_reward']:\n                continue\n            if not np.isscalar(v):\n                continue\n            self._tb_logger.add_scalar('{}_iter/'.format(self._instance_name) + k, v, train_iter)\n            self._tb_logger.add_scalar('{}_step/'.format(self._instance_name) + k, v, envstep)\n        if render:\n            video_title = '{}_{}/'.format(self._instance_name, self._render.mode)\n            videos = eval_monitor.get_video()\n            render_iter = envstep if self._render.mode == 'envstep' else train_iter\n            from ding.utils import fps\n            self._tb_logger.add_video(video_title, videos, render_iter, fps(self._env))\n        episode_return = np.mean(episode_return)\n        if episode_return > self._max_episode_return:\n            if save_ckpt_fn:\n                save_ckpt_fn('ckpt_best.pth.tar')\n            self._max_episode_return = episode_return\n        stop_flag = episode_return >= self._stop_value and train_iter > 0\n        if stop_flag:\n            self._logger.info('[DI-engine serial pipeline] ' + 'Current episode_return: {:.4f} is greater than stop_value: {}'.format(episode_return, self._stop_value) + ', so your RL agent is converged, you can refer to ' + \"'log/evaluator/evaluator_logger.txt' for details.\")\n    if get_world_size() > 1:\n        objects = [stop_flag, episode_info]\n        broadcast_object_list(objects, src=0)\n        (stop_flag, episode_info) = objects\n    episode_info = to_item(episode_info)\n    return (stop_flag, episode_info)",
            "def eval(self, save_ckpt_fn: Callable=None, train_iter: int=-1, envstep: int=-1, n_episode: Optional[int]=None, force_render: bool=False, policy_kwargs: Optional[Dict]={}) -> Tuple[bool, Dict[str, List]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Evaluate policy and store the best policy based on whether it reaches the highest historical reward.\\n        Arguments:\\n            - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward.\\n            - train_iter (:obj:`int`): Current training iteration.\\n            - envstep (:obj:`int`): Current env interaction step.\\n            - n_episode (:obj:`int`): Number of evaluation episodes.\\n        Returns:\\n            - stop_flag (:obj:`bool`): Whether this training program can be ended.\\n            - episode_info (:obj:`Dict[str, List]`): Current evaluation episode information.\\n        '\n    stop_flag = False\n    if get_rank() == 0:\n        if n_episode is None:\n            n_episode = self._default_n_episode\n        assert n_episode is not None, 'please indicate eval n_episode'\n        envstep_count = 0\n        info = {}\n        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)\n        self._env.reset()\n        self._policy.reset()\n        render = force_render or self._should_render(envstep, train_iter)\n        with self._timer:\n            while not eval_monitor.is_finished():\n                obs = self._env.ready_obs\n                obs = to_tensor(obs, dtype=torch.float32)\n                if render:\n                    eval_monitor.update_video(self._env.ready_imgs)\n                if self._policy_cfg.type == 'dreamer_command':\n                    policy_output = self._policy.forward(obs, **policy_kwargs, reset=self._resets, state=self._states)\n                    self._states = [output['state'] for output in policy_output.values()]\n                else:\n                    policy_output = self._policy.forward(obs, **policy_kwargs)\n                actions = {i: a['action'] for (i, a) in policy_output.items()}\n                actions = to_ndarray(actions)\n                timesteps = self._env.step(actions)\n                timesteps = to_tensor(timesteps, dtype=torch.float32)\n                for (env_id, t) in timesteps.items():\n                    if t.info.get('abnormal', False):\n                        self._policy.reset([env_id])\n                        continue\n                    if self._policy_cfg.type == 'dreamer_command':\n                        self._resets[env_id] = t.done\n                    if t.done:\n                        if 'figure_path' in self._cfg and self._cfg.figure_path is not None:\n                            self._env.enable_save_figure(env_id, self._cfg.figure_path)\n                        self._policy.reset([env_id])\n                        reward = t.info['eval_episode_return']\n                        saved_info = {'eval_episode_return': t.info['eval_episode_return']}\n                        if 'episode_info' in t.info:\n                            saved_info.update(t.info['episode_info'])\n                        eval_monitor.update_info(env_id, saved_info)\n                        eval_monitor.update_reward(env_id, reward)\n                        self._logger.info('[EVALUATOR]env {} finish episode, final reward: {:.4f}, current episode: {}'.format(env_id, eval_monitor.get_latest_reward(env_id), eval_monitor.get_current_episode()))\n                    envstep_count += 1\n        duration = self._timer.value\n        episode_return = eval_monitor.get_episode_return()\n        info = {'train_iter': train_iter, 'ckpt_name': 'iteration_{}.pth.tar'.format(train_iter), 'episode_count': n_episode, 'envstep_count': envstep_count, 'avg_envstep_per_episode': envstep_count / n_episode, 'evaluate_time': duration, 'avg_envstep_per_sec': envstep_count / duration, 'avg_time_per_episode': n_episode / duration, 'reward_mean': np.mean(episode_return), 'reward_std': np.std(episode_return), 'reward_max': np.max(episode_return), 'reward_min': np.min(episode_return)}\n        episode_info = eval_monitor.get_episode_info()\n        if episode_info is not None:\n            info.update(episode_info)\n        self._logger.info(self._logger.get_tabulate_vars_hor(info))\n        for (k, v) in info.items():\n            if k in ['train_iter', 'ckpt_name', 'each_reward']:\n                continue\n            if not np.isscalar(v):\n                continue\n            self._tb_logger.add_scalar('{}_iter/'.format(self._instance_name) + k, v, train_iter)\n            self._tb_logger.add_scalar('{}_step/'.format(self._instance_name) + k, v, envstep)\n        if render:\n            video_title = '{}_{}/'.format(self._instance_name, self._render.mode)\n            videos = eval_monitor.get_video()\n            render_iter = envstep if self._render.mode == 'envstep' else train_iter\n            from ding.utils import fps\n            self._tb_logger.add_video(video_title, videos, render_iter, fps(self._env))\n        episode_return = np.mean(episode_return)\n        if episode_return > self._max_episode_return:\n            if save_ckpt_fn:\n                save_ckpt_fn('ckpt_best.pth.tar')\n            self._max_episode_return = episode_return\n        stop_flag = episode_return >= self._stop_value and train_iter > 0\n        if stop_flag:\n            self._logger.info('[DI-engine serial pipeline] ' + 'Current episode_return: {:.4f} is greater than stop_value: {}'.format(episode_return, self._stop_value) + ', so your RL agent is converged, you can refer to ' + \"'log/evaluator/evaluator_logger.txt' for details.\")\n    if get_world_size() > 1:\n        objects = [stop_flag, episode_info]\n        broadcast_object_list(objects, src=0)\n        (stop_flag, episode_info) = objects\n    episode_info = to_item(episode_info)\n    return (stop_flag, episode_info)",
            "def eval(self, save_ckpt_fn: Callable=None, train_iter: int=-1, envstep: int=-1, n_episode: Optional[int]=None, force_render: bool=False, policy_kwargs: Optional[Dict]={}) -> Tuple[bool, Dict[str, List]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Evaluate policy and store the best policy based on whether it reaches the highest historical reward.\\n        Arguments:\\n            - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward.\\n            - train_iter (:obj:`int`): Current training iteration.\\n            - envstep (:obj:`int`): Current env interaction step.\\n            - n_episode (:obj:`int`): Number of evaluation episodes.\\n        Returns:\\n            - stop_flag (:obj:`bool`): Whether this training program can be ended.\\n            - episode_info (:obj:`Dict[str, List]`): Current evaluation episode information.\\n        '\n    stop_flag = False\n    if get_rank() == 0:\n        if n_episode is None:\n            n_episode = self._default_n_episode\n        assert n_episode is not None, 'please indicate eval n_episode'\n        envstep_count = 0\n        info = {}\n        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)\n        self._env.reset()\n        self._policy.reset()\n        render = force_render or self._should_render(envstep, train_iter)\n        with self._timer:\n            while not eval_monitor.is_finished():\n                obs = self._env.ready_obs\n                obs = to_tensor(obs, dtype=torch.float32)\n                if render:\n                    eval_monitor.update_video(self._env.ready_imgs)\n                if self._policy_cfg.type == 'dreamer_command':\n                    policy_output = self._policy.forward(obs, **policy_kwargs, reset=self._resets, state=self._states)\n                    self._states = [output['state'] for output in policy_output.values()]\n                else:\n                    policy_output = self._policy.forward(obs, **policy_kwargs)\n                actions = {i: a['action'] for (i, a) in policy_output.items()}\n                actions = to_ndarray(actions)\n                timesteps = self._env.step(actions)\n                timesteps = to_tensor(timesteps, dtype=torch.float32)\n                for (env_id, t) in timesteps.items():\n                    if t.info.get('abnormal', False):\n                        self._policy.reset([env_id])\n                        continue\n                    if self._policy_cfg.type == 'dreamer_command':\n                        self._resets[env_id] = t.done\n                    if t.done:\n                        if 'figure_path' in self._cfg and self._cfg.figure_path is not None:\n                            self._env.enable_save_figure(env_id, self._cfg.figure_path)\n                        self._policy.reset([env_id])\n                        reward = t.info['eval_episode_return']\n                        saved_info = {'eval_episode_return': t.info['eval_episode_return']}\n                        if 'episode_info' in t.info:\n                            saved_info.update(t.info['episode_info'])\n                        eval_monitor.update_info(env_id, saved_info)\n                        eval_monitor.update_reward(env_id, reward)\n                        self._logger.info('[EVALUATOR]env {} finish episode, final reward: {:.4f}, current episode: {}'.format(env_id, eval_monitor.get_latest_reward(env_id), eval_monitor.get_current_episode()))\n                    envstep_count += 1\n        duration = self._timer.value\n        episode_return = eval_monitor.get_episode_return()\n        info = {'train_iter': train_iter, 'ckpt_name': 'iteration_{}.pth.tar'.format(train_iter), 'episode_count': n_episode, 'envstep_count': envstep_count, 'avg_envstep_per_episode': envstep_count / n_episode, 'evaluate_time': duration, 'avg_envstep_per_sec': envstep_count / duration, 'avg_time_per_episode': n_episode / duration, 'reward_mean': np.mean(episode_return), 'reward_std': np.std(episode_return), 'reward_max': np.max(episode_return), 'reward_min': np.min(episode_return)}\n        episode_info = eval_monitor.get_episode_info()\n        if episode_info is not None:\n            info.update(episode_info)\n        self._logger.info(self._logger.get_tabulate_vars_hor(info))\n        for (k, v) in info.items():\n            if k in ['train_iter', 'ckpt_name', 'each_reward']:\n                continue\n            if not np.isscalar(v):\n                continue\n            self._tb_logger.add_scalar('{}_iter/'.format(self._instance_name) + k, v, train_iter)\n            self._tb_logger.add_scalar('{}_step/'.format(self._instance_name) + k, v, envstep)\n        if render:\n            video_title = '{}_{}/'.format(self._instance_name, self._render.mode)\n            videos = eval_monitor.get_video()\n            render_iter = envstep if self._render.mode == 'envstep' else train_iter\n            from ding.utils import fps\n            self._tb_logger.add_video(video_title, videos, render_iter, fps(self._env))\n        episode_return = np.mean(episode_return)\n        if episode_return > self._max_episode_return:\n            if save_ckpt_fn:\n                save_ckpt_fn('ckpt_best.pth.tar')\n            self._max_episode_return = episode_return\n        stop_flag = episode_return >= self._stop_value and train_iter > 0\n        if stop_flag:\n            self._logger.info('[DI-engine serial pipeline] ' + 'Current episode_return: {:.4f} is greater than stop_value: {}'.format(episode_return, self._stop_value) + ', so your RL agent is converged, you can refer to ' + \"'log/evaluator/evaluator_logger.txt' for details.\")\n    if get_world_size() > 1:\n        objects = [stop_flag, episode_info]\n        broadcast_object_list(objects, src=0)\n        (stop_flag, episode_info) = objects\n    episode_info = to_item(episode_info)\n    return (stop_flag, episode_info)"
        ]
    }
]