[
    {
        "func_name": "__init__",
        "original": "def __init__(self, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', **kwargs):\n    super(CooksDistance, self).__init__(ax=ax, **kwargs)\n    self.draw_threshold = draw_threshold\n    self.linefmt = linefmt\n    self.markerfmt = markerfmt\n    self._model = LinearRegression()",
        "mutated": [
            "def __init__(self, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', **kwargs):\n    if False:\n        i = 10\n    super(CooksDistance, self).__init__(ax=ax, **kwargs)\n    self.draw_threshold = draw_threshold\n    self.linefmt = linefmt\n    self.markerfmt = markerfmt\n    self._model = LinearRegression()",
            "def __init__(self, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(CooksDistance, self).__init__(ax=ax, **kwargs)\n    self.draw_threshold = draw_threshold\n    self.linefmt = linefmt\n    self.markerfmt = markerfmt\n    self._model = LinearRegression()",
            "def __init__(self, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(CooksDistance, self).__init__(ax=ax, **kwargs)\n    self.draw_threshold = draw_threshold\n    self.linefmt = linefmt\n    self.markerfmt = markerfmt\n    self._model = LinearRegression()",
            "def __init__(self, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(CooksDistance, self).__init__(ax=ax, **kwargs)\n    self.draw_threshold = draw_threshold\n    self.linefmt = linefmt\n    self.markerfmt = markerfmt\n    self._model = LinearRegression()",
            "def __init__(self, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(CooksDistance, self).__init__(ax=ax, **kwargs)\n    self.draw_threshold = draw_threshold\n    self.linefmt = linefmt\n    self.markerfmt = markerfmt\n    self._model = LinearRegression()"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, X, y):\n    \"\"\"\n        Computes the leverage of X and uses the residuals of a\n        ``sklearn.linear_model.LinearRegression`` to compute the Cook's Distance of each\n        observation in X, their p-values and the number of outliers defined by the\n        number of observations supplied.\n\n        Parameters\n        ----------\n        X : array-like, 2D\n            The exogenous design matrix, e.g. training data.\n\n        y : array-like, 1D\n            The endogenous response variable, e.g. target data.\n\n        Returns\n        -------\n        self : CooksDistance\n            Fit returns the visualizer instance.\n        \"\"\"\n    self._model.fit(X, y)\n    leverage = (X * np.linalg.pinv(X).T).sum(1)\n    rank = np.linalg.matrix_rank(X)\n    df = X.shape[0] - rank\n    residuals = y - self._model.predict(X)\n    mse = np.dot(residuals, residuals) / df\n    residuals_studentized = residuals / np.sqrt(mse) / np.sqrt(1 - leverage)\n    self.distance_ = residuals_studentized ** 2 / X.shape[1]\n    self.distance_ *= leverage / (1 - leverage)\n    self.p_values_ = sp.stats.f.sf(self.distance_, X.shape[1], df)\n    self.influence_threshold_ = 4 / X.shape[0]\n    self.outlier_percentage_ = sum(self.distance_ > self.influence_threshold_) / X.shape[0]\n    self.outlier_percentage_ *= 100.0\n    self.draw()\n    return self",
        "mutated": [
            "def fit(self, X, y):\n    if False:\n        i = 10\n    \"\\n        Computes the leverage of X and uses the residuals of a\\n        ``sklearn.linear_model.LinearRegression`` to compute the Cook's Distance of each\\n        observation in X, their p-values and the number of outliers defined by the\\n        number of observations supplied.\\n\\n        Parameters\\n        ----------\\n        X : array-like, 2D\\n            The exogenous design matrix, e.g. training data.\\n\\n        y : array-like, 1D\\n            The endogenous response variable, e.g. target data.\\n\\n        Returns\\n        -------\\n        self : CooksDistance\\n            Fit returns the visualizer instance.\\n        \"\n    self._model.fit(X, y)\n    leverage = (X * np.linalg.pinv(X).T).sum(1)\n    rank = np.linalg.matrix_rank(X)\n    df = X.shape[0] - rank\n    residuals = y - self._model.predict(X)\n    mse = np.dot(residuals, residuals) / df\n    residuals_studentized = residuals / np.sqrt(mse) / np.sqrt(1 - leverage)\n    self.distance_ = residuals_studentized ** 2 / X.shape[1]\n    self.distance_ *= leverage / (1 - leverage)\n    self.p_values_ = sp.stats.f.sf(self.distance_, X.shape[1], df)\n    self.influence_threshold_ = 4 / X.shape[0]\n    self.outlier_percentage_ = sum(self.distance_ > self.influence_threshold_) / X.shape[0]\n    self.outlier_percentage_ *= 100.0\n    self.draw()\n    return self",
            "def fit(self, X, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Computes the leverage of X and uses the residuals of a\\n        ``sklearn.linear_model.LinearRegression`` to compute the Cook's Distance of each\\n        observation in X, their p-values and the number of outliers defined by the\\n        number of observations supplied.\\n\\n        Parameters\\n        ----------\\n        X : array-like, 2D\\n            The exogenous design matrix, e.g. training data.\\n\\n        y : array-like, 1D\\n            The endogenous response variable, e.g. target data.\\n\\n        Returns\\n        -------\\n        self : CooksDistance\\n            Fit returns the visualizer instance.\\n        \"\n    self._model.fit(X, y)\n    leverage = (X * np.linalg.pinv(X).T).sum(1)\n    rank = np.linalg.matrix_rank(X)\n    df = X.shape[0] - rank\n    residuals = y - self._model.predict(X)\n    mse = np.dot(residuals, residuals) / df\n    residuals_studentized = residuals / np.sqrt(mse) / np.sqrt(1 - leverage)\n    self.distance_ = residuals_studentized ** 2 / X.shape[1]\n    self.distance_ *= leverage / (1 - leverage)\n    self.p_values_ = sp.stats.f.sf(self.distance_, X.shape[1], df)\n    self.influence_threshold_ = 4 / X.shape[0]\n    self.outlier_percentage_ = sum(self.distance_ > self.influence_threshold_) / X.shape[0]\n    self.outlier_percentage_ *= 100.0\n    self.draw()\n    return self",
            "def fit(self, X, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Computes the leverage of X and uses the residuals of a\\n        ``sklearn.linear_model.LinearRegression`` to compute the Cook's Distance of each\\n        observation in X, their p-values and the number of outliers defined by the\\n        number of observations supplied.\\n\\n        Parameters\\n        ----------\\n        X : array-like, 2D\\n            The exogenous design matrix, e.g. training data.\\n\\n        y : array-like, 1D\\n            The endogenous response variable, e.g. target data.\\n\\n        Returns\\n        -------\\n        self : CooksDistance\\n            Fit returns the visualizer instance.\\n        \"\n    self._model.fit(X, y)\n    leverage = (X * np.linalg.pinv(X).T).sum(1)\n    rank = np.linalg.matrix_rank(X)\n    df = X.shape[0] - rank\n    residuals = y - self._model.predict(X)\n    mse = np.dot(residuals, residuals) / df\n    residuals_studentized = residuals / np.sqrt(mse) / np.sqrt(1 - leverage)\n    self.distance_ = residuals_studentized ** 2 / X.shape[1]\n    self.distance_ *= leverage / (1 - leverage)\n    self.p_values_ = sp.stats.f.sf(self.distance_, X.shape[1], df)\n    self.influence_threshold_ = 4 / X.shape[0]\n    self.outlier_percentage_ = sum(self.distance_ > self.influence_threshold_) / X.shape[0]\n    self.outlier_percentage_ *= 100.0\n    self.draw()\n    return self",
            "def fit(self, X, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Computes the leverage of X and uses the residuals of a\\n        ``sklearn.linear_model.LinearRegression`` to compute the Cook's Distance of each\\n        observation in X, their p-values and the number of outliers defined by the\\n        number of observations supplied.\\n\\n        Parameters\\n        ----------\\n        X : array-like, 2D\\n            The exogenous design matrix, e.g. training data.\\n\\n        y : array-like, 1D\\n            The endogenous response variable, e.g. target data.\\n\\n        Returns\\n        -------\\n        self : CooksDistance\\n            Fit returns the visualizer instance.\\n        \"\n    self._model.fit(X, y)\n    leverage = (X * np.linalg.pinv(X).T).sum(1)\n    rank = np.linalg.matrix_rank(X)\n    df = X.shape[0] - rank\n    residuals = y - self._model.predict(X)\n    mse = np.dot(residuals, residuals) / df\n    residuals_studentized = residuals / np.sqrt(mse) / np.sqrt(1 - leverage)\n    self.distance_ = residuals_studentized ** 2 / X.shape[1]\n    self.distance_ *= leverage / (1 - leverage)\n    self.p_values_ = sp.stats.f.sf(self.distance_, X.shape[1], df)\n    self.influence_threshold_ = 4 / X.shape[0]\n    self.outlier_percentage_ = sum(self.distance_ > self.influence_threshold_) / X.shape[0]\n    self.outlier_percentage_ *= 100.0\n    self.draw()\n    return self",
            "def fit(self, X, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Computes the leverage of X and uses the residuals of a\\n        ``sklearn.linear_model.LinearRegression`` to compute the Cook's Distance of each\\n        observation in X, their p-values and the number of outliers defined by the\\n        number of observations supplied.\\n\\n        Parameters\\n        ----------\\n        X : array-like, 2D\\n            The exogenous design matrix, e.g. training data.\\n\\n        y : array-like, 1D\\n            The endogenous response variable, e.g. target data.\\n\\n        Returns\\n        -------\\n        self : CooksDistance\\n            Fit returns the visualizer instance.\\n        \"\n    self._model.fit(X, y)\n    leverage = (X * np.linalg.pinv(X).T).sum(1)\n    rank = np.linalg.matrix_rank(X)\n    df = X.shape[0] - rank\n    residuals = y - self._model.predict(X)\n    mse = np.dot(residuals, residuals) / df\n    residuals_studentized = residuals / np.sqrt(mse) / np.sqrt(1 - leverage)\n    self.distance_ = residuals_studentized ** 2 / X.shape[1]\n    self.distance_ *= leverage / (1 - leverage)\n    self.p_values_ = sp.stats.f.sf(self.distance_, X.shape[1], df)\n    self.influence_threshold_ = 4 / X.shape[0]\n    self.outlier_percentage_ = sum(self.distance_ > self.influence_threshold_) / X.shape[0]\n    self.outlier_percentage_ *= 100.0\n    self.draw()\n    return self"
        ]
    },
    {
        "func_name": "draw",
        "original": "def draw(self):\n    \"\"\"\n        Draws a stem plot where each stem is the Cook's Distance of the instance at the\n        index specified by the x axis. Optionaly draws a threshold line.\n        \"\"\"\n    (_, _, baseline) = self.ax.stem(self.distance_, linefmt=self.linefmt, markerfmt=self.markerfmt, use_line_collection=True)\n    self.ax.set_xlim(0, len(self.distance_))\n    if self.draw_threshold:\n        label = '{:0.2f}% > $I_t$ ($I_t=\\\\frac {{4}} {{n}}$)'.format(self.outlier_percentage_)\n        self.ax.axhline(self.influence_threshold_, ls='--', label=label, c=baseline.get_color(), lw=baseline.get_linewidth())\n    return self.ax",
        "mutated": [
            "def draw(self):\n    if False:\n        i = 10\n    \"\\n        Draws a stem plot where each stem is the Cook's Distance of the instance at the\\n        index specified by the x axis. Optionaly draws a threshold line.\\n        \"\n    (_, _, baseline) = self.ax.stem(self.distance_, linefmt=self.linefmt, markerfmt=self.markerfmt, use_line_collection=True)\n    self.ax.set_xlim(0, len(self.distance_))\n    if self.draw_threshold:\n        label = '{:0.2f}% > $I_t$ ($I_t=\\\\frac {{4}} {{n}}$)'.format(self.outlier_percentage_)\n        self.ax.axhline(self.influence_threshold_, ls='--', label=label, c=baseline.get_color(), lw=baseline.get_linewidth())\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Draws a stem plot where each stem is the Cook's Distance of the instance at the\\n        index specified by the x axis. Optionaly draws a threshold line.\\n        \"\n    (_, _, baseline) = self.ax.stem(self.distance_, linefmt=self.linefmt, markerfmt=self.markerfmt, use_line_collection=True)\n    self.ax.set_xlim(0, len(self.distance_))\n    if self.draw_threshold:\n        label = '{:0.2f}% > $I_t$ ($I_t=\\\\frac {{4}} {{n}}$)'.format(self.outlier_percentage_)\n        self.ax.axhline(self.influence_threshold_, ls='--', label=label, c=baseline.get_color(), lw=baseline.get_linewidth())\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Draws a stem plot where each stem is the Cook's Distance of the instance at the\\n        index specified by the x axis. Optionaly draws a threshold line.\\n        \"\n    (_, _, baseline) = self.ax.stem(self.distance_, linefmt=self.linefmt, markerfmt=self.markerfmt, use_line_collection=True)\n    self.ax.set_xlim(0, len(self.distance_))\n    if self.draw_threshold:\n        label = '{:0.2f}% > $I_t$ ($I_t=\\\\frac {{4}} {{n}}$)'.format(self.outlier_percentage_)\n        self.ax.axhline(self.influence_threshold_, ls='--', label=label, c=baseline.get_color(), lw=baseline.get_linewidth())\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Draws a stem plot where each stem is the Cook's Distance of the instance at the\\n        index specified by the x axis. Optionaly draws a threshold line.\\n        \"\n    (_, _, baseline) = self.ax.stem(self.distance_, linefmt=self.linefmt, markerfmt=self.markerfmt, use_line_collection=True)\n    self.ax.set_xlim(0, len(self.distance_))\n    if self.draw_threshold:\n        label = '{:0.2f}% > $I_t$ ($I_t=\\\\frac {{4}} {{n}}$)'.format(self.outlier_percentage_)\n        self.ax.axhline(self.influence_threshold_, ls='--', label=label, c=baseline.get_color(), lw=baseline.get_linewidth())\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Draws a stem plot where each stem is the Cook's Distance of the instance at the\\n        index specified by the x axis. Optionaly draws a threshold line.\\n        \"\n    (_, _, baseline) = self.ax.stem(self.distance_, linefmt=self.linefmt, markerfmt=self.markerfmt, use_line_collection=True)\n    self.ax.set_xlim(0, len(self.distance_))\n    if self.draw_threshold:\n        label = '{:0.2f}% > $I_t$ ($I_t=\\\\frac {{4}} {{n}}$)'.format(self.outlier_percentage_)\n        self.ax.axhline(self.influence_threshold_, ls='--', label=label, c=baseline.get_color(), lw=baseline.get_linewidth())\n    return self.ax"
        ]
    },
    {
        "func_name": "finalize",
        "original": "def finalize(self):\n    \"\"\"\n        Prepares the visualization for presentation and reporting.\n        \"\"\"\n    self.set_title(\"Cook's Distance Outlier Detection\")\n    self.ax.set_xlabel('instance index')\n    self.ax.set_ylabel('influence (I)')\n    if self.draw_threshold:\n        self.ax.legend(loc='best', frameon=True)",
        "mutated": [
            "def finalize(self):\n    if False:\n        i = 10\n    '\\n        Prepares the visualization for presentation and reporting.\\n        '\n    self.set_title(\"Cook's Distance Outlier Detection\")\n    self.ax.set_xlabel('instance index')\n    self.ax.set_ylabel('influence (I)')\n    if self.draw_threshold:\n        self.ax.legend(loc='best', frameon=True)",
            "def finalize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Prepares the visualization for presentation and reporting.\\n        '\n    self.set_title(\"Cook's Distance Outlier Detection\")\n    self.ax.set_xlabel('instance index')\n    self.ax.set_ylabel('influence (I)')\n    if self.draw_threshold:\n        self.ax.legend(loc='best', frameon=True)",
            "def finalize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Prepares the visualization for presentation and reporting.\\n        '\n    self.set_title(\"Cook's Distance Outlier Detection\")\n    self.ax.set_xlabel('instance index')\n    self.ax.set_ylabel('influence (I)')\n    if self.draw_threshold:\n        self.ax.legend(loc='best', frameon=True)",
            "def finalize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Prepares the visualization for presentation and reporting.\\n        '\n    self.set_title(\"Cook's Distance Outlier Detection\")\n    self.ax.set_xlabel('instance index')\n    self.ax.set_ylabel('influence (I)')\n    if self.draw_threshold:\n        self.ax.legend(loc='best', frameon=True)",
            "def finalize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Prepares the visualization for presentation and reporting.\\n        '\n    self.set_title(\"Cook's Distance Outlier Detection\")\n    self.ax.set_xlabel('instance index')\n    self.ax.set_ylabel('influence (I)')\n    if self.draw_threshold:\n        self.ax.legend(loc='best', frameon=True)"
        ]
    },
    {
        "func_name": "cooks_distance",
        "original": "def cooks_distance(X, y, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', show=True, **kwargs):\n    \"\"\"\n    Cook's Distance is a measure of how influential an instance is to the computation of\n    a regression, e.g. if the instance is removed would the estimated coeficients of the\n    underlying model be substantially changed? Because of this, Cook's Distance is\n    generally used to detect outliers in standard, OLS regression. In fact, a general\n    rule of thumb is that D(i) > 4/n is a good threshold for determining highly\n    influential points as outliers and this visualizer can report the percentage of data\n    that is above that threshold.\n\n    This implementation of Cook's Distance assumes Ordinary Least Squares regression,\n    and therefore embeds a ``sklearn.linear_model.LinearRegression`` under the hood.\n    Distance is computed via the non-whitened leverage of the projection matrix,\n    computed inside of ``fit()``. The results of this visualizer are therefore similar\n    to, but not as advanced, as a similar computation using statsmodels. Computing the\n    influence for other regression models requires leave one out validation and can be\n    expensive to compute.\n\n    .. seealso::\n        For a longer discussion on detecting outliers in regression and computing\n        leverage and influence, see `linear regression in python, outliers/leverage\n        detect <http://bit.ly/2If2fga>`_ by Huiming Song.\n\n    Parameters\n    ----------\n    X : array-like, 2D\n        The exogenous design matrix, e.g. training data.\n\n    y : array-like, 1D\n        The endogenous response variable, e.g. target data.\n\n    ax : matplotlib Axes, default: None\n        The axes to plot the figure on. If None is passed in the current axes\n        will be used (or generated if required).\n\n    draw_threshold : bool, default: True\n        Draw a horizontal line at D(i) == 4/n to easily identify the most influential\n        points on the final regression. This will also draw a legend that specifies the\n        percentage of data points that are above the threshold.\n\n    linefmt : str, default: 'C0-'\n        A string defining the properties of the vertical lines of the stem plot, usually\n        this will be a color or a color and a line style. The default is simply a solid\n        line with the first color of the color cycle.\n\n    markerfmt: str, default: ','\n        A string defining the properties of the markers at the stem plot heads. The\n        default is \"pixel\", e.g. basically no marker head at the top of the stem plot.\n\n    show: bool, default: True\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however\n        you cannot call ``plt.savefig`` from this signature, nor\n        ``clear_figure``. If False, simply calls ``finalize()``\n\n    kwargs : dict\n        Keyword arguments that are passed to the base class and may influence the final\n        visualization (e.g. size or title parameters).\n    \"\"\"\n    viz = CooksDistance(ax=ax, draw_threshold=draw_threshold, linefmt=linefmt, markerfmt=markerfmt, **kwargs)\n    viz.fit(X, y)\n    if show:\n        viz.show()\n    else:\n        viz.finalize()\n    return viz",
        "mutated": [
            "def cooks_distance(X, y, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', show=True, **kwargs):\n    if False:\n        i = 10\n    '\\n    Cook\\'s Distance is a measure of how influential an instance is to the computation of\\n    a regression, e.g. if the instance is removed would the estimated coeficients of the\\n    underlying model be substantially changed? Because of this, Cook\\'s Distance is\\n    generally used to detect outliers in standard, OLS regression. In fact, a general\\n    rule of thumb is that D(i) > 4/n is a good threshold for determining highly\\n    influential points as outliers and this visualizer can report the percentage of data\\n    that is above that threshold.\\n\\n    This implementation of Cook\\'s Distance assumes Ordinary Least Squares regression,\\n    and therefore embeds a ``sklearn.linear_model.LinearRegression`` under the hood.\\n    Distance is computed via the non-whitened leverage of the projection matrix,\\n    computed inside of ``fit()``. The results of this visualizer are therefore similar\\n    to, but not as advanced, as a similar computation using statsmodels. Computing the\\n    influence for other regression models requires leave one out validation and can be\\n    expensive to compute.\\n\\n    .. seealso::\\n        For a longer discussion on detecting outliers in regression and computing\\n        leverage and influence, see `linear regression in python, outliers/leverage\\n        detect <http://bit.ly/2If2fga>`_ by Huiming Song.\\n\\n    Parameters\\n    ----------\\n    X : array-like, 2D\\n        The exogenous design matrix, e.g. training data.\\n\\n    y : array-like, 1D\\n        The endogenous response variable, e.g. target data.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If None is passed in the current axes\\n        will be used (or generated if required).\\n\\n    draw_threshold : bool, default: True\\n        Draw a horizontal line at D(i) == 4/n to easily identify the most influential\\n        points on the final regression. This will also draw a legend that specifies the\\n        percentage of data points that are above the threshold.\\n\\n    linefmt : str, default: \\'C0-\\'\\n        A string defining the properties of the vertical lines of the stem plot, usually\\n        this will be a color or a color and a line style. The default is simply a solid\\n        line with the first color of the color cycle.\\n\\n    markerfmt: str, default: \\',\\'\\n        A string defining the properties of the markers at the stem plot heads. The\\n        default is \"pixel\", e.g. basically no marker head at the top of the stem plot.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however\\n        you cannot call ``plt.savefig`` from this signature, nor\\n        ``clear_figure``. If False, simply calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments that are passed to the base class and may influence the final\\n        visualization (e.g. size or title parameters).\\n    '\n    viz = CooksDistance(ax=ax, draw_threshold=draw_threshold, linefmt=linefmt, markerfmt=markerfmt, **kwargs)\n    viz.fit(X, y)\n    if show:\n        viz.show()\n    else:\n        viz.finalize()\n    return viz",
            "def cooks_distance(X, y, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Cook\\'s Distance is a measure of how influential an instance is to the computation of\\n    a regression, e.g. if the instance is removed would the estimated coeficients of the\\n    underlying model be substantially changed? Because of this, Cook\\'s Distance is\\n    generally used to detect outliers in standard, OLS regression. In fact, a general\\n    rule of thumb is that D(i) > 4/n is a good threshold for determining highly\\n    influential points as outliers and this visualizer can report the percentage of data\\n    that is above that threshold.\\n\\n    This implementation of Cook\\'s Distance assumes Ordinary Least Squares regression,\\n    and therefore embeds a ``sklearn.linear_model.LinearRegression`` under the hood.\\n    Distance is computed via the non-whitened leverage of the projection matrix,\\n    computed inside of ``fit()``. The results of this visualizer are therefore similar\\n    to, but not as advanced, as a similar computation using statsmodels. Computing the\\n    influence for other regression models requires leave one out validation and can be\\n    expensive to compute.\\n\\n    .. seealso::\\n        For a longer discussion on detecting outliers in regression and computing\\n        leverage and influence, see `linear regression in python, outliers/leverage\\n        detect <http://bit.ly/2If2fga>`_ by Huiming Song.\\n\\n    Parameters\\n    ----------\\n    X : array-like, 2D\\n        The exogenous design matrix, e.g. training data.\\n\\n    y : array-like, 1D\\n        The endogenous response variable, e.g. target data.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If None is passed in the current axes\\n        will be used (or generated if required).\\n\\n    draw_threshold : bool, default: True\\n        Draw a horizontal line at D(i) == 4/n to easily identify the most influential\\n        points on the final regression. This will also draw a legend that specifies the\\n        percentage of data points that are above the threshold.\\n\\n    linefmt : str, default: \\'C0-\\'\\n        A string defining the properties of the vertical lines of the stem plot, usually\\n        this will be a color or a color and a line style. The default is simply a solid\\n        line with the first color of the color cycle.\\n\\n    markerfmt: str, default: \\',\\'\\n        A string defining the properties of the markers at the stem plot heads. The\\n        default is \"pixel\", e.g. basically no marker head at the top of the stem plot.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however\\n        you cannot call ``plt.savefig`` from this signature, nor\\n        ``clear_figure``. If False, simply calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments that are passed to the base class and may influence the final\\n        visualization (e.g. size or title parameters).\\n    '\n    viz = CooksDistance(ax=ax, draw_threshold=draw_threshold, linefmt=linefmt, markerfmt=markerfmt, **kwargs)\n    viz.fit(X, y)\n    if show:\n        viz.show()\n    else:\n        viz.finalize()\n    return viz",
            "def cooks_distance(X, y, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Cook\\'s Distance is a measure of how influential an instance is to the computation of\\n    a regression, e.g. if the instance is removed would the estimated coeficients of the\\n    underlying model be substantially changed? Because of this, Cook\\'s Distance is\\n    generally used to detect outliers in standard, OLS regression. In fact, a general\\n    rule of thumb is that D(i) > 4/n is a good threshold for determining highly\\n    influential points as outliers and this visualizer can report the percentage of data\\n    that is above that threshold.\\n\\n    This implementation of Cook\\'s Distance assumes Ordinary Least Squares regression,\\n    and therefore embeds a ``sklearn.linear_model.LinearRegression`` under the hood.\\n    Distance is computed via the non-whitened leverage of the projection matrix,\\n    computed inside of ``fit()``. The results of this visualizer are therefore similar\\n    to, but not as advanced, as a similar computation using statsmodels. Computing the\\n    influence for other regression models requires leave one out validation and can be\\n    expensive to compute.\\n\\n    .. seealso::\\n        For a longer discussion on detecting outliers in regression and computing\\n        leverage and influence, see `linear regression in python, outliers/leverage\\n        detect <http://bit.ly/2If2fga>`_ by Huiming Song.\\n\\n    Parameters\\n    ----------\\n    X : array-like, 2D\\n        The exogenous design matrix, e.g. training data.\\n\\n    y : array-like, 1D\\n        The endogenous response variable, e.g. target data.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If None is passed in the current axes\\n        will be used (or generated if required).\\n\\n    draw_threshold : bool, default: True\\n        Draw a horizontal line at D(i) == 4/n to easily identify the most influential\\n        points on the final regression. This will also draw a legend that specifies the\\n        percentage of data points that are above the threshold.\\n\\n    linefmt : str, default: \\'C0-\\'\\n        A string defining the properties of the vertical lines of the stem plot, usually\\n        this will be a color or a color and a line style. The default is simply a solid\\n        line with the first color of the color cycle.\\n\\n    markerfmt: str, default: \\',\\'\\n        A string defining the properties of the markers at the stem plot heads. The\\n        default is \"pixel\", e.g. basically no marker head at the top of the stem plot.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however\\n        you cannot call ``plt.savefig`` from this signature, nor\\n        ``clear_figure``. If False, simply calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments that are passed to the base class and may influence the final\\n        visualization (e.g. size or title parameters).\\n    '\n    viz = CooksDistance(ax=ax, draw_threshold=draw_threshold, linefmt=linefmt, markerfmt=markerfmt, **kwargs)\n    viz.fit(X, y)\n    if show:\n        viz.show()\n    else:\n        viz.finalize()\n    return viz",
            "def cooks_distance(X, y, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Cook\\'s Distance is a measure of how influential an instance is to the computation of\\n    a regression, e.g. if the instance is removed would the estimated coeficients of the\\n    underlying model be substantially changed? Because of this, Cook\\'s Distance is\\n    generally used to detect outliers in standard, OLS regression. In fact, a general\\n    rule of thumb is that D(i) > 4/n is a good threshold for determining highly\\n    influential points as outliers and this visualizer can report the percentage of data\\n    that is above that threshold.\\n\\n    This implementation of Cook\\'s Distance assumes Ordinary Least Squares regression,\\n    and therefore embeds a ``sklearn.linear_model.LinearRegression`` under the hood.\\n    Distance is computed via the non-whitened leverage of the projection matrix,\\n    computed inside of ``fit()``. The results of this visualizer are therefore similar\\n    to, but not as advanced, as a similar computation using statsmodels. Computing the\\n    influence for other regression models requires leave one out validation and can be\\n    expensive to compute.\\n\\n    .. seealso::\\n        For a longer discussion on detecting outliers in regression and computing\\n        leverage and influence, see `linear regression in python, outliers/leverage\\n        detect <http://bit.ly/2If2fga>`_ by Huiming Song.\\n\\n    Parameters\\n    ----------\\n    X : array-like, 2D\\n        The exogenous design matrix, e.g. training data.\\n\\n    y : array-like, 1D\\n        The endogenous response variable, e.g. target data.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If None is passed in the current axes\\n        will be used (or generated if required).\\n\\n    draw_threshold : bool, default: True\\n        Draw a horizontal line at D(i) == 4/n to easily identify the most influential\\n        points on the final regression. This will also draw a legend that specifies the\\n        percentage of data points that are above the threshold.\\n\\n    linefmt : str, default: \\'C0-\\'\\n        A string defining the properties of the vertical lines of the stem plot, usually\\n        this will be a color or a color and a line style. The default is simply a solid\\n        line with the first color of the color cycle.\\n\\n    markerfmt: str, default: \\',\\'\\n        A string defining the properties of the markers at the stem plot heads. The\\n        default is \"pixel\", e.g. basically no marker head at the top of the stem plot.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however\\n        you cannot call ``plt.savefig`` from this signature, nor\\n        ``clear_figure``. If False, simply calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments that are passed to the base class and may influence the final\\n        visualization (e.g. size or title parameters).\\n    '\n    viz = CooksDistance(ax=ax, draw_threshold=draw_threshold, linefmt=linefmt, markerfmt=markerfmt, **kwargs)\n    viz.fit(X, y)\n    if show:\n        viz.show()\n    else:\n        viz.finalize()\n    return viz",
            "def cooks_distance(X, y, ax=None, draw_threshold=True, linefmt='C0-', markerfmt=',', show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Cook\\'s Distance is a measure of how influential an instance is to the computation of\\n    a regression, e.g. if the instance is removed would the estimated coeficients of the\\n    underlying model be substantially changed? Because of this, Cook\\'s Distance is\\n    generally used to detect outliers in standard, OLS regression. In fact, a general\\n    rule of thumb is that D(i) > 4/n is a good threshold for determining highly\\n    influential points as outliers and this visualizer can report the percentage of data\\n    that is above that threshold.\\n\\n    This implementation of Cook\\'s Distance assumes Ordinary Least Squares regression,\\n    and therefore embeds a ``sklearn.linear_model.LinearRegression`` under the hood.\\n    Distance is computed via the non-whitened leverage of the projection matrix,\\n    computed inside of ``fit()``. The results of this visualizer are therefore similar\\n    to, but not as advanced, as a similar computation using statsmodels. Computing the\\n    influence for other regression models requires leave one out validation and can be\\n    expensive to compute.\\n\\n    .. seealso::\\n        For a longer discussion on detecting outliers in regression and computing\\n        leverage and influence, see `linear regression in python, outliers/leverage\\n        detect <http://bit.ly/2If2fga>`_ by Huiming Song.\\n\\n    Parameters\\n    ----------\\n    X : array-like, 2D\\n        The exogenous design matrix, e.g. training data.\\n\\n    y : array-like, 1D\\n        The endogenous response variable, e.g. target data.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If None is passed in the current axes\\n        will be used (or generated if required).\\n\\n    draw_threshold : bool, default: True\\n        Draw a horizontal line at D(i) == 4/n to easily identify the most influential\\n        points on the final regression. This will also draw a legend that specifies the\\n        percentage of data points that are above the threshold.\\n\\n    linefmt : str, default: \\'C0-\\'\\n        A string defining the properties of the vertical lines of the stem plot, usually\\n        this will be a color or a color and a line style. The default is simply a solid\\n        line with the first color of the color cycle.\\n\\n    markerfmt: str, default: \\',\\'\\n        A string defining the properties of the markers at the stem plot heads. The\\n        default is \"pixel\", e.g. basically no marker head at the top of the stem plot.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however\\n        you cannot call ``plt.savefig`` from this signature, nor\\n        ``clear_figure``. If False, simply calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments that are passed to the base class and may influence the final\\n        visualization (e.g. size or title parameters).\\n    '\n    viz = CooksDistance(ax=ax, draw_threshold=draw_threshold, linefmt=linefmt, markerfmt=markerfmt, **kwargs)\n    viz.fit(X, y)\n    if show:\n        viz.show()\n    else:\n        viz.finalize()\n    return viz"
        ]
    }
]