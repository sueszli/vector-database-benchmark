[
    {
        "func_name": "__init__",
        "original": "def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, keep_order=False):\n    super().__init__()\n    self.n_token = n_token\n    self.d_embed = d_embed\n    self.d_proj = d_proj\n    self.cutoffs = cutoffs + [n_token]\n    self.cutoff_ends = [0] + self.cutoffs\n    self.div_val = div_val\n    self.shortlist_size = self.cutoffs[0]\n    self.n_clusters = len(self.cutoffs) - 1\n    self.head_size = self.shortlist_size + self.n_clusters\n    if self.n_clusters > 0:\n        self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n        self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n    self.out_layers = nn.ModuleList()\n    self.out_projs = nn.ParameterList()\n    if div_val == 1:\n        for i in range(len(self.cutoffs)):\n            if d_proj != d_embed:\n                self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))\n            else:\n                self.out_projs.append(None)\n        self.out_layers.append(nn.Linear(d_embed, n_token))\n    else:\n        for i in range(len(self.cutoffs)):\n            (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n            d_emb_i = d_embed // div_val ** i\n            self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)))\n            self.out_layers.append(nn.Linear(d_emb_i, r_idx - l_idx))\n    self.keep_order = keep_order",
        "mutated": [
            "def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, keep_order=False):\n    if False:\n        i = 10\n    super().__init__()\n    self.n_token = n_token\n    self.d_embed = d_embed\n    self.d_proj = d_proj\n    self.cutoffs = cutoffs + [n_token]\n    self.cutoff_ends = [0] + self.cutoffs\n    self.div_val = div_val\n    self.shortlist_size = self.cutoffs[0]\n    self.n_clusters = len(self.cutoffs) - 1\n    self.head_size = self.shortlist_size + self.n_clusters\n    if self.n_clusters > 0:\n        self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n        self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n    self.out_layers = nn.ModuleList()\n    self.out_projs = nn.ParameterList()\n    if div_val == 1:\n        for i in range(len(self.cutoffs)):\n            if d_proj != d_embed:\n                self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))\n            else:\n                self.out_projs.append(None)\n        self.out_layers.append(nn.Linear(d_embed, n_token))\n    else:\n        for i in range(len(self.cutoffs)):\n            (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n            d_emb_i = d_embed // div_val ** i\n            self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)))\n            self.out_layers.append(nn.Linear(d_emb_i, r_idx - l_idx))\n    self.keep_order = keep_order",
            "def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, keep_order=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.n_token = n_token\n    self.d_embed = d_embed\n    self.d_proj = d_proj\n    self.cutoffs = cutoffs + [n_token]\n    self.cutoff_ends = [0] + self.cutoffs\n    self.div_val = div_val\n    self.shortlist_size = self.cutoffs[0]\n    self.n_clusters = len(self.cutoffs) - 1\n    self.head_size = self.shortlist_size + self.n_clusters\n    if self.n_clusters > 0:\n        self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n        self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n    self.out_layers = nn.ModuleList()\n    self.out_projs = nn.ParameterList()\n    if div_val == 1:\n        for i in range(len(self.cutoffs)):\n            if d_proj != d_embed:\n                self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))\n            else:\n                self.out_projs.append(None)\n        self.out_layers.append(nn.Linear(d_embed, n_token))\n    else:\n        for i in range(len(self.cutoffs)):\n            (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n            d_emb_i = d_embed // div_val ** i\n            self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)))\n            self.out_layers.append(nn.Linear(d_emb_i, r_idx - l_idx))\n    self.keep_order = keep_order",
            "def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, keep_order=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.n_token = n_token\n    self.d_embed = d_embed\n    self.d_proj = d_proj\n    self.cutoffs = cutoffs + [n_token]\n    self.cutoff_ends = [0] + self.cutoffs\n    self.div_val = div_val\n    self.shortlist_size = self.cutoffs[0]\n    self.n_clusters = len(self.cutoffs) - 1\n    self.head_size = self.shortlist_size + self.n_clusters\n    if self.n_clusters > 0:\n        self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n        self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n    self.out_layers = nn.ModuleList()\n    self.out_projs = nn.ParameterList()\n    if div_val == 1:\n        for i in range(len(self.cutoffs)):\n            if d_proj != d_embed:\n                self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))\n            else:\n                self.out_projs.append(None)\n        self.out_layers.append(nn.Linear(d_embed, n_token))\n    else:\n        for i in range(len(self.cutoffs)):\n            (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n            d_emb_i = d_embed // div_val ** i\n            self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)))\n            self.out_layers.append(nn.Linear(d_emb_i, r_idx - l_idx))\n    self.keep_order = keep_order",
            "def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, keep_order=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.n_token = n_token\n    self.d_embed = d_embed\n    self.d_proj = d_proj\n    self.cutoffs = cutoffs + [n_token]\n    self.cutoff_ends = [0] + self.cutoffs\n    self.div_val = div_val\n    self.shortlist_size = self.cutoffs[0]\n    self.n_clusters = len(self.cutoffs) - 1\n    self.head_size = self.shortlist_size + self.n_clusters\n    if self.n_clusters > 0:\n        self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n        self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n    self.out_layers = nn.ModuleList()\n    self.out_projs = nn.ParameterList()\n    if div_val == 1:\n        for i in range(len(self.cutoffs)):\n            if d_proj != d_embed:\n                self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))\n            else:\n                self.out_projs.append(None)\n        self.out_layers.append(nn.Linear(d_embed, n_token))\n    else:\n        for i in range(len(self.cutoffs)):\n            (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n            d_emb_i = d_embed // div_val ** i\n            self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)))\n            self.out_layers.append(nn.Linear(d_emb_i, r_idx - l_idx))\n    self.keep_order = keep_order",
            "def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, keep_order=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.n_token = n_token\n    self.d_embed = d_embed\n    self.d_proj = d_proj\n    self.cutoffs = cutoffs + [n_token]\n    self.cutoff_ends = [0] + self.cutoffs\n    self.div_val = div_val\n    self.shortlist_size = self.cutoffs[0]\n    self.n_clusters = len(self.cutoffs) - 1\n    self.head_size = self.shortlist_size + self.n_clusters\n    if self.n_clusters > 0:\n        self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n        self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n    self.out_layers = nn.ModuleList()\n    self.out_projs = nn.ParameterList()\n    if div_val == 1:\n        for i in range(len(self.cutoffs)):\n            if d_proj != d_embed:\n                self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))\n            else:\n                self.out_projs.append(None)\n        self.out_layers.append(nn.Linear(d_embed, n_token))\n    else:\n        for i in range(len(self.cutoffs)):\n            (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n            d_emb_i = d_embed // div_val ** i\n            self.out_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)))\n            self.out_layers.append(nn.Linear(d_emb_i, r_idx - l_idx))\n    self.keep_order = keep_order"
        ]
    },
    {
        "func_name": "_compute_logit",
        "original": "def _compute_logit(self, hidden, weight, bias, proj):\n    if proj is None:\n        logit = nn.functional.linear(hidden, weight, bias=bias)\n    else:\n        proj_hid = nn.functional.linear(hidden, proj.t().contiguous())\n        logit = nn.functional.linear(proj_hid, weight, bias=bias)\n    return logit",
        "mutated": [
            "def _compute_logit(self, hidden, weight, bias, proj):\n    if False:\n        i = 10\n    if proj is None:\n        logit = nn.functional.linear(hidden, weight, bias=bias)\n    else:\n        proj_hid = nn.functional.linear(hidden, proj.t().contiguous())\n        logit = nn.functional.linear(proj_hid, weight, bias=bias)\n    return logit",
            "def _compute_logit(self, hidden, weight, bias, proj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if proj is None:\n        logit = nn.functional.linear(hidden, weight, bias=bias)\n    else:\n        proj_hid = nn.functional.linear(hidden, proj.t().contiguous())\n        logit = nn.functional.linear(proj_hid, weight, bias=bias)\n    return logit",
            "def _compute_logit(self, hidden, weight, bias, proj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if proj is None:\n        logit = nn.functional.linear(hidden, weight, bias=bias)\n    else:\n        proj_hid = nn.functional.linear(hidden, proj.t().contiguous())\n        logit = nn.functional.linear(proj_hid, weight, bias=bias)\n    return logit",
            "def _compute_logit(self, hidden, weight, bias, proj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if proj is None:\n        logit = nn.functional.linear(hidden, weight, bias=bias)\n    else:\n        proj_hid = nn.functional.linear(hidden, proj.t().contiguous())\n        logit = nn.functional.linear(proj_hid, weight, bias=bias)\n    return logit",
            "def _compute_logit(self, hidden, weight, bias, proj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if proj is None:\n        logit = nn.functional.linear(hidden, weight, bias=bias)\n    else:\n        proj_hid = nn.functional.linear(hidden, proj.t().contiguous())\n        logit = nn.functional.linear(proj_hid, weight, bias=bias)\n    return logit"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, hidden, labels=None, keep_order=False):\n    \"\"\"\n        Params:\n            hidden :: [len*bsz x d_proj]\n            labels :: [len*bsz]\n\n        Return:\n            if labels is None: out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary else: out ::\n            [(len-1)*bsz] Negative log likelihood. We could replace this implementation by the native PyTorch one if\n            theirs had an option to set bias on all clusters in the native one. here:\n            https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\n        \"\"\"\n    if labels is not None:\n        hidden = hidden[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n        hidden = hidden.view(-1, hidden.size(-1))\n        labels = labels.view(-1)\n        if hidden.size(0) != labels.size(0):\n            raise RuntimeError('Input and labels should have the same size in the batch dimension.')\n    else:\n        hidden = hidden.view(-1, hidden.size(-1))\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        if labels is not None:\n            mask = labels != -100\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n            out[mask] = -nn.functional.log_softmax(logit, dim=-1)[mask].gather(1, labels[mask].unsqueeze(1)).squeeze(1)\n        else:\n            out = nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        if labels is None:\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n        else:\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n        offset = 0\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (l_idx, r_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if labels is not None:\n                mask_i = (labels >= l_idx) & (labels < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n                if indices_i.numel() == 0:\n                    continue\n                target_i = labels.index_select(0, indices_i) - l_idx\n                head_logprob_i = head_logprob.index_select(0, indices_i)\n                hidden_i = hidden.index_select(0, indices_i)\n            else:\n                hidden_i = hidden\n            if i == 0:\n                if labels is not None:\n                    logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                cluster_prob_idx = self.cutoffs[0] + i - 1\n                if labels is not None:\n                    logprob_i = head_logprob_i[:, cluster_prob_idx] + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                    out[:, l_idx:r_idx] = logprob_i\n            if labels is not None:\n                if hasattr(self, 'keep_order') and self.keep_order or keep_order:\n                    out.index_copy_(0, indices_i, -logprob_i)\n                else:\n                    out[offset:offset + logprob_i.size(0)].copy_(-logprob_i)\n                offset += logprob_i.size(0)\n    return out",
        "mutated": [
            "def forward(self, hidden, labels=None, keep_order=False):\n    if False:\n        i = 10\n    '\\n        Params:\\n            hidden :: [len*bsz x d_proj]\\n            labels :: [len*bsz]\\n\\n        Return:\\n            if labels is None: out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary else: out ::\\n            [(len-1)*bsz] Negative log likelihood. We could replace this implementation by the native PyTorch one if\\n            theirs had an option to set bias on all clusters in the native one. here:\\n            https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\\n        '\n    if labels is not None:\n        hidden = hidden[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n        hidden = hidden.view(-1, hidden.size(-1))\n        labels = labels.view(-1)\n        if hidden.size(0) != labels.size(0):\n            raise RuntimeError('Input and labels should have the same size in the batch dimension.')\n    else:\n        hidden = hidden.view(-1, hidden.size(-1))\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        if labels is not None:\n            mask = labels != -100\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n            out[mask] = -nn.functional.log_softmax(logit, dim=-1)[mask].gather(1, labels[mask].unsqueeze(1)).squeeze(1)\n        else:\n            out = nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        if labels is None:\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n        else:\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n        offset = 0\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (l_idx, r_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if labels is not None:\n                mask_i = (labels >= l_idx) & (labels < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n                if indices_i.numel() == 0:\n                    continue\n                target_i = labels.index_select(0, indices_i) - l_idx\n                head_logprob_i = head_logprob.index_select(0, indices_i)\n                hidden_i = hidden.index_select(0, indices_i)\n            else:\n                hidden_i = hidden\n            if i == 0:\n                if labels is not None:\n                    logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                cluster_prob_idx = self.cutoffs[0] + i - 1\n                if labels is not None:\n                    logprob_i = head_logprob_i[:, cluster_prob_idx] + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                    out[:, l_idx:r_idx] = logprob_i\n            if labels is not None:\n                if hasattr(self, 'keep_order') and self.keep_order or keep_order:\n                    out.index_copy_(0, indices_i, -logprob_i)\n                else:\n                    out[offset:offset + logprob_i.size(0)].copy_(-logprob_i)\n                offset += logprob_i.size(0)\n    return out",
            "def forward(self, hidden, labels=None, keep_order=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Params:\\n            hidden :: [len*bsz x d_proj]\\n            labels :: [len*bsz]\\n\\n        Return:\\n            if labels is None: out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary else: out ::\\n            [(len-1)*bsz] Negative log likelihood. We could replace this implementation by the native PyTorch one if\\n            theirs had an option to set bias on all clusters in the native one. here:\\n            https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\\n        '\n    if labels is not None:\n        hidden = hidden[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n        hidden = hidden.view(-1, hidden.size(-1))\n        labels = labels.view(-1)\n        if hidden.size(0) != labels.size(0):\n            raise RuntimeError('Input and labels should have the same size in the batch dimension.')\n    else:\n        hidden = hidden.view(-1, hidden.size(-1))\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        if labels is not None:\n            mask = labels != -100\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n            out[mask] = -nn.functional.log_softmax(logit, dim=-1)[mask].gather(1, labels[mask].unsqueeze(1)).squeeze(1)\n        else:\n            out = nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        if labels is None:\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n        else:\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n        offset = 0\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (l_idx, r_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if labels is not None:\n                mask_i = (labels >= l_idx) & (labels < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n                if indices_i.numel() == 0:\n                    continue\n                target_i = labels.index_select(0, indices_i) - l_idx\n                head_logprob_i = head_logprob.index_select(0, indices_i)\n                hidden_i = hidden.index_select(0, indices_i)\n            else:\n                hidden_i = hidden\n            if i == 0:\n                if labels is not None:\n                    logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                cluster_prob_idx = self.cutoffs[0] + i - 1\n                if labels is not None:\n                    logprob_i = head_logprob_i[:, cluster_prob_idx] + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                    out[:, l_idx:r_idx] = logprob_i\n            if labels is not None:\n                if hasattr(self, 'keep_order') and self.keep_order or keep_order:\n                    out.index_copy_(0, indices_i, -logprob_i)\n                else:\n                    out[offset:offset + logprob_i.size(0)].copy_(-logprob_i)\n                offset += logprob_i.size(0)\n    return out",
            "def forward(self, hidden, labels=None, keep_order=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Params:\\n            hidden :: [len*bsz x d_proj]\\n            labels :: [len*bsz]\\n\\n        Return:\\n            if labels is None: out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary else: out ::\\n            [(len-1)*bsz] Negative log likelihood. We could replace this implementation by the native PyTorch one if\\n            theirs had an option to set bias on all clusters in the native one. here:\\n            https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\\n        '\n    if labels is not None:\n        hidden = hidden[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n        hidden = hidden.view(-1, hidden.size(-1))\n        labels = labels.view(-1)\n        if hidden.size(0) != labels.size(0):\n            raise RuntimeError('Input and labels should have the same size in the batch dimension.')\n    else:\n        hidden = hidden.view(-1, hidden.size(-1))\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        if labels is not None:\n            mask = labels != -100\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n            out[mask] = -nn.functional.log_softmax(logit, dim=-1)[mask].gather(1, labels[mask].unsqueeze(1)).squeeze(1)\n        else:\n            out = nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        if labels is None:\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n        else:\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n        offset = 0\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (l_idx, r_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if labels is not None:\n                mask_i = (labels >= l_idx) & (labels < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n                if indices_i.numel() == 0:\n                    continue\n                target_i = labels.index_select(0, indices_i) - l_idx\n                head_logprob_i = head_logprob.index_select(0, indices_i)\n                hidden_i = hidden.index_select(0, indices_i)\n            else:\n                hidden_i = hidden\n            if i == 0:\n                if labels is not None:\n                    logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                cluster_prob_idx = self.cutoffs[0] + i - 1\n                if labels is not None:\n                    logprob_i = head_logprob_i[:, cluster_prob_idx] + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                    out[:, l_idx:r_idx] = logprob_i\n            if labels is not None:\n                if hasattr(self, 'keep_order') and self.keep_order or keep_order:\n                    out.index_copy_(0, indices_i, -logprob_i)\n                else:\n                    out[offset:offset + logprob_i.size(0)].copy_(-logprob_i)\n                offset += logprob_i.size(0)\n    return out",
            "def forward(self, hidden, labels=None, keep_order=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Params:\\n            hidden :: [len*bsz x d_proj]\\n            labels :: [len*bsz]\\n\\n        Return:\\n            if labels is None: out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary else: out ::\\n            [(len-1)*bsz] Negative log likelihood. We could replace this implementation by the native PyTorch one if\\n            theirs had an option to set bias on all clusters in the native one. here:\\n            https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\\n        '\n    if labels is not None:\n        hidden = hidden[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n        hidden = hidden.view(-1, hidden.size(-1))\n        labels = labels.view(-1)\n        if hidden.size(0) != labels.size(0):\n            raise RuntimeError('Input and labels should have the same size in the batch dimension.')\n    else:\n        hidden = hidden.view(-1, hidden.size(-1))\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        if labels is not None:\n            mask = labels != -100\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n            out[mask] = -nn.functional.log_softmax(logit, dim=-1)[mask].gather(1, labels[mask].unsqueeze(1)).squeeze(1)\n        else:\n            out = nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        if labels is None:\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n        else:\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n        offset = 0\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (l_idx, r_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if labels is not None:\n                mask_i = (labels >= l_idx) & (labels < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n                if indices_i.numel() == 0:\n                    continue\n                target_i = labels.index_select(0, indices_i) - l_idx\n                head_logprob_i = head_logprob.index_select(0, indices_i)\n                hidden_i = hidden.index_select(0, indices_i)\n            else:\n                hidden_i = hidden\n            if i == 0:\n                if labels is not None:\n                    logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                cluster_prob_idx = self.cutoffs[0] + i - 1\n                if labels is not None:\n                    logprob_i = head_logprob_i[:, cluster_prob_idx] + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                    out[:, l_idx:r_idx] = logprob_i\n            if labels is not None:\n                if hasattr(self, 'keep_order') and self.keep_order or keep_order:\n                    out.index_copy_(0, indices_i, -logprob_i)\n                else:\n                    out[offset:offset + logprob_i.size(0)].copy_(-logprob_i)\n                offset += logprob_i.size(0)\n    return out",
            "def forward(self, hidden, labels=None, keep_order=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Params:\\n            hidden :: [len*bsz x d_proj]\\n            labels :: [len*bsz]\\n\\n        Return:\\n            if labels is None: out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary else: out ::\\n            [(len-1)*bsz] Negative log likelihood. We could replace this implementation by the native PyTorch one if\\n            theirs had an option to set bias on all clusters in the native one. here:\\n            https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\\n        '\n    if labels is not None:\n        hidden = hidden[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n        hidden = hidden.view(-1, hidden.size(-1))\n        labels = labels.view(-1)\n        if hidden.size(0) != labels.size(0):\n            raise RuntimeError('Input and labels should have the same size in the batch dimension.')\n    else:\n        hidden = hidden.view(-1, hidden.size(-1))\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        if labels is not None:\n            mask = labels != -100\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n            out[mask] = -nn.functional.log_softmax(logit, dim=-1)[mask].gather(1, labels[mask].unsqueeze(1)).squeeze(1)\n        else:\n            out = nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        if labels is None:\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n        else:\n            out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n        offset = 0\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (l_idx, r_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if labels is not None:\n                mask_i = (labels >= l_idx) & (labels < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n                if indices_i.numel() == 0:\n                    continue\n                target_i = labels.index_select(0, indices_i) - l_idx\n                head_logprob_i = head_logprob.index_select(0, indices_i)\n                hidden_i = hidden.index_select(0, indices_i)\n            else:\n                hidden_i = hidden\n            if i == 0:\n                if labels is not None:\n                    logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                cluster_prob_idx = self.cutoffs[0] + i - 1\n                if labels is not None:\n                    logprob_i = head_logprob_i[:, cluster_prob_idx] + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                else:\n                    logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                    out[:, l_idx:r_idx] = logprob_i\n            if labels is not None:\n                if hasattr(self, 'keep_order') and self.keep_order or keep_order:\n                    out.index_copy_(0, indices_i, -logprob_i)\n                else:\n                    out[offset:offset + logprob_i.size(0)].copy_(-logprob_i)\n                offset += logprob_i.size(0)\n    return out"
        ]
    },
    {
        "func_name": "log_prob",
        "original": "def log_prob(self, hidden):\n    \"\"\"\n        Computes log probabilities for all \\\\\\\\(n\\\\_classes\\\\\\\\) From:\n        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.p\n\n        Args:\n            hidden (Tensor): a minibatch of example\n\n        Returns:\n            log-probabilities of for each class \\\\\\\\(c\\\\\\\\) in range \\\\\\\\(0 <= c <= n\\\\_classes\\\\\\\\), where \\\\\\\\(n\\\\_classes\\\\\\\\) is\n            a parameter passed to `AdaptiveLogSoftmaxWithLoss` constructor. Shape:\n\n            - Input: \\\\\\\\((N, in\\\\_features)\\\\\\\\)\n            - Output: \\\\\\\\((N, n\\\\_classes)\\\\\\\\)\n        \"\"\"\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        return nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        out = hidden.new_empty((head_logit.size(0), self.n_token))\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (start_idx, stop_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if i == 0:\n                out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                logprob_i = head_logprob[:, -i] + tail_logprob_i\n                out[:, start_idx, stop_idx] = logprob_i\n        return out",
        "mutated": [
            "def log_prob(self, hidden):\n    if False:\n        i = 10\n    '\\n        Computes log probabilities for all \\\\\\\\(n\\\\_classes\\\\\\\\) From:\\n        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.p\\n\\n        Args:\\n            hidden (Tensor): a minibatch of example\\n\\n        Returns:\\n            log-probabilities of for each class \\\\\\\\(c\\\\\\\\) in range \\\\\\\\(0 <= c <= n\\\\_classes\\\\\\\\), where \\\\\\\\(n\\\\_classes\\\\\\\\) is\\n            a parameter passed to `AdaptiveLogSoftmaxWithLoss` constructor. Shape:\\n\\n            - Input: \\\\\\\\((N, in\\\\_features)\\\\\\\\)\\n            - Output: \\\\\\\\((N, n\\\\_classes)\\\\\\\\)\\n        '\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        return nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        out = hidden.new_empty((head_logit.size(0), self.n_token))\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (start_idx, stop_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if i == 0:\n                out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                logprob_i = head_logprob[:, -i] + tail_logprob_i\n                out[:, start_idx, stop_idx] = logprob_i\n        return out",
            "def log_prob(self, hidden):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Computes log probabilities for all \\\\\\\\(n\\\\_classes\\\\\\\\) From:\\n        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.p\\n\\n        Args:\\n            hidden (Tensor): a minibatch of example\\n\\n        Returns:\\n            log-probabilities of for each class \\\\\\\\(c\\\\\\\\) in range \\\\\\\\(0 <= c <= n\\\\_classes\\\\\\\\), where \\\\\\\\(n\\\\_classes\\\\\\\\) is\\n            a parameter passed to `AdaptiveLogSoftmaxWithLoss` constructor. Shape:\\n\\n            - Input: \\\\\\\\((N, in\\\\_features)\\\\\\\\)\\n            - Output: \\\\\\\\((N, n\\\\_classes)\\\\\\\\)\\n        '\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        return nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        out = hidden.new_empty((head_logit.size(0), self.n_token))\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (start_idx, stop_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if i == 0:\n                out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                logprob_i = head_logprob[:, -i] + tail_logprob_i\n                out[:, start_idx, stop_idx] = logprob_i\n        return out",
            "def log_prob(self, hidden):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Computes log probabilities for all \\\\\\\\(n\\\\_classes\\\\\\\\) From:\\n        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.p\\n\\n        Args:\\n            hidden (Tensor): a minibatch of example\\n\\n        Returns:\\n            log-probabilities of for each class \\\\\\\\(c\\\\\\\\) in range \\\\\\\\(0 <= c <= n\\\\_classes\\\\\\\\), where \\\\\\\\(n\\\\_classes\\\\\\\\) is\\n            a parameter passed to `AdaptiveLogSoftmaxWithLoss` constructor. Shape:\\n\\n            - Input: \\\\\\\\((N, in\\\\_features)\\\\\\\\)\\n            - Output: \\\\\\\\((N, n\\\\_classes)\\\\\\\\)\\n        '\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        return nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        out = hidden.new_empty((head_logit.size(0), self.n_token))\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (start_idx, stop_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if i == 0:\n                out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                logprob_i = head_logprob[:, -i] + tail_logprob_i\n                out[:, start_idx, stop_idx] = logprob_i\n        return out",
            "def log_prob(self, hidden):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Computes log probabilities for all \\\\\\\\(n\\\\_classes\\\\\\\\) From:\\n        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.p\\n\\n        Args:\\n            hidden (Tensor): a minibatch of example\\n\\n        Returns:\\n            log-probabilities of for each class \\\\\\\\(c\\\\\\\\) in range \\\\\\\\(0 <= c <= n\\\\_classes\\\\\\\\), where \\\\\\\\(n\\\\_classes\\\\\\\\) is\\n            a parameter passed to `AdaptiveLogSoftmaxWithLoss` constructor. Shape:\\n\\n            - Input: \\\\\\\\((N, in\\\\_features)\\\\\\\\)\\n            - Output: \\\\\\\\((N, n\\\\_classes)\\\\\\\\)\\n        '\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        return nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        out = hidden.new_empty((head_logit.size(0), self.n_token))\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (start_idx, stop_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if i == 0:\n                out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                logprob_i = head_logprob[:, -i] + tail_logprob_i\n                out[:, start_idx, stop_idx] = logprob_i\n        return out",
            "def log_prob(self, hidden):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Computes log probabilities for all \\\\\\\\(n\\\\_classes\\\\\\\\) From:\\n        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.p\\n\\n        Args:\\n            hidden (Tensor): a minibatch of example\\n\\n        Returns:\\n            log-probabilities of for each class \\\\\\\\(c\\\\\\\\) in range \\\\\\\\(0 <= c <= n\\\\_classes\\\\\\\\), where \\\\\\\\(n\\\\_classes\\\\\\\\) is\\n            a parameter passed to `AdaptiveLogSoftmaxWithLoss` constructor. Shape:\\n\\n            - Input: \\\\\\\\((N, in\\\\_features)\\\\\\\\)\\n            - Output: \\\\\\\\((N, n\\\\_classes)\\\\\\\\)\\n        '\n    if self.n_clusters == 0:\n        logit = self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])\n        return nn.functional.log_softmax(logit, dim=-1)\n    else:\n        (weights, biases) = ([], [])\n        for i in range(len(self.cutoffs)):\n            if self.div_val == 1:\n                (l_idx, r_idx) = (self.cutoff_ends[i], self.cutoff_ends[i + 1])\n                weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                bias_i = self.out_layers[0].bias[l_idx:r_idx]\n            else:\n                weight_i = self.out_layers[i].weight\n                bias_i = self.out_layers[i].bias\n            if i == 0:\n                weight_i = torch.cat([weight_i, self.cluster_weight], dim=0)\n                bias_i = torch.cat([bias_i, self.cluster_bias], dim=0)\n            weights.append(weight_i)\n            biases.append(bias_i)\n        (head_weight, head_bias, head_proj) = (weights[0], biases[0], self.out_projs[0])\n        head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n        out = hidden.new_empty((head_logit.size(0), self.n_token))\n        head_logprob = nn.functional.log_softmax(head_logit, dim=1)\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            (start_idx, stop_idx) = (cutoff_values[i], cutoff_values[i + 1])\n            if i == 0:\n                out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n            else:\n                (weight_i, bias_i, proj_i) = (weights[i], biases[i], self.out_projs[i])\n                tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                tail_logprob_i = nn.functional.log_softmax(tail_logit_i, dim=1)\n                logprob_i = head_logprob[:, -i] + tail_logprob_i\n                out[:, start_idx, stop_idx] = logprob_i\n        return out"
        ]
    }
]