[
    {
        "func_name": "outlier_test",
        "original": "def outlier_test(model_results, method='bonf', alpha=0.05, labels=None, order=False, cutoff=None):\n    \"\"\"\n    Outlier Tests for RegressionResults instances.\n\n    Parameters\n    ----------\n    model_results : RegressionResults\n        Linear model results\n    method : str\n        - `bonferroni` : one-step correction\n        - `sidak` : one-step correction\n        - `holm-sidak` :\n        - `holm` :\n        - `simes-hochberg` :\n        - `hommel` :\n        - `fdr_bh` : Benjamini/Hochberg\n        - `fdr_by` : Benjamini/Yekutieli\n        See `statsmodels.stats.multitest.multipletests` for details.\n    alpha : float\n        familywise error rate\n    labels : None or array_like\n        If `labels` is not None, then it will be used as index to the\n        returned pandas DataFrame. See also Returns below\n    order : bool\n        Whether or not to order the results by the absolute value of the\n        studentized residuals. If labels are provided they will also be sorted.\n    cutoff : None or float in [0, 1]\n        If cutoff is not None, then the return only includes observations with\n        multiple testing corrected p-values strictly below the cutoff. The\n        returned array or dataframe can be empty if there are no outlier\n        candidates at the specified cutoff.\n\n    Returns\n    -------\n    table : ndarray or DataFrame\n        Returns either an ndarray or a DataFrame if labels is not None.\n        Will attempt to get labels from model_results if available. The\n        columns are the Studentized residuals, the unadjusted p-value,\n        and the corrected p-value according to method.\n\n    Notes\n    -----\n    The unadjusted p-value is stats.t.sf(abs(resid), df) where\n    df = df_resid - 1.\n    \"\"\"\n    from scipy import stats\n    if labels is None:\n        labels = getattr(model_results.model.data, 'row_labels', None)\n    infl = getattr(model_results, 'get_influence', None)\n    if infl is None:\n        results = maybe_unwrap_results(model_results)\n        raise AttributeError('model_results object %s does not have a get_influence method.' % results.__class__.__name__)\n    resid = infl().resid_studentized_external\n    if order:\n        idx = np.abs(resid).argsort()[::-1]\n        resid = resid[idx]\n        if labels is not None:\n            labels = np.asarray(labels)[idx]\n    df = model_results.df_resid - 1\n    unadj_p = stats.t.sf(np.abs(resid), df) * 2\n    adj_p = multipletests(unadj_p, alpha=alpha, method=method)\n    data = np.c_[resid, unadj_p, adj_p[1]]\n    if cutoff is not None:\n        mask = data[:, -1] < cutoff\n        data = data[mask]\n    else:\n        mask = slice(None)\n    if labels is not None:\n        from pandas import DataFrame\n        return DataFrame(data, columns=['student_resid', 'unadj_p', method + '(p)'], index=np.asarray(labels)[mask])\n    return data",
        "mutated": [
            "def outlier_test(model_results, method='bonf', alpha=0.05, labels=None, order=False, cutoff=None):\n    if False:\n        i = 10\n    '\\n    Outlier Tests for RegressionResults instances.\\n\\n    Parameters\\n    ----------\\n    model_results : RegressionResults\\n        Linear model results\\n    method : str\\n        - `bonferroni` : one-step correction\\n        - `sidak` : one-step correction\\n        - `holm-sidak` :\\n        - `holm` :\\n        - `simes-hochberg` :\\n        - `hommel` :\\n        - `fdr_bh` : Benjamini/Hochberg\\n        - `fdr_by` : Benjamini/Yekutieli\\n        See `statsmodels.stats.multitest.multipletests` for details.\\n    alpha : float\\n        familywise error rate\\n    labels : None or array_like\\n        If `labels` is not None, then it will be used as index to the\\n        returned pandas DataFrame. See also Returns below\\n    order : bool\\n        Whether or not to order the results by the absolute value of the\\n        studentized residuals. If labels are provided they will also be sorted.\\n    cutoff : None or float in [0, 1]\\n        If cutoff is not None, then the return only includes observations with\\n        multiple testing corrected p-values strictly below the cutoff. The\\n        returned array or dataframe can be empty if there are no outlier\\n        candidates at the specified cutoff.\\n\\n    Returns\\n    -------\\n    table : ndarray or DataFrame\\n        Returns either an ndarray or a DataFrame if labels is not None.\\n        Will attempt to get labels from model_results if available. The\\n        columns are the Studentized residuals, the unadjusted p-value,\\n        and the corrected p-value according to method.\\n\\n    Notes\\n    -----\\n    The unadjusted p-value is stats.t.sf(abs(resid), df) where\\n    df = df_resid - 1.\\n    '\n    from scipy import stats\n    if labels is None:\n        labels = getattr(model_results.model.data, 'row_labels', None)\n    infl = getattr(model_results, 'get_influence', None)\n    if infl is None:\n        results = maybe_unwrap_results(model_results)\n        raise AttributeError('model_results object %s does not have a get_influence method.' % results.__class__.__name__)\n    resid = infl().resid_studentized_external\n    if order:\n        idx = np.abs(resid).argsort()[::-1]\n        resid = resid[idx]\n        if labels is not None:\n            labels = np.asarray(labels)[idx]\n    df = model_results.df_resid - 1\n    unadj_p = stats.t.sf(np.abs(resid), df) * 2\n    adj_p = multipletests(unadj_p, alpha=alpha, method=method)\n    data = np.c_[resid, unadj_p, adj_p[1]]\n    if cutoff is not None:\n        mask = data[:, -1] < cutoff\n        data = data[mask]\n    else:\n        mask = slice(None)\n    if labels is not None:\n        from pandas import DataFrame\n        return DataFrame(data, columns=['student_resid', 'unadj_p', method + '(p)'], index=np.asarray(labels)[mask])\n    return data",
            "def outlier_test(model_results, method='bonf', alpha=0.05, labels=None, order=False, cutoff=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Outlier Tests for RegressionResults instances.\\n\\n    Parameters\\n    ----------\\n    model_results : RegressionResults\\n        Linear model results\\n    method : str\\n        - `bonferroni` : one-step correction\\n        - `sidak` : one-step correction\\n        - `holm-sidak` :\\n        - `holm` :\\n        - `simes-hochberg` :\\n        - `hommel` :\\n        - `fdr_bh` : Benjamini/Hochberg\\n        - `fdr_by` : Benjamini/Yekutieli\\n        See `statsmodels.stats.multitest.multipletests` for details.\\n    alpha : float\\n        familywise error rate\\n    labels : None or array_like\\n        If `labels` is not None, then it will be used as index to the\\n        returned pandas DataFrame. See also Returns below\\n    order : bool\\n        Whether or not to order the results by the absolute value of the\\n        studentized residuals. If labels are provided they will also be sorted.\\n    cutoff : None or float in [0, 1]\\n        If cutoff is not None, then the return only includes observations with\\n        multiple testing corrected p-values strictly below the cutoff. The\\n        returned array or dataframe can be empty if there are no outlier\\n        candidates at the specified cutoff.\\n\\n    Returns\\n    -------\\n    table : ndarray or DataFrame\\n        Returns either an ndarray or a DataFrame if labels is not None.\\n        Will attempt to get labels from model_results if available. The\\n        columns are the Studentized residuals, the unadjusted p-value,\\n        and the corrected p-value according to method.\\n\\n    Notes\\n    -----\\n    The unadjusted p-value is stats.t.sf(abs(resid), df) where\\n    df = df_resid - 1.\\n    '\n    from scipy import stats\n    if labels is None:\n        labels = getattr(model_results.model.data, 'row_labels', None)\n    infl = getattr(model_results, 'get_influence', None)\n    if infl is None:\n        results = maybe_unwrap_results(model_results)\n        raise AttributeError('model_results object %s does not have a get_influence method.' % results.__class__.__name__)\n    resid = infl().resid_studentized_external\n    if order:\n        idx = np.abs(resid).argsort()[::-1]\n        resid = resid[idx]\n        if labels is not None:\n            labels = np.asarray(labels)[idx]\n    df = model_results.df_resid - 1\n    unadj_p = stats.t.sf(np.abs(resid), df) * 2\n    adj_p = multipletests(unadj_p, alpha=alpha, method=method)\n    data = np.c_[resid, unadj_p, adj_p[1]]\n    if cutoff is not None:\n        mask = data[:, -1] < cutoff\n        data = data[mask]\n    else:\n        mask = slice(None)\n    if labels is not None:\n        from pandas import DataFrame\n        return DataFrame(data, columns=['student_resid', 'unadj_p', method + '(p)'], index=np.asarray(labels)[mask])\n    return data",
            "def outlier_test(model_results, method='bonf', alpha=0.05, labels=None, order=False, cutoff=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Outlier Tests for RegressionResults instances.\\n\\n    Parameters\\n    ----------\\n    model_results : RegressionResults\\n        Linear model results\\n    method : str\\n        - `bonferroni` : one-step correction\\n        - `sidak` : one-step correction\\n        - `holm-sidak` :\\n        - `holm` :\\n        - `simes-hochberg` :\\n        - `hommel` :\\n        - `fdr_bh` : Benjamini/Hochberg\\n        - `fdr_by` : Benjamini/Yekutieli\\n        See `statsmodels.stats.multitest.multipletests` for details.\\n    alpha : float\\n        familywise error rate\\n    labels : None or array_like\\n        If `labels` is not None, then it will be used as index to the\\n        returned pandas DataFrame. See also Returns below\\n    order : bool\\n        Whether or not to order the results by the absolute value of the\\n        studentized residuals. If labels are provided they will also be sorted.\\n    cutoff : None or float in [0, 1]\\n        If cutoff is not None, then the return only includes observations with\\n        multiple testing corrected p-values strictly below the cutoff. The\\n        returned array or dataframe can be empty if there are no outlier\\n        candidates at the specified cutoff.\\n\\n    Returns\\n    -------\\n    table : ndarray or DataFrame\\n        Returns either an ndarray or a DataFrame if labels is not None.\\n        Will attempt to get labels from model_results if available. The\\n        columns are the Studentized residuals, the unadjusted p-value,\\n        and the corrected p-value according to method.\\n\\n    Notes\\n    -----\\n    The unadjusted p-value is stats.t.sf(abs(resid), df) where\\n    df = df_resid - 1.\\n    '\n    from scipy import stats\n    if labels is None:\n        labels = getattr(model_results.model.data, 'row_labels', None)\n    infl = getattr(model_results, 'get_influence', None)\n    if infl is None:\n        results = maybe_unwrap_results(model_results)\n        raise AttributeError('model_results object %s does not have a get_influence method.' % results.__class__.__name__)\n    resid = infl().resid_studentized_external\n    if order:\n        idx = np.abs(resid).argsort()[::-1]\n        resid = resid[idx]\n        if labels is not None:\n            labels = np.asarray(labels)[idx]\n    df = model_results.df_resid - 1\n    unadj_p = stats.t.sf(np.abs(resid), df) * 2\n    adj_p = multipletests(unadj_p, alpha=alpha, method=method)\n    data = np.c_[resid, unadj_p, adj_p[1]]\n    if cutoff is not None:\n        mask = data[:, -1] < cutoff\n        data = data[mask]\n    else:\n        mask = slice(None)\n    if labels is not None:\n        from pandas import DataFrame\n        return DataFrame(data, columns=['student_resid', 'unadj_p', method + '(p)'], index=np.asarray(labels)[mask])\n    return data",
            "def outlier_test(model_results, method='bonf', alpha=0.05, labels=None, order=False, cutoff=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Outlier Tests for RegressionResults instances.\\n\\n    Parameters\\n    ----------\\n    model_results : RegressionResults\\n        Linear model results\\n    method : str\\n        - `bonferroni` : one-step correction\\n        - `sidak` : one-step correction\\n        - `holm-sidak` :\\n        - `holm` :\\n        - `simes-hochberg` :\\n        - `hommel` :\\n        - `fdr_bh` : Benjamini/Hochberg\\n        - `fdr_by` : Benjamini/Yekutieli\\n        See `statsmodels.stats.multitest.multipletests` for details.\\n    alpha : float\\n        familywise error rate\\n    labels : None or array_like\\n        If `labels` is not None, then it will be used as index to the\\n        returned pandas DataFrame. See also Returns below\\n    order : bool\\n        Whether or not to order the results by the absolute value of the\\n        studentized residuals. If labels are provided they will also be sorted.\\n    cutoff : None or float in [0, 1]\\n        If cutoff is not None, then the return only includes observations with\\n        multiple testing corrected p-values strictly below the cutoff. The\\n        returned array or dataframe can be empty if there are no outlier\\n        candidates at the specified cutoff.\\n\\n    Returns\\n    -------\\n    table : ndarray or DataFrame\\n        Returns either an ndarray or a DataFrame if labels is not None.\\n        Will attempt to get labels from model_results if available. The\\n        columns are the Studentized residuals, the unadjusted p-value,\\n        and the corrected p-value according to method.\\n\\n    Notes\\n    -----\\n    The unadjusted p-value is stats.t.sf(abs(resid), df) where\\n    df = df_resid - 1.\\n    '\n    from scipy import stats\n    if labels is None:\n        labels = getattr(model_results.model.data, 'row_labels', None)\n    infl = getattr(model_results, 'get_influence', None)\n    if infl is None:\n        results = maybe_unwrap_results(model_results)\n        raise AttributeError('model_results object %s does not have a get_influence method.' % results.__class__.__name__)\n    resid = infl().resid_studentized_external\n    if order:\n        idx = np.abs(resid).argsort()[::-1]\n        resid = resid[idx]\n        if labels is not None:\n            labels = np.asarray(labels)[idx]\n    df = model_results.df_resid - 1\n    unadj_p = stats.t.sf(np.abs(resid), df) * 2\n    adj_p = multipletests(unadj_p, alpha=alpha, method=method)\n    data = np.c_[resid, unadj_p, adj_p[1]]\n    if cutoff is not None:\n        mask = data[:, -1] < cutoff\n        data = data[mask]\n    else:\n        mask = slice(None)\n    if labels is not None:\n        from pandas import DataFrame\n        return DataFrame(data, columns=['student_resid', 'unadj_p', method + '(p)'], index=np.asarray(labels)[mask])\n    return data",
            "def outlier_test(model_results, method='bonf', alpha=0.05, labels=None, order=False, cutoff=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Outlier Tests for RegressionResults instances.\\n\\n    Parameters\\n    ----------\\n    model_results : RegressionResults\\n        Linear model results\\n    method : str\\n        - `bonferroni` : one-step correction\\n        - `sidak` : one-step correction\\n        - `holm-sidak` :\\n        - `holm` :\\n        - `simes-hochberg` :\\n        - `hommel` :\\n        - `fdr_bh` : Benjamini/Hochberg\\n        - `fdr_by` : Benjamini/Yekutieli\\n        See `statsmodels.stats.multitest.multipletests` for details.\\n    alpha : float\\n        familywise error rate\\n    labels : None or array_like\\n        If `labels` is not None, then it will be used as index to the\\n        returned pandas DataFrame. See also Returns below\\n    order : bool\\n        Whether or not to order the results by the absolute value of the\\n        studentized residuals. If labels are provided they will also be sorted.\\n    cutoff : None or float in [0, 1]\\n        If cutoff is not None, then the return only includes observations with\\n        multiple testing corrected p-values strictly below the cutoff. The\\n        returned array or dataframe can be empty if there are no outlier\\n        candidates at the specified cutoff.\\n\\n    Returns\\n    -------\\n    table : ndarray or DataFrame\\n        Returns either an ndarray or a DataFrame if labels is not None.\\n        Will attempt to get labels from model_results if available. The\\n        columns are the Studentized residuals, the unadjusted p-value,\\n        and the corrected p-value according to method.\\n\\n    Notes\\n    -----\\n    The unadjusted p-value is stats.t.sf(abs(resid), df) where\\n    df = df_resid - 1.\\n    '\n    from scipy import stats\n    if labels is None:\n        labels = getattr(model_results.model.data, 'row_labels', None)\n    infl = getattr(model_results, 'get_influence', None)\n    if infl is None:\n        results = maybe_unwrap_results(model_results)\n        raise AttributeError('model_results object %s does not have a get_influence method.' % results.__class__.__name__)\n    resid = infl().resid_studentized_external\n    if order:\n        idx = np.abs(resid).argsort()[::-1]\n        resid = resid[idx]\n        if labels is not None:\n            labels = np.asarray(labels)[idx]\n    df = model_results.df_resid - 1\n    unadj_p = stats.t.sf(np.abs(resid), df) * 2\n    adj_p = multipletests(unadj_p, alpha=alpha, method=method)\n    data = np.c_[resid, unadj_p, adj_p[1]]\n    if cutoff is not None:\n        mask = data[:, -1] < cutoff\n        data = data[mask]\n    else:\n        mask = slice(None)\n    if labels is not None:\n        from pandas import DataFrame\n        return DataFrame(data, columns=['student_resid', 'unadj_p', method + '(p)'], index=np.asarray(labels)[mask])\n    return data"
        ]
    },
    {
        "func_name": "reset_ramsey",
        "original": "def reset_ramsey(res, degree=5):\n    \"\"\"Ramsey's RESET specification test for linear models\n\n    This is a general specification test, for additional non-linear effects\n    in a model.\n\n    Parameters\n    ----------\n    degree : int\n        Maximum power to include in the RESET test.  Powers 0 and 1 are\n        excluded, so that degree tests powers 2, ..., degree of the fitted\n        values.\n\n    Notes\n    -----\n    The test fits an auxiliary OLS regression where the design matrix, exog,\n    is augmented by powers 2 to degree of the fitted values. Then it performs\n    an F-test whether these additional terms are significant.\n\n    If the p-value of the f-test is below a threshold, e.g. 0.1, then this\n    indicates that there might be additional non-linear effects in the model\n    and that the linear model is mis-specified.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Ramsey_RESET_test\n    \"\"\"\n    order = degree + 1\n    k_vars = res.model.exog.shape[1]\n    norm_values = np.asarray(res.fittedvalues)\n    norm_values = norm_values / np.sqrt((norm_values ** 2).mean())\n    y_fitted_vander = np.vander(norm_values, order)[:, :-2]\n    exog = np.column_stack((res.model.exog, y_fitted_vander))\n    exog /= np.sqrt((exog ** 2).mean(0))\n    endog = res.model.endog / (res.model.endog ** 2).mean()\n    res_aux = OLS(endog, exog).fit()\n    r_matrix = np.eye(degree - 1, exog.shape[1], k_vars)\n    return res_aux.f_test(r_matrix)",
        "mutated": [
            "def reset_ramsey(res, degree=5):\n    if False:\n        i = 10\n    \"Ramsey's RESET specification test for linear models\\n\\n    This is a general specification test, for additional non-linear effects\\n    in a model.\\n\\n    Parameters\\n    ----------\\n    degree : int\\n        Maximum power to include in the RESET test.  Powers 0 and 1 are\\n        excluded, so that degree tests powers 2, ..., degree of the fitted\\n        values.\\n\\n    Notes\\n    -----\\n    The test fits an auxiliary OLS regression where the design matrix, exog,\\n    is augmented by powers 2 to degree of the fitted values. Then it performs\\n    an F-test whether these additional terms are significant.\\n\\n    If the p-value of the f-test is below a threshold, e.g. 0.1, then this\\n    indicates that there might be additional non-linear effects in the model\\n    and that the linear model is mis-specified.\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Ramsey_RESET_test\\n    \"\n    order = degree + 1\n    k_vars = res.model.exog.shape[1]\n    norm_values = np.asarray(res.fittedvalues)\n    norm_values = norm_values / np.sqrt((norm_values ** 2).mean())\n    y_fitted_vander = np.vander(norm_values, order)[:, :-2]\n    exog = np.column_stack((res.model.exog, y_fitted_vander))\n    exog /= np.sqrt((exog ** 2).mean(0))\n    endog = res.model.endog / (res.model.endog ** 2).mean()\n    res_aux = OLS(endog, exog).fit()\n    r_matrix = np.eye(degree - 1, exog.shape[1], k_vars)\n    return res_aux.f_test(r_matrix)",
            "def reset_ramsey(res, degree=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Ramsey's RESET specification test for linear models\\n\\n    This is a general specification test, for additional non-linear effects\\n    in a model.\\n\\n    Parameters\\n    ----------\\n    degree : int\\n        Maximum power to include in the RESET test.  Powers 0 and 1 are\\n        excluded, so that degree tests powers 2, ..., degree of the fitted\\n        values.\\n\\n    Notes\\n    -----\\n    The test fits an auxiliary OLS regression where the design matrix, exog,\\n    is augmented by powers 2 to degree of the fitted values. Then it performs\\n    an F-test whether these additional terms are significant.\\n\\n    If the p-value of the f-test is below a threshold, e.g. 0.1, then this\\n    indicates that there might be additional non-linear effects in the model\\n    and that the linear model is mis-specified.\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Ramsey_RESET_test\\n    \"\n    order = degree + 1\n    k_vars = res.model.exog.shape[1]\n    norm_values = np.asarray(res.fittedvalues)\n    norm_values = norm_values / np.sqrt((norm_values ** 2).mean())\n    y_fitted_vander = np.vander(norm_values, order)[:, :-2]\n    exog = np.column_stack((res.model.exog, y_fitted_vander))\n    exog /= np.sqrt((exog ** 2).mean(0))\n    endog = res.model.endog / (res.model.endog ** 2).mean()\n    res_aux = OLS(endog, exog).fit()\n    r_matrix = np.eye(degree - 1, exog.shape[1], k_vars)\n    return res_aux.f_test(r_matrix)",
            "def reset_ramsey(res, degree=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Ramsey's RESET specification test for linear models\\n\\n    This is a general specification test, for additional non-linear effects\\n    in a model.\\n\\n    Parameters\\n    ----------\\n    degree : int\\n        Maximum power to include in the RESET test.  Powers 0 and 1 are\\n        excluded, so that degree tests powers 2, ..., degree of the fitted\\n        values.\\n\\n    Notes\\n    -----\\n    The test fits an auxiliary OLS regression where the design matrix, exog,\\n    is augmented by powers 2 to degree of the fitted values. Then it performs\\n    an F-test whether these additional terms are significant.\\n\\n    If the p-value of the f-test is below a threshold, e.g. 0.1, then this\\n    indicates that there might be additional non-linear effects in the model\\n    and that the linear model is mis-specified.\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Ramsey_RESET_test\\n    \"\n    order = degree + 1\n    k_vars = res.model.exog.shape[1]\n    norm_values = np.asarray(res.fittedvalues)\n    norm_values = norm_values / np.sqrt((norm_values ** 2).mean())\n    y_fitted_vander = np.vander(norm_values, order)[:, :-2]\n    exog = np.column_stack((res.model.exog, y_fitted_vander))\n    exog /= np.sqrt((exog ** 2).mean(0))\n    endog = res.model.endog / (res.model.endog ** 2).mean()\n    res_aux = OLS(endog, exog).fit()\n    r_matrix = np.eye(degree - 1, exog.shape[1], k_vars)\n    return res_aux.f_test(r_matrix)",
            "def reset_ramsey(res, degree=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Ramsey's RESET specification test for linear models\\n\\n    This is a general specification test, for additional non-linear effects\\n    in a model.\\n\\n    Parameters\\n    ----------\\n    degree : int\\n        Maximum power to include in the RESET test.  Powers 0 and 1 are\\n        excluded, so that degree tests powers 2, ..., degree of the fitted\\n        values.\\n\\n    Notes\\n    -----\\n    The test fits an auxiliary OLS regression where the design matrix, exog,\\n    is augmented by powers 2 to degree of the fitted values. Then it performs\\n    an F-test whether these additional terms are significant.\\n\\n    If the p-value of the f-test is below a threshold, e.g. 0.1, then this\\n    indicates that there might be additional non-linear effects in the model\\n    and that the linear model is mis-specified.\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Ramsey_RESET_test\\n    \"\n    order = degree + 1\n    k_vars = res.model.exog.shape[1]\n    norm_values = np.asarray(res.fittedvalues)\n    norm_values = norm_values / np.sqrt((norm_values ** 2).mean())\n    y_fitted_vander = np.vander(norm_values, order)[:, :-2]\n    exog = np.column_stack((res.model.exog, y_fitted_vander))\n    exog /= np.sqrt((exog ** 2).mean(0))\n    endog = res.model.endog / (res.model.endog ** 2).mean()\n    res_aux = OLS(endog, exog).fit()\n    r_matrix = np.eye(degree - 1, exog.shape[1], k_vars)\n    return res_aux.f_test(r_matrix)",
            "def reset_ramsey(res, degree=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Ramsey's RESET specification test for linear models\\n\\n    This is a general specification test, for additional non-linear effects\\n    in a model.\\n\\n    Parameters\\n    ----------\\n    degree : int\\n        Maximum power to include in the RESET test.  Powers 0 and 1 are\\n        excluded, so that degree tests powers 2, ..., degree of the fitted\\n        values.\\n\\n    Notes\\n    -----\\n    The test fits an auxiliary OLS regression where the design matrix, exog,\\n    is augmented by powers 2 to degree of the fitted values. Then it performs\\n    an F-test whether these additional terms are significant.\\n\\n    If the p-value of the f-test is below a threshold, e.g. 0.1, then this\\n    indicates that there might be additional non-linear effects in the model\\n    and that the linear model is mis-specified.\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Ramsey_RESET_test\\n    \"\n    order = degree + 1\n    k_vars = res.model.exog.shape[1]\n    norm_values = np.asarray(res.fittedvalues)\n    norm_values = norm_values / np.sqrt((norm_values ** 2).mean())\n    y_fitted_vander = np.vander(norm_values, order)[:, :-2]\n    exog = np.column_stack((res.model.exog, y_fitted_vander))\n    exog /= np.sqrt((exog ** 2).mean(0))\n    endog = res.model.endog / (res.model.endog ** 2).mean()\n    res_aux = OLS(endog, exog).fit()\n    r_matrix = np.eye(degree - 1, exog.shape[1], k_vars)\n    return res_aux.f_test(r_matrix)"
        ]
    },
    {
        "func_name": "variance_inflation_factor",
        "original": "def variance_inflation_factor(exog, exog_idx):\n    \"\"\"\n    Variance inflation factor, VIF, for one exogenous variable\n\n    The variance inflation factor is a measure for the increase of the\n    variance of the parameter estimates if an additional variable, given by\n    exog_idx is added to the linear regression. It is a measure for\n    multicollinearity of the design matrix, exog.\n\n    One recommendation is that if VIF is greater than 5, then the explanatory\n    variable given by exog_idx is highly collinear with the other explanatory\n    variables, and the parameter estimates will have large standard errors\n    because of this.\n\n    Parameters\n    ----------\n    exog : {ndarray, DataFrame}\n        design matrix with all explanatory variables, as for example used in\n        regression\n    exog_idx : int\n        index of the exogenous variable in the columns of exog\n\n    Returns\n    -------\n    float\n        variance inflation factor\n\n    Notes\n    -----\n    This function does not save the auxiliary regression.\n\n    See Also\n    --------\n    xxx : class for regression diagnostics  TODO: does not exist yet\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Variance_inflation_factor\n    \"\"\"\n    k_vars = exog.shape[1]\n    exog = np.asarray(exog)\n    x_i = exog[:, exog_idx]\n    mask = np.arange(k_vars) != exog_idx\n    x_noti = exog[:, mask]\n    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n    vif = 1.0 / (1.0 - r_squared_i)\n    return vif",
        "mutated": [
            "def variance_inflation_factor(exog, exog_idx):\n    if False:\n        i = 10\n    '\\n    Variance inflation factor, VIF, for one exogenous variable\\n\\n    The variance inflation factor is a measure for the increase of the\\n    variance of the parameter estimates if an additional variable, given by\\n    exog_idx is added to the linear regression. It is a measure for\\n    multicollinearity of the design matrix, exog.\\n\\n    One recommendation is that if VIF is greater than 5, then the explanatory\\n    variable given by exog_idx is highly collinear with the other explanatory\\n    variables, and the parameter estimates will have large standard errors\\n    because of this.\\n\\n    Parameters\\n    ----------\\n    exog : {ndarray, DataFrame}\\n        design matrix with all explanatory variables, as for example used in\\n        regression\\n    exog_idx : int\\n        index of the exogenous variable in the columns of exog\\n\\n    Returns\\n    -------\\n    float\\n        variance inflation factor\\n\\n    Notes\\n    -----\\n    This function does not save the auxiliary regression.\\n\\n    See Also\\n    --------\\n    xxx : class for regression diagnostics  TODO: does not exist yet\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Variance_inflation_factor\\n    '\n    k_vars = exog.shape[1]\n    exog = np.asarray(exog)\n    x_i = exog[:, exog_idx]\n    mask = np.arange(k_vars) != exog_idx\n    x_noti = exog[:, mask]\n    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n    vif = 1.0 / (1.0 - r_squared_i)\n    return vif",
            "def variance_inflation_factor(exog, exog_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Variance inflation factor, VIF, for one exogenous variable\\n\\n    The variance inflation factor is a measure for the increase of the\\n    variance of the parameter estimates if an additional variable, given by\\n    exog_idx is added to the linear regression. It is a measure for\\n    multicollinearity of the design matrix, exog.\\n\\n    One recommendation is that if VIF is greater than 5, then the explanatory\\n    variable given by exog_idx is highly collinear with the other explanatory\\n    variables, and the parameter estimates will have large standard errors\\n    because of this.\\n\\n    Parameters\\n    ----------\\n    exog : {ndarray, DataFrame}\\n        design matrix with all explanatory variables, as for example used in\\n        regression\\n    exog_idx : int\\n        index of the exogenous variable in the columns of exog\\n\\n    Returns\\n    -------\\n    float\\n        variance inflation factor\\n\\n    Notes\\n    -----\\n    This function does not save the auxiliary regression.\\n\\n    See Also\\n    --------\\n    xxx : class for regression diagnostics  TODO: does not exist yet\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Variance_inflation_factor\\n    '\n    k_vars = exog.shape[1]\n    exog = np.asarray(exog)\n    x_i = exog[:, exog_idx]\n    mask = np.arange(k_vars) != exog_idx\n    x_noti = exog[:, mask]\n    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n    vif = 1.0 / (1.0 - r_squared_i)\n    return vif",
            "def variance_inflation_factor(exog, exog_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Variance inflation factor, VIF, for one exogenous variable\\n\\n    The variance inflation factor is a measure for the increase of the\\n    variance of the parameter estimates if an additional variable, given by\\n    exog_idx is added to the linear regression. It is a measure for\\n    multicollinearity of the design matrix, exog.\\n\\n    One recommendation is that if VIF is greater than 5, then the explanatory\\n    variable given by exog_idx is highly collinear with the other explanatory\\n    variables, and the parameter estimates will have large standard errors\\n    because of this.\\n\\n    Parameters\\n    ----------\\n    exog : {ndarray, DataFrame}\\n        design matrix with all explanatory variables, as for example used in\\n        regression\\n    exog_idx : int\\n        index of the exogenous variable in the columns of exog\\n\\n    Returns\\n    -------\\n    float\\n        variance inflation factor\\n\\n    Notes\\n    -----\\n    This function does not save the auxiliary regression.\\n\\n    See Also\\n    --------\\n    xxx : class for regression diagnostics  TODO: does not exist yet\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Variance_inflation_factor\\n    '\n    k_vars = exog.shape[1]\n    exog = np.asarray(exog)\n    x_i = exog[:, exog_idx]\n    mask = np.arange(k_vars) != exog_idx\n    x_noti = exog[:, mask]\n    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n    vif = 1.0 / (1.0 - r_squared_i)\n    return vif",
            "def variance_inflation_factor(exog, exog_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Variance inflation factor, VIF, for one exogenous variable\\n\\n    The variance inflation factor is a measure for the increase of the\\n    variance of the parameter estimates if an additional variable, given by\\n    exog_idx is added to the linear regression. It is a measure for\\n    multicollinearity of the design matrix, exog.\\n\\n    One recommendation is that if VIF is greater than 5, then the explanatory\\n    variable given by exog_idx is highly collinear with the other explanatory\\n    variables, and the parameter estimates will have large standard errors\\n    because of this.\\n\\n    Parameters\\n    ----------\\n    exog : {ndarray, DataFrame}\\n        design matrix with all explanatory variables, as for example used in\\n        regression\\n    exog_idx : int\\n        index of the exogenous variable in the columns of exog\\n\\n    Returns\\n    -------\\n    float\\n        variance inflation factor\\n\\n    Notes\\n    -----\\n    This function does not save the auxiliary regression.\\n\\n    See Also\\n    --------\\n    xxx : class for regression diagnostics  TODO: does not exist yet\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Variance_inflation_factor\\n    '\n    k_vars = exog.shape[1]\n    exog = np.asarray(exog)\n    x_i = exog[:, exog_idx]\n    mask = np.arange(k_vars) != exog_idx\n    x_noti = exog[:, mask]\n    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n    vif = 1.0 / (1.0 - r_squared_i)\n    return vif",
            "def variance_inflation_factor(exog, exog_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Variance inflation factor, VIF, for one exogenous variable\\n\\n    The variance inflation factor is a measure for the increase of the\\n    variance of the parameter estimates if an additional variable, given by\\n    exog_idx is added to the linear regression. It is a measure for\\n    multicollinearity of the design matrix, exog.\\n\\n    One recommendation is that if VIF is greater than 5, then the explanatory\\n    variable given by exog_idx is highly collinear with the other explanatory\\n    variables, and the parameter estimates will have large standard errors\\n    because of this.\\n\\n    Parameters\\n    ----------\\n    exog : {ndarray, DataFrame}\\n        design matrix with all explanatory variables, as for example used in\\n        regression\\n    exog_idx : int\\n        index of the exogenous variable in the columns of exog\\n\\n    Returns\\n    -------\\n    float\\n        variance inflation factor\\n\\n    Notes\\n    -----\\n    This function does not save the auxiliary regression.\\n\\n    See Also\\n    --------\\n    xxx : class for regression diagnostics  TODO: does not exist yet\\n\\n    References\\n    ----------\\n    https://en.wikipedia.org/wiki/Variance_inflation_factor\\n    '\n    k_vars = exog.shape[1]\n    exog = np.asarray(exog)\n    x_i = exog[:, exog_idx]\n    mask = np.arange(k_vars) != exog_idx\n    x_noti = exog[:, mask]\n    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n    vif = 1.0 / (1.0 - r_squared_i)\n    return vif"
        ]
    },
    {
        "func_name": "plot_influence",
        "original": "@Appender(_plot_influence_doc.format(**{'extra_params_doc': ''}))\ndef plot_influence(self, external=None, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if external is None:\n        external = hasattr(self, '_cache') and 'res_looo' in self._cache\n    from statsmodels.graphics.regressionplots import _influence_plot\n    if self.hat_matrix_diag is not None:\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    else:\n        warnings.warn('Plot uses pearson residuals and exog hat matrix.')\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, leverage=self.hat_matrix_exog_diag, resid=self.resid, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
        "mutated": [
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': ''}))\ndef plot_influence(self, external=None, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n    if external is None:\n        external = hasattr(self, '_cache') and 'res_looo' in self._cache\n    from statsmodels.graphics.regressionplots import _influence_plot\n    if self.hat_matrix_diag is not None:\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    else:\n        warnings.warn('Plot uses pearson residuals and exog hat matrix.')\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, leverage=self.hat_matrix_exog_diag, resid=self.resid, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': ''}))\ndef plot_influence(self, external=None, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if external is None:\n        external = hasattr(self, '_cache') and 'res_looo' in self._cache\n    from statsmodels.graphics.regressionplots import _influence_plot\n    if self.hat_matrix_diag is not None:\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    else:\n        warnings.warn('Plot uses pearson residuals and exog hat matrix.')\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, leverage=self.hat_matrix_exog_diag, resid=self.resid, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': ''}))\ndef plot_influence(self, external=None, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if external is None:\n        external = hasattr(self, '_cache') and 'res_looo' in self._cache\n    from statsmodels.graphics.regressionplots import _influence_plot\n    if self.hat_matrix_diag is not None:\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    else:\n        warnings.warn('Plot uses pearson residuals and exog hat matrix.')\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, leverage=self.hat_matrix_exog_diag, resid=self.resid, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': ''}))\ndef plot_influence(self, external=None, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if external is None:\n        external = hasattr(self, '_cache') and 'res_looo' in self._cache\n    from statsmodels.graphics.regressionplots import _influence_plot\n    if self.hat_matrix_diag is not None:\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    else:\n        warnings.warn('Plot uses pearson residuals and exog hat matrix.')\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, leverage=self.hat_matrix_exog_diag, resid=self.resid, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': ''}))\ndef plot_influence(self, external=None, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if external is None:\n        external = hasattr(self, '_cache') and 'res_looo' in self._cache\n    from statsmodels.graphics.regressionplots import _influence_plot\n    if self.hat_matrix_diag is not None:\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    else:\n        warnings.warn('Plot uses pearson residuals and exog hat matrix.')\n        res = _influence_plot(self.results, self, external=external, alpha=alpha, criterion=criterion, size=size, leverage=self.hat_matrix_exog_diag, resid=self.resid, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res"
        ]
    },
    {
        "func_name": "_plot_index",
        "original": "def _plot_index(self, y, ylabel, threshold=None, title=None, ax=None, **kwds):\n    from statsmodels.graphics import utils\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if title is None:\n        title = 'Index Plot'\n    nobs = len(self.endog)\n    index = np.arange(nobs)\n    ax.scatter(index, y, **kwds)\n    if threshold == 'all':\n        large_points = np.ones(nobs, np.bool_)\n    else:\n        large_points = np.abs(y) > threshold\n    psize = 3 * np.ones(nobs)\n    labels = self.results.model.data.row_labels\n    if labels is None:\n        labels = np.arange(nobs)\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(index, y), lzip(-psize, psize), 'large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Observation', **font)\n    ax.set_title(title, **font)\n    return fig",
        "mutated": [
            "def _plot_index(self, y, ylabel, threshold=None, title=None, ax=None, **kwds):\n    if False:\n        i = 10\n    from statsmodels.graphics import utils\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if title is None:\n        title = 'Index Plot'\n    nobs = len(self.endog)\n    index = np.arange(nobs)\n    ax.scatter(index, y, **kwds)\n    if threshold == 'all':\n        large_points = np.ones(nobs, np.bool_)\n    else:\n        large_points = np.abs(y) > threshold\n    psize = 3 * np.ones(nobs)\n    labels = self.results.model.data.row_labels\n    if labels is None:\n        labels = np.arange(nobs)\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(index, y), lzip(-psize, psize), 'large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Observation', **font)\n    ax.set_title(title, **font)\n    return fig",
            "def _plot_index(self, y, ylabel, threshold=None, title=None, ax=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from statsmodels.graphics import utils\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if title is None:\n        title = 'Index Plot'\n    nobs = len(self.endog)\n    index = np.arange(nobs)\n    ax.scatter(index, y, **kwds)\n    if threshold == 'all':\n        large_points = np.ones(nobs, np.bool_)\n    else:\n        large_points = np.abs(y) > threshold\n    psize = 3 * np.ones(nobs)\n    labels = self.results.model.data.row_labels\n    if labels is None:\n        labels = np.arange(nobs)\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(index, y), lzip(-psize, psize), 'large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Observation', **font)\n    ax.set_title(title, **font)\n    return fig",
            "def _plot_index(self, y, ylabel, threshold=None, title=None, ax=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from statsmodels.graphics import utils\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if title is None:\n        title = 'Index Plot'\n    nobs = len(self.endog)\n    index = np.arange(nobs)\n    ax.scatter(index, y, **kwds)\n    if threshold == 'all':\n        large_points = np.ones(nobs, np.bool_)\n    else:\n        large_points = np.abs(y) > threshold\n    psize = 3 * np.ones(nobs)\n    labels = self.results.model.data.row_labels\n    if labels is None:\n        labels = np.arange(nobs)\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(index, y), lzip(-psize, psize), 'large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Observation', **font)\n    ax.set_title(title, **font)\n    return fig",
            "def _plot_index(self, y, ylabel, threshold=None, title=None, ax=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from statsmodels.graphics import utils\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if title is None:\n        title = 'Index Plot'\n    nobs = len(self.endog)\n    index = np.arange(nobs)\n    ax.scatter(index, y, **kwds)\n    if threshold == 'all':\n        large_points = np.ones(nobs, np.bool_)\n    else:\n        large_points = np.abs(y) > threshold\n    psize = 3 * np.ones(nobs)\n    labels = self.results.model.data.row_labels\n    if labels is None:\n        labels = np.arange(nobs)\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(index, y), lzip(-psize, psize), 'large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Observation', **font)\n    ax.set_title(title, **font)\n    return fig",
            "def _plot_index(self, y, ylabel, threshold=None, title=None, ax=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from statsmodels.graphics import utils\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if title is None:\n        title = 'Index Plot'\n    nobs = len(self.endog)\n    index = np.arange(nobs)\n    ax.scatter(index, y, **kwds)\n    if threshold == 'all':\n        large_points = np.ones(nobs, np.bool_)\n    else:\n        large_points = np.abs(y) > threshold\n    psize = 3 * np.ones(nobs)\n    labels = self.results.model.data.row_labels\n    if labels is None:\n        labels = np.arange(nobs)\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(index, y), lzip(-psize, psize), 'large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Observation', **font)\n    ax.set_title(title, **font)\n    return fig"
        ]
    },
    {
        "func_name": "plot_index",
        "original": "def plot_index(self, y_var='cooks', threshold=None, title=None, ax=None, idx=None, **kwds):\n    \"\"\"index plot for influence attributes\n\n        Parameters\n        ----------\n        y_var : str\n            Name of attribute or shortcut for predefined attributes that will\n            be plotted on the y-axis.\n        threshold : None or float\n            Threshold for adding annotation with observation labels.\n            Observations for which the absolute value of the y_var is larger\n            than the threshold will be annotated. Set to a negative number to\n            label all observations or to a large number to have no annotation.\n        title : str\n            If provided, the title will replace the default \"Index Plot\" title.\n        ax : matplolib axis instance\n            The plot will be added to the `ax` if provided, otherwise a new\n            figure is created.\n        idx : {None, int}\n            Some attributes require an additional index to select the y-var.\n            In dfbetas this refers to the column indes.\n        kwds : optional keywords\n            Keywords will be used in the call to matplotlib scatter function.\n        \"\"\"\n    criterion = y_var\n    if threshold is None:\n        threshold = 'all'\n    if criterion == 'dfbeta':\n        y = self.dfbetas[:, idx]\n        ylabel = 'DFBETA for ' + self.results.model.exog_names[idx]\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('hat') or criterion.startswith('lever'):\n        y = self.hat_matrix_diag\n        ylabel = 'Leverage (diagonal of hat matrix)'\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('resid_stu'):\n        y = self.resid_studentized\n        ylabel = 'Internally Studentized Residuals'\n    else:\n        y = getattr(self, y_var)\n        if idx is not None:\n            y = y[idx]\n        ylabel = y_var\n    fig = self._plot_index(y, ylabel, threshold=threshold, title=title, ax=ax, **kwds)\n    return fig",
        "mutated": [
            "def plot_index(self, y_var='cooks', threshold=None, title=None, ax=None, idx=None, **kwds):\n    if False:\n        i = 10\n    'index plot for influence attributes\\n\\n        Parameters\\n        ----------\\n        y_var : str\\n            Name of attribute or shortcut for predefined attributes that will\\n            be plotted on the y-axis.\\n        threshold : None or float\\n            Threshold for adding annotation with observation labels.\\n            Observations for which the absolute value of the y_var is larger\\n            than the threshold will be annotated. Set to a negative number to\\n            label all observations or to a large number to have no annotation.\\n        title : str\\n            If provided, the title will replace the default \"Index Plot\" title.\\n        ax : matplolib axis instance\\n            The plot will be added to the `ax` if provided, otherwise a new\\n            figure is created.\\n        idx : {None, int}\\n            Some attributes require an additional index to select the y-var.\\n            In dfbetas this refers to the column indes.\\n        kwds : optional keywords\\n            Keywords will be used in the call to matplotlib scatter function.\\n        '\n    criterion = y_var\n    if threshold is None:\n        threshold = 'all'\n    if criterion == 'dfbeta':\n        y = self.dfbetas[:, idx]\n        ylabel = 'DFBETA for ' + self.results.model.exog_names[idx]\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('hat') or criterion.startswith('lever'):\n        y = self.hat_matrix_diag\n        ylabel = 'Leverage (diagonal of hat matrix)'\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('resid_stu'):\n        y = self.resid_studentized\n        ylabel = 'Internally Studentized Residuals'\n    else:\n        y = getattr(self, y_var)\n        if idx is not None:\n            y = y[idx]\n        ylabel = y_var\n    fig = self._plot_index(y, ylabel, threshold=threshold, title=title, ax=ax, **kwds)\n    return fig",
            "def plot_index(self, y_var='cooks', threshold=None, title=None, ax=None, idx=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'index plot for influence attributes\\n\\n        Parameters\\n        ----------\\n        y_var : str\\n            Name of attribute or shortcut for predefined attributes that will\\n            be plotted on the y-axis.\\n        threshold : None or float\\n            Threshold for adding annotation with observation labels.\\n            Observations for which the absolute value of the y_var is larger\\n            than the threshold will be annotated. Set to a negative number to\\n            label all observations or to a large number to have no annotation.\\n        title : str\\n            If provided, the title will replace the default \"Index Plot\" title.\\n        ax : matplolib axis instance\\n            The plot will be added to the `ax` if provided, otherwise a new\\n            figure is created.\\n        idx : {None, int}\\n            Some attributes require an additional index to select the y-var.\\n            In dfbetas this refers to the column indes.\\n        kwds : optional keywords\\n            Keywords will be used in the call to matplotlib scatter function.\\n        '\n    criterion = y_var\n    if threshold is None:\n        threshold = 'all'\n    if criterion == 'dfbeta':\n        y = self.dfbetas[:, idx]\n        ylabel = 'DFBETA for ' + self.results.model.exog_names[idx]\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('hat') or criterion.startswith('lever'):\n        y = self.hat_matrix_diag\n        ylabel = 'Leverage (diagonal of hat matrix)'\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('resid_stu'):\n        y = self.resid_studentized\n        ylabel = 'Internally Studentized Residuals'\n    else:\n        y = getattr(self, y_var)\n        if idx is not None:\n            y = y[idx]\n        ylabel = y_var\n    fig = self._plot_index(y, ylabel, threshold=threshold, title=title, ax=ax, **kwds)\n    return fig",
            "def plot_index(self, y_var='cooks', threshold=None, title=None, ax=None, idx=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'index plot for influence attributes\\n\\n        Parameters\\n        ----------\\n        y_var : str\\n            Name of attribute or shortcut for predefined attributes that will\\n            be plotted on the y-axis.\\n        threshold : None or float\\n            Threshold for adding annotation with observation labels.\\n            Observations for which the absolute value of the y_var is larger\\n            than the threshold will be annotated. Set to a negative number to\\n            label all observations or to a large number to have no annotation.\\n        title : str\\n            If provided, the title will replace the default \"Index Plot\" title.\\n        ax : matplolib axis instance\\n            The plot will be added to the `ax` if provided, otherwise a new\\n            figure is created.\\n        idx : {None, int}\\n            Some attributes require an additional index to select the y-var.\\n            In dfbetas this refers to the column indes.\\n        kwds : optional keywords\\n            Keywords will be used in the call to matplotlib scatter function.\\n        '\n    criterion = y_var\n    if threshold is None:\n        threshold = 'all'\n    if criterion == 'dfbeta':\n        y = self.dfbetas[:, idx]\n        ylabel = 'DFBETA for ' + self.results.model.exog_names[idx]\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('hat') or criterion.startswith('lever'):\n        y = self.hat_matrix_diag\n        ylabel = 'Leverage (diagonal of hat matrix)'\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('resid_stu'):\n        y = self.resid_studentized\n        ylabel = 'Internally Studentized Residuals'\n    else:\n        y = getattr(self, y_var)\n        if idx is not None:\n            y = y[idx]\n        ylabel = y_var\n    fig = self._plot_index(y, ylabel, threshold=threshold, title=title, ax=ax, **kwds)\n    return fig",
            "def plot_index(self, y_var='cooks', threshold=None, title=None, ax=None, idx=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'index plot for influence attributes\\n\\n        Parameters\\n        ----------\\n        y_var : str\\n            Name of attribute or shortcut for predefined attributes that will\\n            be plotted on the y-axis.\\n        threshold : None or float\\n            Threshold for adding annotation with observation labels.\\n            Observations for which the absolute value of the y_var is larger\\n            than the threshold will be annotated. Set to a negative number to\\n            label all observations or to a large number to have no annotation.\\n        title : str\\n            If provided, the title will replace the default \"Index Plot\" title.\\n        ax : matplolib axis instance\\n            The plot will be added to the `ax` if provided, otherwise a new\\n            figure is created.\\n        idx : {None, int}\\n            Some attributes require an additional index to select the y-var.\\n            In dfbetas this refers to the column indes.\\n        kwds : optional keywords\\n            Keywords will be used in the call to matplotlib scatter function.\\n        '\n    criterion = y_var\n    if threshold is None:\n        threshold = 'all'\n    if criterion == 'dfbeta':\n        y = self.dfbetas[:, idx]\n        ylabel = 'DFBETA for ' + self.results.model.exog_names[idx]\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('hat') or criterion.startswith('lever'):\n        y = self.hat_matrix_diag\n        ylabel = 'Leverage (diagonal of hat matrix)'\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('resid_stu'):\n        y = self.resid_studentized\n        ylabel = 'Internally Studentized Residuals'\n    else:\n        y = getattr(self, y_var)\n        if idx is not None:\n            y = y[idx]\n        ylabel = y_var\n    fig = self._plot_index(y, ylabel, threshold=threshold, title=title, ax=ax, **kwds)\n    return fig",
            "def plot_index(self, y_var='cooks', threshold=None, title=None, ax=None, idx=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'index plot for influence attributes\\n\\n        Parameters\\n        ----------\\n        y_var : str\\n            Name of attribute or shortcut for predefined attributes that will\\n            be plotted on the y-axis.\\n        threshold : None or float\\n            Threshold for adding annotation with observation labels.\\n            Observations for which the absolute value of the y_var is larger\\n            than the threshold will be annotated. Set to a negative number to\\n            label all observations or to a large number to have no annotation.\\n        title : str\\n            If provided, the title will replace the default \"Index Plot\" title.\\n        ax : matplolib axis instance\\n            The plot will be added to the `ax` if provided, otherwise a new\\n            figure is created.\\n        idx : {None, int}\\n            Some attributes require an additional index to select the y-var.\\n            In dfbetas this refers to the column indes.\\n        kwds : optional keywords\\n            Keywords will be used in the call to matplotlib scatter function.\\n        '\n    criterion = y_var\n    if threshold is None:\n        threshold = 'all'\n    if criterion == 'dfbeta':\n        y = self.dfbetas[:, idx]\n        ylabel = 'DFBETA for ' + self.results.model.exog_names[idx]\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('hat') or criterion.startswith('lever'):\n        y = self.hat_matrix_diag\n        ylabel = 'Leverage (diagonal of hat matrix)'\n    elif criterion.startswith('cook'):\n        y = self.cooks_distance[0]\n        ylabel = \"Cook's distance\"\n    elif criterion.startswith('resid_stu'):\n        y = self.resid_studentized\n        ylabel = 'Internally Studentized Residuals'\n    else:\n        y = getattr(self, y_var)\n        if idx is not None:\n            y = y[idx]\n        ylabel = y_var\n    fig = self._plot_index(y, ylabel, threshold=threshold, title=title, ax=ax, **kwds)\n    return fig"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, results, resid=None, endog=None, exog=None, hat_matrix_diag=None, cov_params=None, scale=None):\n    self.results = results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.k_params = np.size(results.params)\n    self.endog = endog if endog is not None else results.model.endog\n    self.exog = exog if exog is not None else results.model.exog\n    self.scale = scale if scale is not None else results.scale\n    if resid is not None:\n        self.resid = resid\n    else:\n        self.resid = getattr(results, 'resid_pearson', None)\n        if self.resid is not None:\n            self.resid = self.resid / np.sqrt(self.scale)\n    self.cov_params = cov_params if cov_params is not None else results.cov_params()\n    self.model_class = results.model.__class__\n    self.hessian = self.results.model.hessian(self.results.params)\n    self.score_obs = self.results.model.score_obs(self.results.params)\n    if hat_matrix_diag is not None:\n        self._hat_matrix_diag = hat_matrix_diag",
        "mutated": [
            "def __init__(self, results, resid=None, endog=None, exog=None, hat_matrix_diag=None, cov_params=None, scale=None):\n    if False:\n        i = 10\n    self.results = results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.k_params = np.size(results.params)\n    self.endog = endog if endog is not None else results.model.endog\n    self.exog = exog if exog is not None else results.model.exog\n    self.scale = scale if scale is not None else results.scale\n    if resid is not None:\n        self.resid = resid\n    else:\n        self.resid = getattr(results, 'resid_pearson', None)\n        if self.resid is not None:\n            self.resid = self.resid / np.sqrt(self.scale)\n    self.cov_params = cov_params if cov_params is not None else results.cov_params()\n    self.model_class = results.model.__class__\n    self.hessian = self.results.model.hessian(self.results.params)\n    self.score_obs = self.results.model.score_obs(self.results.params)\n    if hat_matrix_diag is not None:\n        self._hat_matrix_diag = hat_matrix_diag",
            "def __init__(self, results, resid=None, endog=None, exog=None, hat_matrix_diag=None, cov_params=None, scale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.results = results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.k_params = np.size(results.params)\n    self.endog = endog if endog is not None else results.model.endog\n    self.exog = exog if exog is not None else results.model.exog\n    self.scale = scale if scale is not None else results.scale\n    if resid is not None:\n        self.resid = resid\n    else:\n        self.resid = getattr(results, 'resid_pearson', None)\n        if self.resid is not None:\n            self.resid = self.resid / np.sqrt(self.scale)\n    self.cov_params = cov_params if cov_params is not None else results.cov_params()\n    self.model_class = results.model.__class__\n    self.hessian = self.results.model.hessian(self.results.params)\n    self.score_obs = self.results.model.score_obs(self.results.params)\n    if hat_matrix_diag is not None:\n        self._hat_matrix_diag = hat_matrix_diag",
            "def __init__(self, results, resid=None, endog=None, exog=None, hat_matrix_diag=None, cov_params=None, scale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.results = results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.k_params = np.size(results.params)\n    self.endog = endog if endog is not None else results.model.endog\n    self.exog = exog if exog is not None else results.model.exog\n    self.scale = scale if scale is not None else results.scale\n    if resid is not None:\n        self.resid = resid\n    else:\n        self.resid = getattr(results, 'resid_pearson', None)\n        if self.resid is not None:\n            self.resid = self.resid / np.sqrt(self.scale)\n    self.cov_params = cov_params if cov_params is not None else results.cov_params()\n    self.model_class = results.model.__class__\n    self.hessian = self.results.model.hessian(self.results.params)\n    self.score_obs = self.results.model.score_obs(self.results.params)\n    if hat_matrix_diag is not None:\n        self._hat_matrix_diag = hat_matrix_diag",
            "def __init__(self, results, resid=None, endog=None, exog=None, hat_matrix_diag=None, cov_params=None, scale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.results = results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.k_params = np.size(results.params)\n    self.endog = endog if endog is not None else results.model.endog\n    self.exog = exog if exog is not None else results.model.exog\n    self.scale = scale if scale is not None else results.scale\n    if resid is not None:\n        self.resid = resid\n    else:\n        self.resid = getattr(results, 'resid_pearson', None)\n        if self.resid is not None:\n            self.resid = self.resid / np.sqrt(self.scale)\n    self.cov_params = cov_params if cov_params is not None else results.cov_params()\n    self.model_class = results.model.__class__\n    self.hessian = self.results.model.hessian(self.results.params)\n    self.score_obs = self.results.model.score_obs(self.results.params)\n    if hat_matrix_diag is not None:\n        self._hat_matrix_diag = hat_matrix_diag",
            "def __init__(self, results, resid=None, endog=None, exog=None, hat_matrix_diag=None, cov_params=None, scale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.results = results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.k_params = np.size(results.params)\n    self.endog = endog if endog is not None else results.model.endog\n    self.exog = exog if exog is not None else results.model.exog\n    self.scale = scale if scale is not None else results.scale\n    if resid is not None:\n        self.resid = resid\n    else:\n        self.resid = getattr(results, 'resid_pearson', None)\n        if self.resid is not None:\n            self.resid = self.resid / np.sqrt(self.scale)\n    self.cov_params = cov_params if cov_params is not None else results.cov_params()\n    self.model_class = results.model.__class__\n    self.hessian = self.results.model.hessian(self.results.params)\n    self.score_obs = self.results.model.score_obs(self.results.params)\n    if hat_matrix_diag is not None:\n        self._hat_matrix_diag = hat_matrix_diag"
        ]
    },
    {
        "func_name": "hat_matrix_diag",
        "original": "@cache_readonly\ndef hat_matrix_diag(self):\n    \"\"\"Diagonal of the generalized leverage\n\n        This is the analogue of the hat matrix diagonal for general MLE.\n        \"\"\"\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    try:\n        dsdy = self.results.model._deriv_score_obs_dendog(self.results.params)\n    except NotImplementedError:\n        dsdy = None\n    if dsdy is None:\n        warnings.warn('hat matrix is not available, missing derivatives', UserWarning)\n        return None\n    dmu_dp = self.results.model._deriv_mean_dparams(self.results.params)\n    h = (dmu_dp * np.linalg.solve(-self.hessian, dsdy.T).T).sum(1)\n    return h",
        "mutated": [
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n    'Diagonal of the generalized leverage\\n\\n        This is the analogue of the hat matrix diagonal for general MLE.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    try:\n        dsdy = self.results.model._deriv_score_obs_dendog(self.results.params)\n    except NotImplementedError:\n        dsdy = None\n    if dsdy is None:\n        warnings.warn('hat matrix is not available, missing derivatives', UserWarning)\n        return None\n    dmu_dp = self.results.model._deriv_mean_dparams(self.results.params)\n    h = (dmu_dp * np.linalg.solve(-self.hessian, dsdy.T).T).sum(1)\n    return h",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Diagonal of the generalized leverage\\n\\n        This is the analogue of the hat matrix diagonal for general MLE.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    try:\n        dsdy = self.results.model._deriv_score_obs_dendog(self.results.params)\n    except NotImplementedError:\n        dsdy = None\n    if dsdy is None:\n        warnings.warn('hat matrix is not available, missing derivatives', UserWarning)\n        return None\n    dmu_dp = self.results.model._deriv_mean_dparams(self.results.params)\n    h = (dmu_dp * np.linalg.solve(-self.hessian, dsdy.T).T).sum(1)\n    return h",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Diagonal of the generalized leverage\\n\\n        This is the analogue of the hat matrix diagonal for general MLE.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    try:\n        dsdy = self.results.model._deriv_score_obs_dendog(self.results.params)\n    except NotImplementedError:\n        dsdy = None\n    if dsdy is None:\n        warnings.warn('hat matrix is not available, missing derivatives', UserWarning)\n        return None\n    dmu_dp = self.results.model._deriv_mean_dparams(self.results.params)\n    h = (dmu_dp * np.linalg.solve(-self.hessian, dsdy.T).T).sum(1)\n    return h",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Diagonal of the generalized leverage\\n\\n        This is the analogue of the hat matrix diagonal for general MLE.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    try:\n        dsdy = self.results.model._deriv_score_obs_dendog(self.results.params)\n    except NotImplementedError:\n        dsdy = None\n    if dsdy is None:\n        warnings.warn('hat matrix is not available, missing derivatives', UserWarning)\n        return None\n    dmu_dp = self.results.model._deriv_mean_dparams(self.results.params)\n    h = (dmu_dp * np.linalg.solve(-self.hessian, dsdy.T).T).sum(1)\n    return h",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Diagonal of the generalized leverage\\n\\n        This is the analogue of the hat matrix diagonal for general MLE.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    try:\n        dsdy = self.results.model._deriv_score_obs_dendog(self.results.params)\n    except NotImplementedError:\n        dsdy = None\n    if dsdy is None:\n        warnings.warn('hat matrix is not available, missing derivatives', UserWarning)\n        return None\n    dmu_dp = self.results.model._deriv_mean_dparams(self.results.params)\n    h = (dmu_dp * np.linalg.solve(-self.hessian, dsdy.T).T).sum(1)\n    return h"
        ]
    },
    {
        "func_name": "hat_matrix_exog_diag",
        "original": "@cache_readonly\ndef hat_matrix_exog_diag(self):\n    \"\"\"Diagonal of the hat_matrix using only exog as in OLS\n\n        \"\"\"\n    get_exogs = getattr(self.results.model, '_get_exogs', None)\n    if get_exogs is not None:\n        exog = np.column_stack(get_exogs())\n    else:\n        exog = self.exog\n    return (exog * np.linalg.pinv(exog).T).sum(1)",
        "mutated": [
            "@cache_readonly\ndef hat_matrix_exog_diag(self):\n    if False:\n        i = 10\n    'Diagonal of the hat_matrix using only exog as in OLS\\n\\n        '\n    get_exogs = getattr(self.results.model, '_get_exogs', None)\n    if get_exogs is not None:\n        exog = np.column_stack(get_exogs())\n    else:\n        exog = self.exog\n    return (exog * np.linalg.pinv(exog).T).sum(1)",
            "@cache_readonly\ndef hat_matrix_exog_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Diagonal of the hat_matrix using only exog as in OLS\\n\\n        '\n    get_exogs = getattr(self.results.model, '_get_exogs', None)\n    if get_exogs is not None:\n        exog = np.column_stack(get_exogs())\n    else:\n        exog = self.exog\n    return (exog * np.linalg.pinv(exog).T).sum(1)",
            "@cache_readonly\ndef hat_matrix_exog_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Diagonal of the hat_matrix using only exog as in OLS\\n\\n        '\n    get_exogs = getattr(self.results.model, '_get_exogs', None)\n    if get_exogs is not None:\n        exog = np.column_stack(get_exogs())\n    else:\n        exog = self.exog\n    return (exog * np.linalg.pinv(exog).T).sum(1)",
            "@cache_readonly\ndef hat_matrix_exog_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Diagonal of the hat_matrix using only exog as in OLS\\n\\n        '\n    get_exogs = getattr(self.results.model, '_get_exogs', None)\n    if get_exogs is not None:\n        exog = np.column_stack(get_exogs())\n    else:\n        exog = self.exog\n    return (exog * np.linalg.pinv(exog).T).sum(1)",
            "@cache_readonly\ndef hat_matrix_exog_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Diagonal of the hat_matrix using only exog as in OLS\\n\\n        '\n    get_exogs = getattr(self.results.model, '_get_exogs', None)\n    if get_exogs is not None:\n        exog = np.column_stack(get_exogs())\n    else:\n        exog = self.exog\n    return (exog * np.linalg.pinv(exog).T).sum(1)"
        ]
    },
    {
        "func_name": "d_params",
        "original": "@cache_readonly\ndef d_params(self):\n    \"\"\"Approximate change in parameter estimates when dropping observation.\n\n        This uses one-step approximation of the parameter change to deleting\n        one observation.\n        \"\"\"\n    so_noti = self.score_obs.sum(0) - self.score_obs\n    beta_i = np.linalg.solve(self.hessian, so_noti.T).T\n    if self.hat_matrix_diag is not None:\n        beta_i /= (1 - self.hat_matrix_diag)[:, None]\n    return beta_i",
        "mutated": [
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n    'Approximate change in parameter estimates when dropping observation.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    so_noti = self.score_obs.sum(0) - self.score_obs\n    beta_i = np.linalg.solve(self.hessian, so_noti.T).T\n    if self.hat_matrix_diag is not None:\n        beta_i /= (1 - self.hat_matrix_diag)[:, None]\n    return beta_i",
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Approximate change in parameter estimates when dropping observation.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    so_noti = self.score_obs.sum(0) - self.score_obs\n    beta_i = np.linalg.solve(self.hessian, so_noti.T).T\n    if self.hat_matrix_diag is not None:\n        beta_i /= (1 - self.hat_matrix_diag)[:, None]\n    return beta_i",
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Approximate change in parameter estimates when dropping observation.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    so_noti = self.score_obs.sum(0) - self.score_obs\n    beta_i = np.linalg.solve(self.hessian, so_noti.T).T\n    if self.hat_matrix_diag is not None:\n        beta_i /= (1 - self.hat_matrix_diag)[:, None]\n    return beta_i",
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Approximate change in parameter estimates when dropping observation.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    so_noti = self.score_obs.sum(0) - self.score_obs\n    beta_i = np.linalg.solve(self.hessian, so_noti.T).T\n    if self.hat_matrix_diag is not None:\n        beta_i /= (1 - self.hat_matrix_diag)[:, None]\n    return beta_i",
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Approximate change in parameter estimates when dropping observation.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    so_noti = self.score_obs.sum(0) - self.score_obs\n    beta_i = np.linalg.solve(self.hessian, so_noti.T).T\n    if self.hat_matrix_diag is not None:\n        beta_i /= (1 - self.hat_matrix_diag)[:, None]\n    return beta_i"
        ]
    },
    {
        "func_name": "dfbetas",
        "original": "@cache_readonly\ndef dfbetas(self):\n    \"\"\"Scaled change in parameter estimates.\n\n        The one-step change of parameters in d_params is rescaled by dividing\n        by the standard error of the parameter estimate given by results.bse.\n        \"\"\"\n    beta_i = self.d_params / self.results.bse\n    return beta_i",
        "mutated": [
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n    'Scaled change in parameter estimates.\\n\\n        The one-step change of parameters in d_params is rescaled by dividing\\n        by the standard error of the parameter estimate given by results.bse.\\n        '\n    beta_i = self.d_params / self.results.bse\n    return beta_i",
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Scaled change in parameter estimates.\\n\\n        The one-step change of parameters in d_params is rescaled by dividing\\n        by the standard error of the parameter estimate given by results.bse.\\n        '\n    beta_i = self.d_params / self.results.bse\n    return beta_i",
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Scaled change in parameter estimates.\\n\\n        The one-step change of parameters in d_params is rescaled by dividing\\n        by the standard error of the parameter estimate given by results.bse.\\n        '\n    beta_i = self.d_params / self.results.bse\n    return beta_i",
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Scaled change in parameter estimates.\\n\\n        The one-step change of parameters in d_params is rescaled by dividing\\n        by the standard error of the parameter estimate given by results.bse.\\n        '\n    beta_i = self.d_params / self.results.bse\n    return beta_i",
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Scaled change in parameter estimates.\\n\\n        The one-step change of parameters in d_params is rescaled by dividing\\n        by the standard error of the parameter estimate given by results.bse.\\n        '\n    beta_i = self.d_params / self.results.bse\n    return beta_i"
        ]
    },
    {
        "func_name": "params_one",
        "original": "@cache_readonly\ndef params_one(self):\n    \"\"\"Parameter estimate based on one-step approximation.\n\n        This the one step parameter estimate computed as\n        ``params`` from the full sample minus ``d_params``.\n        \"\"\"\n    return self.results.params - self.d_params",
        "mutated": [
            "@cache_readonly\ndef params_one(self):\n    if False:\n        i = 10\n    'Parameter estimate based on one-step approximation.\\n\\n        This the one step parameter estimate computed as\\n        ``params`` from the full sample minus ``d_params``.\\n        '\n    return self.results.params - self.d_params",
            "@cache_readonly\ndef params_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Parameter estimate based on one-step approximation.\\n\\n        This the one step parameter estimate computed as\\n        ``params`` from the full sample minus ``d_params``.\\n        '\n    return self.results.params - self.d_params",
            "@cache_readonly\ndef params_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Parameter estimate based on one-step approximation.\\n\\n        This the one step parameter estimate computed as\\n        ``params`` from the full sample minus ``d_params``.\\n        '\n    return self.results.params - self.d_params",
            "@cache_readonly\ndef params_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Parameter estimate based on one-step approximation.\\n\\n        This the one step parameter estimate computed as\\n        ``params`` from the full sample minus ``d_params``.\\n        '\n    return self.results.params - self.d_params",
            "@cache_readonly\ndef params_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Parameter estimate based on one-step approximation.\\n\\n        This the one step parameter estimate computed as\\n        ``params`` from the full sample minus ``d_params``.\\n        '\n    return self.results.params - self.d_params"
        ]
    },
    {
        "func_name": "cooks_distance",
        "original": "@cache_readonly\ndef cooks_distance(self):\n    \"\"\"Cook's distance and p-values.\n\n        Based on one step approximation d_params and on results.cov_params\n        Cook's distance divides by the number of explanatory variables.\n\n        p-values are based on the F-distribution which are only approximate\n        outside of linear Gaussian models.\n\n        Warning: The definition of p-values might change if we switch to using\n        chi-square distribution instead of F-distribution, or if we make it\n        dependent on the fit keyword use_t.\n        \"\"\"\n    cooks_d2 = (self.d_params * np.linalg.solve(self.cov_params, self.d_params.T).T).sum(1)\n    cooks_d2 /= self.k_params\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_params, self.results.df_resid)\n    return (cooks_d2, pvals)",
        "mutated": [
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n    \"Cook's distance and p-values.\\n\\n        Based on one step approximation d_params and on results.cov_params\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        p-values are based on the F-distribution which are only approximate\\n        outside of linear Gaussian models.\\n\\n        Warning: The definition of p-values might change if we switch to using\\n        chi-square distribution instead of F-distribution, or if we make it\\n        dependent on the fit keyword use_t.\\n        \"\n    cooks_d2 = (self.d_params * np.linalg.solve(self.cov_params, self.d_params.T).T).sum(1)\n    cooks_d2 /= self.k_params\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_params, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Cook's distance and p-values.\\n\\n        Based on one step approximation d_params and on results.cov_params\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        p-values are based on the F-distribution which are only approximate\\n        outside of linear Gaussian models.\\n\\n        Warning: The definition of p-values might change if we switch to using\\n        chi-square distribution instead of F-distribution, or if we make it\\n        dependent on the fit keyword use_t.\\n        \"\n    cooks_d2 = (self.d_params * np.linalg.solve(self.cov_params, self.d_params.T).T).sum(1)\n    cooks_d2 /= self.k_params\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_params, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Cook's distance and p-values.\\n\\n        Based on one step approximation d_params and on results.cov_params\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        p-values are based on the F-distribution which are only approximate\\n        outside of linear Gaussian models.\\n\\n        Warning: The definition of p-values might change if we switch to using\\n        chi-square distribution instead of F-distribution, or if we make it\\n        dependent on the fit keyword use_t.\\n        \"\n    cooks_d2 = (self.d_params * np.linalg.solve(self.cov_params, self.d_params.T).T).sum(1)\n    cooks_d2 /= self.k_params\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_params, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Cook's distance and p-values.\\n\\n        Based on one step approximation d_params and on results.cov_params\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        p-values are based on the F-distribution which are only approximate\\n        outside of linear Gaussian models.\\n\\n        Warning: The definition of p-values might change if we switch to using\\n        chi-square distribution instead of F-distribution, or if we make it\\n        dependent on the fit keyword use_t.\\n        \"\n    cooks_d2 = (self.d_params * np.linalg.solve(self.cov_params, self.d_params.T).T).sum(1)\n    cooks_d2 /= self.k_params\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_params, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Cook's distance and p-values.\\n\\n        Based on one step approximation d_params and on results.cov_params\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        p-values are based on the F-distribution which are only approximate\\n        outside of linear Gaussian models.\\n\\n        Warning: The definition of p-values might change if we switch to using\\n        chi-square distribution instead of F-distribution, or if we make it\\n        dependent on the fit keyword use_t.\\n        \"\n    cooks_d2 = (self.d_params * np.linalg.solve(self.cov_params, self.d_params.T).T).sum(1)\n    cooks_d2 /= self.k_params\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_params, self.results.df_resid)\n    return (cooks_d2, pvals)"
        ]
    },
    {
        "func_name": "resid_studentized",
        "original": "@cache_readonly\ndef resid_studentized(self):\n    \"\"\"studentized default residuals.\n\n        This uses the residual in `resid` attribute, which is by default\n        resid_pearson and studentizes is using the generalized leverage.\n\n        self.resid / np.sqrt(1 - self.hat_matrix_diag)\n\n        Studentized residuals are not available if hat_matrix_diag is None.\n\n        \"\"\"\n    return self.resid / np.sqrt(1 - self.hat_matrix_diag)",
        "mutated": [
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n    'studentized default residuals.\\n\\n        This uses the residual in `resid` attribute, which is by default\\n        resid_pearson and studentizes is using the generalized leverage.\\n\\n        self.resid / np.sqrt(1 - self.hat_matrix_diag)\\n\\n        Studentized residuals are not available if hat_matrix_diag is None.\\n\\n        '\n    return self.resid / np.sqrt(1 - self.hat_matrix_diag)",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'studentized default residuals.\\n\\n        This uses the residual in `resid` attribute, which is by default\\n        resid_pearson and studentizes is using the generalized leverage.\\n\\n        self.resid / np.sqrt(1 - self.hat_matrix_diag)\\n\\n        Studentized residuals are not available if hat_matrix_diag is None.\\n\\n        '\n    return self.resid / np.sqrt(1 - self.hat_matrix_diag)",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'studentized default residuals.\\n\\n        This uses the residual in `resid` attribute, which is by default\\n        resid_pearson and studentizes is using the generalized leverage.\\n\\n        self.resid / np.sqrt(1 - self.hat_matrix_diag)\\n\\n        Studentized residuals are not available if hat_matrix_diag is None.\\n\\n        '\n    return self.resid / np.sqrt(1 - self.hat_matrix_diag)",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'studentized default residuals.\\n\\n        This uses the residual in `resid` attribute, which is by default\\n        resid_pearson and studentizes is using the generalized leverage.\\n\\n        self.resid / np.sqrt(1 - self.hat_matrix_diag)\\n\\n        Studentized residuals are not available if hat_matrix_diag is None.\\n\\n        '\n    return self.resid / np.sqrt(1 - self.hat_matrix_diag)",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'studentized default residuals.\\n\\n        This uses the residual in `resid` attribute, which is by default\\n        resid_pearson and studentizes is using the generalized leverage.\\n\\n        self.resid / np.sqrt(1 - self.hat_matrix_diag)\\n\\n        Studentized residuals are not available if hat_matrix_diag is None.\\n\\n        '\n    return self.resid / np.sqrt(1 - self.hat_matrix_diag)"
        ]
    },
    {
        "func_name": "resid_score_factor",
        "original": "def resid_score_factor(self):\n    \"\"\"Score residual divided by sqrt of hessian factor.\n\n        experimental, agrees with GLMInfluence for Binomial and Gaussian.\n        This corresponds to considering the linear predictors as parameters\n        of the model.\n\n        Note: Nhis might have nan values if second derivative, hessian_factor,\n        is positive, i.e. loglikelihood is not globally concave w.r.t. linear\n        predictor. (This occured in an example for GeneralizedPoisson)\n        \"\"\"\n    from statsmodels.genmod.generalized_linear_model import GLM\n    sf = self.results.model.score_factor(self.results.params)\n    hf = self.results.model.hessian_factor(self.results.params)\n    if isinstance(sf, tuple):\n        sf = sf[0]\n    if isinstance(hf, tuple):\n        hf = hf[0]\n    if not isinstance(self.results.model, GLM):\n        hf = -hf\n    return sf / np.sqrt(hf) / np.sqrt(1 - self.hat_matrix_diag)",
        "mutated": [
            "def resid_score_factor(self):\n    if False:\n        i = 10\n    'Score residual divided by sqrt of hessian factor.\\n\\n        experimental, agrees with GLMInfluence for Binomial and Gaussian.\\n        This corresponds to considering the linear predictors as parameters\\n        of the model.\\n\\n        Note: Nhis might have nan values if second derivative, hessian_factor,\\n        is positive, i.e. loglikelihood is not globally concave w.r.t. linear\\n        predictor. (This occured in an example for GeneralizedPoisson)\\n        '\n    from statsmodels.genmod.generalized_linear_model import GLM\n    sf = self.results.model.score_factor(self.results.params)\n    hf = self.results.model.hessian_factor(self.results.params)\n    if isinstance(sf, tuple):\n        sf = sf[0]\n    if isinstance(hf, tuple):\n        hf = hf[0]\n    if not isinstance(self.results.model, GLM):\n        hf = -hf\n    return sf / np.sqrt(hf) / np.sqrt(1 - self.hat_matrix_diag)",
            "def resid_score_factor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Score residual divided by sqrt of hessian factor.\\n\\n        experimental, agrees with GLMInfluence for Binomial and Gaussian.\\n        This corresponds to considering the linear predictors as parameters\\n        of the model.\\n\\n        Note: Nhis might have nan values if second derivative, hessian_factor,\\n        is positive, i.e. loglikelihood is not globally concave w.r.t. linear\\n        predictor. (This occured in an example for GeneralizedPoisson)\\n        '\n    from statsmodels.genmod.generalized_linear_model import GLM\n    sf = self.results.model.score_factor(self.results.params)\n    hf = self.results.model.hessian_factor(self.results.params)\n    if isinstance(sf, tuple):\n        sf = sf[0]\n    if isinstance(hf, tuple):\n        hf = hf[0]\n    if not isinstance(self.results.model, GLM):\n        hf = -hf\n    return sf / np.sqrt(hf) / np.sqrt(1 - self.hat_matrix_diag)",
            "def resid_score_factor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Score residual divided by sqrt of hessian factor.\\n\\n        experimental, agrees with GLMInfluence for Binomial and Gaussian.\\n        This corresponds to considering the linear predictors as parameters\\n        of the model.\\n\\n        Note: Nhis might have nan values if second derivative, hessian_factor,\\n        is positive, i.e. loglikelihood is not globally concave w.r.t. linear\\n        predictor. (This occured in an example for GeneralizedPoisson)\\n        '\n    from statsmodels.genmod.generalized_linear_model import GLM\n    sf = self.results.model.score_factor(self.results.params)\n    hf = self.results.model.hessian_factor(self.results.params)\n    if isinstance(sf, tuple):\n        sf = sf[0]\n    if isinstance(hf, tuple):\n        hf = hf[0]\n    if not isinstance(self.results.model, GLM):\n        hf = -hf\n    return sf / np.sqrt(hf) / np.sqrt(1 - self.hat_matrix_diag)",
            "def resid_score_factor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Score residual divided by sqrt of hessian factor.\\n\\n        experimental, agrees with GLMInfluence for Binomial and Gaussian.\\n        This corresponds to considering the linear predictors as parameters\\n        of the model.\\n\\n        Note: Nhis might have nan values if second derivative, hessian_factor,\\n        is positive, i.e. loglikelihood is not globally concave w.r.t. linear\\n        predictor. (This occured in an example for GeneralizedPoisson)\\n        '\n    from statsmodels.genmod.generalized_linear_model import GLM\n    sf = self.results.model.score_factor(self.results.params)\n    hf = self.results.model.hessian_factor(self.results.params)\n    if isinstance(sf, tuple):\n        sf = sf[0]\n    if isinstance(hf, tuple):\n        hf = hf[0]\n    if not isinstance(self.results.model, GLM):\n        hf = -hf\n    return sf / np.sqrt(hf) / np.sqrt(1 - self.hat_matrix_diag)",
            "def resid_score_factor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Score residual divided by sqrt of hessian factor.\\n\\n        experimental, agrees with GLMInfluence for Binomial and Gaussian.\\n        This corresponds to considering the linear predictors as parameters\\n        of the model.\\n\\n        Note: Nhis might have nan values if second derivative, hessian_factor,\\n        is positive, i.e. loglikelihood is not globally concave w.r.t. linear\\n        predictor. (This occured in an example for GeneralizedPoisson)\\n        '\n    from statsmodels.genmod.generalized_linear_model import GLM\n    sf = self.results.model.score_factor(self.results.params)\n    hf = self.results.model.hessian_factor(self.results.params)\n    if isinstance(sf, tuple):\n        sf = sf[0]\n    if isinstance(hf, tuple):\n        hf = hf[0]\n    if not isinstance(self.results.model, GLM):\n        hf = -hf\n    return sf / np.sqrt(hf) / np.sqrt(1 - self.hat_matrix_diag)"
        ]
    },
    {
        "func_name": "resid_score",
        "original": "def resid_score(self, joint=True, index=None, studentize=False):\n    \"\"\"Score observations scaled by inverse hessian.\n\n        Score residual in resid_score are defined in analogy to a score test\n        statistic for each observation.\n\n        Parameters\n        ----------\n        joint : bool\n            If joint is true, then a quadratic form similar to score_test is\n            returned for each observation.\n            If joint is false, then standardized score_obs are returned. The\n            returned array is two-dimensional\n        index : ndarray (optional)\n            Optional index to select a subset of score_obs columns.\n            By default, all columns of score_obs will be used.\n        studentize : bool\n            If studentize is true, the the scaled residuals are also\n            studentized using the generalized leverage.\n\n        Returns\n        -------\n        array :  1-D or 2-D residuals\n\n        Notes\n        -----\n        Status: experimental\n\n        Because of the one srep approacimation of d_params, score residuals\n        are identical to cooks_distance, except for\n\n        - cooks_distance is normalized by the number of parameters\n        - cooks_distance uses cov_params, resid_score is based on Hessian.\n          This will make them differ in the case of robust cov_params.\n\n        \"\"\"\n    score_obs = self.results.model.score_obs(self.results.params)\n    hess = self.results.model.hessian(self.results.params)\n    if index is not None:\n        score_obs = score_obs[:, index]\n        hess = hess[index[:, None], index]\n    if joint:\n        resid = (score_obs.T * np.linalg.solve(-hess, score_obs.T)).sum(0)\n    else:\n        resid = score_obs / np.sqrt(np.diag(-hess))\n    if studentize:\n        if joint:\n            resid /= np.sqrt(1 - self.hat_matrix_diag)\n        else:\n            resid /= np.sqrt(1 - self.hat_matrix_diag[:, None])\n    return resid",
        "mutated": [
            "def resid_score(self, joint=True, index=None, studentize=False):\n    if False:\n        i = 10\n    'Score observations scaled by inverse hessian.\\n\\n        Score residual in resid_score are defined in analogy to a score test\\n        statistic for each observation.\\n\\n        Parameters\\n        ----------\\n        joint : bool\\n            If joint is true, then a quadratic form similar to score_test is\\n            returned for each observation.\\n            If joint is false, then standardized score_obs are returned. The\\n            returned array is two-dimensional\\n        index : ndarray (optional)\\n            Optional index to select a subset of score_obs columns.\\n            By default, all columns of score_obs will be used.\\n        studentize : bool\\n            If studentize is true, the the scaled residuals are also\\n            studentized using the generalized leverage.\\n\\n        Returns\\n        -------\\n        array :  1-D or 2-D residuals\\n\\n        Notes\\n        -----\\n        Status: experimental\\n\\n        Because of the one srep approacimation of d_params, score residuals\\n        are identical to cooks_distance, except for\\n\\n        - cooks_distance is normalized by the number of parameters\\n        - cooks_distance uses cov_params, resid_score is based on Hessian.\\n          This will make them differ in the case of robust cov_params.\\n\\n        '\n    score_obs = self.results.model.score_obs(self.results.params)\n    hess = self.results.model.hessian(self.results.params)\n    if index is not None:\n        score_obs = score_obs[:, index]\n        hess = hess[index[:, None], index]\n    if joint:\n        resid = (score_obs.T * np.linalg.solve(-hess, score_obs.T)).sum(0)\n    else:\n        resid = score_obs / np.sqrt(np.diag(-hess))\n    if studentize:\n        if joint:\n            resid /= np.sqrt(1 - self.hat_matrix_diag)\n        else:\n            resid /= np.sqrt(1 - self.hat_matrix_diag[:, None])\n    return resid",
            "def resid_score(self, joint=True, index=None, studentize=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Score observations scaled by inverse hessian.\\n\\n        Score residual in resid_score are defined in analogy to a score test\\n        statistic for each observation.\\n\\n        Parameters\\n        ----------\\n        joint : bool\\n            If joint is true, then a quadratic form similar to score_test is\\n            returned for each observation.\\n            If joint is false, then standardized score_obs are returned. The\\n            returned array is two-dimensional\\n        index : ndarray (optional)\\n            Optional index to select a subset of score_obs columns.\\n            By default, all columns of score_obs will be used.\\n        studentize : bool\\n            If studentize is true, the the scaled residuals are also\\n            studentized using the generalized leverage.\\n\\n        Returns\\n        -------\\n        array :  1-D or 2-D residuals\\n\\n        Notes\\n        -----\\n        Status: experimental\\n\\n        Because of the one srep approacimation of d_params, score residuals\\n        are identical to cooks_distance, except for\\n\\n        - cooks_distance is normalized by the number of parameters\\n        - cooks_distance uses cov_params, resid_score is based on Hessian.\\n          This will make them differ in the case of robust cov_params.\\n\\n        '\n    score_obs = self.results.model.score_obs(self.results.params)\n    hess = self.results.model.hessian(self.results.params)\n    if index is not None:\n        score_obs = score_obs[:, index]\n        hess = hess[index[:, None], index]\n    if joint:\n        resid = (score_obs.T * np.linalg.solve(-hess, score_obs.T)).sum(0)\n    else:\n        resid = score_obs / np.sqrt(np.diag(-hess))\n    if studentize:\n        if joint:\n            resid /= np.sqrt(1 - self.hat_matrix_diag)\n        else:\n            resid /= np.sqrt(1 - self.hat_matrix_diag[:, None])\n    return resid",
            "def resid_score(self, joint=True, index=None, studentize=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Score observations scaled by inverse hessian.\\n\\n        Score residual in resid_score are defined in analogy to a score test\\n        statistic for each observation.\\n\\n        Parameters\\n        ----------\\n        joint : bool\\n            If joint is true, then a quadratic form similar to score_test is\\n            returned for each observation.\\n            If joint is false, then standardized score_obs are returned. The\\n            returned array is two-dimensional\\n        index : ndarray (optional)\\n            Optional index to select a subset of score_obs columns.\\n            By default, all columns of score_obs will be used.\\n        studentize : bool\\n            If studentize is true, the the scaled residuals are also\\n            studentized using the generalized leverage.\\n\\n        Returns\\n        -------\\n        array :  1-D or 2-D residuals\\n\\n        Notes\\n        -----\\n        Status: experimental\\n\\n        Because of the one srep approacimation of d_params, score residuals\\n        are identical to cooks_distance, except for\\n\\n        - cooks_distance is normalized by the number of parameters\\n        - cooks_distance uses cov_params, resid_score is based on Hessian.\\n          This will make them differ in the case of robust cov_params.\\n\\n        '\n    score_obs = self.results.model.score_obs(self.results.params)\n    hess = self.results.model.hessian(self.results.params)\n    if index is not None:\n        score_obs = score_obs[:, index]\n        hess = hess[index[:, None], index]\n    if joint:\n        resid = (score_obs.T * np.linalg.solve(-hess, score_obs.T)).sum(0)\n    else:\n        resid = score_obs / np.sqrt(np.diag(-hess))\n    if studentize:\n        if joint:\n            resid /= np.sqrt(1 - self.hat_matrix_diag)\n        else:\n            resid /= np.sqrt(1 - self.hat_matrix_diag[:, None])\n    return resid",
            "def resid_score(self, joint=True, index=None, studentize=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Score observations scaled by inverse hessian.\\n\\n        Score residual in resid_score are defined in analogy to a score test\\n        statistic for each observation.\\n\\n        Parameters\\n        ----------\\n        joint : bool\\n            If joint is true, then a quadratic form similar to score_test is\\n            returned for each observation.\\n            If joint is false, then standardized score_obs are returned. The\\n            returned array is two-dimensional\\n        index : ndarray (optional)\\n            Optional index to select a subset of score_obs columns.\\n            By default, all columns of score_obs will be used.\\n        studentize : bool\\n            If studentize is true, the the scaled residuals are also\\n            studentized using the generalized leverage.\\n\\n        Returns\\n        -------\\n        array :  1-D or 2-D residuals\\n\\n        Notes\\n        -----\\n        Status: experimental\\n\\n        Because of the one srep approacimation of d_params, score residuals\\n        are identical to cooks_distance, except for\\n\\n        - cooks_distance is normalized by the number of parameters\\n        - cooks_distance uses cov_params, resid_score is based on Hessian.\\n          This will make them differ in the case of robust cov_params.\\n\\n        '\n    score_obs = self.results.model.score_obs(self.results.params)\n    hess = self.results.model.hessian(self.results.params)\n    if index is not None:\n        score_obs = score_obs[:, index]\n        hess = hess[index[:, None], index]\n    if joint:\n        resid = (score_obs.T * np.linalg.solve(-hess, score_obs.T)).sum(0)\n    else:\n        resid = score_obs / np.sqrt(np.diag(-hess))\n    if studentize:\n        if joint:\n            resid /= np.sqrt(1 - self.hat_matrix_diag)\n        else:\n            resid /= np.sqrt(1 - self.hat_matrix_diag[:, None])\n    return resid",
            "def resid_score(self, joint=True, index=None, studentize=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Score observations scaled by inverse hessian.\\n\\n        Score residual in resid_score are defined in analogy to a score test\\n        statistic for each observation.\\n\\n        Parameters\\n        ----------\\n        joint : bool\\n            If joint is true, then a quadratic form similar to score_test is\\n            returned for each observation.\\n            If joint is false, then standardized score_obs are returned. The\\n            returned array is two-dimensional\\n        index : ndarray (optional)\\n            Optional index to select a subset of score_obs columns.\\n            By default, all columns of score_obs will be used.\\n        studentize : bool\\n            If studentize is true, the the scaled residuals are also\\n            studentized using the generalized leverage.\\n\\n        Returns\\n        -------\\n        array :  1-D or 2-D residuals\\n\\n        Notes\\n        -----\\n        Status: experimental\\n\\n        Because of the one srep approacimation of d_params, score residuals\\n        are identical to cooks_distance, except for\\n\\n        - cooks_distance is normalized by the number of parameters\\n        - cooks_distance uses cov_params, resid_score is based on Hessian.\\n          This will make them differ in the case of robust cov_params.\\n\\n        '\n    score_obs = self.results.model.score_obs(self.results.params)\n    hess = self.results.model.hessian(self.results.params)\n    if index is not None:\n        score_obs = score_obs[:, index]\n        hess = hess[index[:, None], index]\n    if joint:\n        resid = (score_obs.T * np.linalg.solve(-hess, score_obs.T)).sum(0)\n    else:\n        resid = score_obs / np.sqrt(np.diag(-hess))\n    if studentize:\n        if joint:\n            resid /= np.sqrt(1 - self.hat_matrix_diag)\n        else:\n            resid /= np.sqrt(1 - self.hat_matrix_diag[:, None])\n    return resid"
        ]
    },
    {
        "func_name": "_get_prediction",
        "original": "@cache_readonly\ndef _get_prediction(self):\n    with warnings.catch_warnings():\n        msg = 'linear keyword is deprecated, use which=\"linear\"'\n        warnings.filterwarnings('ignore', message=msg, category=FutureWarning)\n        pred = self.results.get_prediction()\n    return pred",
        "mutated": [
            "@cache_readonly\ndef _get_prediction(self):\n    if False:\n        i = 10\n    with warnings.catch_warnings():\n        msg = 'linear keyword is deprecated, use which=\"linear\"'\n        warnings.filterwarnings('ignore', message=msg, category=FutureWarning)\n        pred = self.results.get_prediction()\n    return pred",
            "@cache_readonly\ndef _get_prediction(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with warnings.catch_warnings():\n        msg = 'linear keyword is deprecated, use which=\"linear\"'\n        warnings.filterwarnings('ignore', message=msg, category=FutureWarning)\n        pred = self.results.get_prediction()\n    return pred",
            "@cache_readonly\ndef _get_prediction(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with warnings.catch_warnings():\n        msg = 'linear keyword is deprecated, use which=\"linear\"'\n        warnings.filterwarnings('ignore', message=msg, category=FutureWarning)\n        pred = self.results.get_prediction()\n    return pred",
            "@cache_readonly\ndef _get_prediction(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with warnings.catch_warnings():\n        msg = 'linear keyword is deprecated, use which=\"linear\"'\n        warnings.filterwarnings('ignore', message=msg, category=FutureWarning)\n        pred = self.results.get_prediction()\n    return pred",
            "@cache_readonly\ndef _get_prediction(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with warnings.catch_warnings():\n        msg = 'linear keyword is deprecated, use which=\"linear\"'\n        warnings.filterwarnings('ignore', message=msg, category=FutureWarning)\n        pred = self.results.get_prediction()\n    return pred"
        ]
    },
    {
        "func_name": "d_fittedvalues",
        "original": "@cache_readonly\ndef d_fittedvalues(self):\n    \"\"\"Change in expected response, fittedvalues.\n\n        Local change of expected mean given the change in the parameters as\n        computed in d_params.\n\n        Notes\n        -----\n        This uses the one-step approximation of the parameter change to\n        deleting one observation ``d_params``.\n        \"\"\"\n    params = np.asarray(self.results.params)\n    deriv = self.results.model._deriv_mean_dparams(params)\n    return (deriv * self.d_params).sum(1)",
        "mutated": [
            "@cache_readonly\ndef d_fittedvalues(self):\n    if False:\n        i = 10\n    'Change in expected response, fittedvalues.\\n\\n        Local change of expected mean given the change in the parameters as\\n        computed in d_params.\\n\\n        Notes\\n        -----\\n        This uses the one-step approximation of the parameter change to\\n        deleting one observation ``d_params``.\\n        '\n    params = np.asarray(self.results.params)\n    deriv = self.results.model._deriv_mean_dparams(params)\n    return (deriv * self.d_params).sum(1)",
            "@cache_readonly\ndef d_fittedvalues(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Change in expected response, fittedvalues.\\n\\n        Local change of expected mean given the change in the parameters as\\n        computed in d_params.\\n\\n        Notes\\n        -----\\n        This uses the one-step approximation of the parameter change to\\n        deleting one observation ``d_params``.\\n        '\n    params = np.asarray(self.results.params)\n    deriv = self.results.model._deriv_mean_dparams(params)\n    return (deriv * self.d_params).sum(1)",
            "@cache_readonly\ndef d_fittedvalues(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Change in expected response, fittedvalues.\\n\\n        Local change of expected mean given the change in the parameters as\\n        computed in d_params.\\n\\n        Notes\\n        -----\\n        This uses the one-step approximation of the parameter change to\\n        deleting one observation ``d_params``.\\n        '\n    params = np.asarray(self.results.params)\n    deriv = self.results.model._deriv_mean_dparams(params)\n    return (deriv * self.d_params).sum(1)",
            "@cache_readonly\ndef d_fittedvalues(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Change in expected response, fittedvalues.\\n\\n        Local change of expected mean given the change in the parameters as\\n        computed in d_params.\\n\\n        Notes\\n        -----\\n        This uses the one-step approximation of the parameter change to\\n        deleting one observation ``d_params``.\\n        '\n    params = np.asarray(self.results.params)\n    deriv = self.results.model._deriv_mean_dparams(params)\n    return (deriv * self.d_params).sum(1)",
            "@cache_readonly\ndef d_fittedvalues(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Change in expected response, fittedvalues.\\n\\n        Local change of expected mean given the change in the parameters as\\n        computed in d_params.\\n\\n        Notes\\n        -----\\n        This uses the one-step approximation of the parameter change to\\n        deleting one observation ``d_params``.\\n        '\n    params = np.asarray(self.results.params)\n    deriv = self.results.model._deriv_mean_dparams(params)\n    return (deriv * self.d_params).sum(1)"
        ]
    },
    {
        "func_name": "d_fittedvalues_scaled",
        "original": "@property\ndef d_fittedvalues_scaled(self):\n    \"\"\"\n        Change in fittedvalues scaled by standard errors.\n\n        This uses one-step approximation of the parameter change to deleting\n        one observation ``d_params``, and divides by the standard errors\n        for the predicted mean provided by results.get_prediction.\n        \"\"\"\n    return self.d_fittedvalues / self._get_prediction.se",
        "mutated": [
            "@property\ndef d_fittedvalues_scaled(self):\n    if False:\n        i = 10\n    '\\n        Change in fittedvalues scaled by standard errors.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for the predicted mean provided by results.get_prediction.\\n        '\n    return self.d_fittedvalues / self._get_prediction.se",
            "@property\ndef d_fittedvalues_scaled(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Change in fittedvalues scaled by standard errors.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for the predicted mean provided by results.get_prediction.\\n        '\n    return self.d_fittedvalues / self._get_prediction.se",
            "@property\ndef d_fittedvalues_scaled(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Change in fittedvalues scaled by standard errors.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for the predicted mean provided by results.get_prediction.\\n        '\n    return self.d_fittedvalues / self._get_prediction.se",
            "@property\ndef d_fittedvalues_scaled(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Change in fittedvalues scaled by standard errors.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for the predicted mean provided by results.get_prediction.\\n        '\n    return self.d_fittedvalues / self._get_prediction.se",
            "@property\ndef d_fittedvalues_scaled(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Change in fittedvalues scaled by standard errors.\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for the predicted mean provided by results.get_prediction.\\n        '\n    return self.d_fittedvalues / self._get_prediction.se"
        ]
    },
    {
        "func_name": "summary_frame",
        "original": "def summary_frame(self):\n    \"\"\"\n        Creates a DataFrame with influence results.\n\n        Returns\n        -------\n        frame : pandas DataFrame\n            A DataFrame with selected results for each observation.\n            The index will be the same as provided to the model.\n\n        Notes\n        -----\n        The resultant DataFrame contains six variables in addition to the\n        ``dfbetas``. These are:\n\n        * cooks_d : Cook's Distance defined in ``cooks_distance``\n        * standard_resid : Standardized residuals defined in\n          `resid_studentizedl`\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\n          `hat_matrix_diag`. Not included if None.\n        * dffits_internal : DFFITS statistics using internally Studentized\n          residuals defined in `d_fittedvalues_scaled`\n        \"\"\"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    if self.hat_matrix_diag is not None:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized, hat_diag=self.hat_matrix_diag, dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    else:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
        "mutated": [
            "def summary_frame(self):\n    if False:\n        i = 10\n    \"\\n        Creates a DataFrame with influence results.\\n\\n        Returns\\n        -------\\n        frame : pandas DataFrame\\n            A DataFrame with selected results for each observation.\\n            The index will be the same as provided to the model.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        ``dfbetas``. These are:\\n\\n        * cooks_d : Cook's Distance defined in ``cooks_distance``\\n        * standard_resid : Standardized residuals defined in\\n          `resid_studentizedl`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `hat_matrix_diag`. Not included if None.\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `d_fittedvalues_scaled`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    if self.hat_matrix_diag is not None:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized, hat_diag=self.hat_matrix_diag, dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    else:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Creates a DataFrame with influence results.\\n\\n        Returns\\n        -------\\n        frame : pandas DataFrame\\n            A DataFrame with selected results for each observation.\\n            The index will be the same as provided to the model.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        ``dfbetas``. These are:\\n\\n        * cooks_d : Cook's Distance defined in ``cooks_distance``\\n        * standard_resid : Standardized residuals defined in\\n          `resid_studentizedl`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `hat_matrix_diag`. Not included if None.\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `d_fittedvalues_scaled`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    if self.hat_matrix_diag is not None:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized, hat_diag=self.hat_matrix_diag, dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    else:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Creates a DataFrame with influence results.\\n\\n        Returns\\n        -------\\n        frame : pandas DataFrame\\n            A DataFrame with selected results for each observation.\\n            The index will be the same as provided to the model.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        ``dfbetas``. These are:\\n\\n        * cooks_d : Cook's Distance defined in ``cooks_distance``\\n        * standard_resid : Standardized residuals defined in\\n          `resid_studentizedl`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `hat_matrix_diag`. Not included if None.\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `d_fittedvalues_scaled`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    if self.hat_matrix_diag is not None:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized, hat_diag=self.hat_matrix_diag, dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    else:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Creates a DataFrame with influence results.\\n\\n        Returns\\n        -------\\n        frame : pandas DataFrame\\n            A DataFrame with selected results for each observation.\\n            The index will be the same as provided to the model.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        ``dfbetas``. These are:\\n\\n        * cooks_d : Cook's Distance defined in ``cooks_distance``\\n        * standard_resid : Standardized residuals defined in\\n          `resid_studentizedl`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `hat_matrix_diag`. Not included if None.\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `d_fittedvalues_scaled`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    if self.hat_matrix_diag is not None:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized, hat_diag=self.hat_matrix_diag, dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    else:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Creates a DataFrame with influence results.\\n\\n        Returns\\n        -------\\n        frame : pandas DataFrame\\n            A DataFrame with selected results for each observation.\\n            The index will be the same as provided to the model.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        ``dfbetas``. These are:\\n\\n        * cooks_d : Cook's Distance defined in ``cooks_distance``\\n        * standard_resid : Standardized residuals defined in\\n          `resid_studentizedl`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `hat_matrix_diag`. Not included if None.\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `d_fittedvalues_scaled`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    if self.hat_matrix_diag is not None:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized, hat_diag=self.hat_matrix_diag, dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    else:\n        summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], dffits_internal=self.d_fittedvalues_scaled), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, results):\n    self.results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.endog = results.model.endog\n    self.exog = results.model.exog\n    self.resid = results.resid\n    self.model_class = results.model.__class__\n    self.scale = results.mse_resid\n    self.aux_regression_exog = {}\n    self.aux_regression_endog = {}",
        "mutated": [
            "def __init__(self, results):\n    if False:\n        i = 10\n    self.results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.endog = results.model.endog\n    self.exog = results.model.exog\n    self.resid = results.resid\n    self.model_class = results.model.__class__\n    self.scale = results.mse_resid\n    self.aux_regression_exog = {}\n    self.aux_regression_endog = {}",
            "def __init__(self, results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.endog = results.model.endog\n    self.exog = results.model.exog\n    self.resid = results.resid\n    self.model_class = results.model.__class__\n    self.scale = results.mse_resid\n    self.aux_regression_exog = {}\n    self.aux_regression_endog = {}",
            "def __init__(self, results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.endog = results.model.endog\n    self.exog = results.model.exog\n    self.resid = results.resid\n    self.model_class = results.model.__class__\n    self.scale = results.mse_resid\n    self.aux_regression_exog = {}\n    self.aux_regression_endog = {}",
            "def __init__(self, results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.endog = results.model.endog\n    self.exog = results.model.exog\n    self.resid = results.resid\n    self.model_class = results.model.__class__\n    self.scale = results.mse_resid\n    self.aux_regression_exog = {}\n    self.aux_regression_endog = {}",
            "def __init__(self, results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.results = maybe_unwrap_results(results)\n    (self.nobs, self.k_vars) = results.model.exog.shape\n    self.endog = results.model.endog\n    self.exog = results.model.exog\n    self.resid = results.resid\n    self.model_class = results.model.__class__\n    self.scale = results.mse_resid\n    self.aux_regression_exog = {}\n    self.aux_regression_endog = {}"
        ]
    },
    {
        "func_name": "hat_matrix_diag",
        "original": "@cache_readonly\ndef hat_matrix_diag(self):\n    \"\"\"Diagonal of the hat_matrix for OLS\n\n        Notes\n        -----\n        temporarily calculated here, this should go to model class\n        \"\"\"\n    return (self.exog * self.results.model.pinv_wexog.T).sum(1)",
        "mutated": [
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n    'Diagonal of the hat_matrix for OLS\\n\\n        Notes\\n        -----\\n        temporarily calculated here, this should go to model class\\n        '\n    return (self.exog * self.results.model.pinv_wexog.T).sum(1)",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Diagonal of the hat_matrix for OLS\\n\\n        Notes\\n        -----\\n        temporarily calculated here, this should go to model class\\n        '\n    return (self.exog * self.results.model.pinv_wexog.T).sum(1)",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Diagonal of the hat_matrix for OLS\\n\\n        Notes\\n        -----\\n        temporarily calculated here, this should go to model class\\n        '\n    return (self.exog * self.results.model.pinv_wexog.T).sum(1)",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Diagonal of the hat_matrix for OLS\\n\\n        Notes\\n        -----\\n        temporarily calculated here, this should go to model class\\n        '\n    return (self.exog * self.results.model.pinv_wexog.T).sum(1)",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Diagonal of the hat_matrix for OLS\\n\\n        Notes\\n        -----\\n        temporarily calculated here, this should go to model class\\n        '\n    return (self.exog * self.results.model.pinv_wexog.T).sum(1)"
        ]
    },
    {
        "func_name": "resid_press",
        "original": "@cache_readonly\ndef resid_press(self):\n    \"\"\"PRESS residuals\n        \"\"\"\n    hii = self.hat_matrix_diag\n    return self.resid / (1 - hii)",
        "mutated": [
            "@cache_readonly\ndef resid_press(self):\n    if False:\n        i = 10\n    'PRESS residuals\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid / (1 - hii)",
            "@cache_readonly\ndef resid_press(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'PRESS residuals\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid / (1 - hii)",
            "@cache_readonly\ndef resid_press(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'PRESS residuals\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid / (1 - hii)",
            "@cache_readonly\ndef resid_press(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'PRESS residuals\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid / (1 - hii)",
            "@cache_readonly\ndef resid_press(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'PRESS residuals\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid / (1 - hii)"
        ]
    },
    {
        "func_name": "influence",
        "original": "@cache_readonly\ndef influence(self):\n    \"\"\"Influence measure\n\n        matches the influence measure that gretl reports\n        u * h / (1 - h)\n        where u are the residuals and h is the diagonal of the hat_matrix\n        \"\"\"\n    hii = self.hat_matrix_diag\n    return self.resid * hii / (1 - hii)",
        "mutated": [
            "@cache_readonly\ndef influence(self):\n    if False:\n        i = 10\n    'Influence measure\\n\\n        matches the influence measure that gretl reports\\n        u * h / (1 - h)\\n        where u are the residuals and h is the diagonal of the hat_matrix\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid * hii / (1 - hii)",
            "@cache_readonly\ndef influence(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Influence measure\\n\\n        matches the influence measure that gretl reports\\n        u * h / (1 - h)\\n        where u are the residuals and h is the diagonal of the hat_matrix\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid * hii / (1 - hii)",
            "@cache_readonly\ndef influence(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Influence measure\\n\\n        matches the influence measure that gretl reports\\n        u * h / (1 - h)\\n        where u are the residuals and h is the diagonal of the hat_matrix\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid * hii / (1 - hii)",
            "@cache_readonly\ndef influence(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Influence measure\\n\\n        matches the influence measure that gretl reports\\n        u * h / (1 - h)\\n        where u are the residuals and h is the diagonal of the hat_matrix\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid * hii / (1 - hii)",
            "@cache_readonly\ndef influence(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Influence measure\\n\\n        matches the influence measure that gretl reports\\n        u * h / (1 - h)\\n        where u are the residuals and h is the diagonal of the hat_matrix\\n        '\n    hii = self.hat_matrix_diag\n    return self.resid * hii / (1 - hii)"
        ]
    },
    {
        "func_name": "hat_diag_factor",
        "original": "@cache_readonly\ndef hat_diag_factor(self):\n    \"\"\"Factor of diagonal of hat_matrix used in influence\n\n        this might be useful for internal reuse\n        h / (1 - h)\n        \"\"\"\n    hii = self.hat_matrix_diag\n    return hii / (1 - hii)",
        "mutated": [
            "@cache_readonly\ndef hat_diag_factor(self):\n    if False:\n        i = 10\n    'Factor of diagonal of hat_matrix used in influence\\n\\n        this might be useful for internal reuse\\n        h / (1 - h)\\n        '\n    hii = self.hat_matrix_diag\n    return hii / (1 - hii)",
            "@cache_readonly\ndef hat_diag_factor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Factor of diagonal of hat_matrix used in influence\\n\\n        this might be useful for internal reuse\\n        h / (1 - h)\\n        '\n    hii = self.hat_matrix_diag\n    return hii / (1 - hii)",
            "@cache_readonly\ndef hat_diag_factor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Factor of diagonal of hat_matrix used in influence\\n\\n        this might be useful for internal reuse\\n        h / (1 - h)\\n        '\n    hii = self.hat_matrix_diag\n    return hii / (1 - hii)",
            "@cache_readonly\ndef hat_diag_factor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Factor of diagonal of hat_matrix used in influence\\n\\n        this might be useful for internal reuse\\n        h / (1 - h)\\n        '\n    hii = self.hat_matrix_diag\n    return hii / (1 - hii)",
            "@cache_readonly\ndef hat_diag_factor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Factor of diagonal of hat_matrix used in influence\\n\\n        this might be useful for internal reuse\\n        h / (1 - h)\\n        '\n    hii = self.hat_matrix_diag\n    return hii / (1 - hii)"
        ]
    },
    {
        "func_name": "ess_press",
        "original": "@cache_readonly\ndef ess_press(self):\n    \"\"\"Error sum of squares of PRESS residuals\n        \"\"\"\n    return np.dot(self.resid_press, self.resid_press)",
        "mutated": [
            "@cache_readonly\ndef ess_press(self):\n    if False:\n        i = 10\n    'Error sum of squares of PRESS residuals\\n        '\n    return np.dot(self.resid_press, self.resid_press)",
            "@cache_readonly\ndef ess_press(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Error sum of squares of PRESS residuals\\n        '\n    return np.dot(self.resid_press, self.resid_press)",
            "@cache_readonly\ndef ess_press(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Error sum of squares of PRESS residuals\\n        '\n    return np.dot(self.resid_press, self.resid_press)",
            "@cache_readonly\ndef ess_press(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Error sum of squares of PRESS residuals\\n        '\n    return np.dot(self.resid_press, self.resid_press)",
            "@cache_readonly\ndef ess_press(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Error sum of squares of PRESS residuals\\n        '\n    return np.dot(self.resid_press, self.resid_press)"
        ]
    },
    {
        "func_name": "resid_studentized",
        "original": "@cache_readonly\ndef resid_studentized(self):\n    \"\"\"Studentized residuals using variance from OLS\n\n        alias for resid_studentized_internal for compatibility with\n        MLEInfluence this uses sigma from original estimate and does\n        not require leave one out loop\n        \"\"\"\n    return self.resid_studentized_internal",
        "mutated": [
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n    'Studentized residuals using variance from OLS\\n\\n        alias for resid_studentized_internal for compatibility with\\n        MLEInfluence this uses sigma from original estimate and does\\n        not require leave one out loop\\n        '\n    return self.resid_studentized_internal",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Studentized residuals using variance from OLS\\n\\n        alias for resid_studentized_internal for compatibility with\\n        MLEInfluence this uses sigma from original estimate and does\\n        not require leave one out loop\\n        '\n    return self.resid_studentized_internal",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Studentized residuals using variance from OLS\\n\\n        alias for resid_studentized_internal for compatibility with\\n        MLEInfluence this uses sigma from original estimate and does\\n        not require leave one out loop\\n        '\n    return self.resid_studentized_internal",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Studentized residuals using variance from OLS\\n\\n        alias for resid_studentized_internal for compatibility with\\n        MLEInfluence this uses sigma from original estimate and does\\n        not require leave one out loop\\n        '\n    return self.resid_studentized_internal",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Studentized residuals using variance from OLS\\n\\n        alias for resid_studentized_internal for compatibility with\\n        MLEInfluence this uses sigma from original estimate and does\\n        not require leave one out loop\\n        '\n    return self.resid_studentized_internal"
        ]
    },
    {
        "func_name": "resid_studentized_internal",
        "original": "@cache_readonly\ndef resid_studentized_internal(self):\n    \"\"\"Studentized residuals using variance from OLS\n\n        this uses sigma from original estimate\n        does not require leave one out loop\n        \"\"\"\n    return self.get_resid_studentized_external(sigma=None)",
        "mutated": [
            "@cache_readonly\ndef resid_studentized_internal(self):\n    if False:\n        i = 10\n    'Studentized residuals using variance from OLS\\n\\n        this uses sigma from original estimate\\n        does not require leave one out loop\\n        '\n    return self.get_resid_studentized_external(sigma=None)",
            "@cache_readonly\ndef resid_studentized_internal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Studentized residuals using variance from OLS\\n\\n        this uses sigma from original estimate\\n        does not require leave one out loop\\n        '\n    return self.get_resid_studentized_external(sigma=None)",
            "@cache_readonly\ndef resid_studentized_internal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Studentized residuals using variance from OLS\\n\\n        this uses sigma from original estimate\\n        does not require leave one out loop\\n        '\n    return self.get_resid_studentized_external(sigma=None)",
            "@cache_readonly\ndef resid_studentized_internal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Studentized residuals using variance from OLS\\n\\n        this uses sigma from original estimate\\n        does not require leave one out loop\\n        '\n    return self.get_resid_studentized_external(sigma=None)",
            "@cache_readonly\ndef resid_studentized_internal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Studentized residuals using variance from OLS\\n\\n        this uses sigma from original estimate\\n        does not require leave one out loop\\n        '\n    return self.get_resid_studentized_external(sigma=None)"
        ]
    },
    {
        "func_name": "resid_studentized_external",
        "original": "@cache_readonly\ndef resid_studentized_external(self):\n    \"\"\"Studentized residuals using LOOO variance\n\n        this uses sigma from leave-one-out estimates\n\n        requires leave one out loop for observations\n        \"\"\"\n    sigma_looo = np.sqrt(self.sigma2_not_obsi)\n    return self.get_resid_studentized_external(sigma=sigma_looo)",
        "mutated": [
            "@cache_readonly\ndef resid_studentized_external(self):\n    if False:\n        i = 10\n    'Studentized residuals using LOOO variance\\n\\n        this uses sigma from leave-one-out estimates\\n\\n        requires leave one out loop for observations\\n        '\n    sigma_looo = np.sqrt(self.sigma2_not_obsi)\n    return self.get_resid_studentized_external(sigma=sigma_looo)",
            "@cache_readonly\ndef resid_studentized_external(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Studentized residuals using LOOO variance\\n\\n        this uses sigma from leave-one-out estimates\\n\\n        requires leave one out loop for observations\\n        '\n    sigma_looo = np.sqrt(self.sigma2_not_obsi)\n    return self.get_resid_studentized_external(sigma=sigma_looo)",
            "@cache_readonly\ndef resid_studentized_external(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Studentized residuals using LOOO variance\\n\\n        this uses sigma from leave-one-out estimates\\n\\n        requires leave one out loop for observations\\n        '\n    sigma_looo = np.sqrt(self.sigma2_not_obsi)\n    return self.get_resid_studentized_external(sigma=sigma_looo)",
            "@cache_readonly\ndef resid_studentized_external(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Studentized residuals using LOOO variance\\n\\n        this uses sigma from leave-one-out estimates\\n\\n        requires leave one out loop for observations\\n        '\n    sigma_looo = np.sqrt(self.sigma2_not_obsi)\n    return self.get_resid_studentized_external(sigma=sigma_looo)",
            "@cache_readonly\ndef resid_studentized_external(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Studentized residuals using LOOO variance\\n\\n        this uses sigma from leave-one-out estimates\\n\\n        requires leave one out loop for observations\\n        '\n    sigma_looo = np.sqrt(self.sigma2_not_obsi)\n    return self.get_resid_studentized_external(sigma=sigma_looo)"
        ]
    },
    {
        "func_name": "get_resid_studentized_external",
        "original": "def get_resid_studentized_external(self, sigma=None):\n    \"\"\"calculate studentized residuals\n\n        Parameters\n        ----------\n        sigma : None or float\n            estimate of the standard deviation of the residuals. If None, then\n            the estimate from the regression results is used.\n\n        Returns\n        -------\n        stzd_resid : ndarray\n            studentized residuals\n\n        Notes\n        -----\n        studentized residuals are defined as ::\n\n           resid / sigma / np.sqrt(1 - hii)\n\n        where resid are the residuals from the regression, sigma is an\n        estimate of the standard deviation of the residuals, and hii is the\n        diagonal of the hat_matrix.\n        \"\"\"\n    hii = self.hat_matrix_diag\n    if sigma is None:\n        sigma2_est = self.scale\n        sigma = np.sqrt(sigma2_est)\n    return self.resid / sigma / np.sqrt(1 - hii)",
        "mutated": [
            "def get_resid_studentized_external(self, sigma=None):\n    if False:\n        i = 10\n    'calculate studentized residuals\\n\\n        Parameters\\n        ----------\\n        sigma : None or float\\n            estimate of the standard deviation of the residuals. If None, then\\n            the estimate from the regression results is used.\\n\\n        Returns\\n        -------\\n        stzd_resid : ndarray\\n            studentized residuals\\n\\n        Notes\\n        -----\\n        studentized residuals are defined as ::\\n\\n           resid / sigma / np.sqrt(1 - hii)\\n\\n        where resid are the residuals from the regression, sigma is an\\n        estimate of the standard deviation of the residuals, and hii is the\\n        diagonal of the hat_matrix.\\n        '\n    hii = self.hat_matrix_diag\n    if sigma is None:\n        sigma2_est = self.scale\n        sigma = np.sqrt(sigma2_est)\n    return self.resid / sigma / np.sqrt(1 - hii)",
            "def get_resid_studentized_external(self, sigma=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'calculate studentized residuals\\n\\n        Parameters\\n        ----------\\n        sigma : None or float\\n            estimate of the standard deviation of the residuals. If None, then\\n            the estimate from the regression results is used.\\n\\n        Returns\\n        -------\\n        stzd_resid : ndarray\\n            studentized residuals\\n\\n        Notes\\n        -----\\n        studentized residuals are defined as ::\\n\\n           resid / sigma / np.sqrt(1 - hii)\\n\\n        where resid are the residuals from the regression, sigma is an\\n        estimate of the standard deviation of the residuals, and hii is the\\n        diagonal of the hat_matrix.\\n        '\n    hii = self.hat_matrix_diag\n    if sigma is None:\n        sigma2_est = self.scale\n        sigma = np.sqrt(sigma2_est)\n    return self.resid / sigma / np.sqrt(1 - hii)",
            "def get_resid_studentized_external(self, sigma=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'calculate studentized residuals\\n\\n        Parameters\\n        ----------\\n        sigma : None or float\\n            estimate of the standard deviation of the residuals. If None, then\\n            the estimate from the regression results is used.\\n\\n        Returns\\n        -------\\n        stzd_resid : ndarray\\n            studentized residuals\\n\\n        Notes\\n        -----\\n        studentized residuals are defined as ::\\n\\n           resid / sigma / np.sqrt(1 - hii)\\n\\n        where resid are the residuals from the regression, sigma is an\\n        estimate of the standard deviation of the residuals, and hii is the\\n        diagonal of the hat_matrix.\\n        '\n    hii = self.hat_matrix_diag\n    if sigma is None:\n        sigma2_est = self.scale\n        sigma = np.sqrt(sigma2_est)\n    return self.resid / sigma / np.sqrt(1 - hii)",
            "def get_resid_studentized_external(self, sigma=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'calculate studentized residuals\\n\\n        Parameters\\n        ----------\\n        sigma : None or float\\n            estimate of the standard deviation of the residuals. If None, then\\n            the estimate from the regression results is used.\\n\\n        Returns\\n        -------\\n        stzd_resid : ndarray\\n            studentized residuals\\n\\n        Notes\\n        -----\\n        studentized residuals are defined as ::\\n\\n           resid / sigma / np.sqrt(1 - hii)\\n\\n        where resid are the residuals from the regression, sigma is an\\n        estimate of the standard deviation of the residuals, and hii is the\\n        diagonal of the hat_matrix.\\n        '\n    hii = self.hat_matrix_diag\n    if sigma is None:\n        sigma2_est = self.scale\n        sigma = np.sqrt(sigma2_est)\n    return self.resid / sigma / np.sqrt(1 - hii)",
            "def get_resid_studentized_external(self, sigma=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'calculate studentized residuals\\n\\n        Parameters\\n        ----------\\n        sigma : None or float\\n            estimate of the standard deviation of the residuals. If None, then\\n            the estimate from the regression results is used.\\n\\n        Returns\\n        -------\\n        stzd_resid : ndarray\\n            studentized residuals\\n\\n        Notes\\n        -----\\n        studentized residuals are defined as ::\\n\\n           resid / sigma / np.sqrt(1 - hii)\\n\\n        where resid are the residuals from the regression, sigma is an\\n        estimate of the standard deviation of the residuals, and hii is the\\n        diagonal of the hat_matrix.\\n        '\n    hii = self.hat_matrix_diag\n    if sigma is None:\n        sigma2_est = self.scale\n        sigma = np.sqrt(sigma2_est)\n    return self.resid / sigma / np.sqrt(1 - hii)"
        ]
    },
    {
        "func_name": "cooks_distance",
        "original": "@cache_readonly\ndef cooks_distance(self):\n    \"\"\"\n        Cooks distance\n\n        Uses original results, no nobs loop\n\n        References\n        ----------\n        .. [*] Eubank, R. L. (1999). Nonparametric regression and spline\n            smoothing. CRC press.\n        .. [*] Cook's distance. (n.d.). In Wikipedia. July 2019, from\n            https://en.wikipedia.org/wiki/Cook%27s_distance\n        \"\"\"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
        "mutated": [
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n    \"\\n        Cooks distance\\n\\n        Uses original results, no nobs loop\\n\\n        References\\n        ----------\\n        .. [*] Eubank, R. L. (1999). Nonparametric regression and spline\\n            smoothing. CRC press.\\n        .. [*] Cook's distance. (n.d.). In Wikipedia. July 2019, from\\n            https://en.wikipedia.org/wiki/Cook%27s_distance\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Cooks distance\\n\\n        Uses original results, no nobs loop\\n\\n        References\\n        ----------\\n        .. [*] Eubank, R. L. (1999). Nonparametric regression and spline\\n            smoothing. CRC press.\\n        .. [*] Cook's distance. (n.d.). In Wikipedia. July 2019, from\\n            https://en.wikipedia.org/wiki/Cook%27s_distance\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Cooks distance\\n\\n        Uses original results, no nobs loop\\n\\n        References\\n        ----------\\n        .. [*] Eubank, R. L. (1999). Nonparametric regression and spline\\n            smoothing. CRC press.\\n        .. [*] Cook's distance. (n.d.). In Wikipedia. July 2019, from\\n            https://en.wikipedia.org/wiki/Cook%27s_distance\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Cooks distance\\n\\n        Uses original results, no nobs loop\\n\\n        References\\n        ----------\\n        .. [*] Eubank, R. L. (1999). Nonparametric regression and spline\\n            smoothing. CRC press.\\n        .. [*] Cook's distance. (n.d.). In Wikipedia. July 2019, from\\n            https://en.wikipedia.org/wiki/Cook%27s_distance\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Cooks distance\\n\\n        Uses original results, no nobs loop\\n\\n        References\\n        ----------\\n        .. [*] Eubank, R. L. (1999). Nonparametric regression and spline\\n            smoothing. CRC press.\\n        .. [*] Cook's distance. (n.d.). In Wikipedia. July 2019, from\\n            https://en.wikipedia.org/wiki/Cook%27s_distance\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)"
        ]
    },
    {
        "func_name": "dffits_internal",
        "original": "@cache_readonly\ndef dffits_internal(self):\n    \"\"\"dffits measure for influence of an observation\n\n        based on resid_studentized_internal\n        uses original results, no nobs loop\n        \"\"\"\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_internal * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
        "mutated": [
            "@cache_readonly\ndef dffits_internal(self):\n    if False:\n        i = 10\n    'dffits measure for influence of an observation\\n\\n        based on resid_studentized_internal\\n        uses original results, no nobs loop\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_internal * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
            "@cache_readonly\ndef dffits_internal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'dffits measure for influence of an observation\\n\\n        based on resid_studentized_internal\\n        uses original results, no nobs loop\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_internal * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
            "@cache_readonly\ndef dffits_internal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'dffits measure for influence of an observation\\n\\n        based on resid_studentized_internal\\n        uses original results, no nobs loop\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_internal * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
            "@cache_readonly\ndef dffits_internal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'dffits measure for influence of an observation\\n\\n        based on resid_studentized_internal\\n        uses original results, no nobs loop\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_internal * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
            "@cache_readonly\ndef dffits_internal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'dffits measure for influence of an observation\\n\\n        based on resid_studentized_internal\\n        uses original results, no nobs loop\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_internal * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)"
        ]
    },
    {
        "func_name": "dffits",
        "original": "@cache_readonly\ndef dffits(self):\n    \"\"\"\n        dffits measure for influence of an observation\n\n        based on resid_studentized_external,\n        uses results from leave-one-observation-out loop\n\n        It is recommended that observations with dffits large than a\n        threshold of 2 sqrt{k / n} where k is the number of parameters, should\n        be investigated.\n\n        Returns\n        -------\n        dffits : float\n        dffits_threshold : float\n\n        References\n        ----------\n        `Wikipedia <https://en.wikipedia.org/wiki/DFFITS>`_\n        \"\"\"\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_external * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
        "mutated": [
            "@cache_readonly\ndef dffits(self):\n    if False:\n        i = 10\n    '\\n        dffits measure for influence of an observation\\n\\n        based on resid_studentized_external,\\n        uses results from leave-one-observation-out loop\\n\\n        It is recommended that observations with dffits large than a\\n        threshold of 2 sqrt{k / n} where k is the number of parameters, should\\n        be investigated.\\n\\n        Returns\\n        -------\\n        dffits : float\\n        dffits_threshold : float\\n\\n        References\\n        ----------\\n        `Wikipedia <https://en.wikipedia.org/wiki/DFFITS>`_\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_external * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
            "@cache_readonly\ndef dffits(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        dffits measure for influence of an observation\\n\\n        based on resid_studentized_external,\\n        uses results from leave-one-observation-out loop\\n\\n        It is recommended that observations with dffits large than a\\n        threshold of 2 sqrt{k / n} where k is the number of parameters, should\\n        be investigated.\\n\\n        Returns\\n        -------\\n        dffits : float\\n        dffits_threshold : float\\n\\n        References\\n        ----------\\n        `Wikipedia <https://en.wikipedia.org/wiki/DFFITS>`_\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_external * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
            "@cache_readonly\ndef dffits(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        dffits measure for influence of an observation\\n\\n        based on resid_studentized_external,\\n        uses results from leave-one-observation-out loop\\n\\n        It is recommended that observations with dffits large than a\\n        threshold of 2 sqrt{k / n} where k is the number of parameters, should\\n        be investigated.\\n\\n        Returns\\n        -------\\n        dffits : float\\n        dffits_threshold : float\\n\\n        References\\n        ----------\\n        `Wikipedia <https://en.wikipedia.org/wiki/DFFITS>`_\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_external * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
            "@cache_readonly\ndef dffits(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        dffits measure for influence of an observation\\n\\n        based on resid_studentized_external,\\n        uses results from leave-one-observation-out loop\\n\\n        It is recommended that observations with dffits large than a\\n        threshold of 2 sqrt{k / n} where k is the number of parameters, should\\n        be investigated.\\n\\n        Returns\\n        -------\\n        dffits : float\\n        dffits_threshold : float\\n\\n        References\\n        ----------\\n        `Wikipedia <https://en.wikipedia.org/wiki/DFFITS>`_\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_external * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)",
            "@cache_readonly\ndef dffits(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        dffits measure for influence of an observation\\n\\n        based on resid_studentized_external,\\n        uses results from leave-one-observation-out loop\\n\\n        It is recommended that observations with dffits large than a\\n        threshold of 2 sqrt{k / n} where k is the number of parameters, should\\n        be investigated.\\n\\n        Returns\\n        -------\\n        dffits : float\\n        dffits_threshold : float\\n\\n        References\\n        ----------\\n        `Wikipedia <https://en.wikipedia.org/wiki/DFFITS>`_\\n        '\n    hii = self.hat_matrix_diag\n    dffits_ = self.resid_studentized_external * np.sqrt(hii / (1 - hii))\n    dffits_threshold = 2 * np.sqrt(self.k_vars * 1.0 / self.nobs)\n    return (dffits_, dffits_threshold)"
        ]
    },
    {
        "func_name": "dfbetas",
        "original": "@cache_readonly\ndef dfbetas(self):\n    \"\"\"dfbetas\n\n        uses results from leave-one-observation-out loop\n        \"\"\"\n    dfbetas = self.results.params - self.params_not_obsi\n    dfbetas /= np.sqrt(self.sigma2_not_obsi[:, None])\n    dfbetas /= np.sqrt(np.diag(self.results.normalized_cov_params))\n    return dfbetas",
        "mutated": [
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbetas = self.results.params - self.params_not_obsi\n    dfbetas /= np.sqrt(self.sigma2_not_obsi[:, None])\n    dfbetas /= np.sqrt(np.diag(self.results.normalized_cov_params))\n    return dfbetas",
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbetas = self.results.params - self.params_not_obsi\n    dfbetas /= np.sqrt(self.sigma2_not_obsi[:, None])\n    dfbetas /= np.sqrt(np.diag(self.results.normalized_cov_params))\n    return dfbetas",
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbetas = self.results.params - self.params_not_obsi\n    dfbetas /= np.sqrt(self.sigma2_not_obsi[:, None])\n    dfbetas /= np.sqrt(np.diag(self.results.normalized_cov_params))\n    return dfbetas",
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbetas = self.results.params - self.params_not_obsi\n    dfbetas /= np.sqrt(self.sigma2_not_obsi[:, None])\n    dfbetas /= np.sqrt(np.diag(self.results.normalized_cov_params))\n    return dfbetas",
            "@cache_readonly\ndef dfbetas(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbetas = self.results.params - self.params_not_obsi\n    dfbetas /= np.sqrt(self.sigma2_not_obsi[:, None])\n    dfbetas /= np.sqrt(np.diag(self.results.normalized_cov_params))\n    return dfbetas"
        ]
    },
    {
        "func_name": "dfbeta",
        "original": "@cache_readonly\ndef dfbeta(self):\n    \"\"\"dfbetas\n\n        uses results from leave-one-observation-out loop\n        \"\"\"\n    dfbeta = self.results.params - self.params_not_obsi\n    return dfbeta",
        "mutated": [
            "@cache_readonly\ndef dfbeta(self):\n    if False:\n        i = 10\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbeta = self.results.params - self.params_not_obsi\n    return dfbeta",
            "@cache_readonly\ndef dfbeta(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbeta = self.results.params - self.params_not_obsi\n    return dfbeta",
            "@cache_readonly\ndef dfbeta(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbeta = self.results.params - self.params_not_obsi\n    return dfbeta",
            "@cache_readonly\ndef dfbeta(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbeta = self.results.params - self.params_not_obsi\n    return dfbeta",
            "@cache_readonly\ndef dfbeta(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'dfbetas\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    dfbeta = self.results.params - self.params_not_obsi\n    return dfbeta"
        ]
    },
    {
        "func_name": "sigma2_not_obsi",
        "original": "@cache_readonly\ndef sigma2_not_obsi(self):\n    \"\"\"error variance for all LOOO regressions\n\n        This is 'mse_resid' from each auxiliary regression.\n\n        uses results from leave-one-observation-out loop\n        \"\"\"\n    return np.asarray(self._res_looo['mse_resid'])",
        "mutated": [
            "@cache_readonly\ndef sigma2_not_obsi(self):\n    if False:\n        i = 10\n    \"error variance for all LOOO regressions\\n\\n        This is 'mse_resid' from each auxiliary regression.\\n\\n        uses results from leave-one-observation-out loop\\n        \"\n    return np.asarray(self._res_looo['mse_resid'])",
            "@cache_readonly\ndef sigma2_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"error variance for all LOOO regressions\\n\\n        This is 'mse_resid' from each auxiliary regression.\\n\\n        uses results from leave-one-observation-out loop\\n        \"\n    return np.asarray(self._res_looo['mse_resid'])",
            "@cache_readonly\ndef sigma2_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"error variance for all LOOO regressions\\n\\n        This is 'mse_resid' from each auxiliary regression.\\n\\n        uses results from leave-one-observation-out loop\\n        \"\n    return np.asarray(self._res_looo['mse_resid'])",
            "@cache_readonly\ndef sigma2_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"error variance for all LOOO regressions\\n\\n        This is 'mse_resid' from each auxiliary regression.\\n\\n        uses results from leave-one-observation-out loop\\n        \"\n    return np.asarray(self._res_looo['mse_resid'])",
            "@cache_readonly\ndef sigma2_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"error variance for all LOOO regressions\\n\\n        This is 'mse_resid' from each auxiliary regression.\\n\\n        uses results from leave-one-observation-out loop\\n        \"\n    return np.asarray(self._res_looo['mse_resid'])"
        ]
    },
    {
        "func_name": "params_not_obsi",
        "original": "@property\ndef params_not_obsi(self):\n    \"\"\"parameter estimates for all LOOO regressions\n\n        uses results from leave-one-observation-out loop\n        \"\"\"\n    return np.asarray(self._res_looo['params'])",
        "mutated": [
            "@property\ndef params_not_obsi(self):\n    if False:\n        i = 10\n    'parameter estimates for all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['params'])",
            "@property\ndef params_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'parameter estimates for all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['params'])",
            "@property\ndef params_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'parameter estimates for all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['params'])",
            "@property\ndef params_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'parameter estimates for all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['params'])",
            "@property\ndef params_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'parameter estimates for all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['params'])"
        ]
    },
    {
        "func_name": "det_cov_params_not_obsi",
        "original": "@property\ndef det_cov_params_not_obsi(self):\n    \"\"\"determinant of cov_params of all LOOO regressions\n\n        uses results from leave-one-observation-out loop\n        \"\"\"\n    return np.asarray(self._res_looo['det_cov_params'])",
        "mutated": [
            "@property\ndef det_cov_params_not_obsi(self):\n    if False:\n        i = 10\n    'determinant of cov_params of all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['det_cov_params'])",
            "@property\ndef det_cov_params_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'determinant of cov_params of all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['det_cov_params'])",
            "@property\ndef det_cov_params_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'determinant of cov_params of all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['det_cov_params'])",
            "@property\ndef det_cov_params_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'determinant of cov_params of all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['det_cov_params'])",
            "@property\ndef det_cov_params_not_obsi(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'determinant of cov_params of all LOOO regressions\\n\\n        uses results from leave-one-observation-out loop\\n        '\n    return np.asarray(self._res_looo['det_cov_params'])"
        ]
    },
    {
        "func_name": "cov_ratio",
        "original": "@cache_readonly\ndef cov_ratio(self):\n    \"\"\"covariance ratio between LOOO and original\n\n        This uses determinant of the estimate of the parameter covariance\n        from leave-one-out estimates.\n        requires leave one out loop for observations\n        \"\"\"\n    cov_ratio = self.det_cov_params_not_obsi / np.linalg.det(self.results.cov_params())\n    return cov_ratio",
        "mutated": [
            "@cache_readonly\ndef cov_ratio(self):\n    if False:\n        i = 10\n    'covariance ratio between LOOO and original\\n\\n        This uses determinant of the estimate of the parameter covariance\\n        from leave-one-out estimates.\\n        requires leave one out loop for observations\\n        '\n    cov_ratio = self.det_cov_params_not_obsi / np.linalg.det(self.results.cov_params())\n    return cov_ratio",
            "@cache_readonly\ndef cov_ratio(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'covariance ratio between LOOO and original\\n\\n        This uses determinant of the estimate of the parameter covariance\\n        from leave-one-out estimates.\\n        requires leave one out loop for observations\\n        '\n    cov_ratio = self.det_cov_params_not_obsi / np.linalg.det(self.results.cov_params())\n    return cov_ratio",
            "@cache_readonly\ndef cov_ratio(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'covariance ratio between LOOO and original\\n\\n        This uses determinant of the estimate of the parameter covariance\\n        from leave-one-out estimates.\\n        requires leave one out loop for observations\\n        '\n    cov_ratio = self.det_cov_params_not_obsi / np.linalg.det(self.results.cov_params())\n    return cov_ratio",
            "@cache_readonly\ndef cov_ratio(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'covariance ratio between LOOO and original\\n\\n        This uses determinant of the estimate of the parameter covariance\\n        from leave-one-out estimates.\\n        requires leave one out loop for observations\\n        '\n    cov_ratio = self.det_cov_params_not_obsi / np.linalg.det(self.results.cov_params())\n    return cov_ratio",
            "@cache_readonly\ndef cov_ratio(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'covariance ratio between LOOO and original\\n\\n        This uses determinant of the estimate of the parameter covariance\\n        from leave-one-out estimates.\\n        requires leave one out loop for observations\\n        '\n    cov_ratio = self.det_cov_params_not_obsi / np.linalg.det(self.results.cov_params())\n    return cov_ratio"
        ]
    },
    {
        "func_name": "resid_var",
        "original": "@cache_readonly\ndef resid_var(self):\n    \"\"\"estimate of variance of the residuals\n\n        ::\n\n           sigma2 = sigma2_OLS * (1 - hii)\n\n        where hii is the diagonal of the hat matrix\n        \"\"\"\n    return self.scale * (1 - self.hat_matrix_diag)",
        "mutated": [
            "@cache_readonly\ndef resid_var(self):\n    if False:\n        i = 10\n    'estimate of variance of the residuals\\n\\n        ::\\n\\n           sigma2 = sigma2_OLS * (1 - hii)\\n\\n        where hii is the diagonal of the hat matrix\\n        '\n    return self.scale * (1 - self.hat_matrix_diag)",
            "@cache_readonly\ndef resid_var(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'estimate of variance of the residuals\\n\\n        ::\\n\\n           sigma2 = sigma2_OLS * (1 - hii)\\n\\n        where hii is the diagonal of the hat matrix\\n        '\n    return self.scale * (1 - self.hat_matrix_diag)",
            "@cache_readonly\ndef resid_var(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'estimate of variance of the residuals\\n\\n        ::\\n\\n           sigma2 = sigma2_OLS * (1 - hii)\\n\\n        where hii is the diagonal of the hat matrix\\n        '\n    return self.scale * (1 - self.hat_matrix_diag)",
            "@cache_readonly\ndef resid_var(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'estimate of variance of the residuals\\n\\n        ::\\n\\n           sigma2 = sigma2_OLS * (1 - hii)\\n\\n        where hii is the diagonal of the hat matrix\\n        '\n    return self.scale * (1 - self.hat_matrix_diag)",
            "@cache_readonly\ndef resid_var(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'estimate of variance of the residuals\\n\\n        ::\\n\\n           sigma2 = sigma2_OLS * (1 - hii)\\n\\n        where hii is the diagonal of the hat matrix\\n        '\n    return self.scale * (1 - self.hat_matrix_diag)"
        ]
    },
    {
        "func_name": "resid_std",
        "original": "@cache_readonly\ndef resid_std(self):\n    \"\"\"estimate of standard deviation of the residuals\n\n        See Also\n        --------\n        resid_var\n        \"\"\"\n    return np.sqrt(self.resid_var)",
        "mutated": [
            "@cache_readonly\ndef resid_std(self):\n    if False:\n        i = 10\n    'estimate of standard deviation of the residuals\\n\\n        See Also\\n        --------\\n        resid_var\\n        '\n    return np.sqrt(self.resid_var)",
            "@cache_readonly\ndef resid_std(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'estimate of standard deviation of the residuals\\n\\n        See Also\\n        --------\\n        resid_var\\n        '\n    return np.sqrt(self.resid_var)",
            "@cache_readonly\ndef resid_std(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'estimate of standard deviation of the residuals\\n\\n        See Also\\n        --------\\n        resid_var\\n        '\n    return np.sqrt(self.resid_var)",
            "@cache_readonly\ndef resid_std(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'estimate of standard deviation of the residuals\\n\\n        See Also\\n        --------\\n        resid_var\\n        '\n    return np.sqrt(self.resid_var)",
            "@cache_readonly\ndef resid_std(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'estimate of standard deviation of the residuals\\n\\n        See Also\\n        --------\\n        resid_var\\n        '\n    return np.sqrt(self.resid_var)"
        ]
    },
    {
        "func_name": "_ols_xnoti",
        "original": "def _ols_xnoti(self, drop_idx, endog_idx='endog', store=True):\n    \"\"\"regression results from LOVO auxiliary regression with cache\n\n\n        The result instances are stored, which could use a large amount of\n        memory if the datasets are large. There are too many combinations to\n        store them all, except for small problems.\n\n        Parameters\n        ----------\n        drop_idx : int\n            index of exog that is dropped from the regression\n        endog_idx : 'endog' or int\n            If 'endog', then the endogenous variable of the result instance\n            is regressed on the exogenous variables, excluding the one at\n            drop_idx. If endog_idx is an integer, then the exog with that\n            index is regressed with OLS on all other exogenous variables.\n            (The latter is the auxiliary regression for the variance inflation\n            factor.)\n\n        this needs more thought, memory versus speed\n        not yet used in any other parts, not sufficiently tested\n        \"\"\"\n    if endog_idx == 'endog':\n        stored = self.aux_regression_endog\n        if hasattr(stored, drop_idx):\n            return stored[drop_idx]\n        x_i = self.results.model.endog\n    else:\n        try:\n            self.aux_regression_exog[endog_idx][drop_idx]\n        except KeyError:\n            pass\n        stored = self.aux_regression_exog[endog_idx]\n        stored = {}\n        x_i = self.exog[:, endog_idx]\n    k_vars = self.exog.shape[1]\n    mask = np.arange(k_vars) != drop_idx\n    x_noti = self.exog[:, mask]\n    res = OLS(x_i, x_noti).fit()\n    if store:\n        stored[drop_idx] = res\n    return res",
        "mutated": [
            "def _ols_xnoti(self, drop_idx, endog_idx='endog', store=True):\n    if False:\n        i = 10\n    \"regression results from LOVO auxiliary regression with cache\\n\\n\\n        The result instances are stored, which could use a large amount of\\n        memory if the datasets are large. There are too many combinations to\\n        store them all, except for small problems.\\n\\n        Parameters\\n        ----------\\n        drop_idx : int\\n            index of exog that is dropped from the regression\\n        endog_idx : 'endog' or int\\n            If 'endog', then the endogenous variable of the result instance\\n            is regressed on the exogenous variables, excluding the one at\\n            drop_idx. If endog_idx is an integer, then the exog with that\\n            index is regressed with OLS on all other exogenous variables.\\n            (The latter is the auxiliary regression for the variance inflation\\n            factor.)\\n\\n        this needs more thought, memory versus speed\\n        not yet used in any other parts, not sufficiently tested\\n        \"\n    if endog_idx == 'endog':\n        stored = self.aux_regression_endog\n        if hasattr(stored, drop_idx):\n            return stored[drop_idx]\n        x_i = self.results.model.endog\n    else:\n        try:\n            self.aux_regression_exog[endog_idx][drop_idx]\n        except KeyError:\n            pass\n        stored = self.aux_regression_exog[endog_idx]\n        stored = {}\n        x_i = self.exog[:, endog_idx]\n    k_vars = self.exog.shape[1]\n    mask = np.arange(k_vars) != drop_idx\n    x_noti = self.exog[:, mask]\n    res = OLS(x_i, x_noti).fit()\n    if store:\n        stored[drop_idx] = res\n    return res",
            "def _ols_xnoti(self, drop_idx, endog_idx='endog', store=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"regression results from LOVO auxiliary regression with cache\\n\\n\\n        The result instances are stored, which could use a large amount of\\n        memory if the datasets are large. There are too many combinations to\\n        store them all, except for small problems.\\n\\n        Parameters\\n        ----------\\n        drop_idx : int\\n            index of exog that is dropped from the regression\\n        endog_idx : 'endog' or int\\n            If 'endog', then the endogenous variable of the result instance\\n            is regressed on the exogenous variables, excluding the one at\\n            drop_idx. If endog_idx is an integer, then the exog with that\\n            index is regressed with OLS on all other exogenous variables.\\n            (The latter is the auxiliary regression for the variance inflation\\n            factor.)\\n\\n        this needs more thought, memory versus speed\\n        not yet used in any other parts, not sufficiently tested\\n        \"\n    if endog_idx == 'endog':\n        stored = self.aux_regression_endog\n        if hasattr(stored, drop_idx):\n            return stored[drop_idx]\n        x_i = self.results.model.endog\n    else:\n        try:\n            self.aux_regression_exog[endog_idx][drop_idx]\n        except KeyError:\n            pass\n        stored = self.aux_regression_exog[endog_idx]\n        stored = {}\n        x_i = self.exog[:, endog_idx]\n    k_vars = self.exog.shape[1]\n    mask = np.arange(k_vars) != drop_idx\n    x_noti = self.exog[:, mask]\n    res = OLS(x_i, x_noti).fit()\n    if store:\n        stored[drop_idx] = res\n    return res",
            "def _ols_xnoti(self, drop_idx, endog_idx='endog', store=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"regression results from LOVO auxiliary regression with cache\\n\\n\\n        The result instances are stored, which could use a large amount of\\n        memory if the datasets are large. There are too many combinations to\\n        store them all, except for small problems.\\n\\n        Parameters\\n        ----------\\n        drop_idx : int\\n            index of exog that is dropped from the regression\\n        endog_idx : 'endog' or int\\n            If 'endog', then the endogenous variable of the result instance\\n            is regressed on the exogenous variables, excluding the one at\\n            drop_idx. If endog_idx is an integer, then the exog with that\\n            index is regressed with OLS on all other exogenous variables.\\n            (The latter is the auxiliary regression for the variance inflation\\n            factor.)\\n\\n        this needs more thought, memory versus speed\\n        not yet used in any other parts, not sufficiently tested\\n        \"\n    if endog_idx == 'endog':\n        stored = self.aux_regression_endog\n        if hasattr(stored, drop_idx):\n            return stored[drop_idx]\n        x_i = self.results.model.endog\n    else:\n        try:\n            self.aux_regression_exog[endog_idx][drop_idx]\n        except KeyError:\n            pass\n        stored = self.aux_regression_exog[endog_idx]\n        stored = {}\n        x_i = self.exog[:, endog_idx]\n    k_vars = self.exog.shape[1]\n    mask = np.arange(k_vars) != drop_idx\n    x_noti = self.exog[:, mask]\n    res = OLS(x_i, x_noti).fit()\n    if store:\n        stored[drop_idx] = res\n    return res",
            "def _ols_xnoti(self, drop_idx, endog_idx='endog', store=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"regression results from LOVO auxiliary regression with cache\\n\\n\\n        The result instances are stored, which could use a large amount of\\n        memory if the datasets are large. There are too many combinations to\\n        store them all, except for small problems.\\n\\n        Parameters\\n        ----------\\n        drop_idx : int\\n            index of exog that is dropped from the regression\\n        endog_idx : 'endog' or int\\n            If 'endog', then the endogenous variable of the result instance\\n            is regressed on the exogenous variables, excluding the one at\\n            drop_idx. If endog_idx is an integer, then the exog with that\\n            index is regressed with OLS on all other exogenous variables.\\n            (The latter is the auxiliary regression for the variance inflation\\n            factor.)\\n\\n        this needs more thought, memory versus speed\\n        not yet used in any other parts, not sufficiently tested\\n        \"\n    if endog_idx == 'endog':\n        stored = self.aux_regression_endog\n        if hasattr(stored, drop_idx):\n            return stored[drop_idx]\n        x_i = self.results.model.endog\n    else:\n        try:\n            self.aux_regression_exog[endog_idx][drop_idx]\n        except KeyError:\n            pass\n        stored = self.aux_regression_exog[endog_idx]\n        stored = {}\n        x_i = self.exog[:, endog_idx]\n    k_vars = self.exog.shape[1]\n    mask = np.arange(k_vars) != drop_idx\n    x_noti = self.exog[:, mask]\n    res = OLS(x_i, x_noti).fit()\n    if store:\n        stored[drop_idx] = res\n    return res",
            "def _ols_xnoti(self, drop_idx, endog_idx='endog', store=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"regression results from LOVO auxiliary regression with cache\\n\\n\\n        The result instances are stored, which could use a large amount of\\n        memory if the datasets are large. There are too many combinations to\\n        store them all, except for small problems.\\n\\n        Parameters\\n        ----------\\n        drop_idx : int\\n            index of exog that is dropped from the regression\\n        endog_idx : 'endog' or int\\n            If 'endog', then the endogenous variable of the result instance\\n            is regressed on the exogenous variables, excluding the one at\\n            drop_idx. If endog_idx is an integer, then the exog with that\\n            index is regressed with OLS on all other exogenous variables.\\n            (The latter is the auxiliary regression for the variance inflation\\n            factor.)\\n\\n        this needs more thought, memory versus speed\\n        not yet used in any other parts, not sufficiently tested\\n        \"\n    if endog_idx == 'endog':\n        stored = self.aux_regression_endog\n        if hasattr(stored, drop_idx):\n            return stored[drop_idx]\n        x_i = self.results.model.endog\n    else:\n        try:\n            self.aux_regression_exog[endog_idx][drop_idx]\n        except KeyError:\n            pass\n        stored = self.aux_regression_exog[endog_idx]\n        stored = {}\n        x_i = self.exog[:, endog_idx]\n    k_vars = self.exog.shape[1]\n    mask = np.arange(k_vars) != drop_idx\n    x_noti = self.exog[:, mask]\n    res = OLS(x_i, x_noti).fit()\n    if store:\n        stored[drop_idx] = res\n    return res"
        ]
    },
    {
        "func_name": "_get_drop_vari",
        "original": "def _get_drop_vari(self, attributes):\n    \"\"\"\n        regress endog on exog without one of the variables\n\n        This uses a k_vars loop, only attributes of the OLS instance are\n        stored.\n\n        Parameters\n        ----------\n        attributes : list[str]\n           These are the names of the attributes of the auxiliary OLS results\n           instance that are stored and returned.\n\n        not yet used\n        \"\"\"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    endog = self.results.model.endog\n    exog = self.exog\n    cv_iter = LeaveOneOut(self.k_vars)\n    res_loo = defaultdict(list)\n    for (inidx, outidx) in cv_iter:\n        for att in attributes:\n            res_i = self.model_class(endog, exog[:, inidx]).fit()\n            res_loo[att].append(getattr(res_i, att))\n    return res_loo",
        "mutated": [
            "def _get_drop_vari(self, attributes):\n    if False:\n        i = 10\n    '\\n        regress endog on exog without one of the variables\\n\\n        This uses a k_vars loop, only attributes of the OLS instance are\\n        stored.\\n\\n        Parameters\\n        ----------\\n        attributes : list[str]\\n           These are the names of the attributes of the auxiliary OLS results\\n           instance that are stored and returned.\\n\\n        not yet used\\n        '\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    endog = self.results.model.endog\n    exog = self.exog\n    cv_iter = LeaveOneOut(self.k_vars)\n    res_loo = defaultdict(list)\n    for (inidx, outidx) in cv_iter:\n        for att in attributes:\n            res_i = self.model_class(endog, exog[:, inidx]).fit()\n            res_loo[att].append(getattr(res_i, att))\n    return res_loo",
            "def _get_drop_vari(self, attributes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        regress endog on exog without one of the variables\\n\\n        This uses a k_vars loop, only attributes of the OLS instance are\\n        stored.\\n\\n        Parameters\\n        ----------\\n        attributes : list[str]\\n           These are the names of the attributes of the auxiliary OLS results\\n           instance that are stored and returned.\\n\\n        not yet used\\n        '\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    endog = self.results.model.endog\n    exog = self.exog\n    cv_iter = LeaveOneOut(self.k_vars)\n    res_loo = defaultdict(list)\n    for (inidx, outidx) in cv_iter:\n        for att in attributes:\n            res_i = self.model_class(endog, exog[:, inidx]).fit()\n            res_loo[att].append(getattr(res_i, att))\n    return res_loo",
            "def _get_drop_vari(self, attributes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        regress endog on exog without one of the variables\\n\\n        This uses a k_vars loop, only attributes of the OLS instance are\\n        stored.\\n\\n        Parameters\\n        ----------\\n        attributes : list[str]\\n           These are the names of the attributes of the auxiliary OLS results\\n           instance that are stored and returned.\\n\\n        not yet used\\n        '\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    endog = self.results.model.endog\n    exog = self.exog\n    cv_iter = LeaveOneOut(self.k_vars)\n    res_loo = defaultdict(list)\n    for (inidx, outidx) in cv_iter:\n        for att in attributes:\n            res_i = self.model_class(endog, exog[:, inidx]).fit()\n            res_loo[att].append(getattr(res_i, att))\n    return res_loo",
            "def _get_drop_vari(self, attributes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        regress endog on exog without one of the variables\\n\\n        This uses a k_vars loop, only attributes of the OLS instance are\\n        stored.\\n\\n        Parameters\\n        ----------\\n        attributes : list[str]\\n           These are the names of the attributes of the auxiliary OLS results\\n           instance that are stored and returned.\\n\\n        not yet used\\n        '\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    endog = self.results.model.endog\n    exog = self.exog\n    cv_iter = LeaveOneOut(self.k_vars)\n    res_loo = defaultdict(list)\n    for (inidx, outidx) in cv_iter:\n        for att in attributes:\n            res_i = self.model_class(endog, exog[:, inidx]).fit()\n            res_loo[att].append(getattr(res_i, att))\n    return res_loo",
            "def _get_drop_vari(self, attributes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        regress endog on exog without one of the variables\\n\\n        This uses a k_vars loop, only attributes of the OLS instance are\\n        stored.\\n\\n        Parameters\\n        ----------\\n        attributes : list[str]\\n           These are the names of the attributes of the auxiliary OLS results\\n           instance that are stored and returned.\\n\\n        not yet used\\n        '\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    endog = self.results.model.endog\n    exog = self.exog\n    cv_iter = LeaveOneOut(self.k_vars)\n    res_loo = defaultdict(list)\n    for (inidx, outidx) in cv_iter:\n        for att in attributes:\n            res_i = self.model_class(endog, exog[:, inidx]).fit()\n            res_loo[att].append(getattr(res_i, att))\n    return res_loo"
        ]
    },
    {
        "func_name": "get_det_cov_params",
        "original": "def get_det_cov_params(res):\n    return np.linalg.det(res.cov_params())",
        "mutated": [
            "def get_det_cov_params(res):\n    if False:\n        i = 10\n    return np.linalg.det(res.cov_params())",
            "def get_det_cov_params(res):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return np.linalg.det(res.cov_params())",
            "def get_det_cov_params(res):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return np.linalg.det(res.cov_params())",
            "def get_det_cov_params(res):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return np.linalg.det(res.cov_params())",
            "def get_det_cov_params(res):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return np.linalg.det(res.cov_params())"
        ]
    },
    {
        "func_name": "_res_looo",
        "original": "@cache_readonly\ndef _res_looo(self):\n    \"\"\"collect required results from the LOOO loop\n\n        all results will be attached.\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\n\n        regresses endog on exog dropping one observation at a time\n\n        this uses a nobs loop, only attributes of the OLS instance are stored.\n        \"\"\"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n\n    def get_det_cov_params(res):\n        return np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    params = np.zeros(exog.shape, dtype=float)\n    mse_resid = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        res_i = self.model_class(endog[inidx], exog[inidx]).fit()\n        params[outidx] = res_i.params\n        mse_resid[outidx] = res_i.mse_resid\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, mse_resid=mse_resid, det_cov_params=det_cov_params)",
        "mutated": [
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        regresses endog on exog dropping one observation at a time\\n\\n        this uses a nobs loop, only attributes of the OLS instance are stored.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n\n    def get_det_cov_params(res):\n        return np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    params = np.zeros(exog.shape, dtype=float)\n    mse_resid = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        res_i = self.model_class(endog[inidx], exog[inidx]).fit()\n        params[outidx] = res_i.params\n        mse_resid[outidx] = res_i.mse_resid\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, mse_resid=mse_resid, det_cov_params=det_cov_params)",
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        regresses endog on exog dropping one observation at a time\\n\\n        this uses a nobs loop, only attributes of the OLS instance are stored.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n\n    def get_det_cov_params(res):\n        return np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    params = np.zeros(exog.shape, dtype=float)\n    mse_resid = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        res_i = self.model_class(endog[inidx], exog[inidx]).fit()\n        params[outidx] = res_i.params\n        mse_resid[outidx] = res_i.mse_resid\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, mse_resid=mse_resid, det_cov_params=det_cov_params)",
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        regresses endog on exog dropping one observation at a time\\n\\n        this uses a nobs loop, only attributes of the OLS instance are stored.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n\n    def get_det_cov_params(res):\n        return np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    params = np.zeros(exog.shape, dtype=float)\n    mse_resid = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        res_i = self.model_class(endog[inidx], exog[inidx]).fit()\n        params[outidx] = res_i.params\n        mse_resid[outidx] = res_i.mse_resid\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, mse_resid=mse_resid, det_cov_params=det_cov_params)",
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        regresses endog on exog dropping one observation at a time\\n\\n        this uses a nobs loop, only attributes of the OLS instance are stored.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n\n    def get_det_cov_params(res):\n        return np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    params = np.zeros(exog.shape, dtype=float)\n    mse_resid = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        res_i = self.model_class(endog[inidx], exog[inidx]).fit()\n        params[outidx] = res_i.params\n        mse_resid[outidx] = res_i.mse_resid\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, mse_resid=mse_resid, det_cov_params=det_cov_params)",
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        regresses endog on exog dropping one observation at a time\\n\\n        this uses a nobs loop, only attributes of the OLS instance are stored.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n\n    def get_det_cov_params(res):\n        return np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    params = np.zeros(exog.shape, dtype=float)\n    mse_resid = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        res_i = self.model_class(endog[inidx], exog[inidx]).fit()\n        params[outidx] = res_i.params\n        mse_resid[outidx] = res_i.mse_resid\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, mse_resid=mse_resid, det_cov_params=det_cov_params)"
        ]
    },
    {
        "func_name": "summary_frame",
        "original": "def summary_frame(self):\n    \"\"\"\n        Creates a DataFrame with all available influence results.\n\n        Returns\n        -------\n        frame : DataFrame\n            A DataFrame with all results.\n\n        Notes\n        -----\n        The resultant DataFrame contains six variables in addition to the\n        DFBETAS. These are:\n\n        * cooks_d : Cook's Distance defined in `Influence.cooks_distance`\n        * standard_resid : Standardized residuals defined in\n          `Influence.resid_studentized_internal`\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\n          `Influence.hat_matrix_diag`\n        * dffits_internal : DFFITS statistics using internally Studentized\n          residuals defined in `Influence.dffits_internal`\n        * dffits : DFFITS statistics using externally Studentized residuals\n          defined in `Influence.dffits`\n        * student_resid : Externally Studentized residuals defined in\n          `Influence.resid_studentized_external`\n        \"\"\"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized_internal, hat_diag=self.hat_matrix_diag, dffits_internal=self.dffits_internal[0], student_resid=self.resid_studentized_external, dffits=self.dffits[0]), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
        "mutated": [
            "def summary_frame(self):\n    if False:\n        i = 10\n    \"\\n        Creates a DataFrame with all available influence results.\\n\\n        Returns\\n        -------\\n        frame : DataFrame\\n            A DataFrame with all results.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        DFBETAS. These are:\\n\\n        * cooks_d : Cook's Distance defined in `Influence.cooks_distance`\\n        * standard_resid : Standardized residuals defined in\\n          `Influence.resid_studentized_internal`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `Influence.hat_matrix_diag`\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `Influence.dffits_internal`\\n        * dffits : DFFITS statistics using externally Studentized residuals\\n          defined in `Influence.dffits`\\n        * student_resid : Externally Studentized residuals defined in\\n          `Influence.resid_studentized_external`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized_internal, hat_diag=self.hat_matrix_diag, dffits_internal=self.dffits_internal[0], student_resid=self.resid_studentized_external, dffits=self.dffits[0]), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Creates a DataFrame with all available influence results.\\n\\n        Returns\\n        -------\\n        frame : DataFrame\\n            A DataFrame with all results.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        DFBETAS. These are:\\n\\n        * cooks_d : Cook's Distance defined in `Influence.cooks_distance`\\n        * standard_resid : Standardized residuals defined in\\n          `Influence.resid_studentized_internal`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `Influence.hat_matrix_diag`\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `Influence.dffits_internal`\\n        * dffits : DFFITS statistics using externally Studentized residuals\\n          defined in `Influence.dffits`\\n        * student_resid : Externally Studentized residuals defined in\\n          `Influence.resid_studentized_external`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized_internal, hat_diag=self.hat_matrix_diag, dffits_internal=self.dffits_internal[0], student_resid=self.resid_studentized_external, dffits=self.dffits[0]), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Creates a DataFrame with all available influence results.\\n\\n        Returns\\n        -------\\n        frame : DataFrame\\n            A DataFrame with all results.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        DFBETAS. These are:\\n\\n        * cooks_d : Cook's Distance defined in `Influence.cooks_distance`\\n        * standard_resid : Standardized residuals defined in\\n          `Influence.resid_studentized_internal`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `Influence.hat_matrix_diag`\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `Influence.dffits_internal`\\n        * dffits : DFFITS statistics using externally Studentized residuals\\n          defined in `Influence.dffits`\\n        * student_resid : Externally Studentized residuals defined in\\n          `Influence.resid_studentized_external`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized_internal, hat_diag=self.hat_matrix_diag, dffits_internal=self.dffits_internal[0], student_resid=self.resid_studentized_external, dffits=self.dffits[0]), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Creates a DataFrame with all available influence results.\\n\\n        Returns\\n        -------\\n        frame : DataFrame\\n            A DataFrame with all results.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        DFBETAS. These are:\\n\\n        * cooks_d : Cook's Distance defined in `Influence.cooks_distance`\\n        * standard_resid : Standardized residuals defined in\\n          `Influence.resid_studentized_internal`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `Influence.hat_matrix_diag`\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `Influence.dffits_internal`\\n        * dffits : DFFITS statistics using externally Studentized residuals\\n          defined in `Influence.dffits`\\n        * student_resid : Externally Studentized residuals defined in\\n          `Influence.resid_studentized_external`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized_internal, hat_diag=self.hat_matrix_diag, dffits_internal=self.dffits_internal[0], student_resid=self.resid_studentized_external, dffits=self.dffits[0]), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)",
            "def summary_frame(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Creates a DataFrame with all available influence results.\\n\\n        Returns\\n        -------\\n        frame : DataFrame\\n            A DataFrame with all results.\\n\\n        Notes\\n        -----\\n        The resultant DataFrame contains six variables in addition to the\\n        DFBETAS. These are:\\n\\n        * cooks_d : Cook's Distance defined in `Influence.cooks_distance`\\n        * standard_resid : Standardized residuals defined in\\n          `Influence.resid_studentized_internal`\\n        * hat_diag : The diagonal of the projection, or hat, matrix defined in\\n          `Influence.hat_matrix_diag`\\n        * dffits_internal : DFFITS statistics using internally Studentized\\n          residuals defined in `Influence.dffits_internal`\\n        * dffits : DFFITS statistics using externally Studentized residuals\\n          defined in `Influence.dffits`\\n        * student_resid : Externally Studentized residuals defined in\\n          `Influence.resid_studentized_external`\\n        \"\n    from pandas import DataFrame\n    data = self.results.model.data\n    row_labels = data.row_labels\n    beta_labels = ['dfb_' + i for i in data.xnames]\n    summary_data = DataFrame(dict(cooks_d=self.cooks_distance[0], standard_resid=self.resid_studentized_internal, hat_diag=self.hat_matrix_diag, dffits_internal=self.dffits_internal[0], student_resid=self.resid_studentized_external, dffits=self.dffits[0]), index=row_labels)\n    dfbeta = DataFrame(self.dfbetas, columns=beta_labels, index=row_labels)\n    return dfbeta.join(summary_data)"
        ]
    },
    {
        "func_name": "summary_table",
        "original": "def summary_table(self, float_fmt='%6.3f'):\n    \"\"\"create a summary table with all influence and outlier measures\n\n        This does currently not distinguish between statistics that can be\n        calculated from the original regression results and for which a\n        leave-one-observation-out loop is needed\n\n        Returns\n        -------\n        res : SimpleTable\n           SimpleTable instance with the results, can be printed\n\n        Notes\n        -----\n        This also attaches table_data to the instance.\n        \"\"\"\n    table_raw = [('obs', np.arange(self.nobs)), ('endog', self.endog), ('fitted\\nvalue', self.results.fittedvalues), (\"Cook's\\nd\", self.cooks_distance[0]), ('student.\\nresidual', self.resid_studentized_internal), ('hat diag', self.hat_matrix_diag), ('dffits \\ninternal', self.dffits_internal[0]), ('ext.stud.\\nresidual', self.resid_studentized_external), ('dffits', self.dffits[0])]\n    (colnames, data) = lzip(*table_raw)\n    data = np.column_stack(data)\n    self.table_data = data\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + [float_fmt] * (data.shape[1] - 1)\n    return SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)",
        "mutated": [
            "def summary_table(self, float_fmt='%6.3f'):\n    if False:\n        i = 10\n    'create a summary table with all influence and outlier measures\\n\\n        This does currently not distinguish between statistics that can be\\n        calculated from the original regression results and for which a\\n        leave-one-observation-out loop is needed\\n\\n        Returns\\n        -------\\n        res : SimpleTable\\n           SimpleTable instance with the results, can be printed\\n\\n        Notes\\n        -----\\n        This also attaches table_data to the instance.\\n        '\n    table_raw = [('obs', np.arange(self.nobs)), ('endog', self.endog), ('fitted\\nvalue', self.results.fittedvalues), (\"Cook's\\nd\", self.cooks_distance[0]), ('student.\\nresidual', self.resid_studentized_internal), ('hat diag', self.hat_matrix_diag), ('dffits \\ninternal', self.dffits_internal[0]), ('ext.stud.\\nresidual', self.resid_studentized_external), ('dffits', self.dffits[0])]\n    (colnames, data) = lzip(*table_raw)\n    data = np.column_stack(data)\n    self.table_data = data\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + [float_fmt] * (data.shape[1] - 1)\n    return SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)",
            "def summary_table(self, float_fmt='%6.3f'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'create a summary table with all influence and outlier measures\\n\\n        This does currently not distinguish between statistics that can be\\n        calculated from the original regression results and for which a\\n        leave-one-observation-out loop is needed\\n\\n        Returns\\n        -------\\n        res : SimpleTable\\n           SimpleTable instance with the results, can be printed\\n\\n        Notes\\n        -----\\n        This also attaches table_data to the instance.\\n        '\n    table_raw = [('obs', np.arange(self.nobs)), ('endog', self.endog), ('fitted\\nvalue', self.results.fittedvalues), (\"Cook's\\nd\", self.cooks_distance[0]), ('student.\\nresidual', self.resid_studentized_internal), ('hat diag', self.hat_matrix_diag), ('dffits \\ninternal', self.dffits_internal[0]), ('ext.stud.\\nresidual', self.resid_studentized_external), ('dffits', self.dffits[0])]\n    (colnames, data) = lzip(*table_raw)\n    data = np.column_stack(data)\n    self.table_data = data\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + [float_fmt] * (data.shape[1] - 1)\n    return SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)",
            "def summary_table(self, float_fmt='%6.3f'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'create a summary table with all influence and outlier measures\\n\\n        This does currently not distinguish between statistics that can be\\n        calculated from the original regression results and for which a\\n        leave-one-observation-out loop is needed\\n\\n        Returns\\n        -------\\n        res : SimpleTable\\n           SimpleTable instance with the results, can be printed\\n\\n        Notes\\n        -----\\n        This also attaches table_data to the instance.\\n        '\n    table_raw = [('obs', np.arange(self.nobs)), ('endog', self.endog), ('fitted\\nvalue', self.results.fittedvalues), (\"Cook's\\nd\", self.cooks_distance[0]), ('student.\\nresidual', self.resid_studentized_internal), ('hat diag', self.hat_matrix_diag), ('dffits \\ninternal', self.dffits_internal[0]), ('ext.stud.\\nresidual', self.resid_studentized_external), ('dffits', self.dffits[0])]\n    (colnames, data) = lzip(*table_raw)\n    data = np.column_stack(data)\n    self.table_data = data\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + [float_fmt] * (data.shape[1] - 1)\n    return SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)",
            "def summary_table(self, float_fmt='%6.3f'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'create a summary table with all influence and outlier measures\\n\\n        This does currently not distinguish between statistics that can be\\n        calculated from the original regression results and for which a\\n        leave-one-observation-out loop is needed\\n\\n        Returns\\n        -------\\n        res : SimpleTable\\n           SimpleTable instance with the results, can be printed\\n\\n        Notes\\n        -----\\n        This also attaches table_data to the instance.\\n        '\n    table_raw = [('obs', np.arange(self.nobs)), ('endog', self.endog), ('fitted\\nvalue', self.results.fittedvalues), (\"Cook's\\nd\", self.cooks_distance[0]), ('student.\\nresidual', self.resid_studentized_internal), ('hat diag', self.hat_matrix_diag), ('dffits \\ninternal', self.dffits_internal[0]), ('ext.stud.\\nresidual', self.resid_studentized_external), ('dffits', self.dffits[0])]\n    (colnames, data) = lzip(*table_raw)\n    data = np.column_stack(data)\n    self.table_data = data\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + [float_fmt] * (data.shape[1] - 1)\n    return SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)",
            "def summary_table(self, float_fmt='%6.3f'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'create a summary table with all influence and outlier measures\\n\\n        This does currently not distinguish between statistics that can be\\n        calculated from the original regression results and for which a\\n        leave-one-observation-out loop is needed\\n\\n        Returns\\n        -------\\n        res : SimpleTable\\n           SimpleTable instance with the results, can be printed\\n\\n        Notes\\n        -----\\n        This also attaches table_data to the instance.\\n        '\n    table_raw = [('obs', np.arange(self.nobs)), ('endog', self.endog), ('fitted\\nvalue', self.results.fittedvalues), (\"Cook's\\nd\", self.cooks_distance[0]), ('student.\\nresidual', self.resid_studentized_internal), ('hat diag', self.hat_matrix_diag), ('dffits \\ninternal', self.dffits_internal[0]), ('ext.stud.\\nresidual', self.resid_studentized_external), ('dffits', self.dffits[0])]\n    (colnames, data) = lzip(*table_raw)\n    data = np.column_stack(data)\n    self.table_data = data\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + [float_fmt] * (data.shape[1] - 1)\n    return SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)"
        ]
    },
    {
        "func_name": "summary_table",
        "original": "def summary_table(res, alpha=0.05):\n    \"\"\"\n    Generate summary table of outlier and influence similar to SAS\n\n    Parameters\n    ----------\n    alpha : float\n       significance level for confidence interval\n\n    Returns\n    -------\n    st : SimpleTable\n       table with results that can be printed\n    data : ndarray\n       calculated measures and statistics for the table\n    ss2 : list[str]\n       column_names for table (Note: rows of table are observations)\n    \"\"\"\n    from scipy import stats\n    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n    infl = OLSInfluence(res)\n    predict_mean_se = np.sqrt(infl.hat_matrix_diag * res.mse_resid)\n    tppf = stats.t.isf(alpha / 2.0, res.df_resid)\n    predict_mean_ci = np.column_stack([res.fittedvalues - tppf * predict_mean_se, res.fittedvalues + tppf * predict_mean_se])\n    tmp = wls_prediction_std(res, alpha=alpha)\n    (predict_se, predict_ci_low, predict_ci_upp) = tmp\n    predict_ci = np.column_stack((predict_ci_low, predict_ci_upp))\n    resid_se = np.sqrt(res.mse_resid * (1 - infl.hat_matrix_diag))\n    table_sm = np.column_stack([np.arange(res.nobs) + 1, res.model.endog, res.fittedvalues, predict_mean_se, predict_mean_ci[:, 0], predict_mean_ci[:, 1], predict_ci[:, 0], predict_ci[:, 1], res.resid, resid_se, infl.resid_studentized_internal, infl.cooks_distance[0]])\n    data = table_sm\n    ss2 = ['Obs', 'Dep Var\\nPopulation', 'Predicted\\nValue', 'Std Error\\nMean Predict', 'Mean ci\\n95% low', 'Mean ci\\n95% upp', 'Predict ci\\n95% low', 'Predict ci\\n95% upp', 'Residual', 'Std Error\\nResidual', 'Student\\nResidual', \"Cook's\\nD\"]\n    colnames = ss2\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + ['%6.3f'] * (data.shape[1] - 1)\n    st = SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)\n    return (st, data, ss2)",
        "mutated": [
            "def summary_table(res, alpha=0.05):\n    if False:\n        i = 10\n    '\\n    Generate summary table of outlier and influence similar to SAS\\n\\n    Parameters\\n    ----------\\n    alpha : float\\n       significance level for confidence interval\\n\\n    Returns\\n    -------\\n    st : SimpleTable\\n       table with results that can be printed\\n    data : ndarray\\n       calculated measures and statistics for the table\\n    ss2 : list[str]\\n       column_names for table (Note: rows of table are observations)\\n    '\n    from scipy import stats\n    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n    infl = OLSInfluence(res)\n    predict_mean_se = np.sqrt(infl.hat_matrix_diag * res.mse_resid)\n    tppf = stats.t.isf(alpha / 2.0, res.df_resid)\n    predict_mean_ci = np.column_stack([res.fittedvalues - tppf * predict_mean_se, res.fittedvalues + tppf * predict_mean_se])\n    tmp = wls_prediction_std(res, alpha=alpha)\n    (predict_se, predict_ci_low, predict_ci_upp) = tmp\n    predict_ci = np.column_stack((predict_ci_low, predict_ci_upp))\n    resid_se = np.sqrt(res.mse_resid * (1 - infl.hat_matrix_diag))\n    table_sm = np.column_stack([np.arange(res.nobs) + 1, res.model.endog, res.fittedvalues, predict_mean_se, predict_mean_ci[:, 0], predict_mean_ci[:, 1], predict_ci[:, 0], predict_ci[:, 1], res.resid, resid_se, infl.resid_studentized_internal, infl.cooks_distance[0]])\n    data = table_sm\n    ss2 = ['Obs', 'Dep Var\\nPopulation', 'Predicted\\nValue', 'Std Error\\nMean Predict', 'Mean ci\\n95% low', 'Mean ci\\n95% upp', 'Predict ci\\n95% low', 'Predict ci\\n95% upp', 'Residual', 'Std Error\\nResidual', 'Student\\nResidual', \"Cook's\\nD\"]\n    colnames = ss2\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + ['%6.3f'] * (data.shape[1] - 1)\n    st = SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)\n    return (st, data, ss2)",
            "def summary_table(res, alpha=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Generate summary table of outlier and influence similar to SAS\\n\\n    Parameters\\n    ----------\\n    alpha : float\\n       significance level for confidence interval\\n\\n    Returns\\n    -------\\n    st : SimpleTable\\n       table with results that can be printed\\n    data : ndarray\\n       calculated measures and statistics for the table\\n    ss2 : list[str]\\n       column_names for table (Note: rows of table are observations)\\n    '\n    from scipy import stats\n    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n    infl = OLSInfluence(res)\n    predict_mean_se = np.sqrt(infl.hat_matrix_diag * res.mse_resid)\n    tppf = stats.t.isf(alpha / 2.0, res.df_resid)\n    predict_mean_ci = np.column_stack([res.fittedvalues - tppf * predict_mean_se, res.fittedvalues + tppf * predict_mean_se])\n    tmp = wls_prediction_std(res, alpha=alpha)\n    (predict_se, predict_ci_low, predict_ci_upp) = tmp\n    predict_ci = np.column_stack((predict_ci_low, predict_ci_upp))\n    resid_se = np.sqrt(res.mse_resid * (1 - infl.hat_matrix_diag))\n    table_sm = np.column_stack([np.arange(res.nobs) + 1, res.model.endog, res.fittedvalues, predict_mean_se, predict_mean_ci[:, 0], predict_mean_ci[:, 1], predict_ci[:, 0], predict_ci[:, 1], res.resid, resid_se, infl.resid_studentized_internal, infl.cooks_distance[0]])\n    data = table_sm\n    ss2 = ['Obs', 'Dep Var\\nPopulation', 'Predicted\\nValue', 'Std Error\\nMean Predict', 'Mean ci\\n95% low', 'Mean ci\\n95% upp', 'Predict ci\\n95% low', 'Predict ci\\n95% upp', 'Residual', 'Std Error\\nResidual', 'Student\\nResidual', \"Cook's\\nD\"]\n    colnames = ss2\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + ['%6.3f'] * (data.shape[1] - 1)\n    st = SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)\n    return (st, data, ss2)",
            "def summary_table(res, alpha=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Generate summary table of outlier and influence similar to SAS\\n\\n    Parameters\\n    ----------\\n    alpha : float\\n       significance level for confidence interval\\n\\n    Returns\\n    -------\\n    st : SimpleTable\\n       table with results that can be printed\\n    data : ndarray\\n       calculated measures and statistics for the table\\n    ss2 : list[str]\\n       column_names for table (Note: rows of table are observations)\\n    '\n    from scipy import stats\n    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n    infl = OLSInfluence(res)\n    predict_mean_se = np.sqrt(infl.hat_matrix_diag * res.mse_resid)\n    tppf = stats.t.isf(alpha / 2.0, res.df_resid)\n    predict_mean_ci = np.column_stack([res.fittedvalues - tppf * predict_mean_se, res.fittedvalues + tppf * predict_mean_se])\n    tmp = wls_prediction_std(res, alpha=alpha)\n    (predict_se, predict_ci_low, predict_ci_upp) = tmp\n    predict_ci = np.column_stack((predict_ci_low, predict_ci_upp))\n    resid_se = np.sqrt(res.mse_resid * (1 - infl.hat_matrix_diag))\n    table_sm = np.column_stack([np.arange(res.nobs) + 1, res.model.endog, res.fittedvalues, predict_mean_se, predict_mean_ci[:, 0], predict_mean_ci[:, 1], predict_ci[:, 0], predict_ci[:, 1], res.resid, resid_se, infl.resid_studentized_internal, infl.cooks_distance[0]])\n    data = table_sm\n    ss2 = ['Obs', 'Dep Var\\nPopulation', 'Predicted\\nValue', 'Std Error\\nMean Predict', 'Mean ci\\n95% low', 'Mean ci\\n95% upp', 'Predict ci\\n95% low', 'Predict ci\\n95% upp', 'Residual', 'Std Error\\nResidual', 'Student\\nResidual', \"Cook's\\nD\"]\n    colnames = ss2\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + ['%6.3f'] * (data.shape[1] - 1)\n    st = SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)\n    return (st, data, ss2)",
            "def summary_table(res, alpha=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Generate summary table of outlier and influence similar to SAS\\n\\n    Parameters\\n    ----------\\n    alpha : float\\n       significance level for confidence interval\\n\\n    Returns\\n    -------\\n    st : SimpleTable\\n       table with results that can be printed\\n    data : ndarray\\n       calculated measures and statistics for the table\\n    ss2 : list[str]\\n       column_names for table (Note: rows of table are observations)\\n    '\n    from scipy import stats\n    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n    infl = OLSInfluence(res)\n    predict_mean_se = np.sqrt(infl.hat_matrix_diag * res.mse_resid)\n    tppf = stats.t.isf(alpha / 2.0, res.df_resid)\n    predict_mean_ci = np.column_stack([res.fittedvalues - tppf * predict_mean_se, res.fittedvalues + tppf * predict_mean_se])\n    tmp = wls_prediction_std(res, alpha=alpha)\n    (predict_se, predict_ci_low, predict_ci_upp) = tmp\n    predict_ci = np.column_stack((predict_ci_low, predict_ci_upp))\n    resid_se = np.sqrt(res.mse_resid * (1 - infl.hat_matrix_diag))\n    table_sm = np.column_stack([np.arange(res.nobs) + 1, res.model.endog, res.fittedvalues, predict_mean_se, predict_mean_ci[:, 0], predict_mean_ci[:, 1], predict_ci[:, 0], predict_ci[:, 1], res.resid, resid_se, infl.resid_studentized_internal, infl.cooks_distance[0]])\n    data = table_sm\n    ss2 = ['Obs', 'Dep Var\\nPopulation', 'Predicted\\nValue', 'Std Error\\nMean Predict', 'Mean ci\\n95% low', 'Mean ci\\n95% upp', 'Predict ci\\n95% low', 'Predict ci\\n95% upp', 'Residual', 'Std Error\\nResidual', 'Student\\nResidual', \"Cook's\\nD\"]\n    colnames = ss2\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + ['%6.3f'] * (data.shape[1] - 1)\n    st = SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)\n    return (st, data, ss2)",
            "def summary_table(res, alpha=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Generate summary table of outlier and influence similar to SAS\\n\\n    Parameters\\n    ----------\\n    alpha : float\\n       significance level for confidence interval\\n\\n    Returns\\n    -------\\n    st : SimpleTable\\n       table with results that can be printed\\n    data : ndarray\\n       calculated measures and statistics for the table\\n    ss2 : list[str]\\n       column_names for table (Note: rows of table are observations)\\n    '\n    from scipy import stats\n    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n    infl = OLSInfluence(res)\n    predict_mean_se = np.sqrt(infl.hat_matrix_diag * res.mse_resid)\n    tppf = stats.t.isf(alpha / 2.0, res.df_resid)\n    predict_mean_ci = np.column_stack([res.fittedvalues - tppf * predict_mean_se, res.fittedvalues + tppf * predict_mean_se])\n    tmp = wls_prediction_std(res, alpha=alpha)\n    (predict_se, predict_ci_low, predict_ci_upp) = tmp\n    predict_ci = np.column_stack((predict_ci_low, predict_ci_upp))\n    resid_se = np.sqrt(res.mse_resid * (1 - infl.hat_matrix_diag))\n    table_sm = np.column_stack([np.arange(res.nobs) + 1, res.model.endog, res.fittedvalues, predict_mean_se, predict_mean_ci[:, 0], predict_mean_ci[:, 1], predict_ci[:, 0], predict_ci[:, 1], res.resid, resid_se, infl.resid_studentized_internal, infl.cooks_distance[0]])\n    data = table_sm\n    ss2 = ['Obs', 'Dep Var\\nPopulation', 'Predicted\\nValue', 'Std Error\\nMean Predict', 'Mean ci\\n95% low', 'Mean ci\\n95% upp', 'Predict ci\\n95% low', 'Predict ci\\n95% upp', 'Residual', 'Std Error\\nResidual', 'Student\\nResidual', \"Cook's\\nD\"]\n    colnames = ss2\n    from copy import deepcopy\n    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n    from statsmodels.iolib.tableformatting import fmt_base\n    fmt = deepcopy(fmt_base)\n    fmt_html = deepcopy(default_html_fmt)\n    fmt['data_fmts'] = ['%4d'] + ['%6.3f'] * (data.shape[1] - 1)\n    st = SimpleTable(data, headers=colnames, txt_fmt=fmt, html_fmt=fmt_html)\n    return (st, data, ss2)"
        ]
    },
    {
        "func_name": "hat_matrix_diag",
        "original": "@cache_readonly\ndef hat_matrix_diag(self):\n    \"\"\"\n        Diagonal of the hat_matrix for GLM\n\n        Notes\n        -----\n        This returns the diagonal of the hat matrix that was provided as\n        argument to GLMInfluence or computes it using the results method\n        `get_hat_matrix`.\n        \"\"\"\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    else:\n        return self.results.get_hat_matrix()",
        "mutated": [
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n    '\\n        Diagonal of the hat_matrix for GLM\\n\\n        Notes\\n        -----\\n        This returns the diagonal of the hat matrix that was provided as\\n        argument to GLMInfluence or computes it using the results method\\n        `get_hat_matrix`.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    else:\n        return self.results.get_hat_matrix()",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Diagonal of the hat_matrix for GLM\\n\\n        Notes\\n        -----\\n        This returns the diagonal of the hat matrix that was provided as\\n        argument to GLMInfluence or computes it using the results method\\n        `get_hat_matrix`.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    else:\n        return self.results.get_hat_matrix()",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Diagonal of the hat_matrix for GLM\\n\\n        Notes\\n        -----\\n        This returns the diagonal of the hat matrix that was provided as\\n        argument to GLMInfluence or computes it using the results method\\n        `get_hat_matrix`.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    else:\n        return self.results.get_hat_matrix()",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Diagonal of the hat_matrix for GLM\\n\\n        Notes\\n        -----\\n        This returns the diagonal of the hat matrix that was provided as\\n        argument to GLMInfluence or computes it using the results method\\n        `get_hat_matrix`.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    else:\n        return self.results.get_hat_matrix()",
            "@cache_readonly\ndef hat_matrix_diag(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Diagonal of the hat_matrix for GLM\\n\\n        Notes\\n        -----\\n        This returns the diagonal of the hat matrix that was provided as\\n        argument to GLMInfluence or computes it using the results method\\n        `get_hat_matrix`.\\n        '\n    if hasattr(self, '_hat_matrix_diag'):\n        return self._hat_matrix_diag\n    else:\n        return self.results.get_hat_matrix()"
        ]
    },
    {
        "func_name": "d_params",
        "original": "@cache_readonly\ndef d_params(self):\n    \"\"\"Change in parameter estimates\n\n        Notes\n        -----\n        This uses one-step approximation of the parameter change to deleting\n        one observation.\n        \"\"\"\n    beta_i = np.linalg.pinv(self.exog) * self.resid_studentized\n    beta_i /= np.sqrt(1 - self.hat_matrix_diag)\n    return beta_i.T",
        "mutated": [
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n    'Change in parameter estimates\\n\\n        Notes\\n        -----\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    beta_i = np.linalg.pinv(self.exog) * self.resid_studentized\n    beta_i /= np.sqrt(1 - self.hat_matrix_diag)\n    return beta_i.T",
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Change in parameter estimates\\n\\n        Notes\\n        -----\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    beta_i = np.linalg.pinv(self.exog) * self.resid_studentized\n    beta_i /= np.sqrt(1 - self.hat_matrix_diag)\n    return beta_i.T",
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Change in parameter estimates\\n\\n        Notes\\n        -----\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    beta_i = np.linalg.pinv(self.exog) * self.resid_studentized\n    beta_i /= np.sqrt(1 - self.hat_matrix_diag)\n    return beta_i.T",
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Change in parameter estimates\\n\\n        Notes\\n        -----\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    beta_i = np.linalg.pinv(self.exog) * self.resid_studentized\n    beta_i /= np.sqrt(1 - self.hat_matrix_diag)\n    return beta_i.T",
            "@cache_readonly\ndef d_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Change in parameter estimates\\n\\n        Notes\\n        -----\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation.\\n        '\n    beta_i = np.linalg.pinv(self.exog) * self.resid_studentized\n    beta_i /= np.sqrt(1 - self.hat_matrix_diag)\n    return beta_i.T"
        ]
    },
    {
        "func_name": "resid_studentized",
        "original": "@cache_readonly\ndef resid_studentized(self):\n    \"\"\"\n        Internally studentized pearson residuals\n\n        Notes\n        -----\n        residuals / sqrt( scale * (1 - hii))\n\n        where residuals are those provided to GLMInfluence which are\n        pearson residuals by default, and\n        hii is the diagonal of the hat matrix.\n        \"\"\"\n    return super().resid_studentized",
        "mutated": [
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n    '\\n        Internally studentized pearson residuals\\n\\n        Notes\\n        -----\\n        residuals / sqrt( scale * (1 - hii))\\n\\n        where residuals are those provided to GLMInfluence which are\\n        pearson residuals by default, and\\n        hii is the diagonal of the hat matrix.\\n        '\n    return super().resid_studentized",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Internally studentized pearson residuals\\n\\n        Notes\\n        -----\\n        residuals / sqrt( scale * (1 - hii))\\n\\n        where residuals are those provided to GLMInfluence which are\\n        pearson residuals by default, and\\n        hii is the diagonal of the hat matrix.\\n        '\n    return super().resid_studentized",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Internally studentized pearson residuals\\n\\n        Notes\\n        -----\\n        residuals / sqrt( scale * (1 - hii))\\n\\n        where residuals are those provided to GLMInfluence which are\\n        pearson residuals by default, and\\n        hii is the diagonal of the hat matrix.\\n        '\n    return super().resid_studentized",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Internally studentized pearson residuals\\n\\n        Notes\\n        -----\\n        residuals / sqrt( scale * (1 - hii))\\n\\n        where residuals are those provided to GLMInfluence which are\\n        pearson residuals by default, and\\n        hii is the diagonal of the hat matrix.\\n        '\n    return super().resid_studentized",
            "@cache_readonly\ndef resid_studentized(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Internally studentized pearson residuals\\n\\n        Notes\\n        -----\\n        residuals / sqrt( scale * (1 - hii))\\n\\n        where residuals are those provided to GLMInfluence which are\\n        pearson residuals by default, and\\n        hii is the diagonal of the hat matrix.\\n        '\n    return super().resid_studentized"
        ]
    },
    {
        "func_name": "cooks_distance",
        "original": "@cache_readonly\ndef cooks_distance(self):\n    \"\"\"Cook's distance\n\n        Notes\n        -----\n        Based on one step approximation using resid_studentized and\n        hat_matrix_diag for the computation.\n\n        Cook's distance divides by the number of explanatory variables.\n\n        Computed using formulas for GLM and does not use results.cov_params.\n        It includes p-values based on the F-distribution which are only\n        approximate outside of linear Gaussian models.\n        \"\"\"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
        "mutated": [
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n    \"Cook's distance\\n\\n        Notes\\n        -----\\n        Based on one step approximation using resid_studentized and\\n        hat_matrix_diag for the computation.\\n\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        Computed using formulas for GLM and does not use results.cov_params.\\n        It includes p-values based on the F-distribution which are only\\n        approximate outside of linear Gaussian models.\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Cook's distance\\n\\n        Notes\\n        -----\\n        Based on one step approximation using resid_studentized and\\n        hat_matrix_diag for the computation.\\n\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        Computed using formulas for GLM and does not use results.cov_params.\\n        It includes p-values based on the F-distribution which are only\\n        approximate outside of linear Gaussian models.\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Cook's distance\\n\\n        Notes\\n        -----\\n        Based on one step approximation using resid_studentized and\\n        hat_matrix_diag for the computation.\\n\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        Computed using formulas for GLM and does not use results.cov_params.\\n        It includes p-values based on the F-distribution which are only\\n        approximate outside of linear Gaussian models.\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Cook's distance\\n\\n        Notes\\n        -----\\n        Based on one step approximation using resid_studentized and\\n        hat_matrix_diag for the computation.\\n\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        Computed using formulas for GLM and does not use results.cov_params.\\n        It includes p-values based on the F-distribution which are only\\n        approximate outside of linear Gaussian models.\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)",
            "@cache_readonly\ndef cooks_distance(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Cook's distance\\n\\n        Notes\\n        -----\\n        Based on one step approximation using resid_studentized and\\n        hat_matrix_diag for the computation.\\n\\n        Cook's distance divides by the number of explanatory variables.\\n\\n        Computed using formulas for GLM and does not use results.cov_params.\\n        It includes p-values based on the F-distribution which are only\\n        approximate outside of linear Gaussian models.\\n        \"\n    hii = self.hat_matrix_diag\n    cooks_d2 = self.resid_studentized ** 2 / self.k_vars\n    cooks_d2 *= hii / (1 - hii)\n    from scipy import stats\n    pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n    return (cooks_d2, pvals)"
        ]
    },
    {
        "func_name": "d_linpred",
        "original": "@property\ndef d_linpred(self):\n    \"\"\"\n        Change in linear prediction\n\n        This uses one-step approximation of the parameter change to deleting\n        one observation ``d_params``.\n        \"\"\"\n    exog = self.results.model.exog\n    return (exog * self.d_params).sum(1)",
        "mutated": [
            "@property\ndef d_linpred(self):\n    if False:\n        i = 10\n    '\\n        Change in linear prediction\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``.\\n        '\n    exog = self.results.model.exog\n    return (exog * self.d_params).sum(1)",
            "@property\ndef d_linpred(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Change in linear prediction\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``.\\n        '\n    exog = self.results.model.exog\n    return (exog * self.d_params).sum(1)",
            "@property\ndef d_linpred(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Change in linear prediction\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``.\\n        '\n    exog = self.results.model.exog\n    return (exog * self.d_params).sum(1)",
            "@property\ndef d_linpred(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Change in linear prediction\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``.\\n        '\n    exog = self.results.model.exog\n    return (exog * self.d_params).sum(1)",
            "@property\ndef d_linpred(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Change in linear prediction\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``.\\n        '\n    exog = self.results.model.exog\n    return (exog * self.d_params).sum(1)"
        ]
    },
    {
        "func_name": "d_linpred_scaled",
        "original": "@property\ndef d_linpred_scaled(self):\n    \"\"\"\n        Change in linpred scaled by standard errors\n\n        This uses one-step approximation of the parameter change to deleting\n        one observation ``d_params``, and divides by the standard errors\n        for linpred provided by results.get_prediction.\n        \"\"\"\n    return self.d_linpred / self._get_prediction.linpred.se",
        "mutated": [
            "@property\ndef d_linpred_scaled(self):\n    if False:\n        i = 10\n    '\\n        Change in linpred scaled by standard errors\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for linpred provided by results.get_prediction.\\n        '\n    return self.d_linpred / self._get_prediction.linpred.se",
            "@property\ndef d_linpred_scaled(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Change in linpred scaled by standard errors\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for linpred provided by results.get_prediction.\\n        '\n    return self.d_linpred / self._get_prediction.linpred.se",
            "@property\ndef d_linpred_scaled(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Change in linpred scaled by standard errors\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for linpred provided by results.get_prediction.\\n        '\n    return self.d_linpred / self._get_prediction.linpred.se",
            "@property\ndef d_linpred_scaled(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Change in linpred scaled by standard errors\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for linpred provided by results.get_prediction.\\n        '\n    return self.d_linpred / self._get_prediction.linpred.se",
            "@property\ndef d_linpred_scaled(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Change in linpred scaled by standard errors\\n\\n        This uses one-step approximation of the parameter change to deleting\\n        one observation ``d_params``, and divides by the standard errors\\n        for linpred provided by results.get_prediction.\\n        '\n    return self.d_linpred / self._get_prediction.linpred.se"
        ]
    },
    {
        "func_name": "_fittedvalues_one",
        "original": "@property\ndef _fittedvalues_one(self):\n    \"\"\"experimental code\n        \"\"\"\n    warnings.warn('this ignores offset and exposure', UserWarning)\n    exog = self.results.model.exog\n    fitted = np.array([self.results.model.predict(pi, exog[i]) for (i, pi) in enumerate(self.params_one)])\n    return fitted.squeeze()",
        "mutated": [
            "@property\ndef _fittedvalues_one(self):\n    if False:\n        i = 10\n    'experimental code\\n        '\n    warnings.warn('this ignores offset and exposure', UserWarning)\n    exog = self.results.model.exog\n    fitted = np.array([self.results.model.predict(pi, exog[i]) for (i, pi) in enumerate(self.params_one)])\n    return fitted.squeeze()",
            "@property\ndef _fittedvalues_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'experimental code\\n        '\n    warnings.warn('this ignores offset and exposure', UserWarning)\n    exog = self.results.model.exog\n    fitted = np.array([self.results.model.predict(pi, exog[i]) for (i, pi) in enumerate(self.params_one)])\n    return fitted.squeeze()",
            "@property\ndef _fittedvalues_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'experimental code\\n        '\n    warnings.warn('this ignores offset and exposure', UserWarning)\n    exog = self.results.model.exog\n    fitted = np.array([self.results.model.predict(pi, exog[i]) for (i, pi) in enumerate(self.params_one)])\n    return fitted.squeeze()",
            "@property\ndef _fittedvalues_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'experimental code\\n        '\n    warnings.warn('this ignores offset and exposure', UserWarning)\n    exog = self.results.model.exog\n    fitted = np.array([self.results.model.predict(pi, exog[i]) for (i, pi) in enumerate(self.params_one)])\n    return fitted.squeeze()",
            "@property\ndef _fittedvalues_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'experimental code\\n        '\n    warnings.warn('this ignores offset and exposure', UserWarning)\n    exog = self.results.model.exog\n    fitted = np.array([self.results.model.predict(pi, exog[i]) for (i, pi) in enumerate(self.params_one)])\n    return fitted.squeeze()"
        ]
    },
    {
        "func_name": "_diff_fittedvalues_one",
        "original": "@property\ndef _diff_fittedvalues_one(self):\n    \"\"\"experimental code\n        \"\"\"\n    return self.results.predict() - self._fittedvalues_one",
        "mutated": [
            "@property\ndef _diff_fittedvalues_one(self):\n    if False:\n        i = 10\n    'experimental code\\n        '\n    return self.results.predict() - self._fittedvalues_one",
            "@property\ndef _diff_fittedvalues_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'experimental code\\n        '\n    return self.results.predict() - self._fittedvalues_one",
            "@property\ndef _diff_fittedvalues_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'experimental code\\n        '\n    return self.results.predict() - self._fittedvalues_one",
            "@property\ndef _diff_fittedvalues_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'experimental code\\n        '\n    return self.results.predict() - self._fittedvalues_one",
            "@property\ndef _diff_fittedvalues_one(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'experimental code\\n        '\n    return self.results.predict() - self._fittedvalues_one"
        ]
    },
    {
        "func_name": "_res_looo",
        "original": "@cache_readonly\ndef _res_looo(self):\n    \"\"\"collect required results from the LOOO loop\n\n        all results will be attached.\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\n\n        Reestimates the model with endog and exog dropping one observation\n        at a time\n\n        This uses a nobs loop, only attributes of the results instance are\n        stored.\n\n        Warning: This will need refactoring and API changes to be able to\n        add options.\n        \"\"\"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    get_det_cov_params = lambda res: np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    init_kwds = self.results.model._get_init_kwds()\n    freq_weights = init_kwds.pop('freq_weights')\n    var_weights = init_kwds.pop('var_weights')\n    offset = offset_ = init_kwds.pop('offset')\n    exposure = exposure_ = init_kwds.pop('exposure')\n    n_trials = init_kwds.pop('n_trials', None)\n    if hasattr(init_kwds['family'], 'initialize'):\n        is_binomial = True\n    else:\n        is_binomial = False\n    params = np.zeros(exog.shape, dtype=float)\n    scale = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        if offset is not None:\n            offset_ = offset[inidx]\n        if exposure is not None:\n            exposure_ = exposure[inidx]\n        if n_trials is not None:\n            init_kwds['n_trials'] = n_trials[inidx]\n        mod_i = self.model_class(endog[inidx], exog[inidx], offset=offset_, exposure=exposure_, freq_weights=freq_weights[inidx], var_weights=var_weights[inidx], **init_kwds)\n        if is_binomial:\n            mod_i.family.n = init_kwds['n_trials']\n        res_i = mod_i.fit(start_params=self.results.params, method='newton')\n        params[outidx] = res_i.params.copy()\n        scale[outidx] = res_i.scale\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, scale=scale, mse_resid=scale, det_cov_params=det_cov_params)",
        "mutated": [
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        Reestimates the model with endog and exog dropping one observation\\n        at a time\\n\\n        This uses a nobs loop, only attributes of the results instance are\\n        stored.\\n\\n        Warning: This will need refactoring and API changes to be able to\\n        add options.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    get_det_cov_params = lambda res: np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    init_kwds = self.results.model._get_init_kwds()\n    freq_weights = init_kwds.pop('freq_weights')\n    var_weights = init_kwds.pop('var_weights')\n    offset = offset_ = init_kwds.pop('offset')\n    exposure = exposure_ = init_kwds.pop('exposure')\n    n_trials = init_kwds.pop('n_trials', None)\n    if hasattr(init_kwds['family'], 'initialize'):\n        is_binomial = True\n    else:\n        is_binomial = False\n    params = np.zeros(exog.shape, dtype=float)\n    scale = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        if offset is not None:\n            offset_ = offset[inidx]\n        if exposure is not None:\n            exposure_ = exposure[inidx]\n        if n_trials is not None:\n            init_kwds['n_trials'] = n_trials[inidx]\n        mod_i = self.model_class(endog[inidx], exog[inidx], offset=offset_, exposure=exposure_, freq_weights=freq_weights[inidx], var_weights=var_weights[inidx], **init_kwds)\n        if is_binomial:\n            mod_i.family.n = init_kwds['n_trials']\n        res_i = mod_i.fit(start_params=self.results.params, method='newton')\n        params[outidx] = res_i.params.copy()\n        scale[outidx] = res_i.scale\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, scale=scale, mse_resid=scale, det_cov_params=det_cov_params)",
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        Reestimates the model with endog and exog dropping one observation\\n        at a time\\n\\n        This uses a nobs loop, only attributes of the results instance are\\n        stored.\\n\\n        Warning: This will need refactoring and API changes to be able to\\n        add options.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    get_det_cov_params = lambda res: np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    init_kwds = self.results.model._get_init_kwds()\n    freq_weights = init_kwds.pop('freq_weights')\n    var_weights = init_kwds.pop('var_weights')\n    offset = offset_ = init_kwds.pop('offset')\n    exposure = exposure_ = init_kwds.pop('exposure')\n    n_trials = init_kwds.pop('n_trials', None)\n    if hasattr(init_kwds['family'], 'initialize'):\n        is_binomial = True\n    else:\n        is_binomial = False\n    params = np.zeros(exog.shape, dtype=float)\n    scale = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        if offset is not None:\n            offset_ = offset[inidx]\n        if exposure is not None:\n            exposure_ = exposure[inidx]\n        if n_trials is not None:\n            init_kwds['n_trials'] = n_trials[inidx]\n        mod_i = self.model_class(endog[inidx], exog[inidx], offset=offset_, exposure=exposure_, freq_weights=freq_weights[inidx], var_weights=var_weights[inidx], **init_kwds)\n        if is_binomial:\n            mod_i.family.n = init_kwds['n_trials']\n        res_i = mod_i.fit(start_params=self.results.params, method='newton')\n        params[outidx] = res_i.params.copy()\n        scale[outidx] = res_i.scale\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, scale=scale, mse_resid=scale, det_cov_params=det_cov_params)",
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        Reestimates the model with endog and exog dropping one observation\\n        at a time\\n\\n        This uses a nobs loop, only attributes of the results instance are\\n        stored.\\n\\n        Warning: This will need refactoring and API changes to be able to\\n        add options.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    get_det_cov_params = lambda res: np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    init_kwds = self.results.model._get_init_kwds()\n    freq_weights = init_kwds.pop('freq_weights')\n    var_weights = init_kwds.pop('var_weights')\n    offset = offset_ = init_kwds.pop('offset')\n    exposure = exposure_ = init_kwds.pop('exposure')\n    n_trials = init_kwds.pop('n_trials', None)\n    if hasattr(init_kwds['family'], 'initialize'):\n        is_binomial = True\n    else:\n        is_binomial = False\n    params = np.zeros(exog.shape, dtype=float)\n    scale = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        if offset is not None:\n            offset_ = offset[inidx]\n        if exposure is not None:\n            exposure_ = exposure[inidx]\n        if n_trials is not None:\n            init_kwds['n_trials'] = n_trials[inidx]\n        mod_i = self.model_class(endog[inidx], exog[inidx], offset=offset_, exposure=exposure_, freq_weights=freq_weights[inidx], var_weights=var_weights[inidx], **init_kwds)\n        if is_binomial:\n            mod_i.family.n = init_kwds['n_trials']\n        res_i = mod_i.fit(start_params=self.results.params, method='newton')\n        params[outidx] = res_i.params.copy()\n        scale[outidx] = res_i.scale\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, scale=scale, mse_resid=scale, det_cov_params=det_cov_params)",
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        Reestimates the model with endog and exog dropping one observation\\n        at a time\\n\\n        This uses a nobs loop, only attributes of the results instance are\\n        stored.\\n\\n        Warning: This will need refactoring and API changes to be able to\\n        add options.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    get_det_cov_params = lambda res: np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    init_kwds = self.results.model._get_init_kwds()\n    freq_weights = init_kwds.pop('freq_weights')\n    var_weights = init_kwds.pop('var_weights')\n    offset = offset_ = init_kwds.pop('offset')\n    exposure = exposure_ = init_kwds.pop('exposure')\n    n_trials = init_kwds.pop('n_trials', None)\n    if hasattr(init_kwds['family'], 'initialize'):\n        is_binomial = True\n    else:\n        is_binomial = False\n    params = np.zeros(exog.shape, dtype=float)\n    scale = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        if offset is not None:\n            offset_ = offset[inidx]\n        if exposure is not None:\n            exposure_ = exposure[inidx]\n        if n_trials is not None:\n            init_kwds['n_trials'] = n_trials[inidx]\n        mod_i = self.model_class(endog[inidx], exog[inidx], offset=offset_, exposure=exposure_, freq_weights=freq_weights[inidx], var_weights=var_weights[inidx], **init_kwds)\n        if is_binomial:\n            mod_i.family.n = init_kwds['n_trials']\n        res_i = mod_i.fit(start_params=self.results.params, method='newton')\n        params[outidx] = res_i.params.copy()\n        scale[outidx] = res_i.scale\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, scale=scale, mse_resid=scale, det_cov_params=det_cov_params)",
            "@cache_readonly\ndef _res_looo(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"collect required results from the LOOO loop\\n\\n        all results will be attached.\\n        currently only 'params', 'mse_resid', 'det_cov_params' are stored\\n\\n        Reestimates the model with endog and exog dropping one observation\\n        at a time\\n\\n        This uses a nobs loop, only attributes of the results instance are\\n        stored.\\n\\n        Warning: This will need refactoring and API changes to be able to\\n        add options.\\n        \"\n    from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n    get_det_cov_params = lambda res: np.linalg.det(res.cov_params())\n    endog = self.results.model.endog\n    exog = self.results.model.exog\n    init_kwds = self.results.model._get_init_kwds()\n    freq_weights = init_kwds.pop('freq_weights')\n    var_weights = init_kwds.pop('var_weights')\n    offset = offset_ = init_kwds.pop('offset')\n    exposure = exposure_ = init_kwds.pop('exposure')\n    n_trials = init_kwds.pop('n_trials', None)\n    if hasattr(init_kwds['family'], 'initialize'):\n        is_binomial = True\n    else:\n        is_binomial = False\n    params = np.zeros(exog.shape, dtype=float)\n    scale = np.zeros(endog.shape, dtype=float)\n    det_cov_params = np.zeros(endog.shape, dtype=float)\n    cv_iter = LeaveOneOut(self.nobs)\n    for (inidx, outidx) in cv_iter:\n        if offset is not None:\n            offset_ = offset[inidx]\n        if exposure is not None:\n            exposure_ = exposure[inidx]\n        if n_trials is not None:\n            init_kwds['n_trials'] = n_trials[inidx]\n        mod_i = self.model_class(endog[inidx], exog[inidx], offset=offset_, exposure=exposure_, freq_weights=freq_weights[inidx], var_weights=var_weights[inidx], **init_kwds)\n        if is_binomial:\n            mod_i.family.n = init_kwds['n_trials']\n        res_i = mod_i.fit(start_params=self.results.params, method='newton')\n        params[outidx] = res_i.params.copy()\n        scale[outidx] = res_i.scale\n        det_cov_params[outidx] = get_det_cov_params(res_i)\n    return dict(params=params, scale=scale, mse_resid=scale, det_cov_params=det_cov_params)"
        ]
    }
]