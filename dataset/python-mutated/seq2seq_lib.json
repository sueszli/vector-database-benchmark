[
    {
        "func_name": "sequence_loss_by_example",
        "original": "def sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=True, name=None):\n    \"\"\"Sampled softmax loss for a sequence of inputs (per example).\n\n  Args:\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    loss_function: Sampled softmax function (inputs, labels) -> loss\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    name: Optional name for this operation, default: 'sequence_loss_by_example'.\n\n  Returns:\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\n\n  Raises:\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\n  \"\"\"\n    if len(targets) != len(inputs) or len(weights) != len(inputs):\n        raise ValueError('Lengths of logits, weights, and targets must be the same %d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sequence_loss_by_example'):\n        log_perp_list = []\n        for (inp, target, weight) in zip(inputs, targets, weights):\n            crossent = loss_function(inp, target)\n            log_perp_list.append(crossent * weight)\n        log_perps = tf.add_n(log_perp_list)\n        if average_across_timesteps:\n            total_size = tf.add_n(weights)\n            total_size += 1e-12\n            log_perps /= total_size\n    return log_perps",
        "mutated": [
            "def sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=True, name=None):\n    if False:\n        i = 10\n    \"Sampled softmax loss for a sequence of inputs (per example).\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    name: Optional name for this operation, default: 'sequence_loss_by_example'.\\n\\n  Returns:\\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    if len(targets) != len(inputs) or len(weights) != len(inputs):\n        raise ValueError('Lengths of logits, weights, and targets must be the same %d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sequence_loss_by_example'):\n        log_perp_list = []\n        for (inp, target, weight) in zip(inputs, targets, weights):\n            crossent = loss_function(inp, target)\n            log_perp_list.append(crossent * weight)\n        log_perps = tf.add_n(log_perp_list)\n        if average_across_timesteps:\n            total_size = tf.add_n(weights)\n            total_size += 1e-12\n            log_perps /= total_size\n    return log_perps",
            "def sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Sampled softmax loss for a sequence of inputs (per example).\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    name: Optional name for this operation, default: 'sequence_loss_by_example'.\\n\\n  Returns:\\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    if len(targets) != len(inputs) or len(weights) != len(inputs):\n        raise ValueError('Lengths of logits, weights, and targets must be the same %d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sequence_loss_by_example'):\n        log_perp_list = []\n        for (inp, target, weight) in zip(inputs, targets, weights):\n            crossent = loss_function(inp, target)\n            log_perp_list.append(crossent * weight)\n        log_perps = tf.add_n(log_perp_list)\n        if average_across_timesteps:\n            total_size = tf.add_n(weights)\n            total_size += 1e-12\n            log_perps /= total_size\n    return log_perps",
            "def sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Sampled softmax loss for a sequence of inputs (per example).\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    name: Optional name for this operation, default: 'sequence_loss_by_example'.\\n\\n  Returns:\\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    if len(targets) != len(inputs) or len(weights) != len(inputs):\n        raise ValueError('Lengths of logits, weights, and targets must be the same %d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sequence_loss_by_example'):\n        log_perp_list = []\n        for (inp, target, weight) in zip(inputs, targets, weights):\n            crossent = loss_function(inp, target)\n            log_perp_list.append(crossent * weight)\n        log_perps = tf.add_n(log_perp_list)\n        if average_across_timesteps:\n            total_size = tf.add_n(weights)\n            total_size += 1e-12\n            log_perps /= total_size\n    return log_perps",
            "def sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Sampled softmax loss for a sequence of inputs (per example).\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    name: Optional name for this operation, default: 'sequence_loss_by_example'.\\n\\n  Returns:\\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    if len(targets) != len(inputs) or len(weights) != len(inputs):\n        raise ValueError('Lengths of logits, weights, and targets must be the same %d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sequence_loss_by_example'):\n        log_perp_list = []\n        for (inp, target, weight) in zip(inputs, targets, weights):\n            crossent = loss_function(inp, target)\n            log_perp_list.append(crossent * weight)\n        log_perps = tf.add_n(log_perp_list)\n        if average_across_timesteps:\n            total_size = tf.add_n(weights)\n            total_size += 1e-12\n            log_perps /= total_size\n    return log_perps",
            "def sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Sampled softmax loss for a sequence of inputs (per example).\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    name: Optional name for this operation, default: 'sequence_loss_by_example'.\\n\\n  Returns:\\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    if len(targets) != len(inputs) or len(weights) != len(inputs):\n        raise ValueError('Lengths of logits, weights, and targets must be the same %d, %d, %d.' % (len(inputs), len(weights), len(targets)))\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sequence_loss_by_example'):\n        log_perp_list = []\n        for (inp, target, weight) in zip(inputs, targets, weights):\n            crossent = loss_function(inp, target)\n            log_perp_list.append(crossent * weight)\n        log_perps = tf.add_n(log_perp_list)\n        if average_across_timesteps:\n            total_size = tf.add_n(weights)\n            total_size += 1e-12\n            log_perps /= total_size\n    return log_perps"
        ]
    },
    {
        "func_name": "sampled_sequence_loss",
        "original": "def sampled_sequence_loss(inputs, targets, weights, loss_function, average_across_timesteps=True, average_across_batch=True, name=None):\n    \"\"\"Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n\n  Args:\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\n    targets: List of 1D batch-sized int32 Tensors of the same length as inputs.\n    weights: List of 1D batch-sized float-Tensors of the same length as inputs.\n    loss_function: Sampled softmax function (inputs, labels) -> loss\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    average_across_batch: If set, divide the returned cost by the batch size.\n    name: Optional name for this operation, defaults to 'sequence_loss'.\n\n  Returns:\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\n\n  Raises:\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\n  \"\"\"\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sampled_sequence_loss'):\n        cost = tf.reduce_sum(sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=average_across_timesteps))\n        if average_across_batch:\n            batch_size = tf.shape(targets[0])[0]\n            return cost / tf.cast(batch_size, tf.float32)\n        else:\n            return cost",
        "mutated": [
            "def sampled_sequence_loss(inputs, targets, weights, loss_function, average_across_timesteps=True, average_across_batch=True, name=None):\n    if False:\n        i = 10\n    \"Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as inputs.\\n    weights: List of 1D batch-sized float-Tensors of the same length as inputs.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    average_across_batch: If set, divide the returned cost by the batch size.\\n    name: Optional name for this operation, defaults to 'sequence_loss'.\\n\\n  Returns:\\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sampled_sequence_loss'):\n        cost = tf.reduce_sum(sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=average_across_timesteps))\n        if average_across_batch:\n            batch_size = tf.shape(targets[0])[0]\n            return cost / tf.cast(batch_size, tf.float32)\n        else:\n            return cost",
            "def sampled_sequence_loss(inputs, targets, weights, loss_function, average_across_timesteps=True, average_across_batch=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as inputs.\\n    weights: List of 1D batch-sized float-Tensors of the same length as inputs.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    average_across_batch: If set, divide the returned cost by the batch size.\\n    name: Optional name for this operation, defaults to 'sequence_loss'.\\n\\n  Returns:\\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sampled_sequence_loss'):\n        cost = tf.reduce_sum(sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=average_across_timesteps))\n        if average_across_batch:\n            batch_size = tf.shape(targets[0])[0]\n            return cost / tf.cast(batch_size, tf.float32)\n        else:\n            return cost",
            "def sampled_sequence_loss(inputs, targets, weights, loss_function, average_across_timesteps=True, average_across_batch=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as inputs.\\n    weights: List of 1D batch-sized float-Tensors of the same length as inputs.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    average_across_batch: If set, divide the returned cost by the batch size.\\n    name: Optional name for this operation, defaults to 'sequence_loss'.\\n\\n  Returns:\\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sampled_sequence_loss'):\n        cost = tf.reduce_sum(sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=average_across_timesteps))\n        if average_across_batch:\n            batch_size = tf.shape(targets[0])[0]\n            return cost / tf.cast(batch_size, tf.float32)\n        else:\n            return cost",
            "def sampled_sequence_loss(inputs, targets, weights, loss_function, average_across_timesteps=True, average_across_batch=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as inputs.\\n    weights: List of 1D batch-sized float-Tensors of the same length as inputs.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    average_across_batch: If set, divide the returned cost by the batch size.\\n    name: Optional name for this operation, defaults to 'sequence_loss'.\\n\\n  Returns:\\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sampled_sequence_loss'):\n        cost = tf.reduce_sum(sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=average_across_timesteps))\n        if average_across_batch:\n            batch_size = tf.shape(targets[0])[0]\n            return cost / tf.cast(batch_size, tf.float32)\n        else:\n            return cost",
            "def sampled_sequence_loss(inputs, targets, weights, loss_function, average_across_timesteps=True, average_across_batch=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\\n\\n  Args:\\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\\n    targets: List of 1D batch-sized int32 Tensors of the same length as inputs.\\n    weights: List of 1D batch-sized float-Tensors of the same length as inputs.\\n    loss_function: Sampled softmax function (inputs, labels) -> loss\\n    average_across_timesteps: If set, divide the returned cost by the total\\n      label weight.\\n    average_across_batch: If set, divide the returned cost by the batch size.\\n    name: Optional name for this operation, defaults to 'sequence_loss'.\\n\\n  Returns:\\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\\n\\n  Raises:\\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\\n  \"\n    with tf.name_scope(values=inputs + targets + weights, name=name, default_name='sampled_sequence_loss'):\n        cost = tf.reduce_sum(sequence_loss_by_example(inputs, targets, weights, loss_function, average_across_timesteps=average_across_timesteps))\n        if average_across_batch:\n            batch_size = tf.shape(targets[0])[0]\n            return cost / tf.cast(batch_size, tf.float32)\n        else:\n            return cost"
        ]
    },
    {
        "func_name": "linear",
        "original": "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n    \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"\n    if args is None or (isinstance(args, (list, tuple)) and (not args)):\n        raise ValueError('`args` must be specified')\n    if not isinstance(args, (list, tuple)):\n        args = [args]\n    total_arg_size = 0\n    shapes = [a.get_shape().as_list() for a in args]\n    for shape in shapes:\n        if len(shape) != 2:\n            raise ValueError('Linear is expecting 2D arguments: %s' % str(shapes))\n        if not shape[1]:\n            raise ValueError('Linear expects shape[1] of arguments: %s' % str(shapes))\n        else:\n            total_arg_size += shape[1]\n    with tf.variable_scope(scope or 'Linear'):\n        matrix = tf.get_variable('Matrix', [total_arg_size, output_size])\n        if len(args) == 1:\n            res = tf.matmul(args[0], matrix)\n        else:\n            res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n        if not bias:\n            return res\n        bias_term = tf.get_variable('Bias', [output_size], initializer=tf.constant_initializer(bias_start))\n    return res + bias_term",
        "mutated": [
            "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n    if False:\n        i = 10\n    'Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\\n\\n  Args:\\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\\n    output_size: int, second dimension of W[i].\\n    bias: boolean, whether to add a bias term or not.\\n    bias_start: starting value to initialize the bias; 0 by default.\\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\\n\\n  Returns:\\n    A 2D Tensor with shape [batch x output_size] equal to\\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\\n\\n  Raises:\\n    ValueError: if some of the arguments has unspecified or wrong shape.\\n  '\n    if args is None or (isinstance(args, (list, tuple)) and (not args)):\n        raise ValueError('`args` must be specified')\n    if not isinstance(args, (list, tuple)):\n        args = [args]\n    total_arg_size = 0\n    shapes = [a.get_shape().as_list() for a in args]\n    for shape in shapes:\n        if len(shape) != 2:\n            raise ValueError('Linear is expecting 2D arguments: %s' % str(shapes))\n        if not shape[1]:\n            raise ValueError('Linear expects shape[1] of arguments: %s' % str(shapes))\n        else:\n            total_arg_size += shape[1]\n    with tf.variable_scope(scope or 'Linear'):\n        matrix = tf.get_variable('Matrix', [total_arg_size, output_size])\n        if len(args) == 1:\n            res = tf.matmul(args[0], matrix)\n        else:\n            res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n        if not bias:\n            return res\n        bias_term = tf.get_variable('Bias', [output_size], initializer=tf.constant_initializer(bias_start))\n    return res + bias_term",
            "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\\n\\n  Args:\\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\\n    output_size: int, second dimension of W[i].\\n    bias: boolean, whether to add a bias term or not.\\n    bias_start: starting value to initialize the bias; 0 by default.\\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\\n\\n  Returns:\\n    A 2D Tensor with shape [batch x output_size] equal to\\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\\n\\n  Raises:\\n    ValueError: if some of the arguments has unspecified or wrong shape.\\n  '\n    if args is None or (isinstance(args, (list, tuple)) and (not args)):\n        raise ValueError('`args` must be specified')\n    if not isinstance(args, (list, tuple)):\n        args = [args]\n    total_arg_size = 0\n    shapes = [a.get_shape().as_list() for a in args]\n    for shape in shapes:\n        if len(shape) != 2:\n            raise ValueError('Linear is expecting 2D arguments: %s' % str(shapes))\n        if not shape[1]:\n            raise ValueError('Linear expects shape[1] of arguments: %s' % str(shapes))\n        else:\n            total_arg_size += shape[1]\n    with tf.variable_scope(scope or 'Linear'):\n        matrix = tf.get_variable('Matrix', [total_arg_size, output_size])\n        if len(args) == 1:\n            res = tf.matmul(args[0], matrix)\n        else:\n            res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n        if not bias:\n            return res\n        bias_term = tf.get_variable('Bias', [output_size], initializer=tf.constant_initializer(bias_start))\n    return res + bias_term",
            "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\\n\\n  Args:\\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\\n    output_size: int, second dimension of W[i].\\n    bias: boolean, whether to add a bias term or not.\\n    bias_start: starting value to initialize the bias; 0 by default.\\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\\n\\n  Returns:\\n    A 2D Tensor with shape [batch x output_size] equal to\\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\\n\\n  Raises:\\n    ValueError: if some of the arguments has unspecified or wrong shape.\\n  '\n    if args is None or (isinstance(args, (list, tuple)) and (not args)):\n        raise ValueError('`args` must be specified')\n    if not isinstance(args, (list, tuple)):\n        args = [args]\n    total_arg_size = 0\n    shapes = [a.get_shape().as_list() for a in args]\n    for shape in shapes:\n        if len(shape) != 2:\n            raise ValueError('Linear is expecting 2D arguments: %s' % str(shapes))\n        if not shape[1]:\n            raise ValueError('Linear expects shape[1] of arguments: %s' % str(shapes))\n        else:\n            total_arg_size += shape[1]\n    with tf.variable_scope(scope or 'Linear'):\n        matrix = tf.get_variable('Matrix', [total_arg_size, output_size])\n        if len(args) == 1:\n            res = tf.matmul(args[0], matrix)\n        else:\n            res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n        if not bias:\n            return res\n        bias_term = tf.get_variable('Bias', [output_size], initializer=tf.constant_initializer(bias_start))\n    return res + bias_term",
            "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\\n\\n  Args:\\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\\n    output_size: int, second dimension of W[i].\\n    bias: boolean, whether to add a bias term or not.\\n    bias_start: starting value to initialize the bias; 0 by default.\\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\\n\\n  Returns:\\n    A 2D Tensor with shape [batch x output_size] equal to\\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\\n\\n  Raises:\\n    ValueError: if some of the arguments has unspecified or wrong shape.\\n  '\n    if args is None or (isinstance(args, (list, tuple)) and (not args)):\n        raise ValueError('`args` must be specified')\n    if not isinstance(args, (list, tuple)):\n        args = [args]\n    total_arg_size = 0\n    shapes = [a.get_shape().as_list() for a in args]\n    for shape in shapes:\n        if len(shape) != 2:\n            raise ValueError('Linear is expecting 2D arguments: %s' % str(shapes))\n        if not shape[1]:\n            raise ValueError('Linear expects shape[1] of arguments: %s' % str(shapes))\n        else:\n            total_arg_size += shape[1]\n    with tf.variable_scope(scope or 'Linear'):\n        matrix = tf.get_variable('Matrix', [total_arg_size, output_size])\n        if len(args) == 1:\n            res = tf.matmul(args[0], matrix)\n        else:\n            res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n        if not bias:\n            return res\n        bias_term = tf.get_variable('Bias', [output_size], initializer=tf.constant_initializer(bias_start))\n    return res + bias_term",
            "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\\n\\n  Args:\\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\\n    output_size: int, second dimension of W[i].\\n    bias: boolean, whether to add a bias term or not.\\n    bias_start: starting value to initialize the bias; 0 by default.\\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\\n\\n  Returns:\\n    A 2D Tensor with shape [batch x output_size] equal to\\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\\n\\n  Raises:\\n    ValueError: if some of the arguments has unspecified or wrong shape.\\n  '\n    if args is None or (isinstance(args, (list, tuple)) and (not args)):\n        raise ValueError('`args` must be specified')\n    if not isinstance(args, (list, tuple)):\n        args = [args]\n    total_arg_size = 0\n    shapes = [a.get_shape().as_list() for a in args]\n    for shape in shapes:\n        if len(shape) != 2:\n            raise ValueError('Linear is expecting 2D arguments: %s' % str(shapes))\n        if not shape[1]:\n            raise ValueError('Linear expects shape[1] of arguments: %s' % str(shapes))\n        else:\n            total_arg_size += shape[1]\n    with tf.variable_scope(scope or 'Linear'):\n        matrix = tf.get_variable('Matrix', [total_arg_size, output_size])\n        if len(args) == 1:\n            res = tf.matmul(args[0], matrix)\n        else:\n            res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n        if not bias:\n            return res\n        bias_term = tf.get_variable('Bias', [output_size], initializer=tf.constant_initializer(bias_start))\n    return res + bias_term"
        ]
    }
]