[
    {
        "func_name": "anderson_statistic",
        "original": "def anderson_statistic(x, dist='norm', fit=True, params=(), axis=0):\n    \"\"\"\n    Calculate the Anderson-Darling a2 statistic.\n\n    Parameters\n    ----------\n    x : array_like\n        The data to test.\n    dist : {'norm', callable}\n        The assumed distribution under the null of test statistic.\n    fit : bool\n        If True, then the distribution parameters are estimated.\n        Currently only for 1d data x, except in case dist='norm'.\n    params : tuple\n        The optional distribution parameters if fit is False.\n    axis : int\n        If dist is 'norm' or fit is False, then data can be an n-dimensional\n        and axis specifies the axis of a variable.\n\n    Returns\n    -------\n    {float, ndarray}\n        The Anderson-Darling statistic.\n    \"\"\"\n    x = array_like(x, 'x', ndim=None)\n    fit = bool_like(fit, 'fit')\n    axis = int_like(axis, 'axis')\n    y = np.sort(x, axis=axis)\n    nobs = y.shape[axis]\n    if fit:\n        if dist == 'norm':\n            xbar = np.expand_dims(np.mean(x, axis=axis), axis)\n            s = np.expand_dims(np.std(x, ddof=1, axis=axis), axis)\n            w = (y - xbar) / s\n            z = stats.norm.cdf(w)\n        elif callable(dist):\n            params = dist.fit(x)\n            z = dist.cdf(y, *params)\n        else:\n            raise ValueError(\"dist must be 'norm' or a Callable\")\n    elif callable(dist):\n        z = dist.cdf(y, *params)\n    else:\n        raise ValueError('if fit is false, then dist must be callable')\n    i = np.arange(1, nobs + 1)\n    sl1 = [None] * x.ndim\n    sl1[axis] = slice(None)\n    sl1 = tuple(sl1)\n    sl2 = [slice(None)] * x.ndim\n    sl2[axis] = slice(None, None, -1)\n    sl2 = tuple(sl2)\n    s = np.sum((2 * i[sl1] - 1.0) / nobs * (np.log(z) + np.log1p(-z[sl2])), axis=axis)\n    a2 = -nobs - s\n    return a2",
        "mutated": [
            "def anderson_statistic(x, dist='norm', fit=True, params=(), axis=0):\n    if False:\n        i = 10\n    \"\\n    Calculate the Anderson-Darling a2 statistic.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data to test.\\n    dist : {'norm', callable}\\n        The assumed distribution under the null of test statistic.\\n    fit : bool\\n        If True, then the distribution parameters are estimated.\\n        Currently only for 1d data x, except in case dist='norm'.\\n    params : tuple\\n        The optional distribution parameters if fit is False.\\n    axis : int\\n        If dist is 'norm' or fit is False, then data can be an n-dimensional\\n        and axis specifies the axis of a variable.\\n\\n    Returns\\n    -------\\n    {float, ndarray}\\n        The Anderson-Darling statistic.\\n    \"\n    x = array_like(x, 'x', ndim=None)\n    fit = bool_like(fit, 'fit')\n    axis = int_like(axis, 'axis')\n    y = np.sort(x, axis=axis)\n    nobs = y.shape[axis]\n    if fit:\n        if dist == 'norm':\n            xbar = np.expand_dims(np.mean(x, axis=axis), axis)\n            s = np.expand_dims(np.std(x, ddof=1, axis=axis), axis)\n            w = (y - xbar) / s\n            z = stats.norm.cdf(w)\n        elif callable(dist):\n            params = dist.fit(x)\n            z = dist.cdf(y, *params)\n        else:\n            raise ValueError(\"dist must be 'norm' or a Callable\")\n    elif callable(dist):\n        z = dist.cdf(y, *params)\n    else:\n        raise ValueError('if fit is false, then dist must be callable')\n    i = np.arange(1, nobs + 1)\n    sl1 = [None] * x.ndim\n    sl1[axis] = slice(None)\n    sl1 = tuple(sl1)\n    sl2 = [slice(None)] * x.ndim\n    sl2[axis] = slice(None, None, -1)\n    sl2 = tuple(sl2)\n    s = np.sum((2 * i[sl1] - 1.0) / nobs * (np.log(z) + np.log1p(-z[sl2])), axis=axis)\n    a2 = -nobs - s\n    return a2",
            "def anderson_statistic(x, dist='norm', fit=True, params=(), axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Calculate the Anderson-Darling a2 statistic.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data to test.\\n    dist : {'norm', callable}\\n        The assumed distribution under the null of test statistic.\\n    fit : bool\\n        If True, then the distribution parameters are estimated.\\n        Currently only for 1d data x, except in case dist='norm'.\\n    params : tuple\\n        The optional distribution parameters if fit is False.\\n    axis : int\\n        If dist is 'norm' or fit is False, then data can be an n-dimensional\\n        and axis specifies the axis of a variable.\\n\\n    Returns\\n    -------\\n    {float, ndarray}\\n        The Anderson-Darling statistic.\\n    \"\n    x = array_like(x, 'x', ndim=None)\n    fit = bool_like(fit, 'fit')\n    axis = int_like(axis, 'axis')\n    y = np.sort(x, axis=axis)\n    nobs = y.shape[axis]\n    if fit:\n        if dist == 'norm':\n            xbar = np.expand_dims(np.mean(x, axis=axis), axis)\n            s = np.expand_dims(np.std(x, ddof=1, axis=axis), axis)\n            w = (y - xbar) / s\n            z = stats.norm.cdf(w)\n        elif callable(dist):\n            params = dist.fit(x)\n            z = dist.cdf(y, *params)\n        else:\n            raise ValueError(\"dist must be 'norm' or a Callable\")\n    elif callable(dist):\n        z = dist.cdf(y, *params)\n    else:\n        raise ValueError('if fit is false, then dist must be callable')\n    i = np.arange(1, nobs + 1)\n    sl1 = [None] * x.ndim\n    sl1[axis] = slice(None)\n    sl1 = tuple(sl1)\n    sl2 = [slice(None)] * x.ndim\n    sl2[axis] = slice(None, None, -1)\n    sl2 = tuple(sl2)\n    s = np.sum((2 * i[sl1] - 1.0) / nobs * (np.log(z) + np.log1p(-z[sl2])), axis=axis)\n    a2 = -nobs - s\n    return a2",
            "def anderson_statistic(x, dist='norm', fit=True, params=(), axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Calculate the Anderson-Darling a2 statistic.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data to test.\\n    dist : {'norm', callable}\\n        The assumed distribution under the null of test statistic.\\n    fit : bool\\n        If True, then the distribution parameters are estimated.\\n        Currently only for 1d data x, except in case dist='norm'.\\n    params : tuple\\n        The optional distribution parameters if fit is False.\\n    axis : int\\n        If dist is 'norm' or fit is False, then data can be an n-dimensional\\n        and axis specifies the axis of a variable.\\n\\n    Returns\\n    -------\\n    {float, ndarray}\\n        The Anderson-Darling statistic.\\n    \"\n    x = array_like(x, 'x', ndim=None)\n    fit = bool_like(fit, 'fit')\n    axis = int_like(axis, 'axis')\n    y = np.sort(x, axis=axis)\n    nobs = y.shape[axis]\n    if fit:\n        if dist == 'norm':\n            xbar = np.expand_dims(np.mean(x, axis=axis), axis)\n            s = np.expand_dims(np.std(x, ddof=1, axis=axis), axis)\n            w = (y - xbar) / s\n            z = stats.norm.cdf(w)\n        elif callable(dist):\n            params = dist.fit(x)\n            z = dist.cdf(y, *params)\n        else:\n            raise ValueError(\"dist must be 'norm' or a Callable\")\n    elif callable(dist):\n        z = dist.cdf(y, *params)\n    else:\n        raise ValueError('if fit is false, then dist must be callable')\n    i = np.arange(1, nobs + 1)\n    sl1 = [None] * x.ndim\n    sl1[axis] = slice(None)\n    sl1 = tuple(sl1)\n    sl2 = [slice(None)] * x.ndim\n    sl2[axis] = slice(None, None, -1)\n    sl2 = tuple(sl2)\n    s = np.sum((2 * i[sl1] - 1.0) / nobs * (np.log(z) + np.log1p(-z[sl2])), axis=axis)\n    a2 = -nobs - s\n    return a2",
            "def anderson_statistic(x, dist='norm', fit=True, params=(), axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Calculate the Anderson-Darling a2 statistic.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data to test.\\n    dist : {'norm', callable}\\n        The assumed distribution under the null of test statistic.\\n    fit : bool\\n        If True, then the distribution parameters are estimated.\\n        Currently only for 1d data x, except in case dist='norm'.\\n    params : tuple\\n        The optional distribution parameters if fit is False.\\n    axis : int\\n        If dist is 'norm' or fit is False, then data can be an n-dimensional\\n        and axis specifies the axis of a variable.\\n\\n    Returns\\n    -------\\n    {float, ndarray}\\n        The Anderson-Darling statistic.\\n    \"\n    x = array_like(x, 'x', ndim=None)\n    fit = bool_like(fit, 'fit')\n    axis = int_like(axis, 'axis')\n    y = np.sort(x, axis=axis)\n    nobs = y.shape[axis]\n    if fit:\n        if dist == 'norm':\n            xbar = np.expand_dims(np.mean(x, axis=axis), axis)\n            s = np.expand_dims(np.std(x, ddof=1, axis=axis), axis)\n            w = (y - xbar) / s\n            z = stats.norm.cdf(w)\n        elif callable(dist):\n            params = dist.fit(x)\n            z = dist.cdf(y, *params)\n        else:\n            raise ValueError(\"dist must be 'norm' or a Callable\")\n    elif callable(dist):\n        z = dist.cdf(y, *params)\n    else:\n        raise ValueError('if fit is false, then dist must be callable')\n    i = np.arange(1, nobs + 1)\n    sl1 = [None] * x.ndim\n    sl1[axis] = slice(None)\n    sl1 = tuple(sl1)\n    sl2 = [slice(None)] * x.ndim\n    sl2[axis] = slice(None, None, -1)\n    sl2 = tuple(sl2)\n    s = np.sum((2 * i[sl1] - 1.0) / nobs * (np.log(z) + np.log1p(-z[sl2])), axis=axis)\n    a2 = -nobs - s\n    return a2",
            "def anderson_statistic(x, dist='norm', fit=True, params=(), axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Calculate the Anderson-Darling a2 statistic.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data to test.\\n    dist : {'norm', callable}\\n        The assumed distribution under the null of test statistic.\\n    fit : bool\\n        If True, then the distribution parameters are estimated.\\n        Currently only for 1d data x, except in case dist='norm'.\\n    params : tuple\\n        The optional distribution parameters if fit is False.\\n    axis : int\\n        If dist is 'norm' or fit is False, then data can be an n-dimensional\\n        and axis specifies the axis of a variable.\\n\\n    Returns\\n    -------\\n    {float, ndarray}\\n        The Anderson-Darling statistic.\\n    \"\n    x = array_like(x, 'x', ndim=None)\n    fit = bool_like(fit, 'fit')\n    axis = int_like(axis, 'axis')\n    y = np.sort(x, axis=axis)\n    nobs = y.shape[axis]\n    if fit:\n        if dist == 'norm':\n            xbar = np.expand_dims(np.mean(x, axis=axis), axis)\n            s = np.expand_dims(np.std(x, ddof=1, axis=axis), axis)\n            w = (y - xbar) / s\n            z = stats.norm.cdf(w)\n        elif callable(dist):\n            params = dist.fit(x)\n            z = dist.cdf(y, *params)\n        else:\n            raise ValueError(\"dist must be 'norm' or a Callable\")\n    elif callable(dist):\n        z = dist.cdf(y, *params)\n    else:\n        raise ValueError('if fit is false, then dist must be callable')\n    i = np.arange(1, nobs + 1)\n    sl1 = [None] * x.ndim\n    sl1[axis] = slice(None)\n    sl1 = tuple(sl1)\n    sl2 = [slice(None)] * x.ndim\n    sl2[axis] = slice(None, None, -1)\n    sl2 = tuple(sl2)\n    s = np.sum((2 * i[sl1] - 1.0) / nobs * (np.log(z) + np.log1p(-z[sl2])), axis=axis)\n    a2 = -nobs - s\n    return a2"
        ]
    },
    {
        "func_name": "normal_ad",
        "original": "def normal_ad(x, axis=0):\n    \"\"\"\n    Anderson-Darling test for normal distribution unknown mean and variance.\n\n    Parameters\n    ----------\n    x : array_like\n        The data array.\n    axis : int\n        The axis to perform the test along.\n\n    Returns\n    -------\n    ad2 : float\n        Anderson Darling test statistic.\n    pval : float\n        The pvalue for hypothesis that the data comes from a normal\n        distribution with unknown mean and variance.\n\n    See Also\n    --------\n    statsmodels.stats.diagnostic.anderson_statistic\n        The Anderson-Darling a2 statistic.\n    statsmodels.stats.diagnostic.kstest_fit\n        Kolmogorov-Smirnov test with estimated parameters for Normal or\n        Exponential distributions.\n    \"\"\"\n    ad2 = anderson_statistic(x, dist='norm', fit=True, axis=axis)\n    n = x.shape[axis]\n    ad2a = ad2 * (1 + 0.75 / n + 2.25 / n ** 2)\n    if np.size(ad2a) == 1:\n        if ad2a >= 0.0 and ad2a < 0.2:\n            pval = 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        elif ad2a < 0.34:\n            pval = 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        elif ad2a < 0.6:\n            pval = np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        elif ad2a <= 13:\n            pval = np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        else:\n            pval = 0.0\n    else:\n        bounds = np.array([0.0, 0.2, 0.34, 0.6])\n        pval0 = lambda ad2a: np.nan * np.ones_like(ad2a)\n        pval1 = lambda ad2a: 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        pval2 = lambda ad2a: 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        pval3 = lambda ad2a: np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        pval4 = lambda ad2a: np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        pvalli = [pval0, pval1, pval2, pval3, pval4]\n        idx = np.searchsorted(bounds, ad2a, side='right')\n        pval = np.nan * np.ones_like(ad2a)\n        for i in range(5):\n            mask = idx == i\n            pval[mask] = pvalli[i](ad2a[mask])\n    return (ad2, pval)",
        "mutated": [
            "def normal_ad(x, axis=0):\n    if False:\n        i = 10\n    '\\n    Anderson-Darling test for normal distribution unknown mean and variance.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data array.\\n    axis : int\\n        The axis to perform the test along.\\n\\n    Returns\\n    -------\\n    ad2 : float\\n        Anderson Darling test statistic.\\n    pval : float\\n        The pvalue for hypothesis that the data comes from a normal\\n        distribution with unknown mean and variance.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.diagnostic.anderson_statistic\\n        The Anderson-Darling a2 statistic.\\n    statsmodels.stats.diagnostic.kstest_fit\\n        Kolmogorov-Smirnov test with estimated parameters for Normal or\\n        Exponential distributions.\\n    '\n    ad2 = anderson_statistic(x, dist='norm', fit=True, axis=axis)\n    n = x.shape[axis]\n    ad2a = ad2 * (1 + 0.75 / n + 2.25 / n ** 2)\n    if np.size(ad2a) == 1:\n        if ad2a >= 0.0 and ad2a < 0.2:\n            pval = 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        elif ad2a < 0.34:\n            pval = 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        elif ad2a < 0.6:\n            pval = np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        elif ad2a <= 13:\n            pval = np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        else:\n            pval = 0.0\n    else:\n        bounds = np.array([0.0, 0.2, 0.34, 0.6])\n        pval0 = lambda ad2a: np.nan * np.ones_like(ad2a)\n        pval1 = lambda ad2a: 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        pval2 = lambda ad2a: 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        pval3 = lambda ad2a: np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        pval4 = lambda ad2a: np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        pvalli = [pval0, pval1, pval2, pval3, pval4]\n        idx = np.searchsorted(bounds, ad2a, side='right')\n        pval = np.nan * np.ones_like(ad2a)\n        for i in range(5):\n            mask = idx == i\n            pval[mask] = pvalli[i](ad2a[mask])\n    return (ad2, pval)",
            "def normal_ad(x, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Anderson-Darling test for normal distribution unknown mean and variance.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data array.\\n    axis : int\\n        The axis to perform the test along.\\n\\n    Returns\\n    -------\\n    ad2 : float\\n        Anderson Darling test statistic.\\n    pval : float\\n        The pvalue for hypothesis that the data comes from a normal\\n        distribution with unknown mean and variance.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.diagnostic.anderson_statistic\\n        The Anderson-Darling a2 statistic.\\n    statsmodels.stats.diagnostic.kstest_fit\\n        Kolmogorov-Smirnov test with estimated parameters for Normal or\\n        Exponential distributions.\\n    '\n    ad2 = anderson_statistic(x, dist='norm', fit=True, axis=axis)\n    n = x.shape[axis]\n    ad2a = ad2 * (1 + 0.75 / n + 2.25 / n ** 2)\n    if np.size(ad2a) == 1:\n        if ad2a >= 0.0 and ad2a < 0.2:\n            pval = 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        elif ad2a < 0.34:\n            pval = 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        elif ad2a < 0.6:\n            pval = np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        elif ad2a <= 13:\n            pval = np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        else:\n            pval = 0.0\n    else:\n        bounds = np.array([0.0, 0.2, 0.34, 0.6])\n        pval0 = lambda ad2a: np.nan * np.ones_like(ad2a)\n        pval1 = lambda ad2a: 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        pval2 = lambda ad2a: 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        pval3 = lambda ad2a: np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        pval4 = lambda ad2a: np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        pvalli = [pval0, pval1, pval2, pval3, pval4]\n        idx = np.searchsorted(bounds, ad2a, side='right')\n        pval = np.nan * np.ones_like(ad2a)\n        for i in range(5):\n            mask = idx == i\n            pval[mask] = pvalli[i](ad2a[mask])\n    return (ad2, pval)",
            "def normal_ad(x, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Anderson-Darling test for normal distribution unknown mean and variance.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data array.\\n    axis : int\\n        The axis to perform the test along.\\n\\n    Returns\\n    -------\\n    ad2 : float\\n        Anderson Darling test statistic.\\n    pval : float\\n        The pvalue for hypothesis that the data comes from a normal\\n        distribution with unknown mean and variance.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.diagnostic.anderson_statistic\\n        The Anderson-Darling a2 statistic.\\n    statsmodels.stats.diagnostic.kstest_fit\\n        Kolmogorov-Smirnov test with estimated parameters for Normal or\\n        Exponential distributions.\\n    '\n    ad2 = anderson_statistic(x, dist='norm', fit=True, axis=axis)\n    n = x.shape[axis]\n    ad2a = ad2 * (1 + 0.75 / n + 2.25 / n ** 2)\n    if np.size(ad2a) == 1:\n        if ad2a >= 0.0 and ad2a < 0.2:\n            pval = 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        elif ad2a < 0.34:\n            pval = 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        elif ad2a < 0.6:\n            pval = np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        elif ad2a <= 13:\n            pval = np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        else:\n            pval = 0.0\n    else:\n        bounds = np.array([0.0, 0.2, 0.34, 0.6])\n        pval0 = lambda ad2a: np.nan * np.ones_like(ad2a)\n        pval1 = lambda ad2a: 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        pval2 = lambda ad2a: 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        pval3 = lambda ad2a: np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        pval4 = lambda ad2a: np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        pvalli = [pval0, pval1, pval2, pval3, pval4]\n        idx = np.searchsorted(bounds, ad2a, side='right')\n        pval = np.nan * np.ones_like(ad2a)\n        for i in range(5):\n            mask = idx == i\n            pval[mask] = pvalli[i](ad2a[mask])\n    return (ad2, pval)",
            "def normal_ad(x, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Anderson-Darling test for normal distribution unknown mean and variance.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data array.\\n    axis : int\\n        The axis to perform the test along.\\n\\n    Returns\\n    -------\\n    ad2 : float\\n        Anderson Darling test statistic.\\n    pval : float\\n        The pvalue for hypothesis that the data comes from a normal\\n        distribution with unknown mean and variance.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.diagnostic.anderson_statistic\\n        The Anderson-Darling a2 statistic.\\n    statsmodels.stats.diagnostic.kstest_fit\\n        Kolmogorov-Smirnov test with estimated parameters for Normal or\\n        Exponential distributions.\\n    '\n    ad2 = anderson_statistic(x, dist='norm', fit=True, axis=axis)\n    n = x.shape[axis]\n    ad2a = ad2 * (1 + 0.75 / n + 2.25 / n ** 2)\n    if np.size(ad2a) == 1:\n        if ad2a >= 0.0 and ad2a < 0.2:\n            pval = 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        elif ad2a < 0.34:\n            pval = 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        elif ad2a < 0.6:\n            pval = np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        elif ad2a <= 13:\n            pval = np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        else:\n            pval = 0.0\n    else:\n        bounds = np.array([0.0, 0.2, 0.34, 0.6])\n        pval0 = lambda ad2a: np.nan * np.ones_like(ad2a)\n        pval1 = lambda ad2a: 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        pval2 = lambda ad2a: 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        pval3 = lambda ad2a: np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        pval4 = lambda ad2a: np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        pvalli = [pval0, pval1, pval2, pval3, pval4]\n        idx = np.searchsorted(bounds, ad2a, side='right')\n        pval = np.nan * np.ones_like(ad2a)\n        for i in range(5):\n            mask = idx == i\n            pval[mask] = pvalli[i](ad2a[mask])\n    return (ad2, pval)",
            "def normal_ad(x, axis=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Anderson-Darling test for normal distribution unknown mean and variance.\\n\\n    Parameters\\n    ----------\\n    x : array_like\\n        The data array.\\n    axis : int\\n        The axis to perform the test along.\\n\\n    Returns\\n    -------\\n    ad2 : float\\n        Anderson Darling test statistic.\\n    pval : float\\n        The pvalue for hypothesis that the data comes from a normal\\n        distribution with unknown mean and variance.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.diagnostic.anderson_statistic\\n        The Anderson-Darling a2 statistic.\\n    statsmodels.stats.diagnostic.kstest_fit\\n        Kolmogorov-Smirnov test with estimated parameters for Normal or\\n        Exponential distributions.\\n    '\n    ad2 = anderson_statistic(x, dist='norm', fit=True, axis=axis)\n    n = x.shape[axis]\n    ad2a = ad2 * (1 + 0.75 / n + 2.25 / n ** 2)\n    if np.size(ad2a) == 1:\n        if ad2a >= 0.0 and ad2a < 0.2:\n            pval = 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        elif ad2a < 0.34:\n            pval = 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        elif ad2a < 0.6:\n            pval = np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        elif ad2a <= 13:\n            pval = np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        else:\n            pval = 0.0\n    else:\n        bounds = np.array([0.0, 0.2, 0.34, 0.6])\n        pval0 = lambda ad2a: np.nan * np.ones_like(ad2a)\n        pval1 = lambda ad2a: 1 - np.exp(-13.436 + 101.14 * ad2a - 223.73 * ad2a ** 2)\n        pval2 = lambda ad2a: 1 - np.exp(-8.318 + 42.796 * ad2a - 59.938 * ad2a ** 2)\n        pval3 = lambda ad2a: np.exp(0.9177 - 4.279 * ad2a - 1.38 * ad2a ** 2)\n        pval4 = lambda ad2a: np.exp(1.2937 - 5.709 * ad2a + 0.0186 * ad2a ** 2)\n        pvalli = [pval0, pval1, pval2, pval3, pval4]\n        idx = np.searchsorted(bounds, ad2a, side='right')\n        pval = np.nan * np.ones_like(ad2a)\n        for i in range(5):\n            mask = idx == i\n            pval[mask] = pvalli[i](ad2a[mask])\n    return (ad2, pval)"
        ]
    }
]