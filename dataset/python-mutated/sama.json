[
    {
        "func_name": "download_sama_coco_dataset_split",
        "original": "def download_sama_coco_dataset_split(dataset_dir, split, label_types=None, classes=None, image_ids=None, num_workers=None, shuffle=None, seed=None, max_samples=None, raw_dir=None, scratch_dir=None):\n    \"\"\"Utility that downloads full or partial data splits of the\n    `COCO dataset <https://cocodataset.org>`_ with annotation splits found\n    at https://www.sama.com/sama-coco-dataset.\n\n    See :ref:`this page <COCODetectionDataset-export>` for the format in which\n    ``dataset_dir`` will be arranged.\n\n    Any existing files are not re-downloaded.\n\n    Args:\n        dataset_dir: the directory to download the dataset\n        split: the split to download. Supported values are\n            ``(\"train\", \"validation\", \"test\")``\n        label_types (None): a label type or list of label types to load. The\n            supported values are ``(\"detections\", \"segmentations\")``. By\n            default, all label types are loaded\n        classes (None): a string or list of strings specifying required classes\n            to load. Only samples containing at least one instance of a\n            specified class will be loaded\n        image_ids (None): an optional list of specific image IDs to load. Can\n            be provided in any of the following formats:\n\n            -   a list of ``<image-id>`` ints or strings\n            -   a list of ``<split>/<image-id>`` strings\n            -   the path to a text (newline-separated), JSON, or CSV file\n                containing the list of image IDs to load in either of the first\n                two formats\n        num_workers (None): a suggested number of threads to use when\n            downloading individual images\n        shuffle (False): whether to randomly shuffle the order in which samples\n            are chosen for partial downloads\n        seed (None): a random seed to use when shuffling\n        max_samples (None): a maximum number of samples to load. If\n            ``label_types`` and/or ``classes`` are also specified, first\n            priority will be given to samples that contain all of the specified\n            label types and/or classes, followed by samples that contain at\n            least one of the specified labels types or classes. The actual\n            number of samples loaded may be less than this maximum value if the\n            dataset does not contain sufficient samples matching your\n            requirements. By default, all matching samples are loaded\n        raw_dir (None): a directory in which full annotations files may be\n            stored to avoid re-downloads in the future\n        scratch_dir (None): a scratch directory to use to download any\n            necessary temporary files\n\n    Returns:\n        a tuple of:\n        -   num_samples: the total number of downloaded images\n        -   classes: the list of all classes\n        -   did_download: whether any content was downloaded (True) or if all\n            necessary files were already downloaded (False)\n    \"\"\"\n    if split not in _IMAGE_DOWNLOAD_LINKS:\n        raise ValueError(\"Unsupported split '%s'; supported values are %s\" % (split, tuple(_IMAGE_DOWNLOAD_LINKS.keys())))\n    if classes is not None and split == 'test':\n        logger.warning('Test split is unlabeled; ignoring classes requirement')\n        classes = None\n    if scratch_dir is None:\n        scratch_dir = os.path.join(dataset_dir, 'scratch')\n    anno_path = os.path.join(dataset_dir, 'labels.json')\n    images_dir = os.path.join(dataset_dir, 'data')\n    split_size = _SPLIT_SIZES[split]\n    etau.ensure_dir(images_dir)\n    did_download = False\n    if raw_dir is None:\n        raw_dir = os.path.join(dataset_dir, 'raw')\n    etau.ensure_dir(raw_dir)\n    if split != 'test':\n        src_path = _ANNOTATION_DOWNLOAD_LINKS[split]\n        rel_path = _ANNOTATION_PATHS[split]\n        subdir = split\n        anno_type = 'annotations'\n    else:\n        src_path = _TEST_INFO_DOWNLOAD_LINK\n        rel_path = _TEST_INFO_PATHS\n        subdir = 'test'\n        anno_type = 'test info'\n    zip_path = os.path.join(scratch_dir, os.path.basename(src_path))\n    unzip_dir = os.path.join(scratch_dir, subdir)\n    content_dir = os.path.join(unzip_dir, os.path.dirname(rel_path))\n    full_anno_path = os.path.join(raw_dir, os.path.basename(rel_path))\n    if not os.path.isfile(full_anno_path):\n        logger.info(\"Downloading %s to '%s'\", anno_type, zip_path)\n        etaw.download_file(src_path, path=zip_path)\n        logger.info(\"Extracting %s to '%s'\", anno_type, full_anno_path)\n        if split != 'test':\n            merge_dir = tempfile.TemporaryDirectory()\n            etau.extract_zip(zip_path, outdir=merge_dir.name, delete_zip=False)\n            _merge_annotations(merge_dir.name, os.path.join(unzip_dir, 'annotations', f'sama_coco_{split}.json'))\n            merge_dir.cleanup()\n        else:\n            etau.extract_zip(zip_path, outdir=unzip_dir, delete_zip=False)\n        fouc._merge_dir(content_dir, raw_dir)\n        did_download = True\n    else:\n        logger.info(\"Found %s at '%s'\", anno_type, full_anno_path)\n    d = None\n    all_classes = None\n    images_src_path = _IMAGE_DOWNLOAD_LINKS[split]\n    images_zip_path = os.path.join(scratch_dir, os.path.basename(images_src_path))\n    unzip_images_dir = os.path.splitext(images_zip_path)[0]\n    if classes is None and image_ids is None and (max_samples is None):\n        num_existing = len(etau.list_files(images_dir))\n        num_download = split_size - num_existing\n        if num_download > 0:\n            if num_existing > 0:\n                logger.info('Found %d (< %d) downloaded images; must download full image zip', num_existing, split_size)\n            logger.info(\"Downloading images to '%s'\", images_zip_path)\n            etaw.download_file(images_src_path, path=images_zip_path)\n            logger.info(\"Extracting images to '%s'\", images_dir)\n            etau.extract_zip(images_zip_path, delete_zip=False)\n            etau.move_dir(unzip_images_dir, images_dir)\n            did_download = True\n        else:\n            logger.info('Images already downloaded')\n    else:\n        d = etas.load_json(full_anno_path)\n        (_, all_classes, _, images, annotations) = fouc._parse_coco_detection_annotations(d, extra_attrs=True)\n        if image_ids is not None:\n            image_ids = fouc._parse_image_ids(image_ids, images, split=split)\n        else:\n            image_ids = list(images.keys())\n        if classes is not None:\n            (all_ids, any_ids) = fouc._get_images_with_classes(image_ids, annotations, classes, all_classes)\n        else:\n            all_ids = image_ids\n            any_ids = []\n        all_ids = sorted(all_ids)\n        any_ids = sorted(any_ids)\n        if shuffle:\n            if seed is not None:\n                random.seed(seed)\n            random.shuffle(all_ids)\n            random.shuffle(any_ids)\n        image_ids = all_ids + any_ids\n        (existing_ids, downloadable_ids) = fouc._get_existing_ids(images_dir, images, image_ids)\n        if max_samples is not None:\n            num_existing = len(existing_ids)\n            num_downloadable = len(downloadable_ids)\n            num_available = num_existing + num_downloadable\n            if num_available < max_samples:\n                logger.warning('Only found %d (<%d) samples matching your requirements', num_available, max_samples)\n            if max_samples > num_existing:\n                num_download = max_samples - num_existing\n                download_ids = downloadable_ids[:num_download]\n            else:\n                download_ids = []\n        else:\n            download_ids = downloadable_ids\n        num_existing = len(existing_ids)\n        num_download = len(download_ids)\n        if num_existing > 0:\n            if num_download > 0:\n                logger.info('%d images found; downloading the remaining %d', num_existing, num_download)\n            else:\n                logger.info('Sufficient images already downloaded')\n        elif num_download > 0:\n            logger.info('Downloading %d images', num_download)\n        if num_download > 0:\n            fouc._download_images(images_dir, download_ids, images, num_workers)\n            did_download = True\n    downloaded_filenames = etau.list_files(images_dir)\n    num_samples = len(downloaded_filenames)\n    if not os.path.isfile(anno_path):\n        did_download = True\n    if did_download:\n        if d is None:\n            d = etas.load_json(full_anno_path)\n            categories = d.get('categories', None)\n            if categories is not None:\n                (all_classes, _) = fouc.parse_coco_categories(categories)\n            else:\n                all_classes = None\n        if num_samples >= split_size:\n            logger.info(\"Writing annotations to '%s'\", anno_path)\n            etau.copy_file(full_anno_path, anno_path)\n        else:\n            logger.info(\"Writing annotations for %d downloaded samples to '%s'\", num_samples, anno_path)\n            fouc._write_partial_annotations(d, anno_path, split, downloaded_filenames)\n    return (num_samples, all_classes, did_download)",
        "mutated": [
            "def download_sama_coco_dataset_split(dataset_dir, split, label_types=None, classes=None, image_ids=None, num_workers=None, shuffle=None, seed=None, max_samples=None, raw_dir=None, scratch_dir=None):\n    if False:\n        i = 10\n    'Utility that downloads full or partial data splits of the\\n    `COCO dataset <https://cocodataset.org>`_ with annotation splits found\\n    at https://www.sama.com/sama-coco-dataset.\\n\\n    See :ref:`this page <COCODetectionDataset-export>` for the format in which\\n    ``dataset_dir`` will be arranged.\\n\\n    Any existing files are not re-downloaded.\\n\\n    Args:\\n        dataset_dir: the directory to download the dataset\\n        split: the split to download. Supported values are\\n            ``(\"train\", \"validation\", \"test\")``\\n        label_types (None): a label type or list of label types to load. The\\n            supported values are ``(\"detections\", \"segmentations\")``. By\\n            default, all label types are loaded\\n        classes (None): a string or list of strings specifying required classes\\n            to load. Only samples containing at least one instance of a\\n            specified class will be loaded\\n        image_ids (None): an optional list of specific image IDs to load. Can\\n            be provided in any of the following formats:\\n\\n            -   a list of ``<image-id>`` ints or strings\\n            -   a list of ``<split>/<image-id>`` strings\\n            -   the path to a text (newline-separated), JSON, or CSV file\\n                containing the list of image IDs to load in either of the first\\n                two formats\\n        num_workers (None): a suggested number of threads to use when\\n            downloading individual images\\n        shuffle (False): whether to randomly shuffle the order in which samples\\n            are chosen for partial downloads\\n        seed (None): a random seed to use when shuffling\\n        max_samples (None): a maximum number of samples to load. If\\n            ``label_types`` and/or ``classes`` are also specified, first\\n            priority will be given to samples that contain all of the specified\\n            label types and/or classes, followed by samples that contain at\\n            least one of the specified labels types or classes. The actual\\n            number of samples loaded may be less than this maximum value if the\\n            dataset does not contain sufficient samples matching your\\n            requirements. By default, all matching samples are loaded\\n        raw_dir (None): a directory in which full annotations files may be\\n            stored to avoid re-downloads in the future\\n        scratch_dir (None): a scratch directory to use to download any\\n            necessary temporary files\\n\\n    Returns:\\n        a tuple of:\\n        -   num_samples: the total number of downloaded images\\n        -   classes: the list of all classes\\n        -   did_download: whether any content was downloaded (True) or if all\\n            necessary files were already downloaded (False)\\n    '\n    if split not in _IMAGE_DOWNLOAD_LINKS:\n        raise ValueError(\"Unsupported split '%s'; supported values are %s\" % (split, tuple(_IMAGE_DOWNLOAD_LINKS.keys())))\n    if classes is not None and split == 'test':\n        logger.warning('Test split is unlabeled; ignoring classes requirement')\n        classes = None\n    if scratch_dir is None:\n        scratch_dir = os.path.join(dataset_dir, 'scratch')\n    anno_path = os.path.join(dataset_dir, 'labels.json')\n    images_dir = os.path.join(dataset_dir, 'data')\n    split_size = _SPLIT_SIZES[split]\n    etau.ensure_dir(images_dir)\n    did_download = False\n    if raw_dir is None:\n        raw_dir = os.path.join(dataset_dir, 'raw')\n    etau.ensure_dir(raw_dir)\n    if split != 'test':\n        src_path = _ANNOTATION_DOWNLOAD_LINKS[split]\n        rel_path = _ANNOTATION_PATHS[split]\n        subdir = split\n        anno_type = 'annotations'\n    else:\n        src_path = _TEST_INFO_DOWNLOAD_LINK\n        rel_path = _TEST_INFO_PATHS\n        subdir = 'test'\n        anno_type = 'test info'\n    zip_path = os.path.join(scratch_dir, os.path.basename(src_path))\n    unzip_dir = os.path.join(scratch_dir, subdir)\n    content_dir = os.path.join(unzip_dir, os.path.dirname(rel_path))\n    full_anno_path = os.path.join(raw_dir, os.path.basename(rel_path))\n    if not os.path.isfile(full_anno_path):\n        logger.info(\"Downloading %s to '%s'\", anno_type, zip_path)\n        etaw.download_file(src_path, path=zip_path)\n        logger.info(\"Extracting %s to '%s'\", anno_type, full_anno_path)\n        if split != 'test':\n            merge_dir = tempfile.TemporaryDirectory()\n            etau.extract_zip(zip_path, outdir=merge_dir.name, delete_zip=False)\n            _merge_annotations(merge_dir.name, os.path.join(unzip_dir, 'annotations', f'sama_coco_{split}.json'))\n            merge_dir.cleanup()\n        else:\n            etau.extract_zip(zip_path, outdir=unzip_dir, delete_zip=False)\n        fouc._merge_dir(content_dir, raw_dir)\n        did_download = True\n    else:\n        logger.info(\"Found %s at '%s'\", anno_type, full_anno_path)\n    d = None\n    all_classes = None\n    images_src_path = _IMAGE_DOWNLOAD_LINKS[split]\n    images_zip_path = os.path.join(scratch_dir, os.path.basename(images_src_path))\n    unzip_images_dir = os.path.splitext(images_zip_path)[0]\n    if classes is None and image_ids is None and (max_samples is None):\n        num_existing = len(etau.list_files(images_dir))\n        num_download = split_size - num_existing\n        if num_download > 0:\n            if num_existing > 0:\n                logger.info('Found %d (< %d) downloaded images; must download full image zip', num_existing, split_size)\n            logger.info(\"Downloading images to '%s'\", images_zip_path)\n            etaw.download_file(images_src_path, path=images_zip_path)\n            logger.info(\"Extracting images to '%s'\", images_dir)\n            etau.extract_zip(images_zip_path, delete_zip=False)\n            etau.move_dir(unzip_images_dir, images_dir)\n            did_download = True\n        else:\n            logger.info('Images already downloaded')\n    else:\n        d = etas.load_json(full_anno_path)\n        (_, all_classes, _, images, annotations) = fouc._parse_coco_detection_annotations(d, extra_attrs=True)\n        if image_ids is not None:\n            image_ids = fouc._parse_image_ids(image_ids, images, split=split)\n        else:\n            image_ids = list(images.keys())\n        if classes is not None:\n            (all_ids, any_ids) = fouc._get_images_with_classes(image_ids, annotations, classes, all_classes)\n        else:\n            all_ids = image_ids\n            any_ids = []\n        all_ids = sorted(all_ids)\n        any_ids = sorted(any_ids)\n        if shuffle:\n            if seed is not None:\n                random.seed(seed)\n            random.shuffle(all_ids)\n            random.shuffle(any_ids)\n        image_ids = all_ids + any_ids\n        (existing_ids, downloadable_ids) = fouc._get_existing_ids(images_dir, images, image_ids)\n        if max_samples is not None:\n            num_existing = len(existing_ids)\n            num_downloadable = len(downloadable_ids)\n            num_available = num_existing + num_downloadable\n            if num_available < max_samples:\n                logger.warning('Only found %d (<%d) samples matching your requirements', num_available, max_samples)\n            if max_samples > num_existing:\n                num_download = max_samples - num_existing\n                download_ids = downloadable_ids[:num_download]\n            else:\n                download_ids = []\n        else:\n            download_ids = downloadable_ids\n        num_existing = len(existing_ids)\n        num_download = len(download_ids)\n        if num_existing > 0:\n            if num_download > 0:\n                logger.info('%d images found; downloading the remaining %d', num_existing, num_download)\n            else:\n                logger.info('Sufficient images already downloaded')\n        elif num_download > 0:\n            logger.info('Downloading %d images', num_download)\n        if num_download > 0:\n            fouc._download_images(images_dir, download_ids, images, num_workers)\n            did_download = True\n    downloaded_filenames = etau.list_files(images_dir)\n    num_samples = len(downloaded_filenames)\n    if not os.path.isfile(anno_path):\n        did_download = True\n    if did_download:\n        if d is None:\n            d = etas.load_json(full_anno_path)\n            categories = d.get('categories', None)\n            if categories is not None:\n                (all_classes, _) = fouc.parse_coco_categories(categories)\n            else:\n                all_classes = None\n        if num_samples >= split_size:\n            logger.info(\"Writing annotations to '%s'\", anno_path)\n            etau.copy_file(full_anno_path, anno_path)\n        else:\n            logger.info(\"Writing annotations for %d downloaded samples to '%s'\", num_samples, anno_path)\n            fouc._write_partial_annotations(d, anno_path, split, downloaded_filenames)\n    return (num_samples, all_classes, did_download)",
            "def download_sama_coco_dataset_split(dataset_dir, split, label_types=None, classes=None, image_ids=None, num_workers=None, shuffle=None, seed=None, max_samples=None, raw_dir=None, scratch_dir=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Utility that downloads full or partial data splits of the\\n    `COCO dataset <https://cocodataset.org>`_ with annotation splits found\\n    at https://www.sama.com/sama-coco-dataset.\\n\\n    See :ref:`this page <COCODetectionDataset-export>` for the format in which\\n    ``dataset_dir`` will be arranged.\\n\\n    Any existing files are not re-downloaded.\\n\\n    Args:\\n        dataset_dir: the directory to download the dataset\\n        split: the split to download. Supported values are\\n            ``(\"train\", \"validation\", \"test\")``\\n        label_types (None): a label type or list of label types to load. The\\n            supported values are ``(\"detections\", \"segmentations\")``. By\\n            default, all label types are loaded\\n        classes (None): a string or list of strings specifying required classes\\n            to load. Only samples containing at least one instance of a\\n            specified class will be loaded\\n        image_ids (None): an optional list of specific image IDs to load. Can\\n            be provided in any of the following formats:\\n\\n            -   a list of ``<image-id>`` ints or strings\\n            -   a list of ``<split>/<image-id>`` strings\\n            -   the path to a text (newline-separated), JSON, or CSV file\\n                containing the list of image IDs to load in either of the first\\n                two formats\\n        num_workers (None): a suggested number of threads to use when\\n            downloading individual images\\n        shuffle (False): whether to randomly shuffle the order in which samples\\n            are chosen for partial downloads\\n        seed (None): a random seed to use when shuffling\\n        max_samples (None): a maximum number of samples to load. If\\n            ``label_types`` and/or ``classes`` are also specified, first\\n            priority will be given to samples that contain all of the specified\\n            label types and/or classes, followed by samples that contain at\\n            least one of the specified labels types or classes. The actual\\n            number of samples loaded may be less than this maximum value if the\\n            dataset does not contain sufficient samples matching your\\n            requirements. By default, all matching samples are loaded\\n        raw_dir (None): a directory in which full annotations files may be\\n            stored to avoid re-downloads in the future\\n        scratch_dir (None): a scratch directory to use to download any\\n            necessary temporary files\\n\\n    Returns:\\n        a tuple of:\\n        -   num_samples: the total number of downloaded images\\n        -   classes: the list of all classes\\n        -   did_download: whether any content was downloaded (True) or if all\\n            necessary files were already downloaded (False)\\n    '\n    if split not in _IMAGE_DOWNLOAD_LINKS:\n        raise ValueError(\"Unsupported split '%s'; supported values are %s\" % (split, tuple(_IMAGE_DOWNLOAD_LINKS.keys())))\n    if classes is not None and split == 'test':\n        logger.warning('Test split is unlabeled; ignoring classes requirement')\n        classes = None\n    if scratch_dir is None:\n        scratch_dir = os.path.join(dataset_dir, 'scratch')\n    anno_path = os.path.join(dataset_dir, 'labels.json')\n    images_dir = os.path.join(dataset_dir, 'data')\n    split_size = _SPLIT_SIZES[split]\n    etau.ensure_dir(images_dir)\n    did_download = False\n    if raw_dir is None:\n        raw_dir = os.path.join(dataset_dir, 'raw')\n    etau.ensure_dir(raw_dir)\n    if split != 'test':\n        src_path = _ANNOTATION_DOWNLOAD_LINKS[split]\n        rel_path = _ANNOTATION_PATHS[split]\n        subdir = split\n        anno_type = 'annotations'\n    else:\n        src_path = _TEST_INFO_DOWNLOAD_LINK\n        rel_path = _TEST_INFO_PATHS\n        subdir = 'test'\n        anno_type = 'test info'\n    zip_path = os.path.join(scratch_dir, os.path.basename(src_path))\n    unzip_dir = os.path.join(scratch_dir, subdir)\n    content_dir = os.path.join(unzip_dir, os.path.dirname(rel_path))\n    full_anno_path = os.path.join(raw_dir, os.path.basename(rel_path))\n    if not os.path.isfile(full_anno_path):\n        logger.info(\"Downloading %s to '%s'\", anno_type, zip_path)\n        etaw.download_file(src_path, path=zip_path)\n        logger.info(\"Extracting %s to '%s'\", anno_type, full_anno_path)\n        if split != 'test':\n            merge_dir = tempfile.TemporaryDirectory()\n            etau.extract_zip(zip_path, outdir=merge_dir.name, delete_zip=False)\n            _merge_annotations(merge_dir.name, os.path.join(unzip_dir, 'annotations', f'sama_coco_{split}.json'))\n            merge_dir.cleanup()\n        else:\n            etau.extract_zip(zip_path, outdir=unzip_dir, delete_zip=False)\n        fouc._merge_dir(content_dir, raw_dir)\n        did_download = True\n    else:\n        logger.info(\"Found %s at '%s'\", anno_type, full_anno_path)\n    d = None\n    all_classes = None\n    images_src_path = _IMAGE_DOWNLOAD_LINKS[split]\n    images_zip_path = os.path.join(scratch_dir, os.path.basename(images_src_path))\n    unzip_images_dir = os.path.splitext(images_zip_path)[0]\n    if classes is None and image_ids is None and (max_samples is None):\n        num_existing = len(etau.list_files(images_dir))\n        num_download = split_size - num_existing\n        if num_download > 0:\n            if num_existing > 0:\n                logger.info('Found %d (< %d) downloaded images; must download full image zip', num_existing, split_size)\n            logger.info(\"Downloading images to '%s'\", images_zip_path)\n            etaw.download_file(images_src_path, path=images_zip_path)\n            logger.info(\"Extracting images to '%s'\", images_dir)\n            etau.extract_zip(images_zip_path, delete_zip=False)\n            etau.move_dir(unzip_images_dir, images_dir)\n            did_download = True\n        else:\n            logger.info('Images already downloaded')\n    else:\n        d = etas.load_json(full_anno_path)\n        (_, all_classes, _, images, annotations) = fouc._parse_coco_detection_annotations(d, extra_attrs=True)\n        if image_ids is not None:\n            image_ids = fouc._parse_image_ids(image_ids, images, split=split)\n        else:\n            image_ids = list(images.keys())\n        if classes is not None:\n            (all_ids, any_ids) = fouc._get_images_with_classes(image_ids, annotations, classes, all_classes)\n        else:\n            all_ids = image_ids\n            any_ids = []\n        all_ids = sorted(all_ids)\n        any_ids = sorted(any_ids)\n        if shuffle:\n            if seed is not None:\n                random.seed(seed)\n            random.shuffle(all_ids)\n            random.shuffle(any_ids)\n        image_ids = all_ids + any_ids\n        (existing_ids, downloadable_ids) = fouc._get_existing_ids(images_dir, images, image_ids)\n        if max_samples is not None:\n            num_existing = len(existing_ids)\n            num_downloadable = len(downloadable_ids)\n            num_available = num_existing + num_downloadable\n            if num_available < max_samples:\n                logger.warning('Only found %d (<%d) samples matching your requirements', num_available, max_samples)\n            if max_samples > num_existing:\n                num_download = max_samples - num_existing\n                download_ids = downloadable_ids[:num_download]\n            else:\n                download_ids = []\n        else:\n            download_ids = downloadable_ids\n        num_existing = len(existing_ids)\n        num_download = len(download_ids)\n        if num_existing > 0:\n            if num_download > 0:\n                logger.info('%d images found; downloading the remaining %d', num_existing, num_download)\n            else:\n                logger.info('Sufficient images already downloaded')\n        elif num_download > 0:\n            logger.info('Downloading %d images', num_download)\n        if num_download > 0:\n            fouc._download_images(images_dir, download_ids, images, num_workers)\n            did_download = True\n    downloaded_filenames = etau.list_files(images_dir)\n    num_samples = len(downloaded_filenames)\n    if not os.path.isfile(anno_path):\n        did_download = True\n    if did_download:\n        if d is None:\n            d = etas.load_json(full_anno_path)\n            categories = d.get('categories', None)\n            if categories is not None:\n                (all_classes, _) = fouc.parse_coco_categories(categories)\n            else:\n                all_classes = None\n        if num_samples >= split_size:\n            logger.info(\"Writing annotations to '%s'\", anno_path)\n            etau.copy_file(full_anno_path, anno_path)\n        else:\n            logger.info(\"Writing annotations for %d downloaded samples to '%s'\", num_samples, anno_path)\n            fouc._write_partial_annotations(d, anno_path, split, downloaded_filenames)\n    return (num_samples, all_classes, did_download)",
            "def download_sama_coco_dataset_split(dataset_dir, split, label_types=None, classes=None, image_ids=None, num_workers=None, shuffle=None, seed=None, max_samples=None, raw_dir=None, scratch_dir=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Utility that downloads full or partial data splits of the\\n    `COCO dataset <https://cocodataset.org>`_ with annotation splits found\\n    at https://www.sama.com/sama-coco-dataset.\\n\\n    See :ref:`this page <COCODetectionDataset-export>` for the format in which\\n    ``dataset_dir`` will be arranged.\\n\\n    Any existing files are not re-downloaded.\\n\\n    Args:\\n        dataset_dir: the directory to download the dataset\\n        split: the split to download. Supported values are\\n            ``(\"train\", \"validation\", \"test\")``\\n        label_types (None): a label type or list of label types to load. The\\n            supported values are ``(\"detections\", \"segmentations\")``. By\\n            default, all label types are loaded\\n        classes (None): a string or list of strings specifying required classes\\n            to load. Only samples containing at least one instance of a\\n            specified class will be loaded\\n        image_ids (None): an optional list of specific image IDs to load. Can\\n            be provided in any of the following formats:\\n\\n            -   a list of ``<image-id>`` ints or strings\\n            -   a list of ``<split>/<image-id>`` strings\\n            -   the path to a text (newline-separated), JSON, or CSV file\\n                containing the list of image IDs to load in either of the first\\n                two formats\\n        num_workers (None): a suggested number of threads to use when\\n            downloading individual images\\n        shuffle (False): whether to randomly shuffle the order in which samples\\n            are chosen for partial downloads\\n        seed (None): a random seed to use when shuffling\\n        max_samples (None): a maximum number of samples to load. If\\n            ``label_types`` and/or ``classes`` are also specified, first\\n            priority will be given to samples that contain all of the specified\\n            label types and/or classes, followed by samples that contain at\\n            least one of the specified labels types or classes. The actual\\n            number of samples loaded may be less than this maximum value if the\\n            dataset does not contain sufficient samples matching your\\n            requirements. By default, all matching samples are loaded\\n        raw_dir (None): a directory in which full annotations files may be\\n            stored to avoid re-downloads in the future\\n        scratch_dir (None): a scratch directory to use to download any\\n            necessary temporary files\\n\\n    Returns:\\n        a tuple of:\\n        -   num_samples: the total number of downloaded images\\n        -   classes: the list of all classes\\n        -   did_download: whether any content was downloaded (True) or if all\\n            necessary files were already downloaded (False)\\n    '\n    if split not in _IMAGE_DOWNLOAD_LINKS:\n        raise ValueError(\"Unsupported split '%s'; supported values are %s\" % (split, tuple(_IMAGE_DOWNLOAD_LINKS.keys())))\n    if classes is not None and split == 'test':\n        logger.warning('Test split is unlabeled; ignoring classes requirement')\n        classes = None\n    if scratch_dir is None:\n        scratch_dir = os.path.join(dataset_dir, 'scratch')\n    anno_path = os.path.join(dataset_dir, 'labels.json')\n    images_dir = os.path.join(dataset_dir, 'data')\n    split_size = _SPLIT_SIZES[split]\n    etau.ensure_dir(images_dir)\n    did_download = False\n    if raw_dir is None:\n        raw_dir = os.path.join(dataset_dir, 'raw')\n    etau.ensure_dir(raw_dir)\n    if split != 'test':\n        src_path = _ANNOTATION_DOWNLOAD_LINKS[split]\n        rel_path = _ANNOTATION_PATHS[split]\n        subdir = split\n        anno_type = 'annotations'\n    else:\n        src_path = _TEST_INFO_DOWNLOAD_LINK\n        rel_path = _TEST_INFO_PATHS\n        subdir = 'test'\n        anno_type = 'test info'\n    zip_path = os.path.join(scratch_dir, os.path.basename(src_path))\n    unzip_dir = os.path.join(scratch_dir, subdir)\n    content_dir = os.path.join(unzip_dir, os.path.dirname(rel_path))\n    full_anno_path = os.path.join(raw_dir, os.path.basename(rel_path))\n    if not os.path.isfile(full_anno_path):\n        logger.info(\"Downloading %s to '%s'\", anno_type, zip_path)\n        etaw.download_file(src_path, path=zip_path)\n        logger.info(\"Extracting %s to '%s'\", anno_type, full_anno_path)\n        if split != 'test':\n            merge_dir = tempfile.TemporaryDirectory()\n            etau.extract_zip(zip_path, outdir=merge_dir.name, delete_zip=False)\n            _merge_annotations(merge_dir.name, os.path.join(unzip_dir, 'annotations', f'sama_coco_{split}.json'))\n            merge_dir.cleanup()\n        else:\n            etau.extract_zip(zip_path, outdir=unzip_dir, delete_zip=False)\n        fouc._merge_dir(content_dir, raw_dir)\n        did_download = True\n    else:\n        logger.info(\"Found %s at '%s'\", anno_type, full_anno_path)\n    d = None\n    all_classes = None\n    images_src_path = _IMAGE_DOWNLOAD_LINKS[split]\n    images_zip_path = os.path.join(scratch_dir, os.path.basename(images_src_path))\n    unzip_images_dir = os.path.splitext(images_zip_path)[0]\n    if classes is None and image_ids is None and (max_samples is None):\n        num_existing = len(etau.list_files(images_dir))\n        num_download = split_size - num_existing\n        if num_download > 0:\n            if num_existing > 0:\n                logger.info('Found %d (< %d) downloaded images; must download full image zip', num_existing, split_size)\n            logger.info(\"Downloading images to '%s'\", images_zip_path)\n            etaw.download_file(images_src_path, path=images_zip_path)\n            logger.info(\"Extracting images to '%s'\", images_dir)\n            etau.extract_zip(images_zip_path, delete_zip=False)\n            etau.move_dir(unzip_images_dir, images_dir)\n            did_download = True\n        else:\n            logger.info('Images already downloaded')\n    else:\n        d = etas.load_json(full_anno_path)\n        (_, all_classes, _, images, annotations) = fouc._parse_coco_detection_annotations(d, extra_attrs=True)\n        if image_ids is not None:\n            image_ids = fouc._parse_image_ids(image_ids, images, split=split)\n        else:\n            image_ids = list(images.keys())\n        if classes is not None:\n            (all_ids, any_ids) = fouc._get_images_with_classes(image_ids, annotations, classes, all_classes)\n        else:\n            all_ids = image_ids\n            any_ids = []\n        all_ids = sorted(all_ids)\n        any_ids = sorted(any_ids)\n        if shuffle:\n            if seed is not None:\n                random.seed(seed)\n            random.shuffle(all_ids)\n            random.shuffle(any_ids)\n        image_ids = all_ids + any_ids\n        (existing_ids, downloadable_ids) = fouc._get_existing_ids(images_dir, images, image_ids)\n        if max_samples is not None:\n            num_existing = len(existing_ids)\n            num_downloadable = len(downloadable_ids)\n            num_available = num_existing + num_downloadable\n            if num_available < max_samples:\n                logger.warning('Only found %d (<%d) samples matching your requirements', num_available, max_samples)\n            if max_samples > num_existing:\n                num_download = max_samples - num_existing\n                download_ids = downloadable_ids[:num_download]\n            else:\n                download_ids = []\n        else:\n            download_ids = downloadable_ids\n        num_existing = len(existing_ids)\n        num_download = len(download_ids)\n        if num_existing > 0:\n            if num_download > 0:\n                logger.info('%d images found; downloading the remaining %d', num_existing, num_download)\n            else:\n                logger.info('Sufficient images already downloaded')\n        elif num_download > 0:\n            logger.info('Downloading %d images', num_download)\n        if num_download > 0:\n            fouc._download_images(images_dir, download_ids, images, num_workers)\n            did_download = True\n    downloaded_filenames = etau.list_files(images_dir)\n    num_samples = len(downloaded_filenames)\n    if not os.path.isfile(anno_path):\n        did_download = True\n    if did_download:\n        if d is None:\n            d = etas.load_json(full_anno_path)\n            categories = d.get('categories', None)\n            if categories is not None:\n                (all_classes, _) = fouc.parse_coco_categories(categories)\n            else:\n                all_classes = None\n        if num_samples >= split_size:\n            logger.info(\"Writing annotations to '%s'\", anno_path)\n            etau.copy_file(full_anno_path, anno_path)\n        else:\n            logger.info(\"Writing annotations for %d downloaded samples to '%s'\", num_samples, anno_path)\n            fouc._write_partial_annotations(d, anno_path, split, downloaded_filenames)\n    return (num_samples, all_classes, did_download)",
            "def download_sama_coco_dataset_split(dataset_dir, split, label_types=None, classes=None, image_ids=None, num_workers=None, shuffle=None, seed=None, max_samples=None, raw_dir=None, scratch_dir=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Utility that downloads full or partial data splits of the\\n    `COCO dataset <https://cocodataset.org>`_ with annotation splits found\\n    at https://www.sama.com/sama-coco-dataset.\\n\\n    See :ref:`this page <COCODetectionDataset-export>` for the format in which\\n    ``dataset_dir`` will be arranged.\\n\\n    Any existing files are not re-downloaded.\\n\\n    Args:\\n        dataset_dir: the directory to download the dataset\\n        split: the split to download. Supported values are\\n            ``(\"train\", \"validation\", \"test\")``\\n        label_types (None): a label type or list of label types to load. The\\n            supported values are ``(\"detections\", \"segmentations\")``. By\\n            default, all label types are loaded\\n        classes (None): a string or list of strings specifying required classes\\n            to load. Only samples containing at least one instance of a\\n            specified class will be loaded\\n        image_ids (None): an optional list of specific image IDs to load. Can\\n            be provided in any of the following formats:\\n\\n            -   a list of ``<image-id>`` ints or strings\\n            -   a list of ``<split>/<image-id>`` strings\\n            -   the path to a text (newline-separated), JSON, or CSV file\\n                containing the list of image IDs to load in either of the first\\n                two formats\\n        num_workers (None): a suggested number of threads to use when\\n            downloading individual images\\n        shuffle (False): whether to randomly shuffle the order in which samples\\n            are chosen for partial downloads\\n        seed (None): a random seed to use when shuffling\\n        max_samples (None): a maximum number of samples to load. If\\n            ``label_types`` and/or ``classes`` are also specified, first\\n            priority will be given to samples that contain all of the specified\\n            label types and/or classes, followed by samples that contain at\\n            least one of the specified labels types or classes. The actual\\n            number of samples loaded may be less than this maximum value if the\\n            dataset does not contain sufficient samples matching your\\n            requirements. By default, all matching samples are loaded\\n        raw_dir (None): a directory in which full annotations files may be\\n            stored to avoid re-downloads in the future\\n        scratch_dir (None): a scratch directory to use to download any\\n            necessary temporary files\\n\\n    Returns:\\n        a tuple of:\\n        -   num_samples: the total number of downloaded images\\n        -   classes: the list of all classes\\n        -   did_download: whether any content was downloaded (True) or if all\\n            necessary files were already downloaded (False)\\n    '\n    if split not in _IMAGE_DOWNLOAD_LINKS:\n        raise ValueError(\"Unsupported split '%s'; supported values are %s\" % (split, tuple(_IMAGE_DOWNLOAD_LINKS.keys())))\n    if classes is not None and split == 'test':\n        logger.warning('Test split is unlabeled; ignoring classes requirement')\n        classes = None\n    if scratch_dir is None:\n        scratch_dir = os.path.join(dataset_dir, 'scratch')\n    anno_path = os.path.join(dataset_dir, 'labels.json')\n    images_dir = os.path.join(dataset_dir, 'data')\n    split_size = _SPLIT_SIZES[split]\n    etau.ensure_dir(images_dir)\n    did_download = False\n    if raw_dir is None:\n        raw_dir = os.path.join(dataset_dir, 'raw')\n    etau.ensure_dir(raw_dir)\n    if split != 'test':\n        src_path = _ANNOTATION_DOWNLOAD_LINKS[split]\n        rel_path = _ANNOTATION_PATHS[split]\n        subdir = split\n        anno_type = 'annotations'\n    else:\n        src_path = _TEST_INFO_DOWNLOAD_LINK\n        rel_path = _TEST_INFO_PATHS\n        subdir = 'test'\n        anno_type = 'test info'\n    zip_path = os.path.join(scratch_dir, os.path.basename(src_path))\n    unzip_dir = os.path.join(scratch_dir, subdir)\n    content_dir = os.path.join(unzip_dir, os.path.dirname(rel_path))\n    full_anno_path = os.path.join(raw_dir, os.path.basename(rel_path))\n    if not os.path.isfile(full_anno_path):\n        logger.info(\"Downloading %s to '%s'\", anno_type, zip_path)\n        etaw.download_file(src_path, path=zip_path)\n        logger.info(\"Extracting %s to '%s'\", anno_type, full_anno_path)\n        if split != 'test':\n            merge_dir = tempfile.TemporaryDirectory()\n            etau.extract_zip(zip_path, outdir=merge_dir.name, delete_zip=False)\n            _merge_annotations(merge_dir.name, os.path.join(unzip_dir, 'annotations', f'sama_coco_{split}.json'))\n            merge_dir.cleanup()\n        else:\n            etau.extract_zip(zip_path, outdir=unzip_dir, delete_zip=False)\n        fouc._merge_dir(content_dir, raw_dir)\n        did_download = True\n    else:\n        logger.info(\"Found %s at '%s'\", anno_type, full_anno_path)\n    d = None\n    all_classes = None\n    images_src_path = _IMAGE_DOWNLOAD_LINKS[split]\n    images_zip_path = os.path.join(scratch_dir, os.path.basename(images_src_path))\n    unzip_images_dir = os.path.splitext(images_zip_path)[0]\n    if classes is None and image_ids is None and (max_samples is None):\n        num_existing = len(etau.list_files(images_dir))\n        num_download = split_size - num_existing\n        if num_download > 0:\n            if num_existing > 0:\n                logger.info('Found %d (< %d) downloaded images; must download full image zip', num_existing, split_size)\n            logger.info(\"Downloading images to '%s'\", images_zip_path)\n            etaw.download_file(images_src_path, path=images_zip_path)\n            logger.info(\"Extracting images to '%s'\", images_dir)\n            etau.extract_zip(images_zip_path, delete_zip=False)\n            etau.move_dir(unzip_images_dir, images_dir)\n            did_download = True\n        else:\n            logger.info('Images already downloaded')\n    else:\n        d = etas.load_json(full_anno_path)\n        (_, all_classes, _, images, annotations) = fouc._parse_coco_detection_annotations(d, extra_attrs=True)\n        if image_ids is not None:\n            image_ids = fouc._parse_image_ids(image_ids, images, split=split)\n        else:\n            image_ids = list(images.keys())\n        if classes is not None:\n            (all_ids, any_ids) = fouc._get_images_with_classes(image_ids, annotations, classes, all_classes)\n        else:\n            all_ids = image_ids\n            any_ids = []\n        all_ids = sorted(all_ids)\n        any_ids = sorted(any_ids)\n        if shuffle:\n            if seed is not None:\n                random.seed(seed)\n            random.shuffle(all_ids)\n            random.shuffle(any_ids)\n        image_ids = all_ids + any_ids\n        (existing_ids, downloadable_ids) = fouc._get_existing_ids(images_dir, images, image_ids)\n        if max_samples is not None:\n            num_existing = len(existing_ids)\n            num_downloadable = len(downloadable_ids)\n            num_available = num_existing + num_downloadable\n            if num_available < max_samples:\n                logger.warning('Only found %d (<%d) samples matching your requirements', num_available, max_samples)\n            if max_samples > num_existing:\n                num_download = max_samples - num_existing\n                download_ids = downloadable_ids[:num_download]\n            else:\n                download_ids = []\n        else:\n            download_ids = downloadable_ids\n        num_existing = len(existing_ids)\n        num_download = len(download_ids)\n        if num_existing > 0:\n            if num_download > 0:\n                logger.info('%d images found; downloading the remaining %d', num_existing, num_download)\n            else:\n                logger.info('Sufficient images already downloaded')\n        elif num_download > 0:\n            logger.info('Downloading %d images', num_download)\n        if num_download > 0:\n            fouc._download_images(images_dir, download_ids, images, num_workers)\n            did_download = True\n    downloaded_filenames = etau.list_files(images_dir)\n    num_samples = len(downloaded_filenames)\n    if not os.path.isfile(anno_path):\n        did_download = True\n    if did_download:\n        if d is None:\n            d = etas.load_json(full_anno_path)\n            categories = d.get('categories', None)\n            if categories is not None:\n                (all_classes, _) = fouc.parse_coco_categories(categories)\n            else:\n                all_classes = None\n        if num_samples >= split_size:\n            logger.info(\"Writing annotations to '%s'\", anno_path)\n            etau.copy_file(full_anno_path, anno_path)\n        else:\n            logger.info(\"Writing annotations for %d downloaded samples to '%s'\", num_samples, anno_path)\n            fouc._write_partial_annotations(d, anno_path, split, downloaded_filenames)\n    return (num_samples, all_classes, did_download)",
            "def download_sama_coco_dataset_split(dataset_dir, split, label_types=None, classes=None, image_ids=None, num_workers=None, shuffle=None, seed=None, max_samples=None, raw_dir=None, scratch_dir=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Utility that downloads full or partial data splits of the\\n    `COCO dataset <https://cocodataset.org>`_ with annotation splits found\\n    at https://www.sama.com/sama-coco-dataset.\\n\\n    See :ref:`this page <COCODetectionDataset-export>` for the format in which\\n    ``dataset_dir`` will be arranged.\\n\\n    Any existing files are not re-downloaded.\\n\\n    Args:\\n        dataset_dir: the directory to download the dataset\\n        split: the split to download. Supported values are\\n            ``(\"train\", \"validation\", \"test\")``\\n        label_types (None): a label type or list of label types to load. The\\n            supported values are ``(\"detections\", \"segmentations\")``. By\\n            default, all label types are loaded\\n        classes (None): a string or list of strings specifying required classes\\n            to load. Only samples containing at least one instance of a\\n            specified class will be loaded\\n        image_ids (None): an optional list of specific image IDs to load. Can\\n            be provided in any of the following formats:\\n\\n            -   a list of ``<image-id>`` ints or strings\\n            -   a list of ``<split>/<image-id>`` strings\\n            -   the path to a text (newline-separated), JSON, or CSV file\\n                containing the list of image IDs to load in either of the first\\n                two formats\\n        num_workers (None): a suggested number of threads to use when\\n            downloading individual images\\n        shuffle (False): whether to randomly shuffle the order in which samples\\n            are chosen for partial downloads\\n        seed (None): a random seed to use when shuffling\\n        max_samples (None): a maximum number of samples to load. If\\n            ``label_types`` and/or ``classes`` are also specified, first\\n            priority will be given to samples that contain all of the specified\\n            label types and/or classes, followed by samples that contain at\\n            least one of the specified labels types or classes. The actual\\n            number of samples loaded may be less than this maximum value if the\\n            dataset does not contain sufficient samples matching your\\n            requirements. By default, all matching samples are loaded\\n        raw_dir (None): a directory in which full annotations files may be\\n            stored to avoid re-downloads in the future\\n        scratch_dir (None): a scratch directory to use to download any\\n            necessary temporary files\\n\\n    Returns:\\n        a tuple of:\\n        -   num_samples: the total number of downloaded images\\n        -   classes: the list of all classes\\n        -   did_download: whether any content was downloaded (True) or if all\\n            necessary files were already downloaded (False)\\n    '\n    if split not in _IMAGE_DOWNLOAD_LINKS:\n        raise ValueError(\"Unsupported split '%s'; supported values are %s\" % (split, tuple(_IMAGE_DOWNLOAD_LINKS.keys())))\n    if classes is not None and split == 'test':\n        logger.warning('Test split is unlabeled; ignoring classes requirement')\n        classes = None\n    if scratch_dir is None:\n        scratch_dir = os.path.join(dataset_dir, 'scratch')\n    anno_path = os.path.join(dataset_dir, 'labels.json')\n    images_dir = os.path.join(dataset_dir, 'data')\n    split_size = _SPLIT_SIZES[split]\n    etau.ensure_dir(images_dir)\n    did_download = False\n    if raw_dir is None:\n        raw_dir = os.path.join(dataset_dir, 'raw')\n    etau.ensure_dir(raw_dir)\n    if split != 'test':\n        src_path = _ANNOTATION_DOWNLOAD_LINKS[split]\n        rel_path = _ANNOTATION_PATHS[split]\n        subdir = split\n        anno_type = 'annotations'\n    else:\n        src_path = _TEST_INFO_DOWNLOAD_LINK\n        rel_path = _TEST_INFO_PATHS\n        subdir = 'test'\n        anno_type = 'test info'\n    zip_path = os.path.join(scratch_dir, os.path.basename(src_path))\n    unzip_dir = os.path.join(scratch_dir, subdir)\n    content_dir = os.path.join(unzip_dir, os.path.dirname(rel_path))\n    full_anno_path = os.path.join(raw_dir, os.path.basename(rel_path))\n    if not os.path.isfile(full_anno_path):\n        logger.info(\"Downloading %s to '%s'\", anno_type, zip_path)\n        etaw.download_file(src_path, path=zip_path)\n        logger.info(\"Extracting %s to '%s'\", anno_type, full_anno_path)\n        if split != 'test':\n            merge_dir = tempfile.TemporaryDirectory()\n            etau.extract_zip(zip_path, outdir=merge_dir.name, delete_zip=False)\n            _merge_annotations(merge_dir.name, os.path.join(unzip_dir, 'annotations', f'sama_coco_{split}.json'))\n            merge_dir.cleanup()\n        else:\n            etau.extract_zip(zip_path, outdir=unzip_dir, delete_zip=False)\n        fouc._merge_dir(content_dir, raw_dir)\n        did_download = True\n    else:\n        logger.info(\"Found %s at '%s'\", anno_type, full_anno_path)\n    d = None\n    all_classes = None\n    images_src_path = _IMAGE_DOWNLOAD_LINKS[split]\n    images_zip_path = os.path.join(scratch_dir, os.path.basename(images_src_path))\n    unzip_images_dir = os.path.splitext(images_zip_path)[0]\n    if classes is None and image_ids is None and (max_samples is None):\n        num_existing = len(etau.list_files(images_dir))\n        num_download = split_size - num_existing\n        if num_download > 0:\n            if num_existing > 0:\n                logger.info('Found %d (< %d) downloaded images; must download full image zip', num_existing, split_size)\n            logger.info(\"Downloading images to '%s'\", images_zip_path)\n            etaw.download_file(images_src_path, path=images_zip_path)\n            logger.info(\"Extracting images to '%s'\", images_dir)\n            etau.extract_zip(images_zip_path, delete_zip=False)\n            etau.move_dir(unzip_images_dir, images_dir)\n            did_download = True\n        else:\n            logger.info('Images already downloaded')\n    else:\n        d = etas.load_json(full_anno_path)\n        (_, all_classes, _, images, annotations) = fouc._parse_coco_detection_annotations(d, extra_attrs=True)\n        if image_ids is not None:\n            image_ids = fouc._parse_image_ids(image_ids, images, split=split)\n        else:\n            image_ids = list(images.keys())\n        if classes is not None:\n            (all_ids, any_ids) = fouc._get_images_with_classes(image_ids, annotations, classes, all_classes)\n        else:\n            all_ids = image_ids\n            any_ids = []\n        all_ids = sorted(all_ids)\n        any_ids = sorted(any_ids)\n        if shuffle:\n            if seed is not None:\n                random.seed(seed)\n            random.shuffle(all_ids)\n            random.shuffle(any_ids)\n        image_ids = all_ids + any_ids\n        (existing_ids, downloadable_ids) = fouc._get_existing_ids(images_dir, images, image_ids)\n        if max_samples is not None:\n            num_existing = len(existing_ids)\n            num_downloadable = len(downloadable_ids)\n            num_available = num_existing + num_downloadable\n            if num_available < max_samples:\n                logger.warning('Only found %d (<%d) samples matching your requirements', num_available, max_samples)\n            if max_samples > num_existing:\n                num_download = max_samples - num_existing\n                download_ids = downloadable_ids[:num_download]\n            else:\n                download_ids = []\n        else:\n            download_ids = downloadable_ids\n        num_existing = len(existing_ids)\n        num_download = len(download_ids)\n        if num_existing > 0:\n            if num_download > 0:\n                logger.info('%d images found; downloading the remaining %d', num_existing, num_download)\n            else:\n                logger.info('Sufficient images already downloaded')\n        elif num_download > 0:\n            logger.info('Downloading %d images', num_download)\n        if num_download > 0:\n            fouc._download_images(images_dir, download_ids, images, num_workers)\n            did_download = True\n    downloaded_filenames = etau.list_files(images_dir)\n    num_samples = len(downloaded_filenames)\n    if not os.path.isfile(anno_path):\n        did_download = True\n    if did_download:\n        if d is None:\n            d = etas.load_json(full_anno_path)\n            categories = d.get('categories', None)\n            if categories is not None:\n                (all_classes, _) = fouc.parse_coco_categories(categories)\n            else:\n                all_classes = None\n        if num_samples >= split_size:\n            logger.info(\"Writing annotations to '%s'\", anno_path)\n            etau.copy_file(full_anno_path, anno_path)\n        else:\n            logger.info(\"Writing annotations for %d downloaded samples to '%s'\", num_samples, anno_path)\n            fouc._write_partial_annotations(d, anno_path, split, downloaded_filenames)\n    return (num_samples, all_classes, did_download)"
        ]
    },
    {
        "func_name": "_merge_annotations",
        "original": "def _merge_annotations(merge_dir, output):\n    info = licenses = categories = None\n    all_annotations = []\n    all_images = []\n    for json_file in glob.glob(f'{merge_dir}/*json'):\n        with open(json_file) as f:\n            coco = json.load(f)\n            coco_annotations = coco['annotations']\n            all_annotations.extend(coco_annotations)\n            coco_images = coco['images']\n            all_images.extend(coco_images)\n            if not info:\n                info = coco['info']\n            if not licenses:\n                licenses = coco['licenses']\n            if not categories:\n                categories = coco['categories']\n    merged_coco = {'info': info, 'licenses': licenses, 'categories': categories, 'images': all_images, 'annotations': all_annotations}\n    os.makedirs(os.path.dirname(output), exist_ok=True)\n    with open(output, 'w') as f:\n        json.dump(merged_coco, f)",
        "mutated": [
            "def _merge_annotations(merge_dir, output):\n    if False:\n        i = 10\n    info = licenses = categories = None\n    all_annotations = []\n    all_images = []\n    for json_file in glob.glob(f'{merge_dir}/*json'):\n        with open(json_file) as f:\n            coco = json.load(f)\n            coco_annotations = coco['annotations']\n            all_annotations.extend(coco_annotations)\n            coco_images = coco['images']\n            all_images.extend(coco_images)\n            if not info:\n                info = coco['info']\n            if not licenses:\n                licenses = coco['licenses']\n            if not categories:\n                categories = coco['categories']\n    merged_coco = {'info': info, 'licenses': licenses, 'categories': categories, 'images': all_images, 'annotations': all_annotations}\n    os.makedirs(os.path.dirname(output), exist_ok=True)\n    with open(output, 'w') as f:\n        json.dump(merged_coco, f)",
            "def _merge_annotations(merge_dir, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    info = licenses = categories = None\n    all_annotations = []\n    all_images = []\n    for json_file in glob.glob(f'{merge_dir}/*json'):\n        with open(json_file) as f:\n            coco = json.load(f)\n            coco_annotations = coco['annotations']\n            all_annotations.extend(coco_annotations)\n            coco_images = coco['images']\n            all_images.extend(coco_images)\n            if not info:\n                info = coco['info']\n            if not licenses:\n                licenses = coco['licenses']\n            if not categories:\n                categories = coco['categories']\n    merged_coco = {'info': info, 'licenses': licenses, 'categories': categories, 'images': all_images, 'annotations': all_annotations}\n    os.makedirs(os.path.dirname(output), exist_ok=True)\n    with open(output, 'w') as f:\n        json.dump(merged_coco, f)",
            "def _merge_annotations(merge_dir, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    info = licenses = categories = None\n    all_annotations = []\n    all_images = []\n    for json_file in glob.glob(f'{merge_dir}/*json'):\n        with open(json_file) as f:\n            coco = json.load(f)\n            coco_annotations = coco['annotations']\n            all_annotations.extend(coco_annotations)\n            coco_images = coco['images']\n            all_images.extend(coco_images)\n            if not info:\n                info = coco['info']\n            if not licenses:\n                licenses = coco['licenses']\n            if not categories:\n                categories = coco['categories']\n    merged_coco = {'info': info, 'licenses': licenses, 'categories': categories, 'images': all_images, 'annotations': all_annotations}\n    os.makedirs(os.path.dirname(output), exist_ok=True)\n    with open(output, 'w') as f:\n        json.dump(merged_coco, f)",
            "def _merge_annotations(merge_dir, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    info = licenses = categories = None\n    all_annotations = []\n    all_images = []\n    for json_file in glob.glob(f'{merge_dir}/*json'):\n        with open(json_file) as f:\n            coco = json.load(f)\n            coco_annotations = coco['annotations']\n            all_annotations.extend(coco_annotations)\n            coco_images = coco['images']\n            all_images.extend(coco_images)\n            if not info:\n                info = coco['info']\n            if not licenses:\n                licenses = coco['licenses']\n            if not categories:\n                categories = coco['categories']\n    merged_coco = {'info': info, 'licenses': licenses, 'categories': categories, 'images': all_images, 'annotations': all_annotations}\n    os.makedirs(os.path.dirname(output), exist_ok=True)\n    with open(output, 'w') as f:\n        json.dump(merged_coco, f)",
            "def _merge_annotations(merge_dir, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    info = licenses = categories = None\n    all_annotations = []\n    all_images = []\n    for json_file in glob.glob(f'{merge_dir}/*json'):\n        with open(json_file) as f:\n            coco = json.load(f)\n            coco_annotations = coco['annotations']\n            all_annotations.extend(coco_annotations)\n            coco_images = coco['images']\n            all_images.extend(coco_images)\n            if not info:\n                info = coco['info']\n            if not licenses:\n                licenses = coco['licenses']\n            if not categories:\n                categories = coco['categories']\n    merged_coco = {'info': info, 'licenses': licenses, 'categories': categories, 'images': all_images, 'annotations': all_annotations}\n    os.makedirs(os.path.dirname(output), exist_ok=True)\n    with open(output, 'w') as f:\n        json.dump(merged_coco, f)"
        ]
    }
]