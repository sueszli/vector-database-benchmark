[
    {
        "func_name": "__init__",
        "original": "def __init__(self, forecasting_models: List[ForecastingModel], regression_train_n_points: int, regression_model=None, regression_train_num_samples: int=1, regression_train_samples_reduction: Optional[Union[str, float]]='median', train_forecasting_models: bool=True, train_using_historical_forecasts: bool=False, show_warnings: bool=True):\n    \"\"\"\n        Use a regression model for ensembling individual models' predictions using the stacking technique [1]_.\n\n        The provided regression model must implement ``fit()`` and ``predict()`` methods\n        (e.g. scikit-learn regression models). Note that here the regression model is used to learn how to\n        best ensemble the individual forecasting models' forecasts. It is not the same usage of regression\n        as in :class:`RegressionModel`, where the regression model is used to produce forecasts based on the\n        lagged series.\n\n        If `future_covariates` or `past_covariates` are provided at training or inference time,\n        they will be passed only to the forecasting models supporting them.\n\n        If `forecasting_models` contains exclusively GlobalForecastingModels, they can be pre-trained. Otherwise,\n        the `forecasting_models` must be untrained.\n\n        The regression model does not leverage the covariates passed to ``fit()`` and ``predict()``.\n\n        Parameters\n        ----------\n        forecasting_models\n            List of forecasting models whose predictions to ensemble\n        regression_train_n_points\n            The number of points per series to use to train the regression model. Can be set to `-1` to use the\n            entire series to train the regressor if `forecasting_models` are already fitted and\n            `train_forecasting_models=False`.\n        regression_model\n            Any regression model with ``predict()`` and ``fit()`` methods (e.g. from scikit-learn)\n            Default: ``darts.model.LinearRegressionModel(fit_intercept=False)``\n\n            .. note::\n                if `regression_model` is probabilistic, the `RegressionEnsembleModel` will also be probabilistic.\n            ..\n        regression_train_num_samples\n            Number of prediction samples from each forecasting model to train the regression model (samples are\n            averaged). Should be set to 1 for deterministic models. Default: 1.\n\n            .. note::\n                if `forecasting_models` contains a mix of probabilistic and deterministic models,\n                `regression_train_num_samples will be passed only to the probabilistic ones.\n            ..\n        regression_train_samples_reduction\n            If `forecasting_models` are probabilistic and `regression_train_num_samples` > 1, method used to\n            reduce the samples before passing them to the regression model. Possible values: \"mean\", \"median\"\n            or float value corresponding to the desired quantile. Default: \"median\"\n        train_forecasting_models\n            If set to `False`, the `forecasting_models` are not retrained when calling `fit()` (only supported\n            if all the `forecasting_models` are pretrained `GlobalForecastingModels`). Default: ``True``.\n        train_using_historical_forecasts\n            If set to `True`, use `historical_forecasts()` to generate the forecasting models' predictions used to\n            train the regression model in `fit()`. Available when `forecasting_models` contains only\n            `GlobalForecastingModels`. Recommended when `regression_train_n_points` is greater than\n            `output_chunk_length` of the underlying `forecasting_models`.\n            Default: ``False``.\n        show_warnings\n            Whether to show warnings related to forecasting_models covariates support.\n        References\n        ----------\n        .. [1] D. H. Wolpert, \u201cStacked generalization\u201d, Neural Networks, vol. 5, no. 2, pp. 241\u2013259, Jan. 1992\n\n        Examples\n        --------\n        >>> from darts.datasets import AirPassengersDataset\n        >>> from darts.models import RegressionEnsembleModel, NaiveSeasonal, LinearRegressionModel\n        >>> series = AirPassengersDataset().load()\n        >>> model = RegressionEnsembleModel(\n        >>>     forecasting_models = [\n        >>>         NaiveSeasonal(K=12),\n        >>>         LinearRegressionModel(lags=4)\n        >>>     ],\n        >>>     regression_train_n_points=20\n        >>> )\n        >>> model.fit(series)\n        >>> pred = model.predict(6)\n        >>> pred.values()\n        array([[494.24050364],\n               [464.3869697 ],\n               [496.53180506],\n               [544.82269341],\n               [557.35256055],\n               [630.24334385]])\n        \"\"\"\n    super().__init__(forecasting_models=forecasting_models, train_num_samples=regression_train_num_samples, train_samples_reduction=regression_train_samples_reduction, train_forecasting_models=train_forecasting_models, show_warnings=show_warnings)\n    if regression_model is None:\n        regression_model = LinearRegressionModel(lags=None, lags_future_covariates=[0], fit_intercept=False)\n    elif isinstance(regression_model, RegressionModel):\n        regression_model = regression_model\n    else:\n        regression_model = RegressionModel(lags_future_covariates=[0], model=regression_model)\n    raise_if_not(regression_model.lags == {'future': [0]}, f'`lags` and `lags_past_covariates` of regression model must be `None`and `lags_future_covariates` must be [0]. Given:\\n{regression_model.lags}')\n    self.regression_model: RegressionModel = regression_model\n    raise_if(regression_train_n_points == -1 and (not (self.all_trained and (not train_forecasting_models))), '`regression_train_n_points` can only be `-1` if `retrain_forecasting_model=False` and all `forecasting_models` are already fitted.', logger)\n    self.train_n_points: Union[int, List[int]] = regression_train_n_points\n    raise_if(train_using_historical_forecasts and (not self.is_global_ensemble), '`train_using_historical_forecasts=True` is only available when all `forecasting_models` are global models.', logger)\n    self.train_using_historical_forecasts = train_using_historical_forecasts",
        "mutated": [
            "def __init__(self, forecasting_models: List[ForecastingModel], regression_train_n_points: int, regression_model=None, regression_train_num_samples: int=1, regression_train_samples_reduction: Optional[Union[str, float]]='median', train_forecasting_models: bool=True, train_using_historical_forecasts: bool=False, show_warnings: bool=True):\n    if False:\n        i = 10\n    '\\n        Use a regression model for ensembling individual models\\' predictions using the stacking technique [1]_.\\n\\n        The provided regression model must implement ``fit()`` and ``predict()`` methods\\n        (e.g. scikit-learn regression models). Note that here the regression model is used to learn how to\\n        best ensemble the individual forecasting models\\' forecasts. It is not the same usage of regression\\n        as in :class:`RegressionModel`, where the regression model is used to produce forecasts based on the\\n        lagged series.\\n\\n        If `future_covariates` or `past_covariates` are provided at training or inference time,\\n        they will be passed only to the forecasting models supporting them.\\n\\n        If `forecasting_models` contains exclusively GlobalForecastingModels, they can be pre-trained. Otherwise,\\n        the `forecasting_models` must be untrained.\\n\\n        The regression model does not leverage the covariates passed to ``fit()`` and ``predict()``.\\n\\n        Parameters\\n        ----------\\n        forecasting_models\\n            List of forecasting models whose predictions to ensemble\\n        regression_train_n_points\\n            The number of points per series to use to train the regression model. Can be set to `-1` to use the\\n            entire series to train the regressor if `forecasting_models` are already fitted and\\n            `train_forecasting_models=False`.\\n        regression_model\\n            Any regression model with ``predict()`` and ``fit()`` methods (e.g. from scikit-learn)\\n            Default: ``darts.model.LinearRegressionModel(fit_intercept=False)``\\n\\n            .. note::\\n                if `regression_model` is probabilistic, the `RegressionEnsembleModel` will also be probabilistic.\\n            ..\\n        regression_train_num_samples\\n            Number of prediction samples from each forecasting model to train the regression model (samples are\\n            averaged). Should be set to 1 for deterministic models. Default: 1.\\n\\n            .. note::\\n                if `forecasting_models` contains a mix of probabilistic and deterministic models,\\n                `regression_train_num_samples will be passed only to the probabilistic ones.\\n            ..\\n        regression_train_samples_reduction\\n            If `forecasting_models` are probabilistic and `regression_train_num_samples` > 1, method used to\\n            reduce the samples before passing them to the regression model. Possible values: \"mean\", \"median\"\\n            or float value corresponding to the desired quantile. Default: \"median\"\\n        train_forecasting_models\\n            If set to `False`, the `forecasting_models` are not retrained when calling `fit()` (only supported\\n            if all the `forecasting_models` are pretrained `GlobalForecastingModels`). Default: ``True``.\\n        train_using_historical_forecasts\\n            If set to `True`, use `historical_forecasts()` to generate the forecasting models\\' predictions used to\\n            train the regression model in `fit()`. Available when `forecasting_models` contains only\\n            `GlobalForecastingModels`. Recommended when `regression_train_n_points` is greater than\\n            `output_chunk_length` of the underlying `forecasting_models`.\\n            Default: ``False``.\\n        show_warnings\\n            Whether to show warnings related to forecasting_models covariates support.\\n        References\\n        ----------\\n        .. [1] D. H. Wolpert, \u201cStacked generalization\u201d, Neural Networks, vol. 5, no. 2, pp. 241\u2013259, Jan. 1992\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.models import RegressionEnsembleModel, NaiveSeasonal, LinearRegressionModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = RegressionEnsembleModel(\\n        >>>     forecasting_models = [\\n        >>>         NaiveSeasonal(K=12),\\n        >>>         LinearRegressionModel(lags=4)\\n        >>>     ],\\n        >>>     regression_train_n_points=20\\n        >>> )\\n        >>> model.fit(series)\\n        >>> pred = model.predict(6)\\n        >>> pred.values()\\n        array([[494.24050364],\\n               [464.3869697 ],\\n               [496.53180506],\\n               [544.82269341],\\n               [557.35256055],\\n               [630.24334385]])\\n        '\n    super().__init__(forecasting_models=forecasting_models, train_num_samples=regression_train_num_samples, train_samples_reduction=regression_train_samples_reduction, train_forecasting_models=train_forecasting_models, show_warnings=show_warnings)\n    if regression_model is None:\n        regression_model = LinearRegressionModel(lags=None, lags_future_covariates=[0], fit_intercept=False)\n    elif isinstance(regression_model, RegressionModel):\n        regression_model = regression_model\n    else:\n        regression_model = RegressionModel(lags_future_covariates=[0], model=regression_model)\n    raise_if_not(regression_model.lags == {'future': [0]}, f'`lags` and `lags_past_covariates` of regression model must be `None`and `lags_future_covariates` must be [0]. Given:\\n{regression_model.lags}')\n    self.regression_model: RegressionModel = regression_model\n    raise_if(regression_train_n_points == -1 and (not (self.all_trained and (not train_forecasting_models))), '`regression_train_n_points` can only be `-1` if `retrain_forecasting_model=False` and all `forecasting_models` are already fitted.', logger)\n    self.train_n_points: Union[int, List[int]] = regression_train_n_points\n    raise_if(train_using_historical_forecasts and (not self.is_global_ensemble), '`train_using_historical_forecasts=True` is only available when all `forecasting_models` are global models.', logger)\n    self.train_using_historical_forecasts = train_using_historical_forecasts",
            "def __init__(self, forecasting_models: List[ForecastingModel], regression_train_n_points: int, regression_model=None, regression_train_num_samples: int=1, regression_train_samples_reduction: Optional[Union[str, float]]='median', train_forecasting_models: bool=True, train_using_historical_forecasts: bool=False, show_warnings: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Use a regression model for ensembling individual models\\' predictions using the stacking technique [1]_.\\n\\n        The provided regression model must implement ``fit()`` and ``predict()`` methods\\n        (e.g. scikit-learn regression models). Note that here the regression model is used to learn how to\\n        best ensemble the individual forecasting models\\' forecasts. It is not the same usage of regression\\n        as in :class:`RegressionModel`, where the regression model is used to produce forecasts based on the\\n        lagged series.\\n\\n        If `future_covariates` or `past_covariates` are provided at training or inference time,\\n        they will be passed only to the forecasting models supporting them.\\n\\n        If `forecasting_models` contains exclusively GlobalForecastingModels, they can be pre-trained. Otherwise,\\n        the `forecasting_models` must be untrained.\\n\\n        The regression model does not leverage the covariates passed to ``fit()`` and ``predict()``.\\n\\n        Parameters\\n        ----------\\n        forecasting_models\\n            List of forecasting models whose predictions to ensemble\\n        regression_train_n_points\\n            The number of points per series to use to train the regression model. Can be set to `-1` to use the\\n            entire series to train the regressor if `forecasting_models` are already fitted and\\n            `train_forecasting_models=False`.\\n        regression_model\\n            Any regression model with ``predict()`` and ``fit()`` methods (e.g. from scikit-learn)\\n            Default: ``darts.model.LinearRegressionModel(fit_intercept=False)``\\n\\n            .. note::\\n                if `regression_model` is probabilistic, the `RegressionEnsembleModel` will also be probabilistic.\\n            ..\\n        regression_train_num_samples\\n            Number of prediction samples from each forecasting model to train the regression model (samples are\\n            averaged). Should be set to 1 for deterministic models. Default: 1.\\n\\n            .. note::\\n                if `forecasting_models` contains a mix of probabilistic and deterministic models,\\n                `regression_train_num_samples will be passed only to the probabilistic ones.\\n            ..\\n        regression_train_samples_reduction\\n            If `forecasting_models` are probabilistic and `regression_train_num_samples` > 1, method used to\\n            reduce the samples before passing them to the regression model. Possible values: \"mean\", \"median\"\\n            or float value corresponding to the desired quantile. Default: \"median\"\\n        train_forecasting_models\\n            If set to `False`, the `forecasting_models` are not retrained when calling `fit()` (only supported\\n            if all the `forecasting_models` are pretrained `GlobalForecastingModels`). Default: ``True``.\\n        train_using_historical_forecasts\\n            If set to `True`, use `historical_forecasts()` to generate the forecasting models\\' predictions used to\\n            train the regression model in `fit()`. Available when `forecasting_models` contains only\\n            `GlobalForecastingModels`. Recommended when `regression_train_n_points` is greater than\\n            `output_chunk_length` of the underlying `forecasting_models`.\\n            Default: ``False``.\\n        show_warnings\\n            Whether to show warnings related to forecasting_models covariates support.\\n        References\\n        ----------\\n        .. [1] D. H. Wolpert, \u201cStacked generalization\u201d, Neural Networks, vol. 5, no. 2, pp. 241\u2013259, Jan. 1992\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.models import RegressionEnsembleModel, NaiveSeasonal, LinearRegressionModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = RegressionEnsembleModel(\\n        >>>     forecasting_models = [\\n        >>>         NaiveSeasonal(K=12),\\n        >>>         LinearRegressionModel(lags=4)\\n        >>>     ],\\n        >>>     regression_train_n_points=20\\n        >>> )\\n        >>> model.fit(series)\\n        >>> pred = model.predict(6)\\n        >>> pred.values()\\n        array([[494.24050364],\\n               [464.3869697 ],\\n               [496.53180506],\\n               [544.82269341],\\n               [557.35256055],\\n               [630.24334385]])\\n        '\n    super().__init__(forecasting_models=forecasting_models, train_num_samples=regression_train_num_samples, train_samples_reduction=regression_train_samples_reduction, train_forecasting_models=train_forecasting_models, show_warnings=show_warnings)\n    if regression_model is None:\n        regression_model = LinearRegressionModel(lags=None, lags_future_covariates=[0], fit_intercept=False)\n    elif isinstance(regression_model, RegressionModel):\n        regression_model = regression_model\n    else:\n        regression_model = RegressionModel(lags_future_covariates=[0], model=regression_model)\n    raise_if_not(regression_model.lags == {'future': [0]}, f'`lags` and `lags_past_covariates` of regression model must be `None`and `lags_future_covariates` must be [0]. Given:\\n{regression_model.lags}')\n    self.regression_model: RegressionModel = regression_model\n    raise_if(regression_train_n_points == -1 and (not (self.all_trained and (not train_forecasting_models))), '`regression_train_n_points` can only be `-1` if `retrain_forecasting_model=False` and all `forecasting_models` are already fitted.', logger)\n    self.train_n_points: Union[int, List[int]] = regression_train_n_points\n    raise_if(train_using_historical_forecasts and (not self.is_global_ensemble), '`train_using_historical_forecasts=True` is only available when all `forecasting_models` are global models.', logger)\n    self.train_using_historical_forecasts = train_using_historical_forecasts",
            "def __init__(self, forecasting_models: List[ForecastingModel], regression_train_n_points: int, regression_model=None, regression_train_num_samples: int=1, regression_train_samples_reduction: Optional[Union[str, float]]='median', train_forecasting_models: bool=True, train_using_historical_forecasts: bool=False, show_warnings: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Use a regression model for ensembling individual models\\' predictions using the stacking technique [1]_.\\n\\n        The provided regression model must implement ``fit()`` and ``predict()`` methods\\n        (e.g. scikit-learn regression models). Note that here the regression model is used to learn how to\\n        best ensemble the individual forecasting models\\' forecasts. It is not the same usage of regression\\n        as in :class:`RegressionModel`, where the regression model is used to produce forecasts based on the\\n        lagged series.\\n\\n        If `future_covariates` or `past_covariates` are provided at training or inference time,\\n        they will be passed only to the forecasting models supporting them.\\n\\n        If `forecasting_models` contains exclusively GlobalForecastingModels, they can be pre-trained. Otherwise,\\n        the `forecasting_models` must be untrained.\\n\\n        The regression model does not leverage the covariates passed to ``fit()`` and ``predict()``.\\n\\n        Parameters\\n        ----------\\n        forecasting_models\\n            List of forecasting models whose predictions to ensemble\\n        regression_train_n_points\\n            The number of points per series to use to train the regression model. Can be set to `-1` to use the\\n            entire series to train the regressor if `forecasting_models` are already fitted and\\n            `train_forecasting_models=False`.\\n        regression_model\\n            Any regression model with ``predict()`` and ``fit()`` methods (e.g. from scikit-learn)\\n            Default: ``darts.model.LinearRegressionModel(fit_intercept=False)``\\n\\n            .. note::\\n                if `regression_model` is probabilistic, the `RegressionEnsembleModel` will also be probabilistic.\\n            ..\\n        regression_train_num_samples\\n            Number of prediction samples from each forecasting model to train the regression model (samples are\\n            averaged). Should be set to 1 for deterministic models. Default: 1.\\n\\n            .. note::\\n                if `forecasting_models` contains a mix of probabilistic and deterministic models,\\n                `regression_train_num_samples will be passed only to the probabilistic ones.\\n            ..\\n        regression_train_samples_reduction\\n            If `forecasting_models` are probabilistic and `regression_train_num_samples` > 1, method used to\\n            reduce the samples before passing them to the regression model. Possible values: \"mean\", \"median\"\\n            or float value corresponding to the desired quantile. Default: \"median\"\\n        train_forecasting_models\\n            If set to `False`, the `forecasting_models` are not retrained when calling `fit()` (only supported\\n            if all the `forecasting_models` are pretrained `GlobalForecastingModels`). Default: ``True``.\\n        train_using_historical_forecasts\\n            If set to `True`, use `historical_forecasts()` to generate the forecasting models\\' predictions used to\\n            train the regression model in `fit()`. Available when `forecasting_models` contains only\\n            `GlobalForecastingModels`. Recommended when `regression_train_n_points` is greater than\\n            `output_chunk_length` of the underlying `forecasting_models`.\\n            Default: ``False``.\\n        show_warnings\\n            Whether to show warnings related to forecasting_models covariates support.\\n        References\\n        ----------\\n        .. [1] D. H. Wolpert, \u201cStacked generalization\u201d, Neural Networks, vol. 5, no. 2, pp. 241\u2013259, Jan. 1992\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.models import RegressionEnsembleModel, NaiveSeasonal, LinearRegressionModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = RegressionEnsembleModel(\\n        >>>     forecasting_models = [\\n        >>>         NaiveSeasonal(K=12),\\n        >>>         LinearRegressionModel(lags=4)\\n        >>>     ],\\n        >>>     regression_train_n_points=20\\n        >>> )\\n        >>> model.fit(series)\\n        >>> pred = model.predict(6)\\n        >>> pred.values()\\n        array([[494.24050364],\\n               [464.3869697 ],\\n               [496.53180506],\\n               [544.82269341],\\n               [557.35256055],\\n               [630.24334385]])\\n        '\n    super().__init__(forecasting_models=forecasting_models, train_num_samples=regression_train_num_samples, train_samples_reduction=regression_train_samples_reduction, train_forecasting_models=train_forecasting_models, show_warnings=show_warnings)\n    if regression_model is None:\n        regression_model = LinearRegressionModel(lags=None, lags_future_covariates=[0], fit_intercept=False)\n    elif isinstance(regression_model, RegressionModel):\n        regression_model = regression_model\n    else:\n        regression_model = RegressionModel(lags_future_covariates=[0], model=regression_model)\n    raise_if_not(regression_model.lags == {'future': [0]}, f'`lags` and `lags_past_covariates` of regression model must be `None`and `lags_future_covariates` must be [0]. Given:\\n{regression_model.lags}')\n    self.regression_model: RegressionModel = regression_model\n    raise_if(regression_train_n_points == -1 and (not (self.all_trained and (not train_forecasting_models))), '`regression_train_n_points` can only be `-1` if `retrain_forecasting_model=False` and all `forecasting_models` are already fitted.', logger)\n    self.train_n_points: Union[int, List[int]] = regression_train_n_points\n    raise_if(train_using_historical_forecasts and (not self.is_global_ensemble), '`train_using_historical_forecasts=True` is only available when all `forecasting_models` are global models.', logger)\n    self.train_using_historical_forecasts = train_using_historical_forecasts",
            "def __init__(self, forecasting_models: List[ForecastingModel], regression_train_n_points: int, regression_model=None, regression_train_num_samples: int=1, regression_train_samples_reduction: Optional[Union[str, float]]='median', train_forecasting_models: bool=True, train_using_historical_forecasts: bool=False, show_warnings: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Use a regression model for ensembling individual models\\' predictions using the stacking technique [1]_.\\n\\n        The provided regression model must implement ``fit()`` and ``predict()`` methods\\n        (e.g. scikit-learn regression models). Note that here the regression model is used to learn how to\\n        best ensemble the individual forecasting models\\' forecasts. It is not the same usage of regression\\n        as in :class:`RegressionModel`, where the regression model is used to produce forecasts based on the\\n        lagged series.\\n\\n        If `future_covariates` or `past_covariates` are provided at training or inference time,\\n        they will be passed only to the forecasting models supporting them.\\n\\n        If `forecasting_models` contains exclusively GlobalForecastingModels, they can be pre-trained. Otherwise,\\n        the `forecasting_models` must be untrained.\\n\\n        The regression model does not leverage the covariates passed to ``fit()`` and ``predict()``.\\n\\n        Parameters\\n        ----------\\n        forecasting_models\\n            List of forecasting models whose predictions to ensemble\\n        regression_train_n_points\\n            The number of points per series to use to train the regression model. Can be set to `-1` to use the\\n            entire series to train the regressor if `forecasting_models` are already fitted and\\n            `train_forecasting_models=False`.\\n        regression_model\\n            Any regression model with ``predict()`` and ``fit()`` methods (e.g. from scikit-learn)\\n            Default: ``darts.model.LinearRegressionModel(fit_intercept=False)``\\n\\n            .. note::\\n                if `regression_model` is probabilistic, the `RegressionEnsembleModel` will also be probabilistic.\\n            ..\\n        regression_train_num_samples\\n            Number of prediction samples from each forecasting model to train the regression model (samples are\\n            averaged). Should be set to 1 for deterministic models. Default: 1.\\n\\n            .. note::\\n                if `forecasting_models` contains a mix of probabilistic and deterministic models,\\n                `regression_train_num_samples will be passed only to the probabilistic ones.\\n            ..\\n        regression_train_samples_reduction\\n            If `forecasting_models` are probabilistic and `regression_train_num_samples` > 1, method used to\\n            reduce the samples before passing them to the regression model. Possible values: \"mean\", \"median\"\\n            or float value corresponding to the desired quantile. Default: \"median\"\\n        train_forecasting_models\\n            If set to `False`, the `forecasting_models` are not retrained when calling `fit()` (only supported\\n            if all the `forecasting_models` are pretrained `GlobalForecastingModels`). Default: ``True``.\\n        train_using_historical_forecasts\\n            If set to `True`, use `historical_forecasts()` to generate the forecasting models\\' predictions used to\\n            train the regression model in `fit()`. Available when `forecasting_models` contains only\\n            `GlobalForecastingModels`. Recommended when `regression_train_n_points` is greater than\\n            `output_chunk_length` of the underlying `forecasting_models`.\\n            Default: ``False``.\\n        show_warnings\\n            Whether to show warnings related to forecasting_models covariates support.\\n        References\\n        ----------\\n        .. [1] D. H. Wolpert, \u201cStacked generalization\u201d, Neural Networks, vol. 5, no. 2, pp. 241\u2013259, Jan. 1992\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.models import RegressionEnsembleModel, NaiveSeasonal, LinearRegressionModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = RegressionEnsembleModel(\\n        >>>     forecasting_models = [\\n        >>>         NaiveSeasonal(K=12),\\n        >>>         LinearRegressionModel(lags=4)\\n        >>>     ],\\n        >>>     regression_train_n_points=20\\n        >>> )\\n        >>> model.fit(series)\\n        >>> pred = model.predict(6)\\n        >>> pred.values()\\n        array([[494.24050364],\\n               [464.3869697 ],\\n               [496.53180506],\\n               [544.82269341],\\n               [557.35256055],\\n               [630.24334385]])\\n        '\n    super().__init__(forecasting_models=forecasting_models, train_num_samples=regression_train_num_samples, train_samples_reduction=regression_train_samples_reduction, train_forecasting_models=train_forecasting_models, show_warnings=show_warnings)\n    if regression_model is None:\n        regression_model = LinearRegressionModel(lags=None, lags_future_covariates=[0], fit_intercept=False)\n    elif isinstance(regression_model, RegressionModel):\n        regression_model = regression_model\n    else:\n        regression_model = RegressionModel(lags_future_covariates=[0], model=regression_model)\n    raise_if_not(regression_model.lags == {'future': [0]}, f'`lags` and `lags_past_covariates` of regression model must be `None`and `lags_future_covariates` must be [0]. Given:\\n{regression_model.lags}')\n    self.regression_model: RegressionModel = regression_model\n    raise_if(regression_train_n_points == -1 and (not (self.all_trained and (not train_forecasting_models))), '`regression_train_n_points` can only be `-1` if `retrain_forecasting_model=False` and all `forecasting_models` are already fitted.', logger)\n    self.train_n_points: Union[int, List[int]] = regression_train_n_points\n    raise_if(train_using_historical_forecasts and (not self.is_global_ensemble), '`train_using_historical_forecasts=True` is only available when all `forecasting_models` are global models.', logger)\n    self.train_using_historical_forecasts = train_using_historical_forecasts",
            "def __init__(self, forecasting_models: List[ForecastingModel], regression_train_n_points: int, regression_model=None, regression_train_num_samples: int=1, regression_train_samples_reduction: Optional[Union[str, float]]='median', train_forecasting_models: bool=True, train_using_historical_forecasts: bool=False, show_warnings: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Use a regression model for ensembling individual models\\' predictions using the stacking technique [1]_.\\n\\n        The provided regression model must implement ``fit()`` and ``predict()`` methods\\n        (e.g. scikit-learn regression models). Note that here the regression model is used to learn how to\\n        best ensemble the individual forecasting models\\' forecasts. It is not the same usage of regression\\n        as in :class:`RegressionModel`, where the regression model is used to produce forecasts based on the\\n        lagged series.\\n\\n        If `future_covariates` or `past_covariates` are provided at training or inference time,\\n        they will be passed only to the forecasting models supporting them.\\n\\n        If `forecasting_models` contains exclusively GlobalForecastingModels, they can be pre-trained. Otherwise,\\n        the `forecasting_models` must be untrained.\\n\\n        The regression model does not leverage the covariates passed to ``fit()`` and ``predict()``.\\n\\n        Parameters\\n        ----------\\n        forecasting_models\\n            List of forecasting models whose predictions to ensemble\\n        regression_train_n_points\\n            The number of points per series to use to train the regression model. Can be set to `-1` to use the\\n            entire series to train the regressor if `forecasting_models` are already fitted and\\n            `train_forecasting_models=False`.\\n        regression_model\\n            Any regression model with ``predict()`` and ``fit()`` methods (e.g. from scikit-learn)\\n            Default: ``darts.model.LinearRegressionModel(fit_intercept=False)``\\n\\n            .. note::\\n                if `regression_model` is probabilistic, the `RegressionEnsembleModel` will also be probabilistic.\\n            ..\\n        regression_train_num_samples\\n            Number of prediction samples from each forecasting model to train the regression model (samples are\\n            averaged). Should be set to 1 for deterministic models. Default: 1.\\n\\n            .. note::\\n                if `forecasting_models` contains a mix of probabilistic and deterministic models,\\n                `regression_train_num_samples will be passed only to the probabilistic ones.\\n            ..\\n        regression_train_samples_reduction\\n            If `forecasting_models` are probabilistic and `regression_train_num_samples` > 1, method used to\\n            reduce the samples before passing them to the regression model. Possible values: \"mean\", \"median\"\\n            or float value corresponding to the desired quantile. Default: \"median\"\\n        train_forecasting_models\\n            If set to `False`, the `forecasting_models` are not retrained when calling `fit()` (only supported\\n            if all the `forecasting_models` are pretrained `GlobalForecastingModels`). Default: ``True``.\\n        train_using_historical_forecasts\\n            If set to `True`, use `historical_forecasts()` to generate the forecasting models\\' predictions used to\\n            train the regression model in `fit()`. Available when `forecasting_models` contains only\\n            `GlobalForecastingModels`. Recommended when `regression_train_n_points` is greater than\\n            `output_chunk_length` of the underlying `forecasting_models`.\\n            Default: ``False``.\\n        show_warnings\\n            Whether to show warnings related to forecasting_models covariates support.\\n        References\\n        ----------\\n        .. [1] D. H. Wolpert, \u201cStacked generalization\u201d, Neural Networks, vol. 5, no. 2, pp. 241\u2013259, Jan. 1992\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.models import RegressionEnsembleModel, NaiveSeasonal, LinearRegressionModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = RegressionEnsembleModel(\\n        >>>     forecasting_models = [\\n        >>>         NaiveSeasonal(K=12),\\n        >>>         LinearRegressionModel(lags=4)\\n        >>>     ],\\n        >>>     regression_train_n_points=20\\n        >>> )\\n        >>> model.fit(series)\\n        >>> pred = model.predict(6)\\n        >>> pred.values()\\n        array([[494.24050364],\\n               [464.3869697 ],\\n               [496.53180506],\\n               [544.82269341],\\n               [557.35256055],\\n               [630.24334385]])\\n        '\n    super().__init__(forecasting_models=forecasting_models, train_num_samples=regression_train_num_samples, train_samples_reduction=regression_train_samples_reduction, train_forecasting_models=train_forecasting_models, show_warnings=show_warnings)\n    if regression_model is None:\n        regression_model = LinearRegressionModel(lags=None, lags_future_covariates=[0], fit_intercept=False)\n    elif isinstance(regression_model, RegressionModel):\n        regression_model = regression_model\n    else:\n        regression_model = RegressionModel(lags_future_covariates=[0], model=regression_model)\n    raise_if_not(regression_model.lags == {'future': [0]}, f'`lags` and `lags_past_covariates` of regression model must be `None`and `lags_future_covariates` must be [0]. Given:\\n{regression_model.lags}')\n    self.regression_model: RegressionModel = regression_model\n    raise_if(regression_train_n_points == -1 and (not (self.all_trained and (not train_forecasting_models))), '`regression_train_n_points` can only be `-1` if `retrain_forecasting_model=False` and all `forecasting_models` are already fitted.', logger)\n    self.train_n_points: Union[int, List[int]] = regression_train_n_points\n    raise_if(train_using_historical_forecasts and (not self.is_global_ensemble), '`train_using_historical_forecasts=True` is only available when all `forecasting_models` are global models.', logger)\n    self.train_using_historical_forecasts = train_using_historical_forecasts"
        ]
    },
    {
        "func_name": "_split_multi_ts_sequence",
        "original": "def _split_multi_ts_sequence(self, n: Union[int, List[int]], ts_sequence: Sequence[TimeSeries]) -> Tuple[Sequence[TimeSeries], Sequence[TimeSeries]]:\n    if isinstance(n, int):\n        n = [n] * len(ts_sequence)\n    left = [ts[:-n_] for (ts, n_) in zip(ts_sequence, n)]\n    right = [ts[-n_:] for (ts, n_) in zip(ts_sequence, n)]\n    return (left, right)",
        "mutated": [
            "def _split_multi_ts_sequence(self, n: Union[int, List[int]], ts_sequence: Sequence[TimeSeries]) -> Tuple[Sequence[TimeSeries], Sequence[TimeSeries]]:\n    if False:\n        i = 10\n    if isinstance(n, int):\n        n = [n] * len(ts_sequence)\n    left = [ts[:-n_] for (ts, n_) in zip(ts_sequence, n)]\n    right = [ts[-n_:] for (ts, n_) in zip(ts_sequence, n)]\n    return (left, right)",
            "def _split_multi_ts_sequence(self, n: Union[int, List[int]], ts_sequence: Sequence[TimeSeries]) -> Tuple[Sequence[TimeSeries], Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(n, int):\n        n = [n] * len(ts_sequence)\n    left = [ts[:-n_] for (ts, n_) in zip(ts_sequence, n)]\n    right = [ts[-n_:] for (ts, n_) in zip(ts_sequence, n)]\n    return (left, right)",
            "def _split_multi_ts_sequence(self, n: Union[int, List[int]], ts_sequence: Sequence[TimeSeries]) -> Tuple[Sequence[TimeSeries], Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(n, int):\n        n = [n] * len(ts_sequence)\n    left = [ts[:-n_] for (ts, n_) in zip(ts_sequence, n)]\n    right = [ts[-n_:] for (ts, n_) in zip(ts_sequence, n)]\n    return (left, right)",
            "def _split_multi_ts_sequence(self, n: Union[int, List[int]], ts_sequence: Sequence[TimeSeries]) -> Tuple[Sequence[TimeSeries], Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(n, int):\n        n = [n] * len(ts_sequence)\n    left = [ts[:-n_] for (ts, n_) in zip(ts_sequence, n)]\n    right = [ts[-n_:] for (ts, n_) in zip(ts_sequence, n)]\n    return (left, right)",
            "def _split_multi_ts_sequence(self, n: Union[int, List[int]], ts_sequence: Sequence[TimeSeries]) -> Tuple[Sequence[TimeSeries], Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(n, int):\n        n = [n] * len(ts_sequence)\n    left = [ts[:-n_] for (ts, n_) in zip(ts_sequence, n)]\n    right = [ts[-n_:] for (ts, n_) in zip(ts_sequence, n)]\n    return (left, right)"
        ]
    },
    {
        "func_name": "_make_multiple_historical_forecasts",
        "original": "def _make_multiple_historical_forecasts(self, train_n_points: int, series: Union[TimeSeries, Sequence[TimeSeries]], direct_predictions: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, num_samples: int=1) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    \"\"\"\n        For GlobalForecastingModel, when predicting n > output_chunk_length, `historical_forecasts()` generally\n        produce better forecasts than `predict()`.\n\n        To get as close as possible to the predictions generated by the forecasting models during inference,\n        `historical_forecasts` forecast horizon is equal to each model output_chunk_length.\n\n        train_n_points are generated, starting from the end of the series.\n        \"\"\"\n    is_single_series = isinstance(series, TimeSeries)\n    series = series2seq(series)\n    direct_predictions = series2seq(direct_predictions)\n    past_covariates = series2seq(past_covariates)\n    future_covariates = series2seq(future_covariates)\n    n_components = series[0].n_components\n    model_predict_cols = direct_predictions[0].columns.tolist()\n    predictions = []\n    for (m_idx, model) in enumerate(self.forecasting_models):\n        n_ocl_back = train_n_points // model.output_chunk_length\n        start_hist_forecasts = n_ocl_back * model.output_chunk_length\n        missing_steps = train_n_points % model.output_chunk_length\n        tmp_pred = model.historical_forecasts(series=series, past_covariates=past_covariates if model.supports_past_covariates else None, future_covariates=future_covariates if model.supports_future_covariates else None, forecast_horizon=model.output_chunk_length, stride=model.output_chunk_length, num_samples=num_samples if model._is_probabilistic else 1, start=-start_hist_forecasts, start_format='position', retrain=False, overlap_end=False, last_points_only=False, show_warnings=self.show_warnings, predict_likelihood_parameters=False)\n        if is_single_series:\n            tmp_pred = [concatenate(tmp_pred, axis=0)]\n        else:\n            tmp_pred = [concatenate(sub_pred, axis=0) for sub_pred in tmp_pred]\n        if missing_steps:\n            pred_cols = model_predict_cols[m_idx * n_components:(m_idx + 1) * n_components]\n            hfc_cols = tmp_pred[0].columns.tolist()\n            tmp_pred = [concatenate([preds_dir[:missing_steps][pred_cols].with_columns_renamed(pred_cols, hfc_cols), preds_hfc], axis=0) for (preds_dir, preds_hfc) in zip(direct_predictions, tmp_pred)]\n        predictions.append(tmp_pred)\n    tmp_predictions = []\n    for prediction in predictions:\n        tmp_predictions.append([ts for (idx, ts) in enumerate(prediction)])\n    predictions = [seq2series(prediction) for prediction in tmp_predictions]\n    if self.train_samples_reduction is not None and self.train_num_samples > 1:\n        predictions = [self._predictions_reduction(prediction) for prediction in predictions]\n    return self._stack_ts_seq(predictions) if is_single_series else self._stack_ts_multiseq(predictions)",
        "mutated": [
            "def _make_multiple_historical_forecasts(self, train_n_points: int, series: Union[TimeSeries, Sequence[TimeSeries]], direct_predictions: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, num_samples: int=1) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n    '\\n        For GlobalForecastingModel, when predicting n > output_chunk_length, `historical_forecasts()` generally\\n        produce better forecasts than `predict()`.\\n\\n        To get as close as possible to the predictions generated by the forecasting models during inference,\\n        `historical_forecasts` forecast horizon is equal to each model output_chunk_length.\\n\\n        train_n_points are generated, starting from the end of the series.\\n        '\n    is_single_series = isinstance(series, TimeSeries)\n    series = series2seq(series)\n    direct_predictions = series2seq(direct_predictions)\n    past_covariates = series2seq(past_covariates)\n    future_covariates = series2seq(future_covariates)\n    n_components = series[0].n_components\n    model_predict_cols = direct_predictions[0].columns.tolist()\n    predictions = []\n    for (m_idx, model) in enumerate(self.forecasting_models):\n        n_ocl_back = train_n_points // model.output_chunk_length\n        start_hist_forecasts = n_ocl_back * model.output_chunk_length\n        missing_steps = train_n_points % model.output_chunk_length\n        tmp_pred = model.historical_forecasts(series=series, past_covariates=past_covariates if model.supports_past_covariates else None, future_covariates=future_covariates if model.supports_future_covariates else None, forecast_horizon=model.output_chunk_length, stride=model.output_chunk_length, num_samples=num_samples if model._is_probabilistic else 1, start=-start_hist_forecasts, start_format='position', retrain=False, overlap_end=False, last_points_only=False, show_warnings=self.show_warnings, predict_likelihood_parameters=False)\n        if is_single_series:\n            tmp_pred = [concatenate(tmp_pred, axis=0)]\n        else:\n            tmp_pred = [concatenate(sub_pred, axis=0) for sub_pred in tmp_pred]\n        if missing_steps:\n            pred_cols = model_predict_cols[m_idx * n_components:(m_idx + 1) * n_components]\n            hfc_cols = tmp_pred[0].columns.tolist()\n            tmp_pred = [concatenate([preds_dir[:missing_steps][pred_cols].with_columns_renamed(pred_cols, hfc_cols), preds_hfc], axis=0) for (preds_dir, preds_hfc) in zip(direct_predictions, tmp_pred)]\n        predictions.append(tmp_pred)\n    tmp_predictions = []\n    for prediction in predictions:\n        tmp_predictions.append([ts for (idx, ts) in enumerate(prediction)])\n    predictions = [seq2series(prediction) for prediction in tmp_predictions]\n    if self.train_samples_reduction is not None and self.train_num_samples > 1:\n        predictions = [self._predictions_reduction(prediction) for prediction in predictions]\n    return self._stack_ts_seq(predictions) if is_single_series else self._stack_ts_multiseq(predictions)",
            "def _make_multiple_historical_forecasts(self, train_n_points: int, series: Union[TimeSeries, Sequence[TimeSeries]], direct_predictions: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, num_samples: int=1) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        For GlobalForecastingModel, when predicting n > output_chunk_length, `historical_forecasts()` generally\\n        produce better forecasts than `predict()`.\\n\\n        To get as close as possible to the predictions generated by the forecasting models during inference,\\n        `historical_forecasts` forecast horizon is equal to each model output_chunk_length.\\n\\n        train_n_points are generated, starting from the end of the series.\\n        '\n    is_single_series = isinstance(series, TimeSeries)\n    series = series2seq(series)\n    direct_predictions = series2seq(direct_predictions)\n    past_covariates = series2seq(past_covariates)\n    future_covariates = series2seq(future_covariates)\n    n_components = series[0].n_components\n    model_predict_cols = direct_predictions[0].columns.tolist()\n    predictions = []\n    for (m_idx, model) in enumerate(self.forecasting_models):\n        n_ocl_back = train_n_points // model.output_chunk_length\n        start_hist_forecasts = n_ocl_back * model.output_chunk_length\n        missing_steps = train_n_points % model.output_chunk_length\n        tmp_pred = model.historical_forecasts(series=series, past_covariates=past_covariates if model.supports_past_covariates else None, future_covariates=future_covariates if model.supports_future_covariates else None, forecast_horizon=model.output_chunk_length, stride=model.output_chunk_length, num_samples=num_samples if model._is_probabilistic else 1, start=-start_hist_forecasts, start_format='position', retrain=False, overlap_end=False, last_points_only=False, show_warnings=self.show_warnings, predict_likelihood_parameters=False)\n        if is_single_series:\n            tmp_pred = [concatenate(tmp_pred, axis=0)]\n        else:\n            tmp_pred = [concatenate(sub_pred, axis=0) for sub_pred in tmp_pred]\n        if missing_steps:\n            pred_cols = model_predict_cols[m_idx * n_components:(m_idx + 1) * n_components]\n            hfc_cols = tmp_pred[0].columns.tolist()\n            tmp_pred = [concatenate([preds_dir[:missing_steps][pred_cols].with_columns_renamed(pred_cols, hfc_cols), preds_hfc], axis=0) for (preds_dir, preds_hfc) in zip(direct_predictions, tmp_pred)]\n        predictions.append(tmp_pred)\n    tmp_predictions = []\n    for prediction in predictions:\n        tmp_predictions.append([ts for (idx, ts) in enumerate(prediction)])\n    predictions = [seq2series(prediction) for prediction in tmp_predictions]\n    if self.train_samples_reduction is not None and self.train_num_samples > 1:\n        predictions = [self._predictions_reduction(prediction) for prediction in predictions]\n    return self._stack_ts_seq(predictions) if is_single_series else self._stack_ts_multiseq(predictions)",
            "def _make_multiple_historical_forecasts(self, train_n_points: int, series: Union[TimeSeries, Sequence[TimeSeries]], direct_predictions: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, num_samples: int=1) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        For GlobalForecastingModel, when predicting n > output_chunk_length, `historical_forecasts()` generally\\n        produce better forecasts than `predict()`.\\n\\n        To get as close as possible to the predictions generated by the forecasting models during inference,\\n        `historical_forecasts` forecast horizon is equal to each model output_chunk_length.\\n\\n        train_n_points are generated, starting from the end of the series.\\n        '\n    is_single_series = isinstance(series, TimeSeries)\n    series = series2seq(series)\n    direct_predictions = series2seq(direct_predictions)\n    past_covariates = series2seq(past_covariates)\n    future_covariates = series2seq(future_covariates)\n    n_components = series[0].n_components\n    model_predict_cols = direct_predictions[0].columns.tolist()\n    predictions = []\n    for (m_idx, model) in enumerate(self.forecasting_models):\n        n_ocl_back = train_n_points // model.output_chunk_length\n        start_hist_forecasts = n_ocl_back * model.output_chunk_length\n        missing_steps = train_n_points % model.output_chunk_length\n        tmp_pred = model.historical_forecasts(series=series, past_covariates=past_covariates if model.supports_past_covariates else None, future_covariates=future_covariates if model.supports_future_covariates else None, forecast_horizon=model.output_chunk_length, stride=model.output_chunk_length, num_samples=num_samples if model._is_probabilistic else 1, start=-start_hist_forecasts, start_format='position', retrain=False, overlap_end=False, last_points_only=False, show_warnings=self.show_warnings, predict_likelihood_parameters=False)\n        if is_single_series:\n            tmp_pred = [concatenate(tmp_pred, axis=0)]\n        else:\n            tmp_pred = [concatenate(sub_pred, axis=0) for sub_pred in tmp_pred]\n        if missing_steps:\n            pred_cols = model_predict_cols[m_idx * n_components:(m_idx + 1) * n_components]\n            hfc_cols = tmp_pred[0].columns.tolist()\n            tmp_pred = [concatenate([preds_dir[:missing_steps][pred_cols].with_columns_renamed(pred_cols, hfc_cols), preds_hfc], axis=0) for (preds_dir, preds_hfc) in zip(direct_predictions, tmp_pred)]\n        predictions.append(tmp_pred)\n    tmp_predictions = []\n    for prediction in predictions:\n        tmp_predictions.append([ts for (idx, ts) in enumerate(prediction)])\n    predictions = [seq2series(prediction) for prediction in tmp_predictions]\n    if self.train_samples_reduction is not None and self.train_num_samples > 1:\n        predictions = [self._predictions_reduction(prediction) for prediction in predictions]\n    return self._stack_ts_seq(predictions) if is_single_series else self._stack_ts_multiseq(predictions)",
            "def _make_multiple_historical_forecasts(self, train_n_points: int, series: Union[TimeSeries, Sequence[TimeSeries]], direct_predictions: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, num_samples: int=1) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        For GlobalForecastingModel, when predicting n > output_chunk_length, `historical_forecasts()` generally\\n        produce better forecasts than `predict()`.\\n\\n        To get as close as possible to the predictions generated by the forecasting models during inference,\\n        `historical_forecasts` forecast horizon is equal to each model output_chunk_length.\\n\\n        train_n_points are generated, starting from the end of the series.\\n        '\n    is_single_series = isinstance(series, TimeSeries)\n    series = series2seq(series)\n    direct_predictions = series2seq(direct_predictions)\n    past_covariates = series2seq(past_covariates)\n    future_covariates = series2seq(future_covariates)\n    n_components = series[0].n_components\n    model_predict_cols = direct_predictions[0].columns.tolist()\n    predictions = []\n    for (m_idx, model) in enumerate(self.forecasting_models):\n        n_ocl_back = train_n_points // model.output_chunk_length\n        start_hist_forecasts = n_ocl_back * model.output_chunk_length\n        missing_steps = train_n_points % model.output_chunk_length\n        tmp_pred = model.historical_forecasts(series=series, past_covariates=past_covariates if model.supports_past_covariates else None, future_covariates=future_covariates if model.supports_future_covariates else None, forecast_horizon=model.output_chunk_length, stride=model.output_chunk_length, num_samples=num_samples if model._is_probabilistic else 1, start=-start_hist_forecasts, start_format='position', retrain=False, overlap_end=False, last_points_only=False, show_warnings=self.show_warnings, predict_likelihood_parameters=False)\n        if is_single_series:\n            tmp_pred = [concatenate(tmp_pred, axis=0)]\n        else:\n            tmp_pred = [concatenate(sub_pred, axis=0) for sub_pred in tmp_pred]\n        if missing_steps:\n            pred_cols = model_predict_cols[m_idx * n_components:(m_idx + 1) * n_components]\n            hfc_cols = tmp_pred[0].columns.tolist()\n            tmp_pred = [concatenate([preds_dir[:missing_steps][pred_cols].with_columns_renamed(pred_cols, hfc_cols), preds_hfc], axis=0) for (preds_dir, preds_hfc) in zip(direct_predictions, tmp_pred)]\n        predictions.append(tmp_pred)\n    tmp_predictions = []\n    for prediction in predictions:\n        tmp_predictions.append([ts for (idx, ts) in enumerate(prediction)])\n    predictions = [seq2series(prediction) for prediction in tmp_predictions]\n    if self.train_samples_reduction is not None and self.train_num_samples > 1:\n        predictions = [self._predictions_reduction(prediction) for prediction in predictions]\n    return self._stack_ts_seq(predictions) if is_single_series else self._stack_ts_multiseq(predictions)",
            "def _make_multiple_historical_forecasts(self, train_n_points: int, series: Union[TimeSeries, Sequence[TimeSeries]], direct_predictions: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, num_samples: int=1) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        For GlobalForecastingModel, when predicting n > output_chunk_length, `historical_forecasts()` generally\\n        produce better forecasts than `predict()`.\\n\\n        To get as close as possible to the predictions generated by the forecasting models during inference,\\n        `historical_forecasts` forecast horizon is equal to each model output_chunk_length.\\n\\n        train_n_points are generated, starting from the end of the series.\\n        '\n    is_single_series = isinstance(series, TimeSeries)\n    series = series2seq(series)\n    direct_predictions = series2seq(direct_predictions)\n    past_covariates = series2seq(past_covariates)\n    future_covariates = series2seq(future_covariates)\n    n_components = series[0].n_components\n    model_predict_cols = direct_predictions[0].columns.tolist()\n    predictions = []\n    for (m_idx, model) in enumerate(self.forecasting_models):\n        n_ocl_back = train_n_points // model.output_chunk_length\n        start_hist_forecasts = n_ocl_back * model.output_chunk_length\n        missing_steps = train_n_points % model.output_chunk_length\n        tmp_pred = model.historical_forecasts(series=series, past_covariates=past_covariates if model.supports_past_covariates else None, future_covariates=future_covariates if model.supports_future_covariates else None, forecast_horizon=model.output_chunk_length, stride=model.output_chunk_length, num_samples=num_samples if model._is_probabilistic else 1, start=-start_hist_forecasts, start_format='position', retrain=False, overlap_end=False, last_points_only=False, show_warnings=self.show_warnings, predict_likelihood_parameters=False)\n        if is_single_series:\n            tmp_pred = [concatenate(tmp_pred, axis=0)]\n        else:\n            tmp_pred = [concatenate(sub_pred, axis=0) for sub_pred in tmp_pred]\n        if missing_steps:\n            pred_cols = model_predict_cols[m_idx * n_components:(m_idx + 1) * n_components]\n            hfc_cols = tmp_pred[0].columns.tolist()\n            tmp_pred = [concatenate([preds_dir[:missing_steps][pred_cols].with_columns_renamed(pred_cols, hfc_cols), preds_hfc], axis=0) for (preds_dir, preds_hfc) in zip(direct_predictions, tmp_pred)]\n        predictions.append(tmp_pred)\n    tmp_predictions = []\n    for prediction in predictions:\n        tmp_predictions.append([ts for (idx, ts) in enumerate(prediction)])\n    predictions = [seq2series(prediction) for prediction in tmp_predictions]\n    if self.train_samples_reduction is not None and self.train_num_samples > 1:\n        predictions = [self._predictions_reduction(prediction) for prediction in predictions]\n    return self._stack_ts_seq(predictions) if is_single_series else self._stack_ts_multiseq(predictions)"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    \"\"\"\n        Fits the forecasting models with the entire series except the last `regression_train_n_points` values, which\n        are used to train the regression model.\n\n        If `forecasting_models` contains fitted `GlobalForecastingModels` and `train_forecasting_model=False`,\n        only the regression model will be trained.\n\n        Parameters\n        ----------\n        series\n            TimeSeries or Sequence[TimeSeries] object containing the target values.\n        past_covariates\n            Optionally, a series or sequence of series specifying past-observed covariates passed to the\n            forecasting models\n        future_covariates\n            Optionally, a series or sequence of series specifying future-known covariates passed to the\n            forecasting models\n        \"\"\"\n    super().fit(series, past_covariates=past_covariates, future_covariates=future_covariates)\n    is_single_series = isinstance(series, TimeSeries)\n    if self.train_n_points == -1:\n        if is_single_series:\n            train_n_points = [len(series)]\n        else:\n            train_n_points = [len(ts) for ts in series]\n        all_shifts = []\n        for model in self.forecasting_models:\n            (min_target_lag, _, _, _, _, _) = model.extreme_lags\n            if min_target_lag is not None:\n                all_shifts.append(-min_target_lag)\n        input_shift = max(all_shifts)\n        idx_series_too_short = []\n        tmp_train_n_points = []\n        for (idx, ts_length) in enumerate(train_n_points):\n            ajusted_length = ts_length - input_shift\n            if ajusted_length < 0:\n                idx_series_too_short.append(idx)\n            else:\n                tmp_train_n_points.append(ajusted_length)\n        raise_if(len(idx_series_too_short) > 0, f'TimeSeries at indexes {idx_series_too_short} of `series` are too short to train the regression model due to the number of values necessary to produce one prediction : {input_shift}.', logger)\n        if is_single_series:\n            self.train_n_points = tmp_train_n_points[0]\n        else:\n            self.train_n_points = tmp_train_n_points\n        train_n_points_too_big = False\n    elif is_single_series:\n        train_n_points_too_big = len(series) <= self.train_n_points\n    else:\n        train_n_points_too_big = any([len(s) <= self.train_n_points for s in series])\n    raise_if(train_n_points_too_big, '`regression_train_n_points` parameter too big (must be strictly smaller than the number of points in training_series)', logger)\n    if is_single_series:\n        forecast_training = series[:-self.train_n_points]\n        regression_target = series[-self.train_n_points:]\n    else:\n        (forecast_training, regression_target) = self._split_multi_ts_sequence(self.train_n_points, series)\n    if self.train_forecasting_models:\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates)\n    predictions = self._make_multiple_predictions(n=self.train_n_points, series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    if self.train_using_historical_forecasts:\n        predictions = self._make_multiple_historical_forecasts(train_n_points=self.train_n_points, series=series, direct_predictions=predictions, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    self.regression_model.fit(series=regression_target, future_covariates=predictions)\n    if self.train_forecasting_models:\n        self.forecasting_models: List[ForecastingModel] = [model.untrained_model() for model in self.forecasting_models]\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=series, past_covariates=past_covariates, future_covariates=future_covariates)\n    return self",
        "mutated": [
            "def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n    '\\n        Fits the forecasting models with the entire series except the last `regression_train_n_points` values, which\\n        are used to train the regression model.\\n\\n        If `forecasting_models` contains fitted `GlobalForecastingModels` and `train_forecasting_model=False`,\\n        only the regression model will be trained.\\n\\n        Parameters\\n        ----------\\n        series\\n            TimeSeries or Sequence[TimeSeries] object containing the target values.\\n        past_covariates\\n            Optionally, a series or sequence of series specifying past-observed covariates passed to the\\n            forecasting models\\n        future_covariates\\n            Optionally, a series or sequence of series specifying future-known covariates passed to the\\n            forecasting models\\n        '\n    super().fit(series, past_covariates=past_covariates, future_covariates=future_covariates)\n    is_single_series = isinstance(series, TimeSeries)\n    if self.train_n_points == -1:\n        if is_single_series:\n            train_n_points = [len(series)]\n        else:\n            train_n_points = [len(ts) for ts in series]\n        all_shifts = []\n        for model in self.forecasting_models:\n            (min_target_lag, _, _, _, _, _) = model.extreme_lags\n            if min_target_lag is not None:\n                all_shifts.append(-min_target_lag)\n        input_shift = max(all_shifts)\n        idx_series_too_short = []\n        tmp_train_n_points = []\n        for (idx, ts_length) in enumerate(train_n_points):\n            ajusted_length = ts_length - input_shift\n            if ajusted_length < 0:\n                idx_series_too_short.append(idx)\n            else:\n                tmp_train_n_points.append(ajusted_length)\n        raise_if(len(idx_series_too_short) > 0, f'TimeSeries at indexes {idx_series_too_short} of `series` are too short to train the regression model due to the number of values necessary to produce one prediction : {input_shift}.', logger)\n        if is_single_series:\n            self.train_n_points = tmp_train_n_points[0]\n        else:\n            self.train_n_points = tmp_train_n_points\n        train_n_points_too_big = False\n    elif is_single_series:\n        train_n_points_too_big = len(series) <= self.train_n_points\n    else:\n        train_n_points_too_big = any([len(s) <= self.train_n_points for s in series])\n    raise_if(train_n_points_too_big, '`regression_train_n_points` parameter too big (must be strictly smaller than the number of points in training_series)', logger)\n    if is_single_series:\n        forecast_training = series[:-self.train_n_points]\n        regression_target = series[-self.train_n_points:]\n    else:\n        (forecast_training, regression_target) = self._split_multi_ts_sequence(self.train_n_points, series)\n    if self.train_forecasting_models:\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates)\n    predictions = self._make_multiple_predictions(n=self.train_n_points, series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    if self.train_using_historical_forecasts:\n        predictions = self._make_multiple_historical_forecasts(train_n_points=self.train_n_points, series=series, direct_predictions=predictions, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    self.regression_model.fit(series=regression_target, future_covariates=predictions)\n    if self.train_forecasting_models:\n        self.forecasting_models: List[ForecastingModel] = [model.untrained_model() for model in self.forecasting_models]\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=series, past_covariates=past_covariates, future_covariates=future_covariates)\n    return self",
            "def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fits the forecasting models with the entire series except the last `regression_train_n_points` values, which\\n        are used to train the regression model.\\n\\n        If `forecasting_models` contains fitted `GlobalForecastingModels` and `train_forecasting_model=False`,\\n        only the regression model will be trained.\\n\\n        Parameters\\n        ----------\\n        series\\n            TimeSeries or Sequence[TimeSeries] object containing the target values.\\n        past_covariates\\n            Optionally, a series or sequence of series specifying past-observed covariates passed to the\\n            forecasting models\\n        future_covariates\\n            Optionally, a series or sequence of series specifying future-known covariates passed to the\\n            forecasting models\\n        '\n    super().fit(series, past_covariates=past_covariates, future_covariates=future_covariates)\n    is_single_series = isinstance(series, TimeSeries)\n    if self.train_n_points == -1:\n        if is_single_series:\n            train_n_points = [len(series)]\n        else:\n            train_n_points = [len(ts) for ts in series]\n        all_shifts = []\n        for model in self.forecasting_models:\n            (min_target_lag, _, _, _, _, _) = model.extreme_lags\n            if min_target_lag is not None:\n                all_shifts.append(-min_target_lag)\n        input_shift = max(all_shifts)\n        idx_series_too_short = []\n        tmp_train_n_points = []\n        for (idx, ts_length) in enumerate(train_n_points):\n            ajusted_length = ts_length - input_shift\n            if ajusted_length < 0:\n                idx_series_too_short.append(idx)\n            else:\n                tmp_train_n_points.append(ajusted_length)\n        raise_if(len(idx_series_too_short) > 0, f'TimeSeries at indexes {idx_series_too_short} of `series` are too short to train the regression model due to the number of values necessary to produce one prediction : {input_shift}.', logger)\n        if is_single_series:\n            self.train_n_points = tmp_train_n_points[0]\n        else:\n            self.train_n_points = tmp_train_n_points\n        train_n_points_too_big = False\n    elif is_single_series:\n        train_n_points_too_big = len(series) <= self.train_n_points\n    else:\n        train_n_points_too_big = any([len(s) <= self.train_n_points for s in series])\n    raise_if(train_n_points_too_big, '`regression_train_n_points` parameter too big (must be strictly smaller than the number of points in training_series)', logger)\n    if is_single_series:\n        forecast_training = series[:-self.train_n_points]\n        regression_target = series[-self.train_n_points:]\n    else:\n        (forecast_training, regression_target) = self._split_multi_ts_sequence(self.train_n_points, series)\n    if self.train_forecasting_models:\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates)\n    predictions = self._make_multiple_predictions(n=self.train_n_points, series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    if self.train_using_historical_forecasts:\n        predictions = self._make_multiple_historical_forecasts(train_n_points=self.train_n_points, series=series, direct_predictions=predictions, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    self.regression_model.fit(series=regression_target, future_covariates=predictions)\n    if self.train_forecasting_models:\n        self.forecasting_models: List[ForecastingModel] = [model.untrained_model() for model in self.forecasting_models]\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=series, past_covariates=past_covariates, future_covariates=future_covariates)\n    return self",
            "def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fits the forecasting models with the entire series except the last `regression_train_n_points` values, which\\n        are used to train the regression model.\\n\\n        If `forecasting_models` contains fitted `GlobalForecastingModels` and `train_forecasting_model=False`,\\n        only the regression model will be trained.\\n\\n        Parameters\\n        ----------\\n        series\\n            TimeSeries or Sequence[TimeSeries] object containing the target values.\\n        past_covariates\\n            Optionally, a series or sequence of series specifying past-observed covariates passed to the\\n            forecasting models\\n        future_covariates\\n            Optionally, a series or sequence of series specifying future-known covariates passed to the\\n            forecasting models\\n        '\n    super().fit(series, past_covariates=past_covariates, future_covariates=future_covariates)\n    is_single_series = isinstance(series, TimeSeries)\n    if self.train_n_points == -1:\n        if is_single_series:\n            train_n_points = [len(series)]\n        else:\n            train_n_points = [len(ts) for ts in series]\n        all_shifts = []\n        for model in self.forecasting_models:\n            (min_target_lag, _, _, _, _, _) = model.extreme_lags\n            if min_target_lag is not None:\n                all_shifts.append(-min_target_lag)\n        input_shift = max(all_shifts)\n        idx_series_too_short = []\n        tmp_train_n_points = []\n        for (idx, ts_length) in enumerate(train_n_points):\n            ajusted_length = ts_length - input_shift\n            if ajusted_length < 0:\n                idx_series_too_short.append(idx)\n            else:\n                tmp_train_n_points.append(ajusted_length)\n        raise_if(len(idx_series_too_short) > 0, f'TimeSeries at indexes {idx_series_too_short} of `series` are too short to train the regression model due to the number of values necessary to produce one prediction : {input_shift}.', logger)\n        if is_single_series:\n            self.train_n_points = tmp_train_n_points[0]\n        else:\n            self.train_n_points = tmp_train_n_points\n        train_n_points_too_big = False\n    elif is_single_series:\n        train_n_points_too_big = len(series) <= self.train_n_points\n    else:\n        train_n_points_too_big = any([len(s) <= self.train_n_points for s in series])\n    raise_if(train_n_points_too_big, '`regression_train_n_points` parameter too big (must be strictly smaller than the number of points in training_series)', logger)\n    if is_single_series:\n        forecast_training = series[:-self.train_n_points]\n        regression_target = series[-self.train_n_points:]\n    else:\n        (forecast_training, regression_target) = self._split_multi_ts_sequence(self.train_n_points, series)\n    if self.train_forecasting_models:\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates)\n    predictions = self._make_multiple_predictions(n=self.train_n_points, series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    if self.train_using_historical_forecasts:\n        predictions = self._make_multiple_historical_forecasts(train_n_points=self.train_n_points, series=series, direct_predictions=predictions, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    self.regression_model.fit(series=regression_target, future_covariates=predictions)\n    if self.train_forecasting_models:\n        self.forecasting_models: List[ForecastingModel] = [model.untrained_model() for model in self.forecasting_models]\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=series, past_covariates=past_covariates, future_covariates=future_covariates)\n    return self",
            "def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fits the forecasting models with the entire series except the last `regression_train_n_points` values, which\\n        are used to train the regression model.\\n\\n        If `forecasting_models` contains fitted `GlobalForecastingModels` and `train_forecasting_model=False`,\\n        only the regression model will be trained.\\n\\n        Parameters\\n        ----------\\n        series\\n            TimeSeries or Sequence[TimeSeries] object containing the target values.\\n        past_covariates\\n            Optionally, a series or sequence of series specifying past-observed covariates passed to the\\n            forecasting models\\n        future_covariates\\n            Optionally, a series or sequence of series specifying future-known covariates passed to the\\n            forecasting models\\n        '\n    super().fit(series, past_covariates=past_covariates, future_covariates=future_covariates)\n    is_single_series = isinstance(series, TimeSeries)\n    if self.train_n_points == -1:\n        if is_single_series:\n            train_n_points = [len(series)]\n        else:\n            train_n_points = [len(ts) for ts in series]\n        all_shifts = []\n        for model in self.forecasting_models:\n            (min_target_lag, _, _, _, _, _) = model.extreme_lags\n            if min_target_lag is not None:\n                all_shifts.append(-min_target_lag)\n        input_shift = max(all_shifts)\n        idx_series_too_short = []\n        tmp_train_n_points = []\n        for (idx, ts_length) in enumerate(train_n_points):\n            ajusted_length = ts_length - input_shift\n            if ajusted_length < 0:\n                idx_series_too_short.append(idx)\n            else:\n                tmp_train_n_points.append(ajusted_length)\n        raise_if(len(idx_series_too_short) > 0, f'TimeSeries at indexes {idx_series_too_short} of `series` are too short to train the regression model due to the number of values necessary to produce one prediction : {input_shift}.', logger)\n        if is_single_series:\n            self.train_n_points = tmp_train_n_points[0]\n        else:\n            self.train_n_points = tmp_train_n_points\n        train_n_points_too_big = False\n    elif is_single_series:\n        train_n_points_too_big = len(series) <= self.train_n_points\n    else:\n        train_n_points_too_big = any([len(s) <= self.train_n_points for s in series])\n    raise_if(train_n_points_too_big, '`regression_train_n_points` parameter too big (must be strictly smaller than the number of points in training_series)', logger)\n    if is_single_series:\n        forecast_training = series[:-self.train_n_points]\n        regression_target = series[-self.train_n_points:]\n    else:\n        (forecast_training, regression_target) = self._split_multi_ts_sequence(self.train_n_points, series)\n    if self.train_forecasting_models:\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates)\n    predictions = self._make_multiple_predictions(n=self.train_n_points, series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    if self.train_using_historical_forecasts:\n        predictions = self._make_multiple_historical_forecasts(train_n_points=self.train_n_points, series=series, direct_predictions=predictions, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    self.regression_model.fit(series=regression_target, future_covariates=predictions)\n    if self.train_forecasting_models:\n        self.forecasting_models: List[ForecastingModel] = [model.untrained_model() for model in self.forecasting_models]\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=series, past_covariates=past_covariates, future_covariates=future_covariates)\n    return self",
            "def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]], past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fits the forecasting models with the entire series except the last `regression_train_n_points` values, which\\n        are used to train the regression model.\\n\\n        If `forecasting_models` contains fitted `GlobalForecastingModels` and `train_forecasting_model=False`,\\n        only the regression model will be trained.\\n\\n        Parameters\\n        ----------\\n        series\\n            TimeSeries or Sequence[TimeSeries] object containing the target values.\\n        past_covariates\\n            Optionally, a series or sequence of series specifying past-observed covariates passed to the\\n            forecasting models\\n        future_covariates\\n            Optionally, a series or sequence of series specifying future-known covariates passed to the\\n            forecasting models\\n        '\n    super().fit(series, past_covariates=past_covariates, future_covariates=future_covariates)\n    is_single_series = isinstance(series, TimeSeries)\n    if self.train_n_points == -1:\n        if is_single_series:\n            train_n_points = [len(series)]\n        else:\n            train_n_points = [len(ts) for ts in series]\n        all_shifts = []\n        for model in self.forecasting_models:\n            (min_target_lag, _, _, _, _, _) = model.extreme_lags\n            if min_target_lag is not None:\n                all_shifts.append(-min_target_lag)\n        input_shift = max(all_shifts)\n        idx_series_too_short = []\n        tmp_train_n_points = []\n        for (idx, ts_length) in enumerate(train_n_points):\n            ajusted_length = ts_length - input_shift\n            if ajusted_length < 0:\n                idx_series_too_short.append(idx)\n            else:\n                tmp_train_n_points.append(ajusted_length)\n        raise_if(len(idx_series_too_short) > 0, f'TimeSeries at indexes {idx_series_too_short} of `series` are too short to train the regression model due to the number of values necessary to produce one prediction : {input_shift}.', logger)\n        if is_single_series:\n            self.train_n_points = tmp_train_n_points[0]\n        else:\n            self.train_n_points = tmp_train_n_points\n        train_n_points_too_big = False\n    elif is_single_series:\n        train_n_points_too_big = len(series) <= self.train_n_points\n    else:\n        train_n_points_too_big = any([len(s) <= self.train_n_points for s in series])\n    raise_if(train_n_points_too_big, '`regression_train_n_points` parameter too big (must be strictly smaller than the number of points in training_series)', logger)\n    if is_single_series:\n        forecast_training = series[:-self.train_n_points]\n        regression_target = series[-self.train_n_points:]\n    else:\n        (forecast_training, regression_target) = self._split_multi_ts_sequence(self.train_n_points, series)\n    if self.train_forecasting_models:\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates)\n    predictions = self._make_multiple_predictions(n=self.train_n_points, series=forecast_training, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    if self.train_using_historical_forecasts:\n        predictions = self._make_multiple_historical_forecasts(train_n_points=self.train_n_points, series=series, direct_predictions=predictions, past_covariates=past_covariates, future_covariates=future_covariates, num_samples=self.train_num_samples)\n    self.regression_model.fit(series=regression_target, future_covariates=predictions)\n    if self.train_forecasting_models:\n        self.forecasting_models: List[ForecastingModel] = [model.untrained_model() for model in self.forecasting_models]\n        for model in self.forecasting_models:\n            model._fit_wrapper(series=series, past_covariates=past_covariates, future_covariates=future_covariates)\n    return self"
        ]
    },
    {
        "func_name": "ensemble",
        "original": "def ensemble(self, predictions: Union[TimeSeries, Sequence[TimeSeries]], series: Union[TimeSeries, Sequence[TimeSeries]], num_samples: int=1, predict_likelihood_parameters: bool=False) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    is_single_series = isinstance(series, TimeSeries) or series is None\n    predictions = series2seq(predictions)\n    series = series2seq(series) if series is not None else [None]\n    ensembled = [self.regression_model.predict(n=len(prediction), series=serie, future_covariates=prediction, num_samples=num_samples, predict_likelihood_parameters=predict_likelihood_parameters) for (serie, prediction) in zip(series, predictions)]\n    return seq2series(ensembled) if is_single_series else ensembled",
        "mutated": [
            "def ensemble(self, predictions: Union[TimeSeries, Sequence[TimeSeries]], series: Union[TimeSeries, Sequence[TimeSeries]], num_samples: int=1, predict_likelihood_parameters: bool=False) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n    is_single_series = isinstance(series, TimeSeries) or series is None\n    predictions = series2seq(predictions)\n    series = series2seq(series) if series is not None else [None]\n    ensembled = [self.regression_model.predict(n=len(prediction), series=serie, future_covariates=prediction, num_samples=num_samples, predict_likelihood_parameters=predict_likelihood_parameters) for (serie, prediction) in zip(series, predictions)]\n    return seq2series(ensembled) if is_single_series else ensembled",
            "def ensemble(self, predictions: Union[TimeSeries, Sequence[TimeSeries]], series: Union[TimeSeries, Sequence[TimeSeries]], num_samples: int=1, predict_likelihood_parameters: bool=False) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    is_single_series = isinstance(series, TimeSeries) or series is None\n    predictions = series2seq(predictions)\n    series = series2seq(series) if series is not None else [None]\n    ensembled = [self.regression_model.predict(n=len(prediction), series=serie, future_covariates=prediction, num_samples=num_samples, predict_likelihood_parameters=predict_likelihood_parameters) for (serie, prediction) in zip(series, predictions)]\n    return seq2series(ensembled) if is_single_series else ensembled",
            "def ensemble(self, predictions: Union[TimeSeries, Sequence[TimeSeries]], series: Union[TimeSeries, Sequence[TimeSeries]], num_samples: int=1, predict_likelihood_parameters: bool=False) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    is_single_series = isinstance(series, TimeSeries) or series is None\n    predictions = series2seq(predictions)\n    series = series2seq(series) if series is not None else [None]\n    ensembled = [self.regression_model.predict(n=len(prediction), series=serie, future_covariates=prediction, num_samples=num_samples, predict_likelihood_parameters=predict_likelihood_parameters) for (serie, prediction) in zip(series, predictions)]\n    return seq2series(ensembled) if is_single_series else ensembled",
            "def ensemble(self, predictions: Union[TimeSeries, Sequence[TimeSeries]], series: Union[TimeSeries, Sequence[TimeSeries]], num_samples: int=1, predict_likelihood_parameters: bool=False) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    is_single_series = isinstance(series, TimeSeries) or series is None\n    predictions = series2seq(predictions)\n    series = series2seq(series) if series is not None else [None]\n    ensembled = [self.regression_model.predict(n=len(prediction), series=serie, future_covariates=prediction, num_samples=num_samples, predict_likelihood_parameters=predict_likelihood_parameters) for (serie, prediction) in zip(series, predictions)]\n    return seq2series(ensembled) if is_single_series else ensembled",
            "def ensemble(self, predictions: Union[TimeSeries, Sequence[TimeSeries]], series: Union[TimeSeries, Sequence[TimeSeries]], num_samples: int=1, predict_likelihood_parameters: bool=False) -> Union[TimeSeries, Sequence[TimeSeries]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    is_single_series = isinstance(series, TimeSeries) or series is None\n    predictions = series2seq(predictions)\n    series = series2seq(series) if series is not None else [None]\n    ensembled = [self.regression_model.predict(n=len(prediction), series=serie, future_covariates=prediction, num_samples=num_samples, predict_likelihood_parameters=predict_likelihood_parameters) for (serie, prediction) in zip(series, predictions)]\n    return seq2series(ensembled) if is_single_series else ensembled"
        ]
    },
    {
        "func_name": "extreme_lags",
        "original": "@property\ndef extreme_lags(self) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[int], Optional[int], Optional[int]]:\n    extreme_lags_ = super().extreme_lags\n    if extreme_lags_[0] is None:\n        return (-self.train_n_points,) + extreme_lags_[1:]\n    else:\n        return (extreme_lags_[0] - self.train_n_points,) + extreme_lags_[1:]",
        "mutated": [
            "@property\ndef extreme_lags(self) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[int], Optional[int], Optional[int]]:\n    if False:\n        i = 10\n    extreme_lags_ = super().extreme_lags\n    if extreme_lags_[0] is None:\n        return (-self.train_n_points,) + extreme_lags_[1:]\n    else:\n        return (extreme_lags_[0] - self.train_n_points,) + extreme_lags_[1:]",
            "@property\ndef extreme_lags(self) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[int], Optional[int], Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    extreme_lags_ = super().extreme_lags\n    if extreme_lags_[0] is None:\n        return (-self.train_n_points,) + extreme_lags_[1:]\n    else:\n        return (extreme_lags_[0] - self.train_n_points,) + extreme_lags_[1:]",
            "@property\ndef extreme_lags(self) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[int], Optional[int], Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    extreme_lags_ = super().extreme_lags\n    if extreme_lags_[0] is None:\n        return (-self.train_n_points,) + extreme_lags_[1:]\n    else:\n        return (extreme_lags_[0] - self.train_n_points,) + extreme_lags_[1:]",
            "@property\ndef extreme_lags(self) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[int], Optional[int], Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    extreme_lags_ = super().extreme_lags\n    if extreme_lags_[0] is None:\n        return (-self.train_n_points,) + extreme_lags_[1:]\n    else:\n        return (extreme_lags_[0] - self.train_n_points,) + extreme_lags_[1:]",
            "@property\ndef extreme_lags(self) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[int], Optional[int], Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    extreme_lags_ = super().extreme_lags\n    if extreme_lags_[0] is None:\n        return (-self.train_n_points,) + extreme_lags_[1:]\n    else:\n        return (extreme_lags_[0] - self.train_n_points,) + extreme_lags_[1:]"
        ]
    },
    {
        "func_name": "output_chunk_length",
        "original": "@property\ndef output_chunk_length(self) -> int:\n    \"\"\"Return the `output_chunk_length` of the regression model (ensembling layer)\"\"\"\n    return self.regression_model.output_chunk_length",
        "mutated": [
            "@property\ndef output_chunk_length(self) -> int:\n    if False:\n        i = 10\n    'Return the `output_chunk_length` of the regression model (ensembling layer)'\n    return self.regression_model.output_chunk_length",
            "@property\ndef output_chunk_length(self) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return the `output_chunk_length` of the regression model (ensembling layer)'\n    return self.regression_model.output_chunk_length",
            "@property\ndef output_chunk_length(self) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return the `output_chunk_length` of the regression model (ensembling layer)'\n    return self.regression_model.output_chunk_length",
            "@property\ndef output_chunk_length(self) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return the `output_chunk_length` of the regression model (ensembling layer)'\n    return self.regression_model.output_chunk_length",
            "@property\ndef output_chunk_length(self) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return the `output_chunk_length` of the regression model (ensembling layer)'\n    return self.regression_model.output_chunk_length"
        ]
    },
    {
        "func_name": "supports_likelihood_parameter_prediction",
        "original": "@property\ndef supports_likelihood_parameter_prediction(self) -> bool:\n    \"\"\"RegressionEnsembleModel supports likelihood parameters predictions if its regression model does\"\"\"\n    return self.regression_model.supports_likelihood_parameter_prediction",
        "mutated": [
            "@property\ndef supports_likelihood_parameter_prediction(self) -> bool:\n    if False:\n        i = 10\n    'RegressionEnsembleModel supports likelihood parameters predictions if its regression model does'\n    return self.regression_model.supports_likelihood_parameter_prediction",
            "@property\ndef supports_likelihood_parameter_prediction(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'RegressionEnsembleModel supports likelihood parameters predictions if its regression model does'\n    return self.regression_model.supports_likelihood_parameter_prediction",
            "@property\ndef supports_likelihood_parameter_prediction(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'RegressionEnsembleModel supports likelihood parameters predictions if its regression model does'\n    return self.regression_model.supports_likelihood_parameter_prediction",
            "@property\ndef supports_likelihood_parameter_prediction(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'RegressionEnsembleModel supports likelihood parameters predictions if its regression model does'\n    return self.regression_model.supports_likelihood_parameter_prediction",
            "@property\ndef supports_likelihood_parameter_prediction(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'RegressionEnsembleModel supports likelihood parameters predictions if its regression model does'\n    return self.regression_model.supports_likelihood_parameter_prediction"
        ]
    },
    {
        "func_name": "supports_multivariate",
        "original": "@property\ndef supports_multivariate(self) -> bool:\n    return super().supports_multivariate and self.regression_model.supports_multivariate",
        "mutated": [
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n    return super().supports_multivariate and self.regression_model.supports_multivariate",
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return super().supports_multivariate and self.regression_model.supports_multivariate",
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return super().supports_multivariate and self.regression_model.supports_multivariate",
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return super().supports_multivariate and self.regression_model.supports_multivariate",
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return super().supports_multivariate and self.regression_model.supports_multivariate"
        ]
    },
    {
        "func_name": "_is_probabilistic",
        "original": "@property\ndef _is_probabilistic(self) -> bool:\n    \"\"\"\n        A RegressionEnsembleModel is probabilistic if its regression\n        model is probabilistic (ensembling layer)\n        \"\"\"\n    return self.regression_model._is_probabilistic",
        "mutated": [
            "@property\ndef _is_probabilistic(self) -> bool:\n    if False:\n        i = 10\n    '\\n        A RegressionEnsembleModel is probabilistic if its regression\\n        model is probabilistic (ensembling layer)\\n        '\n    return self.regression_model._is_probabilistic",
            "@property\ndef _is_probabilistic(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        A RegressionEnsembleModel is probabilistic if its regression\\n        model is probabilistic (ensembling layer)\\n        '\n    return self.regression_model._is_probabilistic",
            "@property\ndef _is_probabilistic(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        A RegressionEnsembleModel is probabilistic if its regression\\n        model is probabilistic (ensembling layer)\\n        '\n    return self.regression_model._is_probabilistic",
            "@property\ndef _is_probabilistic(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        A RegressionEnsembleModel is probabilistic if its regression\\n        model is probabilistic (ensembling layer)\\n        '\n    return self.regression_model._is_probabilistic",
            "@property\ndef _is_probabilistic(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        A RegressionEnsembleModel is probabilistic if its regression\\n        model is probabilistic (ensembling layer)\\n        '\n    return self.regression_model._is_probabilistic"
        ]
    }
]