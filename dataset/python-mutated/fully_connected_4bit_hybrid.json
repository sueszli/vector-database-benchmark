[
    {
        "func_name": "build_graph",
        "original": "def build_graph(parameters):\n    \"\"\"Build a matmul graph given `parameters`.\"\"\"\n    input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n    float_data = np.random.uniform(-1, 1, parameters['shape2'])\n    scale = np.abs(float_data).max() / 7.0\n    int_data = np.round(float_data / scale)\n    input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n    quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n    out = tf.matmul(input_tensor1, quantized)\n    return ([input_tensor1], [out])",
        "mutated": [
            "def build_graph(parameters):\n    if False:\n        i = 10\n    'Build a matmul graph given `parameters`.'\n    input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n    float_data = np.random.uniform(-1, 1, parameters['shape2'])\n    scale = np.abs(float_data).max() / 7.0\n    int_data = np.round(float_data / scale)\n    input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n    quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n    out = tf.matmul(input_tensor1, quantized)\n    return ([input_tensor1], [out])",
            "def build_graph(parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Build a matmul graph given `parameters`.'\n    input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n    float_data = np.random.uniform(-1, 1, parameters['shape2'])\n    scale = np.abs(float_data).max() / 7.0\n    int_data = np.round(float_data / scale)\n    input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n    quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n    out = tf.matmul(input_tensor1, quantized)\n    return ([input_tensor1], [out])",
            "def build_graph(parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Build a matmul graph given `parameters`.'\n    input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n    float_data = np.random.uniform(-1, 1, parameters['shape2'])\n    scale = np.abs(float_data).max() / 7.0\n    int_data = np.round(float_data / scale)\n    input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n    quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n    out = tf.matmul(input_tensor1, quantized)\n    return ([input_tensor1], [out])",
            "def build_graph(parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Build a matmul graph given `parameters`.'\n    input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n    float_data = np.random.uniform(-1, 1, parameters['shape2'])\n    scale = np.abs(float_data).max() / 7.0\n    int_data = np.round(float_data / scale)\n    input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n    quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n    out = tf.matmul(input_tensor1, quantized)\n    return ([input_tensor1], [out])",
            "def build_graph(parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Build a matmul graph given `parameters`.'\n    input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n    float_data = np.random.uniform(-1, 1, parameters['shape2'])\n    scale = np.abs(float_data).max() / 7.0\n    int_data = np.round(float_data / scale)\n    input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n    quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n    out = tf.matmul(input_tensor1, quantized)\n    return ([input_tensor1], [out])"
        ]
    },
    {
        "func_name": "create_input_data",
        "original": "def create_input_data(parameters):\n    \"\"\"Create a float input with no quantization loss.\"\"\"\n    float_data = np.random.random(parameters['shape1']).astype(np.float32)\n    scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n    return np.round(float_data / scale)",
        "mutated": [
            "def create_input_data(parameters):\n    if False:\n        i = 10\n    'Create a float input with no quantization loss.'\n    float_data = np.random.random(parameters['shape1']).astype(np.float32)\n    scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n    return np.round(float_data / scale)",
            "def create_input_data(parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a float input with no quantization loss.'\n    float_data = np.random.random(parameters['shape1']).astype(np.float32)\n    scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n    return np.round(float_data / scale)",
            "def create_input_data(parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a float input with no quantization loss.'\n    float_data = np.random.random(parameters['shape1']).astype(np.float32)\n    scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n    return np.round(float_data / scale)",
            "def create_input_data(parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a float input with no quantization loss.'\n    float_data = np.random.random(parameters['shape1']).astype(np.float32)\n    scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n    return np.round(float_data / scale)",
            "def create_input_data(parameters):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a float input with no quantization loss.'\n    float_data = np.random.random(parameters['shape1']).astype(np.float32)\n    scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n    return np.round(float_data / scale)"
        ]
    },
    {
        "func_name": "build_inputs",
        "original": "def build_inputs(parameters, sess, inputs, outputs):\n    \"\"\"Build list of input values.\n\n    Use the specialized method, as dynamic range quantization will cause\n    differing outputs from TF, which does not quantize inputs.\n    \"\"\"\n    values = [create_input_data(parameters)]\n    return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))",
        "mutated": [
            "def build_inputs(parameters, sess, inputs, outputs):\n    if False:\n        i = 10\n    'Build list of input values.\\n\\n    Use the specialized method, as dynamic range quantization will cause\\n    differing outputs from TF, which does not quantize inputs.\\n    '\n    values = [create_input_data(parameters)]\n    return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))",
            "def build_inputs(parameters, sess, inputs, outputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Build list of input values.\\n\\n    Use the specialized method, as dynamic range quantization will cause\\n    differing outputs from TF, which does not quantize inputs.\\n    '\n    values = [create_input_data(parameters)]\n    return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))",
            "def build_inputs(parameters, sess, inputs, outputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Build list of input values.\\n\\n    Use the specialized method, as dynamic range quantization will cause\\n    differing outputs from TF, which does not quantize inputs.\\n    '\n    values = [create_input_data(parameters)]\n    return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))",
            "def build_inputs(parameters, sess, inputs, outputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Build list of input values.\\n\\n    Use the specialized method, as dynamic range quantization will cause\\n    differing outputs from TF, which does not quantize inputs.\\n    '\n    values = [create_input_data(parameters)]\n    return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))",
            "def build_inputs(parameters, sess, inputs, outputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Build list of input values.\\n\\n    Use the specialized method, as dynamic range quantization will cause\\n    differing outputs from TF, which does not quantize inputs.\\n    '\n    values = [create_input_data(parameters)]\n    return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))"
        ]
    },
    {
        "func_name": "make_fully_connected_4bit_hybrid_tests",
        "original": "@register_make_test_function()\ndef make_fully_connected_4bit_hybrid_tests(options):\n    \"\"\"Make a set of tests to do fully_connected.\"\"\"\n    test_parameters = [{'shape1': [[3, 3]], 'shape2': [[3, 3]], 'dynamic_range_quantize': [True]}, {'shape1': [[40, 42]], 'shape2': [[42, 40]], 'dynamic_range_quantize': [True]}, {'shape1': [[1, 40]], 'shape2': [[40, 3]], 'dynamic_range_quantize': [True]}]\n\n    def build_graph(parameters):\n        \"\"\"Build a matmul graph given `parameters`.\"\"\"\n        input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n        float_data = np.random.uniform(-1, 1, parameters['shape2'])\n        scale = np.abs(float_data).max() / 7.0\n        int_data = np.round(float_data / scale)\n        input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n        quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n        out = tf.matmul(input_tensor1, quantized)\n        return ([input_tensor1], [out])\n\n    def create_input_data(parameters):\n        \"\"\"Create a float input with no quantization loss.\"\"\"\n        float_data = np.random.random(parameters['shape1']).astype(np.float32)\n        scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n        return np.round(float_data / scale)\n\n    def build_inputs(parameters, sess, inputs, outputs):\n        \"\"\"Build list of input values.\n\n    Use the specialized method, as dynamic range quantization will cause\n    differing outputs from TF, which does not quantize inputs.\n    \"\"\"\n        values = [create_input_data(parameters)]\n        return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))\n    options.experimental_low_bit_qat = True\n    make_zip_of_tests(options, test_parameters, build_graph, build_inputs, expected_tf_failures=0)",
        "mutated": [
            "@register_make_test_function()\ndef make_fully_connected_4bit_hybrid_tests(options):\n    if False:\n        i = 10\n    'Make a set of tests to do fully_connected.'\n    test_parameters = [{'shape1': [[3, 3]], 'shape2': [[3, 3]], 'dynamic_range_quantize': [True]}, {'shape1': [[40, 42]], 'shape2': [[42, 40]], 'dynamic_range_quantize': [True]}, {'shape1': [[1, 40]], 'shape2': [[40, 3]], 'dynamic_range_quantize': [True]}]\n\n    def build_graph(parameters):\n        \"\"\"Build a matmul graph given `parameters`.\"\"\"\n        input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n        float_data = np.random.uniform(-1, 1, parameters['shape2'])\n        scale = np.abs(float_data).max() / 7.0\n        int_data = np.round(float_data / scale)\n        input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n        quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n        out = tf.matmul(input_tensor1, quantized)\n        return ([input_tensor1], [out])\n\n    def create_input_data(parameters):\n        \"\"\"Create a float input with no quantization loss.\"\"\"\n        float_data = np.random.random(parameters['shape1']).astype(np.float32)\n        scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n        return np.round(float_data / scale)\n\n    def build_inputs(parameters, sess, inputs, outputs):\n        \"\"\"Build list of input values.\n\n    Use the specialized method, as dynamic range quantization will cause\n    differing outputs from TF, which does not quantize inputs.\n    \"\"\"\n        values = [create_input_data(parameters)]\n        return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))\n    options.experimental_low_bit_qat = True\n    make_zip_of_tests(options, test_parameters, build_graph, build_inputs, expected_tf_failures=0)",
            "@register_make_test_function()\ndef make_fully_connected_4bit_hybrid_tests(options):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Make a set of tests to do fully_connected.'\n    test_parameters = [{'shape1': [[3, 3]], 'shape2': [[3, 3]], 'dynamic_range_quantize': [True]}, {'shape1': [[40, 42]], 'shape2': [[42, 40]], 'dynamic_range_quantize': [True]}, {'shape1': [[1, 40]], 'shape2': [[40, 3]], 'dynamic_range_quantize': [True]}]\n\n    def build_graph(parameters):\n        \"\"\"Build a matmul graph given `parameters`.\"\"\"\n        input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n        float_data = np.random.uniform(-1, 1, parameters['shape2'])\n        scale = np.abs(float_data).max() / 7.0\n        int_data = np.round(float_data / scale)\n        input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n        quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n        out = tf.matmul(input_tensor1, quantized)\n        return ([input_tensor1], [out])\n\n    def create_input_data(parameters):\n        \"\"\"Create a float input with no quantization loss.\"\"\"\n        float_data = np.random.random(parameters['shape1']).astype(np.float32)\n        scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n        return np.round(float_data / scale)\n\n    def build_inputs(parameters, sess, inputs, outputs):\n        \"\"\"Build list of input values.\n\n    Use the specialized method, as dynamic range quantization will cause\n    differing outputs from TF, which does not quantize inputs.\n    \"\"\"\n        values = [create_input_data(parameters)]\n        return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))\n    options.experimental_low_bit_qat = True\n    make_zip_of_tests(options, test_parameters, build_graph, build_inputs, expected_tf_failures=0)",
            "@register_make_test_function()\ndef make_fully_connected_4bit_hybrid_tests(options):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Make a set of tests to do fully_connected.'\n    test_parameters = [{'shape1': [[3, 3]], 'shape2': [[3, 3]], 'dynamic_range_quantize': [True]}, {'shape1': [[40, 42]], 'shape2': [[42, 40]], 'dynamic_range_quantize': [True]}, {'shape1': [[1, 40]], 'shape2': [[40, 3]], 'dynamic_range_quantize': [True]}]\n\n    def build_graph(parameters):\n        \"\"\"Build a matmul graph given `parameters`.\"\"\"\n        input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n        float_data = np.random.uniform(-1, 1, parameters['shape2'])\n        scale = np.abs(float_data).max() / 7.0\n        int_data = np.round(float_data / scale)\n        input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n        quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n        out = tf.matmul(input_tensor1, quantized)\n        return ([input_tensor1], [out])\n\n    def create_input_data(parameters):\n        \"\"\"Create a float input with no quantization loss.\"\"\"\n        float_data = np.random.random(parameters['shape1']).astype(np.float32)\n        scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n        return np.round(float_data / scale)\n\n    def build_inputs(parameters, sess, inputs, outputs):\n        \"\"\"Build list of input values.\n\n    Use the specialized method, as dynamic range quantization will cause\n    differing outputs from TF, which does not quantize inputs.\n    \"\"\"\n        values = [create_input_data(parameters)]\n        return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))\n    options.experimental_low_bit_qat = True\n    make_zip_of_tests(options, test_parameters, build_graph, build_inputs, expected_tf_failures=0)",
            "@register_make_test_function()\ndef make_fully_connected_4bit_hybrid_tests(options):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Make a set of tests to do fully_connected.'\n    test_parameters = [{'shape1': [[3, 3]], 'shape2': [[3, 3]], 'dynamic_range_quantize': [True]}, {'shape1': [[40, 42]], 'shape2': [[42, 40]], 'dynamic_range_quantize': [True]}, {'shape1': [[1, 40]], 'shape2': [[40, 3]], 'dynamic_range_quantize': [True]}]\n\n    def build_graph(parameters):\n        \"\"\"Build a matmul graph given `parameters`.\"\"\"\n        input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n        float_data = np.random.uniform(-1, 1, parameters['shape2'])\n        scale = np.abs(float_data).max() / 7.0\n        int_data = np.round(float_data / scale)\n        input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n        quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n        out = tf.matmul(input_tensor1, quantized)\n        return ([input_tensor1], [out])\n\n    def create_input_data(parameters):\n        \"\"\"Create a float input with no quantization loss.\"\"\"\n        float_data = np.random.random(parameters['shape1']).astype(np.float32)\n        scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n        return np.round(float_data / scale)\n\n    def build_inputs(parameters, sess, inputs, outputs):\n        \"\"\"Build list of input values.\n\n    Use the specialized method, as dynamic range quantization will cause\n    differing outputs from TF, which does not quantize inputs.\n    \"\"\"\n        values = [create_input_data(parameters)]\n        return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))\n    options.experimental_low_bit_qat = True\n    make_zip_of_tests(options, test_parameters, build_graph, build_inputs, expected_tf_failures=0)",
            "@register_make_test_function()\ndef make_fully_connected_4bit_hybrid_tests(options):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Make a set of tests to do fully_connected.'\n    test_parameters = [{'shape1': [[3, 3]], 'shape2': [[3, 3]], 'dynamic_range_quantize': [True]}, {'shape1': [[40, 42]], 'shape2': [[42, 40]], 'dynamic_range_quantize': [True]}, {'shape1': [[1, 40]], 'shape2': [[40, 3]], 'dynamic_range_quantize': [True]}]\n\n    def build_graph(parameters):\n        \"\"\"Build a matmul graph given `parameters`.\"\"\"\n        input_tensor1 = tf.compat.v1.placeholder(dtype=tf.float32, name='input1', shape=parameters['shape1'])\n        float_data = np.random.uniform(-1, 1, parameters['shape2'])\n        scale = np.abs(float_data).max() / 7.0\n        int_data = np.round(float_data / scale)\n        input_tensor2 = tf.constant(int_data, dtype=tf.float32)\n        quantized = tf.quantization.fake_quant_with_min_max_vars(input_tensor2, min=-7, max=7, num_bits=4, narrow_range=True)\n        out = tf.matmul(input_tensor1, quantized)\n        return ([input_tensor1], [out])\n\n    def create_input_data(parameters):\n        \"\"\"Create a float input with no quantization loss.\"\"\"\n        float_data = np.random.random(parameters['shape1']).astype(np.float32)\n        scale = np.abs(float_data).max(axis=1, keepdims=True) / 127.0\n        return np.round(float_data / scale)\n\n    def build_inputs(parameters, sess, inputs, outputs):\n        \"\"\"Build list of input values.\n\n    Use the specialized method, as dynamic range quantization will cause\n    differing outputs from TF, which does not quantize inputs.\n    \"\"\"\n        values = [create_input_data(parameters)]\n        return (values, sess.run(outputs, feed_dict=dict(zip(inputs, values))))\n    options.experimental_low_bit_qat = True\n    make_zip_of_tests(options, test_parameters, build_graph, build_inputs, expected_tf_failures=0)"
        ]
    }
]