[
    {
        "func_name": "model",
        "original": "def model(num_trials):\n    with pyro.plate('data', num_trials.size(0)):\n        phi_prior = dist.Uniform(num_trials.new_tensor(0.0), num_trials.new_tensor(1.0))\n        success_prob = pyro.sample('phi', phi_prior)\n        return pyro.sample('obs', dist.Binomial(num_trials, success_prob))",
        "mutated": [
            "def model(num_trials):\n    if False:\n        i = 10\n    with pyro.plate('data', num_trials.size(0)):\n        phi_prior = dist.Uniform(num_trials.new_tensor(0.0), num_trials.new_tensor(1.0))\n        success_prob = pyro.sample('phi', phi_prior)\n        return pyro.sample('obs', dist.Binomial(num_trials, success_prob))",
            "def model(num_trials):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with pyro.plate('data', num_trials.size(0)):\n        phi_prior = dist.Uniform(num_trials.new_tensor(0.0), num_trials.new_tensor(1.0))\n        success_prob = pyro.sample('phi', phi_prior)\n        return pyro.sample('obs', dist.Binomial(num_trials, success_prob))",
            "def model(num_trials):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with pyro.plate('data', num_trials.size(0)):\n        phi_prior = dist.Uniform(num_trials.new_tensor(0.0), num_trials.new_tensor(1.0))\n        success_prob = pyro.sample('phi', phi_prior)\n        return pyro.sample('obs', dist.Binomial(num_trials, success_prob))",
            "def model(num_trials):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with pyro.plate('data', num_trials.size(0)):\n        phi_prior = dist.Uniform(num_trials.new_tensor(0.0), num_trials.new_tensor(1.0))\n        success_prob = pyro.sample('phi', phi_prior)\n        return pyro.sample('obs', dist.Binomial(num_trials, success_prob))",
            "def model(num_trials):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with pyro.plate('data', num_trials.size(0)):\n        phi_prior = dist.Uniform(num_trials.new_tensor(0.0), num_trials.new_tensor(1.0))\n        success_prob = pyro.sample('phi', phi_prior)\n        return pyro.sample('obs', dist.Binomial(num_trials, success_prob))"
        ]
    },
    {
        "func_name": "nested",
        "original": "def nested():\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={'obs': num_success})\n    nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n    mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n    return mcmc_run",
        "mutated": [
            "def nested():\n    if False:\n        i = 10\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={'obs': num_success})\n    nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n    mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n    return mcmc_run",
            "def nested():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={'obs': num_success})\n    nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n    mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n    return mcmc_run",
            "def nested():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={'obs': num_success})\n    nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n    mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n    return mcmc_run",
            "def nested():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={'obs': num_success})\n    nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n    mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n    return mcmc_run",
            "def nested():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={'obs': num_success})\n    nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n    mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n    return mcmc_run"
        ]
    },
    {
        "func_name": "test_nesting",
        "original": "def test_nesting():\n\n    def nested():\n        true_probs = torch.ones(5) * 0.7\n        num_trials = torch.ones(5) * 1000\n        num_success = dist.Binomial(num_trials, true_probs).sample()\n        conditioned_model = poutine.condition(model, data={'obs': num_success})\n        nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n        mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n        return mcmc_run\n    with poutine.trace() as tp:\n        nested()\n        nested()\n    assert len(tp.trace.nodes) == 0",
        "mutated": [
            "def test_nesting():\n    if False:\n        i = 10\n\n    def nested():\n        true_probs = torch.ones(5) * 0.7\n        num_trials = torch.ones(5) * 1000\n        num_success = dist.Binomial(num_trials, true_probs).sample()\n        conditioned_model = poutine.condition(model, data={'obs': num_success})\n        nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n        mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n        return mcmc_run\n    with poutine.trace() as tp:\n        nested()\n        nested()\n    assert len(tp.trace.nodes) == 0",
            "def test_nesting():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def nested():\n        true_probs = torch.ones(5) * 0.7\n        num_trials = torch.ones(5) * 1000\n        num_success = dist.Binomial(num_trials, true_probs).sample()\n        conditioned_model = poutine.condition(model, data={'obs': num_success})\n        nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n        mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n        return mcmc_run\n    with poutine.trace() as tp:\n        nested()\n        nested()\n    assert len(tp.trace.nodes) == 0",
            "def test_nesting():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def nested():\n        true_probs = torch.ones(5) * 0.7\n        num_trials = torch.ones(5) * 1000\n        num_success = dist.Binomial(num_trials, true_probs).sample()\n        conditioned_model = poutine.condition(model, data={'obs': num_success})\n        nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n        mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n        return mcmc_run\n    with poutine.trace() as tp:\n        nested()\n        nested()\n    assert len(tp.trace.nodes) == 0",
            "def test_nesting():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def nested():\n        true_probs = torch.ones(5) * 0.7\n        num_trials = torch.ones(5) * 1000\n        num_success = dist.Binomial(num_trials, true_probs).sample()\n        conditioned_model = poutine.condition(model, data={'obs': num_success})\n        nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n        mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n        return mcmc_run\n    with poutine.trace() as tp:\n        nested()\n        nested()\n    assert len(tp.trace.nodes) == 0",
            "def test_nesting():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def nested():\n        true_probs = torch.ones(5) * 0.7\n        num_trials = torch.ones(5) * 1000\n        num_success = dist.Binomial(num_trials, true_probs).sample()\n        conditioned_model = poutine.condition(model, data={'obs': num_success})\n        nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n        mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n        return mcmc_run\n    with poutine.trace() as tp:\n        nested()\n        nested()\n    assert len(tp.trace.nodes) == 0"
        ]
    },
    {
        "func_name": "model",
        "original": "def model():\n    mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n    log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n    with pyro.plate('plate'):\n        pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)",
        "mutated": [
            "def model():\n    if False:\n        i = 10\n    mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n    log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n    with pyro.plate('plate'):\n        pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)",
            "def model():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n    log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n    with pyro.plate('plate'):\n        pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)",
            "def model():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n    log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n    with pyro.plate('plate'):\n        pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)",
            "def model():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n    log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n    with pyro.plate('plate'):\n        pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)",
            "def model():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n    log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n    with pyro.plate('plate'):\n        pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)"
        ]
    },
    {
        "func_name": "test_information_criterion",
        "original": "@pytest.mark.filterwarnings('ignore::FutureWarning')\ndef test_information_criterion():\n    kcal = torch.tensor([0.49, 0.47, 0.56, 0.89, 0.92, 0.8, 0.46, 0.71, 0.68, 0.97, 0.84, 0.62, 0.54, 0.49, 0.48, 0.55, 0.71])\n    kcal_mean = kcal.mean()\n    kcal_logstd = kcal.std().log()\n\n    def model():\n        mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n        log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n        with pyro.plate('plate'):\n            pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)\n    delta_guide = AutoLaplaceApproximation(model)\n    svi = SVI(model, delta_guide, optim.Adam({'lr': 0.05}), loss=Trace_ELBO(), num_steps=0, num_samples=3000)\n    for i in range(100):\n        svi.step()\n    svi.guide = delta_guide.laplace_approximation()\n    posterior = svi.run()\n    ic = posterior.information_criterion()\n    assert_equal(ic['waic'], torch.tensor(-8.3), prec=0.2)\n    assert_equal(ic['p_waic'], torch.tensor(1.8), prec=0.2)",
        "mutated": [
            "@pytest.mark.filterwarnings('ignore::FutureWarning')\ndef test_information_criterion():\n    if False:\n        i = 10\n    kcal = torch.tensor([0.49, 0.47, 0.56, 0.89, 0.92, 0.8, 0.46, 0.71, 0.68, 0.97, 0.84, 0.62, 0.54, 0.49, 0.48, 0.55, 0.71])\n    kcal_mean = kcal.mean()\n    kcal_logstd = kcal.std().log()\n\n    def model():\n        mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n        log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n        with pyro.plate('plate'):\n            pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)\n    delta_guide = AutoLaplaceApproximation(model)\n    svi = SVI(model, delta_guide, optim.Adam({'lr': 0.05}), loss=Trace_ELBO(), num_steps=0, num_samples=3000)\n    for i in range(100):\n        svi.step()\n    svi.guide = delta_guide.laplace_approximation()\n    posterior = svi.run()\n    ic = posterior.information_criterion()\n    assert_equal(ic['waic'], torch.tensor(-8.3), prec=0.2)\n    assert_equal(ic['p_waic'], torch.tensor(1.8), prec=0.2)",
            "@pytest.mark.filterwarnings('ignore::FutureWarning')\ndef test_information_criterion():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    kcal = torch.tensor([0.49, 0.47, 0.56, 0.89, 0.92, 0.8, 0.46, 0.71, 0.68, 0.97, 0.84, 0.62, 0.54, 0.49, 0.48, 0.55, 0.71])\n    kcal_mean = kcal.mean()\n    kcal_logstd = kcal.std().log()\n\n    def model():\n        mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n        log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n        with pyro.plate('plate'):\n            pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)\n    delta_guide = AutoLaplaceApproximation(model)\n    svi = SVI(model, delta_guide, optim.Adam({'lr': 0.05}), loss=Trace_ELBO(), num_steps=0, num_samples=3000)\n    for i in range(100):\n        svi.step()\n    svi.guide = delta_guide.laplace_approximation()\n    posterior = svi.run()\n    ic = posterior.information_criterion()\n    assert_equal(ic['waic'], torch.tensor(-8.3), prec=0.2)\n    assert_equal(ic['p_waic'], torch.tensor(1.8), prec=0.2)",
            "@pytest.mark.filterwarnings('ignore::FutureWarning')\ndef test_information_criterion():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    kcal = torch.tensor([0.49, 0.47, 0.56, 0.89, 0.92, 0.8, 0.46, 0.71, 0.68, 0.97, 0.84, 0.62, 0.54, 0.49, 0.48, 0.55, 0.71])\n    kcal_mean = kcal.mean()\n    kcal_logstd = kcal.std().log()\n\n    def model():\n        mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n        log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n        with pyro.plate('plate'):\n            pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)\n    delta_guide = AutoLaplaceApproximation(model)\n    svi = SVI(model, delta_guide, optim.Adam({'lr': 0.05}), loss=Trace_ELBO(), num_steps=0, num_samples=3000)\n    for i in range(100):\n        svi.step()\n    svi.guide = delta_guide.laplace_approximation()\n    posterior = svi.run()\n    ic = posterior.information_criterion()\n    assert_equal(ic['waic'], torch.tensor(-8.3), prec=0.2)\n    assert_equal(ic['p_waic'], torch.tensor(1.8), prec=0.2)",
            "@pytest.mark.filterwarnings('ignore::FutureWarning')\ndef test_information_criterion():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    kcal = torch.tensor([0.49, 0.47, 0.56, 0.89, 0.92, 0.8, 0.46, 0.71, 0.68, 0.97, 0.84, 0.62, 0.54, 0.49, 0.48, 0.55, 0.71])\n    kcal_mean = kcal.mean()\n    kcal_logstd = kcal.std().log()\n\n    def model():\n        mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n        log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n        with pyro.plate('plate'):\n            pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)\n    delta_guide = AutoLaplaceApproximation(model)\n    svi = SVI(model, delta_guide, optim.Adam({'lr': 0.05}), loss=Trace_ELBO(), num_steps=0, num_samples=3000)\n    for i in range(100):\n        svi.step()\n    svi.guide = delta_guide.laplace_approximation()\n    posterior = svi.run()\n    ic = posterior.information_criterion()\n    assert_equal(ic['waic'], torch.tensor(-8.3), prec=0.2)\n    assert_equal(ic['p_waic'], torch.tensor(1.8), prec=0.2)",
            "@pytest.mark.filterwarnings('ignore::FutureWarning')\ndef test_information_criterion():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    kcal = torch.tensor([0.49, 0.47, 0.56, 0.89, 0.92, 0.8, 0.46, 0.71, 0.68, 0.97, 0.84, 0.62, 0.54, 0.49, 0.48, 0.55, 0.71])\n    kcal_mean = kcal.mean()\n    kcal_logstd = kcal.std().log()\n\n    def model():\n        mu = pyro.sample('mu', dist.Normal(kcal_mean, 1))\n        log_sigma = pyro.sample('log_sigma', dist.Normal(kcal_logstd, 1))\n        with pyro.plate('plate'):\n            pyro.sample('kcal', dist.Normal(mu, log_sigma.exp()), obs=kcal)\n    delta_guide = AutoLaplaceApproximation(model)\n    svi = SVI(model, delta_guide, optim.Adam({'lr': 0.05}), loss=Trace_ELBO(), num_steps=0, num_samples=3000)\n    for i in range(100):\n        svi.step()\n    svi.guide = delta_guide.laplace_approximation()\n    posterior = svi.run()\n    ic = posterior.information_criterion()\n    assert_equal(ic['waic'], torch.tensor(-8.3), prec=0.2)\n    assert_equal(ic['p_waic'], torch.tensor(1.8), prec=0.2)"
        ]
    }
]