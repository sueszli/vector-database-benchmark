[
    {
        "func_name": "test_main",
        "original": "def test_main(self):\n    places = [core.CPUPlace()]\n    if core.is_compiled_with_cuda():\n        places.append(core.CUDAPlace(0))\n    for p in places:\n        with base.program_guard(base.Program(), base.Program()):\n            with base.scope_guard(base.Scope()):\n                self.run_main(p)",
        "mutated": [
            "def test_main(self):\n    if False:\n        i = 10\n    places = [core.CPUPlace()]\n    if core.is_compiled_with_cuda():\n        places.append(core.CUDAPlace(0))\n    for p in places:\n        with base.program_guard(base.Program(), base.Program()):\n            with base.scope_guard(base.Scope()):\n                self.run_main(p)",
            "def test_main(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    places = [core.CPUPlace()]\n    if core.is_compiled_with_cuda():\n        places.append(core.CUDAPlace(0))\n    for p in places:\n        with base.program_guard(base.Program(), base.Program()):\n            with base.scope_guard(base.Scope()):\n                self.run_main(p)",
            "def test_main(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    places = [core.CPUPlace()]\n    if core.is_compiled_with_cuda():\n        places.append(core.CUDAPlace(0))\n    for p in places:\n        with base.program_guard(base.Program(), base.Program()):\n            with base.scope_guard(base.Scope()):\n                self.run_main(p)",
            "def test_main(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    places = [core.CPUPlace()]\n    if core.is_compiled_with_cuda():\n        places.append(core.CUDAPlace(0))\n    for p in places:\n        with base.program_guard(base.Program(), base.Program()):\n            with base.scope_guard(base.Scope()):\n                self.run_main(p)",
            "def test_main(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    places = [core.CPUPlace()]\n    if core.is_compiled_with_cuda():\n        places.append(core.CUDAPlace(0))\n    for p in places:\n        with base.program_guard(base.Program(), base.Program()):\n            with base.scope_guard(base.Scope()):\n                self.run_main(p)"
        ]
    },
    {
        "func_name": "run_main",
        "original": "def run_main(self, place):\n    self.place = place\n    if not core.is_compiled_with_cuda() and isinstance(self.place, core.CUDAPlace):\n        return\n    device_cnt = 1\n    d0 = paddle.static.data('d0', shape=[-1, 10], dtype='float32')\n    d1 = paddle.static.data('d1', shape=[-1, 10], dtype='float32')\n    d2 = paddle.static.data('d2', shape=[-1, 10], dtype='float32')\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    init = paddle.zeros(shape=[10], dtype='float32')\n    mem_array = paddle.tensor.array_write(x=init, i=i)\n    data_array = paddle.tensor.array_write(x=d0, i=i)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d1, i, array=data_array)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d2, i, array=data_array)\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    array_len = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    array_len.stop_gradient = True\n    cond = paddle.less_than(x=i, y=array_len)\n    j = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    j.stop_gradient = True\n    array_len2 = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=3)\n    array_len2.stop_gradient = True\n    cond2 = paddle.less_than(x=j, y=array_len2)\n    while_op = paddle.static.nn.control_flow.While(cond=cond)\n    while_op2 = paddle.static.nn.control_flow.While(cond=cond2)\n    with while_op.block():\n        d = paddle.tensor.array_read(array=data_array, i=i)\n        prev = paddle.tensor.array_read(array=mem_array, i=i)\n        d = paddle.reshape(d, shape=[10])\n        prev = paddle.reshape(prev, shape=[10])\n        result = paddle.add_n([d, prev])\n        i = paddle.increment(x=i)\n        paddle.tensor.array_write(result, i=i, array=mem_array)\n        paddle.assign(paddle.less_than(x=i, y=array_len), cond)\n        with while_op2.block():\n            d2 = paddle.tensor.array_read(array=data_array, i=j)\n            prev2 = paddle.tensor.array_read(array=mem_array, i=j)\n            d2 = paddle.reshape(d2, shape=[10])\n            prev2 = paddle.reshape(prev2, shape=[10])\n            result2 = paddle.add_n([d2, prev2])\n            j = paddle.increment(x=j)\n            paddle.tensor.array_write(result2, i=j, array=mem_array)\n            paddle.assign(paddle.less_than(x=j, y=array_len2), cond2)\n    sum_result = paddle.tensor.array_read(array=mem_array, i=j)\n    sum_result.persistable = True\n    tmp = paddle.unsqueeze(sum_result, axis=[0])\n    tmp = paddle.expand(tmp, [10, -1])\n    fc = paddle.static.nn.fc(tmp, size=256)\n    loss = paddle.mean(sum_result)\n    optim = paddle.optimizer.Adam(learning_rate=0.001)\n    optim.minimize(loss)\n    gc_vars = core._get_eager_deletion_vars(base.default_main_program().desc, [loss.name])\n    self.assertEqual(len(gc_vars), 3)\n    exe = Executor(self.place)\n    exe.run(base.default_startup_program())\n    prog = base.default_main_program()\n    for _ in range(5):\n        d = []\n        for i in range(3):\n            tmp = numpy.random.random(size=[10]).astype('float32')\n            d.append(numpy.array([tmp] * device_cnt))\n        outs = exe.run(program=prog, feed={'d0': d[0], 'd1': d[1], 'd2': d[2]}, fetch_list=[sum_result])\n        self.assertAlmostEqual(numpy.sum(d), numpy.sum(outs[0]), delta=0.01)",
        "mutated": [
            "def run_main(self, place):\n    if False:\n        i = 10\n    self.place = place\n    if not core.is_compiled_with_cuda() and isinstance(self.place, core.CUDAPlace):\n        return\n    device_cnt = 1\n    d0 = paddle.static.data('d0', shape=[-1, 10], dtype='float32')\n    d1 = paddle.static.data('d1', shape=[-1, 10], dtype='float32')\n    d2 = paddle.static.data('d2', shape=[-1, 10], dtype='float32')\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    init = paddle.zeros(shape=[10], dtype='float32')\n    mem_array = paddle.tensor.array_write(x=init, i=i)\n    data_array = paddle.tensor.array_write(x=d0, i=i)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d1, i, array=data_array)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d2, i, array=data_array)\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    array_len = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    array_len.stop_gradient = True\n    cond = paddle.less_than(x=i, y=array_len)\n    j = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    j.stop_gradient = True\n    array_len2 = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=3)\n    array_len2.stop_gradient = True\n    cond2 = paddle.less_than(x=j, y=array_len2)\n    while_op = paddle.static.nn.control_flow.While(cond=cond)\n    while_op2 = paddle.static.nn.control_flow.While(cond=cond2)\n    with while_op.block():\n        d = paddle.tensor.array_read(array=data_array, i=i)\n        prev = paddle.tensor.array_read(array=mem_array, i=i)\n        d = paddle.reshape(d, shape=[10])\n        prev = paddle.reshape(prev, shape=[10])\n        result = paddle.add_n([d, prev])\n        i = paddle.increment(x=i)\n        paddle.tensor.array_write(result, i=i, array=mem_array)\n        paddle.assign(paddle.less_than(x=i, y=array_len), cond)\n        with while_op2.block():\n            d2 = paddle.tensor.array_read(array=data_array, i=j)\n            prev2 = paddle.tensor.array_read(array=mem_array, i=j)\n            d2 = paddle.reshape(d2, shape=[10])\n            prev2 = paddle.reshape(prev2, shape=[10])\n            result2 = paddle.add_n([d2, prev2])\n            j = paddle.increment(x=j)\n            paddle.tensor.array_write(result2, i=j, array=mem_array)\n            paddle.assign(paddle.less_than(x=j, y=array_len2), cond2)\n    sum_result = paddle.tensor.array_read(array=mem_array, i=j)\n    sum_result.persistable = True\n    tmp = paddle.unsqueeze(sum_result, axis=[0])\n    tmp = paddle.expand(tmp, [10, -1])\n    fc = paddle.static.nn.fc(tmp, size=256)\n    loss = paddle.mean(sum_result)\n    optim = paddle.optimizer.Adam(learning_rate=0.001)\n    optim.minimize(loss)\n    gc_vars = core._get_eager_deletion_vars(base.default_main_program().desc, [loss.name])\n    self.assertEqual(len(gc_vars), 3)\n    exe = Executor(self.place)\n    exe.run(base.default_startup_program())\n    prog = base.default_main_program()\n    for _ in range(5):\n        d = []\n        for i in range(3):\n            tmp = numpy.random.random(size=[10]).astype('float32')\n            d.append(numpy.array([tmp] * device_cnt))\n        outs = exe.run(program=prog, feed={'d0': d[0], 'd1': d[1], 'd2': d[2]}, fetch_list=[sum_result])\n        self.assertAlmostEqual(numpy.sum(d), numpy.sum(outs[0]), delta=0.01)",
            "def run_main(self, place):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.place = place\n    if not core.is_compiled_with_cuda() and isinstance(self.place, core.CUDAPlace):\n        return\n    device_cnt = 1\n    d0 = paddle.static.data('d0', shape=[-1, 10], dtype='float32')\n    d1 = paddle.static.data('d1', shape=[-1, 10], dtype='float32')\n    d2 = paddle.static.data('d2', shape=[-1, 10], dtype='float32')\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    init = paddle.zeros(shape=[10], dtype='float32')\n    mem_array = paddle.tensor.array_write(x=init, i=i)\n    data_array = paddle.tensor.array_write(x=d0, i=i)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d1, i, array=data_array)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d2, i, array=data_array)\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    array_len = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    array_len.stop_gradient = True\n    cond = paddle.less_than(x=i, y=array_len)\n    j = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    j.stop_gradient = True\n    array_len2 = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=3)\n    array_len2.stop_gradient = True\n    cond2 = paddle.less_than(x=j, y=array_len2)\n    while_op = paddle.static.nn.control_flow.While(cond=cond)\n    while_op2 = paddle.static.nn.control_flow.While(cond=cond2)\n    with while_op.block():\n        d = paddle.tensor.array_read(array=data_array, i=i)\n        prev = paddle.tensor.array_read(array=mem_array, i=i)\n        d = paddle.reshape(d, shape=[10])\n        prev = paddle.reshape(prev, shape=[10])\n        result = paddle.add_n([d, prev])\n        i = paddle.increment(x=i)\n        paddle.tensor.array_write(result, i=i, array=mem_array)\n        paddle.assign(paddle.less_than(x=i, y=array_len), cond)\n        with while_op2.block():\n            d2 = paddle.tensor.array_read(array=data_array, i=j)\n            prev2 = paddle.tensor.array_read(array=mem_array, i=j)\n            d2 = paddle.reshape(d2, shape=[10])\n            prev2 = paddle.reshape(prev2, shape=[10])\n            result2 = paddle.add_n([d2, prev2])\n            j = paddle.increment(x=j)\n            paddle.tensor.array_write(result2, i=j, array=mem_array)\n            paddle.assign(paddle.less_than(x=j, y=array_len2), cond2)\n    sum_result = paddle.tensor.array_read(array=mem_array, i=j)\n    sum_result.persistable = True\n    tmp = paddle.unsqueeze(sum_result, axis=[0])\n    tmp = paddle.expand(tmp, [10, -1])\n    fc = paddle.static.nn.fc(tmp, size=256)\n    loss = paddle.mean(sum_result)\n    optim = paddle.optimizer.Adam(learning_rate=0.001)\n    optim.minimize(loss)\n    gc_vars = core._get_eager_deletion_vars(base.default_main_program().desc, [loss.name])\n    self.assertEqual(len(gc_vars), 3)\n    exe = Executor(self.place)\n    exe.run(base.default_startup_program())\n    prog = base.default_main_program()\n    for _ in range(5):\n        d = []\n        for i in range(3):\n            tmp = numpy.random.random(size=[10]).astype('float32')\n            d.append(numpy.array([tmp] * device_cnt))\n        outs = exe.run(program=prog, feed={'d0': d[0], 'd1': d[1], 'd2': d[2]}, fetch_list=[sum_result])\n        self.assertAlmostEqual(numpy.sum(d), numpy.sum(outs[0]), delta=0.01)",
            "def run_main(self, place):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.place = place\n    if not core.is_compiled_with_cuda() and isinstance(self.place, core.CUDAPlace):\n        return\n    device_cnt = 1\n    d0 = paddle.static.data('d0', shape=[-1, 10], dtype='float32')\n    d1 = paddle.static.data('d1', shape=[-1, 10], dtype='float32')\n    d2 = paddle.static.data('d2', shape=[-1, 10], dtype='float32')\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    init = paddle.zeros(shape=[10], dtype='float32')\n    mem_array = paddle.tensor.array_write(x=init, i=i)\n    data_array = paddle.tensor.array_write(x=d0, i=i)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d1, i, array=data_array)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d2, i, array=data_array)\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    array_len = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    array_len.stop_gradient = True\n    cond = paddle.less_than(x=i, y=array_len)\n    j = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    j.stop_gradient = True\n    array_len2 = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=3)\n    array_len2.stop_gradient = True\n    cond2 = paddle.less_than(x=j, y=array_len2)\n    while_op = paddle.static.nn.control_flow.While(cond=cond)\n    while_op2 = paddle.static.nn.control_flow.While(cond=cond2)\n    with while_op.block():\n        d = paddle.tensor.array_read(array=data_array, i=i)\n        prev = paddle.tensor.array_read(array=mem_array, i=i)\n        d = paddle.reshape(d, shape=[10])\n        prev = paddle.reshape(prev, shape=[10])\n        result = paddle.add_n([d, prev])\n        i = paddle.increment(x=i)\n        paddle.tensor.array_write(result, i=i, array=mem_array)\n        paddle.assign(paddle.less_than(x=i, y=array_len), cond)\n        with while_op2.block():\n            d2 = paddle.tensor.array_read(array=data_array, i=j)\n            prev2 = paddle.tensor.array_read(array=mem_array, i=j)\n            d2 = paddle.reshape(d2, shape=[10])\n            prev2 = paddle.reshape(prev2, shape=[10])\n            result2 = paddle.add_n([d2, prev2])\n            j = paddle.increment(x=j)\n            paddle.tensor.array_write(result2, i=j, array=mem_array)\n            paddle.assign(paddle.less_than(x=j, y=array_len2), cond2)\n    sum_result = paddle.tensor.array_read(array=mem_array, i=j)\n    sum_result.persistable = True\n    tmp = paddle.unsqueeze(sum_result, axis=[0])\n    tmp = paddle.expand(tmp, [10, -1])\n    fc = paddle.static.nn.fc(tmp, size=256)\n    loss = paddle.mean(sum_result)\n    optim = paddle.optimizer.Adam(learning_rate=0.001)\n    optim.minimize(loss)\n    gc_vars = core._get_eager_deletion_vars(base.default_main_program().desc, [loss.name])\n    self.assertEqual(len(gc_vars), 3)\n    exe = Executor(self.place)\n    exe.run(base.default_startup_program())\n    prog = base.default_main_program()\n    for _ in range(5):\n        d = []\n        for i in range(3):\n            tmp = numpy.random.random(size=[10]).astype('float32')\n            d.append(numpy.array([tmp] * device_cnt))\n        outs = exe.run(program=prog, feed={'d0': d[0], 'd1': d[1], 'd2': d[2]}, fetch_list=[sum_result])\n        self.assertAlmostEqual(numpy.sum(d), numpy.sum(outs[0]), delta=0.01)",
            "def run_main(self, place):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.place = place\n    if not core.is_compiled_with_cuda() and isinstance(self.place, core.CUDAPlace):\n        return\n    device_cnt = 1\n    d0 = paddle.static.data('d0', shape=[-1, 10], dtype='float32')\n    d1 = paddle.static.data('d1', shape=[-1, 10], dtype='float32')\n    d2 = paddle.static.data('d2', shape=[-1, 10], dtype='float32')\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    init = paddle.zeros(shape=[10], dtype='float32')\n    mem_array = paddle.tensor.array_write(x=init, i=i)\n    data_array = paddle.tensor.array_write(x=d0, i=i)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d1, i, array=data_array)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d2, i, array=data_array)\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    array_len = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    array_len.stop_gradient = True\n    cond = paddle.less_than(x=i, y=array_len)\n    j = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    j.stop_gradient = True\n    array_len2 = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=3)\n    array_len2.stop_gradient = True\n    cond2 = paddle.less_than(x=j, y=array_len2)\n    while_op = paddle.static.nn.control_flow.While(cond=cond)\n    while_op2 = paddle.static.nn.control_flow.While(cond=cond2)\n    with while_op.block():\n        d = paddle.tensor.array_read(array=data_array, i=i)\n        prev = paddle.tensor.array_read(array=mem_array, i=i)\n        d = paddle.reshape(d, shape=[10])\n        prev = paddle.reshape(prev, shape=[10])\n        result = paddle.add_n([d, prev])\n        i = paddle.increment(x=i)\n        paddle.tensor.array_write(result, i=i, array=mem_array)\n        paddle.assign(paddle.less_than(x=i, y=array_len), cond)\n        with while_op2.block():\n            d2 = paddle.tensor.array_read(array=data_array, i=j)\n            prev2 = paddle.tensor.array_read(array=mem_array, i=j)\n            d2 = paddle.reshape(d2, shape=[10])\n            prev2 = paddle.reshape(prev2, shape=[10])\n            result2 = paddle.add_n([d2, prev2])\n            j = paddle.increment(x=j)\n            paddle.tensor.array_write(result2, i=j, array=mem_array)\n            paddle.assign(paddle.less_than(x=j, y=array_len2), cond2)\n    sum_result = paddle.tensor.array_read(array=mem_array, i=j)\n    sum_result.persistable = True\n    tmp = paddle.unsqueeze(sum_result, axis=[0])\n    tmp = paddle.expand(tmp, [10, -1])\n    fc = paddle.static.nn.fc(tmp, size=256)\n    loss = paddle.mean(sum_result)\n    optim = paddle.optimizer.Adam(learning_rate=0.001)\n    optim.minimize(loss)\n    gc_vars = core._get_eager_deletion_vars(base.default_main_program().desc, [loss.name])\n    self.assertEqual(len(gc_vars), 3)\n    exe = Executor(self.place)\n    exe.run(base.default_startup_program())\n    prog = base.default_main_program()\n    for _ in range(5):\n        d = []\n        for i in range(3):\n            tmp = numpy.random.random(size=[10]).astype('float32')\n            d.append(numpy.array([tmp] * device_cnt))\n        outs = exe.run(program=prog, feed={'d0': d[0], 'd1': d[1], 'd2': d[2]}, fetch_list=[sum_result])\n        self.assertAlmostEqual(numpy.sum(d), numpy.sum(outs[0]), delta=0.01)",
            "def run_main(self, place):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.place = place\n    if not core.is_compiled_with_cuda() and isinstance(self.place, core.CUDAPlace):\n        return\n    device_cnt = 1\n    d0 = paddle.static.data('d0', shape=[-1, 10], dtype='float32')\n    d1 = paddle.static.data('d1', shape=[-1, 10], dtype='float32')\n    d2 = paddle.static.data('d2', shape=[-1, 10], dtype='float32')\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    init = paddle.zeros(shape=[10], dtype='float32')\n    mem_array = paddle.tensor.array_write(x=init, i=i)\n    data_array = paddle.tensor.array_write(x=d0, i=i)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d1, i, array=data_array)\n    i = paddle.increment(i)\n    paddle.tensor.array_write(d2, i, array=data_array)\n    i = paddle.zeros(shape=[1], dtype='int64')\n    i.stop_gradient = True\n    array_len = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    array_len.stop_gradient = True\n    cond = paddle.less_than(x=i, y=array_len)\n    j = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=1)\n    j.stop_gradient = True\n    array_len2 = paddle.tensor.fill_constant(shape=[1], dtype='int64', value=3)\n    array_len2.stop_gradient = True\n    cond2 = paddle.less_than(x=j, y=array_len2)\n    while_op = paddle.static.nn.control_flow.While(cond=cond)\n    while_op2 = paddle.static.nn.control_flow.While(cond=cond2)\n    with while_op.block():\n        d = paddle.tensor.array_read(array=data_array, i=i)\n        prev = paddle.tensor.array_read(array=mem_array, i=i)\n        d = paddle.reshape(d, shape=[10])\n        prev = paddle.reshape(prev, shape=[10])\n        result = paddle.add_n([d, prev])\n        i = paddle.increment(x=i)\n        paddle.tensor.array_write(result, i=i, array=mem_array)\n        paddle.assign(paddle.less_than(x=i, y=array_len), cond)\n        with while_op2.block():\n            d2 = paddle.tensor.array_read(array=data_array, i=j)\n            prev2 = paddle.tensor.array_read(array=mem_array, i=j)\n            d2 = paddle.reshape(d2, shape=[10])\n            prev2 = paddle.reshape(prev2, shape=[10])\n            result2 = paddle.add_n([d2, prev2])\n            j = paddle.increment(x=j)\n            paddle.tensor.array_write(result2, i=j, array=mem_array)\n            paddle.assign(paddle.less_than(x=j, y=array_len2), cond2)\n    sum_result = paddle.tensor.array_read(array=mem_array, i=j)\n    sum_result.persistable = True\n    tmp = paddle.unsqueeze(sum_result, axis=[0])\n    tmp = paddle.expand(tmp, [10, -1])\n    fc = paddle.static.nn.fc(tmp, size=256)\n    loss = paddle.mean(sum_result)\n    optim = paddle.optimizer.Adam(learning_rate=0.001)\n    optim.minimize(loss)\n    gc_vars = core._get_eager_deletion_vars(base.default_main_program().desc, [loss.name])\n    self.assertEqual(len(gc_vars), 3)\n    exe = Executor(self.place)\n    exe.run(base.default_startup_program())\n    prog = base.default_main_program()\n    for _ in range(5):\n        d = []\n        for i in range(3):\n            tmp = numpy.random.random(size=[10]).astype('float32')\n            d.append(numpy.array([tmp] * device_cnt))\n        outs = exe.run(program=prog, feed={'d0': d[0], 'd1': d[1], 'd2': d[2]}, fetch_list=[sum_result])\n        self.assertAlmostEqual(numpy.sum(d), numpy.sum(outs[0]), delta=0.01)"
        ]
    }
]