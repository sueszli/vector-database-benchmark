[
    {
        "func_name": "average_checkpoints",
        "original": "def average_checkpoints(inputs):\n    \"\"\"Loads checkpoints from inputs and returns a model with averaged weights.\n\n    Args:\n      inputs: An iterable of string paths of checkpoints to load from.\n\n    Returns:\n      A dict of string keys mapping to various values. The 'model' key\n      from the returned dict should correspond to an OrderedDict mapping\n      string parameter names to torch Tensors.\n    \"\"\"\n    params_dict = collections.OrderedDict()\n    params_keys = None\n    new_state = None\n    num_models = len(inputs)\n    for fpath in inputs:\n        with PathManager.open(fpath, 'rb') as f:\n            state = torch.load(f, map_location=lambda s, _: torch.serialization.default_restore_location(s, 'cpu'))\n        if new_state is None:\n            new_state = state\n        model_params = state['model']\n        model_params_keys = list(model_params.keys())\n        if params_keys is None:\n            params_keys = model_params_keys\n        elif params_keys != model_params_keys:\n            raise KeyError('For checkpoint {}, expected list of params: {}, but found: {}'.format(f, params_keys, model_params_keys))\n        for k in params_keys:\n            p = model_params[k]\n            if isinstance(p, torch.HalfTensor):\n                p = p.float()\n            if k not in params_dict:\n                params_dict[k] = p.clone()\n            else:\n                params_dict[k] += p\n    averaged_params = collections.OrderedDict()\n    for (k, v) in params_dict.items():\n        averaged_params[k] = v\n        if averaged_params[k].is_floating_point():\n            averaged_params[k].div_(num_models)\n        else:\n            averaged_params[k] //= num_models\n    new_state['model'] = averaged_params\n    return new_state",
        "mutated": [
            "def average_checkpoints(inputs):\n    if False:\n        i = 10\n    \"Loads checkpoints from inputs and returns a model with averaged weights.\\n\\n    Args:\\n      inputs: An iterable of string paths of checkpoints to load from.\\n\\n    Returns:\\n      A dict of string keys mapping to various values. The 'model' key\\n      from the returned dict should correspond to an OrderedDict mapping\\n      string parameter names to torch Tensors.\\n    \"\n    params_dict = collections.OrderedDict()\n    params_keys = None\n    new_state = None\n    num_models = len(inputs)\n    for fpath in inputs:\n        with PathManager.open(fpath, 'rb') as f:\n            state = torch.load(f, map_location=lambda s, _: torch.serialization.default_restore_location(s, 'cpu'))\n        if new_state is None:\n            new_state = state\n        model_params = state['model']\n        model_params_keys = list(model_params.keys())\n        if params_keys is None:\n            params_keys = model_params_keys\n        elif params_keys != model_params_keys:\n            raise KeyError('For checkpoint {}, expected list of params: {}, but found: {}'.format(f, params_keys, model_params_keys))\n        for k in params_keys:\n            p = model_params[k]\n            if isinstance(p, torch.HalfTensor):\n                p = p.float()\n            if k not in params_dict:\n                params_dict[k] = p.clone()\n            else:\n                params_dict[k] += p\n    averaged_params = collections.OrderedDict()\n    for (k, v) in params_dict.items():\n        averaged_params[k] = v\n        if averaged_params[k].is_floating_point():\n            averaged_params[k].div_(num_models)\n        else:\n            averaged_params[k] //= num_models\n    new_state['model'] = averaged_params\n    return new_state",
            "def average_checkpoints(inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Loads checkpoints from inputs and returns a model with averaged weights.\\n\\n    Args:\\n      inputs: An iterable of string paths of checkpoints to load from.\\n\\n    Returns:\\n      A dict of string keys mapping to various values. The 'model' key\\n      from the returned dict should correspond to an OrderedDict mapping\\n      string parameter names to torch Tensors.\\n    \"\n    params_dict = collections.OrderedDict()\n    params_keys = None\n    new_state = None\n    num_models = len(inputs)\n    for fpath in inputs:\n        with PathManager.open(fpath, 'rb') as f:\n            state = torch.load(f, map_location=lambda s, _: torch.serialization.default_restore_location(s, 'cpu'))\n        if new_state is None:\n            new_state = state\n        model_params = state['model']\n        model_params_keys = list(model_params.keys())\n        if params_keys is None:\n            params_keys = model_params_keys\n        elif params_keys != model_params_keys:\n            raise KeyError('For checkpoint {}, expected list of params: {}, but found: {}'.format(f, params_keys, model_params_keys))\n        for k in params_keys:\n            p = model_params[k]\n            if isinstance(p, torch.HalfTensor):\n                p = p.float()\n            if k not in params_dict:\n                params_dict[k] = p.clone()\n            else:\n                params_dict[k] += p\n    averaged_params = collections.OrderedDict()\n    for (k, v) in params_dict.items():\n        averaged_params[k] = v\n        if averaged_params[k].is_floating_point():\n            averaged_params[k].div_(num_models)\n        else:\n            averaged_params[k] //= num_models\n    new_state['model'] = averaged_params\n    return new_state",
            "def average_checkpoints(inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Loads checkpoints from inputs and returns a model with averaged weights.\\n\\n    Args:\\n      inputs: An iterable of string paths of checkpoints to load from.\\n\\n    Returns:\\n      A dict of string keys mapping to various values. The 'model' key\\n      from the returned dict should correspond to an OrderedDict mapping\\n      string parameter names to torch Tensors.\\n    \"\n    params_dict = collections.OrderedDict()\n    params_keys = None\n    new_state = None\n    num_models = len(inputs)\n    for fpath in inputs:\n        with PathManager.open(fpath, 'rb') as f:\n            state = torch.load(f, map_location=lambda s, _: torch.serialization.default_restore_location(s, 'cpu'))\n        if new_state is None:\n            new_state = state\n        model_params = state['model']\n        model_params_keys = list(model_params.keys())\n        if params_keys is None:\n            params_keys = model_params_keys\n        elif params_keys != model_params_keys:\n            raise KeyError('For checkpoint {}, expected list of params: {}, but found: {}'.format(f, params_keys, model_params_keys))\n        for k in params_keys:\n            p = model_params[k]\n            if isinstance(p, torch.HalfTensor):\n                p = p.float()\n            if k not in params_dict:\n                params_dict[k] = p.clone()\n            else:\n                params_dict[k] += p\n    averaged_params = collections.OrderedDict()\n    for (k, v) in params_dict.items():\n        averaged_params[k] = v\n        if averaged_params[k].is_floating_point():\n            averaged_params[k].div_(num_models)\n        else:\n            averaged_params[k] //= num_models\n    new_state['model'] = averaged_params\n    return new_state",
            "def average_checkpoints(inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Loads checkpoints from inputs and returns a model with averaged weights.\\n\\n    Args:\\n      inputs: An iterable of string paths of checkpoints to load from.\\n\\n    Returns:\\n      A dict of string keys mapping to various values. The 'model' key\\n      from the returned dict should correspond to an OrderedDict mapping\\n      string parameter names to torch Tensors.\\n    \"\n    params_dict = collections.OrderedDict()\n    params_keys = None\n    new_state = None\n    num_models = len(inputs)\n    for fpath in inputs:\n        with PathManager.open(fpath, 'rb') as f:\n            state = torch.load(f, map_location=lambda s, _: torch.serialization.default_restore_location(s, 'cpu'))\n        if new_state is None:\n            new_state = state\n        model_params = state['model']\n        model_params_keys = list(model_params.keys())\n        if params_keys is None:\n            params_keys = model_params_keys\n        elif params_keys != model_params_keys:\n            raise KeyError('For checkpoint {}, expected list of params: {}, but found: {}'.format(f, params_keys, model_params_keys))\n        for k in params_keys:\n            p = model_params[k]\n            if isinstance(p, torch.HalfTensor):\n                p = p.float()\n            if k not in params_dict:\n                params_dict[k] = p.clone()\n            else:\n                params_dict[k] += p\n    averaged_params = collections.OrderedDict()\n    for (k, v) in params_dict.items():\n        averaged_params[k] = v\n        if averaged_params[k].is_floating_point():\n            averaged_params[k].div_(num_models)\n        else:\n            averaged_params[k] //= num_models\n    new_state['model'] = averaged_params\n    return new_state",
            "def average_checkpoints(inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Loads checkpoints from inputs and returns a model with averaged weights.\\n\\n    Args:\\n      inputs: An iterable of string paths of checkpoints to load from.\\n\\n    Returns:\\n      A dict of string keys mapping to various values. The 'model' key\\n      from the returned dict should correspond to an OrderedDict mapping\\n      string parameter names to torch Tensors.\\n    \"\n    params_dict = collections.OrderedDict()\n    params_keys = None\n    new_state = None\n    num_models = len(inputs)\n    for fpath in inputs:\n        with PathManager.open(fpath, 'rb') as f:\n            state = torch.load(f, map_location=lambda s, _: torch.serialization.default_restore_location(s, 'cpu'))\n        if new_state is None:\n            new_state = state\n        model_params = state['model']\n        model_params_keys = list(model_params.keys())\n        if params_keys is None:\n            params_keys = model_params_keys\n        elif params_keys != model_params_keys:\n            raise KeyError('For checkpoint {}, expected list of params: {}, but found: {}'.format(f, params_keys, model_params_keys))\n        for k in params_keys:\n            p = model_params[k]\n            if isinstance(p, torch.HalfTensor):\n                p = p.float()\n            if k not in params_dict:\n                params_dict[k] = p.clone()\n            else:\n                params_dict[k] += p\n    averaged_params = collections.OrderedDict()\n    for (k, v) in params_dict.items():\n        averaged_params[k] = v\n        if averaged_params[k].is_floating_point():\n            averaged_params[k].div_(num_models)\n        else:\n            averaged_params[k] //= num_models\n    new_state['model'] = averaged_params\n    return new_state"
        ]
    },
    {
        "func_name": "last_n_checkpoints",
        "original": "def last_n_checkpoints(paths, n, update_based, upper_bound=None):\n    assert len(paths) == 1\n    path = paths[0]\n    if update_based:\n        pt_regexp = re.compile('checkpoint_\\\\d+_(\\\\d+)\\\\.pt')\n    else:\n        pt_regexp = re.compile('checkpoint(\\\\d+)\\\\.pt')\n    files = PathManager.ls(path)\n    entries = []\n    for f in files:\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            sort_key = int(m.group(1))\n            if upper_bound is None or sort_key <= upper_bound:\n                entries.append((sort_key, m.group(0)))\n    if len(entries) < n:\n        raise Exception('Found {} checkpoint files but need at least {}', len(entries), n)\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]",
        "mutated": [
            "def last_n_checkpoints(paths, n, update_based, upper_bound=None):\n    if False:\n        i = 10\n    assert len(paths) == 1\n    path = paths[0]\n    if update_based:\n        pt_regexp = re.compile('checkpoint_\\\\d+_(\\\\d+)\\\\.pt')\n    else:\n        pt_regexp = re.compile('checkpoint(\\\\d+)\\\\.pt')\n    files = PathManager.ls(path)\n    entries = []\n    for f in files:\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            sort_key = int(m.group(1))\n            if upper_bound is None or sort_key <= upper_bound:\n                entries.append((sort_key, m.group(0)))\n    if len(entries) < n:\n        raise Exception('Found {} checkpoint files but need at least {}', len(entries), n)\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]",
            "def last_n_checkpoints(paths, n, update_based, upper_bound=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert len(paths) == 1\n    path = paths[0]\n    if update_based:\n        pt_regexp = re.compile('checkpoint_\\\\d+_(\\\\d+)\\\\.pt')\n    else:\n        pt_regexp = re.compile('checkpoint(\\\\d+)\\\\.pt')\n    files = PathManager.ls(path)\n    entries = []\n    for f in files:\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            sort_key = int(m.group(1))\n            if upper_bound is None or sort_key <= upper_bound:\n                entries.append((sort_key, m.group(0)))\n    if len(entries) < n:\n        raise Exception('Found {} checkpoint files but need at least {}', len(entries), n)\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]",
            "def last_n_checkpoints(paths, n, update_based, upper_bound=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert len(paths) == 1\n    path = paths[0]\n    if update_based:\n        pt_regexp = re.compile('checkpoint_\\\\d+_(\\\\d+)\\\\.pt')\n    else:\n        pt_regexp = re.compile('checkpoint(\\\\d+)\\\\.pt')\n    files = PathManager.ls(path)\n    entries = []\n    for f in files:\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            sort_key = int(m.group(1))\n            if upper_bound is None or sort_key <= upper_bound:\n                entries.append((sort_key, m.group(0)))\n    if len(entries) < n:\n        raise Exception('Found {} checkpoint files but need at least {}', len(entries), n)\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]",
            "def last_n_checkpoints(paths, n, update_based, upper_bound=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert len(paths) == 1\n    path = paths[0]\n    if update_based:\n        pt_regexp = re.compile('checkpoint_\\\\d+_(\\\\d+)\\\\.pt')\n    else:\n        pt_regexp = re.compile('checkpoint(\\\\d+)\\\\.pt')\n    files = PathManager.ls(path)\n    entries = []\n    for f in files:\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            sort_key = int(m.group(1))\n            if upper_bound is None or sort_key <= upper_bound:\n                entries.append((sort_key, m.group(0)))\n    if len(entries) < n:\n        raise Exception('Found {} checkpoint files but need at least {}', len(entries), n)\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]",
            "def last_n_checkpoints(paths, n, update_based, upper_bound=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert len(paths) == 1\n    path = paths[0]\n    if update_based:\n        pt_regexp = re.compile('checkpoint_\\\\d+_(\\\\d+)\\\\.pt')\n    else:\n        pt_regexp = re.compile('checkpoint(\\\\d+)\\\\.pt')\n    files = PathManager.ls(path)\n    entries = []\n    for f in files:\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            sort_key = int(m.group(1))\n            if upper_bound is None or sort_key <= upper_bound:\n                entries.append((sort_key, m.group(0)))\n    if len(entries) < n:\n        raise Exception('Found {} checkpoint files but need at least {}', len(entries), n)\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser(description='Tool to average the params of input checkpoints to produce a new checkpoint')\n    parser.add_argument('--inputs', required=True, nargs='+', help='Input checkpoint file paths.')\n    parser.add_argument('--output', required=True, metavar='FILE', help='Write the new checkpoint containing the averaged weights to this path.')\n    num_group = parser.add_mutually_exclusive_group()\n    num_group.add_argument('--num-epoch-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-update-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-best-checkpoints', type=int, default=0, help='if set, will try to find checkpoints with names checkpoint_best_ee_xx.pt in the path specified by input, and average last this many of them.')\n    parser.add_argument('--checkpoint-upper-bound', type=int, help='when using --num-epoch-checkpoints, this will set an upper bound on which epoch to use, when using --num-update-checkpoints, this will set an upper bound on which update to usee.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.e.g., with --num-update-checkpoints=10 --checkpoint-upper-bound=50000, checkpoints 40500-50000 would be averaged assuming --save-interval-updates 500')\n    args = parser.parse_args()\n    print(args)\n    num = None\n    is_update_based = False\n    if args.num_update_checkpoints is not None:\n        num = args.num_update_checkpoints\n        is_update_based = True\n    elif args.num_epoch_checkpoints is not None:\n        num = args.num_epoch_checkpoints\n    assert args.checkpoint_upper_bound is None or (args.num_epoch_checkpoints is not None or args.num_update_checkpoints is not None), '--checkpoint-upper-bound requires --num-epoch-checkpoints or --num-update-checkpoints'\n    assert args.num_epoch_checkpoints is None or args.num_update_checkpoints is None, 'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'\n    if num is not None:\n        args.inputs = last_n_checkpoints(args.inputs, num, is_update_based, upper_bound=args.checkpoint_upper_bound)\n        print('averaging checkpoints: ', args.inputs)\n    if args.num_best_checkpoints > 0:\n        args.inputs = list(sorted(args.inputs, key=lambda x: float(os.path.basename(x).split('_')[-1].replace('.pt', ''))))\n        args.inputs = args.inputs[:args.num_best_checkpoints]\n        for path in args.inputs:\n            print(os.path.basename(path))\n    new_state = average_checkpoints(args.inputs)\n    with PathManager.open(args.output, 'wb') as f:\n        torch.save(new_state, f)\n    print('Finished writing averaged checkpoint to {}'.format(args.output))",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser(description='Tool to average the params of input checkpoints to produce a new checkpoint')\n    parser.add_argument('--inputs', required=True, nargs='+', help='Input checkpoint file paths.')\n    parser.add_argument('--output', required=True, metavar='FILE', help='Write the new checkpoint containing the averaged weights to this path.')\n    num_group = parser.add_mutually_exclusive_group()\n    num_group.add_argument('--num-epoch-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-update-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-best-checkpoints', type=int, default=0, help='if set, will try to find checkpoints with names checkpoint_best_ee_xx.pt in the path specified by input, and average last this many of them.')\n    parser.add_argument('--checkpoint-upper-bound', type=int, help='when using --num-epoch-checkpoints, this will set an upper bound on which epoch to use, when using --num-update-checkpoints, this will set an upper bound on which update to usee.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.e.g., with --num-update-checkpoints=10 --checkpoint-upper-bound=50000, checkpoints 40500-50000 would be averaged assuming --save-interval-updates 500')\n    args = parser.parse_args()\n    print(args)\n    num = None\n    is_update_based = False\n    if args.num_update_checkpoints is not None:\n        num = args.num_update_checkpoints\n        is_update_based = True\n    elif args.num_epoch_checkpoints is not None:\n        num = args.num_epoch_checkpoints\n    assert args.checkpoint_upper_bound is None or (args.num_epoch_checkpoints is not None or args.num_update_checkpoints is not None), '--checkpoint-upper-bound requires --num-epoch-checkpoints or --num-update-checkpoints'\n    assert args.num_epoch_checkpoints is None or args.num_update_checkpoints is None, 'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'\n    if num is not None:\n        args.inputs = last_n_checkpoints(args.inputs, num, is_update_based, upper_bound=args.checkpoint_upper_bound)\n        print('averaging checkpoints: ', args.inputs)\n    if args.num_best_checkpoints > 0:\n        args.inputs = list(sorted(args.inputs, key=lambda x: float(os.path.basename(x).split('_')[-1].replace('.pt', ''))))\n        args.inputs = args.inputs[:args.num_best_checkpoints]\n        for path in args.inputs:\n            print(os.path.basename(path))\n    new_state = average_checkpoints(args.inputs)\n    with PathManager.open(args.output, 'wb') as f:\n        torch.save(new_state, f)\n    print('Finished writing averaged checkpoint to {}'.format(args.output))",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser(description='Tool to average the params of input checkpoints to produce a new checkpoint')\n    parser.add_argument('--inputs', required=True, nargs='+', help='Input checkpoint file paths.')\n    parser.add_argument('--output', required=True, metavar='FILE', help='Write the new checkpoint containing the averaged weights to this path.')\n    num_group = parser.add_mutually_exclusive_group()\n    num_group.add_argument('--num-epoch-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-update-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-best-checkpoints', type=int, default=0, help='if set, will try to find checkpoints with names checkpoint_best_ee_xx.pt in the path specified by input, and average last this many of them.')\n    parser.add_argument('--checkpoint-upper-bound', type=int, help='when using --num-epoch-checkpoints, this will set an upper bound on which epoch to use, when using --num-update-checkpoints, this will set an upper bound on which update to usee.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.e.g., with --num-update-checkpoints=10 --checkpoint-upper-bound=50000, checkpoints 40500-50000 would be averaged assuming --save-interval-updates 500')\n    args = parser.parse_args()\n    print(args)\n    num = None\n    is_update_based = False\n    if args.num_update_checkpoints is not None:\n        num = args.num_update_checkpoints\n        is_update_based = True\n    elif args.num_epoch_checkpoints is not None:\n        num = args.num_epoch_checkpoints\n    assert args.checkpoint_upper_bound is None or (args.num_epoch_checkpoints is not None or args.num_update_checkpoints is not None), '--checkpoint-upper-bound requires --num-epoch-checkpoints or --num-update-checkpoints'\n    assert args.num_epoch_checkpoints is None or args.num_update_checkpoints is None, 'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'\n    if num is not None:\n        args.inputs = last_n_checkpoints(args.inputs, num, is_update_based, upper_bound=args.checkpoint_upper_bound)\n        print('averaging checkpoints: ', args.inputs)\n    if args.num_best_checkpoints > 0:\n        args.inputs = list(sorted(args.inputs, key=lambda x: float(os.path.basename(x).split('_')[-1].replace('.pt', ''))))\n        args.inputs = args.inputs[:args.num_best_checkpoints]\n        for path in args.inputs:\n            print(os.path.basename(path))\n    new_state = average_checkpoints(args.inputs)\n    with PathManager.open(args.output, 'wb') as f:\n        torch.save(new_state, f)\n    print('Finished writing averaged checkpoint to {}'.format(args.output))",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser(description='Tool to average the params of input checkpoints to produce a new checkpoint')\n    parser.add_argument('--inputs', required=True, nargs='+', help='Input checkpoint file paths.')\n    parser.add_argument('--output', required=True, metavar='FILE', help='Write the new checkpoint containing the averaged weights to this path.')\n    num_group = parser.add_mutually_exclusive_group()\n    num_group.add_argument('--num-epoch-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-update-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-best-checkpoints', type=int, default=0, help='if set, will try to find checkpoints with names checkpoint_best_ee_xx.pt in the path specified by input, and average last this many of them.')\n    parser.add_argument('--checkpoint-upper-bound', type=int, help='when using --num-epoch-checkpoints, this will set an upper bound on which epoch to use, when using --num-update-checkpoints, this will set an upper bound on which update to usee.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.e.g., with --num-update-checkpoints=10 --checkpoint-upper-bound=50000, checkpoints 40500-50000 would be averaged assuming --save-interval-updates 500')\n    args = parser.parse_args()\n    print(args)\n    num = None\n    is_update_based = False\n    if args.num_update_checkpoints is not None:\n        num = args.num_update_checkpoints\n        is_update_based = True\n    elif args.num_epoch_checkpoints is not None:\n        num = args.num_epoch_checkpoints\n    assert args.checkpoint_upper_bound is None or (args.num_epoch_checkpoints is not None or args.num_update_checkpoints is not None), '--checkpoint-upper-bound requires --num-epoch-checkpoints or --num-update-checkpoints'\n    assert args.num_epoch_checkpoints is None or args.num_update_checkpoints is None, 'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'\n    if num is not None:\n        args.inputs = last_n_checkpoints(args.inputs, num, is_update_based, upper_bound=args.checkpoint_upper_bound)\n        print('averaging checkpoints: ', args.inputs)\n    if args.num_best_checkpoints > 0:\n        args.inputs = list(sorted(args.inputs, key=lambda x: float(os.path.basename(x).split('_')[-1].replace('.pt', ''))))\n        args.inputs = args.inputs[:args.num_best_checkpoints]\n        for path in args.inputs:\n            print(os.path.basename(path))\n    new_state = average_checkpoints(args.inputs)\n    with PathManager.open(args.output, 'wb') as f:\n        torch.save(new_state, f)\n    print('Finished writing averaged checkpoint to {}'.format(args.output))",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser(description='Tool to average the params of input checkpoints to produce a new checkpoint')\n    parser.add_argument('--inputs', required=True, nargs='+', help='Input checkpoint file paths.')\n    parser.add_argument('--output', required=True, metavar='FILE', help='Write the new checkpoint containing the averaged weights to this path.')\n    num_group = parser.add_mutually_exclusive_group()\n    num_group.add_argument('--num-epoch-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-update-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-best-checkpoints', type=int, default=0, help='if set, will try to find checkpoints with names checkpoint_best_ee_xx.pt in the path specified by input, and average last this many of them.')\n    parser.add_argument('--checkpoint-upper-bound', type=int, help='when using --num-epoch-checkpoints, this will set an upper bound on which epoch to use, when using --num-update-checkpoints, this will set an upper bound on which update to usee.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.e.g., with --num-update-checkpoints=10 --checkpoint-upper-bound=50000, checkpoints 40500-50000 would be averaged assuming --save-interval-updates 500')\n    args = parser.parse_args()\n    print(args)\n    num = None\n    is_update_based = False\n    if args.num_update_checkpoints is not None:\n        num = args.num_update_checkpoints\n        is_update_based = True\n    elif args.num_epoch_checkpoints is not None:\n        num = args.num_epoch_checkpoints\n    assert args.checkpoint_upper_bound is None or (args.num_epoch_checkpoints is not None or args.num_update_checkpoints is not None), '--checkpoint-upper-bound requires --num-epoch-checkpoints or --num-update-checkpoints'\n    assert args.num_epoch_checkpoints is None or args.num_update_checkpoints is None, 'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'\n    if num is not None:\n        args.inputs = last_n_checkpoints(args.inputs, num, is_update_based, upper_bound=args.checkpoint_upper_bound)\n        print('averaging checkpoints: ', args.inputs)\n    if args.num_best_checkpoints > 0:\n        args.inputs = list(sorted(args.inputs, key=lambda x: float(os.path.basename(x).split('_')[-1].replace('.pt', ''))))\n        args.inputs = args.inputs[:args.num_best_checkpoints]\n        for path in args.inputs:\n            print(os.path.basename(path))\n    new_state = average_checkpoints(args.inputs)\n    with PathManager.open(args.output, 'wb') as f:\n        torch.save(new_state, f)\n    print('Finished writing averaged checkpoint to {}'.format(args.output))",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser(description='Tool to average the params of input checkpoints to produce a new checkpoint')\n    parser.add_argument('--inputs', required=True, nargs='+', help='Input checkpoint file paths.')\n    parser.add_argument('--output', required=True, metavar='FILE', help='Write the new checkpoint containing the averaged weights to this path.')\n    num_group = parser.add_mutually_exclusive_group()\n    num_group.add_argument('--num-epoch-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-update-checkpoints', type=int, help='if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, and average last this many of them.')\n    num_group.add_argument('--num-best-checkpoints', type=int, default=0, help='if set, will try to find checkpoints with names checkpoint_best_ee_xx.pt in the path specified by input, and average last this many of them.')\n    parser.add_argument('--checkpoint-upper-bound', type=int, help='when using --num-epoch-checkpoints, this will set an upper bound on which epoch to use, when using --num-update-checkpoints, this will set an upper bound on which update to usee.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.e.g., with --num-update-checkpoints=10 --checkpoint-upper-bound=50000, checkpoints 40500-50000 would be averaged assuming --save-interval-updates 500')\n    args = parser.parse_args()\n    print(args)\n    num = None\n    is_update_based = False\n    if args.num_update_checkpoints is not None:\n        num = args.num_update_checkpoints\n        is_update_based = True\n    elif args.num_epoch_checkpoints is not None:\n        num = args.num_epoch_checkpoints\n    assert args.checkpoint_upper_bound is None or (args.num_epoch_checkpoints is not None or args.num_update_checkpoints is not None), '--checkpoint-upper-bound requires --num-epoch-checkpoints or --num-update-checkpoints'\n    assert args.num_epoch_checkpoints is None or args.num_update_checkpoints is None, 'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'\n    if num is not None:\n        args.inputs = last_n_checkpoints(args.inputs, num, is_update_based, upper_bound=args.checkpoint_upper_bound)\n        print('averaging checkpoints: ', args.inputs)\n    if args.num_best_checkpoints > 0:\n        args.inputs = list(sorted(args.inputs, key=lambda x: float(os.path.basename(x).split('_')[-1].replace('.pt', ''))))\n        args.inputs = args.inputs[:args.num_best_checkpoints]\n        for path in args.inputs:\n            print(os.path.basename(path))\n    new_state = average_checkpoints(args.inputs)\n    with PathManager.open(args.output, 'wb') as f:\n        torch.save(new_state, f)\n    print('Finished writing averaged checkpoint to {}'.format(args.output))"
        ]
    }
]