[
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_path', type=str, default='saved_models/ner', help='Where to find NER models (dir or filename)')\n    parser.add_argument('--output_path', type=str, default='saved_models/shrunk', help='Where to write shrunk NER models (dir)')\n    parser.add_argument('--pretrain_path', type=str, default='saved_models/pretrain', help='Where to find pretrains (dir or filename)')\n    args = parser.parse_args()\n    if os.path.isdir(args.input_path):\n        ner_model_dir = args.input_path\n        ners = os.listdir(ner_model_dir)\n        if len(ners) == 0:\n            raise FileNotFoundError('No ner models found in {}'.format(args.input_path))\n    else:\n        if not os.path.isfile(args.input_path):\n            raise FileNotFoundError('No ner model found at path {}'.format(args.input_path))\n        (ner_model_dir, ners) = os.path.split(args.input_path)\n        ners = [ners]\n    if os.path.isdir(args.pretrain_path):\n        pt_model_dir = args.pretrain_path\n        pretrains = os.listdir(pt_model_dir)\n        lang_to_pretrain = defaultdict(list)\n        for pt in pretrains:\n            lang_to_pretrain[pt.split('_')[0]].append(pt)\n    else:\n        (pt_model_dir, pretrains) = os.path.split(pt_model_dir)\n        pretrains = [pretrains]\n        lang_to_pretrain = defaultdict(lambda : pretrains)\n    new_dir = args.output_path\n    os.makedirs(new_dir, exist_ok=True)\n    final_pretrains = []\n    missing_pretrains = []\n    no_finetune = []\n    for ner_model in ners:\n        ner_path = os.path.join(ner_model_dir, ner_model)\n        expected_ending = '_nertagger.pt'\n        if not ner_model.endswith(expected_ending):\n            raise ValueError('Unexpected name: {}'.format(ner_model))\n        short_name = ner_model[:-len(expected_ending)]\n        (lang, package) = short_name.split('_', maxsplit=1)\n        print('===============================================')\n        print('Processing lang %s package %s' % (lang, package))\n        pipe = Pipeline(lang, processors='tokenize,ner', tokenize_pretokenized=True, package={'ner': package}, ner_model_path=ner_path)\n        ner_processor = pipe.processors['ner']\n        print('Loaded NER processor: {}'.format(ner_processor))\n        trainer = ner_processor.trainers[0]\n        vocab = trainer.model.vocab\n        word_vocab = vocab['word']\n        num_vectors = trainer.model.word_emb.weight.shape[0]\n        lcode = lang_to_langcode(trainer.args['lang'])\n        if lang != lcode and (not (lcode == 'zh' and lang == 'zh-hans')):\n            raise ValueError('lang not as expected: {} vs {} ({})'.format(lang, trainer.args['lang'], lcode))\n        ner_pretrains = sorted(set(lang_to_pretrain[lang] + lang_to_pretrain[lcode]))\n        for pt_model in ner_pretrains:\n            pt_path = os.path.join(pt_model_dir, pt_model)\n            print('Attempting pretrain: {}'.format(pt_path))\n            pt = Pretrain(filename=pt_path)\n            print('  pretrain shape:               {}'.format(pt.emb.shape))\n            print('  embedding in ner model shape: {}'.format(trainer.model.word_emb.weight.shape))\n            if pt.emb.shape[1] != trainer.model.word_emb.weight.shape[1]:\n                print('  DIMENSION DOES NOT MATCH.  SKIPPING')\n                continue\n            N = min(pt.emb.shape[0], trainer.model.word_emb.weight.shape[0])\n            if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                if all((word_vocab.id2unit(x) == word_vocab.id2unit(x) for x in range(N))):\n                    print(\"  Attempting to use pt vectors to replace ner model's vectors\")\n                else:\n                    print('  NUM VECTORS DO NOT MATCH.  WORDS DO NOT MATCH.  SKIPPING')\n                    continue\n                if pt.emb.shape[0] < trainer.model.word_emb.weight.shape[0]:\n                    print('  WARNING: if any vectors beyond {} were fine tuned, that fine tuning will be lost'.format(N))\n            device = next(trainer.model.parameters()).device\n            delta = trainer.model.word_emb.weight[:N, :] - torch.from_numpy(pt.emb).to(device)[:N, :]\n            delta = delta.detach()\n            delta_norms = torch.linalg.norm(delta, dim=1).cpu().numpy()\n            if np.sum(delta_norms < 0) > 0:\n                raise ValueError('This should not be - a norm was less than 0!')\n            num_matching = np.sum(delta_norms < EPS)\n            if num_matching > N / 2:\n                print('  Accepted!  %d of %d vectors match for %s' % (num_matching, N, pt_path))\n                if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                    print('  Setting model vocab to match the pretrain')\n                    word_vocab = pt.vocab\n                    vocab['word'] = word_vocab\n                    trainer.args['word_emb_dim'] = pt.emb.shape[1]\n                break\n            else:\n                print('  %d of %d vectors matched for %s - SKIPPING' % (num_matching, N, pt_path))\n                vocab_same = sum((x in pt.vocab for x in word_vocab))\n                print('  %d words were in both vocabs' % vocab_same)\n                if DEBUG:\n                    rearranged_count = 0\n                    for x in word_vocab:\n                        if x not in pt.vocab:\n                            continue\n                        x_id = word_vocab.unit2id(x)\n                        x_vec = trainer.model.word_emb.weight[x_id, :]\n                        pt_id = pt.vocab.unit2id(x)\n                        pt_vec = pt.emb[pt_id, :]\n                        if (x_vec.detach().cpu() - pt_vec).norm() < EPS:\n                            rearranged_count += 1\n                    print('  %d vectors were close when ignoring id ordering' % rearranged_count)\n        else:\n            print('COULD NOT FIND A MATCHING PT: {}'.format(ner_processor))\n            missing_pretrains.append(ner_model)\n            continue\n        assert 'delta' not in vocab.keys()\n        delta_vectors = [delta[i].cpu() for i in range(4)]\n        delta_vocab = []\n        for i in range(4, len(delta_norms)):\n            if delta_norms[i] > 0.0:\n                delta_vocab.append(word_vocab.id2unit(i))\n                delta_vectors.append(delta[i].cpu())\n        trainer.model.unsaved_modules.append('word_emb')\n        if len(delta_vocab) == 0:\n            print('No vectors were changed!  Perhaps this model was trained without finetune.')\n            no_finetune.append(ner_model)\n        else:\n            print('%d delta vocab' % len(delta_vocab))\n            print('%d vectors in the delta set' % len(delta_vectors))\n            delta_vectors = np.stack(delta_vectors)\n            delta_vectors = torch.from_numpy(delta_vectors)\n            assert delta_vectors.shape[0] == len(delta_vocab) + len(VOCAB_PREFIX)\n            print(delta_vectors.shape)\n            delta_vocab = PretrainedWordVocab(delta_vocab, lang=word_vocab.lang, lower=word_vocab.lower)\n            vocab['delta'] = delta_vocab\n            trainer.model.delta_emb = nn.Embedding(delta_vectors.shape[0], delta_vectors.shape[1], PAD_ID)\n            trainer.model.delta_emb.weight.data.copy_(delta_vectors)\n        new_path = os.path.join(new_dir, ner_model)\n        trainer.save(new_path)\n        final_pretrains.append((ner_model, pt_model))\n    print()\n    if len(final_pretrains) > 0:\n        print('Final pretrain mappings:')\n        for i in final_pretrains:\n            print(i)\n    if len(missing_pretrains) > 0:\n        print('MISSING EMBEDDINGS:')\n        for i in missing_pretrains:\n            print(i)\n    if len(no_finetune) > 0:\n        print('NOT FINE TUNED:')\n        for i in no_finetune:\n            print(i)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_path', type=str, default='saved_models/ner', help='Where to find NER models (dir or filename)')\n    parser.add_argument('--output_path', type=str, default='saved_models/shrunk', help='Where to write shrunk NER models (dir)')\n    parser.add_argument('--pretrain_path', type=str, default='saved_models/pretrain', help='Where to find pretrains (dir or filename)')\n    args = parser.parse_args()\n    if os.path.isdir(args.input_path):\n        ner_model_dir = args.input_path\n        ners = os.listdir(ner_model_dir)\n        if len(ners) == 0:\n            raise FileNotFoundError('No ner models found in {}'.format(args.input_path))\n    else:\n        if not os.path.isfile(args.input_path):\n            raise FileNotFoundError('No ner model found at path {}'.format(args.input_path))\n        (ner_model_dir, ners) = os.path.split(args.input_path)\n        ners = [ners]\n    if os.path.isdir(args.pretrain_path):\n        pt_model_dir = args.pretrain_path\n        pretrains = os.listdir(pt_model_dir)\n        lang_to_pretrain = defaultdict(list)\n        for pt in pretrains:\n            lang_to_pretrain[pt.split('_')[0]].append(pt)\n    else:\n        (pt_model_dir, pretrains) = os.path.split(pt_model_dir)\n        pretrains = [pretrains]\n        lang_to_pretrain = defaultdict(lambda : pretrains)\n    new_dir = args.output_path\n    os.makedirs(new_dir, exist_ok=True)\n    final_pretrains = []\n    missing_pretrains = []\n    no_finetune = []\n    for ner_model in ners:\n        ner_path = os.path.join(ner_model_dir, ner_model)\n        expected_ending = '_nertagger.pt'\n        if not ner_model.endswith(expected_ending):\n            raise ValueError('Unexpected name: {}'.format(ner_model))\n        short_name = ner_model[:-len(expected_ending)]\n        (lang, package) = short_name.split('_', maxsplit=1)\n        print('===============================================')\n        print('Processing lang %s package %s' % (lang, package))\n        pipe = Pipeline(lang, processors='tokenize,ner', tokenize_pretokenized=True, package={'ner': package}, ner_model_path=ner_path)\n        ner_processor = pipe.processors['ner']\n        print('Loaded NER processor: {}'.format(ner_processor))\n        trainer = ner_processor.trainers[0]\n        vocab = trainer.model.vocab\n        word_vocab = vocab['word']\n        num_vectors = trainer.model.word_emb.weight.shape[0]\n        lcode = lang_to_langcode(trainer.args['lang'])\n        if lang != lcode and (not (lcode == 'zh' and lang == 'zh-hans')):\n            raise ValueError('lang not as expected: {} vs {} ({})'.format(lang, trainer.args['lang'], lcode))\n        ner_pretrains = sorted(set(lang_to_pretrain[lang] + lang_to_pretrain[lcode]))\n        for pt_model in ner_pretrains:\n            pt_path = os.path.join(pt_model_dir, pt_model)\n            print('Attempting pretrain: {}'.format(pt_path))\n            pt = Pretrain(filename=pt_path)\n            print('  pretrain shape:               {}'.format(pt.emb.shape))\n            print('  embedding in ner model shape: {}'.format(trainer.model.word_emb.weight.shape))\n            if pt.emb.shape[1] != trainer.model.word_emb.weight.shape[1]:\n                print('  DIMENSION DOES NOT MATCH.  SKIPPING')\n                continue\n            N = min(pt.emb.shape[0], trainer.model.word_emb.weight.shape[0])\n            if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                if all((word_vocab.id2unit(x) == word_vocab.id2unit(x) for x in range(N))):\n                    print(\"  Attempting to use pt vectors to replace ner model's vectors\")\n                else:\n                    print('  NUM VECTORS DO NOT MATCH.  WORDS DO NOT MATCH.  SKIPPING')\n                    continue\n                if pt.emb.shape[0] < trainer.model.word_emb.weight.shape[0]:\n                    print('  WARNING: if any vectors beyond {} were fine tuned, that fine tuning will be lost'.format(N))\n            device = next(trainer.model.parameters()).device\n            delta = trainer.model.word_emb.weight[:N, :] - torch.from_numpy(pt.emb).to(device)[:N, :]\n            delta = delta.detach()\n            delta_norms = torch.linalg.norm(delta, dim=1).cpu().numpy()\n            if np.sum(delta_norms < 0) > 0:\n                raise ValueError('This should not be - a norm was less than 0!')\n            num_matching = np.sum(delta_norms < EPS)\n            if num_matching > N / 2:\n                print('  Accepted!  %d of %d vectors match for %s' % (num_matching, N, pt_path))\n                if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                    print('  Setting model vocab to match the pretrain')\n                    word_vocab = pt.vocab\n                    vocab['word'] = word_vocab\n                    trainer.args['word_emb_dim'] = pt.emb.shape[1]\n                break\n            else:\n                print('  %d of %d vectors matched for %s - SKIPPING' % (num_matching, N, pt_path))\n                vocab_same = sum((x in pt.vocab for x in word_vocab))\n                print('  %d words were in both vocabs' % vocab_same)\n                if DEBUG:\n                    rearranged_count = 0\n                    for x in word_vocab:\n                        if x not in pt.vocab:\n                            continue\n                        x_id = word_vocab.unit2id(x)\n                        x_vec = trainer.model.word_emb.weight[x_id, :]\n                        pt_id = pt.vocab.unit2id(x)\n                        pt_vec = pt.emb[pt_id, :]\n                        if (x_vec.detach().cpu() - pt_vec).norm() < EPS:\n                            rearranged_count += 1\n                    print('  %d vectors were close when ignoring id ordering' % rearranged_count)\n        else:\n            print('COULD NOT FIND A MATCHING PT: {}'.format(ner_processor))\n            missing_pretrains.append(ner_model)\n            continue\n        assert 'delta' not in vocab.keys()\n        delta_vectors = [delta[i].cpu() for i in range(4)]\n        delta_vocab = []\n        for i in range(4, len(delta_norms)):\n            if delta_norms[i] > 0.0:\n                delta_vocab.append(word_vocab.id2unit(i))\n                delta_vectors.append(delta[i].cpu())\n        trainer.model.unsaved_modules.append('word_emb')\n        if len(delta_vocab) == 0:\n            print('No vectors were changed!  Perhaps this model was trained without finetune.')\n            no_finetune.append(ner_model)\n        else:\n            print('%d delta vocab' % len(delta_vocab))\n            print('%d vectors in the delta set' % len(delta_vectors))\n            delta_vectors = np.stack(delta_vectors)\n            delta_vectors = torch.from_numpy(delta_vectors)\n            assert delta_vectors.shape[0] == len(delta_vocab) + len(VOCAB_PREFIX)\n            print(delta_vectors.shape)\n            delta_vocab = PretrainedWordVocab(delta_vocab, lang=word_vocab.lang, lower=word_vocab.lower)\n            vocab['delta'] = delta_vocab\n            trainer.model.delta_emb = nn.Embedding(delta_vectors.shape[0], delta_vectors.shape[1], PAD_ID)\n            trainer.model.delta_emb.weight.data.copy_(delta_vectors)\n        new_path = os.path.join(new_dir, ner_model)\n        trainer.save(new_path)\n        final_pretrains.append((ner_model, pt_model))\n    print()\n    if len(final_pretrains) > 0:\n        print('Final pretrain mappings:')\n        for i in final_pretrains:\n            print(i)\n    if len(missing_pretrains) > 0:\n        print('MISSING EMBEDDINGS:')\n        for i in missing_pretrains:\n            print(i)\n    if len(no_finetune) > 0:\n        print('NOT FINE TUNED:')\n        for i in no_finetune:\n            print(i)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_path', type=str, default='saved_models/ner', help='Where to find NER models (dir or filename)')\n    parser.add_argument('--output_path', type=str, default='saved_models/shrunk', help='Where to write shrunk NER models (dir)')\n    parser.add_argument('--pretrain_path', type=str, default='saved_models/pretrain', help='Where to find pretrains (dir or filename)')\n    args = parser.parse_args()\n    if os.path.isdir(args.input_path):\n        ner_model_dir = args.input_path\n        ners = os.listdir(ner_model_dir)\n        if len(ners) == 0:\n            raise FileNotFoundError('No ner models found in {}'.format(args.input_path))\n    else:\n        if not os.path.isfile(args.input_path):\n            raise FileNotFoundError('No ner model found at path {}'.format(args.input_path))\n        (ner_model_dir, ners) = os.path.split(args.input_path)\n        ners = [ners]\n    if os.path.isdir(args.pretrain_path):\n        pt_model_dir = args.pretrain_path\n        pretrains = os.listdir(pt_model_dir)\n        lang_to_pretrain = defaultdict(list)\n        for pt in pretrains:\n            lang_to_pretrain[pt.split('_')[0]].append(pt)\n    else:\n        (pt_model_dir, pretrains) = os.path.split(pt_model_dir)\n        pretrains = [pretrains]\n        lang_to_pretrain = defaultdict(lambda : pretrains)\n    new_dir = args.output_path\n    os.makedirs(new_dir, exist_ok=True)\n    final_pretrains = []\n    missing_pretrains = []\n    no_finetune = []\n    for ner_model in ners:\n        ner_path = os.path.join(ner_model_dir, ner_model)\n        expected_ending = '_nertagger.pt'\n        if not ner_model.endswith(expected_ending):\n            raise ValueError('Unexpected name: {}'.format(ner_model))\n        short_name = ner_model[:-len(expected_ending)]\n        (lang, package) = short_name.split('_', maxsplit=1)\n        print('===============================================')\n        print('Processing lang %s package %s' % (lang, package))\n        pipe = Pipeline(lang, processors='tokenize,ner', tokenize_pretokenized=True, package={'ner': package}, ner_model_path=ner_path)\n        ner_processor = pipe.processors['ner']\n        print('Loaded NER processor: {}'.format(ner_processor))\n        trainer = ner_processor.trainers[0]\n        vocab = trainer.model.vocab\n        word_vocab = vocab['word']\n        num_vectors = trainer.model.word_emb.weight.shape[0]\n        lcode = lang_to_langcode(trainer.args['lang'])\n        if lang != lcode and (not (lcode == 'zh' and lang == 'zh-hans')):\n            raise ValueError('lang not as expected: {} vs {} ({})'.format(lang, trainer.args['lang'], lcode))\n        ner_pretrains = sorted(set(lang_to_pretrain[lang] + lang_to_pretrain[lcode]))\n        for pt_model in ner_pretrains:\n            pt_path = os.path.join(pt_model_dir, pt_model)\n            print('Attempting pretrain: {}'.format(pt_path))\n            pt = Pretrain(filename=pt_path)\n            print('  pretrain shape:               {}'.format(pt.emb.shape))\n            print('  embedding in ner model shape: {}'.format(trainer.model.word_emb.weight.shape))\n            if pt.emb.shape[1] != trainer.model.word_emb.weight.shape[1]:\n                print('  DIMENSION DOES NOT MATCH.  SKIPPING')\n                continue\n            N = min(pt.emb.shape[0], trainer.model.word_emb.weight.shape[0])\n            if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                if all((word_vocab.id2unit(x) == word_vocab.id2unit(x) for x in range(N))):\n                    print(\"  Attempting to use pt vectors to replace ner model's vectors\")\n                else:\n                    print('  NUM VECTORS DO NOT MATCH.  WORDS DO NOT MATCH.  SKIPPING')\n                    continue\n                if pt.emb.shape[0] < trainer.model.word_emb.weight.shape[0]:\n                    print('  WARNING: if any vectors beyond {} were fine tuned, that fine tuning will be lost'.format(N))\n            device = next(trainer.model.parameters()).device\n            delta = trainer.model.word_emb.weight[:N, :] - torch.from_numpy(pt.emb).to(device)[:N, :]\n            delta = delta.detach()\n            delta_norms = torch.linalg.norm(delta, dim=1).cpu().numpy()\n            if np.sum(delta_norms < 0) > 0:\n                raise ValueError('This should not be - a norm was less than 0!')\n            num_matching = np.sum(delta_norms < EPS)\n            if num_matching > N / 2:\n                print('  Accepted!  %d of %d vectors match for %s' % (num_matching, N, pt_path))\n                if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                    print('  Setting model vocab to match the pretrain')\n                    word_vocab = pt.vocab\n                    vocab['word'] = word_vocab\n                    trainer.args['word_emb_dim'] = pt.emb.shape[1]\n                break\n            else:\n                print('  %d of %d vectors matched for %s - SKIPPING' % (num_matching, N, pt_path))\n                vocab_same = sum((x in pt.vocab for x in word_vocab))\n                print('  %d words were in both vocabs' % vocab_same)\n                if DEBUG:\n                    rearranged_count = 0\n                    for x in word_vocab:\n                        if x not in pt.vocab:\n                            continue\n                        x_id = word_vocab.unit2id(x)\n                        x_vec = trainer.model.word_emb.weight[x_id, :]\n                        pt_id = pt.vocab.unit2id(x)\n                        pt_vec = pt.emb[pt_id, :]\n                        if (x_vec.detach().cpu() - pt_vec).norm() < EPS:\n                            rearranged_count += 1\n                    print('  %d vectors were close when ignoring id ordering' % rearranged_count)\n        else:\n            print('COULD NOT FIND A MATCHING PT: {}'.format(ner_processor))\n            missing_pretrains.append(ner_model)\n            continue\n        assert 'delta' not in vocab.keys()\n        delta_vectors = [delta[i].cpu() for i in range(4)]\n        delta_vocab = []\n        for i in range(4, len(delta_norms)):\n            if delta_norms[i] > 0.0:\n                delta_vocab.append(word_vocab.id2unit(i))\n                delta_vectors.append(delta[i].cpu())\n        trainer.model.unsaved_modules.append('word_emb')\n        if len(delta_vocab) == 0:\n            print('No vectors were changed!  Perhaps this model was trained without finetune.')\n            no_finetune.append(ner_model)\n        else:\n            print('%d delta vocab' % len(delta_vocab))\n            print('%d vectors in the delta set' % len(delta_vectors))\n            delta_vectors = np.stack(delta_vectors)\n            delta_vectors = torch.from_numpy(delta_vectors)\n            assert delta_vectors.shape[0] == len(delta_vocab) + len(VOCAB_PREFIX)\n            print(delta_vectors.shape)\n            delta_vocab = PretrainedWordVocab(delta_vocab, lang=word_vocab.lang, lower=word_vocab.lower)\n            vocab['delta'] = delta_vocab\n            trainer.model.delta_emb = nn.Embedding(delta_vectors.shape[0], delta_vectors.shape[1], PAD_ID)\n            trainer.model.delta_emb.weight.data.copy_(delta_vectors)\n        new_path = os.path.join(new_dir, ner_model)\n        trainer.save(new_path)\n        final_pretrains.append((ner_model, pt_model))\n    print()\n    if len(final_pretrains) > 0:\n        print('Final pretrain mappings:')\n        for i in final_pretrains:\n            print(i)\n    if len(missing_pretrains) > 0:\n        print('MISSING EMBEDDINGS:')\n        for i in missing_pretrains:\n            print(i)\n    if len(no_finetune) > 0:\n        print('NOT FINE TUNED:')\n        for i in no_finetune:\n            print(i)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_path', type=str, default='saved_models/ner', help='Where to find NER models (dir or filename)')\n    parser.add_argument('--output_path', type=str, default='saved_models/shrunk', help='Where to write shrunk NER models (dir)')\n    parser.add_argument('--pretrain_path', type=str, default='saved_models/pretrain', help='Where to find pretrains (dir or filename)')\n    args = parser.parse_args()\n    if os.path.isdir(args.input_path):\n        ner_model_dir = args.input_path\n        ners = os.listdir(ner_model_dir)\n        if len(ners) == 0:\n            raise FileNotFoundError('No ner models found in {}'.format(args.input_path))\n    else:\n        if not os.path.isfile(args.input_path):\n            raise FileNotFoundError('No ner model found at path {}'.format(args.input_path))\n        (ner_model_dir, ners) = os.path.split(args.input_path)\n        ners = [ners]\n    if os.path.isdir(args.pretrain_path):\n        pt_model_dir = args.pretrain_path\n        pretrains = os.listdir(pt_model_dir)\n        lang_to_pretrain = defaultdict(list)\n        for pt in pretrains:\n            lang_to_pretrain[pt.split('_')[0]].append(pt)\n    else:\n        (pt_model_dir, pretrains) = os.path.split(pt_model_dir)\n        pretrains = [pretrains]\n        lang_to_pretrain = defaultdict(lambda : pretrains)\n    new_dir = args.output_path\n    os.makedirs(new_dir, exist_ok=True)\n    final_pretrains = []\n    missing_pretrains = []\n    no_finetune = []\n    for ner_model in ners:\n        ner_path = os.path.join(ner_model_dir, ner_model)\n        expected_ending = '_nertagger.pt'\n        if not ner_model.endswith(expected_ending):\n            raise ValueError('Unexpected name: {}'.format(ner_model))\n        short_name = ner_model[:-len(expected_ending)]\n        (lang, package) = short_name.split('_', maxsplit=1)\n        print('===============================================')\n        print('Processing lang %s package %s' % (lang, package))\n        pipe = Pipeline(lang, processors='tokenize,ner', tokenize_pretokenized=True, package={'ner': package}, ner_model_path=ner_path)\n        ner_processor = pipe.processors['ner']\n        print('Loaded NER processor: {}'.format(ner_processor))\n        trainer = ner_processor.trainers[0]\n        vocab = trainer.model.vocab\n        word_vocab = vocab['word']\n        num_vectors = trainer.model.word_emb.weight.shape[0]\n        lcode = lang_to_langcode(trainer.args['lang'])\n        if lang != lcode and (not (lcode == 'zh' and lang == 'zh-hans')):\n            raise ValueError('lang not as expected: {} vs {} ({})'.format(lang, trainer.args['lang'], lcode))\n        ner_pretrains = sorted(set(lang_to_pretrain[lang] + lang_to_pretrain[lcode]))\n        for pt_model in ner_pretrains:\n            pt_path = os.path.join(pt_model_dir, pt_model)\n            print('Attempting pretrain: {}'.format(pt_path))\n            pt = Pretrain(filename=pt_path)\n            print('  pretrain shape:               {}'.format(pt.emb.shape))\n            print('  embedding in ner model shape: {}'.format(trainer.model.word_emb.weight.shape))\n            if pt.emb.shape[1] != trainer.model.word_emb.weight.shape[1]:\n                print('  DIMENSION DOES NOT MATCH.  SKIPPING')\n                continue\n            N = min(pt.emb.shape[0], trainer.model.word_emb.weight.shape[0])\n            if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                if all((word_vocab.id2unit(x) == word_vocab.id2unit(x) for x in range(N))):\n                    print(\"  Attempting to use pt vectors to replace ner model's vectors\")\n                else:\n                    print('  NUM VECTORS DO NOT MATCH.  WORDS DO NOT MATCH.  SKIPPING')\n                    continue\n                if pt.emb.shape[0] < trainer.model.word_emb.weight.shape[0]:\n                    print('  WARNING: if any vectors beyond {} were fine tuned, that fine tuning will be lost'.format(N))\n            device = next(trainer.model.parameters()).device\n            delta = trainer.model.word_emb.weight[:N, :] - torch.from_numpy(pt.emb).to(device)[:N, :]\n            delta = delta.detach()\n            delta_norms = torch.linalg.norm(delta, dim=1).cpu().numpy()\n            if np.sum(delta_norms < 0) > 0:\n                raise ValueError('This should not be - a norm was less than 0!')\n            num_matching = np.sum(delta_norms < EPS)\n            if num_matching > N / 2:\n                print('  Accepted!  %d of %d vectors match for %s' % (num_matching, N, pt_path))\n                if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                    print('  Setting model vocab to match the pretrain')\n                    word_vocab = pt.vocab\n                    vocab['word'] = word_vocab\n                    trainer.args['word_emb_dim'] = pt.emb.shape[1]\n                break\n            else:\n                print('  %d of %d vectors matched for %s - SKIPPING' % (num_matching, N, pt_path))\n                vocab_same = sum((x in pt.vocab for x in word_vocab))\n                print('  %d words were in both vocabs' % vocab_same)\n                if DEBUG:\n                    rearranged_count = 0\n                    for x in word_vocab:\n                        if x not in pt.vocab:\n                            continue\n                        x_id = word_vocab.unit2id(x)\n                        x_vec = trainer.model.word_emb.weight[x_id, :]\n                        pt_id = pt.vocab.unit2id(x)\n                        pt_vec = pt.emb[pt_id, :]\n                        if (x_vec.detach().cpu() - pt_vec).norm() < EPS:\n                            rearranged_count += 1\n                    print('  %d vectors were close when ignoring id ordering' % rearranged_count)\n        else:\n            print('COULD NOT FIND A MATCHING PT: {}'.format(ner_processor))\n            missing_pretrains.append(ner_model)\n            continue\n        assert 'delta' not in vocab.keys()\n        delta_vectors = [delta[i].cpu() for i in range(4)]\n        delta_vocab = []\n        for i in range(4, len(delta_norms)):\n            if delta_norms[i] > 0.0:\n                delta_vocab.append(word_vocab.id2unit(i))\n                delta_vectors.append(delta[i].cpu())\n        trainer.model.unsaved_modules.append('word_emb')\n        if len(delta_vocab) == 0:\n            print('No vectors were changed!  Perhaps this model was trained without finetune.')\n            no_finetune.append(ner_model)\n        else:\n            print('%d delta vocab' % len(delta_vocab))\n            print('%d vectors in the delta set' % len(delta_vectors))\n            delta_vectors = np.stack(delta_vectors)\n            delta_vectors = torch.from_numpy(delta_vectors)\n            assert delta_vectors.shape[0] == len(delta_vocab) + len(VOCAB_PREFIX)\n            print(delta_vectors.shape)\n            delta_vocab = PretrainedWordVocab(delta_vocab, lang=word_vocab.lang, lower=word_vocab.lower)\n            vocab['delta'] = delta_vocab\n            trainer.model.delta_emb = nn.Embedding(delta_vectors.shape[0], delta_vectors.shape[1], PAD_ID)\n            trainer.model.delta_emb.weight.data.copy_(delta_vectors)\n        new_path = os.path.join(new_dir, ner_model)\n        trainer.save(new_path)\n        final_pretrains.append((ner_model, pt_model))\n    print()\n    if len(final_pretrains) > 0:\n        print('Final pretrain mappings:')\n        for i in final_pretrains:\n            print(i)\n    if len(missing_pretrains) > 0:\n        print('MISSING EMBEDDINGS:')\n        for i in missing_pretrains:\n            print(i)\n    if len(no_finetune) > 0:\n        print('NOT FINE TUNED:')\n        for i in no_finetune:\n            print(i)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_path', type=str, default='saved_models/ner', help='Where to find NER models (dir or filename)')\n    parser.add_argument('--output_path', type=str, default='saved_models/shrunk', help='Where to write shrunk NER models (dir)')\n    parser.add_argument('--pretrain_path', type=str, default='saved_models/pretrain', help='Where to find pretrains (dir or filename)')\n    args = parser.parse_args()\n    if os.path.isdir(args.input_path):\n        ner_model_dir = args.input_path\n        ners = os.listdir(ner_model_dir)\n        if len(ners) == 0:\n            raise FileNotFoundError('No ner models found in {}'.format(args.input_path))\n    else:\n        if not os.path.isfile(args.input_path):\n            raise FileNotFoundError('No ner model found at path {}'.format(args.input_path))\n        (ner_model_dir, ners) = os.path.split(args.input_path)\n        ners = [ners]\n    if os.path.isdir(args.pretrain_path):\n        pt_model_dir = args.pretrain_path\n        pretrains = os.listdir(pt_model_dir)\n        lang_to_pretrain = defaultdict(list)\n        for pt in pretrains:\n            lang_to_pretrain[pt.split('_')[0]].append(pt)\n    else:\n        (pt_model_dir, pretrains) = os.path.split(pt_model_dir)\n        pretrains = [pretrains]\n        lang_to_pretrain = defaultdict(lambda : pretrains)\n    new_dir = args.output_path\n    os.makedirs(new_dir, exist_ok=True)\n    final_pretrains = []\n    missing_pretrains = []\n    no_finetune = []\n    for ner_model in ners:\n        ner_path = os.path.join(ner_model_dir, ner_model)\n        expected_ending = '_nertagger.pt'\n        if not ner_model.endswith(expected_ending):\n            raise ValueError('Unexpected name: {}'.format(ner_model))\n        short_name = ner_model[:-len(expected_ending)]\n        (lang, package) = short_name.split('_', maxsplit=1)\n        print('===============================================')\n        print('Processing lang %s package %s' % (lang, package))\n        pipe = Pipeline(lang, processors='tokenize,ner', tokenize_pretokenized=True, package={'ner': package}, ner_model_path=ner_path)\n        ner_processor = pipe.processors['ner']\n        print('Loaded NER processor: {}'.format(ner_processor))\n        trainer = ner_processor.trainers[0]\n        vocab = trainer.model.vocab\n        word_vocab = vocab['word']\n        num_vectors = trainer.model.word_emb.weight.shape[0]\n        lcode = lang_to_langcode(trainer.args['lang'])\n        if lang != lcode and (not (lcode == 'zh' and lang == 'zh-hans')):\n            raise ValueError('lang not as expected: {} vs {} ({})'.format(lang, trainer.args['lang'], lcode))\n        ner_pretrains = sorted(set(lang_to_pretrain[lang] + lang_to_pretrain[lcode]))\n        for pt_model in ner_pretrains:\n            pt_path = os.path.join(pt_model_dir, pt_model)\n            print('Attempting pretrain: {}'.format(pt_path))\n            pt = Pretrain(filename=pt_path)\n            print('  pretrain shape:               {}'.format(pt.emb.shape))\n            print('  embedding in ner model shape: {}'.format(trainer.model.word_emb.weight.shape))\n            if pt.emb.shape[1] != trainer.model.word_emb.weight.shape[1]:\n                print('  DIMENSION DOES NOT MATCH.  SKIPPING')\n                continue\n            N = min(pt.emb.shape[0], trainer.model.word_emb.weight.shape[0])\n            if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                if all((word_vocab.id2unit(x) == word_vocab.id2unit(x) for x in range(N))):\n                    print(\"  Attempting to use pt vectors to replace ner model's vectors\")\n                else:\n                    print('  NUM VECTORS DO NOT MATCH.  WORDS DO NOT MATCH.  SKIPPING')\n                    continue\n                if pt.emb.shape[0] < trainer.model.word_emb.weight.shape[0]:\n                    print('  WARNING: if any vectors beyond {} were fine tuned, that fine tuning will be lost'.format(N))\n            device = next(trainer.model.parameters()).device\n            delta = trainer.model.word_emb.weight[:N, :] - torch.from_numpy(pt.emb).to(device)[:N, :]\n            delta = delta.detach()\n            delta_norms = torch.linalg.norm(delta, dim=1).cpu().numpy()\n            if np.sum(delta_norms < 0) > 0:\n                raise ValueError('This should not be - a norm was less than 0!')\n            num_matching = np.sum(delta_norms < EPS)\n            if num_matching > N / 2:\n                print('  Accepted!  %d of %d vectors match for %s' % (num_matching, N, pt_path))\n                if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                    print('  Setting model vocab to match the pretrain')\n                    word_vocab = pt.vocab\n                    vocab['word'] = word_vocab\n                    trainer.args['word_emb_dim'] = pt.emb.shape[1]\n                break\n            else:\n                print('  %d of %d vectors matched for %s - SKIPPING' % (num_matching, N, pt_path))\n                vocab_same = sum((x in pt.vocab for x in word_vocab))\n                print('  %d words were in both vocabs' % vocab_same)\n                if DEBUG:\n                    rearranged_count = 0\n                    for x in word_vocab:\n                        if x not in pt.vocab:\n                            continue\n                        x_id = word_vocab.unit2id(x)\n                        x_vec = trainer.model.word_emb.weight[x_id, :]\n                        pt_id = pt.vocab.unit2id(x)\n                        pt_vec = pt.emb[pt_id, :]\n                        if (x_vec.detach().cpu() - pt_vec).norm() < EPS:\n                            rearranged_count += 1\n                    print('  %d vectors were close when ignoring id ordering' % rearranged_count)\n        else:\n            print('COULD NOT FIND A MATCHING PT: {}'.format(ner_processor))\n            missing_pretrains.append(ner_model)\n            continue\n        assert 'delta' not in vocab.keys()\n        delta_vectors = [delta[i].cpu() for i in range(4)]\n        delta_vocab = []\n        for i in range(4, len(delta_norms)):\n            if delta_norms[i] > 0.0:\n                delta_vocab.append(word_vocab.id2unit(i))\n                delta_vectors.append(delta[i].cpu())\n        trainer.model.unsaved_modules.append('word_emb')\n        if len(delta_vocab) == 0:\n            print('No vectors were changed!  Perhaps this model was trained without finetune.')\n            no_finetune.append(ner_model)\n        else:\n            print('%d delta vocab' % len(delta_vocab))\n            print('%d vectors in the delta set' % len(delta_vectors))\n            delta_vectors = np.stack(delta_vectors)\n            delta_vectors = torch.from_numpy(delta_vectors)\n            assert delta_vectors.shape[0] == len(delta_vocab) + len(VOCAB_PREFIX)\n            print(delta_vectors.shape)\n            delta_vocab = PretrainedWordVocab(delta_vocab, lang=word_vocab.lang, lower=word_vocab.lower)\n            vocab['delta'] = delta_vocab\n            trainer.model.delta_emb = nn.Embedding(delta_vectors.shape[0], delta_vectors.shape[1], PAD_ID)\n            trainer.model.delta_emb.weight.data.copy_(delta_vectors)\n        new_path = os.path.join(new_dir, ner_model)\n        trainer.save(new_path)\n        final_pretrains.append((ner_model, pt_model))\n    print()\n    if len(final_pretrains) > 0:\n        print('Final pretrain mappings:')\n        for i in final_pretrains:\n            print(i)\n    if len(missing_pretrains) > 0:\n        print('MISSING EMBEDDINGS:')\n        for i in missing_pretrains:\n            print(i)\n    if len(no_finetune) > 0:\n        print('NOT FINE TUNED:')\n        for i in no_finetune:\n            print(i)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_path', type=str, default='saved_models/ner', help='Where to find NER models (dir or filename)')\n    parser.add_argument('--output_path', type=str, default='saved_models/shrunk', help='Where to write shrunk NER models (dir)')\n    parser.add_argument('--pretrain_path', type=str, default='saved_models/pretrain', help='Where to find pretrains (dir or filename)')\n    args = parser.parse_args()\n    if os.path.isdir(args.input_path):\n        ner_model_dir = args.input_path\n        ners = os.listdir(ner_model_dir)\n        if len(ners) == 0:\n            raise FileNotFoundError('No ner models found in {}'.format(args.input_path))\n    else:\n        if not os.path.isfile(args.input_path):\n            raise FileNotFoundError('No ner model found at path {}'.format(args.input_path))\n        (ner_model_dir, ners) = os.path.split(args.input_path)\n        ners = [ners]\n    if os.path.isdir(args.pretrain_path):\n        pt_model_dir = args.pretrain_path\n        pretrains = os.listdir(pt_model_dir)\n        lang_to_pretrain = defaultdict(list)\n        for pt in pretrains:\n            lang_to_pretrain[pt.split('_')[0]].append(pt)\n    else:\n        (pt_model_dir, pretrains) = os.path.split(pt_model_dir)\n        pretrains = [pretrains]\n        lang_to_pretrain = defaultdict(lambda : pretrains)\n    new_dir = args.output_path\n    os.makedirs(new_dir, exist_ok=True)\n    final_pretrains = []\n    missing_pretrains = []\n    no_finetune = []\n    for ner_model in ners:\n        ner_path = os.path.join(ner_model_dir, ner_model)\n        expected_ending = '_nertagger.pt'\n        if not ner_model.endswith(expected_ending):\n            raise ValueError('Unexpected name: {}'.format(ner_model))\n        short_name = ner_model[:-len(expected_ending)]\n        (lang, package) = short_name.split('_', maxsplit=1)\n        print('===============================================')\n        print('Processing lang %s package %s' % (lang, package))\n        pipe = Pipeline(lang, processors='tokenize,ner', tokenize_pretokenized=True, package={'ner': package}, ner_model_path=ner_path)\n        ner_processor = pipe.processors['ner']\n        print('Loaded NER processor: {}'.format(ner_processor))\n        trainer = ner_processor.trainers[0]\n        vocab = trainer.model.vocab\n        word_vocab = vocab['word']\n        num_vectors = trainer.model.word_emb.weight.shape[0]\n        lcode = lang_to_langcode(trainer.args['lang'])\n        if lang != lcode and (not (lcode == 'zh' and lang == 'zh-hans')):\n            raise ValueError('lang not as expected: {} vs {} ({})'.format(lang, trainer.args['lang'], lcode))\n        ner_pretrains = sorted(set(lang_to_pretrain[lang] + lang_to_pretrain[lcode]))\n        for pt_model in ner_pretrains:\n            pt_path = os.path.join(pt_model_dir, pt_model)\n            print('Attempting pretrain: {}'.format(pt_path))\n            pt = Pretrain(filename=pt_path)\n            print('  pretrain shape:               {}'.format(pt.emb.shape))\n            print('  embedding in ner model shape: {}'.format(trainer.model.word_emb.weight.shape))\n            if pt.emb.shape[1] != trainer.model.word_emb.weight.shape[1]:\n                print('  DIMENSION DOES NOT MATCH.  SKIPPING')\n                continue\n            N = min(pt.emb.shape[0], trainer.model.word_emb.weight.shape[0])\n            if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                if all((word_vocab.id2unit(x) == word_vocab.id2unit(x) for x in range(N))):\n                    print(\"  Attempting to use pt vectors to replace ner model's vectors\")\n                else:\n                    print('  NUM VECTORS DO NOT MATCH.  WORDS DO NOT MATCH.  SKIPPING')\n                    continue\n                if pt.emb.shape[0] < trainer.model.word_emb.weight.shape[0]:\n                    print('  WARNING: if any vectors beyond {} were fine tuned, that fine tuning will be lost'.format(N))\n            device = next(trainer.model.parameters()).device\n            delta = trainer.model.word_emb.weight[:N, :] - torch.from_numpy(pt.emb).to(device)[:N, :]\n            delta = delta.detach()\n            delta_norms = torch.linalg.norm(delta, dim=1).cpu().numpy()\n            if np.sum(delta_norms < 0) > 0:\n                raise ValueError('This should not be - a norm was less than 0!')\n            num_matching = np.sum(delta_norms < EPS)\n            if num_matching > N / 2:\n                print('  Accepted!  %d of %d vectors match for %s' % (num_matching, N, pt_path))\n                if pt.emb.shape[0] != trainer.model.word_emb.weight.shape[0]:\n                    print('  Setting model vocab to match the pretrain')\n                    word_vocab = pt.vocab\n                    vocab['word'] = word_vocab\n                    trainer.args['word_emb_dim'] = pt.emb.shape[1]\n                break\n            else:\n                print('  %d of %d vectors matched for %s - SKIPPING' % (num_matching, N, pt_path))\n                vocab_same = sum((x in pt.vocab for x in word_vocab))\n                print('  %d words were in both vocabs' % vocab_same)\n                if DEBUG:\n                    rearranged_count = 0\n                    for x in word_vocab:\n                        if x not in pt.vocab:\n                            continue\n                        x_id = word_vocab.unit2id(x)\n                        x_vec = trainer.model.word_emb.weight[x_id, :]\n                        pt_id = pt.vocab.unit2id(x)\n                        pt_vec = pt.emb[pt_id, :]\n                        if (x_vec.detach().cpu() - pt_vec).norm() < EPS:\n                            rearranged_count += 1\n                    print('  %d vectors were close when ignoring id ordering' % rearranged_count)\n        else:\n            print('COULD NOT FIND A MATCHING PT: {}'.format(ner_processor))\n            missing_pretrains.append(ner_model)\n            continue\n        assert 'delta' not in vocab.keys()\n        delta_vectors = [delta[i].cpu() for i in range(4)]\n        delta_vocab = []\n        for i in range(4, len(delta_norms)):\n            if delta_norms[i] > 0.0:\n                delta_vocab.append(word_vocab.id2unit(i))\n                delta_vectors.append(delta[i].cpu())\n        trainer.model.unsaved_modules.append('word_emb')\n        if len(delta_vocab) == 0:\n            print('No vectors were changed!  Perhaps this model was trained without finetune.')\n            no_finetune.append(ner_model)\n        else:\n            print('%d delta vocab' % len(delta_vocab))\n            print('%d vectors in the delta set' % len(delta_vectors))\n            delta_vectors = np.stack(delta_vectors)\n            delta_vectors = torch.from_numpy(delta_vectors)\n            assert delta_vectors.shape[0] == len(delta_vocab) + len(VOCAB_PREFIX)\n            print(delta_vectors.shape)\n            delta_vocab = PretrainedWordVocab(delta_vocab, lang=word_vocab.lang, lower=word_vocab.lower)\n            vocab['delta'] = delta_vocab\n            trainer.model.delta_emb = nn.Embedding(delta_vectors.shape[0], delta_vectors.shape[1], PAD_ID)\n            trainer.model.delta_emb.weight.data.copy_(delta_vectors)\n        new_path = os.path.join(new_dir, ner_model)\n        trainer.save(new_path)\n        final_pretrains.append((ner_model, pt_model))\n    print()\n    if len(final_pretrains) > 0:\n        print('Final pretrain mappings:')\n        for i in final_pretrains:\n            print(i)\n    if len(missing_pretrains) > 0:\n        print('MISSING EMBEDDINGS:')\n        for i in missing_pretrains:\n            print(i)\n    if len(no_finetune) > 0:\n        print('NOT FINE TUNED:')\n        for i in no_finetune:\n            print(i)"
        ]
    }
]