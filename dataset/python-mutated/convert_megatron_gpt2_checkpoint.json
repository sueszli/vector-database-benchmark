[
    {
        "func_name": "recursive_print",
        "original": "def recursive_print(name, val, spaces=0):\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
        "mutated": [
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)"
        ]
    },
    {
        "func_name": "fix_query_key_value_ordering",
        "original": "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
        "mutated": [
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param"
        ]
    },
    {
        "func_name": "convert_megatron_checkpoint",
        "original": "def convert_megatron_checkpoint(args, input_state_dict, config):\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.vocab_size = ds_args.padded_vocab_size\n        config.n_positions = ds_args.max_position_embeddings\n        config.n_embd = ds_args.hidden_size\n        config.n_layer = ds_args.num_layers\n        config.n_head = ds_args.num_attention_heads\n        config.n_inner = ds_args.ffn_hidden_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    n_positions = pos_embeddings.size(0)\n    if n_positions != config.n_positions:\n        raise ValueError(f\"pos_embeddings.max_sequence_length={n_positions} and config.n_positions={config.n_positions} don't match\")\n    output_state_dict['transformer.wpe.weight'] = pos_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attn.c_proj.', 'self_attention.dense': '.attn.c_proj.', 'mlp.dense_h_to_4h': '.mlp.c_fc.', 'mlp.dense_4h_to_h': '.mlp.c_proj.'}\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'transformer.h.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.float16)).view(1, 1, n_positions, n_positions)\n            output_state_dict[layer_name + '.attn.bias'] = causal_mask\n            masked_bias = torch.tensor(-10000.0, dtype=torch.float16)\n            output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            out_val = out_val.transpose(0, 1).contiguous()\n            output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n        elif weight_or_bias == 'weight':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'weight'] = val.transpose(0, 1)\n        elif weight_or_bias == 'bias':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'bias'] = val\n    assert config.n_layer == layer_idx + 1\n    output_state_dict['transformer.ln_f.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['transformer.ln_f.bias'] = transformer['final_layernorm.bias']\n    output_state_dict['lm_head.weight'] = word_embeddings\n    return output_state_dict",
        "mutated": [
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.vocab_size = ds_args.padded_vocab_size\n        config.n_positions = ds_args.max_position_embeddings\n        config.n_embd = ds_args.hidden_size\n        config.n_layer = ds_args.num_layers\n        config.n_head = ds_args.num_attention_heads\n        config.n_inner = ds_args.ffn_hidden_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    n_positions = pos_embeddings.size(0)\n    if n_positions != config.n_positions:\n        raise ValueError(f\"pos_embeddings.max_sequence_length={n_positions} and config.n_positions={config.n_positions} don't match\")\n    output_state_dict['transformer.wpe.weight'] = pos_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attn.c_proj.', 'self_attention.dense': '.attn.c_proj.', 'mlp.dense_h_to_4h': '.mlp.c_fc.', 'mlp.dense_4h_to_h': '.mlp.c_proj.'}\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'transformer.h.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.float16)).view(1, 1, n_positions, n_positions)\n            output_state_dict[layer_name + '.attn.bias'] = causal_mask\n            masked_bias = torch.tensor(-10000.0, dtype=torch.float16)\n            output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            out_val = out_val.transpose(0, 1).contiguous()\n            output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n        elif weight_or_bias == 'weight':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'weight'] = val.transpose(0, 1)\n        elif weight_or_bias == 'bias':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'bias'] = val\n    assert config.n_layer == layer_idx + 1\n    output_state_dict['transformer.ln_f.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['transformer.ln_f.bias'] = transformer['final_layernorm.bias']\n    output_state_dict['lm_head.weight'] = word_embeddings\n    return output_state_dict",
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.vocab_size = ds_args.padded_vocab_size\n        config.n_positions = ds_args.max_position_embeddings\n        config.n_embd = ds_args.hidden_size\n        config.n_layer = ds_args.num_layers\n        config.n_head = ds_args.num_attention_heads\n        config.n_inner = ds_args.ffn_hidden_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    n_positions = pos_embeddings.size(0)\n    if n_positions != config.n_positions:\n        raise ValueError(f\"pos_embeddings.max_sequence_length={n_positions} and config.n_positions={config.n_positions} don't match\")\n    output_state_dict['transformer.wpe.weight'] = pos_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attn.c_proj.', 'self_attention.dense': '.attn.c_proj.', 'mlp.dense_h_to_4h': '.mlp.c_fc.', 'mlp.dense_4h_to_h': '.mlp.c_proj.'}\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'transformer.h.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.float16)).view(1, 1, n_positions, n_positions)\n            output_state_dict[layer_name + '.attn.bias'] = causal_mask\n            masked_bias = torch.tensor(-10000.0, dtype=torch.float16)\n            output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            out_val = out_val.transpose(0, 1).contiguous()\n            output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n        elif weight_or_bias == 'weight':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'weight'] = val.transpose(0, 1)\n        elif weight_or_bias == 'bias':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'bias'] = val\n    assert config.n_layer == layer_idx + 1\n    output_state_dict['transformer.ln_f.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['transformer.ln_f.bias'] = transformer['final_layernorm.bias']\n    output_state_dict['lm_head.weight'] = word_embeddings\n    return output_state_dict",
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.vocab_size = ds_args.padded_vocab_size\n        config.n_positions = ds_args.max_position_embeddings\n        config.n_embd = ds_args.hidden_size\n        config.n_layer = ds_args.num_layers\n        config.n_head = ds_args.num_attention_heads\n        config.n_inner = ds_args.ffn_hidden_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    n_positions = pos_embeddings.size(0)\n    if n_positions != config.n_positions:\n        raise ValueError(f\"pos_embeddings.max_sequence_length={n_positions} and config.n_positions={config.n_positions} don't match\")\n    output_state_dict['transformer.wpe.weight'] = pos_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attn.c_proj.', 'self_attention.dense': '.attn.c_proj.', 'mlp.dense_h_to_4h': '.mlp.c_fc.', 'mlp.dense_4h_to_h': '.mlp.c_proj.'}\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'transformer.h.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.float16)).view(1, 1, n_positions, n_positions)\n            output_state_dict[layer_name + '.attn.bias'] = causal_mask\n            masked_bias = torch.tensor(-10000.0, dtype=torch.float16)\n            output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            out_val = out_val.transpose(0, 1).contiguous()\n            output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n        elif weight_or_bias == 'weight':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'weight'] = val.transpose(0, 1)\n        elif weight_or_bias == 'bias':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'bias'] = val\n    assert config.n_layer == layer_idx + 1\n    output_state_dict['transformer.ln_f.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['transformer.ln_f.bias'] = transformer['final_layernorm.bias']\n    output_state_dict['lm_head.weight'] = word_embeddings\n    return output_state_dict",
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.vocab_size = ds_args.padded_vocab_size\n        config.n_positions = ds_args.max_position_embeddings\n        config.n_embd = ds_args.hidden_size\n        config.n_layer = ds_args.num_layers\n        config.n_head = ds_args.num_attention_heads\n        config.n_inner = ds_args.ffn_hidden_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    n_positions = pos_embeddings.size(0)\n    if n_positions != config.n_positions:\n        raise ValueError(f\"pos_embeddings.max_sequence_length={n_positions} and config.n_positions={config.n_positions} don't match\")\n    output_state_dict['transformer.wpe.weight'] = pos_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attn.c_proj.', 'self_attention.dense': '.attn.c_proj.', 'mlp.dense_h_to_4h': '.mlp.c_fc.', 'mlp.dense_4h_to_h': '.mlp.c_proj.'}\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'transformer.h.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.float16)).view(1, 1, n_positions, n_positions)\n            output_state_dict[layer_name + '.attn.bias'] = causal_mask\n            masked_bias = torch.tensor(-10000.0, dtype=torch.float16)\n            output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            out_val = out_val.transpose(0, 1).contiguous()\n            output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n        elif weight_or_bias == 'weight':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'weight'] = val.transpose(0, 1)\n        elif weight_or_bias == 'bias':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'bias'] = val\n    assert config.n_layer == layer_idx + 1\n    output_state_dict['transformer.ln_f.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['transformer.ln_f.bias'] = transformer['final_layernorm.bias']\n    output_state_dict['lm_head.weight'] = word_embeddings\n    return output_state_dict",
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.vocab_size = ds_args.padded_vocab_size\n        config.n_positions = ds_args.max_position_embeddings\n        config.n_embd = ds_args.hidden_size\n        config.n_layer = ds_args.num_layers\n        config.n_head = ds_args.num_attention_heads\n        config.n_inner = ds_args.ffn_hidden_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    n_positions = pos_embeddings.size(0)\n    if n_positions != config.n_positions:\n        raise ValueError(f\"pos_embeddings.max_sequence_length={n_positions} and config.n_positions={config.n_positions} don't match\")\n    output_state_dict['transformer.wpe.weight'] = pos_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attn.c_proj.', 'self_attention.dense': '.attn.c_proj.', 'mlp.dense_h_to_4h': '.mlp.c_fc.', 'mlp.dense_4h_to_h': '.mlp.c_proj.'}\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'transformer.h.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.float16)).view(1, 1, n_positions, n_positions)\n            output_state_dict[layer_name + '.attn.bias'] = causal_mask\n            masked_bias = torch.tensor(-10000.0, dtype=torch.float16)\n            output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            out_val = out_val.transpose(0, 1).contiguous()\n            output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n        elif weight_or_bias == 'weight':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'weight'] = val.transpose(0, 1)\n        elif weight_or_bias == 'bias':\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + 'bias'] = val\n    assert config.n_layer == layer_idx + 1\n    output_state_dict['transformer.ln_f.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['transformer.ln_f.bias'] = transformer['final_layernorm.bias']\n    output_state_dict['lm_head.weight'] = word_embeddings\n    return output_state_dict"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the checkpoint file (.zip archive or direct .pt file)')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from {args.path_to_checkpoint}')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    ds_args = input_state_dict.get('args', None)\n    if args.config_file == '':\n        if ds_args is not None:\n            if ds_args.bias_gelu_fusion:\n                activation_function = 'gelu_fast'\n            elif ds_args.openai_gelu:\n                activation_function = 'gelu_new'\n            else:\n                activation_function = 'gelu'\n        else:\n            activation_function = 'gelu_new'\n        config = GPT2Config(vocab_size=50257, n_positions=1024, n_embd=1024, n_layer=24, n_head=16, n_inner=4096, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=50256, eos_token_id=50256)\n    else:\n        config = GPT2Config.from_json_file(args.config_file)\n    config.architectures = ['GPT2LMHeadModel']\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if ds_args is not None:\n        tokenizer_type = ds_args.tokenizer_type\n        if tokenizer_type == 'GPT2BPETokenizer':\n            tokenizer_model_name = 'gpt2'\n        elif tokenizer_type == 'PretrainedFromHF':\n            tokenizer_model_name = ds_args.tokenizer_name_or_path\n        else:\n            raise ValueError(f'Unrecognized tokenizer_type {tokenizer_type}')\n    else:\n        tokenizer_model_name = 'gpt2'\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(basename)\n    print(f'Adding {tokenizer_class} tokenizer files')\n    tokenizer.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the checkpoint file (.zip archive or direct .pt file)')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from {args.path_to_checkpoint}')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    ds_args = input_state_dict.get('args', None)\n    if args.config_file == '':\n        if ds_args is not None:\n            if ds_args.bias_gelu_fusion:\n                activation_function = 'gelu_fast'\n            elif ds_args.openai_gelu:\n                activation_function = 'gelu_new'\n            else:\n                activation_function = 'gelu'\n        else:\n            activation_function = 'gelu_new'\n        config = GPT2Config(vocab_size=50257, n_positions=1024, n_embd=1024, n_layer=24, n_head=16, n_inner=4096, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=50256, eos_token_id=50256)\n    else:\n        config = GPT2Config.from_json_file(args.config_file)\n    config.architectures = ['GPT2LMHeadModel']\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if ds_args is not None:\n        tokenizer_type = ds_args.tokenizer_type\n        if tokenizer_type == 'GPT2BPETokenizer':\n            tokenizer_model_name = 'gpt2'\n        elif tokenizer_type == 'PretrainedFromHF':\n            tokenizer_model_name = ds_args.tokenizer_name_or_path\n        else:\n            raise ValueError(f'Unrecognized tokenizer_type {tokenizer_type}')\n    else:\n        tokenizer_model_name = 'gpt2'\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(basename)\n    print(f'Adding {tokenizer_class} tokenizer files')\n    tokenizer.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the checkpoint file (.zip archive or direct .pt file)')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from {args.path_to_checkpoint}')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    ds_args = input_state_dict.get('args', None)\n    if args.config_file == '':\n        if ds_args is not None:\n            if ds_args.bias_gelu_fusion:\n                activation_function = 'gelu_fast'\n            elif ds_args.openai_gelu:\n                activation_function = 'gelu_new'\n            else:\n                activation_function = 'gelu'\n        else:\n            activation_function = 'gelu_new'\n        config = GPT2Config(vocab_size=50257, n_positions=1024, n_embd=1024, n_layer=24, n_head=16, n_inner=4096, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=50256, eos_token_id=50256)\n    else:\n        config = GPT2Config.from_json_file(args.config_file)\n    config.architectures = ['GPT2LMHeadModel']\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if ds_args is not None:\n        tokenizer_type = ds_args.tokenizer_type\n        if tokenizer_type == 'GPT2BPETokenizer':\n            tokenizer_model_name = 'gpt2'\n        elif tokenizer_type == 'PretrainedFromHF':\n            tokenizer_model_name = ds_args.tokenizer_name_or_path\n        else:\n            raise ValueError(f'Unrecognized tokenizer_type {tokenizer_type}')\n    else:\n        tokenizer_model_name = 'gpt2'\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(basename)\n    print(f'Adding {tokenizer_class} tokenizer files')\n    tokenizer.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the checkpoint file (.zip archive or direct .pt file)')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from {args.path_to_checkpoint}')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    ds_args = input_state_dict.get('args', None)\n    if args.config_file == '':\n        if ds_args is not None:\n            if ds_args.bias_gelu_fusion:\n                activation_function = 'gelu_fast'\n            elif ds_args.openai_gelu:\n                activation_function = 'gelu_new'\n            else:\n                activation_function = 'gelu'\n        else:\n            activation_function = 'gelu_new'\n        config = GPT2Config(vocab_size=50257, n_positions=1024, n_embd=1024, n_layer=24, n_head=16, n_inner=4096, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=50256, eos_token_id=50256)\n    else:\n        config = GPT2Config.from_json_file(args.config_file)\n    config.architectures = ['GPT2LMHeadModel']\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if ds_args is not None:\n        tokenizer_type = ds_args.tokenizer_type\n        if tokenizer_type == 'GPT2BPETokenizer':\n            tokenizer_model_name = 'gpt2'\n        elif tokenizer_type == 'PretrainedFromHF':\n            tokenizer_model_name = ds_args.tokenizer_name_or_path\n        else:\n            raise ValueError(f'Unrecognized tokenizer_type {tokenizer_type}')\n    else:\n        tokenizer_model_name = 'gpt2'\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(basename)\n    print(f'Adding {tokenizer_class} tokenizer files')\n    tokenizer.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the checkpoint file (.zip archive or direct .pt file)')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from {args.path_to_checkpoint}')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    ds_args = input_state_dict.get('args', None)\n    if args.config_file == '':\n        if ds_args is not None:\n            if ds_args.bias_gelu_fusion:\n                activation_function = 'gelu_fast'\n            elif ds_args.openai_gelu:\n                activation_function = 'gelu_new'\n            else:\n                activation_function = 'gelu'\n        else:\n            activation_function = 'gelu_new'\n        config = GPT2Config(vocab_size=50257, n_positions=1024, n_embd=1024, n_layer=24, n_head=16, n_inner=4096, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=50256, eos_token_id=50256)\n    else:\n        config = GPT2Config.from_json_file(args.config_file)\n    config.architectures = ['GPT2LMHeadModel']\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if ds_args is not None:\n        tokenizer_type = ds_args.tokenizer_type\n        if tokenizer_type == 'GPT2BPETokenizer':\n            tokenizer_model_name = 'gpt2'\n        elif tokenizer_type == 'PretrainedFromHF':\n            tokenizer_model_name = ds_args.tokenizer_name_or_path\n        else:\n            raise ValueError(f'Unrecognized tokenizer_type {tokenizer_type}')\n    else:\n        tokenizer_model_name = 'gpt2'\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(basename)\n    print(f'Adding {tokenizer_class} tokenizer files')\n    tokenizer.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the checkpoint file (.zip archive or direct .pt file)')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from {args.path_to_checkpoint}')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    ds_args = input_state_dict.get('args', None)\n    if args.config_file == '':\n        if ds_args is not None:\n            if ds_args.bias_gelu_fusion:\n                activation_function = 'gelu_fast'\n            elif ds_args.openai_gelu:\n                activation_function = 'gelu_new'\n            else:\n                activation_function = 'gelu'\n        else:\n            activation_function = 'gelu_new'\n        config = GPT2Config(vocab_size=50257, n_positions=1024, n_embd=1024, n_layer=24, n_head=16, n_inner=4096, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=50256, eos_token_id=50256)\n    else:\n        config = GPT2Config.from_json_file(args.config_file)\n    config.architectures = ['GPT2LMHeadModel']\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if ds_args is not None:\n        tokenizer_type = ds_args.tokenizer_type\n        if tokenizer_type == 'GPT2BPETokenizer':\n            tokenizer_model_name = 'gpt2'\n        elif tokenizer_type == 'PretrainedFromHF':\n            tokenizer_model_name = ds_args.tokenizer_name_or_path\n        else:\n            raise ValueError(f'Unrecognized tokenizer_type {tokenizer_type}')\n    else:\n        tokenizer_model_name = 'gpt2'\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(basename)\n    print(f'Adding {tokenizer_class} tokenizer files')\n    tokenizer.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)"
        ]
    }
]