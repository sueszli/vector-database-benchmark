[
    {
        "func_name": "replace_with_awq_linear",
        "original": "def replace_with_awq_linear(model, modules_to_not_convert=None, quantization_config=None, current_key_name=None, has_been_replaced=False) -> bool:\n    \"\"\"\n    Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\n    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n    conversion has been successfull or not.\n\n    During the module replacement, we also infer the backend to use through the `quantization_config` object.\n\n    Args:\n        model (`torch.nn.Module`):\n            The model to convert, can be any `torch.nn.Module` instance.\n        quantization_config (`AwqConfig`):\n            The quantization config object that contains the quantization parameters.\n        modules_to_not_convert (`list`, *optional*):\n            A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\n            converted.\n        current_key_name (`list`, *optional*):\n            A list that contains the current key name. This is used for recursion and should not be passed by the user.\n        has_been_replaced (`bool`, *optional*):\n            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\n            should not be passed by the user.\n    \"\"\"\n    if modules_to_not_convert is None:\n        modules_to_not_convert = []\n    backend = quantization_config.backend\n    if not is_auto_awq_available():\n        raise ValueError('AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq')\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n    elif backend == AwqBackendPackingMethod.LLMAWQ:\n        from awq.quantize.qmodule import WQLinear\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        target_cls = WQLinear_GEMM if quantization_config.version == AWQLinearVersion.GEMM else WQLinear_GEMV\n    else:\n        target_cls = WQLinear\n    for (name, module) in model.named_children():\n        if current_key_name is None:\n            current_key_name = []\n        current_key_name.append(name)\n        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n            if not any((key in '.'.join(current_key_name) for key in modules_to_not_convert)):\n                in_features = module.in_features\n                out_features = module.out_features\n                model._modules[name] = target_cls(w_bit=quantization_config.bits, group_size=quantization_config.group_size, in_features=in_features, out_features=out_features, bias=module.bias is not None, dev=module.weight.device)\n                has_been_replaced = True\n                model._modules[name].requires_grad_(False)\n        if len(list(module.children())) > 0:\n            (_, has_been_replaced) = replace_with_awq_linear(module, modules_to_not_convert=modules_to_not_convert, current_key_name=current_key_name, quantization_config=quantization_config, has_been_replaced=has_been_replaced)\n        current_key_name.pop(-1)\n    return (model, has_been_replaced)",
        "mutated": [
            "def replace_with_awq_linear(model, modules_to_not_convert=None, quantization_config=None, current_key_name=None, has_been_replaced=False) -> bool:\n    if False:\n        i = 10\n    '\\n    Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\\n    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\\n    conversion has been successfull or not.\\n\\n    During the module replacement, we also infer the backend to use through the `quantization_config` object.\\n\\n    Args:\\n        model (`torch.nn.Module`):\\n            The model to convert, can be any `torch.nn.Module` instance.\\n        quantization_config (`AwqConfig`):\\n            The quantization config object that contains the quantization parameters.\\n        modules_to_not_convert (`list`, *optional*):\\n            A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\\n            converted.\\n        current_key_name (`list`, *optional*):\\n            A list that contains the current key name. This is used for recursion and should not be passed by the user.\\n        has_been_replaced (`bool`, *optional*):\\n            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\\n            should not be passed by the user.\\n    '\n    if modules_to_not_convert is None:\n        modules_to_not_convert = []\n    backend = quantization_config.backend\n    if not is_auto_awq_available():\n        raise ValueError('AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq')\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n    elif backend == AwqBackendPackingMethod.LLMAWQ:\n        from awq.quantize.qmodule import WQLinear\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        target_cls = WQLinear_GEMM if quantization_config.version == AWQLinearVersion.GEMM else WQLinear_GEMV\n    else:\n        target_cls = WQLinear\n    for (name, module) in model.named_children():\n        if current_key_name is None:\n            current_key_name = []\n        current_key_name.append(name)\n        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n            if not any((key in '.'.join(current_key_name) for key in modules_to_not_convert)):\n                in_features = module.in_features\n                out_features = module.out_features\n                model._modules[name] = target_cls(w_bit=quantization_config.bits, group_size=quantization_config.group_size, in_features=in_features, out_features=out_features, bias=module.bias is not None, dev=module.weight.device)\n                has_been_replaced = True\n                model._modules[name].requires_grad_(False)\n        if len(list(module.children())) > 0:\n            (_, has_been_replaced) = replace_with_awq_linear(module, modules_to_not_convert=modules_to_not_convert, current_key_name=current_key_name, quantization_config=quantization_config, has_been_replaced=has_been_replaced)\n        current_key_name.pop(-1)\n    return (model, has_been_replaced)",
            "def replace_with_awq_linear(model, modules_to_not_convert=None, quantization_config=None, current_key_name=None, has_been_replaced=False) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\\n    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\\n    conversion has been successfull or not.\\n\\n    During the module replacement, we also infer the backend to use through the `quantization_config` object.\\n\\n    Args:\\n        model (`torch.nn.Module`):\\n            The model to convert, can be any `torch.nn.Module` instance.\\n        quantization_config (`AwqConfig`):\\n            The quantization config object that contains the quantization parameters.\\n        modules_to_not_convert (`list`, *optional*):\\n            A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\\n            converted.\\n        current_key_name (`list`, *optional*):\\n            A list that contains the current key name. This is used for recursion and should not be passed by the user.\\n        has_been_replaced (`bool`, *optional*):\\n            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\\n            should not be passed by the user.\\n    '\n    if modules_to_not_convert is None:\n        modules_to_not_convert = []\n    backend = quantization_config.backend\n    if not is_auto_awq_available():\n        raise ValueError('AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq')\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n    elif backend == AwqBackendPackingMethod.LLMAWQ:\n        from awq.quantize.qmodule import WQLinear\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        target_cls = WQLinear_GEMM if quantization_config.version == AWQLinearVersion.GEMM else WQLinear_GEMV\n    else:\n        target_cls = WQLinear\n    for (name, module) in model.named_children():\n        if current_key_name is None:\n            current_key_name = []\n        current_key_name.append(name)\n        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n            if not any((key in '.'.join(current_key_name) for key in modules_to_not_convert)):\n                in_features = module.in_features\n                out_features = module.out_features\n                model._modules[name] = target_cls(w_bit=quantization_config.bits, group_size=quantization_config.group_size, in_features=in_features, out_features=out_features, bias=module.bias is not None, dev=module.weight.device)\n                has_been_replaced = True\n                model._modules[name].requires_grad_(False)\n        if len(list(module.children())) > 0:\n            (_, has_been_replaced) = replace_with_awq_linear(module, modules_to_not_convert=modules_to_not_convert, current_key_name=current_key_name, quantization_config=quantization_config, has_been_replaced=has_been_replaced)\n        current_key_name.pop(-1)\n    return (model, has_been_replaced)",
            "def replace_with_awq_linear(model, modules_to_not_convert=None, quantization_config=None, current_key_name=None, has_been_replaced=False) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\\n    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\\n    conversion has been successfull or not.\\n\\n    During the module replacement, we also infer the backend to use through the `quantization_config` object.\\n\\n    Args:\\n        model (`torch.nn.Module`):\\n            The model to convert, can be any `torch.nn.Module` instance.\\n        quantization_config (`AwqConfig`):\\n            The quantization config object that contains the quantization parameters.\\n        modules_to_not_convert (`list`, *optional*):\\n            A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\\n            converted.\\n        current_key_name (`list`, *optional*):\\n            A list that contains the current key name. This is used for recursion and should not be passed by the user.\\n        has_been_replaced (`bool`, *optional*):\\n            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\\n            should not be passed by the user.\\n    '\n    if modules_to_not_convert is None:\n        modules_to_not_convert = []\n    backend = quantization_config.backend\n    if not is_auto_awq_available():\n        raise ValueError('AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq')\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n    elif backend == AwqBackendPackingMethod.LLMAWQ:\n        from awq.quantize.qmodule import WQLinear\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        target_cls = WQLinear_GEMM if quantization_config.version == AWQLinearVersion.GEMM else WQLinear_GEMV\n    else:\n        target_cls = WQLinear\n    for (name, module) in model.named_children():\n        if current_key_name is None:\n            current_key_name = []\n        current_key_name.append(name)\n        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n            if not any((key in '.'.join(current_key_name) for key in modules_to_not_convert)):\n                in_features = module.in_features\n                out_features = module.out_features\n                model._modules[name] = target_cls(w_bit=quantization_config.bits, group_size=quantization_config.group_size, in_features=in_features, out_features=out_features, bias=module.bias is not None, dev=module.weight.device)\n                has_been_replaced = True\n                model._modules[name].requires_grad_(False)\n        if len(list(module.children())) > 0:\n            (_, has_been_replaced) = replace_with_awq_linear(module, modules_to_not_convert=modules_to_not_convert, current_key_name=current_key_name, quantization_config=quantization_config, has_been_replaced=has_been_replaced)\n        current_key_name.pop(-1)\n    return (model, has_been_replaced)",
            "def replace_with_awq_linear(model, modules_to_not_convert=None, quantization_config=None, current_key_name=None, has_been_replaced=False) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\\n    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\\n    conversion has been successfull or not.\\n\\n    During the module replacement, we also infer the backend to use through the `quantization_config` object.\\n\\n    Args:\\n        model (`torch.nn.Module`):\\n            The model to convert, can be any `torch.nn.Module` instance.\\n        quantization_config (`AwqConfig`):\\n            The quantization config object that contains the quantization parameters.\\n        modules_to_not_convert (`list`, *optional*):\\n            A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\\n            converted.\\n        current_key_name (`list`, *optional*):\\n            A list that contains the current key name. This is used for recursion and should not be passed by the user.\\n        has_been_replaced (`bool`, *optional*):\\n            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\\n            should not be passed by the user.\\n    '\n    if modules_to_not_convert is None:\n        modules_to_not_convert = []\n    backend = quantization_config.backend\n    if not is_auto_awq_available():\n        raise ValueError('AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq')\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n    elif backend == AwqBackendPackingMethod.LLMAWQ:\n        from awq.quantize.qmodule import WQLinear\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        target_cls = WQLinear_GEMM if quantization_config.version == AWQLinearVersion.GEMM else WQLinear_GEMV\n    else:\n        target_cls = WQLinear\n    for (name, module) in model.named_children():\n        if current_key_name is None:\n            current_key_name = []\n        current_key_name.append(name)\n        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n            if not any((key in '.'.join(current_key_name) for key in modules_to_not_convert)):\n                in_features = module.in_features\n                out_features = module.out_features\n                model._modules[name] = target_cls(w_bit=quantization_config.bits, group_size=quantization_config.group_size, in_features=in_features, out_features=out_features, bias=module.bias is not None, dev=module.weight.device)\n                has_been_replaced = True\n                model._modules[name].requires_grad_(False)\n        if len(list(module.children())) > 0:\n            (_, has_been_replaced) = replace_with_awq_linear(module, modules_to_not_convert=modules_to_not_convert, current_key_name=current_key_name, quantization_config=quantization_config, has_been_replaced=has_been_replaced)\n        current_key_name.pop(-1)\n    return (model, has_been_replaced)",
            "def replace_with_awq_linear(model, modules_to_not_convert=None, quantization_config=None, current_key_name=None, has_been_replaced=False) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\\n    `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\\n    conversion has been successfull or not.\\n\\n    During the module replacement, we also infer the backend to use through the `quantization_config` object.\\n\\n    Args:\\n        model (`torch.nn.Module`):\\n            The model to convert, can be any `torch.nn.Module` instance.\\n        quantization_config (`AwqConfig`):\\n            The quantization config object that contains the quantization parameters.\\n        modules_to_not_convert (`list`, *optional*):\\n            A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\\n            converted.\\n        current_key_name (`list`, *optional*):\\n            A list that contains the current key name. This is used for recursion and should not be passed by the user.\\n        has_been_replaced (`bool`, *optional*):\\n            A boolean that indicates if the conversion has been successful or not. This is used for recursion and\\n            should not be passed by the user.\\n    '\n    if modules_to_not_convert is None:\n        modules_to_not_convert = []\n    backend = quantization_config.backend\n    if not is_auto_awq_available():\n        raise ValueError('AWQ (either `autoawq` or `llmawq`) is not available. Please install it with `pip install autoawq` or check out the installation guide in https://github.com/mit-han-lab/llm-awq')\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n    elif backend == AwqBackendPackingMethod.LLMAWQ:\n        from awq.quantize.qmodule import WQLinear\n    if backend == AwqBackendPackingMethod.AUTOAWQ:\n        target_cls = WQLinear_GEMM if quantization_config.version == AWQLinearVersion.GEMM else WQLinear_GEMV\n    else:\n        target_cls = WQLinear\n    for (name, module) in model.named_children():\n        if current_key_name is None:\n            current_key_name = []\n        current_key_name.append(name)\n        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n            if not any((key in '.'.join(current_key_name) for key in modules_to_not_convert)):\n                in_features = module.in_features\n                out_features = module.out_features\n                model._modules[name] = target_cls(w_bit=quantization_config.bits, group_size=quantization_config.group_size, in_features=in_features, out_features=out_features, bias=module.bias is not None, dev=module.weight.device)\n                has_been_replaced = True\n                model._modules[name].requires_grad_(False)\n        if len(list(module.children())) > 0:\n            (_, has_been_replaced) = replace_with_awq_linear(module, modules_to_not_convert=modules_to_not_convert, current_key_name=current_key_name, quantization_config=quantization_config, has_been_replaced=has_been_replaced)\n        current_key_name.pop(-1)\n    return (model, has_been_replaced)"
        ]
    }
]