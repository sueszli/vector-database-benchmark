[
    {
        "func_name": "test_flash_attention_patch",
        "original": "def test_flash_attention_patch(dtype=torch.float16, device='cuda:0', llama_path='/mnt/data/llama2/Llama-2-7b'):\n    tokenizer = AutoTokenizer.from_pretrained(llama_path)\n    tokenizer.add_special_tokens({'pad_token': '</s>', 'eos_token': '</s>', 'sep_token': '<s>'})\n    model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patched_model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n    device = model.device\n    n_heads = model.config.num_attention_heads\n    head_dim = model.config.hidden_size // n_heads\n    with torch.no_grad():\n        for (layer1, layer2) in zip(model.model.layers, patched_model.model.layers):\n            hidden_states = torch.randn(4, 10, head_dim * n_heads, dtype=dtype, device=device)\n            attention_mask = (torch.randn(4, 10, device=device).sort(dim=-1).values < 0.5).int()\n            attn_mask = patched_model.model._prepare_decoder_attention_mask(attention_mask, (4, 10), hidden_states, 0)\n            position_ids = torch.arange(10, device=device).unsqueeze(0).expand(4, -1)\n            (attn1, attn2) = (layer1.self_attn, layer2.self_attn)\n            (out1, _, _) = attn1(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            (out2, _, _) = attn2(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            assert (((out1 - out2) * attention_mask.unsqueeze(-1)).mean(dim=-1).abs() < 0.001).all()\n        batch = tokenizer(['hello world', 'lorem ipsum dolor sit amet'], padding=True, return_tensors='pt').to(device)\n        out1 = model(**batch).logits\n        out2 = patched_model(**batch).logits\n        diff = (out1 - out2) * batch['attention_mask'].unsqueeze(-1)\n        assert (diff.abs() < 0.1).all()\n    input_ids = torch.randint(0, model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()",
        "mutated": [
            "def test_flash_attention_patch(dtype=torch.float16, device='cuda:0', llama_path='/mnt/data/llama2/Llama-2-7b'):\n    if False:\n        i = 10\n    tokenizer = AutoTokenizer.from_pretrained(llama_path)\n    tokenizer.add_special_tokens({'pad_token': '</s>', 'eos_token': '</s>', 'sep_token': '<s>'})\n    model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patched_model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n    device = model.device\n    n_heads = model.config.num_attention_heads\n    head_dim = model.config.hidden_size // n_heads\n    with torch.no_grad():\n        for (layer1, layer2) in zip(model.model.layers, patched_model.model.layers):\n            hidden_states = torch.randn(4, 10, head_dim * n_heads, dtype=dtype, device=device)\n            attention_mask = (torch.randn(4, 10, device=device).sort(dim=-1).values < 0.5).int()\n            attn_mask = patched_model.model._prepare_decoder_attention_mask(attention_mask, (4, 10), hidden_states, 0)\n            position_ids = torch.arange(10, device=device).unsqueeze(0).expand(4, -1)\n            (attn1, attn2) = (layer1.self_attn, layer2.self_attn)\n            (out1, _, _) = attn1(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            (out2, _, _) = attn2(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            assert (((out1 - out2) * attention_mask.unsqueeze(-1)).mean(dim=-1).abs() < 0.001).all()\n        batch = tokenizer(['hello world', 'lorem ipsum dolor sit amet'], padding=True, return_tensors='pt').to(device)\n        out1 = model(**batch).logits\n        out2 = patched_model(**batch).logits\n        diff = (out1 - out2) * batch['attention_mask'].unsqueeze(-1)\n        assert (diff.abs() < 0.1).all()\n    input_ids = torch.randint(0, model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()",
            "def test_flash_attention_patch(dtype=torch.float16, device='cuda:0', llama_path='/mnt/data/llama2/Llama-2-7b'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokenizer = AutoTokenizer.from_pretrained(llama_path)\n    tokenizer.add_special_tokens({'pad_token': '</s>', 'eos_token': '</s>', 'sep_token': '<s>'})\n    model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patched_model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n    device = model.device\n    n_heads = model.config.num_attention_heads\n    head_dim = model.config.hidden_size // n_heads\n    with torch.no_grad():\n        for (layer1, layer2) in zip(model.model.layers, patched_model.model.layers):\n            hidden_states = torch.randn(4, 10, head_dim * n_heads, dtype=dtype, device=device)\n            attention_mask = (torch.randn(4, 10, device=device).sort(dim=-1).values < 0.5).int()\n            attn_mask = patched_model.model._prepare_decoder_attention_mask(attention_mask, (4, 10), hidden_states, 0)\n            position_ids = torch.arange(10, device=device).unsqueeze(0).expand(4, -1)\n            (attn1, attn2) = (layer1.self_attn, layer2.self_attn)\n            (out1, _, _) = attn1(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            (out2, _, _) = attn2(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            assert (((out1 - out2) * attention_mask.unsqueeze(-1)).mean(dim=-1).abs() < 0.001).all()\n        batch = tokenizer(['hello world', 'lorem ipsum dolor sit amet'], padding=True, return_tensors='pt').to(device)\n        out1 = model(**batch).logits\n        out2 = patched_model(**batch).logits\n        diff = (out1 - out2) * batch['attention_mask'].unsqueeze(-1)\n        assert (diff.abs() < 0.1).all()\n    input_ids = torch.randint(0, model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()",
            "def test_flash_attention_patch(dtype=torch.float16, device='cuda:0', llama_path='/mnt/data/llama2/Llama-2-7b'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokenizer = AutoTokenizer.from_pretrained(llama_path)\n    tokenizer.add_special_tokens({'pad_token': '</s>', 'eos_token': '</s>', 'sep_token': '<s>'})\n    model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patched_model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n    device = model.device\n    n_heads = model.config.num_attention_heads\n    head_dim = model.config.hidden_size // n_heads\n    with torch.no_grad():\n        for (layer1, layer2) in zip(model.model.layers, patched_model.model.layers):\n            hidden_states = torch.randn(4, 10, head_dim * n_heads, dtype=dtype, device=device)\n            attention_mask = (torch.randn(4, 10, device=device).sort(dim=-1).values < 0.5).int()\n            attn_mask = patched_model.model._prepare_decoder_attention_mask(attention_mask, (4, 10), hidden_states, 0)\n            position_ids = torch.arange(10, device=device).unsqueeze(0).expand(4, -1)\n            (attn1, attn2) = (layer1.self_attn, layer2.self_attn)\n            (out1, _, _) = attn1(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            (out2, _, _) = attn2(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            assert (((out1 - out2) * attention_mask.unsqueeze(-1)).mean(dim=-1).abs() < 0.001).all()\n        batch = tokenizer(['hello world', 'lorem ipsum dolor sit amet'], padding=True, return_tensors='pt').to(device)\n        out1 = model(**batch).logits\n        out2 = patched_model(**batch).logits\n        diff = (out1 - out2) * batch['attention_mask'].unsqueeze(-1)\n        assert (diff.abs() < 0.1).all()\n    input_ids = torch.randint(0, model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()",
            "def test_flash_attention_patch(dtype=torch.float16, device='cuda:0', llama_path='/mnt/data/llama2/Llama-2-7b'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokenizer = AutoTokenizer.from_pretrained(llama_path)\n    tokenizer.add_special_tokens({'pad_token': '</s>', 'eos_token': '</s>', 'sep_token': '<s>'})\n    model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patched_model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n    device = model.device\n    n_heads = model.config.num_attention_heads\n    head_dim = model.config.hidden_size // n_heads\n    with torch.no_grad():\n        for (layer1, layer2) in zip(model.model.layers, patched_model.model.layers):\n            hidden_states = torch.randn(4, 10, head_dim * n_heads, dtype=dtype, device=device)\n            attention_mask = (torch.randn(4, 10, device=device).sort(dim=-1).values < 0.5).int()\n            attn_mask = patched_model.model._prepare_decoder_attention_mask(attention_mask, (4, 10), hidden_states, 0)\n            position_ids = torch.arange(10, device=device).unsqueeze(0).expand(4, -1)\n            (attn1, attn2) = (layer1.self_attn, layer2.self_attn)\n            (out1, _, _) = attn1(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            (out2, _, _) = attn2(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            assert (((out1 - out2) * attention_mask.unsqueeze(-1)).mean(dim=-1).abs() < 0.001).all()\n        batch = tokenizer(['hello world', 'lorem ipsum dolor sit amet'], padding=True, return_tensors='pt').to(device)\n        out1 = model(**batch).logits\n        out2 = patched_model(**batch).logits\n        diff = (out1 - out2) * batch['attention_mask'].unsqueeze(-1)\n        assert (diff.abs() < 0.1).all()\n    input_ids = torch.randint(0, model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()",
            "def test_flash_attention_patch(dtype=torch.float16, device='cuda:0', llama_path='/mnt/data/llama2/Llama-2-7b'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokenizer = AutoTokenizer.from_pretrained(llama_path)\n    tokenizer.add_special_tokens({'pad_token': '</s>', 'eos_token': '</s>', 'sep_token': '<s>'})\n    model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patched_model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n    device = model.device\n    n_heads = model.config.num_attention_heads\n    head_dim = model.config.hidden_size // n_heads\n    with torch.no_grad():\n        for (layer1, layer2) in zip(model.model.layers, patched_model.model.layers):\n            hidden_states = torch.randn(4, 10, head_dim * n_heads, dtype=dtype, device=device)\n            attention_mask = (torch.randn(4, 10, device=device).sort(dim=-1).values < 0.5).int()\n            attn_mask = patched_model.model._prepare_decoder_attention_mask(attention_mask, (4, 10), hidden_states, 0)\n            position_ids = torch.arange(10, device=device).unsqueeze(0).expand(4, -1)\n            (attn1, attn2) = (layer1.self_attn, layer2.self_attn)\n            (out1, _, _) = attn1(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            (out2, _, _) = attn2(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            assert (((out1 - out2) * attention_mask.unsqueeze(-1)).mean(dim=-1).abs() < 0.001).all()\n        batch = tokenizer(['hello world', 'lorem ipsum dolor sit amet'], padding=True, return_tensors='pt').to(device)\n        out1 = model(**batch).logits\n        out2 = patched_model(**batch).logits\n        diff = (out1 - out2) * batch['attention_mask'].unsqueeze(-1)\n        assert (diff.abs() < 0.1).all()\n    input_ids = torch.randint(0, model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()"
        ]
    }
]