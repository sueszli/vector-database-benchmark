[
    {
        "func_name": "infer_observed_and_model_labels",
        "original": "def infer_observed_and_model_labels(train_dataset=None, test_dataset=None, model: BaseEstimator=None, y_pred_train: np.ndarray=None, y_pred_test: np.ndarray=None, model_classes: list=None, task_type: TaskType=None) -> Tuple[List, List]:\n    \"\"\"\n    Infer the observed labels from the given datasets and predictions.\n\n    Parameters\n    ----------\n    train_dataset : Union[TextData, None], default None\n        TextData object, representing data an estimator was fitted on\n    test_dataset : Union[TextData, None], default None\n        TextData object, representing data an estimator predicts on\n    model : Union[BaseEstimator, None], default None\n        A fitted estimator instance\n    y_pred_train : np.array\n        Predictions on train_dataset\n    y_pred_test : np.array\n        Predictions on test_dataset\n    model_classes : Optional[List], default None\n        list of classes known to the model\n    task_type : Union[TaskType, None], default None\n        The task type of the model\n\n    Returns\n    -------\n        observed_classes : list\n            List of observed label values. For multi-label, returns number of observed labels.\n        model_classes : list\n            List of the user-given model classes. For multi-label, if not given by the user, returns a range of\n            len(label)\n    \"\"\"\n    train_labels = []\n    test_labels = []\n    have_model = model is not None\n    if train_dataset:\n        if train_dataset.has_label():\n            train_labels += list(train_dataset.label)\n        if have_model:\n            train_labels += list(model.predict(train_dataset))\n    if test_dataset:\n        if test_dataset.has_label():\n            test_labels += list(test_dataset.label)\n        if have_model:\n            test_labels += list(model.predict(test_dataset))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        train_labels = [token_label for sentence in train_labels for token_label in sentence]\n        test_labels = [token_label for sentence in test_labels for token_label in sentence]\n        if model_classes and 'O' in model_classes:\n            model_classes = [c for c in model_classes if c != 'O']\n            warnings.warn('\"O\" label was removed from model_classes as it is ignored by metrics for token classification', UserWarning)\n    observed_classes = np.array(test_labels + train_labels, dtype=object)\n    if len(observed_classes.shape) == 2:\n        len_observed_label = observed_classes.shape[1]\n        if not model_classes:\n            model_classes = list(range(len_observed_label))\n            observed_classes = list(range(len_observed_label))\n        else:\n            if len(model_classes) != len_observed_label:\n                raise DeepchecksValueError(f'Received model_classes of length {len(model_classes)}, but data indicates labels of length {len_observed_label}')\n            observed_classes = model_classes\n    else:\n        observed_classes = observed_classes[~pd.isnull(observed_classes)]\n        observed_classes = sorted(np.unique(observed_classes))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        observed_classes = [c for c in observed_classes if c != 'O']\n        observed_classes = sorted({tag for (tag, _, _) in get_entities(observed_classes)})\n    return (observed_classes, model_classes)",
        "mutated": [
            "def infer_observed_and_model_labels(train_dataset=None, test_dataset=None, model: BaseEstimator=None, y_pred_train: np.ndarray=None, y_pred_test: np.ndarray=None, model_classes: list=None, task_type: TaskType=None) -> Tuple[List, List]:\n    if False:\n        i = 10\n    '\\n    Infer the observed labels from the given datasets and predictions.\\n\\n    Parameters\\n    ----------\\n    train_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator was fitted on\\n    test_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator predicts on\\n    model : Union[BaseEstimator, None], default None\\n        A fitted estimator instance\\n    y_pred_train : np.array\\n        Predictions on train_dataset\\n    y_pred_test : np.array\\n        Predictions on test_dataset\\n    model_classes : Optional[List], default None\\n        list of classes known to the model\\n    task_type : Union[TaskType, None], default None\\n        The task type of the model\\n\\n    Returns\\n    -------\\n        observed_classes : list\\n            List of observed label values. For multi-label, returns number of observed labels.\\n        model_classes : list\\n            List of the user-given model classes. For multi-label, if not given by the user, returns a range of\\n            len(label)\\n    '\n    train_labels = []\n    test_labels = []\n    have_model = model is not None\n    if train_dataset:\n        if train_dataset.has_label():\n            train_labels += list(train_dataset.label)\n        if have_model:\n            train_labels += list(model.predict(train_dataset))\n    if test_dataset:\n        if test_dataset.has_label():\n            test_labels += list(test_dataset.label)\n        if have_model:\n            test_labels += list(model.predict(test_dataset))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        train_labels = [token_label for sentence in train_labels for token_label in sentence]\n        test_labels = [token_label for sentence in test_labels for token_label in sentence]\n        if model_classes and 'O' in model_classes:\n            model_classes = [c for c in model_classes if c != 'O']\n            warnings.warn('\"O\" label was removed from model_classes as it is ignored by metrics for token classification', UserWarning)\n    observed_classes = np.array(test_labels + train_labels, dtype=object)\n    if len(observed_classes.shape) == 2:\n        len_observed_label = observed_classes.shape[1]\n        if not model_classes:\n            model_classes = list(range(len_observed_label))\n            observed_classes = list(range(len_observed_label))\n        else:\n            if len(model_classes) != len_observed_label:\n                raise DeepchecksValueError(f'Received model_classes of length {len(model_classes)}, but data indicates labels of length {len_observed_label}')\n            observed_classes = model_classes\n    else:\n        observed_classes = observed_classes[~pd.isnull(observed_classes)]\n        observed_classes = sorted(np.unique(observed_classes))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        observed_classes = [c for c in observed_classes if c != 'O']\n        observed_classes = sorted({tag for (tag, _, _) in get_entities(observed_classes)})\n    return (observed_classes, model_classes)",
            "def infer_observed_and_model_labels(train_dataset=None, test_dataset=None, model: BaseEstimator=None, y_pred_train: np.ndarray=None, y_pred_test: np.ndarray=None, model_classes: list=None, task_type: TaskType=None) -> Tuple[List, List]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Infer the observed labels from the given datasets and predictions.\\n\\n    Parameters\\n    ----------\\n    train_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator was fitted on\\n    test_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator predicts on\\n    model : Union[BaseEstimator, None], default None\\n        A fitted estimator instance\\n    y_pred_train : np.array\\n        Predictions on train_dataset\\n    y_pred_test : np.array\\n        Predictions on test_dataset\\n    model_classes : Optional[List], default None\\n        list of classes known to the model\\n    task_type : Union[TaskType, None], default None\\n        The task type of the model\\n\\n    Returns\\n    -------\\n        observed_classes : list\\n            List of observed label values. For multi-label, returns number of observed labels.\\n        model_classes : list\\n            List of the user-given model classes. For multi-label, if not given by the user, returns a range of\\n            len(label)\\n    '\n    train_labels = []\n    test_labels = []\n    have_model = model is not None\n    if train_dataset:\n        if train_dataset.has_label():\n            train_labels += list(train_dataset.label)\n        if have_model:\n            train_labels += list(model.predict(train_dataset))\n    if test_dataset:\n        if test_dataset.has_label():\n            test_labels += list(test_dataset.label)\n        if have_model:\n            test_labels += list(model.predict(test_dataset))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        train_labels = [token_label for sentence in train_labels for token_label in sentence]\n        test_labels = [token_label for sentence in test_labels for token_label in sentence]\n        if model_classes and 'O' in model_classes:\n            model_classes = [c for c in model_classes if c != 'O']\n            warnings.warn('\"O\" label was removed from model_classes as it is ignored by metrics for token classification', UserWarning)\n    observed_classes = np.array(test_labels + train_labels, dtype=object)\n    if len(observed_classes.shape) == 2:\n        len_observed_label = observed_classes.shape[1]\n        if not model_classes:\n            model_classes = list(range(len_observed_label))\n            observed_classes = list(range(len_observed_label))\n        else:\n            if len(model_classes) != len_observed_label:\n                raise DeepchecksValueError(f'Received model_classes of length {len(model_classes)}, but data indicates labels of length {len_observed_label}')\n            observed_classes = model_classes\n    else:\n        observed_classes = observed_classes[~pd.isnull(observed_classes)]\n        observed_classes = sorted(np.unique(observed_classes))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        observed_classes = [c for c in observed_classes if c != 'O']\n        observed_classes = sorted({tag for (tag, _, _) in get_entities(observed_classes)})\n    return (observed_classes, model_classes)",
            "def infer_observed_and_model_labels(train_dataset=None, test_dataset=None, model: BaseEstimator=None, y_pred_train: np.ndarray=None, y_pred_test: np.ndarray=None, model_classes: list=None, task_type: TaskType=None) -> Tuple[List, List]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Infer the observed labels from the given datasets and predictions.\\n\\n    Parameters\\n    ----------\\n    train_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator was fitted on\\n    test_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator predicts on\\n    model : Union[BaseEstimator, None], default None\\n        A fitted estimator instance\\n    y_pred_train : np.array\\n        Predictions on train_dataset\\n    y_pred_test : np.array\\n        Predictions on test_dataset\\n    model_classes : Optional[List], default None\\n        list of classes known to the model\\n    task_type : Union[TaskType, None], default None\\n        The task type of the model\\n\\n    Returns\\n    -------\\n        observed_classes : list\\n            List of observed label values. For multi-label, returns number of observed labels.\\n        model_classes : list\\n            List of the user-given model classes. For multi-label, if not given by the user, returns a range of\\n            len(label)\\n    '\n    train_labels = []\n    test_labels = []\n    have_model = model is not None\n    if train_dataset:\n        if train_dataset.has_label():\n            train_labels += list(train_dataset.label)\n        if have_model:\n            train_labels += list(model.predict(train_dataset))\n    if test_dataset:\n        if test_dataset.has_label():\n            test_labels += list(test_dataset.label)\n        if have_model:\n            test_labels += list(model.predict(test_dataset))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        train_labels = [token_label for sentence in train_labels for token_label in sentence]\n        test_labels = [token_label for sentence in test_labels for token_label in sentence]\n        if model_classes and 'O' in model_classes:\n            model_classes = [c for c in model_classes if c != 'O']\n            warnings.warn('\"O\" label was removed from model_classes as it is ignored by metrics for token classification', UserWarning)\n    observed_classes = np.array(test_labels + train_labels, dtype=object)\n    if len(observed_classes.shape) == 2:\n        len_observed_label = observed_classes.shape[1]\n        if not model_classes:\n            model_classes = list(range(len_observed_label))\n            observed_classes = list(range(len_observed_label))\n        else:\n            if len(model_classes) != len_observed_label:\n                raise DeepchecksValueError(f'Received model_classes of length {len(model_classes)}, but data indicates labels of length {len_observed_label}')\n            observed_classes = model_classes\n    else:\n        observed_classes = observed_classes[~pd.isnull(observed_classes)]\n        observed_classes = sorted(np.unique(observed_classes))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        observed_classes = [c for c in observed_classes if c != 'O']\n        observed_classes = sorted({tag for (tag, _, _) in get_entities(observed_classes)})\n    return (observed_classes, model_classes)",
            "def infer_observed_and_model_labels(train_dataset=None, test_dataset=None, model: BaseEstimator=None, y_pred_train: np.ndarray=None, y_pred_test: np.ndarray=None, model_classes: list=None, task_type: TaskType=None) -> Tuple[List, List]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Infer the observed labels from the given datasets and predictions.\\n\\n    Parameters\\n    ----------\\n    train_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator was fitted on\\n    test_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator predicts on\\n    model : Union[BaseEstimator, None], default None\\n        A fitted estimator instance\\n    y_pred_train : np.array\\n        Predictions on train_dataset\\n    y_pred_test : np.array\\n        Predictions on test_dataset\\n    model_classes : Optional[List], default None\\n        list of classes known to the model\\n    task_type : Union[TaskType, None], default None\\n        The task type of the model\\n\\n    Returns\\n    -------\\n        observed_classes : list\\n            List of observed label values. For multi-label, returns number of observed labels.\\n        model_classes : list\\n            List of the user-given model classes. For multi-label, if not given by the user, returns a range of\\n            len(label)\\n    '\n    train_labels = []\n    test_labels = []\n    have_model = model is not None\n    if train_dataset:\n        if train_dataset.has_label():\n            train_labels += list(train_dataset.label)\n        if have_model:\n            train_labels += list(model.predict(train_dataset))\n    if test_dataset:\n        if test_dataset.has_label():\n            test_labels += list(test_dataset.label)\n        if have_model:\n            test_labels += list(model.predict(test_dataset))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        train_labels = [token_label for sentence in train_labels for token_label in sentence]\n        test_labels = [token_label for sentence in test_labels for token_label in sentence]\n        if model_classes and 'O' in model_classes:\n            model_classes = [c for c in model_classes if c != 'O']\n            warnings.warn('\"O\" label was removed from model_classes as it is ignored by metrics for token classification', UserWarning)\n    observed_classes = np.array(test_labels + train_labels, dtype=object)\n    if len(observed_classes.shape) == 2:\n        len_observed_label = observed_classes.shape[1]\n        if not model_classes:\n            model_classes = list(range(len_observed_label))\n            observed_classes = list(range(len_observed_label))\n        else:\n            if len(model_classes) != len_observed_label:\n                raise DeepchecksValueError(f'Received model_classes of length {len(model_classes)}, but data indicates labels of length {len_observed_label}')\n            observed_classes = model_classes\n    else:\n        observed_classes = observed_classes[~pd.isnull(observed_classes)]\n        observed_classes = sorted(np.unique(observed_classes))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        observed_classes = [c for c in observed_classes if c != 'O']\n        observed_classes = sorted({tag for (tag, _, _) in get_entities(observed_classes)})\n    return (observed_classes, model_classes)",
            "def infer_observed_and_model_labels(train_dataset=None, test_dataset=None, model: BaseEstimator=None, y_pred_train: np.ndarray=None, y_pred_test: np.ndarray=None, model_classes: list=None, task_type: TaskType=None) -> Tuple[List, List]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Infer the observed labels from the given datasets and predictions.\\n\\n    Parameters\\n    ----------\\n    train_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator was fitted on\\n    test_dataset : Union[TextData, None], default None\\n        TextData object, representing data an estimator predicts on\\n    model : Union[BaseEstimator, None], default None\\n        A fitted estimator instance\\n    y_pred_train : np.array\\n        Predictions on train_dataset\\n    y_pred_test : np.array\\n        Predictions on test_dataset\\n    model_classes : Optional[List], default None\\n        list of classes known to the model\\n    task_type : Union[TaskType, None], default None\\n        The task type of the model\\n\\n    Returns\\n    -------\\n        observed_classes : list\\n            List of observed label values. For multi-label, returns number of observed labels.\\n        model_classes : list\\n            List of the user-given model classes. For multi-label, if not given by the user, returns a range of\\n            len(label)\\n    '\n    train_labels = []\n    test_labels = []\n    have_model = model is not None\n    if train_dataset:\n        if train_dataset.has_label():\n            train_labels += list(train_dataset.label)\n        if have_model:\n            train_labels += list(model.predict(train_dataset))\n    if test_dataset:\n        if test_dataset.has_label():\n            test_labels += list(test_dataset.label)\n        if have_model:\n            test_labels += list(model.predict(test_dataset))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        train_labels = [token_label for sentence in train_labels for token_label in sentence]\n        test_labels = [token_label for sentence in test_labels for token_label in sentence]\n        if model_classes and 'O' in model_classes:\n            model_classes = [c for c in model_classes if c != 'O']\n            warnings.warn('\"O\" label was removed from model_classes as it is ignored by metrics for token classification', UserWarning)\n    observed_classes = np.array(test_labels + train_labels, dtype=object)\n    if len(observed_classes.shape) == 2:\n        len_observed_label = observed_classes.shape[1]\n        if not model_classes:\n            model_classes = list(range(len_observed_label))\n            observed_classes = list(range(len_observed_label))\n        else:\n            if len(model_classes) != len_observed_label:\n                raise DeepchecksValueError(f'Received model_classes of length {len(model_classes)}, but data indicates labels of length {len_observed_label}')\n            observed_classes = model_classes\n    else:\n        observed_classes = observed_classes[~pd.isnull(observed_classes)]\n        observed_classes = sorted(np.unique(observed_classes))\n    if task_type == TaskType.TOKEN_CLASSIFICATION:\n        observed_classes = [c for c in observed_classes if c != 'O']\n        observed_classes = sorted({tag for (tag, _, _) in get_entities(observed_classes)})\n    return (observed_classes, model_classes)"
        ]
    }
]