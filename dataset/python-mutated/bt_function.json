[
    {
        "func_name": "pbt_function",
        "original": "def pbt_function(config):\n    \"\"\"Toy PBT problem for benchmarking adaptive learning rate.\n\n    The goal is to optimize this trainable's accuracy. The accuracy increases\n    fastest at the optimal lr, which is a function of the current accuracy.\n\n    The optimal lr schedule for this problem is the triangle wave as follows.\n    Note that many lr schedules for real models also follow this shape:\n\n     best lr\n      ^\n      |    /      |   /        |  /          | /            ------------> accuracy\n\n    In this problem, using PBT with a population of 2-4 is sufficient to\n    roughly approximate this lr schedule. Higher population sizes will yield\n    faster convergence. Training will not converge without PBT.\n    \"\"\"\n    lr = config['lr']\n    checkpoint_interval = config.get('checkpoint_interval', 1)\n    accuracy = 0.0\n    step = 1\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            with open(os.path.join(checkpoint_dir, 'checkpoint.json'), 'r') as f:\n                checkpoint_dict = json.load(f)\n        accuracy = checkpoint_dict['acc']\n        last_step = checkpoint_dict['step']\n        step = last_step + 1\n    midpoint = 100\n    q_tolerance = 3\n    noise_level = 2\n    while True:\n        if accuracy < midpoint:\n            optimal_lr = 0.01 * accuracy / midpoint\n        else:\n            optimal_lr = 0.01 - 0.01 * (accuracy - midpoint) / midpoint\n        optimal_lr = min(0.01, max(0.001, optimal_lr))\n        q_err = max(lr, optimal_lr) / min(lr, optimal_lr)\n        if q_err < q_tolerance:\n            accuracy += 1.0 / q_err * random.random()\n        elif lr > optimal_lr:\n            accuracy -= (q_err - q_tolerance) * random.random()\n        accuracy += noise_level * np.random.normal()\n        accuracy = max(0, accuracy)\n        metrics = {'mean_accuracy': accuracy, 'cur_lr': lr, 'optimal_lr': optimal_lr, 'q_err': q_err, 'done': accuracy > midpoint * 2}\n        if step % checkpoint_interval == 0:\n            with tempfile.TemporaryDirectory() as tempdir:\n                with open(os.path.join(tempdir, 'checkpoint.json'), 'w') as f:\n                    checkpoint_dict = {'acc': accuracy, 'step': step}\n                    json.dump(checkpoint_dict, f)\n                train.report(metrics, checkpoint=Checkpoint.from_directory(tempdir))\n        else:\n            train.report(metrics)\n        step += 1",
        "mutated": [
            "def pbt_function(config):\n    if False:\n        i = 10\n    \"Toy PBT problem for benchmarking adaptive learning rate.\\n\\n    The goal is to optimize this trainable's accuracy. The accuracy increases\\n    fastest at the optimal lr, which is a function of the current accuracy.\\n\\n    The optimal lr schedule for this problem is the triangle wave as follows.\\n    Note that many lr schedules for real models also follow this shape:\\n\\n     best lr\\n      ^\\n      |    /      |   /        |  /          | /            ------------> accuracy\\n\\n    In this problem, using PBT with a population of 2-4 is sufficient to\\n    roughly approximate this lr schedule. Higher population sizes will yield\\n    faster convergence. Training will not converge without PBT.\\n    \"\n    lr = config['lr']\n    checkpoint_interval = config.get('checkpoint_interval', 1)\n    accuracy = 0.0\n    step = 1\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            with open(os.path.join(checkpoint_dir, 'checkpoint.json'), 'r') as f:\n                checkpoint_dict = json.load(f)\n        accuracy = checkpoint_dict['acc']\n        last_step = checkpoint_dict['step']\n        step = last_step + 1\n    midpoint = 100\n    q_tolerance = 3\n    noise_level = 2\n    while True:\n        if accuracy < midpoint:\n            optimal_lr = 0.01 * accuracy / midpoint\n        else:\n            optimal_lr = 0.01 - 0.01 * (accuracy - midpoint) / midpoint\n        optimal_lr = min(0.01, max(0.001, optimal_lr))\n        q_err = max(lr, optimal_lr) / min(lr, optimal_lr)\n        if q_err < q_tolerance:\n            accuracy += 1.0 / q_err * random.random()\n        elif lr > optimal_lr:\n            accuracy -= (q_err - q_tolerance) * random.random()\n        accuracy += noise_level * np.random.normal()\n        accuracy = max(0, accuracy)\n        metrics = {'mean_accuracy': accuracy, 'cur_lr': lr, 'optimal_lr': optimal_lr, 'q_err': q_err, 'done': accuracy > midpoint * 2}\n        if step % checkpoint_interval == 0:\n            with tempfile.TemporaryDirectory() as tempdir:\n                with open(os.path.join(tempdir, 'checkpoint.json'), 'w') as f:\n                    checkpoint_dict = {'acc': accuracy, 'step': step}\n                    json.dump(checkpoint_dict, f)\n                train.report(metrics, checkpoint=Checkpoint.from_directory(tempdir))\n        else:\n            train.report(metrics)\n        step += 1",
            "def pbt_function(config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Toy PBT problem for benchmarking adaptive learning rate.\\n\\n    The goal is to optimize this trainable's accuracy. The accuracy increases\\n    fastest at the optimal lr, which is a function of the current accuracy.\\n\\n    The optimal lr schedule for this problem is the triangle wave as follows.\\n    Note that many lr schedules for real models also follow this shape:\\n\\n     best lr\\n      ^\\n      |    /      |   /        |  /          | /            ------------> accuracy\\n\\n    In this problem, using PBT with a population of 2-4 is sufficient to\\n    roughly approximate this lr schedule. Higher population sizes will yield\\n    faster convergence. Training will not converge without PBT.\\n    \"\n    lr = config['lr']\n    checkpoint_interval = config.get('checkpoint_interval', 1)\n    accuracy = 0.0\n    step = 1\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            with open(os.path.join(checkpoint_dir, 'checkpoint.json'), 'r') as f:\n                checkpoint_dict = json.load(f)\n        accuracy = checkpoint_dict['acc']\n        last_step = checkpoint_dict['step']\n        step = last_step + 1\n    midpoint = 100\n    q_tolerance = 3\n    noise_level = 2\n    while True:\n        if accuracy < midpoint:\n            optimal_lr = 0.01 * accuracy / midpoint\n        else:\n            optimal_lr = 0.01 - 0.01 * (accuracy - midpoint) / midpoint\n        optimal_lr = min(0.01, max(0.001, optimal_lr))\n        q_err = max(lr, optimal_lr) / min(lr, optimal_lr)\n        if q_err < q_tolerance:\n            accuracy += 1.0 / q_err * random.random()\n        elif lr > optimal_lr:\n            accuracy -= (q_err - q_tolerance) * random.random()\n        accuracy += noise_level * np.random.normal()\n        accuracy = max(0, accuracy)\n        metrics = {'mean_accuracy': accuracy, 'cur_lr': lr, 'optimal_lr': optimal_lr, 'q_err': q_err, 'done': accuracy > midpoint * 2}\n        if step % checkpoint_interval == 0:\n            with tempfile.TemporaryDirectory() as tempdir:\n                with open(os.path.join(tempdir, 'checkpoint.json'), 'w') as f:\n                    checkpoint_dict = {'acc': accuracy, 'step': step}\n                    json.dump(checkpoint_dict, f)\n                train.report(metrics, checkpoint=Checkpoint.from_directory(tempdir))\n        else:\n            train.report(metrics)\n        step += 1",
            "def pbt_function(config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Toy PBT problem for benchmarking adaptive learning rate.\\n\\n    The goal is to optimize this trainable's accuracy. The accuracy increases\\n    fastest at the optimal lr, which is a function of the current accuracy.\\n\\n    The optimal lr schedule for this problem is the triangle wave as follows.\\n    Note that many lr schedules for real models also follow this shape:\\n\\n     best lr\\n      ^\\n      |    /      |   /        |  /          | /            ------------> accuracy\\n\\n    In this problem, using PBT with a population of 2-4 is sufficient to\\n    roughly approximate this lr schedule. Higher population sizes will yield\\n    faster convergence. Training will not converge without PBT.\\n    \"\n    lr = config['lr']\n    checkpoint_interval = config.get('checkpoint_interval', 1)\n    accuracy = 0.0\n    step = 1\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            with open(os.path.join(checkpoint_dir, 'checkpoint.json'), 'r') as f:\n                checkpoint_dict = json.load(f)\n        accuracy = checkpoint_dict['acc']\n        last_step = checkpoint_dict['step']\n        step = last_step + 1\n    midpoint = 100\n    q_tolerance = 3\n    noise_level = 2\n    while True:\n        if accuracy < midpoint:\n            optimal_lr = 0.01 * accuracy / midpoint\n        else:\n            optimal_lr = 0.01 - 0.01 * (accuracy - midpoint) / midpoint\n        optimal_lr = min(0.01, max(0.001, optimal_lr))\n        q_err = max(lr, optimal_lr) / min(lr, optimal_lr)\n        if q_err < q_tolerance:\n            accuracy += 1.0 / q_err * random.random()\n        elif lr > optimal_lr:\n            accuracy -= (q_err - q_tolerance) * random.random()\n        accuracy += noise_level * np.random.normal()\n        accuracy = max(0, accuracy)\n        metrics = {'mean_accuracy': accuracy, 'cur_lr': lr, 'optimal_lr': optimal_lr, 'q_err': q_err, 'done': accuracy > midpoint * 2}\n        if step % checkpoint_interval == 0:\n            with tempfile.TemporaryDirectory() as tempdir:\n                with open(os.path.join(tempdir, 'checkpoint.json'), 'w') as f:\n                    checkpoint_dict = {'acc': accuracy, 'step': step}\n                    json.dump(checkpoint_dict, f)\n                train.report(metrics, checkpoint=Checkpoint.from_directory(tempdir))\n        else:\n            train.report(metrics)\n        step += 1",
            "def pbt_function(config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Toy PBT problem for benchmarking adaptive learning rate.\\n\\n    The goal is to optimize this trainable's accuracy. The accuracy increases\\n    fastest at the optimal lr, which is a function of the current accuracy.\\n\\n    The optimal lr schedule for this problem is the triangle wave as follows.\\n    Note that many lr schedules for real models also follow this shape:\\n\\n     best lr\\n      ^\\n      |    /      |   /        |  /          | /            ------------> accuracy\\n\\n    In this problem, using PBT with a population of 2-4 is sufficient to\\n    roughly approximate this lr schedule. Higher population sizes will yield\\n    faster convergence. Training will not converge without PBT.\\n    \"\n    lr = config['lr']\n    checkpoint_interval = config.get('checkpoint_interval', 1)\n    accuracy = 0.0\n    step = 1\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            with open(os.path.join(checkpoint_dir, 'checkpoint.json'), 'r') as f:\n                checkpoint_dict = json.load(f)\n        accuracy = checkpoint_dict['acc']\n        last_step = checkpoint_dict['step']\n        step = last_step + 1\n    midpoint = 100\n    q_tolerance = 3\n    noise_level = 2\n    while True:\n        if accuracy < midpoint:\n            optimal_lr = 0.01 * accuracy / midpoint\n        else:\n            optimal_lr = 0.01 - 0.01 * (accuracy - midpoint) / midpoint\n        optimal_lr = min(0.01, max(0.001, optimal_lr))\n        q_err = max(lr, optimal_lr) / min(lr, optimal_lr)\n        if q_err < q_tolerance:\n            accuracy += 1.0 / q_err * random.random()\n        elif lr > optimal_lr:\n            accuracy -= (q_err - q_tolerance) * random.random()\n        accuracy += noise_level * np.random.normal()\n        accuracy = max(0, accuracy)\n        metrics = {'mean_accuracy': accuracy, 'cur_lr': lr, 'optimal_lr': optimal_lr, 'q_err': q_err, 'done': accuracy > midpoint * 2}\n        if step % checkpoint_interval == 0:\n            with tempfile.TemporaryDirectory() as tempdir:\n                with open(os.path.join(tempdir, 'checkpoint.json'), 'w') as f:\n                    checkpoint_dict = {'acc': accuracy, 'step': step}\n                    json.dump(checkpoint_dict, f)\n                train.report(metrics, checkpoint=Checkpoint.from_directory(tempdir))\n        else:\n            train.report(metrics)\n        step += 1",
            "def pbt_function(config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Toy PBT problem for benchmarking adaptive learning rate.\\n\\n    The goal is to optimize this trainable's accuracy. The accuracy increases\\n    fastest at the optimal lr, which is a function of the current accuracy.\\n\\n    The optimal lr schedule for this problem is the triangle wave as follows.\\n    Note that many lr schedules for real models also follow this shape:\\n\\n     best lr\\n      ^\\n      |    /      |   /        |  /          | /            ------------> accuracy\\n\\n    In this problem, using PBT with a population of 2-4 is sufficient to\\n    roughly approximate this lr schedule. Higher population sizes will yield\\n    faster convergence. Training will not converge without PBT.\\n    \"\n    lr = config['lr']\n    checkpoint_interval = config.get('checkpoint_interval', 1)\n    accuracy = 0.0\n    step = 1\n    checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            with open(os.path.join(checkpoint_dir, 'checkpoint.json'), 'r') as f:\n                checkpoint_dict = json.load(f)\n        accuracy = checkpoint_dict['acc']\n        last_step = checkpoint_dict['step']\n        step = last_step + 1\n    midpoint = 100\n    q_tolerance = 3\n    noise_level = 2\n    while True:\n        if accuracy < midpoint:\n            optimal_lr = 0.01 * accuracy / midpoint\n        else:\n            optimal_lr = 0.01 - 0.01 * (accuracy - midpoint) / midpoint\n        optimal_lr = min(0.01, max(0.001, optimal_lr))\n        q_err = max(lr, optimal_lr) / min(lr, optimal_lr)\n        if q_err < q_tolerance:\n            accuracy += 1.0 / q_err * random.random()\n        elif lr > optimal_lr:\n            accuracy -= (q_err - q_tolerance) * random.random()\n        accuracy += noise_level * np.random.normal()\n        accuracy = max(0, accuracy)\n        metrics = {'mean_accuracy': accuracy, 'cur_lr': lr, 'optimal_lr': optimal_lr, 'q_err': q_err, 'done': accuracy > midpoint * 2}\n        if step % checkpoint_interval == 0:\n            with tempfile.TemporaryDirectory() as tempdir:\n                with open(os.path.join(tempdir, 'checkpoint.json'), 'w') as f:\n                    checkpoint_dict = {'acc': accuracy, 'step': step}\n                    json.dump(checkpoint_dict, f)\n                train.report(metrics, checkpoint=Checkpoint.from_directory(tempdir))\n        else:\n            train.report(metrics)\n        step += 1"
        ]
    },
    {
        "func_name": "run_tune_pbt",
        "original": "def run_tune_pbt(smoke_test=False):\n    perturbation_interval = 5\n    pbt = PopulationBasedTraining(time_attr='training_iteration', perturbation_interval=perturbation_interval, hyperparam_mutations={'lr': tune.uniform(0.0001, 0.02), 'some_other_factor': [1, 2]})\n    tuner = tune.Tuner(pbt_function, run_config=train.RunConfig(name='pbt_function_api_example', verbose=False, stop={'done': True, 'training_iteration': 10 if smoke_test else 1000}, failure_config=train.FailureConfig(fail_fast=True), checkpoint_config=train.CheckpointConfig(checkpoint_score_attribute='mean_accuracy', num_to_keep=2)), tune_config=tune.TuneConfig(scheduler=pbt, metric='mean_accuracy', mode='max', num_samples=8), param_space={'lr': 0.0001, 'some_other_factor': 1, 'checkpoint_interval': perturbation_interval})\n    results = tuner.fit()\n    print('Best hyperparameters found were: ', results.get_best_result().config)",
        "mutated": [
            "def run_tune_pbt(smoke_test=False):\n    if False:\n        i = 10\n    perturbation_interval = 5\n    pbt = PopulationBasedTraining(time_attr='training_iteration', perturbation_interval=perturbation_interval, hyperparam_mutations={'lr': tune.uniform(0.0001, 0.02), 'some_other_factor': [1, 2]})\n    tuner = tune.Tuner(pbt_function, run_config=train.RunConfig(name='pbt_function_api_example', verbose=False, stop={'done': True, 'training_iteration': 10 if smoke_test else 1000}, failure_config=train.FailureConfig(fail_fast=True), checkpoint_config=train.CheckpointConfig(checkpoint_score_attribute='mean_accuracy', num_to_keep=2)), tune_config=tune.TuneConfig(scheduler=pbt, metric='mean_accuracy', mode='max', num_samples=8), param_space={'lr': 0.0001, 'some_other_factor': 1, 'checkpoint_interval': perturbation_interval})\n    results = tuner.fit()\n    print('Best hyperparameters found were: ', results.get_best_result().config)",
            "def run_tune_pbt(smoke_test=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    perturbation_interval = 5\n    pbt = PopulationBasedTraining(time_attr='training_iteration', perturbation_interval=perturbation_interval, hyperparam_mutations={'lr': tune.uniform(0.0001, 0.02), 'some_other_factor': [1, 2]})\n    tuner = tune.Tuner(pbt_function, run_config=train.RunConfig(name='pbt_function_api_example', verbose=False, stop={'done': True, 'training_iteration': 10 if smoke_test else 1000}, failure_config=train.FailureConfig(fail_fast=True), checkpoint_config=train.CheckpointConfig(checkpoint_score_attribute='mean_accuracy', num_to_keep=2)), tune_config=tune.TuneConfig(scheduler=pbt, metric='mean_accuracy', mode='max', num_samples=8), param_space={'lr': 0.0001, 'some_other_factor': 1, 'checkpoint_interval': perturbation_interval})\n    results = tuner.fit()\n    print('Best hyperparameters found were: ', results.get_best_result().config)",
            "def run_tune_pbt(smoke_test=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    perturbation_interval = 5\n    pbt = PopulationBasedTraining(time_attr='training_iteration', perturbation_interval=perturbation_interval, hyperparam_mutations={'lr': tune.uniform(0.0001, 0.02), 'some_other_factor': [1, 2]})\n    tuner = tune.Tuner(pbt_function, run_config=train.RunConfig(name='pbt_function_api_example', verbose=False, stop={'done': True, 'training_iteration': 10 if smoke_test else 1000}, failure_config=train.FailureConfig(fail_fast=True), checkpoint_config=train.CheckpointConfig(checkpoint_score_attribute='mean_accuracy', num_to_keep=2)), tune_config=tune.TuneConfig(scheduler=pbt, metric='mean_accuracy', mode='max', num_samples=8), param_space={'lr': 0.0001, 'some_other_factor': 1, 'checkpoint_interval': perturbation_interval})\n    results = tuner.fit()\n    print('Best hyperparameters found were: ', results.get_best_result().config)",
            "def run_tune_pbt(smoke_test=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    perturbation_interval = 5\n    pbt = PopulationBasedTraining(time_attr='training_iteration', perturbation_interval=perturbation_interval, hyperparam_mutations={'lr': tune.uniform(0.0001, 0.02), 'some_other_factor': [1, 2]})\n    tuner = tune.Tuner(pbt_function, run_config=train.RunConfig(name='pbt_function_api_example', verbose=False, stop={'done': True, 'training_iteration': 10 if smoke_test else 1000}, failure_config=train.FailureConfig(fail_fast=True), checkpoint_config=train.CheckpointConfig(checkpoint_score_attribute='mean_accuracy', num_to_keep=2)), tune_config=tune.TuneConfig(scheduler=pbt, metric='mean_accuracy', mode='max', num_samples=8), param_space={'lr': 0.0001, 'some_other_factor': 1, 'checkpoint_interval': perturbation_interval})\n    results = tuner.fit()\n    print('Best hyperparameters found were: ', results.get_best_result().config)",
            "def run_tune_pbt(smoke_test=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    perturbation_interval = 5\n    pbt = PopulationBasedTraining(time_attr='training_iteration', perturbation_interval=perturbation_interval, hyperparam_mutations={'lr': tune.uniform(0.0001, 0.02), 'some_other_factor': [1, 2]})\n    tuner = tune.Tuner(pbt_function, run_config=train.RunConfig(name='pbt_function_api_example', verbose=False, stop={'done': True, 'training_iteration': 10 if smoke_test else 1000}, failure_config=train.FailureConfig(fail_fast=True), checkpoint_config=train.CheckpointConfig(checkpoint_score_attribute='mean_accuracy', num_to_keep=2)), tune_config=tune.TuneConfig(scheduler=pbt, metric='mean_accuracy', mode='max', num_samples=8), param_space={'lr': 0.0001, 'some_other_factor': 1, 'checkpoint_interval': perturbation_interval})\n    results = tuner.fit()\n    print('Best hyperparameters found were: ', results.get_best_result().config)"
        ]
    }
]