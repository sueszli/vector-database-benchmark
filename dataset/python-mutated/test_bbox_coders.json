[
    {
        "func_name": "test_partial_bin_based_box_coder",
        "original": "def test_partial_bin_based_box_coder():\n    box_coder_cfg = dict(type='PartialBinBasedBBoxCoder', num_sizes=10, num_dir_bins=12, with_rot=True, mean_sizes=[[2.114256, 1.6203, 0.927272], [0.791118, 1.279516, 0.718182], [0.923508, 1.867419, 0.845495], [0.591958, 0.552978, 0.827272], [0.699104, 0.454178, 0.75625], [0.69519, 1.346299, 0.736364], [0.528526, 1.002642, 1.172878], [0.500618, 0.632163, 0.683424], [0.404671, 1.071108, 1.688889], [0.76584, 1.398258, 0.472728]])\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = DepthInstance3DBoxes([[0.8308, 4.1168, -1.2035, 2.2493, 1.8444, 1.9245, 1.6486], [2.3002, 4.8149, -1.2442, 0.5718, 0.8629, 0.951, 1.603], [-1.1477, 1.809, -1.1725, 0.6965, 1.5273, 2.0563, 0.0552]])\n    gt_labels = torch.tensor([0, 1, 2])\n    (center_target, size_class_target, size_res_target, dir_class_target, dir_res_target) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[0.8308, 4.1168, -0.2413], [2.3002, 4.8149, -0.7687], [-1.1477, 1.809, -0.1444]])\n    expected_size_class_target = torch.tensor([0, 1, 2])\n    expected_size_res_target = torch.tensor([[0.135, 0.2241, 0.9972], [-0.2193, -0.4166, 0.2328], [-0.227, -0.3401, 1.2108]])\n    expected_dir_class_target = torch.tensor([3, 3, 0])\n    expected_dir_res_target = torch.tensor([0.0778, 0.0322, 0.0552])\n    assert torch.allclose(center_target, expected_center_target, atol=0.0001)\n    assert torch.all(size_class_target == expected_size_class_target)\n    assert torch.allclose(size_res_target, expected_size_res_target, atol=0.0001)\n    assert torch.all(dir_class_target == expected_dir_class_target)\n    assert torch.allclose(dir_res_target, expected_dir_res_target, atol=0.0001)\n    center = torch.tensor([[[0.8014, 3.4134, -0.6133], [2.6375, 8.4191, 2.0438], [4.2017, 5.2504, -0.7851], [-1.0088, 5.4107, 1.6293], [1.4837, 4.0268, 0.6222]]])\n    size_class = torch.tensor([[[-1.0061, -2.2788, 1.1322, -4.438, -11.0526, -2.8113, -2.0642, -7.5886, -4.8627, -5.0437], [-2.2058, -0.3527, -1.9976, 0.8815, -2.798, -1.9053, -0.5097, -2.0232, -1.4242, -4.1192], [-1.4783, -0.1009, -1.1537, 0.3052, -4.3147, -2.6529, 0.2729, -0.3755, -2.6479, -3.7548], [-6.1809, -3.5024, -8.3273, 1.1252, -4.3315, -7.8288, -4.6091, -5.8153, 0.748, -10.1396], [-9.0424, -3.7883, -6.0788, -1.8855, -10.2493, -9.7164, -1.0658, -4.1713, 1.1173, -10.6204]]])\n    size_res = torch.tensor([[[[-0.098976, -0.52152, -0.076421], [0.14593, 0.56099, 0.089421], [0.051481, 0.3928, 0.12705], [0.36869, 0.70558, 0.14647], [0.47683, 0.33644, 0.23481], [0.087346, 0.84987, 0.33265], [0.21393, 0.85585, 0.098948], [0.07853, 0.059694, -0.087211], [0.18551, 1.1308, -0.51864], [0.36485, 0.73757, 0.15264]], [[-0.95593, -0.50455, 0.19554], [-0.1087, 0.18025, 0.10228], [-0.082882, -0.43771, 0.092135], [-0.04084, -0.059841, 0.11982], [0.073448, 0.052045, 0.17301], [-0.04044, 0.049532, 0.11266], [0.035857, 0.013564, 0.10212], [-0.10407, -0.059321, 0.092622], [0.0074691, 0.09308, -0.44077], [-0.060121, -0.13381, -0.068083]], [[-0.9397, -0.97823, -0.051075], [-0.12843, -0.18381, 0.071327], [-0.12247, -0.81115, 0.036495], [0.049154, -0.04544, 0.08952], [0.15653, 0.03599, 0.16414], [-0.059621, 0.0049357, 0.14264], [0.00085235, -0.1003, -0.030712], [-0.037255, 0.028996, 0.055545], [0.039298, -0.04742, -0.49147], [-0.11548, -0.15895, -0.039155]], [[-1.8725, -0.74102, 1.0524], [-0.3321, 0.047828, -0.032666], [-0.27949, 0.055541, -0.10059], [-0.085533, 0.1487, -0.16709], [0.38283, 0.26609, 0.21361], [-0.42156, 0.32455, 0.67309], [-0.024336, -0.083366, 0.39913], [0.0082142, 0.048323, -0.15247], [-0.048142, -0.30074, -0.16829], [0.13274, -0.23825, -0.18127]], [[-1.2576, -0.6155, 0.7943], [-0.47222, 1.5634, -0.05946], [-0.35367, 1.3616, -0.16421], [-0.016611, 0.24231, -0.096188], [0.54486, 0.46833, 0.51151], [-0.61755, 1.0292, 1.2458], [-0.068152, 0.24786, 0.95088], [-0.048745, 0.15134, -0.099962], [0.0024485, -0.075991, 0.13545], [0.41608, -0.12093, -0.31643]]]])\n    dir_class = torch.tensor([[[-1.023, -5.1965, -5.2195, 2.403, -2.7661, -7.3399, -1.164, -4.063, -5.294, 0.8245, -3.1869, -6.1743], [-1.9503, -1.694, -0.8716, -1.1494, -0.8196, 0.2862, -0.2921, -0.7894, -0.2481, -0.9916, -1.4304, -1.2466], [-1.7435, -1.2043, -0.1265, 0.5083, -0.0717, -0.956, -1.6171, -2.6463, -2.3863, -2.1358, -1.8812, -2.3117], [-1.9282, 0.3792, -1.8426, -1.4587, -0.8582, -3.4639, -3.2133, -3.7867, -7.6781, -6.4459, -6.2455, -5.4797], [-3.1869, 0.4456, -0.5824, 0.9994, -1.0554, -8.4232, -7.7019, -7.1382, -10.2724, -7.8229, -8.186, -8.6194]]])\n    dir_res = torch.tensor([[[0.11022, -0.2375, 0.20381, 0.12177, -0.28501, 0.15351, 0.12218, -0.20677, 0.14468, 0.11593, -0.26864, 0.1129], [-0.015788, 0.041538, -0.00022857, -0.014011, 0.04256, -0.0031186, -0.050343, 0.006811, -0.026728, -0.032781, 0.036889, -0.0015609], [0.019004, 0.0057105, 0.060329, 0.013074, -0.025546, -0.011456, -0.032484, -0.033487, 0.0016609, 0.017095, 1.2647e-05, 0.024814], [0.14482, -0.063083, 0.058307, 0.091396, -0.084571, 0.04589, 0.056243, -0.12448, -0.095244, 0.045746, -0.01739, 0.090267], [0.18065, -0.020078, 0.085401, 0.10784, -0.12495, 0.022796, 0.1131, -0.084364, -0.11904, 0.06118, -0.018109, 0.11229]]])\n    bbox_out = dict(center=center, size_class=size_class, size_res=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[0.8014, 3.4134, -0.6133, 0.975, 2.2602, 0.9725, 1.6926], [2.6375, 8.4191, 2.0438, 0.5511, 0.4931, 0.9471, 2.6149], [4.2017, 5.2504, -0.7851, 0.6411, 0.5075, 0.9168, 1.5839], [-1.0088, 5.4107, 1.6293, 0.5064, 0.7017, 0.6602, 0.4605], [1.4837, 4.0268, 0.6222, 0.4071, 0.9951, 1.8243, 1.6786]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 12, 256)\n    reg_preds = torch.rand(2, 67, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size_class = results['size_class']\n    size_res_norm = results['size_res_norm']\n    size_res = results['size_res']\n    sem_scores = results['sem_scores']\n    assert obj_scores.shape == torch.Size([2, 256, 2])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size_class.shape == torch.Size([2, 256, 10])\n    assert size_res_norm.shape == torch.Size([2, 256, 10, 3])\n    assert size_res.shape == torch.Size([2, 256, 10, 3])\n    assert sem_scores.shape == torch.Size([2, 256, 10])",
        "mutated": [
            "def test_partial_bin_based_box_coder():\n    if False:\n        i = 10\n    box_coder_cfg = dict(type='PartialBinBasedBBoxCoder', num_sizes=10, num_dir_bins=12, with_rot=True, mean_sizes=[[2.114256, 1.6203, 0.927272], [0.791118, 1.279516, 0.718182], [0.923508, 1.867419, 0.845495], [0.591958, 0.552978, 0.827272], [0.699104, 0.454178, 0.75625], [0.69519, 1.346299, 0.736364], [0.528526, 1.002642, 1.172878], [0.500618, 0.632163, 0.683424], [0.404671, 1.071108, 1.688889], [0.76584, 1.398258, 0.472728]])\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = DepthInstance3DBoxes([[0.8308, 4.1168, -1.2035, 2.2493, 1.8444, 1.9245, 1.6486], [2.3002, 4.8149, -1.2442, 0.5718, 0.8629, 0.951, 1.603], [-1.1477, 1.809, -1.1725, 0.6965, 1.5273, 2.0563, 0.0552]])\n    gt_labels = torch.tensor([0, 1, 2])\n    (center_target, size_class_target, size_res_target, dir_class_target, dir_res_target) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[0.8308, 4.1168, -0.2413], [2.3002, 4.8149, -0.7687], [-1.1477, 1.809, -0.1444]])\n    expected_size_class_target = torch.tensor([0, 1, 2])\n    expected_size_res_target = torch.tensor([[0.135, 0.2241, 0.9972], [-0.2193, -0.4166, 0.2328], [-0.227, -0.3401, 1.2108]])\n    expected_dir_class_target = torch.tensor([3, 3, 0])\n    expected_dir_res_target = torch.tensor([0.0778, 0.0322, 0.0552])\n    assert torch.allclose(center_target, expected_center_target, atol=0.0001)\n    assert torch.all(size_class_target == expected_size_class_target)\n    assert torch.allclose(size_res_target, expected_size_res_target, atol=0.0001)\n    assert torch.all(dir_class_target == expected_dir_class_target)\n    assert torch.allclose(dir_res_target, expected_dir_res_target, atol=0.0001)\n    center = torch.tensor([[[0.8014, 3.4134, -0.6133], [2.6375, 8.4191, 2.0438], [4.2017, 5.2504, -0.7851], [-1.0088, 5.4107, 1.6293], [1.4837, 4.0268, 0.6222]]])\n    size_class = torch.tensor([[[-1.0061, -2.2788, 1.1322, -4.438, -11.0526, -2.8113, -2.0642, -7.5886, -4.8627, -5.0437], [-2.2058, -0.3527, -1.9976, 0.8815, -2.798, -1.9053, -0.5097, -2.0232, -1.4242, -4.1192], [-1.4783, -0.1009, -1.1537, 0.3052, -4.3147, -2.6529, 0.2729, -0.3755, -2.6479, -3.7548], [-6.1809, -3.5024, -8.3273, 1.1252, -4.3315, -7.8288, -4.6091, -5.8153, 0.748, -10.1396], [-9.0424, -3.7883, -6.0788, -1.8855, -10.2493, -9.7164, -1.0658, -4.1713, 1.1173, -10.6204]]])\n    size_res = torch.tensor([[[[-0.098976, -0.52152, -0.076421], [0.14593, 0.56099, 0.089421], [0.051481, 0.3928, 0.12705], [0.36869, 0.70558, 0.14647], [0.47683, 0.33644, 0.23481], [0.087346, 0.84987, 0.33265], [0.21393, 0.85585, 0.098948], [0.07853, 0.059694, -0.087211], [0.18551, 1.1308, -0.51864], [0.36485, 0.73757, 0.15264]], [[-0.95593, -0.50455, 0.19554], [-0.1087, 0.18025, 0.10228], [-0.082882, -0.43771, 0.092135], [-0.04084, -0.059841, 0.11982], [0.073448, 0.052045, 0.17301], [-0.04044, 0.049532, 0.11266], [0.035857, 0.013564, 0.10212], [-0.10407, -0.059321, 0.092622], [0.0074691, 0.09308, -0.44077], [-0.060121, -0.13381, -0.068083]], [[-0.9397, -0.97823, -0.051075], [-0.12843, -0.18381, 0.071327], [-0.12247, -0.81115, 0.036495], [0.049154, -0.04544, 0.08952], [0.15653, 0.03599, 0.16414], [-0.059621, 0.0049357, 0.14264], [0.00085235, -0.1003, -0.030712], [-0.037255, 0.028996, 0.055545], [0.039298, -0.04742, -0.49147], [-0.11548, -0.15895, -0.039155]], [[-1.8725, -0.74102, 1.0524], [-0.3321, 0.047828, -0.032666], [-0.27949, 0.055541, -0.10059], [-0.085533, 0.1487, -0.16709], [0.38283, 0.26609, 0.21361], [-0.42156, 0.32455, 0.67309], [-0.024336, -0.083366, 0.39913], [0.0082142, 0.048323, -0.15247], [-0.048142, -0.30074, -0.16829], [0.13274, -0.23825, -0.18127]], [[-1.2576, -0.6155, 0.7943], [-0.47222, 1.5634, -0.05946], [-0.35367, 1.3616, -0.16421], [-0.016611, 0.24231, -0.096188], [0.54486, 0.46833, 0.51151], [-0.61755, 1.0292, 1.2458], [-0.068152, 0.24786, 0.95088], [-0.048745, 0.15134, -0.099962], [0.0024485, -0.075991, 0.13545], [0.41608, -0.12093, -0.31643]]]])\n    dir_class = torch.tensor([[[-1.023, -5.1965, -5.2195, 2.403, -2.7661, -7.3399, -1.164, -4.063, -5.294, 0.8245, -3.1869, -6.1743], [-1.9503, -1.694, -0.8716, -1.1494, -0.8196, 0.2862, -0.2921, -0.7894, -0.2481, -0.9916, -1.4304, -1.2466], [-1.7435, -1.2043, -0.1265, 0.5083, -0.0717, -0.956, -1.6171, -2.6463, -2.3863, -2.1358, -1.8812, -2.3117], [-1.9282, 0.3792, -1.8426, -1.4587, -0.8582, -3.4639, -3.2133, -3.7867, -7.6781, -6.4459, -6.2455, -5.4797], [-3.1869, 0.4456, -0.5824, 0.9994, -1.0554, -8.4232, -7.7019, -7.1382, -10.2724, -7.8229, -8.186, -8.6194]]])\n    dir_res = torch.tensor([[[0.11022, -0.2375, 0.20381, 0.12177, -0.28501, 0.15351, 0.12218, -0.20677, 0.14468, 0.11593, -0.26864, 0.1129], [-0.015788, 0.041538, -0.00022857, -0.014011, 0.04256, -0.0031186, -0.050343, 0.006811, -0.026728, -0.032781, 0.036889, -0.0015609], [0.019004, 0.0057105, 0.060329, 0.013074, -0.025546, -0.011456, -0.032484, -0.033487, 0.0016609, 0.017095, 1.2647e-05, 0.024814], [0.14482, -0.063083, 0.058307, 0.091396, -0.084571, 0.04589, 0.056243, -0.12448, -0.095244, 0.045746, -0.01739, 0.090267], [0.18065, -0.020078, 0.085401, 0.10784, -0.12495, 0.022796, 0.1131, -0.084364, -0.11904, 0.06118, -0.018109, 0.11229]]])\n    bbox_out = dict(center=center, size_class=size_class, size_res=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[0.8014, 3.4134, -0.6133, 0.975, 2.2602, 0.9725, 1.6926], [2.6375, 8.4191, 2.0438, 0.5511, 0.4931, 0.9471, 2.6149], [4.2017, 5.2504, -0.7851, 0.6411, 0.5075, 0.9168, 1.5839], [-1.0088, 5.4107, 1.6293, 0.5064, 0.7017, 0.6602, 0.4605], [1.4837, 4.0268, 0.6222, 0.4071, 0.9951, 1.8243, 1.6786]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 12, 256)\n    reg_preds = torch.rand(2, 67, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size_class = results['size_class']\n    size_res_norm = results['size_res_norm']\n    size_res = results['size_res']\n    sem_scores = results['sem_scores']\n    assert obj_scores.shape == torch.Size([2, 256, 2])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size_class.shape == torch.Size([2, 256, 10])\n    assert size_res_norm.shape == torch.Size([2, 256, 10, 3])\n    assert size_res.shape == torch.Size([2, 256, 10, 3])\n    assert sem_scores.shape == torch.Size([2, 256, 10])",
            "def test_partial_bin_based_box_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    box_coder_cfg = dict(type='PartialBinBasedBBoxCoder', num_sizes=10, num_dir_bins=12, with_rot=True, mean_sizes=[[2.114256, 1.6203, 0.927272], [0.791118, 1.279516, 0.718182], [0.923508, 1.867419, 0.845495], [0.591958, 0.552978, 0.827272], [0.699104, 0.454178, 0.75625], [0.69519, 1.346299, 0.736364], [0.528526, 1.002642, 1.172878], [0.500618, 0.632163, 0.683424], [0.404671, 1.071108, 1.688889], [0.76584, 1.398258, 0.472728]])\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = DepthInstance3DBoxes([[0.8308, 4.1168, -1.2035, 2.2493, 1.8444, 1.9245, 1.6486], [2.3002, 4.8149, -1.2442, 0.5718, 0.8629, 0.951, 1.603], [-1.1477, 1.809, -1.1725, 0.6965, 1.5273, 2.0563, 0.0552]])\n    gt_labels = torch.tensor([0, 1, 2])\n    (center_target, size_class_target, size_res_target, dir_class_target, dir_res_target) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[0.8308, 4.1168, -0.2413], [2.3002, 4.8149, -0.7687], [-1.1477, 1.809, -0.1444]])\n    expected_size_class_target = torch.tensor([0, 1, 2])\n    expected_size_res_target = torch.tensor([[0.135, 0.2241, 0.9972], [-0.2193, -0.4166, 0.2328], [-0.227, -0.3401, 1.2108]])\n    expected_dir_class_target = torch.tensor([3, 3, 0])\n    expected_dir_res_target = torch.tensor([0.0778, 0.0322, 0.0552])\n    assert torch.allclose(center_target, expected_center_target, atol=0.0001)\n    assert torch.all(size_class_target == expected_size_class_target)\n    assert torch.allclose(size_res_target, expected_size_res_target, atol=0.0001)\n    assert torch.all(dir_class_target == expected_dir_class_target)\n    assert torch.allclose(dir_res_target, expected_dir_res_target, atol=0.0001)\n    center = torch.tensor([[[0.8014, 3.4134, -0.6133], [2.6375, 8.4191, 2.0438], [4.2017, 5.2504, -0.7851], [-1.0088, 5.4107, 1.6293], [1.4837, 4.0268, 0.6222]]])\n    size_class = torch.tensor([[[-1.0061, -2.2788, 1.1322, -4.438, -11.0526, -2.8113, -2.0642, -7.5886, -4.8627, -5.0437], [-2.2058, -0.3527, -1.9976, 0.8815, -2.798, -1.9053, -0.5097, -2.0232, -1.4242, -4.1192], [-1.4783, -0.1009, -1.1537, 0.3052, -4.3147, -2.6529, 0.2729, -0.3755, -2.6479, -3.7548], [-6.1809, -3.5024, -8.3273, 1.1252, -4.3315, -7.8288, -4.6091, -5.8153, 0.748, -10.1396], [-9.0424, -3.7883, -6.0788, -1.8855, -10.2493, -9.7164, -1.0658, -4.1713, 1.1173, -10.6204]]])\n    size_res = torch.tensor([[[[-0.098976, -0.52152, -0.076421], [0.14593, 0.56099, 0.089421], [0.051481, 0.3928, 0.12705], [0.36869, 0.70558, 0.14647], [0.47683, 0.33644, 0.23481], [0.087346, 0.84987, 0.33265], [0.21393, 0.85585, 0.098948], [0.07853, 0.059694, -0.087211], [0.18551, 1.1308, -0.51864], [0.36485, 0.73757, 0.15264]], [[-0.95593, -0.50455, 0.19554], [-0.1087, 0.18025, 0.10228], [-0.082882, -0.43771, 0.092135], [-0.04084, -0.059841, 0.11982], [0.073448, 0.052045, 0.17301], [-0.04044, 0.049532, 0.11266], [0.035857, 0.013564, 0.10212], [-0.10407, -0.059321, 0.092622], [0.0074691, 0.09308, -0.44077], [-0.060121, -0.13381, -0.068083]], [[-0.9397, -0.97823, -0.051075], [-0.12843, -0.18381, 0.071327], [-0.12247, -0.81115, 0.036495], [0.049154, -0.04544, 0.08952], [0.15653, 0.03599, 0.16414], [-0.059621, 0.0049357, 0.14264], [0.00085235, -0.1003, -0.030712], [-0.037255, 0.028996, 0.055545], [0.039298, -0.04742, -0.49147], [-0.11548, -0.15895, -0.039155]], [[-1.8725, -0.74102, 1.0524], [-0.3321, 0.047828, -0.032666], [-0.27949, 0.055541, -0.10059], [-0.085533, 0.1487, -0.16709], [0.38283, 0.26609, 0.21361], [-0.42156, 0.32455, 0.67309], [-0.024336, -0.083366, 0.39913], [0.0082142, 0.048323, -0.15247], [-0.048142, -0.30074, -0.16829], [0.13274, -0.23825, -0.18127]], [[-1.2576, -0.6155, 0.7943], [-0.47222, 1.5634, -0.05946], [-0.35367, 1.3616, -0.16421], [-0.016611, 0.24231, -0.096188], [0.54486, 0.46833, 0.51151], [-0.61755, 1.0292, 1.2458], [-0.068152, 0.24786, 0.95088], [-0.048745, 0.15134, -0.099962], [0.0024485, -0.075991, 0.13545], [0.41608, -0.12093, -0.31643]]]])\n    dir_class = torch.tensor([[[-1.023, -5.1965, -5.2195, 2.403, -2.7661, -7.3399, -1.164, -4.063, -5.294, 0.8245, -3.1869, -6.1743], [-1.9503, -1.694, -0.8716, -1.1494, -0.8196, 0.2862, -0.2921, -0.7894, -0.2481, -0.9916, -1.4304, -1.2466], [-1.7435, -1.2043, -0.1265, 0.5083, -0.0717, -0.956, -1.6171, -2.6463, -2.3863, -2.1358, -1.8812, -2.3117], [-1.9282, 0.3792, -1.8426, -1.4587, -0.8582, -3.4639, -3.2133, -3.7867, -7.6781, -6.4459, -6.2455, -5.4797], [-3.1869, 0.4456, -0.5824, 0.9994, -1.0554, -8.4232, -7.7019, -7.1382, -10.2724, -7.8229, -8.186, -8.6194]]])\n    dir_res = torch.tensor([[[0.11022, -0.2375, 0.20381, 0.12177, -0.28501, 0.15351, 0.12218, -0.20677, 0.14468, 0.11593, -0.26864, 0.1129], [-0.015788, 0.041538, -0.00022857, -0.014011, 0.04256, -0.0031186, -0.050343, 0.006811, -0.026728, -0.032781, 0.036889, -0.0015609], [0.019004, 0.0057105, 0.060329, 0.013074, -0.025546, -0.011456, -0.032484, -0.033487, 0.0016609, 0.017095, 1.2647e-05, 0.024814], [0.14482, -0.063083, 0.058307, 0.091396, -0.084571, 0.04589, 0.056243, -0.12448, -0.095244, 0.045746, -0.01739, 0.090267], [0.18065, -0.020078, 0.085401, 0.10784, -0.12495, 0.022796, 0.1131, -0.084364, -0.11904, 0.06118, -0.018109, 0.11229]]])\n    bbox_out = dict(center=center, size_class=size_class, size_res=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[0.8014, 3.4134, -0.6133, 0.975, 2.2602, 0.9725, 1.6926], [2.6375, 8.4191, 2.0438, 0.5511, 0.4931, 0.9471, 2.6149], [4.2017, 5.2504, -0.7851, 0.6411, 0.5075, 0.9168, 1.5839], [-1.0088, 5.4107, 1.6293, 0.5064, 0.7017, 0.6602, 0.4605], [1.4837, 4.0268, 0.6222, 0.4071, 0.9951, 1.8243, 1.6786]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 12, 256)\n    reg_preds = torch.rand(2, 67, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size_class = results['size_class']\n    size_res_norm = results['size_res_norm']\n    size_res = results['size_res']\n    sem_scores = results['sem_scores']\n    assert obj_scores.shape == torch.Size([2, 256, 2])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size_class.shape == torch.Size([2, 256, 10])\n    assert size_res_norm.shape == torch.Size([2, 256, 10, 3])\n    assert size_res.shape == torch.Size([2, 256, 10, 3])\n    assert sem_scores.shape == torch.Size([2, 256, 10])",
            "def test_partial_bin_based_box_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    box_coder_cfg = dict(type='PartialBinBasedBBoxCoder', num_sizes=10, num_dir_bins=12, with_rot=True, mean_sizes=[[2.114256, 1.6203, 0.927272], [0.791118, 1.279516, 0.718182], [0.923508, 1.867419, 0.845495], [0.591958, 0.552978, 0.827272], [0.699104, 0.454178, 0.75625], [0.69519, 1.346299, 0.736364], [0.528526, 1.002642, 1.172878], [0.500618, 0.632163, 0.683424], [0.404671, 1.071108, 1.688889], [0.76584, 1.398258, 0.472728]])\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = DepthInstance3DBoxes([[0.8308, 4.1168, -1.2035, 2.2493, 1.8444, 1.9245, 1.6486], [2.3002, 4.8149, -1.2442, 0.5718, 0.8629, 0.951, 1.603], [-1.1477, 1.809, -1.1725, 0.6965, 1.5273, 2.0563, 0.0552]])\n    gt_labels = torch.tensor([0, 1, 2])\n    (center_target, size_class_target, size_res_target, dir_class_target, dir_res_target) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[0.8308, 4.1168, -0.2413], [2.3002, 4.8149, -0.7687], [-1.1477, 1.809, -0.1444]])\n    expected_size_class_target = torch.tensor([0, 1, 2])\n    expected_size_res_target = torch.tensor([[0.135, 0.2241, 0.9972], [-0.2193, -0.4166, 0.2328], [-0.227, -0.3401, 1.2108]])\n    expected_dir_class_target = torch.tensor([3, 3, 0])\n    expected_dir_res_target = torch.tensor([0.0778, 0.0322, 0.0552])\n    assert torch.allclose(center_target, expected_center_target, atol=0.0001)\n    assert torch.all(size_class_target == expected_size_class_target)\n    assert torch.allclose(size_res_target, expected_size_res_target, atol=0.0001)\n    assert torch.all(dir_class_target == expected_dir_class_target)\n    assert torch.allclose(dir_res_target, expected_dir_res_target, atol=0.0001)\n    center = torch.tensor([[[0.8014, 3.4134, -0.6133], [2.6375, 8.4191, 2.0438], [4.2017, 5.2504, -0.7851], [-1.0088, 5.4107, 1.6293], [1.4837, 4.0268, 0.6222]]])\n    size_class = torch.tensor([[[-1.0061, -2.2788, 1.1322, -4.438, -11.0526, -2.8113, -2.0642, -7.5886, -4.8627, -5.0437], [-2.2058, -0.3527, -1.9976, 0.8815, -2.798, -1.9053, -0.5097, -2.0232, -1.4242, -4.1192], [-1.4783, -0.1009, -1.1537, 0.3052, -4.3147, -2.6529, 0.2729, -0.3755, -2.6479, -3.7548], [-6.1809, -3.5024, -8.3273, 1.1252, -4.3315, -7.8288, -4.6091, -5.8153, 0.748, -10.1396], [-9.0424, -3.7883, -6.0788, -1.8855, -10.2493, -9.7164, -1.0658, -4.1713, 1.1173, -10.6204]]])\n    size_res = torch.tensor([[[[-0.098976, -0.52152, -0.076421], [0.14593, 0.56099, 0.089421], [0.051481, 0.3928, 0.12705], [0.36869, 0.70558, 0.14647], [0.47683, 0.33644, 0.23481], [0.087346, 0.84987, 0.33265], [0.21393, 0.85585, 0.098948], [0.07853, 0.059694, -0.087211], [0.18551, 1.1308, -0.51864], [0.36485, 0.73757, 0.15264]], [[-0.95593, -0.50455, 0.19554], [-0.1087, 0.18025, 0.10228], [-0.082882, -0.43771, 0.092135], [-0.04084, -0.059841, 0.11982], [0.073448, 0.052045, 0.17301], [-0.04044, 0.049532, 0.11266], [0.035857, 0.013564, 0.10212], [-0.10407, -0.059321, 0.092622], [0.0074691, 0.09308, -0.44077], [-0.060121, -0.13381, -0.068083]], [[-0.9397, -0.97823, -0.051075], [-0.12843, -0.18381, 0.071327], [-0.12247, -0.81115, 0.036495], [0.049154, -0.04544, 0.08952], [0.15653, 0.03599, 0.16414], [-0.059621, 0.0049357, 0.14264], [0.00085235, -0.1003, -0.030712], [-0.037255, 0.028996, 0.055545], [0.039298, -0.04742, -0.49147], [-0.11548, -0.15895, -0.039155]], [[-1.8725, -0.74102, 1.0524], [-0.3321, 0.047828, -0.032666], [-0.27949, 0.055541, -0.10059], [-0.085533, 0.1487, -0.16709], [0.38283, 0.26609, 0.21361], [-0.42156, 0.32455, 0.67309], [-0.024336, -0.083366, 0.39913], [0.0082142, 0.048323, -0.15247], [-0.048142, -0.30074, -0.16829], [0.13274, -0.23825, -0.18127]], [[-1.2576, -0.6155, 0.7943], [-0.47222, 1.5634, -0.05946], [-0.35367, 1.3616, -0.16421], [-0.016611, 0.24231, -0.096188], [0.54486, 0.46833, 0.51151], [-0.61755, 1.0292, 1.2458], [-0.068152, 0.24786, 0.95088], [-0.048745, 0.15134, -0.099962], [0.0024485, -0.075991, 0.13545], [0.41608, -0.12093, -0.31643]]]])\n    dir_class = torch.tensor([[[-1.023, -5.1965, -5.2195, 2.403, -2.7661, -7.3399, -1.164, -4.063, -5.294, 0.8245, -3.1869, -6.1743], [-1.9503, -1.694, -0.8716, -1.1494, -0.8196, 0.2862, -0.2921, -0.7894, -0.2481, -0.9916, -1.4304, -1.2466], [-1.7435, -1.2043, -0.1265, 0.5083, -0.0717, -0.956, -1.6171, -2.6463, -2.3863, -2.1358, -1.8812, -2.3117], [-1.9282, 0.3792, -1.8426, -1.4587, -0.8582, -3.4639, -3.2133, -3.7867, -7.6781, -6.4459, -6.2455, -5.4797], [-3.1869, 0.4456, -0.5824, 0.9994, -1.0554, -8.4232, -7.7019, -7.1382, -10.2724, -7.8229, -8.186, -8.6194]]])\n    dir_res = torch.tensor([[[0.11022, -0.2375, 0.20381, 0.12177, -0.28501, 0.15351, 0.12218, -0.20677, 0.14468, 0.11593, -0.26864, 0.1129], [-0.015788, 0.041538, -0.00022857, -0.014011, 0.04256, -0.0031186, -0.050343, 0.006811, -0.026728, -0.032781, 0.036889, -0.0015609], [0.019004, 0.0057105, 0.060329, 0.013074, -0.025546, -0.011456, -0.032484, -0.033487, 0.0016609, 0.017095, 1.2647e-05, 0.024814], [0.14482, -0.063083, 0.058307, 0.091396, -0.084571, 0.04589, 0.056243, -0.12448, -0.095244, 0.045746, -0.01739, 0.090267], [0.18065, -0.020078, 0.085401, 0.10784, -0.12495, 0.022796, 0.1131, -0.084364, -0.11904, 0.06118, -0.018109, 0.11229]]])\n    bbox_out = dict(center=center, size_class=size_class, size_res=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[0.8014, 3.4134, -0.6133, 0.975, 2.2602, 0.9725, 1.6926], [2.6375, 8.4191, 2.0438, 0.5511, 0.4931, 0.9471, 2.6149], [4.2017, 5.2504, -0.7851, 0.6411, 0.5075, 0.9168, 1.5839], [-1.0088, 5.4107, 1.6293, 0.5064, 0.7017, 0.6602, 0.4605], [1.4837, 4.0268, 0.6222, 0.4071, 0.9951, 1.8243, 1.6786]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 12, 256)\n    reg_preds = torch.rand(2, 67, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size_class = results['size_class']\n    size_res_norm = results['size_res_norm']\n    size_res = results['size_res']\n    sem_scores = results['sem_scores']\n    assert obj_scores.shape == torch.Size([2, 256, 2])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size_class.shape == torch.Size([2, 256, 10])\n    assert size_res_norm.shape == torch.Size([2, 256, 10, 3])\n    assert size_res.shape == torch.Size([2, 256, 10, 3])\n    assert sem_scores.shape == torch.Size([2, 256, 10])",
            "def test_partial_bin_based_box_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    box_coder_cfg = dict(type='PartialBinBasedBBoxCoder', num_sizes=10, num_dir_bins=12, with_rot=True, mean_sizes=[[2.114256, 1.6203, 0.927272], [0.791118, 1.279516, 0.718182], [0.923508, 1.867419, 0.845495], [0.591958, 0.552978, 0.827272], [0.699104, 0.454178, 0.75625], [0.69519, 1.346299, 0.736364], [0.528526, 1.002642, 1.172878], [0.500618, 0.632163, 0.683424], [0.404671, 1.071108, 1.688889], [0.76584, 1.398258, 0.472728]])\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = DepthInstance3DBoxes([[0.8308, 4.1168, -1.2035, 2.2493, 1.8444, 1.9245, 1.6486], [2.3002, 4.8149, -1.2442, 0.5718, 0.8629, 0.951, 1.603], [-1.1477, 1.809, -1.1725, 0.6965, 1.5273, 2.0563, 0.0552]])\n    gt_labels = torch.tensor([0, 1, 2])\n    (center_target, size_class_target, size_res_target, dir_class_target, dir_res_target) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[0.8308, 4.1168, -0.2413], [2.3002, 4.8149, -0.7687], [-1.1477, 1.809, -0.1444]])\n    expected_size_class_target = torch.tensor([0, 1, 2])\n    expected_size_res_target = torch.tensor([[0.135, 0.2241, 0.9972], [-0.2193, -0.4166, 0.2328], [-0.227, -0.3401, 1.2108]])\n    expected_dir_class_target = torch.tensor([3, 3, 0])\n    expected_dir_res_target = torch.tensor([0.0778, 0.0322, 0.0552])\n    assert torch.allclose(center_target, expected_center_target, atol=0.0001)\n    assert torch.all(size_class_target == expected_size_class_target)\n    assert torch.allclose(size_res_target, expected_size_res_target, atol=0.0001)\n    assert torch.all(dir_class_target == expected_dir_class_target)\n    assert torch.allclose(dir_res_target, expected_dir_res_target, atol=0.0001)\n    center = torch.tensor([[[0.8014, 3.4134, -0.6133], [2.6375, 8.4191, 2.0438], [4.2017, 5.2504, -0.7851], [-1.0088, 5.4107, 1.6293], [1.4837, 4.0268, 0.6222]]])\n    size_class = torch.tensor([[[-1.0061, -2.2788, 1.1322, -4.438, -11.0526, -2.8113, -2.0642, -7.5886, -4.8627, -5.0437], [-2.2058, -0.3527, -1.9976, 0.8815, -2.798, -1.9053, -0.5097, -2.0232, -1.4242, -4.1192], [-1.4783, -0.1009, -1.1537, 0.3052, -4.3147, -2.6529, 0.2729, -0.3755, -2.6479, -3.7548], [-6.1809, -3.5024, -8.3273, 1.1252, -4.3315, -7.8288, -4.6091, -5.8153, 0.748, -10.1396], [-9.0424, -3.7883, -6.0788, -1.8855, -10.2493, -9.7164, -1.0658, -4.1713, 1.1173, -10.6204]]])\n    size_res = torch.tensor([[[[-0.098976, -0.52152, -0.076421], [0.14593, 0.56099, 0.089421], [0.051481, 0.3928, 0.12705], [0.36869, 0.70558, 0.14647], [0.47683, 0.33644, 0.23481], [0.087346, 0.84987, 0.33265], [0.21393, 0.85585, 0.098948], [0.07853, 0.059694, -0.087211], [0.18551, 1.1308, -0.51864], [0.36485, 0.73757, 0.15264]], [[-0.95593, -0.50455, 0.19554], [-0.1087, 0.18025, 0.10228], [-0.082882, -0.43771, 0.092135], [-0.04084, -0.059841, 0.11982], [0.073448, 0.052045, 0.17301], [-0.04044, 0.049532, 0.11266], [0.035857, 0.013564, 0.10212], [-0.10407, -0.059321, 0.092622], [0.0074691, 0.09308, -0.44077], [-0.060121, -0.13381, -0.068083]], [[-0.9397, -0.97823, -0.051075], [-0.12843, -0.18381, 0.071327], [-0.12247, -0.81115, 0.036495], [0.049154, -0.04544, 0.08952], [0.15653, 0.03599, 0.16414], [-0.059621, 0.0049357, 0.14264], [0.00085235, -0.1003, -0.030712], [-0.037255, 0.028996, 0.055545], [0.039298, -0.04742, -0.49147], [-0.11548, -0.15895, -0.039155]], [[-1.8725, -0.74102, 1.0524], [-0.3321, 0.047828, -0.032666], [-0.27949, 0.055541, -0.10059], [-0.085533, 0.1487, -0.16709], [0.38283, 0.26609, 0.21361], [-0.42156, 0.32455, 0.67309], [-0.024336, -0.083366, 0.39913], [0.0082142, 0.048323, -0.15247], [-0.048142, -0.30074, -0.16829], [0.13274, -0.23825, -0.18127]], [[-1.2576, -0.6155, 0.7943], [-0.47222, 1.5634, -0.05946], [-0.35367, 1.3616, -0.16421], [-0.016611, 0.24231, -0.096188], [0.54486, 0.46833, 0.51151], [-0.61755, 1.0292, 1.2458], [-0.068152, 0.24786, 0.95088], [-0.048745, 0.15134, -0.099962], [0.0024485, -0.075991, 0.13545], [0.41608, -0.12093, -0.31643]]]])\n    dir_class = torch.tensor([[[-1.023, -5.1965, -5.2195, 2.403, -2.7661, -7.3399, -1.164, -4.063, -5.294, 0.8245, -3.1869, -6.1743], [-1.9503, -1.694, -0.8716, -1.1494, -0.8196, 0.2862, -0.2921, -0.7894, -0.2481, -0.9916, -1.4304, -1.2466], [-1.7435, -1.2043, -0.1265, 0.5083, -0.0717, -0.956, -1.6171, -2.6463, -2.3863, -2.1358, -1.8812, -2.3117], [-1.9282, 0.3792, -1.8426, -1.4587, -0.8582, -3.4639, -3.2133, -3.7867, -7.6781, -6.4459, -6.2455, -5.4797], [-3.1869, 0.4456, -0.5824, 0.9994, -1.0554, -8.4232, -7.7019, -7.1382, -10.2724, -7.8229, -8.186, -8.6194]]])\n    dir_res = torch.tensor([[[0.11022, -0.2375, 0.20381, 0.12177, -0.28501, 0.15351, 0.12218, -0.20677, 0.14468, 0.11593, -0.26864, 0.1129], [-0.015788, 0.041538, -0.00022857, -0.014011, 0.04256, -0.0031186, -0.050343, 0.006811, -0.026728, -0.032781, 0.036889, -0.0015609], [0.019004, 0.0057105, 0.060329, 0.013074, -0.025546, -0.011456, -0.032484, -0.033487, 0.0016609, 0.017095, 1.2647e-05, 0.024814], [0.14482, -0.063083, 0.058307, 0.091396, -0.084571, 0.04589, 0.056243, -0.12448, -0.095244, 0.045746, -0.01739, 0.090267], [0.18065, -0.020078, 0.085401, 0.10784, -0.12495, 0.022796, 0.1131, -0.084364, -0.11904, 0.06118, -0.018109, 0.11229]]])\n    bbox_out = dict(center=center, size_class=size_class, size_res=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[0.8014, 3.4134, -0.6133, 0.975, 2.2602, 0.9725, 1.6926], [2.6375, 8.4191, 2.0438, 0.5511, 0.4931, 0.9471, 2.6149], [4.2017, 5.2504, -0.7851, 0.6411, 0.5075, 0.9168, 1.5839], [-1.0088, 5.4107, 1.6293, 0.5064, 0.7017, 0.6602, 0.4605], [1.4837, 4.0268, 0.6222, 0.4071, 0.9951, 1.8243, 1.6786]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 12, 256)\n    reg_preds = torch.rand(2, 67, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size_class = results['size_class']\n    size_res_norm = results['size_res_norm']\n    size_res = results['size_res']\n    sem_scores = results['sem_scores']\n    assert obj_scores.shape == torch.Size([2, 256, 2])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size_class.shape == torch.Size([2, 256, 10])\n    assert size_res_norm.shape == torch.Size([2, 256, 10, 3])\n    assert size_res.shape == torch.Size([2, 256, 10, 3])\n    assert sem_scores.shape == torch.Size([2, 256, 10])",
            "def test_partial_bin_based_box_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    box_coder_cfg = dict(type='PartialBinBasedBBoxCoder', num_sizes=10, num_dir_bins=12, with_rot=True, mean_sizes=[[2.114256, 1.6203, 0.927272], [0.791118, 1.279516, 0.718182], [0.923508, 1.867419, 0.845495], [0.591958, 0.552978, 0.827272], [0.699104, 0.454178, 0.75625], [0.69519, 1.346299, 0.736364], [0.528526, 1.002642, 1.172878], [0.500618, 0.632163, 0.683424], [0.404671, 1.071108, 1.688889], [0.76584, 1.398258, 0.472728]])\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = DepthInstance3DBoxes([[0.8308, 4.1168, -1.2035, 2.2493, 1.8444, 1.9245, 1.6486], [2.3002, 4.8149, -1.2442, 0.5718, 0.8629, 0.951, 1.603], [-1.1477, 1.809, -1.1725, 0.6965, 1.5273, 2.0563, 0.0552]])\n    gt_labels = torch.tensor([0, 1, 2])\n    (center_target, size_class_target, size_res_target, dir_class_target, dir_res_target) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[0.8308, 4.1168, -0.2413], [2.3002, 4.8149, -0.7687], [-1.1477, 1.809, -0.1444]])\n    expected_size_class_target = torch.tensor([0, 1, 2])\n    expected_size_res_target = torch.tensor([[0.135, 0.2241, 0.9972], [-0.2193, -0.4166, 0.2328], [-0.227, -0.3401, 1.2108]])\n    expected_dir_class_target = torch.tensor([3, 3, 0])\n    expected_dir_res_target = torch.tensor([0.0778, 0.0322, 0.0552])\n    assert torch.allclose(center_target, expected_center_target, atol=0.0001)\n    assert torch.all(size_class_target == expected_size_class_target)\n    assert torch.allclose(size_res_target, expected_size_res_target, atol=0.0001)\n    assert torch.all(dir_class_target == expected_dir_class_target)\n    assert torch.allclose(dir_res_target, expected_dir_res_target, atol=0.0001)\n    center = torch.tensor([[[0.8014, 3.4134, -0.6133], [2.6375, 8.4191, 2.0438], [4.2017, 5.2504, -0.7851], [-1.0088, 5.4107, 1.6293], [1.4837, 4.0268, 0.6222]]])\n    size_class = torch.tensor([[[-1.0061, -2.2788, 1.1322, -4.438, -11.0526, -2.8113, -2.0642, -7.5886, -4.8627, -5.0437], [-2.2058, -0.3527, -1.9976, 0.8815, -2.798, -1.9053, -0.5097, -2.0232, -1.4242, -4.1192], [-1.4783, -0.1009, -1.1537, 0.3052, -4.3147, -2.6529, 0.2729, -0.3755, -2.6479, -3.7548], [-6.1809, -3.5024, -8.3273, 1.1252, -4.3315, -7.8288, -4.6091, -5.8153, 0.748, -10.1396], [-9.0424, -3.7883, -6.0788, -1.8855, -10.2493, -9.7164, -1.0658, -4.1713, 1.1173, -10.6204]]])\n    size_res = torch.tensor([[[[-0.098976, -0.52152, -0.076421], [0.14593, 0.56099, 0.089421], [0.051481, 0.3928, 0.12705], [0.36869, 0.70558, 0.14647], [0.47683, 0.33644, 0.23481], [0.087346, 0.84987, 0.33265], [0.21393, 0.85585, 0.098948], [0.07853, 0.059694, -0.087211], [0.18551, 1.1308, -0.51864], [0.36485, 0.73757, 0.15264]], [[-0.95593, -0.50455, 0.19554], [-0.1087, 0.18025, 0.10228], [-0.082882, -0.43771, 0.092135], [-0.04084, -0.059841, 0.11982], [0.073448, 0.052045, 0.17301], [-0.04044, 0.049532, 0.11266], [0.035857, 0.013564, 0.10212], [-0.10407, -0.059321, 0.092622], [0.0074691, 0.09308, -0.44077], [-0.060121, -0.13381, -0.068083]], [[-0.9397, -0.97823, -0.051075], [-0.12843, -0.18381, 0.071327], [-0.12247, -0.81115, 0.036495], [0.049154, -0.04544, 0.08952], [0.15653, 0.03599, 0.16414], [-0.059621, 0.0049357, 0.14264], [0.00085235, -0.1003, -0.030712], [-0.037255, 0.028996, 0.055545], [0.039298, -0.04742, -0.49147], [-0.11548, -0.15895, -0.039155]], [[-1.8725, -0.74102, 1.0524], [-0.3321, 0.047828, -0.032666], [-0.27949, 0.055541, -0.10059], [-0.085533, 0.1487, -0.16709], [0.38283, 0.26609, 0.21361], [-0.42156, 0.32455, 0.67309], [-0.024336, -0.083366, 0.39913], [0.0082142, 0.048323, -0.15247], [-0.048142, -0.30074, -0.16829], [0.13274, -0.23825, -0.18127]], [[-1.2576, -0.6155, 0.7943], [-0.47222, 1.5634, -0.05946], [-0.35367, 1.3616, -0.16421], [-0.016611, 0.24231, -0.096188], [0.54486, 0.46833, 0.51151], [-0.61755, 1.0292, 1.2458], [-0.068152, 0.24786, 0.95088], [-0.048745, 0.15134, -0.099962], [0.0024485, -0.075991, 0.13545], [0.41608, -0.12093, -0.31643]]]])\n    dir_class = torch.tensor([[[-1.023, -5.1965, -5.2195, 2.403, -2.7661, -7.3399, -1.164, -4.063, -5.294, 0.8245, -3.1869, -6.1743], [-1.9503, -1.694, -0.8716, -1.1494, -0.8196, 0.2862, -0.2921, -0.7894, -0.2481, -0.9916, -1.4304, -1.2466], [-1.7435, -1.2043, -0.1265, 0.5083, -0.0717, -0.956, -1.6171, -2.6463, -2.3863, -2.1358, -1.8812, -2.3117], [-1.9282, 0.3792, -1.8426, -1.4587, -0.8582, -3.4639, -3.2133, -3.7867, -7.6781, -6.4459, -6.2455, -5.4797], [-3.1869, 0.4456, -0.5824, 0.9994, -1.0554, -8.4232, -7.7019, -7.1382, -10.2724, -7.8229, -8.186, -8.6194]]])\n    dir_res = torch.tensor([[[0.11022, -0.2375, 0.20381, 0.12177, -0.28501, 0.15351, 0.12218, -0.20677, 0.14468, 0.11593, -0.26864, 0.1129], [-0.015788, 0.041538, -0.00022857, -0.014011, 0.04256, -0.0031186, -0.050343, 0.006811, -0.026728, -0.032781, 0.036889, -0.0015609], [0.019004, 0.0057105, 0.060329, 0.013074, -0.025546, -0.011456, -0.032484, -0.033487, 0.0016609, 0.017095, 1.2647e-05, 0.024814], [0.14482, -0.063083, 0.058307, 0.091396, -0.084571, 0.04589, 0.056243, -0.12448, -0.095244, 0.045746, -0.01739, 0.090267], [0.18065, -0.020078, 0.085401, 0.10784, -0.12495, 0.022796, 0.1131, -0.084364, -0.11904, 0.06118, -0.018109, 0.11229]]])\n    bbox_out = dict(center=center, size_class=size_class, size_res=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[0.8014, 3.4134, -0.6133, 0.975, 2.2602, 0.9725, 1.6926], [2.6375, 8.4191, 2.0438, 0.5511, 0.4931, 0.9471, 2.6149], [4.2017, 5.2504, -0.7851, 0.6411, 0.5075, 0.9168, 1.5839], [-1.0088, 5.4107, 1.6293, 0.5064, 0.7017, 0.6602, 0.4605], [1.4837, 4.0268, 0.6222, 0.4071, 0.9951, 1.8243, 1.6786]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 12, 256)\n    reg_preds = torch.rand(2, 67, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size_class = results['size_class']\n    size_res_norm = results['size_res_norm']\n    size_res = results['size_res']\n    sem_scores = results['sem_scores']\n    assert obj_scores.shape == torch.Size([2, 256, 2])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size_class.shape == torch.Size([2, 256, 10])\n    assert size_res_norm.shape == torch.Size([2, 256, 10, 3])\n    assert size_res.shape == torch.Size([2, 256, 10, 3])\n    assert sem_scores.shape == torch.Size([2, 256, 10])"
        ]
    },
    {
        "func_name": "test_anchor_free_box_coder",
        "original": "def test_anchor_free_box_coder():\n    box_coder_cfg = dict(type='AnchorFreeBBoxCoder', num_dir_bins=12, with_rot=True)\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = LiDARInstance3DBoxes([[2.1227, 5.7951, -0.999, 1.6736, 4.2419, 1.5473, -1.5501], [11.791, 9.0276, -0.85772, 1.621, 3.5367, 1.4841, -1.7369], [23.638, 9.6997, -0.56713, 1.7578, 4.6103, 1.5999, -1.4556]])\n    gt_labels = torch.tensor([0, 0, 0])\n    (center_targets, size_targets, dir_class_targets, dir_res_targets) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[2.1227, 5.7951, -0.2253], [11.7908, 9.0276, -0.1156], [23.638, 9.6997, 0.2328]])\n    expected_size_targets = torch.tensor([[0.8368, 2.121, 0.7736], [0.8105, 1.7683, 0.7421], [0.8789, 2.3052, 0.8]])\n    expected_dir_class_target = torch.tensor([9, 9, 9])\n    expected_dir_res_target = torch.tensor([0.0394, -0.3172, 0.2199])\n    assert torch.allclose(center_targets, expected_center_target, atol=0.0001)\n    assert torch.allclose(size_targets, expected_size_targets, atol=0.0001)\n    assert torch.all(dir_class_targets == expected_dir_class_target)\n    assert torch.allclose(dir_res_targets, expected_dir_res_target, atol=0.001)\n    center = torch.tensor([[[14.5954, 6.3312, 0.7671], [67.5245, 22.4422, 1.561], [47.7693, -6.798, 1.4395]]])\n    size_res = torch.tensor([[[-1.0752, 1.876, 0.7715], [-0.8016, 1.1754, 0.0102], [-1.2789, 0.5948, 0.4728]]])\n    dir_class = torch.tensor([[[0.1512, 1.7914, -1.7658, 2.1572, -0.9215, 1.2139, 0.1749, 0.8606, 1.1743, -0.7679, -1.6005, 0.4623], [-0.3957, 1.2026, -1.2677, 1.3863, -0.5754, 1.7083, 0.2601, 0.1129, 0.7146, -0.1367, -1.2892, -0.0083], [-0.8862, 1.205, -1.3881, 1.6604, -0.9087, 1.1907, -0.028, 0.2027, 1.0644, -0.7205, -1.0738, 0.4748]]])\n    dir_res = torch.tensor([[[1.1151, 0.5535, -0.2053, -0.6582, -0.1616, -0.1821, 0.4675, 0.6621, 0.8146, -0.0448, -0.7253, -0.7171], [0.7888, 0.2478, -0.1962, -0.7267, 0.0573, -0.2398, 0.6984, 0.5859, 0.7507, -0.198, -0.6538, -0.6602], [0.9039, 0.6109, 0.196, -0.5016, 0.0551, -0.4086, 0.3398, 0.2759, 0.7247, -0.0655, -0.5052, -0.9026]]])\n    bbox_out = dict(center=center, size=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[14.5954, 6.3312, 0.7671, 0.1, 3.7521, 1.5429, 0.9126], [67.5245, 22.4422, 1.561, 0.1, 2.3508, 0.1, 2.3782], [47.7693, -6.798, 1.4395, 0.1, 1.1897, 0.9456, 1.0692]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 1, 256)\n    reg_preds = torch.rand(2, 30, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    center_offset = results['center_offset']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size = results['size']\n    assert obj_scores.shape == torch.Size([2, 1, 256])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert center_offset.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size.shape == torch.Size([2, 256, 3])",
        "mutated": [
            "def test_anchor_free_box_coder():\n    if False:\n        i = 10\n    box_coder_cfg = dict(type='AnchorFreeBBoxCoder', num_dir_bins=12, with_rot=True)\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = LiDARInstance3DBoxes([[2.1227, 5.7951, -0.999, 1.6736, 4.2419, 1.5473, -1.5501], [11.791, 9.0276, -0.85772, 1.621, 3.5367, 1.4841, -1.7369], [23.638, 9.6997, -0.56713, 1.7578, 4.6103, 1.5999, -1.4556]])\n    gt_labels = torch.tensor([0, 0, 0])\n    (center_targets, size_targets, dir_class_targets, dir_res_targets) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[2.1227, 5.7951, -0.2253], [11.7908, 9.0276, -0.1156], [23.638, 9.6997, 0.2328]])\n    expected_size_targets = torch.tensor([[0.8368, 2.121, 0.7736], [0.8105, 1.7683, 0.7421], [0.8789, 2.3052, 0.8]])\n    expected_dir_class_target = torch.tensor([9, 9, 9])\n    expected_dir_res_target = torch.tensor([0.0394, -0.3172, 0.2199])\n    assert torch.allclose(center_targets, expected_center_target, atol=0.0001)\n    assert torch.allclose(size_targets, expected_size_targets, atol=0.0001)\n    assert torch.all(dir_class_targets == expected_dir_class_target)\n    assert torch.allclose(dir_res_targets, expected_dir_res_target, atol=0.001)\n    center = torch.tensor([[[14.5954, 6.3312, 0.7671], [67.5245, 22.4422, 1.561], [47.7693, -6.798, 1.4395]]])\n    size_res = torch.tensor([[[-1.0752, 1.876, 0.7715], [-0.8016, 1.1754, 0.0102], [-1.2789, 0.5948, 0.4728]]])\n    dir_class = torch.tensor([[[0.1512, 1.7914, -1.7658, 2.1572, -0.9215, 1.2139, 0.1749, 0.8606, 1.1743, -0.7679, -1.6005, 0.4623], [-0.3957, 1.2026, -1.2677, 1.3863, -0.5754, 1.7083, 0.2601, 0.1129, 0.7146, -0.1367, -1.2892, -0.0083], [-0.8862, 1.205, -1.3881, 1.6604, -0.9087, 1.1907, -0.028, 0.2027, 1.0644, -0.7205, -1.0738, 0.4748]]])\n    dir_res = torch.tensor([[[1.1151, 0.5535, -0.2053, -0.6582, -0.1616, -0.1821, 0.4675, 0.6621, 0.8146, -0.0448, -0.7253, -0.7171], [0.7888, 0.2478, -0.1962, -0.7267, 0.0573, -0.2398, 0.6984, 0.5859, 0.7507, -0.198, -0.6538, -0.6602], [0.9039, 0.6109, 0.196, -0.5016, 0.0551, -0.4086, 0.3398, 0.2759, 0.7247, -0.0655, -0.5052, -0.9026]]])\n    bbox_out = dict(center=center, size=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[14.5954, 6.3312, 0.7671, 0.1, 3.7521, 1.5429, 0.9126], [67.5245, 22.4422, 1.561, 0.1, 2.3508, 0.1, 2.3782], [47.7693, -6.798, 1.4395, 0.1, 1.1897, 0.9456, 1.0692]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 1, 256)\n    reg_preds = torch.rand(2, 30, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    center_offset = results['center_offset']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size = results['size']\n    assert obj_scores.shape == torch.Size([2, 1, 256])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert center_offset.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size.shape == torch.Size([2, 256, 3])",
            "def test_anchor_free_box_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    box_coder_cfg = dict(type='AnchorFreeBBoxCoder', num_dir_bins=12, with_rot=True)\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = LiDARInstance3DBoxes([[2.1227, 5.7951, -0.999, 1.6736, 4.2419, 1.5473, -1.5501], [11.791, 9.0276, -0.85772, 1.621, 3.5367, 1.4841, -1.7369], [23.638, 9.6997, -0.56713, 1.7578, 4.6103, 1.5999, -1.4556]])\n    gt_labels = torch.tensor([0, 0, 0])\n    (center_targets, size_targets, dir_class_targets, dir_res_targets) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[2.1227, 5.7951, -0.2253], [11.7908, 9.0276, -0.1156], [23.638, 9.6997, 0.2328]])\n    expected_size_targets = torch.tensor([[0.8368, 2.121, 0.7736], [0.8105, 1.7683, 0.7421], [0.8789, 2.3052, 0.8]])\n    expected_dir_class_target = torch.tensor([9, 9, 9])\n    expected_dir_res_target = torch.tensor([0.0394, -0.3172, 0.2199])\n    assert torch.allclose(center_targets, expected_center_target, atol=0.0001)\n    assert torch.allclose(size_targets, expected_size_targets, atol=0.0001)\n    assert torch.all(dir_class_targets == expected_dir_class_target)\n    assert torch.allclose(dir_res_targets, expected_dir_res_target, atol=0.001)\n    center = torch.tensor([[[14.5954, 6.3312, 0.7671], [67.5245, 22.4422, 1.561], [47.7693, -6.798, 1.4395]]])\n    size_res = torch.tensor([[[-1.0752, 1.876, 0.7715], [-0.8016, 1.1754, 0.0102], [-1.2789, 0.5948, 0.4728]]])\n    dir_class = torch.tensor([[[0.1512, 1.7914, -1.7658, 2.1572, -0.9215, 1.2139, 0.1749, 0.8606, 1.1743, -0.7679, -1.6005, 0.4623], [-0.3957, 1.2026, -1.2677, 1.3863, -0.5754, 1.7083, 0.2601, 0.1129, 0.7146, -0.1367, -1.2892, -0.0083], [-0.8862, 1.205, -1.3881, 1.6604, -0.9087, 1.1907, -0.028, 0.2027, 1.0644, -0.7205, -1.0738, 0.4748]]])\n    dir_res = torch.tensor([[[1.1151, 0.5535, -0.2053, -0.6582, -0.1616, -0.1821, 0.4675, 0.6621, 0.8146, -0.0448, -0.7253, -0.7171], [0.7888, 0.2478, -0.1962, -0.7267, 0.0573, -0.2398, 0.6984, 0.5859, 0.7507, -0.198, -0.6538, -0.6602], [0.9039, 0.6109, 0.196, -0.5016, 0.0551, -0.4086, 0.3398, 0.2759, 0.7247, -0.0655, -0.5052, -0.9026]]])\n    bbox_out = dict(center=center, size=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[14.5954, 6.3312, 0.7671, 0.1, 3.7521, 1.5429, 0.9126], [67.5245, 22.4422, 1.561, 0.1, 2.3508, 0.1, 2.3782], [47.7693, -6.798, 1.4395, 0.1, 1.1897, 0.9456, 1.0692]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 1, 256)\n    reg_preds = torch.rand(2, 30, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    center_offset = results['center_offset']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size = results['size']\n    assert obj_scores.shape == torch.Size([2, 1, 256])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert center_offset.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size.shape == torch.Size([2, 256, 3])",
            "def test_anchor_free_box_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    box_coder_cfg = dict(type='AnchorFreeBBoxCoder', num_dir_bins=12, with_rot=True)\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = LiDARInstance3DBoxes([[2.1227, 5.7951, -0.999, 1.6736, 4.2419, 1.5473, -1.5501], [11.791, 9.0276, -0.85772, 1.621, 3.5367, 1.4841, -1.7369], [23.638, 9.6997, -0.56713, 1.7578, 4.6103, 1.5999, -1.4556]])\n    gt_labels = torch.tensor([0, 0, 0])\n    (center_targets, size_targets, dir_class_targets, dir_res_targets) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[2.1227, 5.7951, -0.2253], [11.7908, 9.0276, -0.1156], [23.638, 9.6997, 0.2328]])\n    expected_size_targets = torch.tensor([[0.8368, 2.121, 0.7736], [0.8105, 1.7683, 0.7421], [0.8789, 2.3052, 0.8]])\n    expected_dir_class_target = torch.tensor([9, 9, 9])\n    expected_dir_res_target = torch.tensor([0.0394, -0.3172, 0.2199])\n    assert torch.allclose(center_targets, expected_center_target, atol=0.0001)\n    assert torch.allclose(size_targets, expected_size_targets, atol=0.0001)\n    assert torch.all(dir_class_targets == expected_dir_class_target)\n    assert torch.allclose(dir_res_targets, expected_dir_res_target, atol=0.001)\n    center = torch.tensor([[[14.5954, 6.3312, 0.7671], [67.5245, 22.4422, 1.561], [47.7693, -6.798, 1.4395]]])\n    size_res = torch.tensor([[[-1.0752, 1.876, 0.7715], [-0.8016, 1.1754, 0.0102], [-1.2789, 0.5948, 0.4728]]])\n    dir_class = torch.tensor([[[0.1512, 1.7914, -1.7658, 2.1572, -0.9215, 1.2139, 0.1749, 0.8606, 1.1743, -0.7679, -1.6005, 0.4623], [-0.3957, 1.2026, -1.2677, 1.3863, -0.5754, 1.7083, 0.2601, 0.1129, 0.7146, -0.1367, -1.2892, -0.0083], [-0.8862, 1.205, -1.3881, 1.6604, -0.9087, 1.1907, -0.028, 0.2027, 1.0644, -0.7205, -1.0738, 0.4748]]])\n    dir_res = torch.tensor([[[1.1151, 0.5535, -0.2053, -0.6582, -0.1616, -0.1821, 0.4675, 0.6621, 0.8146, -0.0448, -0.7253, -0.7171], [0.7888, 0.2478, -0.1962, -0.7267, 0.0573, -0.2398, 0.6984, 0.5859, 0.7507, -0.198, -0.6538, -0.6602], [0.9039, 0.6109, 0.196, -0.5016, 0.0551, -0.4086, 0.3398, 0.2759, 0.7247, -0.0655, -0.5052, -0.9026]]])\n    bbox_out = dict(center=center, size=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[14.5954, 6.3312, 0.7671, 0.1, 3.7521, 1.5429, 0.9126], [67.5245, 22.4422, 1.561, 0.1, 2.3508, 0.1, 2.3782], [47.7693, -6.798, 1.4395, 0.1, 1.1897, 0.9456, 1.0692]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 1, 256)\n    reg_preds = torch.rand(2, 30, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    center_offset = results['center_offset']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size = results['size']\n    assert obj_scores.shape == torch.Size([2, 1, 256])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert center_offset.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size.shape == torch.Size([2, 256, 3])",
            "def test_anchor_free_box_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    box_coder_cfg = dict(type='AnchorFreeBBoxCoder', num_dir_bins=12, with_rot=True)\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = LiDARInstance3DBoxes([[2.1227, 5.7951, -0.999, 1.6736, 4.2419, 1.5473, -1.5501], [11.791, 9.0276, -0.85772, 1.621, 3.5367, 1.4841, -1.7369], [23.638, 9.6997, -0.56713, 1.7578, 4.6103, 1.5999, -1.4556]])\n    gt_labels = torch.tensor([0, 0, 0])\n    (center_targets, size_targets, dir_class_targets, dir_res_targets) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[2.1227, 5.7951, -0.2253], [11.7908, 9.0276, -0.1156], [23.638, 9.6997, 0.2328]])\n    expected_size_targets = torch.tensor([[0.8368, 2.121, 0.7736], [0.8105, 1.7683, 0.7421], [0.8789, 2.3052, 0.8]])\n    expected_dir_class_target = torch.tensor([9, 9, 9])\n    expected_dir_res_target = torch.tensor([0.0394, -0.3172, 0.2199])\n    assert torch.allclose(center_targets, expected_center_target, atol=0.0001)\n    assert torch.allclose(size_targets, expected_size_targets, atol=0.0001)\n    assert torch.all(dir_class_targets == expected_dir_class_target)\n    assert torch.allclose(dir_res_targets, expected_dir_res_target, atol=0.001)\n    center = torch.tensor([[[14.5954, 6.3312, 0.7671], [67.5245, 22.4422, 1.561], [47.7693, -6.798, 1.4395]]])\n    size_res = torch.tensor([[[-1.0752, 1.876, 0.7715], [-0.8016, 1.1754, 0.0102], [-1.2789, 0.5948, 0.4728]]])\n    dir_class = torch.tensor([[[0.1512, 1.7914, -1.7658, 2.1572, -0.9215, 1.2139, 0.1749, 0.8606, 1.1743, -0.7679, -1.6005, 0.4623], [-0.3957, 1.2026, -1.2677, 1.3863, -0.5754, 1.7083, 0.2601, 0.1129, 0.7146, -0.1367, -1.2892, -0.0083], [-0.8862, 1.205, -1.3881, 1.6604, -0.9087, 1.1907, -0.028, 0.2027, 1.0644, -0.7205, -1.0738, 0.4748]]])\n    dir_res = torch.tensor([[[1.1151, 0.5535, -0.2053, -0.6582, -0.1616, -0.1821, 0.4675, 0.6621, 0.8146, -0.0448, -0.7253, -0.7171], [0.7888, 0.2478, -0.1962, -0.7267, 0.0573, -0.2398, 0.6984, 0.5859, 0.7507, -0.198, -0.6538, -0.6602], [0.9039, 0.6109, 0.196, -0.5016, 0.0551, -0.4086, 0.3398, 0.2759, 0.7247, -0.0655, -0.5052, -0.9026]]])\n    bbox_out = dict(center=center, size=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[14.5954, 6.3312, 0.7671, 0.1, 3.7521, 1.5429, 0.9126], [67.5245, 22.4422, 1.561, 0.1, 2.3508, 0.1, 2.3782], [47.7693, -6.798, 1.4395, 0.1, 1.1897, 0.9456, 1.0692]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 1, 256)\n    reg_preds = torch.rand(2, 30, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    center_offset = results['center_offset']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size = results['size']\n    assert obj_scores.shape == torch.Size([2, 1, 256])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert center_offset.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size.shape == torch.Size([2, 256, 3])",
            "def test_anchor_free_box_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    box_coder_cfg = dict(type='AnchorFreeBBoxCoder', num_dir_bins=12, with_rot=True)\n    box_coder = build_bbox_coder(box_coder_cfg)\n    gt_bboxes = LiDARInstance3DBoxes([[2.1227, 5.7951, -0.999, 1.6736, 4.2419, 1.5473, -1.5501], [11.791, 9.0276, -0.85772, 1.621, 3.5367, 1.4841, -1.7369], [23.638, 9.6997, -0.56713, 1.7578, 4.6103, 1.5999, -1.4556]])\n    gt_labels = torch.tensor([0, 0, 0])\n    (center_targets, size_targets, dir_class_targets, dir_res_targets) = box_coder.encode(gt_bboxes, gt_labels)\n    expected_center_target = torch.tensor([[2.1227, 5.7951, -0.2253], [11.7908, 9.0276, -0.1156], [23.638, 9.6997, 0.2328]])\n    expected_size_targets = torch.tensor([[0.8368, 2.121, 0.7736], [0.8105, 1.7683, 0.7421], [0.8789, 2.3052, 0.8]])\n    expected_dir_class_target = torch.tensor([9, 9, 9])\n    expected_dir_res_target = torch.tensor([0.0394, -0.3172, 0.2199])\n    assert torch.allclose(center_targets, expected_center_target, atol=0.0001)\n    assert torch.allclose(size_targets, expected_size_targets, atol=0.0001)\n    assert torch.all(dir_class_targets == expected_dir_class_target)\n    assert torch.allclose(dir_res_targets, expected_dir_res_target, atol=0.001)\n    center = torch.tensor([[[14.5954, 6.3312, 0.7671], [67.5245, 22.4422, 1.561], [47.7693, -6.798, 1.4395]]])\n    size_res = torch.tensor([[[-1.0752, 1.876, 0.7715], [-0.8016, 1.1754, 0.0102], [-1.2789, 0.5948, 0.4728]]])\n    dir_class = torch.tensor([[[0.1512, 1.7914, -1.7658, 2.1572, -0.9215, 1.2139, 0.1749, 0.8606, 1.1743, -0.7679, -1.6005, 0.4623], [-0.3957, 1.2026, -1.2677, 1.3863, -0.5754, 1.7083, 0.2601, 0.1129, 0.7146, -0.1367, -1.2892, -0.0083], [-0.8862, 1.205, -1.3881, 1.6604, -0.9087, 1.1907, -0.028, 0.2027, 1.0644, -0.7205, -1.0738, 0.4748]]])\n    dir_res = torch.tensor([[[1.1151, 0.5535, -0.2053, -0.6582, -0.1616, -0.1821, 0.4675, 0.6621, 0.8146, -0.0448, -0.7253, -0.7171], [0.7888, 0.2478, -0.1962, -0.7267, 0.0573, -0.2398, 0.6984, 0.5859, 0.7507, -0.198, -0.6538, -0.6602], [0.9039, 0.6109, 0.196, -0.5016, 0.0551, -0.4086, 0.3398, 0.2759, 0.7247, -0.0655, -0.5052, -0.9026]]])\n    bbox_out = dict(center=center, size=size_res, dir_class=dir_class, dir_res=dir_res)\n    bbox3d = box_coder.decode(bbox_out)\n    expected_bbox3d = torch.tensor([[[14.5954, 6.3312, 0.7671, 0.1, 3.7521, 1.5429, 0.9126], [67.5245, 22.4422, 1.561, 0.1, 2.3508, 0.1, 2.3782], [47.7693, -6.798, 1.4395, 0.1, 1.1897, 0.9456, 1.0692]]])\n    assert torch.allclose(bbox3d, expected_bbox3d, atol=0.0001)\n    cls_preds = torch.rand(2, 1, 256)\n    reg_preds = torch.rand(2, 30, 256)\n    base_xyz = torch.rand(2, 256, 3)\n    results = box_coder.split_pred(cls_preds, reg_preds, base_xyz)\n    obj_scores = results['obj_scores']\n    center = results['center']\n    center_offset = results['center_offset']\n    dir_class = results['dir_class']\n    dir_res_norm = results['dir_res_norm']\n    dir_res = results['dir_res']\n    size = results['size']\n    assert obj_scores.shape == torch.Size([2, 1, 256])\n    assert center.shape == torch.Size([2, 256, 3])\n    assert center_offset.shape == torch.Size([2, 256, 3])\n    assert dir_class.shape == torch.Size([2, 256, 12])\n    assert dir_res_norm.shape == torch.Size([2, 256, 12])\n    assert dir_res.shape == torch.Size([2, 256, 12])\n    assert size.shape == torch.Size([2, 256, 3])"
        ]
    },
    {
        "func_name": "test_centerpoint_bbox_coder",
        "original": "def test_centerpoint_bbox_coder():\n    bbox_coder_cfg = dict(type='CenterPointBBoxCoder', post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0], max_num=500, score_threshold=0.1, pc_range=[-51.2, -51.2], out_size_factor=4, voxel_size=[0.2, 0.2])\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_dim = torch.rand([2, 3, 128, 128])\n    batch_hei = torch.rand([2, 1, 128, 128])\n    batch_hm = torch.rand([2, 2, 128, 128])\n    batch_reg = torch.rand([2, 2, 128, 128])\n    batch_rotc = torch.rand([2, 1, 128, 128])\n    batch_rots = torch.rand([2, 1, 128, 128])\n    batch_vel = torch.rand([2, 2, 128, 128])\n    temp = bbox_coder.decode(batch_hm, batch_rots, batch_rotc, batch_hei, batch_dim, batch_vel, batch_reg, 5)\n    for i in range(len(temp)):\n        assert temp[i]['bboxes'].shape == torch.Size([500, 9])\n        assert temp[i]['scores'].shape == torch.Size([500])\n        assert temp[i]['labels'].shape == torch.Size([500])",
        "mutated": [
            "def test_centerpoint_bbox_coder():\n    if False:\n        i = 10\n    bbox_coder_cfg = dict(type='CenterPointBBoxCoder', post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0], max_num=500, score_threshold=0.1, pc_range=[-51.2, -51.2], out_size_factor=4, voxel_size=[0.2, 0.2])\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_dim = torch.rand([2, 3, 128, 128])\n    batch_hei = torch.rand([2, 1, 128, 128])\n    batch_hm = torch.rand([2, 2, 128, 128])\n    batch_reg = torch.rand([2, 2, 128, 128])\n    batch_rotc = torch.rand([2, 1, 128, 128])\n    batch_rots = torch.rand([2, 1, 128, 128])\n    batch_vel = torch.rand([2, 2, 128, 128])\n    temp = bbox_coder.decode(batch_hm, batch_rots, batch_rotc, batch_hei, batch_dim, batch_vel, batch_reg, 5)\n    for i in range(len(temp)):\n        assert temp[i]['bboxes'].shape == torch.Size([500, 9])\n        assert temp[i]['scores'].shape == torch.Size([500])\n        assert temp[i]['labels'].shape == torch.Size([500])",
            "def test_centerpoint_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bbox_coder_cfg = dict(type='CenterPointBBoxCoder', post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0], max_num=500, score_threshold=0.1, pc_range=[-51.2, -51.2], out_size_factor=4, voxel_size=[0.2, 0.2])\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_dim = torch.rand([2, 3, 128, 128])\n    batch_hei = torch.rand([2, 1, 128, 128])\n    batch_hm = torch.rand([2, 2, 128, 128])\n    batch_reg = torch.rand([2, 2, 128, 128])\n    batch_rotc = torch.rand([2, 1, 128, 128])\n    batch_rots = torch.rand([2, 1, 128, 128])\n    batch_vel = torch.rand([2, 2, 128, 128])\n    temp = bbox_coder.decode(batch_hm, batch_rots, batch_rotc, batch_hei, batch_dim, batch_vel, batch_reg, 5)\n    for i in range(len(temp)):\n        assert temp[i]['bboxes'].shape == torch.Size([500, 9])\n        assert temp[i]['scores'].shape == torch.Size([500])\n        assert temp[i]['labels'].shape == torch.Size([500])",
            "def test_centerpoint_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bbox_coder_cfg = dict(type='CenterPointBBoxCoder', post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0], max_num=500, score_threshold=0.1, pc_range=[-51.2, -51.2], out_size_factor=4, voxel_size=[0.2, 0.2])\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_dim = torch.rand([2, 3, 128, 128])\n    batch_hei = torch.rand([2, 1, 128, 128])\n    batch_hm = torch.rand([2, 2, 128, 128])\n    batch_reg = torch.rand([2, 2, 128, 128])\n    batch_rotc = torch.rand([2, 1, 128, 128])\n    batch_rots = torch.rand([2, 1, 128, 128])\n    batch_vel = torch.rand([2, 2, 128, 128])\n    temp = bbox_coder.decode(batch_hm, batch_rots, batch_rotc, batch_hei, batch_dim, batch_vel, batch_reg, 5)\n    for i in range(len(temp)):\n        assert temp[i]['bboxes'].shape == torch.Size([500, 9])\n        assert temp[i]['scores'].shape == torch.Size([500])\n        assert temp[i]['labels'].shape == torch.Size([500])",
            "def test_centerpoint_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bbox_coder_cfg = dict(type='CenterPointBBoxCoder', post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0], max_num=500, score_threshold=0.1, pc_range=[-51.2, -51.2], out_size_factor=4, voxel_size=[0.2, 0.2])\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_dim = torch.rand([2, 3, 128, 128])\n    batch_hei = torch.rand([2, 1, 128, 128])\n    batch_hm = torch.rand([2, 2, 128, 128])\n    batch_reg = torch.rand([2, 2, 128, 128])\n    batch_rotc = torch.rand([2, 1, 128, 128])\n    batch_rots = torch.rand([2, 1, 128, 128])\n    batch_vel = torch.rand([2, 2, 128, 128])\n    temp = bbox_coder.decode(batch_hm, batch_rots, batch_rotc, batch_hei, batch_dim, batch_vel, batch_reg, 5)\n    for i in range(len(temp)):\n        assert temp[i]['bboxes'].shape == torch.Size([500, 9])\n        assert temp[i]['scores'].shape == torch.Size([500])\n        assert temp[i]['labels'].shape == torch.Size([500])",
            "def test_centerpoint_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bbox_coder_cfg = dict(type='CenterPointBBoxCoder', post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0], max_num=500, score_threshold=0.1, pc_range=[-51.2, -51.2], out_size_factor=4, voxel_size=[0.2, 0.2])\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_dim = torch.rand([2, 3, 128, 128])\n    batch_hei = torch.rand([2, 1, 128, 128])\n    batch_hm = torch.rand([2, 2, 128, 128])\n    batch_reg = torch.rand([2, 2, 128, 128])\n    batch_rotc = torch.rand([2, 1, 128, 128])\n    batch_rots = torch.rand([2, 1, 128, 128])\n    batch_vel = torch.rand([2, 2, 128, 128])\n    temp = bbox_coder.decode(batch_hm, batch_rots, batch_rotc, batch_hei, batch_dim, batch_vel, batch_reg, 5)\n    for i in range(len(temp)):\n        assert temp[i]['bboxes'].shape == torch.Size([500, 9])\n        assert temp[i]['scores'].shape == torch.Size([500])\n        assert temp[i]['labels'].shape == torch.Size([500])"
        ]
    },
    {
        "func_name": "test_point_xyzwhlr_bbox_coder",
        "original": "def test_point_xyzwhlr_bbox_coder():\n    bbox_coder_cfg = dict(type='PointXYZWHLRBBoxCoder', use_mean_size=True, mean_size=[[3.9, 1.6, 1.56], [0.8, 0.6, 1.73], [1.76, 0.6, 1.73]])\n    boxcoder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = torch.tensor([[13.3329, 2.3514, -0.7004, 1.7508, 0.4702, 1.7909, -3.0522], [2.2068, -2.6994, -0.3277, 3.8703, 1.6602, 1.6913, -1.9057], [5.5269, 2.5085, -1.0129, 1.1496, 0.8006, 1.8887, 2.1756]])\n    points = torch.tensor([[13.7, 2.4, 0.12], [3.2, -3.0, 0.2], [5.7, 2.2, -0.4]])\n    gt_labels_3d = torch.tensor([2, 0, 1])\n    bbox_target = boxcoder.encode(gt_bboxes_3d, points, gt_labels_3d)\n    expected_bbox_target = torch.tensor([[-0.1974, -0.0261, -0.4742, -0.0052, -0.2438, 0.0346, -0.996, -0.0893], [-0.2356, 0.0713, -0.3383, -0.0076, 0.0369, 0.0808, -0.3287, -0.9444], [-0.1731, 0.3085, -0.3543, 0.3626, 0.2884, 0.0878, -0.5686, 0.8226]])\n    assert torch.allclose(expected_bbox_target, bbox_target, atol=0.0001)\n    bbox3d_out = boxcoder.decode(bbox_target, points, gt_labels_3d)\n    assert torch.allclose(bbox3d_out, gt_bboxes_3d, atol=0.0001)",
        "mutated": [
            "def test_point_xyzwhlr_bbox_coder():\n    if False:\n        i = 10\n    bbox_coder_cfg = dict(type='PointXYZWHLRBBoxCoder', use_mean_size=True, mean_size=[[3.9, 1.6, 1.56], [0.8, 0.6, 1.73], [1.76, 0.6, 1.73]])\n    boxcoder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = torch.tensor([[13.3329, 2.3514, -0.7004, 1.7508, 0.4702, 1.7909, -3.0522], [2.2068, -2.6994, -0.3277, 3.8703, 1.6602, 1.6913, -1.9057], [5.5269, 2.5085, -1.0129, 1.1496, 0.8006, 1.8887, 2.1756]])\n    points = torch.tensor([[13.7, 2.4, 0.12], [3.2, -3.0, 0.2], [5.7, 2.2, -0.4]])\n    gt_labels_3d = torch.tensor([2, 0, 1])\n    bbox_target = boxcoder.encode(gt_bboxes_3d, points, gt_labels_3d)\n    expected_bbox_target = torch.tensor([[-0.1974, -0.0261, -0.4742, -0.0052, -0.2438, 0.0346, -0.996, -0.0893], [-0.2356, 0.0713, -0.3383, -0.0076, 0.0369, 0.0808, -0.3287, -0.9444], [-0.1731, 0.3085, -0.3543, 0.3626, 0.2884, 0.0878, -0.5686, 0.8226]])\n    assert torch.allclose(expected_bbox_target, bbox_target, atol=0.0001)\n    bbox3d_out = boxcoder.decode(bbox_target, points, gt_labels_3d)\n    assert torch.allclose(bbox3d_out, gt_bboxes_3d, atol=0.0001)",
            "def test_point_xyzwhlr_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bbox_coder_cfg = dict(type='PointXYZWHLRBBoxCoder', use_mean_size=True, mean_size=[[3.9, 1.6, 1.56], [0.8, 0.6, 1.73], [1.76, 0.6, 1.73]])\n    boxcoder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = torch.tensor([[13.3329, 2.3514, -0.7004, 1.7508, 0.4702, 1.7909, -3.0522], [2.2068, -2.6994, -0.3277, 3.8703, 1.6602, 1.6913, -1.9057], [5.5269, 2.5085, -1.0129, 1.1496, 0.8006, 1.8887, 2.1756]])\n    points = torch.tensor([[13.7, 2.4, 0.12], [3.2, -3.0, 0.2], [5.7, 2.2, -0.4]])\n    gt_labels_3d = torch.tensor([2, 0, 1])\n    bbox_target = boxcoder.encode(gt_bboxes_3d, points, gt_labels_3d)\n    expected_bbox_target = torch.tensor([[-0.1974, -0.0261, -0.4742, -0.0052, -0.2438, 0.0346, -0.996, -0.0893], [-0.2356, 0.0713, -0.3383, -0.0076, 0.0369, 0.0808, -0.3287, -0.9444], [-0.1731, 0.3085, -0.3543, 0.3626, 0.2884, 0.0878, -0.5686, 0.8226]])\n    assert torch.allclose(expected_bbox_target, bbox_target, atol=0.0001)\n    bbox3d_out = boxcoder.decode(bbox_target, points, gt_labels_3d)\n    assert torch.allclose(bbox3d_out, gt_bboxes_3d, atol=0.0001)",
            "def test_point_xyzwhlr_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bbox_coder_cfg = dict(type='PointXYZWHLRBBoxCoder', use_mean_size=True, mean_size=[[3.9, 1.6, 1.56], [0.8, 0.6, 1.73], [1.76, 0.6, 1.73]])\n    boxcoder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = torch.tensor([[13.3329, 2.3514, -0.7004, 1.7508, 0.4702, 1.7909, -3.0522], [2.2068, -2.6994, -0.3277, 3.8703, 1.6602, 1.6913, -1.9057], [5.5269, 2.5085, -1.0129, 1.1496, 0.8006, 1.8887, 2.1756]])\n    points = torch.tensor([[13.7, 2.4, 0.12], [3.2, -3.0, 0.2], [5.7, 2.2, -0.4]])\n    gt_labels_3d = torch.tensor([2, 0, 1])\n    bbox_target = boxcoder.encode(gt_bboxes_3d, points, gt_labels_3d)\n    expected_bbox_target = torch.tensor([[-0.1974, -0.0261, -0.4742, -0.0052, -0.2438, 0.0346, -0.996, -0.0893], [-0.2356, 0.0713, -0.3383, -0.0076, 0.0369, 0.0808, -0.3287, -0.9444], [-0.1731, 0.3085, -0.3543, 0.3626, 0.2884, 0.0878, -0.5686, 0.8226]])\n    assert torch.allclose(expected_bbox_target, bbox_target, atol=0.0001)\n    bbox3d_out = boxcoder.decode(bbox_target, points, gt_labels_3d)\n    assert torch.allclose(bbox3d_out, gt_bboxes_3d, atol=0.0001)",
            "def test_point_xyzwhlr_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bbox_coder_cfg = dict(type='PointXYZWHLRBBoxCoder', use_mean_size=True, mean_size=[[3.9, 1.6, 1.56], [0.8, 0.6, 1.73], [1.76, 0.6, 1.73]])\n    boxcoder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = torch.tensor([[13.3329, 2.3514, -0.7004, 1.7508, 0.4702, 1.7909, -3.0522], [2.2068, -2.6994, -0.3277, 3.8703, 1.6602, 1.6913, -1.9057], [5.5269, 2.5085, -1.0129, 1.1496, 0.8006, 1.8887, 2.1756]])\n    points = torch.tensor([[13.7, 2.4, 0.12], [3.2, -3.0, 0.2], [5.7, 2.2, -0.4]])\n    gt_labels_3d = torch.tensor([2, 0, 1])\n    bbox_target = boxcoder.encode(gt_bboxes_3d, points, gt_labels_3d)\n    expected_bbox_target = torch.tensor([[-0.1974, -0.0261, -0.4742, -0.0052, -0.2438, 0.0346, -0.996, -0.0893], [-0.2356, 0.0713, -0.3383, -0.0076, 0.0369, 0.0808, -0.3287, -0.9444], [-0.1731, 0.3085, -0.3543, 0.3626, 0.2884, 0.0878, -0.5686, 0.8226]])\n    assert torch.allclose(expected_bbox_target, bbox_target, atol=0.0001)\n    bbox3d_out = boxcoder.decode(bbox_target, points, gt_labels_3d)\n    assert torch.allclose(bbox3d_out, gt_bboxes_3d, atol=0.0001)",
            "def test_point_xyzwhlr_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bbox_coder_cfg = dict(type='PointXYZWHLRBBoxCoder', use_mean_size=True, mean_size=[[3.9, 1.6, 1.56], [0.8, 0.6, 1.73], [1.76, 0.6, 1.73]])\n    boxcoder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = torch.tensor([[13.3329, 2.3514, -0.7004, 1.7508, 0.4702, 1.7909, -3.0522], [2.2068, -2.6994, -0.3277, 3.8703, 1.6602, 1.6913, -1.9057], [5.5269, 2.5085, -1.0129, 1.1496, 0.8006, 1.8887, 2.1756]])\n    points = torch.tensor([[13.7, 2.4, 0.12], [3.2, -3.0, 0.2], [5.7, 2.2, -0.4]])\n    gt_labels_3d = torch.tensor([2, 0, 1])\n    bbox_target = boxcoder.encode(gt_bboxes_3d, points, gt_labels_3d)\n    expected_bbox_target = torch.tensor([[-0.1974, -0.0261, -0.4742, -0.0052, -0.2438, 0.0346, -0.996, -0.0893], [-0.2356, 0.0713, -0.3383, -0.0076, 0.0369, 0.0808, -0.3287, -0.9444], [-0.1731, 0.3085, -0.3543, 0.3626, 0.2884, 0.0878, -0.5686, 0.8226]])\n    assert torch.allclose(expected_bbox_target, bbox_target, atol=0.0001)\n    bbox3d_out = boxcoder.decode(bbox_target, points, gt_labels_3d)\n    assert torch.allclose(bbox3d_out, gt_bboxes_3d, atol=0.0001)"
        ]
    },
    {
        "func_name": "test_fcos3d_bbox_coder",
        "original": "def test_fcos3d_bbox_coder():\n    bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.6261]], [[1.4188]], [[2.3971]], [[1.0586]], [[1.747]], [[1.1727]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[1.4806]], [[1.879]], [[1.5492]], [[1.3965]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    prior_bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=((28.0, 13.0), (25.0, 12.0)), base_dims=((2.0, 3.0, 1.0), (1.0, 2.0, 3.0)), code_size=7, norm_on_bbox=True)\n    prior_bbox_coder = build_bbox_coder(prior_bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.tensor([[[[0.5811]], [[0.6198]]], [[[0.4889]], [[0.8142]]]])\n    decode_bbox = prior_bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.626]], [[1.4188]], [[35.4916]], [[1.0587]], [[3.494]], [[3.5181]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[29.71]], [[1.8789]], [[3.0983]], [[4.1892]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    decode_bbox = decode_bbox.permute(0, 2, 3, 1).view(-1, 7)\n    batch_centers2d = torch.tensor([[100.0, 150.0], [200.0, 100.0]])\n    batch_dir_cls = torch.tensor([0.0, 1.0])\n    dir_offset = 0.7854\n    cam2img = torch.tensor([[700.0, 0.0, 450.0, 0.0], [0.0, 700.0, 200.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n    decode_bbox = prior_bbox_coder.decode_yaw(decode_bbox, batch_centers2d, batch_dir_cls, dir_offset, cam2img)\n    expected_bbox = torch.tensor([[0.626, 1.4188, 35.4916, 1.0587, 3.494, 3.5181, 3.1332], [1.5516, 0.4596, 29.71, 1.8789, 3.0983, 4.1892, 6.1368]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)",
        "mutated": [
            "def test_fcos3d_bbox_coder():\n    if False:\n        i = 10\n    bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.6261]], [[1.4188]], [[2.3971]], [[1.0586]], [[1.747]], [[1.1727]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[1.4806]], [[1.879]], [[1.5492]], [[1.3965]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    prior_bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=((28.0, 13.0), (25.0, 12.0)), base_dims=((2.0, 3.0, 1.0), (1.0, 2.0, 3.0)), code_size=7, norm_on_bbox=True)\n    prior_bbox_coder = build_bbox_coder(prior_bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.tensor([[[[0.5811]], [[0.6198]]], [[[0.4889]], [[0.8142]]]])\n    decode_bbox = prior_bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.626]], [[1.4188]], [[35.4916]], [[1.0587]], [[3.494]], [[3.5181]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[29.71]], [[1.8789]], [[3.0983]], [[4.1892]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    decode_bbox = decode_bbox.permute(0, 2, 3, 1).view(-1, 7)\n    batch_centers2d = torch.tensor([[100.0, 150.0], [200.0, 100.0]])\n    batch_dir_cls = torch.tensor([0.0, 1.0])\n    dir_offset = 0.7854\n    cam2img = torch.tensor([[700.0, 0.0, 450.0, 0.0], [0.0, 700.0, 200.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n    decode_bbox = prior_bbox_coder.decode_yaw(decode_bbox, batch_centers2d, batch_dir_cls, dir_offset, cam2img)\n    expected_bbox = torch.tensor([[0.626, 1.4188, 35.4916, 1.0587, 3.494, 3.5181, 3.1332], [1.5516, 0.4596, 29.71, 1.8789, 3.0983, 4.1892, 6.1368]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)",
            "def test_fcos3d_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.6261]], [[1.4188]], [[2.3971]], [[1.0586]], [[1.747]], [[1.1727]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[1.4806]], [[1.879]], [[1.5492]], [[1.3965]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    prior_bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=((28.0, 13.0), (25.0, 12.0)), base_dims=((2.0, 3.0, 1.0), (1.0, 2.0, 3.0)), code_size=7, norm_on_bbox=True)\n    prior_bbox_coder = build_bbox_coder(prior_bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.tensor([[[[0.5811]], [[0.6198]]], [[[0.4889]], [[0.8142]]]])\n    decode_bbox = prior_bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.626]], [[1.4188]], [[35.4916]], [[1.0587]], [[3.494]], [[3.5181]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[29.71]], [[1.8789]], [[3.0983]], [[4.1892]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    decode_bbox = decode_bbox.permute(0, 2, 3, 1).view(-1, 7)\n    batch_centers2d = torch.tensor([[100.0, 150.0], [200.0, 100.0]])\n    batch_dir_cls = torch.tensor([0.0, 1.0])\n    dir_offset = 0.7854\n    cam2img = torch.tensor([[700.0, 0.0, 450.0, 0.0], [0.0, 700.0, 200.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n    decode_bbox = prior_bbox_coder.decode_yaw(decode_bbox, batch_centers2d, batch_dir_cls, dir_offset, cam2img)\n    expected_bbox = torch.tensor([[0.626, 1.4188, 35.4916, 1.0587, 3.494, 3.5181, 3.1332], [1.5516, 0.4596, 29.71, 1.8789, 3.0983, 4.1892, 6.1368]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)",
            "def test_fcos3d_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.6261]], [[1.4188]], [[2.3971]], [[1.0586]], [[1.747]], [[1.1727]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[1.4806]], [[1.879]], [[1.5492]], [[1.3965]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    prior_bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=((28.0, 13.0), (25.0, 12.0)), base_dims=((2.0, 3.0, 1.0), (1.0, 2.0, 3.0)), code_size=7, norm_on_bbox=True)\n    prior_bbox_coder = build_bbox_coder(prior_bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.tensor([[[[0.5811]], [[0.6198]]], [[[0.4889]], [[0.8142]]]])\n    decode_bbox = prior_bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.626]], [[1.4188]], [[35.4916]], [[1.0587]], [[3.494]], [[3.5181]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[29.71]], [[1.8789]], [[3.0983]], [[4.1892]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    decode_bbox = decode_bbox.permute(0, 2, 3, 1).view(-1, 7)\n    batch_centers2d = torch.tensor([[100.0, 150.0], [200.0, 100.0]])\n    batch_dir_cls = torch.tensor([0.0, 1.0])\n    dir_offset = 0.7854\n    cam2img = torch.tensor([[700.0, 0.0, 450.0, 0.0], [0.0, 700.0, 200.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n    decode_bbox = prior_bbox_coder.decode_yaw(decode_bbox, batch_centers2d, batch_dir_cls, dir_offset, cam2img)\n    expected_bbox = torch.tensor([[0.626, 1.4188, 35.4916, 1.0587, 3.494, 3.5181, 3.1332], [1.5516, 0.4596, 29.71, 1.8789, 3.0983, 4.1892, 6.1368]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)",
            "def test_fcos3d_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.6261]], [[1.4188]], [[2.3971]], [[1.0586]], [[1.747]], [[1.1727]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[1.4806]], [[1.879]], [[1.5492]], [[1.3965]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    prior_bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=((28.0, 13.0), (25.0, 12.0)), base_dims=((2.0, 3.0, 1.0), (1.0, 2.0, 3.0)), code_size=7, norm_on_bbox=True)\n    prior_bbox_coder = build_bbox_coder(prior_bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.tensor([[[[0.5811]], [[0.6198]]], [[[0.4889]], [[0.8142]]]])\n    decode_bbox = prior_bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.626]], [[1.4188]], [[35.4916]], [[1.0587]], [[3.494]], [[3.5181]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[29.71]], [[1.8789]], [[3.0983]], [[4.1892]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    decode_bbox = decode_bbox.permute(0, 2, 3, 1).view(-1, 7)\n    batch_centers2d = torch.tensor([[100.0, 150.0], [200.0, 100.0]])\n    batch_dir_cls = torch.tensor([0.0, 1.0])\n    dir_offset = 0.7854\n    cam2img = torch.tensor([[700.0, 0.0, 450.0, 0.0], [0.0, 700.0, 200.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n    decode_bbox = prior_bbox_coder.decode_yaw(decode_bbox, batch_centers2d, batch_dir_cls, dir_offset, cam2img)\n    expected_bbox = torch.tensor([[0.626, 1.4188, 35.4916, 1.0587, 3.494, 3.5181, 3.1332], [1.5516, 0.4596, 29.71, 1.8789, 3.0983, 4.1892, 6.1368]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)",
            "def test_fcos3d_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.6261]], [[1.4188]], [[2.3971]], [[1.0586]], [[1.747]], [[1.1727]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[1.4806]], [[1.879]], [[1.5492]], [[1.3965]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    prior_bbox_coder_cfg = dict(type='FCOS3DBBoxCoder', base_depths=((28.0, 13.0), (25.0, 12.0)), base_dims=((2.0, 3.0, 1.0), (1.0, 2.0, 3.0)), code_size=7, norm_on_bbox=True)\n    prior_bbox_coder = build_bbox_coder(prior_bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.313]], [[0.7094]], [[0.8743]], [[0.057]], [[0.5579]], [[0.1593]], [[0.4553]]], [[[0.7758]], [[0.2298]], [[0.3925]], [[0.6307]], [[0.4377]], [[0.3339]], [[0.1966]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(3)])\n    stride = 2\n    training = False\n    cls_score = torch.tensor([[[[0.5811]], [[0.6198]]], [[[0.4889]], [[0.8142]]]])\n    decode_bbox = prior_bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    expected_bbox = torch.tensor([[[[0.626]], [[1.4188]], [[35.4916]], [[1.0587]], [[3.494]], [[3.5181]], [[0.4553]]], [[[1.5516]], [[0.4596]], [[29.71]], [[1.8789]], [[3.0983]], [[4.1892]], [[0.1966]]]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)\n    decode_bbox = decode_bbox.permute(0, 2, 3, 1).view(-1, 7)\n    batch_centers2d = torch.tensor([[100.0, 150.0], [200.0, 100.0]])\n    batch_dir_cls = torch.tensor([0.0, 1.0])\n    dir_offset = 0.7854\n    cam2img = torch.tensor([[700.0, 0.0, 450.0, 0.0], [0.0, 700.0, 200.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n    decode_bbox = prior_bbox_coder.decode_yaw(decode_bbox, batch_centers2d, batch_dir_cls, dir_offset, cam2img)\n    expected_bbox = torch.tensor([[0.626, 1.4188, 35.4916, 1.0587, 3.494, 3.5181, 3.1332], [1.5516, 0.4596, 29.71, 1.8789, 3.0983, 4.1892, 6.1368]])\n    assert torch.allclose(decode_bbox, expected_bbox, atol=0.001)"
        ]
    },
    {
        "func_name": "test_pgd_bbox_coder",
        "original": "def test_pgd_bbox_coder():\n    bbox_coder_cfg = dict(type='PGDBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.0103]], [[0.7394]], [[0.3296]], [[0.4708]], [[0.1439]], [[0.0778]], [[0.9399]], [[0.8366]], [[0.1264]], [[0.303]], [[0.1898]], [[0.0714]], [[0.4144]], [[0.4341]], [[0.6442]], [[0.2951]], [[0.289]], [[0.4486]], [[0.2848]], [[0.1071]], [[0.953]], [[0.946]], [[0.3822]], [[0.932]], [[0.2611]], [[0.558]], [[0.0397]]], [[[0.8612]], [[0.168]], [[0.5167]], [[0.8502]], [[0.0377]], [[0.3615]], [[0.955]], [[0.5219]], [[0.1402]], [[0.6843]], [[0.2121]], [[0.9468]], [[0.6238]], [[0.7918]], [[0.1646]], [[0.05]], [[0.629]], [[0.3956]], [[0.2901]], [[0.4612]], [[0.7333]], [[0.1194]], [[0.6999]], [[0.398]], [[0.3262]], [[0.7185]], [[0.4474]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(5)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    max_regress_range = 16\n    pred_keypoints = True\n    pred_bbox2d = True\n    decode_bbox_w2d = bbox_coder.decode_2d(decode_bbox, batch_scale, stride, max_regress_range, training, pred_keypoints, pred_bbox2d)\n    expected_decode_bbox_w2d = torch.tensor([[[[0.0206]], [[1.4788]], [[1.3904]], [[1.6013]], [[1.1548]], [[1.0809]], [[0.9399]], [[10.9441]], [[2.0117]], [[4.7049]], [[3.0009]], [[1.1405]], [[6.2752]], [[6.5399]], [[9.084]], [[4.5892]], [[4.4994]], [[6.732]], [[4.4375]], [[1.7071]], [[11.8582]], [[11.8075]], [[5.8339]], [[1.864]], [[0.5222]], [[1.116]], [[0.0794]]], [[[1.7224]], [[0.336]], [[1.6765]], [[2.3401]], [[1.0384]], [[1.4355]], [[0.955]], [[7.6666]], [[2.2286]], [[9.5089]], [[3.3436]], [[11.8133]], [[8.8603]], [[10.5508]], [[2.6101]], [[0.7993]], [[8.9178]], [[6.0188]], [[4.5156]], [[6.897]], [[10.0013]], [[1.9014]], [[9.6689]], [[0.796]], [[0.6524]], [[1.437]], [[0.8948]]]])\n    assert torch.allclose(expected_decode_bbox_w2d, decode_bbox_w2d, atol=0.001)\n    depth_cls_preds = torch.tensor([[-0.4383, 0.7207, -0.4092, 0.4649, 0.8526, 0.6186, -1.4312, -0.715], [0.0621, 0.2369, 0.517, 0.8484, -0.1099, 0.1829, -0.0072, 1.0618], [-1.6114, -0.1057, 0.5721, -0.5986, -2.0471, 0.814, -0.8385, -0.4822], [0.0742, -0.3261, 0.4607, 1.8155, -0.3571, -0.0234, 0.3787, 2.3251], [1.0492, -0.6881, -0.0136, -1.8291, 0.846, -1.0171, 2.5691, -0.8114], [0.0968, -0.5601, 1.0458, 0.256, 1.3018, 0.1635, 0.068, -1.0263], [-0.0765, 0.1498, -2.7321, 1.0047, -0.2505, 0.0871, -0.482, -0.3003], [-0.4123, 0.2298, -0.133, -0.6008, 0.6526, 0.7118, 0.9728, -0.7793], [1.694, 0.3355, 1.4661, 0.5477, 0.8667, 0.0527, -0.9975, -0.0689], [0.4724, -0.3632, -0.0654, 0.4034, -0.3494, -0.7548, 0.7297, 1.2754]])\n    depth_range = (0, 70)\n    depth_unit = 10\n    num_depth_cls = 8\n    uniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'uniform', num_depth_cls)\n    expected_preds = torch.tensor([32.0441, 38.4689, 36.1831, 48.2096, 46.156, 32.7973, 33.2155, 39.9822, 21.9905, 43.0161])\n    assert torch.allclose(uniform_prob_depth_preds, expected_preds, atol=0.001)\n    linear_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'linear', num_depth_cls)\n    expected_preds = torch.tensor([21.1431, 30.2421, 25.8964, 41.6116, 38.6234, 21.4582, 23.2993, 30.1111, 13.9273, 36.8419])\n    assert torch.allclose(linear_prob_depth_preds, expected_preds, atol=0.001)\n    log_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'log', num_depth_cls)\n    expected_preds = torch.tensor([12.6458, 24.2487, 17.4015, 36.9375, 27.5982, 12.551, 15.6635, 19.8408, 9.1605, 31.3765])\n    assert torch.allclose(log_prob_depth_preds, expected_preds, atol=0.001)\n    loguniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'loguniform', num_depth_cls)\n    expected_preds = torch.tensor([6.9925, 10.3273, 8.9895, 18.6524, 16.4667, 7.3196, 7.5078, 11.3207, 3.7987, 13.6095])\n    assert torch.allclose(loguniform_prob_depth_preds, expected_preds, atol=0.001)",
        "mutated": [
            "def test_pgd_bbox_coder():\n    if False:\n        i = 10\n    bbox_coder_cfg = dict(type='PGDBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.0103]], [[0.7394]], [[0.3296]], [[0.4708]], [[0.1439]], [[0.0778]], [[0.9399]], [[0.8366]], [[0.1264]], [[0.303]], [[0.1898]], [[0.0714]], [[0.4144]], [[0.4341]], [[0.6442]], [[0.2951]], [[0.289]], [[0.4486]], [[0.2848]], [[0.1071]], [[0.953]], [[0.946]], [[0.3822]], [[0.932]], [[0.2611]], [[0.558]], [[0.0397]]], [[[0.8612]], [[0.168]], [[0.5167]], [[0.8502]], [[0.0377]], [[0.3615]], [[0.955]], [[0.5219]], [[0.1402]], [[0.6843]], [[0.2121]], [[0.9468]], [[0.6238]], [[0.7918]], [[0.1646]], [[0.05]], [[0.629]], [[0.3956]], [[0.2901]], [[0.4612]], [[0.7333]], [[0.1194]], [[0.6999]], [[0.398]], [[0.3262]], [[0.7185]], [[0.4474]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(5)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    max_regress_range = 16\n    pred_keypoints = True\n    pred_bbox2d = True\n    decode_bbox_w2d = bbox_coder.decode_2d(decode_bbox, batch_scale, stride, max_regress_range, training, pred_keypoints, pred_bbox2d)\n    expected_decode_bbox_w2d = torch.tensor([[[[0.0206]], [[1.4788]], [[1.3904]], [[1.6013]], [[1.1548]], [[1.0809]], [[0.9399]], [[10.9441]], [[2.0117]], [[4.7049]], [[3.0009]], [[1.1405]], [[6.2752]], [[6.5399]], [[9.084]], [[4.5892]], [[4.4994]], [[6.732]], [[4.4375]], [[1.7071]], [[11.8582]], [[11.8075]], [[5.8339]], [[1.864]], [[0.5222]], [[1.116]], [[0.0794]]], [[[1.7224]], [[0.336]], [[1.6765]], [[2.3401]], [[1.0384]], [[1.4355]], [[0.955]], [[7.6666]], [[2.2286]], [[9.5089]], [[3.3436]], [[11.8133]], [[8.8603]], [[10.5508]], [[2.6101]], [[0.7993]], [[8.9178]], [[6.0188]], [[4.5156]], [[6.897]], [[10.0013]], [[1.9014]], [[9.6689]], [[0.796]], [[0.6524]], [[1.437]], [[0.8948]]]])\n    assert torch.allclose(expected_decode_bbox_w2d, decode_bbox_w2d, atol=0.001)\n    depth_cls_preds = torch.tensor([[-0.4383, 0.7207, -0.4092, 0.4649, 0.8526, 0.6186, -1.4312, -0.715], [0.0621, 0.2369, 0.517, 0.8484, -0.1099, 0.1829, -0.0072, 1.0618], [-1.6114, -0.1057, 0.5721, -0.5986, -2.0471, 0.814, -0.8385, -0.4822], [0.0742, -0.3261, 0.4607, 1.8155, -0.3571, -0.0234, 0.3787, 2.3251], [1.0492, -0.6881, -0.0136, -1.8291, 0.846, -1.0171, 2.5691, -0.8114], [0.0968, -0.5601, 1.0458, 0.256, 1.3018, 0.1635, 0.068, -1.0263], [-0.0765, 0.1498, -2.7321, 1.0047, -0.2505, 0.0871, -0.482, -0.3003], [-0.4123, 0.2298, -0.133, -0.6008, 0.6526, 0.7118, 0.9728, -0.7793], [1.694, 0.3355, 1.4661, 0.5477, 0.8667, 0.0527, -0.9975, -0.0689], [0.4724, -0.3632, -0.0654, 0.4034, -0.3494, -0.7548, 0.7297, 1.2754]])\n    depth_range = (0, 70)\n    depth_unit = 10\n    num_depth_cls = 8\n    uniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'uniform', num_depth_cls)\n    expected_preds = torch.tensor([32.0441, 38.4689, 36.1831, 48.2096, 46.156, 32.7973, 33.2155, 39.9822, 21.9905, 43.0161])\n    assert torch.allclose(uniform_prob_depth_preds, expected_preds, atol=0.001)\n    linear_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'linear', num_depth_cls)\n    expected_preds = torch.tensor([21.1431, 30.2421, 25.8964, 41.6116, 38.6234, 21.4582, 23.2993, 30.1111, 13.9273, 36.8419])\n    assert torch.allclose(linear_prob_depth_preds, expected_preds, atol=0.001)\n    log_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'log', num_depth_cls)\n    expected_preds = torch.tensor([12.6458, 24.2487, 17.4015, 36.9375, 27.5982, 12.551, 15.6635, 19.8408, 9.1605, 31.3765])\n    assert torch.allclose(log_prob_depth_preds, expected_preds, atol=0.001)\n    loguniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'loguniform', num_depth_cls)\n    expected_preds = torch.tensor([6.9925, 10.3273, 8.9895, 18.6524, 16.4667, 7.3196, 7.5078, 11.3207, 3.7987, 13.6095])\n    assert torch.allclose(loguniform_prob_depth_preds, expected_preds, atol=0.001)",
            "def test_pgd_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bbox_coder_cfg = dict(type='PGDBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.0103]], [[0.7394]], [[0.3296]], [[0.4708]], [[0.1439]], [[0.0778]], [[0.9399]], [[0.8366]], [[0.1264]], [[0.303]], [[0.1898]], [[0.0714]], [[0.4144]], [[0.4341]], [[0.6442]], [[0.2951]], [[0.289]], [[0.4486]], [[0.2848]], [[0.1071]], [[0.953]], [[0.946]], [[0.3822]], [[0.932]], [[0.2611]], [[0.558]], [[0.0397]]], [[[0.8612]], [[0.168]], [[0.5167]], [[0.8502]], [[0.0377]], [[0.3615]], [[0.955]], [[0.5219]], [[0.1402]], [[0.6843]], [[0.2121]], [[0.9468]], [[0.6238]], [[0.7918]], [[0.1646]], [[0.05]], [[0.629]], [[0.3956]], [[0.2901]], [[0.4612]], [[0.7333]], [[0.1194]], [[0.6999]], [[0.398]], [[0.3262]], [[0.7185]], [[0.4474]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(5)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    max_regress_range = 16\n    pred_keypoints = True\n    pred_bbox2d = True\n    decode_bbox_w2d = bbox_coder.decode_2d(decode_bbox, batch_scale, stride, max_regress_range, training, pred_keypoints, pred_bbox2d)\n    expected_decode_bbox_w2d = torch.tensor([[[[0.0206]], [[1.4788]], [[1.3904]], [[1.6013]], [[1.1548]], [[1.0809]], [[0.9399]], [[10.9441]], [[2.0117]], [[4.7049]], [[3.0009]], [[1.1405]], [[6.2752]], [[6.5399]], [[9.084]], [[4.5892]], [[4.4994]], [[6.732]], [[4.4375]], [[1.7071]], [[11.8582]], [[11.8075]], [[5.8339]], [[1.864]], [[0.5222]], [[1.116]], [[0.0794]]], [[[1.7224]], [[0.336]], [[1.6765]], [[2.3401]], [[1.0384]], [[1.4355]], [[0.955]], [[7.6666]], [[2.2286]], [[9.5089]], [[3.3436]], [[11.8133]], [[8.8603]], [[10.5508]], [[2.6101]], [[0.7993]], [[8.9178]], [[6.0188]], [[4.5156]], [[6.897]], [[10.0013]], [[1.9014]], [[9.6689]], [[0.796]], [[0.6524]], [[1.437]], [[0.8948]]]])\n    assert torch.allclose(expected_decode_bbox_w2d, decode_bbox_w2d, atol=0.001)\n    depth_cls_preds = torch.tensor([[-0.4383, 0.7207, -0.4092, 0.4649, 0.8526, 0.6186, -1.4312, -0.715], [0.0621, 0.2369, 0.517, 0.8484, -0.1099, 0.1829, -0.0072, 1.0618], [-1.6114, -0.1057, 0.5721, -0.5986, -2.0471, 0.814, -0.8385, -0.4822], [0.0742, -0.3261, 0.4607, 1.8155, -0.3571, -0.0234, 0.3787, 2.3251], [1.0492, -0.6881, -0.0136, -1.8291, 0.846, -1.0171, 2.5691, -0.8114], [0.0968, -0.5601, 1.0458, 0.256, 1.3018, 0.1635, 0.068, -1.0263], [-0.0765, 0.1498, -2.7321, 1.0047, -0.2505, 0.0871, -0.482, -0.3003], [-0.4123, 0.2298, -0.133, -0.6008, 0.6526, 0.7118, 0.9728, -0.7793], [1.694, 0.3355, 1.4661, 0.5477, 0.8667, 0.0527, -0.9975, -0.0689], [0.4724, -0.3632, -0.0654, 0.4034, -0.3494, -0.7548, 0.7297, 1.2754]])\n    depth_range = (0, 70)\n    depth_unit = 10\n    num_depth_cls = 8\n    uniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'uniform', num_depth_cls)\n    expected_preds = torch.tensor([32.0441, 38.4689, 36.1831, 48.2096, 46.156, 32.7973, 33.2155, 39.9822, 21.9905, 43.0161])\n    assert torch.allclose(uniform_prob_depth_preds, expected_preds, atol=0.001)\n    linear_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'linear', num_depth_cls)\n    expected_preds = torch.tensor([21.1431, 30.2421, 25.8964, 41.6116, 38.6234, 21.4582, 23.2993, 30.1111, 13.9273, 36.8419])\n    assert torch.allclose(linear_prob_depth_preds, expected_preds, atol=0.001)\n    log_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'log', num_depth_cls)\n    expected_preds = torch.tensor([12.6458, 24.2487, 17.4015, 36.9375, 27.5982, 12.551, 15.6635, 19.8408, 9.1605, 31.3765])\n    assert torch.allclose(log_prob_depth_preds, expected_preds, atol=0.001)\n    loguniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'loguniform', num_depth_cls)\n    expected_preds = torch.tensor([6.9925, 10.3273, 8.9895, 18.6524, 16.4667, 7.3196, 7.5078, 11.3207, 3.7987, 13.6095])\n    assert torch.allclose(loguniform_prob_depth_preds, expected_preds, atol=0.001)",
            "def test_pgd_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bbox_coder_cfg = dict(type='PGDBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.0103]], [[0.7394]], [[0.3296]], [[0.4708]], [[0.1439]], [[0.0778]], [[0.9399]], [[0.8366]], [[0.1264]], [[0.303]], [[0.1898]], [[0.0714]], [[0.4144]], [[0.4341]], [[0.6442]], [[0.2951]], [[0.289]], [[0.4486]], [[0.2848]], [[0.1071]], [[0.953]], [[0.946]], [[0.3822]], [[0.932]], [[0.2611]], [[0.558]], [[0.0397]]], [[[0.8612]], [[0.168]], [[0.5167]], [[0.8502]], [[0.0377]], [[0.3615]], [[0.955]], [[0.5219]], [[0.1402]], [[0.6843]], [[0.2121]], [[0.9468]], [[0.6238]], [[0.7918]], [[0.1646]], [[0.05]], [[0.629]], [[0.3956]], [[0.2901]], [[0.4612]], [[0.7333]], [[0.1194]], [[0.6999]], [[0.398]], [[0.3262]], [[0.7185]], [[0.4474]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(5)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    max_regress_range = 16\n    pred_keypoints = True\n    pred_bbox2d = True\n    decode_bbox_w2d = bbox_coder.decode_2d(decode_bbox, batch_scale, stride, max_regress_range, training, pred_keypoints, pred_bbox2d)\n    expected_decode_bbox_w2d = torch.tensor([[[[0.0206]], [[1.4788]], [[1.3904]], [[1.6013]], [[1.1548]], [[1.0809]], [[0.9399]], [[10.9441]], [[2.0117]], [[4.7049]], [[3.0009]], [[1.1405]], [[6.2752]], [[6.5399]], [[9.084]], [[4.5892]], [[4.4994]], [[6.732]], [[4.4375]], [[1.7071]], [[11.8582]], [[11.8075]], [[5.8339]], [[1.864]], [[0.5222]], [[1.116]], [[0.0794]]], [[[1.7224]], [[0.336]], [[1.6765]], [[2.3401]], [[1.0384]], [[1.4355]], [[0.955]], [[7.6666]], [[2.2286]], [[9.5089]], [[3.3436]], [[11.8133]], [[8.8603]], [[10.5508]], [[2.6101]], [[0.7993]], [[8.9178]], [[6.0188]], [[4.5156]], [[6.897]], [[10.0013]], [[1.9014]], [[9.6689]], [[0.796]], [[0.6524]], [[1.437]], [[0.8948]]]])\n    assert torch.allclose(expected_decode_bbox_w2d, decode_bbox_w2d, atol=0.001)\n    depth_cls_preds = torch.tensor([[-0.4383, 0.7207, -0.4092, 0.4649, 0.8526, 0.6186, -1.4312, -0.715], [0.0621, 0.2369, 0.517, 0.8484, -0.1099, 0.1829, -0.0072, 1.0618], [-1.6114, -0.1057, 0.5721, -0.5986, -2.0471, 0.814, -0.8385, -0.4822], [0.0742, -0.3261, 0.4607, 1.8155, -0.3571, -0.0234, 0.3787, 2.3251], [1.0492, -0.6881, -0.0136, -1.8291, 0.846, -1.0171, 2.5691, -0.8114], [0.0968, -0.5601, 1.0458, 0.256, 1.3018, 0.1635, 0.068, -1.0263], [-0.0765, 0.1498, -2.7321, 1.0047, -0.2505, 0.0871, -0.482, -0.3003], [-0.4123, 0.2298, -0.133, -0.6008, 0.6526, 0.7118, 0.9728, -0.7793], [1.694, 0.3355, 1.4661, 0.5477, 0.8667, 0.0527, -0.9975, -0.0689], [0.4724, -0.3632, -0.0654, 0.4034, -0.3494, -0.7548, 0.7297, 1.2754]])\n    depth_range = (0, 70)\n    depth_unit = 10\n    num_depth_cls = 8\n    uniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'uniform', num_depth_cls)\n    expected_preds = torch.tensor([32.0441, 38.4689, 36.1831, 48.2096, 46.156, 32.7973, 33.2155, 39.9822, 21.9905, 43.0161])\n    assert torch.allclose(uniform_prob_depth_preds, expected_preds, atol=0.001)\n    linear_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'linear', num_depth_cls)\n    expected_preds = torch.tensor([21.1431, 30.2421, 25.8964, 41.6116, 38.6234, 21.4582, 23.2993, 30.1111, 13.9273, 36.8419])\n    assert torch.allclose(linear_prob_depth_preds, expected_preds, atol=0.001)\n    log_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'log', num_depth_cls)\n    expected_preds = torch.tensor([12.6458, 24.2487, 17.4015, 36.9375, 27.5982, 12.551, 15.6635, 19.8408, 9.1605, 31.3765])\n    assert torch.allclose(log_prob_depth_preds, expected_preds, atol=0.001)\n    loguniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'loguniform', num_depth_cls)\n    expected_preds = torch.tensor([6.9925, 10.3273, 8.9895, 18.6524, 16.4667, 7.3196, 7.5078, 11.3207, 3.7987, 13.6095])\n    assert torch.allclose(loguniform_prob_depth_preds, expected_preds, atol=0.001)",
            "def test_pgd_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bbox_coder_cfg = dict(type='PGDBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.0103]], [[0.7394]], [[0.3296]], [[0.4708]], [[0.1439]], [[0.0778]], [[0.9399]], [[0.8366]], [[0.1264]], [[0.303]], [[0.1898]], [[0.0714]], [[0.4144]], [[0.4341]], [[0.6442]], [[0.2951]], [[0.289]], [[0.4486]], [[0.2848]], [[0.1071]], [[0.953]], [[0.946]], [[0.3822]], [[0.932]], [[0.2611]], [[0.558]], [[0.0397]]], [[[0.8612]], [[0.168]], [[0.5167]], [[0.8502]], [[0.0377]], [[0.3615]], [[0.955]], [[0.5219]], [[0.1402]], [[0.6843]], [[0.2121]], [[0.9468]], [[0.6238]], [[0.7918]], [[0.1646]], [[0.05]], [[0.629]], [[0.3956]], [[0.2901]], [[0.4612]], [[0.7333]], [[0.1194]], [[0.6999]], [[0.398]], [[0.3262]], [[0.7185]], [[0.4474]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(5)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    max_regress_range = 16\n    pred_keypoints = True\n    pred_bbox2d = True\n    decode_bbox_w2d = bbox_coder.decode_2d(decode_bbox, batch_scale, stride, max_regress_range, training, pred_keypoints, pred_bbox2d)\n    expected_decode_bbox_w2d = torch.tensor([[[[0.0206]], [[1.4788]], [[1.3904]], [[1.6013]], [[1.1548]], [[1.0809]], [[0.9399]], [[10.9441]], [[2.0117]], [[4.7049]], [[3.0009]], [[1.1405]], [[6.2752]], [[6.5399]], [[9.084]], [[4.5892]], [[4.4994]], [[6.732]], [[4.4375]], [[1.7071]], [[11.8582]], [[11.8075]], [[5.8339]], [[1.864]], [[0.5222]], [[1.116]], [[0.0794]]], [[[1.7224]], [[0.336]], [[1.6765]], [[2.3401]], [[1.0384]], [[1.4355]], [[0.955]], [[7.6666]], [[2.2286]], [[9.5089]], [[3.3436]], [[11.8133]], [[8.8603]], [[10.5508]], [[2.6101]], [[0.7993]], [[8.9178]], [[6.0188]], [[4.5156]], [[6.897]], [[10.0013]], [[1.9014]], [[9.6689]], [[0.796]], [[0.6524]], [[1.437]], [[0.8948]]]])\n    assert torch.allclose(expected_decode_bbox_w2d, decode_bbox_w2d, atol=0.001)\n    depth_cls_preds = torch.tensor([[-0.4383, 0.7207, -0.4092, 0.4649, 0.8526, 0.6186, -1.4312, -0.715], [0.0621, 0.2369, 0.517, 0.8484, -0.1099, 0.1829, -0.0072, 1.0618], [-1.6114, -0.1057, 0.5721, -0.5986, -2.0471, 0.814, -0.8385, -0.4822], [0.0742, -0.3261, 0.4607, 1.8155, -0.3571, -0.0234, 0.3787, 2.3251], [1.0492, -0.6881, -0.0136, -1.8291, 0.846, -1.0171, 2.5691, -0.8114], [0.0968, -0.5601, 1.0458, 0.256, 1.3018, 0.1635, 0.068, -1.0263], [-0.0765, 0.1498, -2.7321, 1.0047, -0.2505, 0.0871, -0.482, -0.3003], [-0.4123, 0.2298, -0.133, -0.6008, 0.6526, 0.7118, 0.9728, -0.7793], [1.694, 0.3355, 1.4661, 0.5477, 0.8667, 0.0527, -0.9975, -0.0689], [0.4724, -0.3632, -0.0654, 0.4034, -0.3494, -0.7548, 0.7297, 1.2754]])\n    depth_range = (0, 70)\n    depth_unit = 10\n    num_depth_cls = 8\n    uniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'uniform', num_depth_cls)\n    expected_preds = torch.tensor([32.0441, 38.4689, 36.1831, 48.2096, 46.156, 32.7973, 33.2155, 39.9822, 21.9905, 43.0161])\n    assert torch.allclose(uniform_prob_depth_preds, expected_preds, atol=0.001)\n    linear_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'linear', num_depth_cls)\n    expected_preds = torch.tensor([21.1431, 30.2421, 25.8964, 41.6116, 38.6234, 21.4582, 23.2993, 30.1111, 13.9273, 36.8419])\n    assert torch.allclose(linear_prob_depth_preds, expected_preds, atol=0.001)\n    log_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'log', num_depth_cls)\n    expected_preds = torch.tensor([12.6458, 24.2487, 17.4015, 36.9375, 27.5982, 12.551, 15.6635, 19.8408, 9.1605, 31.3765])\n    assert torch.allclose(log_prob_depth_preds, expected_preds, atol=0.001)\n    loguniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'loguniform', num_depth_cls)\n    expected_preds = torch.tensor([6.9925, 10.3273, 8.9895, 18.6524, 16.4667, 7.3196, 7.5078, 11.3207, 3.7987, 13.6095])\n    assert torch.allclose(loguniform_prob_depth_preds, expected_preds, atol=0.001)",
            "def test_pgd_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bbox_coder_cfg = dict(type='PGDBBoxCoder', base_depths=None, base_dims=None, code_size=7, norm_on_bbox=True)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    batch_bbox = torch.tensor([[[[0.0103]], [[0.7394]], [[0.3296]], [[0.4708]], [[0.1439]], [[0.0778]], [[0.9399]], [[0.8366]], [[0.1264]], [[0.303]], [[0.1898]], [[0.0714]], [[0.4144]], [[0.4341]], [[0.6442]], [[0.2951]], [[0.289]], [[0.4486]], [[0.2848]], [[0.1071]], [[0.953]], [[0.946]], [[0.3822]], [[0.932]], [[0.2611]], [[0.558]], [[0.0397]]], [[[0.8612]], [[0.168]], [[0.5167]], [[0.8502]], [[0.0377]], [[0.3615]], [[0.955]], [[0.5219]], [[0.1402]], [[0.6843]], [[0.2121]], [[0.9468]], [[0.6238]], [[0.7918]], [[0.1646]], [[0.05]], [[0.629]], [[0.3956]], [[0.2901]], [[0.4612]], [[0.7333]], [[0.1194]], [[0.6999]], [[0.398]], [[0.3262]], [[0.7185]], [[0.4474]]]])\n    batch_scale = nn.ModuleList([Scale(1.0) for _ in range(5)])\n    stride = 2\n    training = False\n    cls_score = torch.randn([2, 2, 1, 1]).sigmoid()\n    decode_bbox = bbox_coder.decode(batch_bbox, batch_scale, stride, training, cls_score)\n    max_regress_range = 16\n    pred_keypoints = True\n    pred_bbox2d = True\n    decode_bbox_w2d = bbox_coder.decode_2d(decode_bbox, batch_scale, stride, max_regress_range, training, pred_keypoints, pred_bbox2d)\n    expected_decode_bbox_w2d = torch.tensor([[[[0.0206]], [[1.4788]], [[1.3904]], [[1.6013]], [[1.1548]], [[1.0809]], [[0.9399]], [[10.9441]], [[2.0117]], [[4.7049]], [[3.0009]], [[1.1405]], [[6.2752]], [[6.5399]], [[9.084]], [[4.5892]], [[4.4994]], [[6.732]], [[4.4375]], [[1.7071]], [[11.8582]], [[11.8075]], [[5.8339]], [[1.864]], [[0.5222]], [[1.116]], [[0.0794]]], [[[1.7224]], [[0.336]], [[1.6765]], [[2.3401]], [[1.0384]], [[1.4355]], [[0.955]], [[7.6666]], [[2.2286]], [[9.5089]], [[3.3436]], [[11.8133]], [[8.8603]], [[10.5508]], [[2.6101]], [[0.7993]], [[8.9178]], [[6.0188]], [[4.5156]], [[6.897]], [[10.0013]], [[1.9014]], [[9.6689]], [[0.796]], [[0.6524]], [[1.437]], [[0.8948]]]])\n    assert torch.allclose(expected_decode_bbox_w2d, decode_bbox_w2d, atol=0.001)\n    depth_cls_preds = torch.tensor([[-0.4383, 0.7207, -0.4092, 0.4649, 0.8526, 0.6186, -1.4312, -0.715], [0.0621, 0.2369, 0.517, 0.8484, -0.1099, 0.1829, -0.0072, 1.0618], [-1.6114, -0.1057, 0.5721, -0.5986, -2.0471, 0.814, -0.8385, -0.4822], [0.0742, -0.3261, 0.4607, 1.8155, -0.3571, -0.0234, 0.3787, 2.3251], [1.0492, -0.6881, -0.0136, -1.8291, 0.846, -1.0171, 2.5691, -0.8114], [0.0968, -0.5601, 1.0458, 0.256, 1.3018, 0.1635, 0.068, -1.0263], [-0.0765, 0.1498, -2.7321, 1.0047, -0.2505, 0.0871, -0.482, -0.3003], [-0.4123, 0.2298, -0.133, -0.6008, 0.6526, 0.7118, 0.9728, -0.7793], [1.694, 0.3355, 1.4661, 0.5477, 0.8667, 0.0527, -0.9975, -0.0689], [0.4724, -0.3632, -0.0654, 0.4034, -0.3494, -0.7548, 0.7297, 1.2754]])\n    depth_range = (0, 70)\n    depth_unit = 10\n    num_depth_cls = 8\n    uniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'uniform', num_depth_cls)\n    expected_preds = torch.tensor([32.0441, 38.4689, 36.1831, 48.2096, 46.156, 32.7973, 33.2155, 39.9822, 21.9905, 43.0161])\n    assert torch.allclose(uniform_prob_depth_preds, expected_preds, atol=0.001)\n    linear_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'linear', num_depth_cls)\n    expected_preds = torch.tensor([21.1431, 30.2421, 25.8964, 41.6116, 38.6234, 21.4582, 23.2993, 30.1111, 13.9273, 36.8419])\n    assert torch.allclose(linear_prob_depth_preds, expected_preds, atol=0.001)\n    log_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'log', num_depth_cls)\n    expected_preds = torch.tensor([12.6458, 24.2487, 17.4015, 36.9375, 27.5982, 12.551, 15.6635, 19.8408, 9.1605, 31.3765])\n    assert torch.allclose(log_prob_depth_preds, expected_preds, atol=0.001)\n    loguniform_prob_depth_preds = bbox_coder.decode_prob_depth(depth_cls_preds, depth_range, depth_unit, 'loguniform', num_depth_cls)\n    expected_preds = torch.tensor([6.9925, 10.3273, 8.9895, 18.6524, 16.4667, 7.3196, 7.5078, 11.3207, 3.7987, 13.6095])\n    assert torch.allclose(loguniform_prob_depth_preds, expected_preds, atol=0.001)"
        ]
    },
    {
        "func_name": "test_smoke_bbox_coder",
        "original": "def test_smoke_bbox_coder():\n    bbox_coder_cfg = dict(type='SMOKECoder', base_depth=(28.01, 16.32), base_dims=((3.88, 1.63, 1.53), (1.78, 1.7, 0.58), (0.88, 1.73, 0.67)), code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    regression = torch.rand([200, 8])\n    points = torch.rand([200, 2])\n    labels = torch.ones([2, 100])\n    cam2imgs = torch.rand([2, 4, 4])\n    trans_mats = torch.rand([2, 3, 3])\n    img_metas = [dict(box_type_3d=CameraInstance3DBoxes) for i in range(2)]\n    (locations, dimensions, orientations) = bbox_coder.decode(regression, points, labels, cam2imgs, trans_mats)\n    assert locations.shape == torch.Size([200, 3])\n    assert dimensions.shape == torch.Size([200, 3])\n    assert orientations.shape == torch.Size([200, 1])\n    bboxes = bbox_coder.encode(locations, dimensions, orientations, img_metas)\n    assert bboxes.tensor.shape == torch.Size([200, 7])\n    ori_vector = torch.tensor([[-0.9, -0.01], [-0.9, 0.01]])\n    locations = torch.tensor([[15.0, 2.0, 1.0], [15.0, 2.0, -1.0]])\n    orientations = bbox_coder._decode_orientation(ori_vector, locations)\n    assert orientations.shape == torch.Size([2, 1])",
        "mutated": [
            "def test_smoke_bbox_coder():\n    if False:\n        i = 10\n    bbox_coder_cfg = dict(type='SMOKECoder', base_depth=(28.01, 16.32), base_dims=((3.88, 1.63, 1.53), (1.78, 1.7, 0.58), (0.88, 1.73, 0.67)), code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    regression = torch.rand([200, 8])\n    points = torch.rand([200, 2])\n    labels = torch.ones([2, 100])\n    cam2imgs = torch.rand([2, 4, 4])\n    trans_mats = torch.rand([2, 3, 3])\n    img_metas = [dict(box_type_3d=CameraInstance3DBoxes) for i in range(2)]\n    (locations, dimensions, orientations) = bbox_coder.decode(regression, points, labels, cam2imgs, trans_mats)\n    assert locations.shape == torch.Size([200, 3])\n    assert dimensions.shape == torch.Size([200, 3])\n    assert orientations.shape == torch.Size([200, 1])\n    bboxes = bbox_coder.encode(locations, dimensions, orientations, img_metas)\n    assert bboxes.tensor.shape == torch.Size([200, 7])\n    ori_vector = torch.tensor([[-0.9, -0.01], [-0.9, 0.01]])\n    locations = torch.tensor([[15.0, 2.0, 1.0], [15.0, 2.0, -1.0]])\n    orientations = bbox_coder._decode_orientation(ori_vector, locations)\n    assert orientations.shape == torch.Size([2, 1])",
            "def test_smoke_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bbox_coder_cfg = dict(type='SMOKECoder', base_depth=(28.01, 16.32), base_dims=((3.88, 1.63, 1.53), (1.78, 1.7, 0.58), (0.88, 1.73, 0.67)), code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    regression = torch.rand([200, 8])\n    points = torch.rand([200, 2])\n    labels = torch.ones([2, 100])\n    cam2imgs = torch.rand([2, 4, 4])\n    trans_mats = torch.rand([2, 3, 3])\n    img_metas = [dict(box_type_3d=CameraInstance3DBoxes) for i in range(2)]\n    (locations, dimensions, orientations) = bbox_coder.decode(regression, points, labels, cam2imgs, trans_mats)\n    assert locations.shape == torch.Size([200, 3])\n    assert dimensions.shape == torch.Size([200, 3])\n    assert orientations.shape == torch.Size([200, 1])\n    bboxes = bbox_coder.encode(locations, dimensions, orientations, img_metas)\n    assert bboxes.tensor.shape == torch.Size([200, 7])\n    ori_vector = torch.tensor([[-0.9, -0.01], [-0.9, 0.01]])\n    locations = torch.tensor([[15.0, 2.0, 1.0], [15.0, 2.0, -1.0]])\n    orientations = bbox_coder._decode_orientation(ori_vector, locations)\n    assert orientations.shape == torch.Size([2, 1])",
            "def test_smoke_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bbox_coder_cfg = dict(type='SMOKECoder', base_depth=(28.01, 16.32), base_dims=((3.88, 1.63, 1.53), (1.78, 1.7, 0.58), (0.88, 1.73, 0.67)), code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    regression = torch.rand([200, 8])\n    points = torch.rand([200, 2])\n    labels = torch.ones([2, 100])\n    cam2imgs = torch.rand([2, 4, 4])\n    trans_mats = torch.rand([2, 3, 3])\n    img_metas = [dict(box_type_3d=CameraInstance3DBoxes) for i in range(2)]\n    (locations, dimensions, orientations) = bbox_coder.decode(regression, points, labels, cam2imgs, trans_mats)\n    assert locations.shape == torch.Size([200, 3])\n    assert dimensions.shape == torch.Size([200, 3])\n    assert orientations.shape == torch.Size([200, 1])\n    bboxes = bbox_coder.encode(locations, dimensions, orientations, img_metas)\n    assert bboxes.tensor.shape == torch.Size([200, 7])\n    ori_vector = torch.tensor([[-0.9, -0.01], [-0.9, 0.01]])\n    locations = torch.tensor([[15.0, 2.0, 1.0], [15.0, 2.0, -1.0]])\n    orientations = bbox_coder._decode_orientation(ori_vector, locations)\n    assert orientations.shape == torch.Size([2, 1])",
            "def test_smoke_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bbox_coder_cfg = dict(type='SMOKECoder', base_depth=(28.01, 16.32), base_dims=((3.88, 1.63, 1.53), (1.78, 1.7, 0.58), (0.88, 1.73, 0.67)), code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    regression = torch.rand([200, 8])\n    points = torch.rand([200, 2])\n    labels = torch.ones([2, 100])\n    cam2imgs = torch.rand([2, 4, 4])\n    trans_mats = torch.rand([2, 3, 3])\n    img_metas = [dict(box_type_3d=CameraInstance3DBoxes) for i in range(2)]\n    (locations, dimensions, orientations) = bbox_coder.decode(regression, points, labels, cam2imgs, trans_mats)\n    assert locations.shape == torch.Size([200, 3])\n    assert dimensions.shape == torch.Size([200, 3])\n    assert orientations.shape == torch.Size([200, 1])\n    bboxes = bbox_coder.encode(locations, dimensions, orientations, img_metas)\n    assert bboxes.tensor.shape == torch.Size([200, 7])\n    ori_vector = torch.tensor([[-0.9, -0.01], [-0.9, 0.01]])\n    locations = torch.tensor([[15.0, 2.0, 1.0], [15.0, 2.0, -1.0]])\n    orientations = bbox_coder._decode_orientation(ori_vector, locations)\n    assert orientations.shape == torch.Size([2, 1])",
            "def test_smoke_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bbox_coder_cfg = dict(type='SMOKECoder', base_depth=(28.01, 16.32), base_dims=((3.88, 1.63, 1.53), (1.78, 1.7, 0.58), (0.88, 1.73, 0.67)), code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    regression = torch.rand([200, 8])\n    points = torch.rand([200, 2])\n    labels = torch.ones([2, 100])\n    cam2imgs = torch.rand([2, 4, 4])\n    trans_mats = torch.rand([2, 3, 3])\n    img_metas = [dict(box_type_3d=CameraInstance3DBoxes) for i in range(2)]\n    (locations, dimensions, orientations) = bbox_coder.decode(regression, points, labels, cam2imgs, trans_mats)\n    assert locations.shape == torch.Size([200, 3])\n    assert dimensions.shape == torch.Size([200, 3])\n    assert orientations.shape == torch.Size([200, 1])\n    bboxes = bbox_coder.encode(locations, dimensions, orientations, img_metas)\n    assert bboxes.tensor.shape == torch.Size([200, 7])\n    ori_vector = torch.tensor([[-0.9, -0.01], [-0.9, 0.01]])\n    locations = torch.tensor([[15.0, 2.0, 1.0], [15.0, 2.0, -1.0]])\n    orientations = bbox_coder._decode_orientation(ori_vector, locations)\n    assert orientations.shape == torch.Size([2, 1])"
        ]
    },
    {
        "func_name": "test_monoflex_bbox_coder",
        "original": "def test_monoflex_bbox_coder():\n    bbox_coder_cfg = dict(type='MonoFlexCoder', depth_mode='exp', base_depth=(26.494627, 16.05988), depth_range=[0.1, 100], combine_depth=True, uncertainty_range=[-10, 10], base_dims=((3.884, 1.5261, 1.6286, 0.4259, 0.1367, 0.1022), (0.8423, 1.7607, 0.6602, 0.2349, 0.1133, 0.1427), (1.7635, 1.7372, 0.5968, 0.1766, 0.0948, 0.1242)), dims_mode='linear', multibin=True, num_dir_bins=4, bin_centers=[0, np.pi / 2, np.pi, -np.pi / 2], bin_margin=np.pi / 6, code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = CameraInstance3DBoxes(torch.rand([6, 7]))\n    orientation_target = bbox_coder.encode(gt_bboxes_3d)\n    assert orientation_target.shape == torch.Size([6, 8])\n    regression = torch.rand([100, 50])\n    base_centers2d = torch.rand([100, 2])\n    labels = torch.ones([100])\n    downsample_ratio = 4\n    cam2imgs = torch.rand([100, 4, 4])\n    preds = bbox_coder.decode(regression, base_centers2d, labels, downsample_ratio, cam2imgs)\n    assert preds['bboxes2d'].shape == torch.Size([100, 4])\n    assert preds['dimensions'].shape == torch.Size([100, 3])\n    assert preds['offsets2d'].shape == torch.Size([100, 2])\n    assert preds['keypoints2d'].shape == torch.Size([100, 10, 2])\n    assert preds['orientations'].shape == torch.Size([100, 16])\n    assert preds['direct_depth'].shape == torch.Size([100])\n    assert preds['keypoints_depth'].shape == torch.Size([100, 3])\n    assert preds['combined_depth'].shape == torch.Size([100])\n    assert preds['direct_depth_uncertainty'].shape == torch.Size([100])\n    assert preds['keypoints_depth_uncertainty'].shape == torch.Size([100, 3])\n    offsets_2d = torch.randn([100, 2])\n    depths = torch.randn([100])\n    locations = bbox_coder.decode_location(base_centers2d, offsets_2d, depths, cam2imgs, downsample_ratio)\n    assert locations.shape == torch.Size([100, 3])\n    orientations = torch.randn([100, 16])\n    (yaws, local_yaws) = bbox_coder.decode_orientation(orientations, locations)\n    assert yaws.shape == torch.Size([100])\n    assert local_yaws.shape == torch.Size([100])",
        "mutated": [
            "def test_monoflex_bbox_coder():\n    if False:\n        i = 10\n    bbox_coder_cfg = dict(type='MonoFlexCoder', depth_mode='exp', base_depth=(26.494627, 16.05988), depth_range=[0.1, 100], combine_depth=True, uncertainty_range=[-10, 10], base_dims=((3.884, 1.5261, 1.6286, 0.4259, 0.1367, 0.1022), (0.8423, 1.7607, 0.6602, 0.2349, 0.1133, 0.1427), (1.7635, 1.7372, 0.5968, 0.1766, 0.0948, 0.1242)), dims_mode='linear', multibin=True, num_dir_bins=4, bin_centers=[0, np.pi / 2, np.pi, -np.pi / 2], bin_margin=np.pi / 6, code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = CameraInstance3DBoxes(torch.rand([6, 7]))\n    orientation_target = bbox_coder.encode(gt_bboxes_3d)\n    assert orientation_target.shape == torch.Size([6, 8])\n    regression = torch.rand([100, 50])\n    base_centers2d = torch.rand([100, 2])\n    labels = torch.ones([100])\n    downsample_ratio = 4\n    cam2imgs = torch.rand([100, 4, 4])\n    preds = bbox_coder.decode(regression, base_centers2d, labels, downsample_ratio, cam2imgs)\n    assert preds['bboxes2d'].shape == torch.Size([100, 4])\n    assert preds['dimensions'].shape == torch.Size([100, 3])\n    assert preds['offsets2d'].shape == torch.Size([100, 2])\n    assert preds['keypoints2d'].shape == torch.Size([100, 10, 2])\n    assert preds['orientations'].shape == torch.Size([100, 16])\n    assert preds['direct_depth'].shape == torch.Size([100])\n    assert preds['keypoints_depth'].shape == torch.Size([100, 3])\n    assert preds['combined_depth'].shape == torch.Size([100])\n    assert preds['direct_depth_uncertainty'].shape == torch.Size([100])\n    assert preds['keypoints_depth_uncertainty'].shape == torch.Size([100, 3])\n    offsets_2d = torch.randn([100, 2])\n    depths = torch.randn([100])\n    locations = bbox_coder.decode_location(base_centers2d, offsets_2d, depths, cam2imgs, downsample_ratio)\n    assert locations.shape == torch.Size([100, 3])\n    orientations = torch.randn([100, 16])\n    (yaws, local_yaws) = bbox_coder.decode_orientation(orientations, locations)\n    assert yaws.shape == torch.Size([100])\n    assert local_yaws.shape == torch.Size([100])",
            "def test_monoflex_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bbox_coder_cfg = dict(type='MonoFlexCoder', depth_mode='exp', base_depth=(26.494627, 16.05988), depth_range=[0.1, 100], combine_depth=True, uncertainty_range=[-10, 10], base_dims=((3.884, 1.5261, 1.6286, 0.4259, 0.1367, 0.1022), (0.8423, 1.7607, 0.6602, 0.2349, 0.1133, 0.1427), (1.7635, 1.7372, 0.5968, 0.1766, 0.0948, 0.1242)), dims_mode='linear', multibin=True, num_dir_bins=4, bin_centers=[0, np.pi / 2, np.pi, -np.pi / 2], bin_margin=np.pi / 6, code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = CameraInstance3DBoxes(torch.rand([6, 7]))\n    orientation_target = bbox_coder.encode(gt_bboxes_3d)\n    assert orientation_target.shape == torch.Size([6, 8])\n    regression = torch.rand([100, 50])\n    base_centers2d = torch.rand([100, 2])\n    labels = torch.ones([100])\n    downsample_ratio = 4\n    cam2imgs = torch.rand([100, 4, 4])\n    preds = bbox_coder.decode(regression, base_centers2d, labels, downsample_ratio, cam2imgs)\n    assert preds['bboxes2d'].shape == torch.Size([100, 4])\n    assert preds['dimensions'].shape == torch.Size([100, 3])\n    assert preds['offsets2d'].shape == torch.Size([100, 2])\n    assert preds['keypoints2d'].shape == torch.Size([100, 10, 2])\n    assert preds['orientations'].shape == torch.Size([100, 16])\n    assert preds['direct_depth'].shape == torch.Size([100])\n    assert preds['keypoints_depth'].shape == torch.Size([100, 3])\n    assert preds['combined_depth'].shape == torch.Size([100])\n    assert preds['direct_depth_uncertainty'].shape == torch.Size([100])\n    assert preds['keypoints_depth_uncertainty'].shape == torch.Size([100, 3])\n    offsets_2d = torch.randn([100, 2])\n    depths = torch.randn([100])\n    locations = bbox_coder.decode_location(base_centers2d, offsets_2d, depths, cam2imgs, downsample_ratio)\n    assert locations.shape == torch.Size([100, 3])\n    orientations = torch.randn([100, 16])\n    (yaws, local_yaws) = bbox_coder.decode_orientation(orientations, locations)\n    assert yaws.shape == torch.Size([100])\n    assert local_yaws.shape == torch.Size([100])",
            "def test_monoflex_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bbox_coder_cfg = dict(type='MonoFlexCoder', depth_mode='exp', base_depth=(26.494627, 16.05988), depth_range=[0.1, 100], combine_depth=True, uncertainty_range=[-10, 10], base_dims=((3.884, 1.5261, 1.6286, 0.4259, 0.1367, 0.1022), (0.8423, 1.7607, 0.6602, 0.2349, 0.1133, 0.1427), (1.7635, 1.7372, 0.5968, 0.1766, 0.0948, 0.1242)), dims_mode='linear', multibin=True, num_dir_bins=4, bin_centers=[0, np.pi / 2, np.pi, -np.pi / 2], bin_margin=np.pi / 6, code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = CameraInstance3DBoxes(torch.rand([6, 7]))\n    orientation_target = bbox_coder.encode(gt_bboxes_3d)\n    assert orientation_target.shape == torch.Size([6, 8])\n    regression = torch.rand([100, 50])\n    base_centers2d = torch.rand([100, 2])\n    labels = torch.ones([100])\n    downsample_ratio = 4\n    cam2imgs = torch.rand([100, 4, 4])\n    preds = bbox_coder.decode(regression, base_centers2d, labels, downsample_ratio, cam2imgs)\n    assert preds['bboxes2d'].shape == torch.Size([100, 4])\n    assert preds['dimensions'].shape == torch.Size([100, 3])\n    assert preds['offsets2d'].shape == torch.Size([100, 2])\n    assert preds['keypoints2d'].shape == torch.Size([100, 10, 2])\n    assert preds['orientations'].shape == torch.Size([100, 16])\n    assert preds['direct_depth'].shape == torch.Size([100])\n    assert preds['keypoints_depth'].shape == torch.Size([100, 3])\n    assert preds['combined_depth'].shape == torch.Size([100])\n    assert preds['direct_depth_uncertainty'].shape == torch.Size([100])\n    assert preds['keypoints_depth_uncertainty'].shape == torch.Size([100, 3])\n    offsets_2d = torch.randn([100, 2])\n    depths = torch.randn([100])\n    locations = bbox_coder.decode_location(base_centers2d, offsets_2d, depths, cam2imgs, downsample_ratio)\n    assert locations.shape == torch.Size([100, 3])\n    orientations = torch.randn([100, 16])\n    (yaws, local_yaws) = bbox_coder.decode_orientation(orientations, locations)\n    assert yaws.shape == torch.Size([100])\n    assert local_yaws.shape == torch.Size([100])",
            "def test_monoflex_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bbox_coder_cfg = dict(type='MonoFlexCoder', depth_mode='exp', base_depth=(26.494627, 16.05988), depth_range=[0.1, 100], combine_depth=True, uncertainty_range=[-10, 10], base_dims=((3.884, 1.5261, 1.6286, 0.4259, 0.1367, 0.1022), (0.8423, 1.7607, 0.6602, 0.2349, 0.1133, 0.1427), (1.7635, 1.7372, 0.5968, 0.1766, 0.0948, 0.1242)), dims_mode='linear', multibin=True, num_dir_bins=4, bin_centers=[0, np.pi / 2, np.pi, -np.pi / 2], bin_margin=np.pi / 6, code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = CameraInstance3DBoxes(torch.rand([6, 7]))\n    orientation_target = bbox_coder.encode(gt_bboxes_3d)\n    assert orientation_target.shape == torch.Size([6, 8])\n    regression = torch.rand([100, 50])\n    base_centers2d = torch.rand([100, 2])\n    labels = torch.ones([100])\n    downsample_ratio = 4\n    cam2imgs = torch.rand([100, 4, 4])\n    preds = bbox_coder.decode(regression, base_centers2d, labels, downsample_ratio, cam2imgs)\n    assert preds['bboxes2d'].shape == torch.Size([100, 4])\n    assert preds['dimensions'].shape == torch.Size([100, 3])\n    assert preds['offsets2d'].shape == torch.Size([100, 2])\n    assert preds['keypoints2d'].shape == torch.Size([100, 10, 2])\n    assert preds['orientations'].shape == torch.Size([100, 16])\n    assert preds['direct_depth'].shape == torch.Size([100])\n    assert preds['keypoints_depth'].shape == torch.Size([100, 3])\n    assert preds['combined_depth'].shape == torch.Size([100])\n    assert preds['direct_depth_uncertainty'].shape == torch.Size([100])\n    assert preds['keypoints_depth_uncertainty'].shape == torch.Size([100, 3])\n    offsets_2d = torch.randn([100, 2])\n    depths = torch.randn([100])\n    locations = bbox_coder.decode_location(base_centers2d, offsets_2d, depths, cam2imgs, downsample_ratio)\n    assert locations.shape == torch.Size([100, 3])\n    orientations = torch.randn([100, 16])\n    (yaws, local_yaws) = bbox_coder.decode_orientation(orientations, locations)\n    assert yaws.shape == torch.Size([100])\n    assert local_yaws.shape == torch.Size([100])",
            "def test_monoflex_bbox_coder():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bbox_coder_cfg = dict(type='MonoFlexCoder', depth_mode='exp', base_depth=(26.494627, 16.05988), depth_range=[0.1, 100], combine_depth=True, uncertainty_range=[-10, 10], base_dims=((3.884, 1.5261, 1.6286, 0.4259, 0.1367, 0.1022), (0.8423, 1.7607, 0.6602, 0.2349, 0.1133, 0.1427), (1.7635, 1.7372, 0.5968, 0.1766, 0.0948, 0.1242)), dims_mode='linear', multibin=True, num_dir_bins=4, bin_centers=[0, np.pi / 2, np.pi, -np.pi / 2], bin_margin=np.pi / 6, code_size=7)\n    bbox_coder = build_bbox_coder(bbox_coder_cfg)\n    gt_bboxes_3d = CameraInstance3DBoxes(torch.rand([6, 7]))\n    orientation_target = bbox_coder.encode(gt_bboxes_3d)\n    assert orientation_target.shape == torch.Size([6, 8])\n    regression = torch.rand([100, 50])\n    base_centers2d = torch.rand([100, 2])\n    labels = torch.ones([100])\n    downsample_ratio = 4\n    cam2imgs = torch.rand([100, 4, 4])\n    preds = bbox_coder.decode(regression, base_centers2d, labels, downsample_ratio, cam2imgs)\n    assert preds['bboxes2d'].shape == torch.Size([100, 4])\n    assert preds['dimensions'].shape == torch.Size([100, 3])\n    assert preds['offsets2d'].shape == torch.Size([100, 2])\n    assert preds['keypoints2d'].shape == torch.Size([100, 10, 2])\n    assert preds['orientations'].shape == torch.Size([100, 16])\n    assert preds['direct_depth'].shape == torch.Size([100])\n    assert preds['keypoints_depth'].shape == torch.Size([100, 3])\n    assert preds['combined_depth'].shape == torch.Size([100])\n    assert preds['direct_depth_uncertainty'].shape == torch.Size([100])\n    assert preds['keypoints_depth_uncertainty'].shape == torch.Size([100, 3])\n    offsets_2d = torch.randn([100, 2])\n    depths = torch.randn([100])\n    locations = bbox_coder.decode_location(base_centers2d, offsets_2d, depths, cam2imgs, downsample_ratio)\n    assert locations.shape == torch.Size([100, 3])\n    orientations = torch.randn([100, 16])\n    (yaws, local_yaws) = bbox_coder.decode_orientation(orientations, locations)\n    assert yaws.shape == torch.Size([100])\n    assert local_yaws.shape == torch.Size([100])"
        ]
    }
]