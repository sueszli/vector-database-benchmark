[
    {
        "func_name": "__init__",
        "original": "def __init__(self, state_dim, action_dim, action_bound, method='clip'):\n    with tf.name_scope('critic'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        v = tl.layers.Dense(1)(layer)\n    self.critic = tl.models.Model(inputs, v)\n    self.critic.train()\n    with tf.name_scope('actor'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\n        mean = tl.layers.Lambda(lambda x: x * action_bound, name='lambda')(a)\n        logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))\n    self.actor = tl.models.Model(inputs, mean)\n    self.actor.trainable_weights.append(logstd)\n    self.actor.logstd = logstd\n    self.actor.train()\n    self.actor_opt = tf.optimizers.Adam(LR_A)\n    self.critic_opt = tf.optimizers.Adam(LR_C)\n    self.method = method\n    if method == 'penalty':\n        self.kl_target = KL_TARGET\n        self.lam = LAM\n    elif method == 'clip':\n        self.epsilon = EPSILON\n    (self.state_buffer, self.action_buffer) = ([], [])\n    (self.reward_buffer, self.cumulative_reward_buffer) = ([], [])\n    self.action_bound = action_bound",
        "mutated": [
            "def __init__(self, state_dim, action_dim, action_bound, method='clip'):\n    if False:\n        i = 10\n    with tf.name_scope('critic'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        v = tl.layers.Dense(1)(layer)\n    self.critic = tl.models.Model(inputs, v)\n    self.critic.train()\n    with tf.name_scope('actor'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\n        mean = tl.layers.Lambda(lambda x: x * action_bound, name='lambda')(a)\n        logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))\n    self.actor = tl.models.Model(inputs, mean)\n    self.actor.trainable_weights.append(logstd)\n    self.actor.logstd = logstd\n    self.actor.train()\n    self.actor_opt = tf.optimizers.Adam(LR_A)\n    self.critic_opt = tf.optimizers.Adam(LR_C)\n    self.method = method\n    if method == 'penalty':\n        self.kl_target = KL_TARGET\n        self.lam = LAM\n    elif method == 'clip':\n        self.epsilon = EPSILON\n    (self.state_buffer, self.action_buffer) = ([], [])\n    (self.reward_buffer, self.cumulative_reward_buffer) = ([], [])\n    self.action_bound = action_bound",
            "def __init__(self, state_dim, action_dim, action_bound, method='clip'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with tf.name_scope('critic'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        v = tl.layers.Dense(1)(layer)\n    self.critic = tl.models.Model(inputs, v)\n    self.critic.train()\n    with tf.name_scope('actor'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\n        mean = tl.layers.Lambda(lambda x: x * action_bound, name='lambda')(a)\n        logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))\n    self.actor = tl.models.Model(inputs, mean)\n    self.actor.trainable_weights.append(logstd)\n    self.actor.logstd = logstd\n    self.actor.train()\n    self.actor_opt = tf.optimizers.Adam(LR_A)\n    self.critic_opt = tf.optimizers.Adam(LR_C)\n    self.method = method\n    if method == 'penalty':\n        self.kl_target = KL_TARGET\n        self.lam = LAM\n    elif method == 'clip':\n        self.epsilon = EPSILON\n    (self.state_buffer, self.action_buffer) = ([], [])\n    (self.reward_buffer, self.cumulative_reward_buffer) = ([], [])\n    self.action_bound = action_bound",
            "def __init__(self, state_dim, action_dim, action_bound, method='clip'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with tf.name_scope('critic'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        v = tl.layers.Dense(1)(layer)\n    self.critic = tl.models.Model(inputs, v)\n    self.critic.train()\n    with tf.name_scope('actor'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\n        mean = tl.layers.Lambda(lambda x: x * action_bound, name='lambda')(a)\n        logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))\n    self.actor = tl.models.Model(inputs, mean)\n    self.actor.trainable_weights.append(logstd)\n    self.actor.logstd = logstd\n    self.actor.train()\n    self.actor_opt = tf.optimizers.Adam(LR_A)\n    self.critic_opt = tf.optimizers.Adam(LR_C)\n    self.method = method\n    if method == 'penalty':\n        self.kl_target = KL_TARGET\n        self.lam = LAM\n    elif method == 'clip':\n        self.epsilon = EPSILON\n    (self.state_buffer, self.action_buffer) = ([], [])\n    (self.reward_buffer, self.cumulative_reward_buffer) = ([], [])\n    self.action_bound = action_bound",
            "def __init__(self, state_dim, action_dim, action_bound, method='clip'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with tf.name_scope('critic'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        v = tl.layers.Dense(1)(layer)\n    self.critic = tl.models.Model(inputs, v)\n    self.critic.train()\n    with tf.name_scope('actor'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\n        mean = tl.layers.Lambda(lambda x: x * action_bound, name='lambda')(a)\n        logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))\n    self.actor = tl.models.Model(inputs, mean)\n    self.actor.trainable_weights.append(logstd)\n    self.actor.logstd = logstd\n    self.actor.train()\n    self.actor_opt = tf.optimizers.Adam(LR_A)\n    self.critic_opt = tf.optimizers.Adam(LR_C)\n    self.method = method\n    if method == 'penalty':\n        self.kl_target = KL_TARGET\n        self.lam = LAM\n    elif method == 'clip':\n        self.epsilon = EPSILON\n    (self.state_buffer, self.action_buffer) = ([], [])\n    (self.reward_buffer, self.cumulative_reward_buffer) = ([], [])\n    self.action_bound = action_bound",
            "def __init__(self, state_dim, action_dim, action_bound, method='clip'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with tf.name_scope('critic'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        v = tl.layers.Dense(1)(layer)\n    self.critic = tl.models.Model(inputs, v)\n    self.critic.train()\n    with tf.name_scope('actor'):\n        inputs = tl.layers.Input([None, state_dim], tf.float32, 'state')\n        layer = tl.layers.Dense(64, tf.nn.relu)(inputs)\n        layer = tl.layers.Dense(64, tf.nn.relu)(layer)\n        a = tl.layers.Dense(action_dim, tf.nn.tanh)(layer)\n        mean = tl.layers.Lambda(lambda x: x * action_bound, name='lambda')(a)\n        logstd = tf.Variable(np.zeros(action_dim, dtype=np.float32))\n    self.actor = tl.models.Model(inputs, mean)\n    self.actor.trainable_weights.append(logstd)\n    self.actor.logstd = logstd\n    self.actor.train()\n    self.actor_opt = tf.optimizers.Adam(LR_A)\n    self.critic_opt = tf.optimizers.Adam(LR_C)\n    self.method = method\n    if method == 'penalty':\n        self.kl_target = KL_TARGET\n        self.lam = LAM\n    elif method == 'clip':\n        self.epsilon = EPSILON\n    (self.state_buffer, self.action_buffer) = ([], [])\n    (self.reward_buffer, self.cumulative_reward_buffer) = ([], [])\n    self.action_bound = action_bound"
        ]
    },
    {
        "func_name": "train_actor",
        "original": "def train_actor(self, state, action, adv, old_pi):\n    \"\"\"\n        Update policy network\n        :param state: state batch\n        :param action: action batch\n        :param adv: advantage batch\n        :param old_pi: old pi distribution\n        :return: kl_mean or None\n        \"\"\"\n    with tf.GradientTape() as tape:\n        (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n        pi = tfp.distributions.Normal(mean, std)\n        ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))\n        surr = ratio * adv\n        if self.method == 'penalty':\n            kl = tfp.distributions.kl_divergence(old_pi, pi)\n            kl_mean = tf.reduce_mean(kl)\n            loss = -tf.reduce_mean(surr - self.lam * kl)\n        else:\n            loss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * adv))\n    a_gard = tape.gradient(loss, self.actor.trainable_weights)\n    self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\n    if self.method == 'kl_pen':\n        return kl_mean",
        "mutated": [
            "def train_actor(self, state, action, adv, old_pi):\n    if False:\n        i = 10\n    '\\n        Update policy network\\n        :param state: state batch\\n        :param action: action batch\\n        :param adv: advantage batch\\n        :param old_pi: old pi distribution\\n        :return: kl_mean or None\\n        '\n    with tf.GradientTape() as tape:\n        (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n        pi = tfp.distributions.Normal(mean, std)\n        ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))\n        surr = ratio * adv\n        if self.method == 'penalty':\n            kl = tfp.distributions.kl_divergence(old_pi, pi)\n            kl_mean = tf.reduce_mean(kl)\n            loss = -tf.reduce_mean(surr - self.lam * kl)\n        else:\n            loss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * adv))\n    a_gard = tape.gradient(loss, self.actor.trainable_weights)\n    self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\n    if self.method == 'kl_pen':\n        return kl_mean",
            "def train_actor(self, state, action, adv, old_pi):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update policy network\\n        :param state: state batch\\n        :param action: action batch\\n        :param adv: advantage batch\\n        :param old_pi: old pi distribution\\n        :return: kl_mean or None\\n        '\n    with tf.GradientTape() as tape:\n        (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n        pi = tfp.distributions.Normal(mean, std)\n        ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))\n        surr = ratio * adv\n        if self.method == 'penalty':\n            kl = tfp.distributions.kl_divergence(old_pi, pi)\n            kl_mean = tf.reduce_mean(kl)\n            loss = -tf.reduce_mean(surr - self.lam * kl)\n        else:\n            loss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * adv))\n    a_gard = tape.gradient(loss, self.actor.trainable_weights)\n    self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\n    if self.method == 'kl_pen':\n        return kl_mean",
            "def train_actor(self, state, action, adv, old_pi):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update policy network\\n        :param state: state batch\\n        :param action: action batch\\n        :param adv: advantage batch\\n        :param old_pi: old pi distribution\\n        :return: kl_mean or None\\n        '\n    with tf.GradientTape() as tape:\n        (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n        pi = tfp.distributions.Normal(mean, std)\n        ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))\n        surr = ratio * adv\n        if self.method == 'penalty':\n            kl = tfp.distributions.kl_divergence(old_pi, pi)\n            kl_mean = tf.reduce_mean(kl)\n            loss = -tf.reduce_mean(surr - self.lam * kl)\n        else:\n            loss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * adv))\n    a_gard = tape.gradient(loss, self.actor.trainable_weights)\n    self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\n    if self.method == 'kl_pen':\n        return kl_mean",
            "def train_actor(self, state, action, adv, old_pi):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update policy network\\n        :param state: state batch\\n        :param action: action batch\\n        :param adv: advantage batch\\n        :param old_pi: old pi distribution\\n        :return: kl_mean or None\\n        '\n    with tf.GradientTape() as tape:\n        (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n        pi = tfp.distributions.Normal(mean, std)\n        ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))\n        surr = ratio * adv\n        if self.method == 'penalty':\n            kl = tfp.distributions.kl_divergence(old_pi, pi)\n            kl_mean = tf.reduce_mean(kl)\n            loss = -tf.reduce_mean(surr - self.lam * kl)\n        else:\n            loss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * adv))\n    a_gard = tape.gradient(loss, self.actor.trainable_weights)\n    self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\n    if self.method == 'kl_pen':\n        return kl_mean",
            "def train_actor(self, state, action, adv, old_pi):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update policy network\\n        :param state: state batch\\n        :param action: action batch\\n        :param adv: advantage batch\\n        :param old_pi: old pi distribution\\n        :return: kl_mean or None\\n        '\n    with tf.GradientTape() as tape:\n        (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n        pi = tfp.distributions.Normal(mean, std)\n        ratio = tf.exp(pi.log_prob(action) - old_pi.log_prob(action))\n        surr = ratio * adv\n        if self.method == 'penalty':\n            kl = tfp.distributions.kl_divergence(old_pi, pi)\n            kl_mean = tf.reduce_mean(kl)\n            loss = -tf.reduce_mean(surr - self.lam * kl)\n        else:\n            loss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * adv))\n    a_gard = tape.gradient(loss, self.actor.trainable_weights)\n    self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\n    if self.method == 'kl_pen':\n        return kl_mean"
        ]
    },
    {
        "func_name": "train_critic",
        "original": "def train_critic(self, reward, state):\n    \"\"\"\n        Update actor network\n        :param reward: cumulative reward batch\n        :param state: state batch\n        :return: None\n        \"\"\"\n    reward = np.array(reward, dtype=np.float32)\n    with tf.GradientTape() as tape:\n        advantage = reward - self.critic(state)\n        loss = tf.reduce_mean(tf.square(advantage))\n    grad = tape.gradient(loss, self.critic.trainable_weights)\n    self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))",
        "mutated": [
            "def train_critic(self, reward, state):\n    if False:\n        i = 10\n    '\\n        Update actor network\\n        :param reward: cumulative reward batch\\n        :param state: state batch\\n        :return: None\\n        '\n    reward = np.array(reward, dtype=np.float32)\n    with tf.GradientTape() as tape:\n        advantage = reward - self.critic(state)\n        loss = tf.reduce_mean(tf.square(advantage))\n    grad = tape.gradient(loss, self.critic.trainable_weights)\n    self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))",
            "def train_critic(self, reward, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update actor network\\n        :param reward: cumulative reward batch\\n        :param state: state batch\\n        :return: None\\n        '\n    reward = np.array(reward, dtype=np.float32)\n    with tf.GradientTape() as tape:\n        advantage = reward - self.critic(state)\n        loss = tf.reduce_mean(tf.square(advantage))\n    grad = tape.gradient(loss, self.critic.trainable_weights)\n    self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))",
            "def train_critic(self, reward, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update actor network\\n        :param reward: cumulative reward batch\\n        :param state: state batch\\n        :return: None\\n        '\n    reward = np.array(reward, dtype=np.float32)\n    with tf.GradientTape() as tape:\n        advantage = reward - self.critic(state)\n        loss = tf.reduce_mean(tf.square(advantage))\n    grad = tape.gradient(loss, self.critic.trainable_weights)\n    self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))",
            "def train_critic(self, reward, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update actor network\\n        :param reward: cumulative reward batch\\n        :param state: state batch\\n        :return: None\\n        '\n    reward = np.array(reward, dtype=np.float32)\n    with tf.GradientTape() as tape:\n        advantage = reward - self.critic(state)\n        loss = tf.reduce_mean(tf.square(advantage))\n    grad = tape.gradient(loss, self.critic.trainable_weights)\n    self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))",
            "def train_critic(self, reward, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update actor network\\n        :param reward: cumulative reward batch\\n        :param state: state batch\\n        :return: None\\n        '\n    reward = np.array(reward, dtype=np.float32)\n    with tf.GradientTape() as tape:\n        advantage = reward - self.critic(state)\n        loss = tf.reduce_mean(tf.square(advantage))\n    grad = tape.gradient(loss, self.critic.trainable_weights)\n    self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))"
        ]
    },
    {
        "func_name": "update",
        "original": "def update(self):\n    \"\"\"\n        Update parameter with the constraint of KL divergent\n        :return: None\n        \"\"\"\n    s = np.array(self.state_buffer, np.float32)\n    a = np.array(self.action_buffer, np.float32)\n    r = np.array(self.cumulative_reward_buffer, np.float32)\n    (mean, std) = (self.actor(s), tf.exp(self.actor.logstd))\n    pi = tfp.distributions.Normal(mean, std)\n    adv = r - self.critic(s)\n    if self.method == 'kl_pen':\n        for _ in range(ACTOR_UPDATE_STEPS):\n            kl = self.train_actor(s, a, adv, pi)\n        if kl < self.kl_target / 1.5:\n            self.lam /= 2\n        elif kl > self.kl_target * 1.5:\n            self.lam *= 2\n    else:\n        for _ in range(ACTOR_UPDATE_STEPS):\n            self.train_actor(s, a, adv, pi)\n    for _ in range(CRITIC_UPDATE_STEPS):\n        self.train_critic(r, s)\n    self.state_buffer.clear()\n    self.action_buffer.clear()\n    self.cumulative_reward_buffer.clear()\n    self.reward_buffer.clear()",
        "mutated": [
            "def update(self):\n    if False:\n        i = 10\n    '\\n        Update parameter with the constraint of KL divergent\\n        :return: None\\n        '\n    s = np.array(self.state_buffer, np.float32)\n    a = np.array(self.action_buffer, np.float32)\n    r = np.array(self.cumulative_reward_buffer, np.float32)\n    (mean, std) = (self.actor(s), tf.exp(self.actor.logstd))\n    pi = tfp.distributions.Normal(mean, std)\n    adv = r - self.critic(s)\n    if self.method == 'kl_pen':\n        for _ in range(ACTOR_UPDATE_STEPS):\n            kl = self.train_actor(s, a, adv, pi)\n        if kl < self.kl_target / 1.5:\n            self.lam /= 2\n        elif kl > self.kl_target * 1.5:\n            self.lam *= 2\n    else:\n        for _ in range(ACTOR_UPDATE_STEPS):\n            self.train_actor(s, a, adv, pi)\n    for _ in range(CRITIC_UPDATE_STEPS):\n        self.train_critic(r, s)\n    self.state_buffer.clear()\n    self.action_buffer.clear()\n    self.cumulative_reward_buffer.clear()\n    self.reward_buffer.clear()",
            "def update(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update parameter with the constraint of KL divergent\\n        :return: None\\n        '\n    s = np.array(self.state_buffer, np.float32)\n    a = np.array(self.action_buffer, np.float32)\n    r = np.array(self.cumulative_reward_buffer, np.float32)\n    (mean, std) = (self.actor(s), tf.exp(self.actor.logstd))\n    pi = tfp.distributions.Normal(mean, std)\n    adv = r - self.critic(s)\n    if self.method == 'kl_pen':\n        for _ in range(ACTOR_UPDATE_STEPS):\n            kl = self.train_actor(s, a, adv, pi)\n        if kl < self.kl_target / 1.5:\n            self.lam /= 2\n        elif kl > self.kl_target * 1.5:\n            self.lam *= 2\n    else:\n        for _ in range(ACTOR_UPDATE_STEPS):\n            self.train_actor(s, a, adv, pi)\n    for _ in range(CRITIC_UPDATE_STEPS):\n        self.train_critic(r, s)\n    self.state_buffer.clear()\n    self.action_buffer.clear()\n    self.cumulative_reward_buffer.clear()\n    self.reward_buffer.clear()",
            "def update(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update parameter with the constraint of KL divergent\\n        :return: None\\n        '\n    s = np.array(self.state_buffer, np.float32)\n    a = np.array(self.action_buffer, np.float32)\n    r = np.array(self.cumulative_reward_buffer, np.float32)\n    (mean, std) = (self.actor(s), tf.exp(self.actor.logstd))\n    pi = tfp.distributions.Normal(mean, std)\n    adv = r - self.critic(s)\n    if self.method == 'kl_pen':\n        for _ in range(ACTOR_UPDATE_STEPS):\n            kl = self.train_actor(s, a, adv, pi)\n        if kl < self.kl_target / 1.5:\n            self.lam /= 2\n        elif kl > self.kl_target * 1.5:\n            self.lam *= 2\n    else:\n        for _ in range(ACTOR_UPDATE_STEPS):\n            self.train_actor(s, a, adv, pi)\n    for _ in range(CRITIC_UPDATE_STEPS):\n        self.train_critic(r, s)\n    self.state_buffer.clear()\n    self.action_buffer.clear()\n    self.cumulative_reward_buffer.clear()\n    self.reward_buffer.clear()",
            "def update(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update parameter with the constraint of KL divergent\\n        :return: None\\n        '\n    s = np.array(self.state_buffer, np.float32)\n    a = np.array(self.action_buffer, np.float32)\n    r = np.array(self.cumulative_reward_buffer, np.float32)\n    (mean, std) = (self.actor(s), tf.exp(self.actor.logstd))\n    pi = tfp.distributions.Normal(mean, std)\n    adv = r - self.critic(s)\n    if self.method == 'kl_pen':\n        for _ in range(ACTOR_UPDATE_STEPS):\n            kl = self.train_actor(s, a, adv, pi)\n        if kl < self.kl_target / 1.5:\n            self.lam /= 2\n        elif kl > self.kl_target * 1.5:\n            self.lam *= 2\n    else:\n        for _ in range(ACTOR_UPDATE_STEPS):\n            self.train_actor(s, a, adv, pi)\n    for _ in range(CRITIC_UPDATE_STEPS):\n        self.train_critic(r, s)\n    self.state_buffer.clear()\n    self.action_buffer.clear()\n    self.cumulative_reward_buffer.clear()\n    self.reward_buffer.clear()",
            "def update(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update parameter with the constraint of KL divergent\\n        :return: None\\n        '\n    s = np.array(self.state_buffer, np.float32)\n    a = np.array(self.action_buffer, np.float32)\n    r = np.array(self.cumulative_reward_buffer, np.float32)\n    (mean, std) = (self.actor(s), tf.exp(self.actor.logstd))\n    pi = tfp.distributions.Normal(mean, std)\n    adv = r - self.critic(s)\n    if self.method == 'kl_pen':\n        for _ in range(ACTOR_UPDATE_STEPS):\n            kl = self.train_actor(s, a, adv, pi)\n        if kl < self.kl_target / 1.5:\n            self.lam /= 2\n        elif kl > self.kl_target * 1.5:\n            self.lam *= 2\n    else:\n        for _ in range(ACTOR_UPDATE_STEPS):\n            self.train_actor(s, a, adv, pi)\n    for _ in range(CRITIC_UPDATE_STEPS):\n        self.train_critic(r, s)\n    self.state_buffer.clear()\n    self.action_buffer.clear()\n    self.cumulative_reward_buffer.clear()\n    self.reward_buffer.clear()"
        ]
    },
    {
        "func_name": "get_action",
        "original": "def get_action(self, state, greedy=False):\n    \"\"\"\n        Choose action\n        :param state: state\n        :param greedy: choose action greedy or not\n        :return: clipped action\n        \"\"\"\n    state = state[np.newaxis, :].astype(np.float32)\n    (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n    if greedy:\n        action = mean[0]\n    else:\n        pi = tfp.distributions.Normal(mean, std)\n        action = tf.squeeze(pi.sample(1), axis=0)[0]\n    return np.clip(action, -self.action_bound, self.action_bound)",
        "mutated": [
            "def get_action(self, state, greedy=False):\n    if False:\n        i = 10\n    '\\n        Choose action\\n        :param state: state\\n        :param greedy: choose action greedy or not\\n        :return: clipped action\\n        '\n    state = state[np.newaxis, :].astype(np.float32)\n    (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n    if greedy:\n        action = mean[0]\n    else:\n        pi = tfp.distributions.Normal(mean, std)\n        action = tf.squeeze(pi.sample(1), axis=0)[0]\n    return np.clip(action, -self.action_bound, self.action_bound)",
            "def get_action(self, state, greedy=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Choose action\\n        :param state: state\\n        :param greedy: choose action greedy or not\\n        :return: clipped action\\n        '\n    state = state[np.newaxis, :].astype(np.float32)\n    (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n    if greedy:\n        action = mean[0]\n    else:\n        pi = tfp.distributions.Normal(mean, std)\n        action = tf.squeeze(pi.sample(1), axis=0)[0]\n    return np.clip(action, -self.action_bound, self.action_bound)",
            "def get_action(self, state, greedy=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Choose action\\n        :param state: state\\n        :param greedy: choose action greedy or not\\n        :return: clipped action\\n        '\n    state = state[np.newaxis, :].astype(np.float32)\n    (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n    if greedy:\n        action = mean[0]\n    else:\n        pi = tfp.distributions.Normal(mean, std)\n        action = tf.squeeze(pi.sample(1), axis=0)[0]\n    return np.clip(action, -self.action_bound, self.action_bound)",
            "def get_action(self, state, greedy=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Choose action\\n        :param state: state\\n        :param greedy: choose action greedy or not\\n        :return: clipped action\\n        '\n    state = state[np.newaxis, :].astype(np.float32)\n    (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n    if greedy:\n        action = mean[0]\n    else:\n        pi = tfp.distributions.Normal(mean, std)\n        action = tf.squeeze(pi.sample(1), axis=0)[0]\n    return np.clip(action, -self.action_bound, self.action_bound)",
            "def get_action(self, state, greedy=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Choose action\\n        :param state: state\\n        :param greedy: choose action greedy or not\\n        :return: clipped action\\n        '\n    state = state[np.newaxis, :].astype(np.float32)\n    (mean, std) = (self.actor(state), tf.exp(self.actor.logstd))\n    if greedy:\n        action = mean[0]\n    else:\n        pi = tfp.distributions.Normal(mean, std)\n        action = tf.squeeze(pi.sample(1), axis=0)[0]\n    return np.clip(action, -self.action_bound, self.action_bound)"
        ]
    },
    {
        "func_name": "save",
        "original": "def save(self):\n    \"\"\"\n        save trained weights\n        :return: None\n        \"\"\"\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    if not os.path.exists(path):\n        os.makedirs(path)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)",
        "mutated": [
            "def save(self):\n    if False:\n        i = 10\n    '\\n        save trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    if not os.path.exists(path):\n        os.makedirs(path)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)",
            "def save(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        save trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    if not os.path.exists(path):\n        os.makedirs(path)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)",
            "def save(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        save trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    if not os.path.exists(path):\n        os.makedirs(path)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)",
            "def save(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        save trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    if not os.path.exists(path):\n        os.makedirs(path)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)",
            "def save(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        save trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    if not os.path.exists(path):\n        os.makedirs(path)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)"
        ]
    },
    {
        "func_name": "load",
        "original": "def load(self):\n    \"\"\"\n        load trained weights\n        :return: None\n        \"\"\"\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)",
        "mutated": [
            "def load(self):\n    if False:\n        i = 10\n    '\\n        load trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)",
            "def load(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        load trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)",
            "def load(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        load trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)",
            "def load(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        load trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)",
            "def load(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        load trained weights\\n        :return: None\\n        '\n    path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)\n    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)"
        ]
    },
    {
        "func_name": "store_transition",
        "original": "def store_transition(self, state, action, reward):\n    \"\"\"\n        Store state, action, reward at each step\n        :param state:\n        :param action:\n        :param reward:\n        :return: None\n        \"\"\"\n    self.state_buffer.append(state)\n    self.action_buffer.append(action)\n    self.reward_buffer.append(reward)",
        "mutated": [
            "def store_transition(self, state, action, reward):\n    if False:\n        i = 10\n    '\\n        Store state, action, reward at each step\\n        :param state:\\n        :param action:\\n        :param reward:\\n        :return: None\\n        '\n    self.state_buffer.append(state)\n    self.action_buffer.append(action)\n    self.reward_buffer.append(reward)",
            "def store_transition(self, state, action, reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Store state, action, reward at each step\\n        :param state:\\n        :param action:\\n        :param reward:\\n        :return: None\\n        '\n    self.state_buffer.append(state)\n    self.action_buffer.append(action)\n    self.reward_buffer.append(reward)",
            "def store_transition(self, state, action, reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Store state, action, reward at each step\\n        :param state:\\n        :param action:\\n        :param reward:\\n        :return: None\\n        '\n    self.state_buffer.append(state)\n    self.action_buffer.append(action)\n    self.reward_buffer.append(reward)",
            "def store_transition(self, state, action, reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Store state, action, reward at each step\\n        :param state:\\n        :param action:\\n        :param reward:\\n        :return: None\\n        '\n    self.state_buffer.append(state)\n    self.action_buffer.append(action)\n    self.reward_buffer.append(reward)",
            "def store_transition(self, state, action, reward):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Store state, action, reward at each step\\n        :param state:\\n        :param action:\\n        :param reward:\\n        :return: None\\n        '\n    self.state_buffer.append(state)\n    self.action_buffer.append(action)\n    self.reward_buffer.append(reward)"
        ]
    },
    {
        "func_name": "finish_path",
        "original": "def finish_path(self, next_state, done):\n    \"\"\"\n        Calculate cumulative reward\n        :param next_state:\n        :return: None\n        \"\"\"\n    if done:\n        v_s_ = 0\n    else:\n        v_s_ = self.critic(np.array([next_state], np.float32))[0, 0]\n    discounted_r = []\n    for r in self.reward_buffer[::-1]:\n        v_s_ = r + GAMMA * v_s_\n        discounted_r.append(v_s_)\n    discounted_r.reverse()\n    discounted_r = np.array(discounted_r)[:, np.newaxis]\n    self.cumulative_reward_buffer.extend(discounted_r)\n    self.reward_buffer.clear()",
        "mutated": [
            "def finish_path(self, next_state, done):\n    if False:\n        i = 10\n    '\\n        Calculate cumulative reward\\n        :param next_state:\\n        :return: None\\n        '\n    if done:\n        v_s_ = 0\n    else:\n        v_s_ = self.critic(np.array([next_state], np.float32))[0, 0]\n    discounted_r = []\n    for r in self.reward_buffer[::-1]:\n        v_s_ = r + GAMMA * v_s_\n        discounted_r.append(v_s_)\n    discounted_r.reverse()\n    discounted_r = np.array(discounted_r)[:, np.newaxis]\n    self.cumulative_reward_buffer.extend(discounted_r)\n    self.reward_buffer.clear()",
            "def finish_path(self, next_state, done):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Calculate cumulative reward\\n        :param next_state:\\n        :return: None\\n        '\n    if done:\n        v_s_ = 0\n    else:\n        v_s_ = self.critic(np.array([next_state], np.float32))[0, 0]\n    discounted_r = []\n    for r in self.reward_buffer[::-1]:\n        v_s_ = r + GAMMA * v_s_\n        discounted_r.append(v_s_)\n    discounted_r.reverse()\n    discounted_r = np.array(discounted_r)[:, np.newaxis]\n    self.cumulative_reward_buffer.extend(discounted_r)\n    self.reward_buffer.clear()",
            "def finish_path(self, next_state, done):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Calculate cumulative reward\\n        :param next_state:\\n        :return: None\\n        '\n    if done:\n        v_s_ = 0\n    else:\n        v_s_ = self.critic(np.array([next_state], np.float32))[0, 0]\n    discounted_r = []\n    for r in self.reward_buffer[::-1]:\n        v_s_ = r + GAMMA * v_s_\n        discounted_r.append(v_s_)\n    discounted_r.reverse()\n    discounted_r = np.array(discounted_r)[:, np.newaxis]\n    self.cumulative_reward_buffer.extend(discounted_r)\n    self.reward_buffer.clear()",
            "def finish_path(self, next_state, done):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Calculate cumulative reward\\n        :param next_state:\\n        :return: None\\n        '\n    if done:\n        v_s_ = 0\n    else:\n        v_s_ = self.critic(np.array([next_state], np.float32))[0, 0]\n    discounted_r = []\n    for r in self.reward_buffer[::-1]:\n        v_s_ = r + GAMMA * v_s_\n        discounted_r.append(v_s_)\n    discounted_r.reverse()\n    discounted_r = np.array(discounted_r)[:, np.newaxis]\n    self.cumulative_reward_buffer.extend(discounted_r)\n    self.reward_buffer.clear()",
            "def finish_path(self, next_state, done):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Calculate cumulative reward\\n        :param next_state:\\n        :return: None\\n        '\n    if done:\n        v_s_ = 0\n    else:\n        v_s_ = self.critic(np.array([next_state], np.float32))[0, 0]\n    discounted_r = []\n    for r in self.reward_buffer[::-1]:\n        v_s_ = r + GAMMA * v_s_\n        discounted_r.append(v_s_)\n    discounted_r.reverse()\n    discounted_r = np.array(discounted_r)[:, np.newaxis]\n    self.cumulative_reward_buffer.extend(discounted_r)\n    self.reward_buffer.clear()"
        ]
    }
]