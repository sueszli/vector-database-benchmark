[
    {
        "func_name": "_convert_to_sparse_tensor",
        "original": "def _convert_to_sparse_tensor(sp_input):\n    \"\"\"Convert `sp_input` to `SparseTensor` and return it.\n\n  Args:\n    sp_input: `SparseTensor` or `SparseTensorValue`.\n\n  Returns:\n    `sp_input` converted to `SparseTensor`.\n\n  Raises:\n    ValueError: if `sp_input` is neither `SparseTensor` nor `SparseTensorValue`.\n  \"\"\"\n    if isinstance(sp_input, sparse_tensor.SparseTensorValue):\n        return sparse_tensor.SparseTensor.from_value(sp_input)\n    if not isinstance(sp_input, sparse_tensor.SparseTensor):\n        raise TypeError('Input must be a SparseTensor.')\n    return sp_input",
        "mutated": [
            "def _convert_to_sparse_tensor(sp_input):\n    if False:\n        i = 10\n    'Convert `sp_input` to `SparseTensor` and return it.\\n\\n  Args:\\n    sp_input: `SparseTensor` or `SparseTensorValue`.\\n\\n  Returns:\\n    `sp_input` converted to `SparseTensor`.\\n\\n  Raises:\\n    ValueError: if `sp_input` is neither `SparseTensor` nor `SparseTensorValue`.\\n  '\n    if isinstance(sp_input, sparse_tensor.SparseTensorValue):\n        return sparse_tensor.SparseTensor.from_value(sp_input)\n    if not isinstance(sp_input, sparse_tensor.SparseTensor):\n        raise TypeError('Input must be a SparseTensor.')\n    return sp_input",
            "def _convert_to_sparse_tensor(sp_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert `sp_input` to `SparseTensor` and return it.\\n\\n  Args:\\n    sp_input: `SparseTensor` or `SparseTensorValue`.\\n\\n  Returns:\\n    `sp_input` converted to `SparseTensor`.\\n\\n  Raises:\\n    ValueError: if `sp_input` is neither `SparseTensor` nor `SparseTensorValue`.\\n  '\n    if isinstance(sp_input, sparse_tensor.SparseTensorValue):\n        return sparse_tensor.SparseTensor.from_value(sp_input)\n    if not isinstance(sp_input, sparse_tensor.SparseTensor):\n        raise TypeError('Input must be a SparseTensor.')\n    return sp_input",
            "def _convert_to_sparse_tensor(sp_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert `sp_input` to `SparseTensor` and return it.\\n\\n  Args:\\n    sp_input: `SparseTensor` or `SparseTensorValue`.\\n\\n  Returns:\\n    `sp_input` converted to `SparseTensor`.\\n\\n  Raises:\\n    ValueError: if `sp_input` is neither `SparseTensor` nor `SparseTensorValue`.\\n  '\n    if isinstance(sp_input, sparse_tensor.SparseTensorValue):\n        return sparse_tensor.SparseTensor.from_value(sp_input)\n    if not isinstance(sp_input, sparse_tensor.SparseTensor):\n        raise TypeError('Input must be a SparseTensor.')\n    return sp_input",
            "def _convert_to_sparse_tensor(sp_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert `sp_input` to `SparseTensor` and return it.\\n\\n  Args:\\n    sp_input: `SparseTensor` or `SparseTensorValue`.\\n\\n  Returns:\\n    `sp_input` converted to `SparseTensor`.\\n\\n  Raises:\\n    ValueError: if `sp_input` is neither `SparseTensor` nor `SparseTensorValue`.\\n  '\n    if isinstance(sp_input, sparse_tensor.SparseTensorValue):\n        return sparse_tensor.SparseTensor.from_value(sp_input)\n    if not isinstance(sp_input, sparse_tensor.SparseTensor):\n        raise TypeError('Input must be a SparseTensor.')\n    return sp_input",
            "def _convert_to_sparse_tensor(sp_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert `sp_input` to `SparseTensor` and return it.\\n\\n  Args:\\n    sp_input: `SparseTensor` or `SparseTensorValue`.\\n\\n  Returns:\\n    `sp_input` converted to `SparseTensor`.\\n\\n  Raises:\\n    ValueError: if `sp_input` is neither `SparseTensor` nor `SparseTensorValue`.\\n  '\n    if isinstance(sp_input, sparse_tensor.SparseTensorValue):\n        return sparse_tensor.SparseTensor.from_value(sp_input)\n    if not isinstance(sp_input, sparse_tensor.SparseTensor):\n        raise TypeError('Input must be a SparseTensor.')\n    return sp_input"
        ]
    },
    {
        "func_name": "_convert_to_sparse_tensors",
        "original": "def _convert_to_sparse_tensors(sp_inputs):\n    \"\"\"Convert `sp_inputs` to `SparseTensor` objects and return them.\n\n  Args:\n    sp_inputs: `list` or `tuple` of `SparseTensor` or `SparseTensorValue`\n      objects.\n\n  Returns:\n    `sp_inputs` converted to `SparseTensor` objects.\n\n  Raises:\n    ValueError: if any item in `sp_inputs` is neither `SparseTensor` nor\n      `SparseTensorValue`.\n  \"\"\"\n    if isinstance(sp_inputs, list):\n        return [_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs]\n    if isinstance(sp_inputs, tuple):\n        return (_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs)\n    raise TypeError('Inputs must be a list or tuple.')",
        "mutated": [
            "def _convert_to_sparse_tensors(sp_inputs):\n    if False:\n        i = 10\n    'Convert `sp_inputs` to `SparseTensor` objects and return them.\\n\\n  Args:\\n    sp_inputs: `list` or `tuple` of `SparseTensor` or `SparseTensorValue`\\n      objects.\\n\\n  Returns:\\n    `sp_inputs` converted to `SparseTensor` objects.\\n\\n  Raises:\\n    ValueError: if any item in `sp_inputs` is neither `SparseTensor` nor\\n      `SparseTensorValue`.\\n  '\n    if isinstance(sp_inputs, list):\n        return [_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs]\n    if isinstance(sp_inputs, tuple):\n        return (_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs)\n    raise TypeError('Inputs must be a list or tuple.')",
            "def _convert_to_sparse_tensors(sp_inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert `sp_inputs` to `SparseTensor` objects and return them.\\n\\n  Args:\\n    sp_inputs: `list` or `tuple` of `SparseTensor` or `SparseTensorValue`\\n      objects.\\n\\n  Returns:\\n    `sp_inputs` converted to `SparseTensor` objects.\\n\\n  Raises:\\n    ValueError: if any item in `sp_inputs` is neither `SparseTensor` nor\\n      `SparseTensorValue`.\\n  '\n    if isinstance(sp_inputs, list):\n        return [_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs]\n    if isinstance(sp_inputs, tuple):\n        return (_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs)\n    raise TypeError('Inputs must be a list or tuple.')",
            "def _convert_to_sparse_tensors(sp_inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert `sp_inputs` to `SparseTensor` objects and return them.\\n\\n  Args:\\n    sp_inputs: `list` or `tuple` of `SparseTensor` or `SparseTensorValue`\\n      objects.\\n\\n  Returns:\\n    `sp_inputs` converted to `SparseTensor` objects.\\n\\n  Raises:\\n    ValueError: if any item in `sp_inputs` is neither `SparseTensor` nor\\n      `SparseTensorValue`.\\n  '\n    if isinstance(sp_inputs, list):\n        return [_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs]\n    if isinstance(sp_inputs, tuple):\n        return (_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs)\n    raise TypeError('Inputs must be a list or tuple.')",
            "def _convert_to_sparse_tensors(sp_inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert `sp_inputs` to `SparseTensor` objects and return them.\\n\\n  Args:\\n    sp_inputs: `list` or `tuple` of `SparseTensor` or `SparseTensorValue`\\n      objects.\\n\\n  Returns:\\n    `sp_inputs` converted to `SparseTensor` objects.\\n\\n  Raises:\\n    ValueError: if any item in `sp_inputs` is neither `SparseTensor` nor\\n      `SparseTensorValue`.\\n  '\n    if isinstance(sp_inputs, list):\n        return [_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs]\n    if isinstance(sp_inputs, tuple):\n        return (_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs)\n    raise TypeError('Inputs must be a list or tuple.')",
            "def _convert_to_sparse_tensors(sp_inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert `sp_inputs` to `SparseTensor` objects and return them.\\n\\n  Args:\\n    sp_inputs: `list` or `tuple` of `SparseTensor` or `SparseTensorValue`\\n      objects.\\n\\n  Returns:\\n    `sp_inputs` converted to `SparseTensor` objects.\\n\\n  Raises:\\n    ValueError: if any item in `sp_inputs` is neither `SparseTensor` nor\\n      `SparseTensorValue`.\\n  '\n    if isinstance(sp_inputs, list):\n        return [_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs]\n    if isinstance(sp_inputs, tuple):\n        return (_convert_to_sparse_tensor(sp_input) for sp_input in sp_inputs)\n    raise TypeError('Inputs must be a list or tuple.')"
        ]
    },
    {
        "func_name": "_make_int64_tensor",
        "original": "def _make_int64_tensor(value, name):\n    if isinstance(value, compat.integral_types):\n        return ops.convert_to_tensor(value, name=name, dtype=dtypes.int64)\n    if not isinstance(value, tensor_lib.Tensor):\n        raise TypeError('{} must be an integer value'.format(name))\n    if value.dtype == dtypes.int64:\n        return value\n    return math_ops.cast(value, dtypes.int64)",
        "mutated": [
            "def _make_int64_tensor(value, name):\n    if False:\n        i = 10\n    if isinstance(value, compat.integral_types):\n        return ops.convert_to_tensor(value, name=name, dtype=dtypes.int64)\n    if not isinstance(value, tensor_lib.Tensor):\n        raise TypeError('{} must be an integer value'.format(name))\n    if value.dtype == dtypes.int64:\n        return value\n    return math_ops.cast(value, dtypes.int64)",
            "def _make_int64_tensor(value, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(value, compat.integral_types):\n        return ops.convert_to_tensor(value, name=name, dtype=dtypes.int64)\n    if not isinstance(value, tensor_lib.Tensor):\n        raise TypeError('{} must be an integer value'.format(name))\n    if value.dtype == dtypes.int64:\n        return value\n    return math_ops.cast(value, dtypes.int64)",
            "def _make_int64_tensor(value, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(value, compat.integral_types):\n        return ops.convert_to_tensor(value, name=name, dtype=dtypes.int64)\n    if not isinstance(value, tensor_lib.Tensor):\n        raise TypeError('{} must be an integer value'.format(name))\n    if value.dtype == dtypes.int64:\n        return value\n    return math_ops.cast(value, dtypes.int64)",
            "def _make_int64_tensor(value, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(value, compat.integral_types):\n        return ops.convert_to_tensor(value, name=name, dtype=dtypes.int64)\n    if not isinstance(value, tensor_lib.Tensor):\n        raise TypeError('{} must be an integer value'.format(name))\n    if value.dtype == dtypes.int64:\n        return value\n    return math_ops.cast(value, dtypes.int64)",
            "def _make_int64_tensor(value, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(value, compat.integral_types):\n        return ops.convert_to_tensor(value, name=name, dtype=dtypes.int64)\n    if not isinstance(value, tensor_lib.Tensor):\n        raise TypeError('{} must be an integer value'.format(name))\n    if value.dtype == dtypes.int64:\n        return value\n    return math_ops.cast(value, dtypes.int64)"
        ]
    },
    {
        "func_name": "from_dense",
        "original": "@tf_export('sparse.from_dense')\ndef from_dense(tensor, name=None):\n    \"\"\"Converts a dense tensor into a sparse tensor.\n\n  Only elements not equal to zero will be present in the result. The resulting\n  `SparseTensor` has the same dtype and shape as the input.\n\n  >>> sp = tf.sparse.from_dense([0, 0, 3, 0, 1])\n  >>> sp.shape.as_list()\n  [5]\n  >>> sp.values.numpy()\n  array([3, 1], dtype=int32)\n  >>> sp.indices.numpy()\n  array([[2],\n         [4]])\n\n  Args:\n    tensor: A dense `Tensor` to be converted to a `SparseTensor`.\n    name: Optional name for the op.\n\n  Returns:\n    The `SparseTensor`.\n  \"\"\"\n    with ops.name_scope(name, 'dense_to_sparse'):\n        tensor = ops.convert_to_tensor(tensor)\n        indices = array_ops.where_v2(math_ops.not_equal(tensor, array_ops.zeros_like(tensor)))\n        values = array_ops.gather_nd(tensor, indices)\n        shape = array_ops.shape(tensor, out_type=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices, values, shape)",
        "mutated": [
            "@tf_export('sparse.from_dense')\ndef from_dense(tensor, name=None):\n    if False:\n        i = 10\n    'Converts a dense tensor into a sparse tensor.\\n\\n  Only elements not equal to zero will be present in the result. The resulting\\n  `SparseTensor` has the same dtype and shape as the input.\\n\\n  >>> sp = tf.sparse.from_dense([0, 0, 3, 0, 1])\\n  >>> sp.shape.as_list()\\n  [5]\\n  >>> sp.values.numpy()\\n  array([3, 1], dtype=int32)\\n  >>> sp.indices.numpy()\\n  array([[2],\\n         [4]])\\n\\n  Args:\\n    tensor: A dense `Tensor` to be converted to a `SparseTensor`.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    The `SparseTensor`.\\n  '\n    with ops.name_scope(name, 'dense_to_sparse'):\n        tensor = ops.convert_to_tensor(tensor)\n        indices = array_ops.where_v2(math_ops.not_equal(tensor, array_ops.zeros_like(tensor)))\n        values = array_ops.gather_nd(tensor, indices)\n        shape = array_ops.shape(tensor, out_type=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices, values, shape)",
            "@tf_export('sparse.from_dense')\ndef from_dense(tensor, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Converts a dense tensor into a sparse tensor.\\n\\n  Only elements not equal to zero will be present in the result. The resulting\\n  `SparseTensor` has the same dtype and shape as the input.\\n\\n  >>> sp = tf.sparse.from_dense([0, 0, 3, 0, 1])\\n  >>> sp.shape.as_list()\\n  [5]\\n  >>> sp.values.numpy()\\n  array([3, 1], dtype=int32)\\n  >>> sp.indices.numpy()\\n  array([[2],\\n         [4]])\\n\\n  Args:\\n    tensor: A dense `Tensor` to be converted to a `SparseTensor`.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    The `SparseTensor`.\\n  '\n    with ops.name_scope(name, 'dense_to_sparse'):\n        tensor = ops.convert_to_tensor(tensor)\n        indices = array_ops.where_v2(math_ops.not_equal(tensor, array_ops.zeros_like(tensor)))\n        values = array_ops.gather_nd(tensor, indices)\n        shape = array_ops.shape(tensor, out_type=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices, values, shape)",
            "@tf_export('sparse.from_dense')\ndef from_dense(tensor, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Converts a dense tensor into a sparse tensor.\\n\\n  Only elements not equal to zero will be present in the result. The resulting\\n  `SparseTensor` has the same dtype and shape as the input.\\n\\n  >>> sp = tf.sparse.from_dense([0, 0, 3, 0, 1])\\n  >>> sp.shape.as_list()\\n  [5]\\n  >>> sp.values.numpy()\\n  array([3, 1], dtype=int32)\\n  >>> sp.indices.numpy()\\n  array([[2],\\n         [4]])\\n\\n  Args:\\n    tensor: A dense `Tensor` to be converted to a `SparseTensor`.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    The `SparseTensor`.\\n  '\n    with ops.name_scope(name, 'dense_to_sparse'):\n        tensor = ops.convert_to_tensor(tensor)\n        indices = array_ops.where_v2(math_ops.not_equal(tensor, array_ops.zeros_like(tensor)))\n        values = array_ops.gather_nd(tensor, indices)\n        shape = array_ops.shape(tensor, out_type=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices, values, shape)",
            "@tf_export('sparse.from_dense')\ndef from_dense(tensor, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Converts a dense tensor into a sparse tensor.\\n\\n  Only elements not equal to zero will be present in the result. The resulting\\n  `SparseTensor` has the same dtype and shape as the input.\\n\\n  >>> sp = tf.sparse.from_dense([0, 0, 3, 0, 1])\\n  >>> sp.shape.as_list()\\n  [5]\\n  >>> sp.values.numpy()\\n  array([3, 1], dtype=int32)\\n  >>> sp.indices.numpy()\\n  array([[2],\\n         [4]])\\n\\n  Args:\\n    tensor: A dense `Tensor` to be converted to a `SparseTensor`.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    The `SparseTensor`.\\n  '\n    with ops.name_scope(name, 'dense_to_sparse'):\n        tensor = ops.convert_to_tensor(tensor)\n        indices = array_ops.where_v2(math_ops.not_equal(tensor, array_ops.zeros_like(tensor)))\n        values = array_ops.gather_nd(tensor, indices)\n        shape = array_ops.shape(tensor, out_type=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices, values, shape)",
            "@tf_export('sparse.from_dense')\ndef from_dense(tensor, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Converts a dense tensor into a sparse tensor.\\n\\n  Only elements not equal to zero will be present in the result. The resulting\\n  `SparseTensor` has the same dtype and shape as the input.\\n\\n  >>> sp = tf.sparse.from_dense([0, 0, 3, 0, 1])\\n  >>> sp.shape.as_list()\\n  [5]\\n  >>> sp.values.numpy()\\n  array([3, 1], dtype=int32)\\n  >>> sp.indices.numpy()\\n  array([[2],\\n         [4]])\\n\\n  Args:\\n    tensor: A dense `Tensor` to be converted to a `SparseTensor`.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    The `SparseTensor`.\\n  '\n    with ops.name_scope(name, 'dense_to_sparse'):\n        tensor = ops.convert_to_tensor(tensor)\n        indices = array_ops.where_v2(math_ops.not_equal(tensor, array_ops.zeros_like(tensor)))\n        values = array_ops.gather_nd(tensor, indices)\n        shape = array_ops.shape(tensor, out_type=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices, values, shape)"
        ]
    },
    {
        "func_name": "sparse_expand_dims",
        "original": "@tf_export('sparse.expand_dims')\ndef sparse_expand_dims(sp_input, axis=None, name=None):\n    \"\"\"Returns a tensor with an length 1 axis inserted at index `axis`.\n\n  Given a tensor `input`, this operation inserts a dimension of length 1 at the\n  dimension index `axis` of `input`'s shape. The dimension index follows python\n  indexing rules: It's zero-based, a negative index it is counted backward\n  from the end.\n\n  This operation is useful to:\n\n  * Add an outer \"batch\" dimension to a single element.\n  * Align axes for broadcasting.\n  * To add an inner vector length axis to a tensor of scalars.\n\n  For example:\n\n  If you have a sparse tensor with shape `[height, width, depth]`:\n\n  >>> sp = tf.sparse.SparseTensor(indices=[[3,4,1]], values=[7,],\n  ...                             dense_shape=[10,10,3])\n\n  You can add an outer `batch` axis by passing `axis=0`:\n\n  >>> tf.sparse.expand_dims(sp, axis=0).shape.as_list()\n  [1, 10, 10, 3]\n\n  The new axis location matches Python `list.insert(axis, 1)`:\n\n  >>> tf.sparse.expand_dims(sp, axis=1).shape.as_list()\n  [10, 1, 10, 3]\n\n  Following standard python indexing rules, a negative `axis` counts from the\n  end so `axis=-1` adds an inner most dimension:\n\n  >>> tf.sparse.expand_dims(sp, axis=-1).shape.as_list()\n  [10, 10, 3, 1]\n\n  Note: Unlike `tf.expand_dims` this function includes a default value for the\n  `axis`: `-1`. So if `axis is not specified, an inner dimension is added.\n\n  >>> sp.shape.as_list()\n  [10, 10, 3]\n  >>> tf.sparse.expand_dims(sp).shape.as_list()\n  [10, 10, 3, 1]\n\n  This operation requires that `axis` is a valid index for `input.shape`,\n  following python indexing rules:\n\n  ```\n  -1-tf.rank(input) <= axis <= tf.rank(input)\n  ```\n\n  This operation is related to:\n\n  * `tf.expand_dims`, which provides this functionality for dense tensors.\n  * `tf.squeeze`, which removes dimensions of size 1, from dense tensors.\n  * `tf.sparse.reshape`, which provides more flexible reshaping capability.\n\n  Args:\n    sp_input: A `SparseTensor`.\n    axis: 0-D (scalar). Specifies the dimension index at which to expand the\n      shape of `input`. Must be in the range `[-rank(sp_input) - 1,\n      rank(sp_input)]`. Defaults to `-1`.\n    name: The name of the output `SparseTensor`.\n\n  Returns:\n    A `SparseTensor` with the same data as `sp_input`, but its shape has an\n    additional dimension of size 1 added.\n  \"\"\"\n    rank = sp_input.dense_shape.get_shape()[0]\n    if rank is None:\n        rank = array_ops.shape(sp_input.dense_shape)[0]\n    axis = -1 if axis is None else axis\n    with ops.name_scope(name, default_name='expand_dims', values=[sp_input]):\n        if isinstance(axis, compat.integral_types):\n            axis = ops.convert_to_tensor(axis, name='axis', dtype=dtypes.int32)\n        elif not isinstance(axis, tensor_lib.Tensor):\n            raise TypeError('axis must be an integer value in range [-rank(sp_input) - 1, rank(sp_input)]')\n        axis = array_ops.where_v2(axis >= 0, axis, axis + rank + 1)\n        column_size = array_ops.shape(sp_input.indices)[0]\n        new_index = array_ops.zeros([column_size, 1], dtype=dtypes.int64)\n        indices_before = array_ops.slice(sp_input.indices, [0, 0], [-1, axis])\n        indices_after = array_ops.slice(sp_input.indices, [0, axis], [-1, -1])\n        indices = array_ops.concat([indices_before, new_index, indices_after], axis=1)\n        shape_before = array_ops.slice(sp_input.dense_shape, [0], [axis])\n        shape_after = array_ops.slice(sp_input.dense_shape, [axis], [-1])\n        new_shape = ops.convert_to_tensor([1], name='new_shape', dtype=dtypes.int64)\n        shape = array_ops.concat([shape_before, new_shape, shape_after], axis=0)\n        return sparse_tensor.SparseTensor(indices=indices, values=sp_input.values, dense_shape=shape)",
        "mutated": [
            "@tf_export('sparse.expand_dims')\ndef sparse_expand_dims(sp_input, axis=None, name=None):\n    if False:\n        i = 10\n    'Returns a tensor with an length 1 axis inserted at index `axis`.\\n\\n  Given a tensor `input`, this operation inserts a dimension of length 1 at the\\n  dimension index `axis` of `input`\\'s shape. The dimension index follows python\\n  indexing rules: It\\'s zero-based, a negative index it is counted backward\\n  from the end.\\n\\n  This operation is useful to:\\n\\n  * Add an outer \"batch\" dimension to a single element.\\n  * Align axes for broadcasting.\\n  * To add an inner vector length axis to a tensor of scalars.\\n\\n  For example:\\n\\n  If you have a sparse tensor with shape `[height, width, depth]`:\\n\\n  >>> sp = tf.sparse.SparseTensor(indices=[[3,4,1]], values=[7,],\\n  ...                             dense_shape=[10,10,3])\\n\\n  You can add an outer `batch` axis by passing `axis=0`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=0).shape.as_list()\\n  [1, 10, 10, 3]\\n\\n  The new axis location matches Python `list.insert(axis, 1)`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=1).shape.as_list()\\n  [10, 1, 10, 3]\\n\\n  Following standard python indexing rules, a negative `axis` counts from the\\n  end so `axis=-1` adds an inner most dimension:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=-1).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  Note: Unlike `tf.expand_dims` this function includes a default value for the\\n  `axis`: `-1`. So if `axis is not specified, an inner dimension is added.\\n\\n  >>> sp.shape.as_list()\\n  [10, 10, 3]\\n  >>> tf.sparse.expand_dims(sp).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  This operation requires that `axis` is a valid index for `input.shape`,\\n  following python indexing rules:\\n\\n  ```\\n  -1-tf.rank(input) <= axis <= tf.rank(input)\\n  ```\\n\\n  This operation is related to:\\n\\n  * `tf.expand_dims`, which provides this functionality for dense tensors.\\n  * `tf.squeeze`, which removes dimensions of size 1, from dense tensors.\\n  * `tf.sparse.reshape`, which provides more flexible reshaping capability.\\n\\n  Args:\\n    sp_input: A `SparseTensor`.\\n    axis: 0-D (scalar). Specifies the dimension index at which to expand the\\n      shape of `input`. Must be in the range `[-rank(sp_input) - 1,\\n      rank(sp_input)]`. Defaults to `-1`.\\n    name: The name of the output `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` with the same data as `sp_input`, but its shape has an\\n    additional dimension of size 1 added.\\n  '\n    rank = sp_input.dense_shape.get_shape()[0]\n    if rank is None:\n        rank = array_ops.shape(sp_input.dense_shape)[0]\n    axis = -1 if axis is None else axis\n    with ops.name_scope(name, default_name='expand_dims', values=[sp_input]):\n        if isinstance(axis, compat.integral_types):\n            axis = ops.convert_to_tensor(axis, name='axis', dtype=dtypes.int32)\n        elif not isinstance(axis, tensor_lib.Tensor):\n            raise TypeError('axis must be an integer value in range [-rank(sp_input) - 1, rank(sp_input)]')\n        axis = array_ops.where_v2(axis >= 0, axis, axis + rank + 1)\n        column_size = array_ops.shape(sp_input.indices)[0]\n        new_index = array_ops.zeros([column_size, 1], dtype=dtypes.int64)\n        indices_before = array_ops.slice(sp_input.indices, [0, 0], [-1, axis])\n        indices_after = array_ops.slice(sp_input.indices, [0, axis], [-1, -1])\n        indices = array_ops.concat([indices_before, new_index, indices_after], axis=1)\n        shape_before = array_ops.slice(sp_input.dense_shape, [0], [axis])\n        shape_after = array_ops.slice(sp_input.dense_shape, [axis], [-1])\n        new_shape = ops.convert_to_tensor([1], name='new_shape', dtype=dtypes.int64)\n        shape = array_ops.concat([shape_before, new_shape, shape_after], axis=0)\n        return sparse_tensor.SparseTensor(indices=indices, values=sp_input.values, dense_shape=shape)",
            "@tf_export('sparse.expand_dims')\ndef sparse_expand_dims(sp_input, axis=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a tensor with an length 1 axis inserted at index `axis`.\\n\\n  Given a tensor `input`, this operation inserts a dimension of length 1 at the\\n  dimension index `axis` of `input`\\'s shape. The dimension index follows python\\n  indexing rules: It\\'s zero-based, a negative index it is counted backward\\n  from the end.\\n\\n  This operation is useful to:\\n\\n  * Add an outer \"batch\" dimension to a single element.\\n  * Align axes for broadcasting.\\n  * To add an inner vector length axis to a tensor of scalars.\\n\\n  For example:\\n\\n  If you have a sparse tensor with shape `[height, width, depth]`:\\n\\n  >>> sp = tf.sparse.SparseTensor(indices=[[3,4,1]], values=[7,],\\n  ...                             dense_shape=[10,10,3])\\n\\n  You can add an outer `batch` axis by passing `axis=0`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=0).shape.as_list()\\n  [1, 10, 10, 3]\\n\\n  The new axis location matches Python `list.insert(axis, 1)`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=1).shape.as_list()\\n  [10, 1, 10, 3]\\n\\n  Following standard python indexing rules, a negative `axis` counts from the\\n  end so `axis=-1` adds an inner most dimension:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=-1).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  Note: Unlike `tf.expand_dims` this function includes a default value for the\\n  `axis`: `-1`. So if `axis is not specified, an inner dimension is added.\\n\\n  >>> sp.shape.as_list()\\n  [10, 10, 3]\\n  >>> tf.sparse.expand_dims(sp).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  This operation requires that `axis` is a valid index for `input.shape`,\\n  following python indexing rules:\\n\\n  ```\\n  -1-tf.rank(input) <= axis <= tf.rank(input)\\n  ```\\n\\n  This operation is related to:\\n\\n  * `tf.expand_dims`, which provides this functionality for dense tensors.\\n  * `tf.squeeze`, which removes dimensions of size 1, from dense tensors.\\n  * `tf.sparse.reshape`, which provides more flexible reshaping capability.\\n\\n  Args:\\n    sp_input: A `SparseTensor`.\\n    axis: 0-D (scalar). Specifies the dimension index at which to expand the\\n      shape of `input`. Must be in the range `[-rank(sp_input) - 1,\\n      rank(sp_input)]`. Defaults to `-1`.\\n    name: The name of the output `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` with the same data as `sp_input`, but its shape has an\\n    additional dimension of size 1 added.\\n  '\n    rank = sp_input.dense_shape.get_shape()[0]\n    if rank is None:\n        rank = array_ops.shape(sp_input.dense_shape)[0]\n    axis = -1 if axis is None else axis\n    with ops.name_scope(name, default_name='expand_dims', values=[sp_input]):\n        if isinstance(axis, compat.integral_types):\n            axis = ops.convert_to_tensor(axis, name='axis', dtype=dtypes.int32)\n        elif not isinstance(axis, tensor_lib.Tensor):\n            raise TypeError('axis must be an integer value in range [-rank(sp_input) - 1, rank(sp_input)]')\n        axis = array_ops.where_v2(axis >= 0, axis, axis + rank + 1)\n        column_size = array_ops.shape(sp_input.indices)[0]\n        new_index = array_ops.zeros([column_size, 1], dtype=dtypes.int64)\n        indices_before = array_ops.slice(sp_input.indices, [0, 0], [-1, axis])\n        indices_after = array_ops.slice(sp_input.indices, [0, axis], [-1, -1])\n        indices = array_ops.concat([indices_before, new_index, indices_after], axis=1)\n        shape_before = array_ops.slice(sp_input.dense_shape, [0], [axis])\n        shape_after = array_ops.slice(sp_input.dense_shape, [axis], [-1])\n        new_shape = ops.convert_to_tensor([1], name='new_shape', dtype=dtypes.int64)\n        shape = array_ops.concat([shape_before, new_shape, shape_after], axis=0)\n        return sparse_tensor.SparseTensor(indices=indices, values=sp_input.values, dense_shape=shape)",
            "@tf_export('sparse.expand_dims')\ndef sparse_expand_dims(sp_input, axis=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a tensor with an length 1 axis inserted at index `axis`.\\n\\n  Given a tensor `input`, this operation inserts a dimension of length 1 at the\\n  dimension index `axis` of `input`\\'s shape. The dimension index follows python\\n  indexing rules: It\\'s zero-based, a negative index it is counted backward\\n  from the end.\\n\\n  This operation is useful to:\\n\\n  * Add an outer \"batch\" dimension to a single element.\\n  * Align axes for broadcasting.\\n  * To add an inner vector length axis to a tensor of scalars.\\n\\n  For example:\\n\\n  If you have a sparse tensor with shape `[height, width, depth]`:\\n\\n  >>> sp = tf.sparse.SparseTensor(indices=[[3,4,1]], values=[7,],\\n  ...                             dense_shape=[10,10,3])\\n\\n  You can add an outer `batch` axis by passing `axis=0`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=0).shape.as_list()\\n  [1, 10, 10, 3]\\n\\n  The new axis location matches Python `list.insert(axis, 1)`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=1).shape.as_list()\\n  [10, 1, 10, 3]\\n\\n  Following standard python indexing rules, a negative `axis` counts from the\\n  end so `axis=-1` adds an inner most dimension:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=-1).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  Note: Unlike `tf.expand_dims` this function includes a default value for the\\n  `axis`: `-1`. So if `axis is not specified, an inner dimension is added.\\n\\n  >>> sp.shape.as_list()\\n  [10, 10, 3]\\n  >>> tf.sparse.expand_dims(sp).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  This operation requires that `axis` is a valid index for `input.shape`,\\n  following python indexing rules:\\n\\n  ```\\n  -1-tf.rank(input) <= axis <= tf.rank(input)\\n  ```\\n\\n  This operation is related to:\\n\\n  * `tf.expand_dims`, which provides this functionality for dense tensors.\\n  * `tf.squeeze`, which removes dimensions of size 1, from dense tensors.\\n  * `tf.sparse.reshape`, which provides more flexible reshaping capability.\\n\\n  Args:\\n    sp_input: A `SparseTensor`.\\n    axis: 0-D (scalar). Specifies the dimension index at which to expand the\\n      shape of `input`. Must be in the range `[-rank(sp_input) - 1,\\n      rank(sp_input)]`. Defaults to `-1`.\\n    name: The name of the output `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` with the same data as `sp_input`, but its shape has an\\n    additional dimension of size 1 added.\\n  '\n    rank = sp_input.dense_shape.get_shape()[0]\n    if rank is None:\n        rank = array_ops.shape(sp_input.dense_shape)[0]\n    axis = -1 if axis is None else axis\n    with ops.name_scope(name, default_name='expand_dims', values=[sp_input]):\n        if isinstance(axis, compat.integral_types):\n            axis = ops.convert_to_tensor(axis, name='axis', dtype=dtypes.int32)\n        elif not isinstance(axis, tensor_lib.Tensor):\n            raise TypeError('axis must be an integer value in range [-rank(sp_input) - 1, rank(sp_input)]')\n        axis = array_ops.where_v2(axis >= 0, axis, axis + rank + 1)\n        column_size = array_ops.shape(sp_input.indices)[0]\n        new_index = array_ops.zeros([column_size, 1], dtype=dtypes.int64)\n        indices_before = array_ops.slice(sp_input.indices, [0, 0], [-1, axis])\n        indices_after = array_ops.slice(sp_input.indices, [0, axis], [-1, -1])\n        indices = array_ops.concat([indices_before, new_index, indices_after], axis=1)\n        shape_before = array_ops.slice(sp_input.dense_shape, [0], [axis])\n        shape_after = array_ops.slice(sp_input.dense_shape, [axis], [-1])\n        new_shape = ops.convert_to_tensor([1], name='new_shape', dtype=dtypes.int64)\n        shape = array_ops.concat([shape_before, new_shape, shape_after], axis=0)\n        return sparse_tensor.SparseTensor(indices=indices, values=sp_input.values, dense_shape=shape)",
            "@tf_export('sparse.expand_dims')\ndef sparse_expand_dims(sp_input, axis=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a tensor with an length 1 axis inserted at index `axis`.\\n\\n  Given a tensor `input`, this operation inserts a dimension of length 1 at the\\n  dimension index `axis` of `input`\\'s shape. The dimension index follows python\\n  indexing rules: It\\'s zero-based, a negative index it is counted backward\\n  from the end.\\n\\n  This operation is useful to:\\n\\n  * Add an outer \"batch\" dimension to a single element.\\n  * Align axes for broadcasting.\\n  * To add an inner vector length axis to a tensor of scalars.\\n\\n  For example:\\n\\n  If you have a sparse tensor with shape `[height, width, depth]`:\\n\\n  >>> sp = tf.sparse.SparseTensor(indices=[[3,4,1]], values=[7,],\\n  ...                             dense_shape=[10,10,3])\\n\\n  You can add an outer `batch` axis by passing `axis=0`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=0).shape.as_list()\\n  [1, 10, 10, 3]\\n\\n  The new axis location matches Python `list.insert(axis, 1)`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=1).shape.as_list()\\n  [10, 1, 10, 3]\\n\\n  Following standard python indexing rules, a negative `axis` counts from the\\n  end so `axis=-1` adds an inner most dimension:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=-1).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  Note: Unlike `tf.expand_dims` this function includes a default value for the\\n  `axis`: `-1`. So if `axis is not specified, an inner dimension is added.\\n\\n  >>> sp.shape.as_list()\\n  [10, 10, 3]\\n  >>> tf.sparse.expand_dims(sp).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  This operation requires that `axis` is a valid index for `input.shape`,\\n  following python indexing rules:\\n\\n  ```\\n  -1-tf.rank(input) <= axis <= tf.rank(input)\\n  ```\\n\\n  This operation is related to:\\n\\n  * `tf.expand_dims`, which provides this functionality for dense tensors.\\n  * `tf.squeeze`, which removes dimensions of size 1, from dense tensors.\\n  * `tf.sparse.reshape`, which provides more flexible reshaping capability.\\n\\n  Args:\\n    sp_input: A `SparseTensor`.\\n    axis: 0-D (scalar). Specifies the dimension index at which to expand the\\n      shape of `input`. Must be in the range `[-rank(sp_input) - 1,\\n      rank(sp_input)]`. Defaults to `-1`.\\n    name: The name of the output `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` with the same data as `sp_input`, but its shape has an\\n    additional dimension of size 1 added.\\n  '\n    rank = sp_input.dense_shape.get_shape()[0]\n    if rank is None:\n        rank = array_ops.shape(sp_input.dense_shape)[0]\n    axis = -1 if axis is None else axis\n    with ops.name_scope(name, default_name='expand_dims', values=[sp_input]):\n        if isinstance(axis, compat.integral_types):\n            axis = ops.convert_to_tensor(axis, name='axis', dtype=dtypes.int32)\n        elif not isinstance(axis, tensor_lib.Tensor):\n            raise TypeError('axis must be an integer value in range [-rank(sp_input) - 1, rank(sp_input)]')\n        axis = array_ops.where_v2(axis >= 0, axis, axis + rank + 1)\n        column_size = array_ops.shape(sp_input.indices)[0]\n        new_index = array_ops.zeros([column_size, 1], dtype=dtypes.int64)\n        indices_before = array_ops.slice(sp_input.indices, [0, 0], [-1, axis])\n        indices_after = array_ops.slice(sp_input.indices, [0, axis], [-1, -1])\n        indices = array_ops.concat([indices_before, new_index, indices_after], axis=1)\n        shape_before = array_ops.slice(sp_input.dense_shape, [0], [axis])\n        shape_after = array_ops.slice(sp_input.dense_shape, [axis], [-1])\n        new_shape = ops.convert_to_tensor([1], name='new_shape', dtype=dtypes.int64)\n        shape = array_ops.concat([shape_before, new_shape, shape_after], axis=0)\n        return sparse_tensor.SparseTensor(indices=indices, values=sp_input.values, dense_shape=shape)",
            "@tf_export('sparse.expand_dims')\ndef sparse_expand_dims(sp_input, axis=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a tensor with an length 1 axis inserted at index `axis`.\\n\\n  Given a tensor `input`, this operation inserts a dimension of length 1 at the\\n  dimension index `axis` of `input`\\'s shape. The dimension index follows python\\n  indexing rules: It\\'s zero-based, a negative index it is counted backward\\n  from the end.\\n\\n  This operation is useful to:\\n\\n  * Add an outer \"batch\" dimension to a single element.\\n  * Align axes for broadcasting.\\n  * To add an inner vector length axis to a tensor of scalars.\\n\\n  For example:\\n\\n  If you have a sparse tensor with shape `[height, width, depth]`:\\n\\n  >>> sp = tf.sparse.SparseTensor(indices=[[3,4,1]], values=[7,],\\n  ...                             dense_shape=[10,10,3])\\n\\n  You can add an outer `batch` axis by passing `axis=0`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=0).shape.as_list()\\n  [1, 10, 10, 3]\\n\\n  The new axis location matches Python `list.insert(axis, 1)`:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=1).shape.as_list()\\n  [10, 1, 10, 3]\\n\\n  Following standard python indexing rules, a negative `axis` counts from the\\n  end so `axis=-1` adds an inner most dimension:\\n\\n  >>> tf.sparse.expand_dims(sp, axis=-1).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  Note: Unlike `tf.expand_dims` this function includes a default value for the\\n  `axis`: `-1`. So if `axis is not specified, an inner dimension is added.\\n\\n  >>> sp.shape.as_list()\\n  [10, 10, 3]\\n  >>> tf.sparse.expand_dims(sp).shape.as_list()\\n  [10, 10, 3, 1]\\n\\n  This operation requires that `axis` is a valid index for `input.shape`,\\n  following python indexing rules:\\n\\n  ```\\n  -1-tf.rank(input) <= axis <= tf.rank(input)\\n  ```\\n\\n  This operation is related to:\\n\\n  * `tf.expand_dims`, which provides this functionality for dense tensors.\\n  * `tf.squeeze`, which removes dimensions of size 1, from dense tensors.\\n  * `tf.sparse.reshape`, which provides more flexible reshaping capability.\\n\\n  Args:\\n    sp_input: A `SparseTensor`.\\n    axis: 0-D (scalar). Specifies the dimension index at which to expand the\\n      shape of `input`. Must be in the range `[-rank(sp_input) - 1,\\n      rank(sp_input)]`. Defaults to `-1`.\\n    name: The name of the output `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` with the same data as `sp_input`, but its shape has an\\n    additional dimension of size 1 added.\\n  '\n    rank = sp_input.dense_shape.get_shape()[0]\n    if rank is None:\n        rank = array_ops.shape(sp_input.dense_shape)[0]\n    axis = -1 if axis is None else axis\n    with ops.name_scope(name, default_name='expand_dims', values=[sp_input]):\n        if isinstance(axis, compat.integral_types):\n            axis = ops.convert_to_tensor(axis, name='axis', dtype=dtypes.int32)\n        elif not isinstance(axis, tensor_lib.Tensor):\n            raise TypeError('axis must be an integer value in range [-rank(sp_input) - 1, rank(sp_input)]')\n        axis = array_ops.where_v2(axis >= 0, axis, axis + rank + 1)\n        column_size = array_ops.shape(sp_input.indices)[0]\n        new_index = array_ops.zeros([column_size, 1], dtype=dtypes.int64)\n        indices_before = array_ops.slice(sp_input.indices, [0, 0], [-1, axis])\n        indices_after = array_ops.slice(sp_input.indices, [0, axis], [-1, -1])\n        indices = array_ops.concat([indices_before, new_index, indices_after], axis=1)\n        shape_before = array_ops.slice(sp_input.dense_shape, [0], [axis])\n        shape_after = array_ops.slice(sp_input.dense_shape, [axis], [-1])\n        new_shape = ops.convert_to_tensor([1], name='new_shape', dtype=dtypes.int64)\n        shape = array_ops.concat([shape_before, new_shape, shape_after], axis=0)\n        return sparse_tensor.SparseTensor(indices=indices, values=sp_input.values, dense_shape=shape)"
        ]
    },
    {
        "func_name": "sparse_eye",
        "original": "@tf_export('sparse.eye')\ndef sparse_eye(num_rows, num_columns=None, dtype=dtypes.float32, name=None):\n    \"\"\"Creates a two-dimensional sparse tensor with ones along the diagonal.\n\n  Args:\n    num_rows: Non-negative integer or `int32` scalar `tensor` giving the number\n      of rows in the resulting matrix.\n    num_columns: Optional non-negative integer or `int32` scalar `tensor` giving\n      the number of columns in the resulting matrix. Defaults to `num_rows`.\n    dtype: The type of element in the resulting `Tensor`.\n    name: A name for this `Op`. Defaults to \"eye\".\n\n  Returns:\n    A `SparseTensor` of shape [num_rows, num_columns] with ones along the\n    diagonal.\n  \"\"\"\n    with ops.name_scope(name, default_name='eye', values=[num_rows, num_columns]):\n        num_rows = _make_int64_tensor(num_rows, 'num_rows')\n        num_columns = num_rows if num_columns is None else _make_int64_tensor(num_columns, 'num_columns')\n        diag_size = math_ops.minimum(num_rows, num_columns)\n        diag_range = math_ops.range(diag_size, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices=array_ops_stack.stack([diag_range, diag_range], axis=1), values=array_ops.ones(diag_size, dtype=dtype), dense_shape=[num_rows, num_columns])",
        "mutated": [
            "@tf_export('sparse.eye')\ndef sparse_eye(num_rows, num_columns=None, dtype=dtypes.float32, name=None):\n    if False:\n        i = 10\n    'Creates a two-dimensional sparse tensor with ones along the diagonal.\\n\\n  Args:\\n    num_rows: Non-negative integer or `int32` scalar `tensor` giving the number\\n      of rows in the resulting matrix.\\n    num_columns: Optional non-negative integer or `int32` scalar `tensor` giving\\n      the number of columns in the resulting matrix. Defaults to `num_rows`.\\n    dtype: The type of element in the resulting `Tensor`.\\n    name: A name for this `Op`. Defaults to \"eye\".\\n\\n  Returns:\\n    A `SparseTensor` of shape [num_rows, num_columns] with ones along the\\n    diagonal.\\n  '\n    with ops.name_scope(name, default_name='eye', values=[num_rows, num_columns]):\n        num_rows = _make_int64_tensor(num_rows, 'num_rows')\n        num_columns = num_rows if num_columns is None else _make_int64_tensor(num_columns, 'num_columns')\n        diag_size = math_ops.minimum(num_rows, num_columns)\n        diag_range = math_ops.range(diag_size, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices=array_ops_stack.stack([diag_range, diag_range], axis=1), values=array_ops.ones(diag_size, dtype=dtype), dense_shape=[num_rows, num_columns])",
            "@tf_export('sparse.eye')\ndef sparse_eye(num_rows, num_columns=None, dtype=dtypes.float32, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a two-dimensional sparse tensor with ones along the diagonal.\\n\\n  Args:\\n    num_rows: Non-negative integer or `int32` scalar `tensor` giving the number\\n      of rows in the resulting matrix.\\n    num_columns: Optional non-negative integer or `int32` scalar `tensor` giving\\n      the number of columns in the resulting matrix. Defaults to `num_rows`.\\n    dtype: The type of element in the resulting `Tensor`.\\n    name: A name for this `Op`. Defaults to \"eye\".\\n\\n  Returns:\\n    A `SparseTensor` of shape [num_rows, num_columns] with ones along the\\n    diagonal.\\n  '\n    with ops.name_scope(name, default_name='eye', values=[num_rows, num_columns]):\n        num_rows = _make_int64_tensor(num_rows, 'num_rows')\n        num_columns = num_rows if num_columns is None else _make_int64_tensor(num_columns, 'num_columns')\n        diag_size = math_ops.minimum(num_rows, num_columns)\n        diag_range = math_ops.range(diag_size, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices=array_ops_stack.stack([diag_range, diag_range], axis=1), values=array_ops.ones(diag_size, dtype=dtype), dense_shape=[num_rows, num_columns])",
            "@tf_export('sparse.eye')\ndef sparse_eye(num_rows, num_columns=None, dtype=dtypes.float32, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a two-dimensional sparse tensor with ones along the diagonal.\\n\\n  Args:\\n    num_rows: Non-negative integer or `int32` scalar `tensor` giving the number\\n      of rows in the resulting matrix.\\n    num_columns: Optional non-negative integer or `int32` scalar `tensor` giving\\n      the number of columns in the resulting matrix. Defaults to `num_rows`.\\n    dtype: The type of element in the resulting `Tensor`.\\n    name: A name for this `Op`. Defaults to \"eye\".\\n\\n  Returns:\\n    A `SparseTensor` of shape [num_rows, num_columns] with ones along the\\n    diagonal.\\n  '\n    with ops.name_scope(name, default_name='eye', values=[num_rows, num_columns]):\n        num_rows = _make_int64_tensor(num_rows, 'num_rows')\n        num_columns = num_rows if num_columns is None else _make_int64_tensor(num_columns, 'num_columns')\n        diag_size = math_ops.minimum(num_rows, num_columns)\n        diag_range = math_ops.range(diag_size, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices=array_ops_stack.stack([diag_range, diag_range], axis=1), values=array_ops.ones(diag_size, dtype=dtype), dense_shape=[num_rows, num_columns])",
            "@tf_export('sparse.eye')\ndef sparse_eye(num_rows, num_columns=None, dtype=dtypes.float32, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a two-dimensional sparse tensor with ones along the diagonal.\\n\\n  Args:\\n    num_rows: Non-negative integer or `int32` scalar `tensor` giving the number\\n      of rows in the resulting matrix.\\n    num_columns: Optional non-negative integer or `int32` scalar `tensor` giving\\n      the number of columns in the resulting matrix. Defaults to `num_rows`.\\n    dtype: The type of element in the resulting `Tensor`.\\n    name: A name for this `Op`. Defaults to \"eye\".\\n\\n  Returns:\\n    A `SparseTensor` of shape [num_rows, num_columns] with ones along the\\n    diagonal.\\n  '\n    with ops.name_scope(name, default_name='eye', values=[num_rows, num_columns]):\n        num_rows = _make_int64_tensor(num_rows, 'num_rows')\n        num_columns = num_rows if num_columns is None else _make_int64_tensor(num_columns, 'num_columns')\n        diag_size = math_ops.minimum(num_rows, num_columns)\n        diag_range = math_ops.range(diag_size, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices=array_ops_stack.stack([diag_range, diag_range], axis=1), values=array_ops.ones(diag_size, dtype=dtype), dense_shape=[num_rows, num_columns])",
            "@tf_export('sparse.eye')\ndef sparse_eye(num_rows, num_columns=None, dtype=dtypes.float32, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a two-dimensional sparse tensor with ones along the diagonal.\\n\\n  Args:\\n    num_rows: Non-negative integer or `int32` scalar `tensor` giving the number\\n      of rows in the resulting matrix.\\n    num_columns: Optional non-negative integer or `int32` scalar `tensor` giving\\n      the number of columns in the resulting matrix. Defaults to `num_rows`.\\n    dtype: The type of element in the resulting `Tensor`.\\n    name: A name for this `Op`. Defaults to \"eye\".\\n\\n  Returns:\\n    A `SparseTensor` of shape [num_rows, num_columns] with ones along the\\n    diagonal.\\n  '\n    with ops.name_scope(name, default_name='eye', values=[num_rows, num_columns]):\n        num_rows = _make_int64_tensor(num_rows, 'num_rows')\n        num_columns = num_rows if num_columns is None else _make_int64_tensor(num_columns, 'num_columns')\n        diag_size = math_ops.minimum(num_rows, num_columns)\n        diag_range = math_ops.range(diag_size, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(indices=array_ops_stack.stack([diag_range, diag_range], axis=1), values=array_ops.ones(diag_size, dtype=dtype), dense_shape=[num_rows, num_columns])"
        ]
    },
    {
        "func_name": "sparse_concat",
        "original": "@tf_export(v1=['sparse.concat', 'sparse_concat'])\n@deprecation.deprecated_endpoints('sparse_concat')\n@deprecation.deprecated_args(None, 'concat_dim is deprecated, use axis instead', 'concat_dim')\ndef sparse_concat(axis, sp_inputs, name=None, expand_nonconcat_dim=False, concat_dim=None, expand_nonconcat_dims=None):\n    \"\"\"Concatenates a list of `SparseTensor` along the specified dimension.\n\n  Concatenation is with respect to the dense versions of each sparse input.\n  It is assumed that each inputs is a `SparseTensor` whose elements are ordered\n  along increasing dimension number.\n\n  If expand_nonconcat_dim is False, all inputs' shapes must match, except for\n  the concat dimension. If expand_nonconcat_dim is True, then inputs' shapes are\n  allowed to vary among all inputs.\n\n  The `indices`, `values`, and `shapes` lists must have the same length.\n\n  If expand_nonconcat_dim is False, then the output shape is identical to the\n  inputs', except along the concat dimension, where it is the sum of the inputs'\n  sizes along that dimension.\n\n  If expand_nonconcat_dim is True, then the output shape along the non-concat\n  dimensions will be expand to be the largest among all inputs, and it is the\n  sum of the inputs sizes along the concat dimension.\n\n  The output elements will be resorted to preserve the sort order along\n  increasing dimension number.\n\n  This op runs in `O(M log M)` time, where `M` is the total number of non-empty\n  values across all inputs. This is due to the need for an internal sort in\n  order to concatenate efficiently across an arbitrary dimension.\n\n  For example, if `axis = 1` and the inputs are\n\n      sp_inputs[0]: shape = [2, 3]\n      [0, 2]: \"a\"\n      [1, 0]: \"b\"\n      [1, 1]: \"c\"\n\n      sp_inputs[1]: shape = [2, 4]\n      [0, 1]: \"d\"\n      [0, 2]: \"e\"\n\n  then the output will be\n\n      shape = [2, 7]\n      [0, 2]: \"a\"\n      [0, 4]: \"d\"\n      [0, 5]: \"e\"\n      [1, 0]: \"b\"\n      [1, 1]: \"c\"\n\n  Graphically this is equivalent to doing\n\n      [    a] concat [  d e  ] = [    a   d e  ]\n      [b c  ]        [       ]   [b c          ]\n\n  Another example, if 'axis = 1' and the inputs are\n\n      sp_inputs[0]: shape = [3, 3]\n      [0, 2]: \"a\"\n      [1, 0]: \"b\"\n      [2, 1]: \"c\"\n\n      sp_inputs[1]: shape = [2, 4]\n      [0, 1]: \"d\"\n      [0, 2]: \"e\"\n\n  if expand_nonconcat_dim = False, this will result in an error. But if\n  expand_nonconcat_dim = True, this will result in:\n\n      shape = [3, 7]\n      [0, 2]: \"a\"\n      [0, 4]: \"d\"\n      [0, 5]: \"e\"\n      [1, 0]: \"b\"\n      [2, 1]: \"c\"\n\n  Graphically this is equivalent to doing\n\n      [    a] concat [  d e  ] = [    a   d e  ]\n      [b    ]        [       ]   [b            ]\n      [  c  ]                    [  c          ]\n\n\n  Args:\n    axis: Dimension to concatenate along. Must be in range [-rank, rank),\n      where rank is the number of dimensions in each input `SparseTensor`.\n    sp_inputs: List of `SparseTensor` to concatenate.\n    name: A name prefix for the returned tensors (optional).\n    expand_nonconcat_dim: Whether to allow the expansion in the non-concat\n      dimensions. Defaulted to False.\n    concat_dim: The old (deprecated) name for axis.\n    expand_nonconcat_dims: alias for expand_nonconcat_dim\n\n  Returns:\n    A `SparseTensor` with the concatenated output.\n\n  Raises:\n    TypeError: If `sp_inputs` is not a list of `SparseTensor`.\n  \"\"\"\n    expand_nonconcat_dim = deprecation.deprecated_argument_lookup('expand_nonconcat_dims', expand_nonconcat_dims, 'expand_nonconcat_dim', expand_nonconcat_dim)\n    if expand_nonconcat_dims is not None:\n        expand_nonconcat_dim = expand_nonconcat_dims\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'concat_dim', concat_dim)\n    return sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dim, name)",
        "mutated": [
            "@tf_export(v1=['sparse.concat', 'sparse_concat'])\n@deprecation.deprecated_endpoints('sparse_concat')\n@deprecation.deprecated_args(None, 'concat_dim is deprecated, use axis instead', 'concat_dim')\ndef sparse_concat(axis, sp_inputs, name=None, expand_nonconcat_dim=False, concat_dim=None, expand_nonconcat_dims=None):\n    if False:\n        i = 10\n    'Concatenates a list of `SparseTensor` along the specified dimension.\\n\\n  Concatenation is with respect to the dense versions of each sparse input.\\n  It is assumed that each inputs is a `SparseTensor` whose elements are ordered\\n  along increasing dimension number.\\n\\n  If expand_nonconcat_dim is False, all inputs\\' shapes must match, except for\\n  the concat dimension. If expand_nonconcat_dim is True, then inputs\\' shapes are\\n  allowed to vary among all inputs.\\n\\n  The `indices`, `values`, and `shapes` lists must have the same length.\\n\\n  If expand_nonconcat_dim is False, then the output shape is identical to the\\n  inputs\\', except along the concat dimension, where it is the sum of the inputs\\'\\n  sizes along that dimension.\\n\\n  If expand_nonconcat_dim is True, then the output shape along the non-concat\\n  dimensions will be expand to be the largest among all inputs, and it is the\\n  sum of the inputs sizes along the concat dimension.\\n\\n  The output elements will be resorted to preserve the sort order along\\n  increasing dimension number.\\n\\n  This op runs in `O(M log M)` time, where `M` is the total number of non-empty\\n  values across all inputs. This is due to the need for an internal sort in\\n  order to concatenate efficiently across an arbitrary dimension.\\n\\n  For example, if `axis = 1` and the inputs are\\n\\n      sp_inputs[0]: shape = [2, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  then the output will be\\n\\n      shape = [2, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b c  ]        [       ]   [b c          ]\\n\\n  Another example, if \\'axis = 1\\' and the inputs are\\n\\n      sp_inputs[0]: shape = [3, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  if expand_nonconcat_dim = False, this will result in an error. But if\\n  expand_nonconcat_dim = True, this will result in:\\n\\n      shape = [3, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b    ]        [       ]   [b            ]\\n      [  c  ]                    [  c          ]\\n\\n\\n  Args:\\n    axis: Dimension to concatenate along. Must be in range [-rank, rank),\\n      where rank is the number of dimensions in each input `SparseTensor`.\\n    sp_inputs: List of `SparseTensor` to concatenate.\\n    name: A name prefix for the returned tensors (optional).\\n    expand_nonconcat_dim: Whether to allow the expansion in the non-concat\\n      dimensions. Defaulted to False.\\n    concat_dim: The old (deprecated) name for axis.\\n    expand_nonconcat_dims: alias for expand_nonconcat_dim\\n\\n  Returns:\\n    A `SparseTensor` with the concatenated output.\\n\\n  Raises:\\n    TypeError: If `sp_inputs` is not a list of `SparseTensor`.\\n  '\n    expand_nonconcat_dim = deprecation.deprecated_argument_lookup('expand_nonconcat_dims', expand_nonconcat_dims, 'expand_nonconcat_dim', expand_nonconcat_dim)\n    if expand_nonconcat_dims is not None:\n        expand_nonconcat_dim = expand_nonconcat_dims\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'concat_dim', concat_dim)\n    return sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dim, name)",
            "@tf_export(v1=['sparse.concat', 'sparse_concat'])\n@deprecation.deprecated_endpoints('sparse_concat')\n@deprecation.deprecated_args(None, 'concat_dim is deprecated, use axis instead', 'concat_dim')\ndef sparse_concat(axis, sp_inputs, name=None, expand_nonconcat_dim=False, concat_dim=None, expand_nonconcat_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Concatenates a list of `SparseTensor` along the specified dimension.\\n\\n  Concatenation is with respect to the dense versions of each sparse input.\\n  It is assumed that each inputs is a `SparseTensor` whose elements are ordered\\n  along increasing dimension number.\\n\\n  If expand_nonconcat_dim is False, all inputs\\' shapes must match, except for\\n  the concat dimension. If expand_nonconcat_dim is True, then inputs\\' shapes are\\n  allowed to vary among all inputs.\\n\\n  The `indices`, `values`, and `shapes` lists must have the same length.\\n\\n  If expand_nonconcat_dim is False, then the output shape is identical to the\\n  inputs\\', except along the concat dimension, where it is the sum of the inputs\\'\\n  sizes along that dimension.\\n\\n  If expand_nonconcat_dim is True, then the output shape along the non-concat\\n  dimensions will be expand to be the largest among all inputs, and it is the\\n  sum of the inputs sizes along the concat dimension.\\n\\n  The output elements will be resorted to preserve the sort order along\\n  increasing dimension number.\\n\\n  This op runs in `O(M log M)` time, where `M` is the total number of non-empty\\n  values across all inputs. This is due to the need for an internal sort in\\n  order to concatenate efficiently across an arbitrary dimension.\\n\\n  For example, if `axis = 1` and the inputs are\\n\\n      sp_inputs[0]: shape = [2, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  then the output will be\\n\\n      shape = [2, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b c  ]        [       ]   [b c          ]\\n\\n  Another example, if \\'axis = 1\\' and the inputs are\\n\\n      sp_inputs[0]: shape = [3, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  if expand_nonconcat_dim = False, this will result in an error. But if\\n  expand_nonconcat_dim = True, this will result in:\\n\\n      shape = [3, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b    ]        [       ]   [b            ]\\n      [  c  ]                    [  c          ]\\n\\n\\n  Args:\\n    axis: Dimension to concatenate along. Must be in range [-rank, rank),\\n      where rank is the number of dimensions in each input `SparseTensor`.\\n    sp_inputs: List of `SparseTensor` to concatenate.\\n    name: A name prefix for the returned tensors (optional).\\n    expand_nonconcat_dim: Whether to allow the expansion in the non-concat\\n      dimensions. Defaulted to False.\\n    concat_dim: The old (deprecated) name for axis.\\n    expand_nonconcat_dims: alias for expand_nonconcat_dim\\n\\n  Returns:\\n    A `SparseTensor` with the concatenated output.\\n\\n  Raises:\\n    TypeError: If `sp_inputs` is not a list of `SparseTensor`.\\n  '\n    expand_nonconcat_dim = deprecation.deprecated_argument_lookup('expand_nonconcat_dims', expand_nonconcat_dims, 'expand_nonconcat_dim', expand_nonconcat_dim)\n    if expand_nonconcat_dims is not None:\n        expand_nonconcat_dim = expand_nonconcat_dims\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'concat_dim', concat_dim)\n    return sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dim, name)",
            "@tf_export(v1=['sparse.concat', 'sparse_concat'])\n@deprecation.deprecated_endpoints('sparse_concat')\n@deprecation.deprecated_args(None, 'concat_dim is deprecated, use axis instead', 'concat_dim')\ndef sparse_concat(axis, sp_inputs, name=None, expand_nonconcat_dim=False, concat_dim=None, expand_nonconcat_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Concatenates a list of `SparseTensor` along the specified dimension.\\n\\n  Concatenation is with respect to the dense versions of each sparse input.\\n  It is assumed that each inputs is a `SparseTensor` whose elements are ordered\\n  along increasing dimension number.\\n\\n  If expand_nonconcat_dim is False, all inputs\\' shapes must match, except for\\n  the concat dimension. If expand_nonconcat_dim is True, then inputs\\' shapes are\\n  allowed to vary among all inputs.\\n\\n  The `indices`, `values`, and `shapes` lists must have the same length.\\n\\n  If expand_nonconcat_dim is False, then the output shape is identical to the\\n  inputs\\', except along the concat dimension, where it is the sum of the inputs\\'\\n  sizes along that dimension.\\n\\n  If expand_nonconcat_dim is True, then the output shape along the non-concat\\n  dimensions will be expand to be the largest among all inputs, and it is the\\n  sum of the inputs sizes along the concat dimension.\\n\\n  The output elements will be resorted to preserve the sort order along\\n  increasing dimension number.\\n\\n  This op runs in `O(M log M)` time, where `M` is the total number of non-empty\\n  values across all inputs. This is due to the need for an internal sort in\\n  order to concatenate efficiently across an arbitrary dimension.\\n\\n  For example, if `axis = 1` and the inputs are\\n\\n      sp_inputs[0]: shape = [2, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  then the output will be\\n\\n      shape = [2, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b c  ]        [       ]   [b c          ]\\n\\n  Another example, if \\'axis = 1\\' and the inputs are\\n\\n      sp_inputs[0]: shape = [3, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  if expand_nonconcat_dim = False, this will result in an error. But if\\n  expand_nonconcat_dim = True, this will result in:\\n\\n      shape = [3, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b    ]        [       ]   [b            ]\\n      [  c  ]                    [  c          ]\\n\\n\\n  Args:\\n    axis: Dimension to concatenate along. Must be in range [-rank, rank),\\n      where rank is the number of dimensions in each input `SparseTensor`.\\n    sp_inputs: List of `SparseTensor` to concatenate.\\n    name: A name prefix for the returned tensors (optional).\\n    expand_nonconcat_dim: Whether to allow the expansion in the non-concat\\n      dimensions. Defaulted to False.\\n    concat_dim: The old (deprecated) name for axis.\\n    expand_nonconcat_dims: alias for expand_nonconcat_dim\\n\\n  Returns:\\n    A `SparseTensor` with the concatenated output.\\n\\n  Raises:\\n    TypeError: If `sp_inputs` is not a list of `SparseTensor`.\\n  '\n    expand_nonconcat_dim = deprecation.deprecated_argument_lookup('expand_nonconcat_dims', expand_nonconcat_dims, 'expand_nonconcat_dim', expand_nonconcat_dim)\n    if expand_nonconcat_dims is not None:\n        expand_nonconcat_dim = expand_nonconcat_dims\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'concat_dim', concat_dim)\n    return sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dim, name)",
            "@tf_export(v1=['sparse.concat', 'sparse_concat'])\n@deprecation.deprecated_endpoints('sparse_concat')\n@deprecation.deprecated_args(None, 'concat_dim is deprecated, use axis instead', 'concat_dim')\ndef sparse_concat(axis, sp_inputs, name=None, expand_nonconcat_dim=False, concat_dim=None, expand_nonconcat_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Concatenates a list of `SparseTensor` along the specified dimension.\\n\\n  Concatenation is with respect to the dense versions of each sparse input.\\n  It is assumed that each inputs is a `SparseTensor` whose elements are ordered\\n  along increasing dimension number.\\n\\n  If expand_nonconcat_dim is False, all inputs\\' shapes must match, except for\\n  the concat dimension. If expand_nonconcat_dim is True, then inputs\\' shapes are\\n  allowed to vary among all inputs.\\n\\n  The `indices`, `values`, and `shapes` lists must have the same length.\\n\\n  If expand_nonconcat_dim is False, then the output shape is identical to the\\n  inputs\\', except along the concat dimension, where it is the sum of the inputs\\'\\n  sizes along that dimension.\\n\\n  If expand_nonconcat_dim is True, then the output shape along the non-concat\\n  dimensions will be expand to be the largest among all inputs, and it is the\\n  sum of the inputs sizes along the concat dimension.\\n\\n  The output elements will be resorted to preserve the sort order along\\n  increasing dimension number.\\n\\n  This op runs in `O(M log M)` time, where `M` is the total number of non-empty\\n  values across all inputs. This is due to the need for an internal sort in\\n  order to concatenate efficiently across an arbitrary dimension.\\n\\n  For example, if `axis = 1` and the inputs are\\n\\n      sp_inputs[0]: shape = [2, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  then the output will be\\n\\n      shape = [2, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b c  ]        [       ]   [b c          ]\\n\\n  Another example, if \\'axis = 1\\' and the inputs are\\n\\n      sp_inputs[0]: shape = [3, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  if expand_nonconcat_dim = False, this will result in an error. But if\\n  expand_nonconcat_dim = True, this will result in:\\n\\n      shape = [3, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b    ]        [       ]   [b            ]\\n      [  c  ]                    [  c          ]\\n\\n\\n  Args:\\n    axis: Dimension to concatenate along. Must be in range [-rank, rank),\\n      where rank is the number of dimensions in each input `SparseTensor`.\\n    sp_inputs: List of `SparseTensor` to concatenate.\\n    name: A name prefix for the returned tensors (optional).\\n    expand_nonconcat_dim: Whether to allow the expansion in the non-concat\\n      dimensions. Defaulted to False.\\n    concat_dim: The old (deprecated) name for axis.\\n    expand_nonconcat_dims: alias for expand_nonconcat_dim\\n\\n  Returns:\\n    A `SparseTensor` with the concatenated output.\\n\\n  Raises:\\n    TypeError: If `sp_inputs` is not a list of `SparseTensor`.\\n  '\n    expand_nonconcat_dim = deprecation.deprecated_argument_lookup('expand_nonconcat_dims', expand_nonconcat_dims, 'expand_nonconcat_dim', expand_nonconcat_dim)\n    if expand_nonconcat_dims is not None:\n        expand_nonconcat_dim = expand_nonconcat_dims\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'concat_dim', concat_dim)\n    return sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dim, name)",
            "@tf_export(v1=['sparse.concat', 'sparse_concat'])\n@deprecation.deprecated_endpoints('sparse_concat')\n@deprecation.deprecated_args(None, 'concat_dim is deprecated, use axis instead', 'concat_dim')\ndef sparse_concat(axis, sp_inputs, name=None, expand_nonconcat_dim=False, concat_dim=None, expand_nonconcat_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Concatenates a list of `SparseTensor` along the specified dimension.\\n\\n  Concatenation is with respect to the dense versions of each sparse input.\\n  It is assumed that each inputs is a `SparseTensor` whose elements are ordered\\n  along increasing dimension number.\\n\\n  If expand_nonconcat_dim is False, all inputs\\' shapes must match, except for\\n  the concat dimension. If expand_nonconcat_dim is True, then inputs\\' shapes are\\n  allowed to vary among all inputs.\\n\\n  The `indices`, `values`, and `shapes` lists must have the same length.\\n\\n  If expand_nonconcat_dim is False, then the output shape is identical to the\\n  inputs\\', except along the concat dimension, where it is the sum of the inputs\\'\\n  sizes along that dimension.\\n\\n  If expand_nonconcat_dim is True, then the output shape along the non-concat\\n  dimensions will be expand to be the largest among all inputs, and it is the\\n  sum of the inputs sizes along the concat dimension.\\n\\n  The output elements will be resorted to preserve the sort order along\\n  increasing dimension number.\\n\\n  This op runs in `O(M log M)` time, where `M` is the total number of non-empty\\n  values across all inputs. This is due to the need for an internal sort in\\n  order to concatenate efficiently across an arbitrary dimension.\\n\\n  For example, if `axis = 1` and the inputs are\\n\\n      sp_inputs[0]: shape = [2, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  then the output will be\\n\\n      shape = [2, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [1, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b c  ]        [       ]   [b c          ]\\n\\n  Another example, if \\'axis = 1\\' and the inputs are\\n\\n      sp_inputs[0]: shape = [3, 3]\\n      [0, 2]: \"a\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n      sp_inputs[1]: shape = [2, 4]\\n      [0, 1]: \"d\"\\n      [0, 2]: \"e\"\\n\\n  if expand_nonconcat_dim = False, this will result in an error. But if\\n  expand_nonconcat_dim = True, this will result in:\\n\\n      shape = [3, 7]\\n      [0, 2]: \"a\"\\n      [0, 4]: \"d\"\\n      [0, 5]: \"e\"\\n      [1, 0]: \"b\"\\n      [2, 1]: \"c\"\\n\\n  Graphically this is equivalent to doing\\n\\n      [    a] concat [  d e  ] = [    a   d e  ]\\n      [b    ]        [       ]   [b            ]\\n      [  c  ]                    [  c          ]\\n\\n\\n  Args:\\n    axis: Dimension to concatenate along. Must be in range [-rank, rank),\\n      where rank is the number of dimensions in each input `SparseTensor`.\\n    sp_inputs: List of `SparseTensor` to concatenate.\\n    name: A name prefix for the returned tensors (optional).\\n    expand_nonconcat_dim: Whether to allow the expansion in the non-concat\\n      dimensions. Defaulted to False.\\n    concat_dim: The old (deprecated) name for axis.\\n    expand_nonconcat_dims: alias for expand_nonconcat_dim\\n\\n  Returns:\\n    A `SparseTensor` with the concatenated output.\\n\\n  Raises:\\n    TypeError: If `sp_inputs` is not a list of `SparseTensor`.\\n  '\n    expand_nonconcat_dim = deprecation.deprecated_argument_lookup('expand_nonconcat_dims', expand_nonconcat_dims, 'expand_nonconcat_dim', expand_nonconcat_dim)\n    if expand_nonconcat_dims is not None:\n        expand_nonconcat_dim = expand_nonconcat_dims\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'concat_dim', concat_dim)\n    return sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dim, name)"
        ]
    },
    {
        "func_name": "sparse_concat_v2",
        "original": "@tf_export('sparse.concat', v1=[])\ndef sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dims=False, name=None):\n    sp_inputs = _convert_to_sparse_tensors(sp_inputs)\n    if len(sp_inputs) == 1:\n        return sp_inputs[0]\n    inds = [sp_input.indices for sp_input in sp_inputs]\n    vals = [sp_input.values for sp_input in sp_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sp_inputs]\n    if expand_nonconcat_dims:\n        max_shape = math_ops.reduce_max(array_ops.concat([array_ops.reshape(shape, [1, -1]) for shape in shapes], 0), 0)\n        shapes = [array_ops.concat([max_shape[:axis], shape[-1:] if axis == -1 else shape[axis:axis + 1], [] if axis == -1 else max_shape[axis + 1:]], 0) for shape in shapes]\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name)\n    input_shapes = [inp.shape for inp in sp_inputs]\n    if all((shape.rank is not None for shape in input_shapes)):\n        if expand_nonconcat_dims:\n            static_output_shape = []\n            for dim in range(input_shapes[0].rank):\n                static_output_shape.append(max((tensor_shape.dimension_at_index(shape, dim) for shape in input_shapes)))\n        else:\n            static_output_shape = input_shapes[0].as_list()\n        static_output_shape[axis] = sum((tensor_shape.dimension_at_index(shape, axis) for shape in input_shapes))\n    else:\n        static_output_shape = tensor_shape.unknown_shape()\n    if all((shape.is_fully_defined() for shape in input_shapes)):\n        output_shape = ops.convert_to_tensor(static_output_shape, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        output = sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n        output.set_shape(tensor_shape.TensorShape(static_output_shape))\n        return output",
        "mutated": [
            "@tf_export('sparse.concat', v1=[])\ndef sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dims=False, name=None):\n    if False:\n        i = 10\n    sp_inputs = _convert_to_sparse_tensors(sp_inputs)\n    if len(sp_inputs) == 1:\n        return sp_inputs[0]\n    inds = [sp_input.indices for sp_input in sp_inputs]\n    vals = [sp_input.values for sp_input in sp_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sp_inputs]\n    if expand_nonconcat_dims:\n        max_shape = math_ops.reduce_max(array_ops.concat([array_ops.reshape(shape, [1, -1]) for shape in shapes], 0), 0)\n        shapes = [array_ops.concat([max_shape[:axis], shape[-1:] if axis == -1 else shape[axis:axis + 1], [] if axis == -1 else max_shape[axis + 1:]], 0) for shape in shapes]\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name)\n    input_shapes = [inp.shape for inp in sp_inputs]\n    if all((shape.rank is not None for shape in input_shapes)):\n        if expand_nonconcat_dims:\n            static_output_shape = []\n            for dim in range(input_shapes[0].rank):\n                static_output_shape.append(max((tensor_shape.dimension_at_index(shape, dim) for shape in input_shapes)))\n        else:\n            static_output_shape = input_shapes[0].as_list()\n        static_output_shape[axis] = sum((tensor_shape.dimension_at_index(shape, axis) for shape in input_shapes))\n    else:\n        static_output_shape = tensor_shape.unknown_shape()\n    if all((shape.is_fully_defined() for shape in input_shapes)):\n        output_shape = ops.convert_to_tensor(static_output_shape, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        output = sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n        output.set_shape(tensor_shape.TensorShape(static_output_shape))\n        return output",
            "@tf_export('sparse.concat', v1=[])\ndef sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dims=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sp_inputs = _convert_to_sparse_tensors(sp_inputs)\n    if len(sp_inputs) == 1:\n        return sp_inputs[0]\n    inds = [sp_input.indices for sp_input in sp_inputs]\n    vals = [sp_input.values for sp_input in sp_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sp_inputs]\n    if expand_nonconcat_dims:\n        max_shape = math_ops.reduce_max(array_ops.concat([array_ops.reshape(shape, [1, -1]) for shape in shapes], 0), 0)\n        shapes = [array_ops.concat([max_shape[:axis], shape[-1:] if axis == -1 else shape[axis:axis + 1], [] if axis == -1 else max_shape[axis + 1:]], 0) for shape in shapes]\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name)\n    input_shapes = [inp.shape for inp in sp_inputs]\n    if all((shape.rank is not None for shape in input_shapes)):\n        if expand_nonconcat_dims:\n            static_output_shape = []\n            for dim in range(input_shapes[0].rank):\n                static_output_shape.append(max((tensor_shape.dimension_at_index(shape, dim) for shape in input_shapes)))\n        else:\n            static_output_shape = input_shapes[0].as_list()\n        static_output_shape[axis] = sum((tensor_shape.dimension_at_index(shape, axis) for shape in input_shapes))\n    else:\n        static_output_shape = tensor_shape.unknown_shape()\n    if all((shape.is_fully_defined() for shape in input_shapes)):\n        output_shape = ops.convert_to_tensor(static_output_shape, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        output = sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n        output.set_shape(tensor_shape.TensorShape(static_output_shape))\n        return output",
            "@tf_export('sparse.concat', v1=[])\ndef sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dims=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sp_inputs = _convert_to_sparse_tensors(sp_inputs)\n    if len(sp_inputs) == 1:\n        return sp_inputs[0]\n    inds = [sp_input.indices for sp_input in sp_inputs]\n    vals = [sp_input.values for sp_input in sp_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sp_inputs]\n    if expand_nonconcat_dims:\n        max_shape = math_ops.reduce_max(array_ops.concat([array_ops.reshape(shape, [1, -1]) for shape in shapes], 0), 0)\n        shapes = [array_ops.concat([max_shape[:axis], shape[-1:] if axis == -1 else shape[axis:axis + 1], [] if axis == -1 else max_shape[axis + 1:]], 0) for shape in shapes]\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name)\n    input_shapes = [inp.shape for inp in sp_inputs]\n    if all((shape.rank is not None for shape in input_shapes)):\n        if expand_nonconcat_dims:\n            static_output_shape = []\n            for dim in range(input_shapes[0].rank):\n                static_output_shape.append(max((tensor_shape.dimension_at_index(shape, dim) for shape in input_shapes)))\n        else:\n            static_output_shape = input_shapes[0].as_list()\n        static_output_shape[axis] = sum((tensor_shape.dimension_at_index(shape, axis) for shape in input_shapes))\n    else:\n        static_output_shape = tensor_shape.unknown_shape()\n    if all((shape.is_fully_defined() for shape in input_shapes)):\n        output_shape = ops.convert_to_tensor(static_output_shape, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        output = sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n        output.set_shape(tensor_shape.TensorShape(static_output_shape))\n        return output",
            "@tf_export('sparse.concat', v1=[])\ndef sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dims=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sp_inputs = _convert_to_sparse_tensors(sp_inputs)\n    if len(sp_inputs) == 1:\n        return sp_inputs[0]\n    inds = [sp_input.indices for sp_input in sp_inputs]\n    vals = [sp_input.values for sp_input in sp_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sp_inputs]\n    if expand_nonconcat_dims:\n        max_shape = math_ops.reduce_max(array_ops.concat([array_ops.reshape(shape, [1, -1]) for shape in shapes], 0), 0)\n        shapes = [array_ops.concat([max_shape[:axis], shape[-1:] if axis == -1 else shape[axis:axis + 1], [] if axis == -1 else max_shape[axis + 1:]], 0) for shape in shapes]\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name)\n    input_shapes = [inp.shape for inp in sp_inputs]\n    if all((shape.rank is not None for shape in input_shapes)):\n        if expand_nonconcat_dims:\n            static_output_shape = []\n            for dim in range(input_shapes[0].rank):\n                static_output_shape.append(max((tensor_shape.dimension_at_index(shape, dim) for shape in input_shapes)))\n        else:\n            static_output_shape = input_shapes[0].as_list()\n        static_output_shape[axis] = sum((tensor_shape.dimension_at_index(shape, axis) for shape in input_shapes))\n    else:\n        static_output_shape = tensor_shape.unknown_shape()\n    if all((shape.is_fully_defined() for shape in input_shapes)):\n        output_shape = ops.convert_to_tensor(static_output_shape, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        output = sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n        output.set_shape(tensor_shape.TensorShape(static_output_shape))\n        return output",
            "@tf_export('sparse.concat', v1=[])\ndef sparse_concat_v2(axis, sp_inputs, expand_nonconcat_dims=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sp_inputs = _convert_to_sparse_tensors(sp_inputs)\n    if len(sp_inputs) == 1:\n        return sp_inputs[0]\n    inds = [sp_input.indices for sp_input in sp_inputs]\n    vals = [sp_input.values for sp_input in sp_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sp_inputs]\n    if expand_nonconcat_dims:\n        max_shape = math_ops.reduce_max(array_ops.concat([array_ops.reshape(shape, [1, -1]) for shape in shapes], 0), 0)\n        shapes = [array_ops.concat([max_shape[:axis], shape[-1:] if axis == -1 else shape[axis:axis + 1], [] if axis == -1 else max_shape[axis + 1:]], 0) for shape in shapes]\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name)\n    input_shapes = [inp.shape for inp in sp_inputs]\n    if all((shape.rank is not None for shape in input_shapes)):\n        if expand_nonconcat_dims:\n            static_output_shape = []\n            for dim in range(input_shapes[0].rank):\n                static_output_shape.append(max((tensor_shape.dimension_at_index(shape, dim) for shape in input_shapes)))\n        else:\n            static_output_shape = input_shapes[0].as_list()\n        static_output_shape[axis] = sum((tensor_shape.dimension_at_index(shape, axis) for shape in input_shapes))\n    else:\n        static_output_shape = tensor_shape.unknown_shape()\n    if all((shape.is_fully_defined() for shape in input_shapes)):\n        output_shape = ops.convert_to_tensor(static_output_shape, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        output = sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n        output.set_shape(tensor_shape.TensorShape(static_output_shape))\n        return output"
        ]
    },
    {
        "func_name": "sparse_add",
        "original": "@tf_export(v1=['sparse.add', 'sparse_add'])\n@deprecation.deprecated_endpoints('sparse_add')\n@deprecation.deprecated_args(None, 'thresh is deprecated, use threshold instead', 'thresh')\ndef sparse_add(a, b, threshold=None, thresh=None):\n    \"\"\"Adds two tensors, at least one of each is a `SparseTensor`.\n\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\n  `Tensor`s.\n\n  The shapes of the two operands must match: broadcasting is not supported.\n\n  The indices of any input `SparseTensor` are assumed ordered in standard\n  lexicographic order.  If this is not the case, before this step run\n  `SparseReorder` to restore index ordering.\n\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\n  if two values sum to zero at some index, the output `SparseTensor` would still\n  include that particular location in its index, storing a zero in the\n  corresponding value slot.  To override this, callers can specify `thresh`,\n  indicating that if the sum has a magnitude strictly smaller than `thresh`, its\n  corresponding value and index would then not be included.  In particular,\n  `thresh == 0.0` (default) means everything is kept and actual thresholding\n  happens only for a positive value.\n\n  For example, suppose the logical sum of two sparse operands is (densified):\n\n      [       2]\n      [.1     0]\n      [ 6   -.2]\n\n  Then,\n\n  * `thresh == 0` (the default): all 5 index/value pairs will be returned.\n  * `thresh == 0.11`: only .1 and 0 will vanish, and the remaining three\n      index/value pairs will be returned.\n  * `thresh == 0.21`: .1, 0, and -.2 will vanish.\n\n  Args:\n    a: The first operand; `SparseTensor` or `Tensor`.\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\n      must be sparse.\n    threshold: An optional 0-D `Tensor` (defaults to `0`). The magnitude\n      threshold that determines if an output value/index pair takes space. Its\n      dtype should match that of the values if they are real; if the latter are\n      complex64/complex128, then the dtype should be float32/float64,\n      correspondingly.\n    thresh: Deprecated alias for `threshold`.\n\n  Returns:\n    A `SparseTensor` or a `Tensor`, representing the sum.\n\n  Raises:\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\n  \"\"\"\n    threshold = deprecation.deprecated_argument_lookup('threshold', threshold, 'thresh', thresh)\n    if threshold is None:\n        threshold = 0\n    return sparse_add_v2(a, b, threshold)",
        "mutated": [
            "@tf_export(v1=['sparse.add', 'sparse_add'])\n@deprecation.deprecated_endpoints('sparse_add')\n@deprecation.deprecated_args(None, 'thresh is deprecated, use threshold instead', 'thresh')\ndef sparse_add(a, b, threshold=None, thresh=None):\n    if False:\n        i = 10\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `thresh`,\\n  indicating that if the sum has a magnitude strictly smaller than `thresh`, its\\n  corresponding value and index would then not be included.  In particular,\\n  `thresh == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `thresh == 0` (the default): all 5 index/value pairs will be returned.\\n  * `thresh == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `thresh == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: An optional 0-D `Tensor` (defaults to `0`). The magnitude\\n      threshold that determines if an output value/index pair takes space. Its\\n      dtype should match that of the values if they are real; if the latter are\\n      complex64/complex128, then the dtype should be float32/float64,\\n      correspondingly.\\n    thresh: Deprecated alias for `threshold`.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    threshold = deprecation.deprecated_argument_lookup('threshold', threshold, 'thresh', thresh)\n    if threshold is None:\n        threshold = 0\n    return sparse_add_v2(a, b, threshold)",
            "@tf_export(v1=['sparse.add', 'sparse_add'])\n@deprecation.deprecated_endpoints('sparse_add')\n@deprecation.deprecated_args(None, 'thresh is deprecated, use threshold instead', 'thresh')\ndef sparse_add(a, b, threshold=None, thresh=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `thresh`,\\n  indicating that if the sum has a magnitude strictly smaller than `thresh`, its\\n  corresponding value and index would then not be included.  In particular,\\n  `thresh == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `thresh == 0` (the default): all 5 index/value pairs will be returned.\\n  * `thresh == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `thresh == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: An optional 0-D `Tensor` (defaults to `0`). The magnitude\\n      threshold that determines if an output value/index pair takes space. Its\\n      dtype should match that of the values if they are real; if the latter are\\n      complex64/complex128, then the dtype should be float32/float64,\\n      correspondingly.\\n    thresh: Deprecated alias for `threshold`.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    threshold = deprecation.deprecated_argument_lookup('threshold', threshold, 'thresh', thresh)\n    if threshold is None:\n        threshold = 0\n    return sparse_add_v2(a, b, threshold)",
            "@tf_export(v1=['sparse.add', 'sparse_add'])\n@deprecation.deprecated_endpoints('sparse_add')\n@deprecation.deprecated_args(None, 'thresh is deprecated, use threshold instead', 'thresh')\ndef sparse_add(a, b, threshold=None, thresh=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `thresh`,\\n  indicating that if the sum has a magnitude strictly smaller than `thresh`, its\\n  corresponding value and index would then not be included.  In particular,\\n  `thresh == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `thresh == 0` (the default): all 5 index/value pairs will be returned.\\n  * `thresh == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `thresh == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: An optional 0-D `Tensor` (defaults to `0`). The magnitude\\n      threshold that determines if an output value/index pair takes space. Its\\n      dtype should match that of the values if they are real; if the latter are\\n      complex64/complex128, then the dtype should be float32/float64,\\n      correspondingly.\\n    thresh: Deprecated alias for `threshold`.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    threshold = deprecation.deprecated_argument_lookup('threshold', threshold, 'thresh', thresh)\n    if threshold is None:\n        threshold = 0\n    return sparse_add_v2(a, b, threshold)",
            "@tf_export(v1=['sparse.add', 'sparse_add'])\n@deprecation.deprecated_endpoints('sparse_add')\n@deprecation.deprecated_args(None, 'thresh is deprecated, use threshold instead', 'thresh')\ndef sparse_add(a, b, threshold=None, thresh=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `thresh`,\\n  indicating that if the sum has a magnitude strictly smaller than `thresh`, its\\n  corresponding value and index would then not be included.  In particular,\\n  `thresh == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `thresh == 0` (the default): all 5 index/value pairs will be returned.\\n  * `thresh == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `thresh == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: An optional 0-D `Tensor` (defaults to `0`). The magnitude\\n      threshold that determines if an output value/index pair takes space. Its\\n      dtype should match that of the values if they are real; if the latter are\\n      complex64/complex128, then the dtype should be float32/float64,\\n      correspondingly.\\n    thresh: Deprecated alias for `threshold`.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    threshold = deprecation.deprecated_argument_lookup('threshold', threshold, 'thresh', thresh)\n    if threshold is None:\n        threshold = 0\n    return sparse_add_v2(a, b, threshold)",
            "@tf_export(v1=['sparse.add', 'sparse_add'])\n@deprecation.deprecated_endpoints('sparse_add')\n@deprecation.deprecated_args(None, 'thresh is deprecated, use threshold instead', 'thresh')\ndef sparse_add(a, b, threshold=None, thresh=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `thresh`,\\n  indicating that if the sum has a magnitude strictly smaller than `thresh`, its\\n  corresponding value and index would then not be included.  In particular,\\n  `thresh == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `thresh == 0` (the default): all 5 index/value pairs will be returned.\\n  * `thresh == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `thresh == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: An optional 0-D `Tensor` (defaults to `0`). The magnitude\\n      threshold that determines if an output value/index pair takes space. Its\\n      dtype should match that of the values if they are real; if the latter are\\n      complex64/complex128, then the dtype should be float32/float64,\\n      correspondingly.\\n    thresh: Deprecated alias for `threshold`.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    threshold = deprecation.deprecated_argument_lookup('threshold', threshold, 'thresh', thresh)\n    if threshold is None:\n        threshold = 0\n    return sparse_add_v2(a, b, threshold)"
        ]
    },
    {
        "func_name": "sparse_add_v2",
        "original": "@tf_export('sparse.add', v1=[])\ndef sparse_add_v2(a, b, threshold=0):\n    \"\"\"Adds two tensors, at least one of each is a `SparseTensor`.\n\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\n  `Tensor`s.\n\n  The shapes of the two operands must match: broadcasting is not supported.\n\n  The indices of any input `SparseTensor` are assumed ordered in standard\n  lexicographic order.  If this is not the case, before this step run\n  `SparseReorder` to restore index ordering.\n\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\n  if two values sum to zero at some index, the output `SparseTensor` would still\n  include that particular location in its index, storing a zero in the\n  corresponding value slot.  To override this, callers can specify `threshold`,\n  indicating that if the sum has a magnitude strictly smaller than `threshold`,\n  its corresponding value and index would then not be included.  In particular,\n  `threshold == 0.0` (default) means everything is kept and actual thresholding\n  happens only for a positive value.\n\n  For example, suppose the logical sum of two sparse operands is (densified):\n\n      [       2]\n      [.1     0]\n      [ 6   -.2]\n\n  Then,\n\n  * `threshold == 0` (the default): all 5 index/value pairs will be\n      returned.\n  * `threshold == 0.11`: only .1 and 0 will vanish, and the remaining three\n      index/value pairs will be returned.\n  * `threshold == 0.21`: .1, 0, and -.2 will vanish.\n\n  Args:\n    a: The first operand; `SparseTensor` or `Tensor`.\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\n      must be sparse.\n    threshold: A 0-D `Tensor`. The magnitude threshold that determines if an\n      output value/index pair takes space. Its dtype should match that of the\n      values if they are real; if the latter are complex64/complex128, then the\n      dtype should be float32/float64, correspondingly.\n\n  Returns:\n    A `SparseTensor` or a `Tensor`, representing the sum.\n\n  Raises:\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\n  \"\"\"\n    sparse_classes = (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)\n    if not any((isinstance(inp, sparse_classes) for inp in [a, b])):\n        raise TypeError('At least one input should be SparseTensor; do you mean to use tf.add()?')\n    if all((isinstance(inp, sparse_classes) for inp in [a, b])):\n        a = _convert_to_sparse_tensor(a)\n        b = _convert_to_sparse_tensor(b)\n        threshold = ops.convert_to_tensor(threshold, dtype=a.values.dtype.real_dtype.base_dtype, name='threshold')\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_add(a.indices, a.values, a.dense_shape, b.indices, b.values, b.dense_shape, threshold)\n        a.get_shape().assert_is_compatible_with(b.get_shape())\n        static_shape = array_ops.broadcast_static_shape(a.get_shape(), b.get_shape())\n        if static_shape.is_fully_defined():\n            output_shape = static_shape.as_list()\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        if isinstance(b, sparse_classes):\n            (a, b) = (b, a)\n        return gen_sparse_ops.sparse_tensor_dense_add(a.indices, a.values, a.dense_shape, b)",
        "mutated": [
            "@tf_export('sparse.add', v1=[])\ndef sparse_add_v2(a, b, threshold=0):\n    if False:\n        i = 10\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `threshold`,\\n  indicating that if the sum has a magnitude strictly smaller than `threshold`,\\n  its corresponding value and index would then not be included.  In particular,\\n  `threshold == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `threshold == 0` (the default): all 5 index/value pairs will be\\n      returned.\\n  * `threshold == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `threshold == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: A 0-D `Tensor`. The magnitude threshold that determines if an\\n      output value/index pair takes space. Its dtype should match that of the\\n      values if they are real; if the latter are complex64/complex128, then the\\n      dtype should be float32/float64, correspondingly.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    sparse_classes = (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)\n    if not any((isinstance(inp, sparse_classes) for inp in [a, b])):\n        raise TypeError('At least one input should be SparseTensor; do you mean to use tf.add()?')\n    if all((isinstance(inp, sparse_classes) for inp in [a, b])):\n        a = _convert_to_sparse_tensor(a)\n        b = _convert_to_sparse_tensor(b)\n        threshold = ops.convert_to_tensor(threshold, dtype=a.values.dtype.real_dtype.base_dtype, name='threshold')\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_add(a.indices, a.values, a.dense_shape, b.indices, b.values, b.dense_shape, threshold)\n        a.get_shape().assert_is_compatible_with(b.get_shape())\n        static_shape = array_ops.broadcast_static_shape(a.get_shape(), b.get_shape())\n        if static_shape.is_fully_defined():\n            output_shape = static_shape.as_list()\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        if isinstance(b, sparse_classes):\n            (a, b) = (b, a)\n        return gen_sparse_ops.sparse_tensor_dense_add(a.indices, a.values, a.dense_shape, b)",
            "@tf_export('sparse.add', v1=[])\ndef sparse_add_v2(a, b, threshold=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `threshold`,\\n  indicating that if the sum has a magnitude strictly smaller than `threshold`,\\n  its corresponding value and index would then not be included.  In particular,\\n  `threshold == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `threshold == 0` (the default): all 5 index/value pairs will be\\n      returned.\\n  * `threshold == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `threshold == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: A 0-D `Tensor`. The magnitude threshold that determines if an\\n      output value/index pair takes space. Its dtype should match that of the\\n      values if they are real; if the latter are complex64/complex128, then the\\n      dtype should be float32/float64, correspondingly.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    sparse_classes = (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)\n    if not any((isinstance(inp, sparse_classes) for inp in [a, b])):\n        raise TypeError('At least one input should be SparseTensor; do you mean to use tf.add()?')\n    if all((isinstance(inp, sparse_classes) for inp in [a, b])):\n        a = _convert_to_sparse_tensor(a)\n        b = _convert_to_sparse_tensor(b)\n        threshold = ops.convert_to_tensor(threshold, dtype=a.values.dtype.real_dtype.base_dtype, name='threshold')\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_add(a.indices, a.values, a.dense_shape, b.indices, b.values, b.dense_shape, threshold)\n        a.get_shape().assert_is_compatible_with(b.get_shape())\n        static_shape = array_ops.broadcast_static_shape(a.get_shape(), b.get_shape())\n        if static_shape.is_fully_defined():\n            output_shape = static_shape.as_list()\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        if isinstance(b, sparse_classes):\n            (a, b) = (b, a)\n        return gen_sparse_ops.sparse_tensor_dense_add(a.indices, a.values, a.dense_shape, b)",
            "@tf_export('sparse.add', v1=[])\ndef sparse_add_v2(a, b, threshold=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `threshold`,\\n  indicating that if the sum has a magnitude strictly smaller than `threshold`,\\n  its corresponding value and index would then not be included.  In particular,\\n  `threshold == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `threshold == 0` (the default): all 5 index/value pairs will be\\n      returned.\\n  * `threshold == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `threshold == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: A 0-D `Tensor`. The magnitude threshold that determines if an\\n      output value/index pair takes space. Its dtype should match that of the\\n      values if they are real; if the latter are complex64/complex128, then the\\n      dtype should be float32/float64, correspondingly.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    sparse_classes = (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)\n    if not any((isinstance(inp, sparse_classes) for inp in [a, b])):\n        raise TypeError('At least one input should be SparseTensor; do you mean to use tf.add()?')\n    if all((isinstance(inp, sparse_classes) for inp in [a, b])):\n        a = _convert_to_sparse_tensor(a)\n        b = _convert_to_sparse_tensor(b)\n        threshold = ops.convert_to_tensor(threshold, dtype=a.values.dtype.real_dtype.base_dtype, name='threshold')\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_add(a.indices, a.values, a.dense_shape, b.indices, b.values, b.dense_shape, threshold)\n        a.get_shape().assert_is_compatible_with(b.get_shape())\n        static_shape = array_ops.broadcast_static_shape(a.get_shape(), b.get_shape())\n        if static_shape.is_fully_defined():\n            output_shape = static_shape.as_list()\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        if isinstance(b, sparse_classes):\n            (a, b) = (b, a)\n        return gen_sparse_ops.sparse_tensor_dense_add(a.indices, a.values, a.dense_shape, b)",
            "@tf_export('sparse.add', v1=[])\ndef sparse_add_v2(a, b, threshold=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `threshold`,\\n  indicating that if the sum has a magnitude strictly smaller than `threshold`,\\n  its corresponding value and index would then not be included.  In particular,\\n  `threshold == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `threshold == 0` (the default): all 5 index/value pairs will be\\n      returned.\\n  * `threshold == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `threshold == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: A 0-D `Tensor`. The magnitude threshold that determines if an\\n      output value/index pair takes space. Its dtype should match that of the\\n      values if they are real; if the latter are complex64/complex128, then the\\n      dtype should be float32/float64, correspondingly.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    sparse_classes = (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)\n    if not any((isinstance(inp, sparse_classes) for inp in [a, b])):\n        raise TypeError('At least one input should be SparseTensor; do you mean to use tf.add()?')\n    if all((isinstance(inp, sparse_classes) for inp in [a, b])):\n        a = _convert_to_sparse_tensor(a)\n        b = _convert_to_sparse_tensor(b)\n        threshold = ops.convert_to_tensor(threshold, dtype=a.values.dtype.real_dtype.base_dtype, name='threshold')\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_add(a.indices, a.values, a.dense_shape, b.indices, b.values, b.dense_shape, threshold)\n        a.get_shape().assert_is_compatible_with(b.get_shape())\n        static_shape = array_ops.broadcast_static_shape(a.get_shape(), b.get_shape())\n        if static_shape.is_fully_defined():\n            output_shape = static_shape.as_list()\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        if isinstance(b, sparse_classes):\n            (a, b) = (b, a)\n        return gen_sparse_ops.sparse_tensor_dense_add(a.indices, a.values, a.dense_shape, b)",
            "@tf_export('sparse.add', v1=[])\ndef sparse_add_v2(a, b, threshold=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Adds two tensors, at least one of each is a `SparseTensor`.\\n\\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\\n  `Tensor`s.\\n\\n  The shapes of the two operands must match: broadcasting is not supported.\\n\\n  The indices of any input `SparseTensor` are assumed ordered in standard\\n  lexicographic order.  If this is not the case, before this step run\\n  `SparseReorder` to restore index ordering.\\n\\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\\n  if two values sum to zero at some index, the output `SparseTensor` would still\\n  include that particular location in its index, storing a zero in the\\n  corresponding value slot.  To override this, callers can specify `threshold`,\\n  indicating that if the sum has a magnitude strictly smaller than `threshold`,\\n  its corresponding value and index would then not be included.  In particular,\\n  `threshold == 0.0` (default) means everything is kept and actual thresholding\\n  happens only for a positive value.\\n\\n  For example, suppose the logical sum of two sparse operands is (densified):\\n\\n      [       2]\\n      [.1     0]\\n      [ 6   -.2]\\n\\n  Then,\\n\\n  * `threshold == 0` (the default): all 5 index/value pairs will be\\n      returned.\\n  * `threshold == 0.11`: only .1 and 0 will vanish, and the remaining three\\n      index/value pairs will be returned.\\n  * `threshold == 0.21`: .1, 0, and -.2 will vanish.\\n\\n  Args:\\n    a: The first operand; `SparseTensor` or `Tensor`.\\n    b: The second operand; `SparseTensor` or `Tensor`. At least one operand\\n      must be sparse.\\n    threshold: A 0-D `Tensor`. The magnitude threshold that determines if an\\n      output value/index pair takes space. Its dtype should match that of the\\n      values if they are real; if the latter are complex64/complex128, then the\\n      dtype should be float32/float64, correspondingly.\\n\\n  Returns:\\n    A `SparseTensor` or a `Tensor`, representing the sum.\\n\\n  Raises:\\n    TypeError: If both `a` and `b` are `Tensor`s.  Use `tf.add()` instead.\\n  '\n    sparse_classes = (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)\n    if not any((isinstance(inp, sparse_classes) for inp in [a, b])):\n        raise TypeError('At least one input should be SparseTensor; do you mean to use tf.add()?')\n    if all((isinstance(inp, sparse_classes) for inp in [a, b])):\n        a = _convert_to_sparse_tensor(a)\n        b = _convert_to_sparse_tensor(b)\n        threshold = ops.convert_to_tensor(threshold, dtype=a.values.dtype.real_dtype.base_dtype, name='threshold')\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_add(a.indices, a.values, a.dense_shape, b.indices, b.values, b.dense_shape, threshold)\n        a.get_shape().assert_is_compatible_with(b.get_shape())\n        static_shape = array_ops.broadcast_static_shape(a.get_shape(), b.get_shape())\n        if static_shape.is_fully_defined():\n            output_shape = static_shape.as_list()\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    else:\n        if isinstance(b, sparse_classes):\n            (a, b) = (b, a)\n        return gen_sparse_ops.sparse_tensor_dense_add(a.indices, a.values, a.dense_shape, b)"
        ]
    },
    {
        "func_name": "sparse_cross",
        "original": "@tf_export('sparse.cross')\ndef sparse_cross(inputs, name=None, separator=None):\n    \"\"\"Generates sparse cross from a list of sparse and dense tensors.\n\n  For example, if the inputs are\n\n      * inputs[0]: SparseTensor with shape = [2, 2]\n        [0, 0]: \"a\"\n        [1, 0]: \"b\"\n        [1, 1]: \"c\"\n      * inputs[1]: SparseTensor with shape = [2, 1]\n        [0, 0]: \"d\"\n        [1, 0]: \"e\"\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\n\n  then the output will be:\n\n      shape = [2, 2]\n      [0, 0]: \"a_X_d_X_f\"\n      [1, 0]: \"b_X_e_X_g\"\n      [1, 1]: \"c_X_e_X_g\"\n\n  Customized separator \"_Y_\":\n\n  >>> inp_0 = tf.constant([['a'], ['b']])\n  >>> inp_1 = tf.constant([['c'], ['d']])\n  >>> output = tf.sparse.cross([inp_0, inp_1], separator='_Y_')\n  >>> output.values\n  <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a_Y_c', b'b_Y_d'],\n    dtype=object)>\n\n\n  Args:\n    inputs: An iterable of `Tensor` or `SparseTensor`.\n    name: Optional name for the op.\n    separator: A string added between each string being joined. Defaults to\n      '_X_'.\n\n  Returns:\n    A `SparseTensor` of type `string`.\n  \"\"\"\n    if separator is None:\n        separator = '_X_'\n    separator = ops.convert_to_tensor(separator, dtypes.string)\n    (indices, values, shapes, dense_inputs) = _sparse_cross_internal_v2(inputs)\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross_v2(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, sep=separator, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
        "mutated": [
            "@tf_export('sparse.cross')\ndef sparse_cross(inputs, name=None, separator=None):\n    if False:\n        i = 10\n    'Generates sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: \"a_X_d_X_f\"\\n      [1, 0]: \"b_X_e_X_g\"\\n      [1, 1]: \"c_X_e_X_g\"\\n\\n  Customized separator \"_Y_\":\\n\\n  >>> inp_0 = tf.constant([[\\'a\\'], [\\'b\\']])\\n  >>> inp_1 = tf.constant([[\\'c\\'], [\\'d\\']])\\n  >>> output = tf.sparse.cross([inp_0, inp_1], separator=\\'_Y_\\')\\n  >>> output.values\\n  <tf.Tensor: shape=(2,), dtype=string, numpy=array([b\\'a_Y_c\\', b\\'b_Y_d\\'],\\n    dtype=object)>\\n\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    name: Optional name for the op.\\n    separator: A string added between each string being joined. Defaults to\\n      \\'_X_\\'.\\n\\n  Returns:\\n    A `SparseTensor` of type `string`.\\n  '\n    if separator is None:\n        separator = '_X_'\n    separator = ops.convert_to_tensor(separator, dtypes.string)\n    (indices, values, shapes, dense_inputs) = _sparse_cross_internal_v2(inputs)\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross_v2(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, sep=separator, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
            "@tf_export('sparse.cross')\ndef sparse_cross(inputs, name=None, separator=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generates sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: \"a_X_d_X_f\"\\n      [1, 0]: \"b_X_e_X_g\"\\n      [1, 1]: \"c_X_e_X_g\"\\n\\n  Customized separator \"_Y_\":\\n\\n  >>> inp_0 = tf.constant([[\\'a\\'], [\\'b\\']])\\n  >>> inp_1 = tf.constant([[\\'c\\'], [\\'d\\']])\\n  >>> output = tf.sparse.cross([inp_0, inp_1], separator=\\'_Y_\\')\\n  >>> output.values\\n  <tf.Tensor: shape=(2,), dtype=string, numpy=array([b\\'a_Y_c\\', b\\'b_Y_d\\'],\\n    dtype=object)>\\n\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    name: Optional name for the op.\\n    separator: A string added between each string being joined. Defaults to\\n      \\'_X_\\'.\\n\\n  Returns:\\n    A `SparseTensor` of type `string`.\\n  '\n    if separator is None:\n        separator = '_X_'\n    separator = ops.convert_to_tensor(separator, dtypes.string)\n    (indices, values, shapes, dense_inputs) = _sparse_cross_internal_v2(inputs)\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross_v2(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, sep=separator, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
            "@tf_export('sparse.cross')\ndef sparse_cross(inputs, name=None, separator=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generates sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: \"a_X_d_X_f\"\\n      [1, 0]: \"b_X_e_X_g\"\\n      [1, 1]: \"c_X_e_X_g\"\\n\\n  Customized separator \"_Y_\":\\n\\n  >>> inp_0 = tf.constant([[\\'a\\'], [\\'b\\']])\\n  >>> inp_1 = tf.constant([[\\'c\\'], [\\'d\\']])\\n  >>> output = tf.sparse.cross([inp_0, inp_1], separator=\\'_Y_\\')\\n  >>> output.values\\n  <tf.Tensor: shape=(2,), dtype=string, numpy=array([b\\'a_Y_c\\', b\\'b_Y_d\\'],\\n    dtype=object)>\\n\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    name: Optional name for the op.\\n    separator: A string added between each string being joined. Defaults to\\n      \\'_X_\\'.\\n\\n  Returns:\\n    A `SparseTensor` of type `string`.\\n  '\n    if separator is None:\n        separator = '_X_'\n    separator = ops.convert_to_tensor(separator, dtypes.string)\n    (indices, values, shapes, dense_inputs) = _sparse_cross_internal_v2(inputs)\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross_v2(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, sep=separator, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
            "@tf_export('sparse.cross')\ndef sparse_cross(inputs, name=None, separator=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generates sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: \"a_X_d_X_f\"\\n      [1, 0]: \"b_X_e_X_g\"\\n      [1, 1]: \"c_X_e_X_g\"\\n\\n  Customized separator \"_Y_\":\\n\\n  >>> inp_0 = tf.constant([[\\'a\\'], [\\'b\\']])\\n  >>> inp_1 = tf.constant([[\\'c\\'], [\\'d\\']])\\n  >>> output = tf.sparse.cross([inp_0, inp_1], separator=\\'_Y_\\')\\n  >>> output.values\\n  <tf.Tensor: shape=(2,), dtype=string, numpy=array([b\\'a_Y_c\\', b\\'b_Y_d\\'],\\n    dtype=object)>\\n\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    name: Optional name for the op.\\n    separator: A string added between each string being joined. Defaults to\\n      \\'_X_\\'.\\n\\n  Returns:\\n    A `SparseTensor` of type `string`.\\n  '\n    if separator is None:\n        separator = '_X_'\n    separator = ops.convert_to_tensor(separator, dtypes.string)\n    (indices, values, shapes, dense_inputs) = _sparse_cross_internal_v2(inputs)\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross_v2(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, sep=separator, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
            "@tf_export('sparse.cross')\ndef sparse_cross(inputs, name=None, separator=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generates sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: \"a_X_d_X_f\"\\n      [1, 0]: \"b_X_e_X_g\"\\n      [1, 1]: \"c_X_e_X_g\"\\n\\n  Customized separator \"_Y_\":\\n\\n  >>> inp_0 = tf.constant([[\\'a\\'], [\\'b\\']])\\n  >>> inp_1 = tf.constant([[\\'c\\'], [\\'d\\']])\\n  >>> output = tf.sparse.cross([inp_0, inp_1], separator=\\'_Y_\\')\\n  >>> output.values\\n  <tf.Tensor: shape=(2,), dtype=string, numpy=array([b\\'a_Y_c\\', b\\'b_Y_d\\'],\\n    dtype=object)>\\n\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    name: Optional name for the op.\\n    separator: A string added between each string being joined. Defaults to\\n      \\'_X_\\'.\\n\\n  Returns:\\n    A `SparseTensor` of type `string`.\\n  '\n    if separator is None:\n        separator = '_X_'\n    separator = ops.convert_to_tensor(separator, dtypes.string)\n    (indices, values, shapes, dense_inputs) = _sparse_cross_internal_v2(inputs)\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross_v2(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, sep=separator, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)"
        ]
    },
    {
        "func_name": "sparse_cross_hashed",
        "original": "@tf_export('sparse.cross_hashed')\ndef sparse_cross_hashed(inputs, num_buckets=0, hash_key=None, name=None):\n    \"\"\"Generates hashed sparse cross from a list of sparse and dense tensors.\n\n  For example, if the inputs are\n\n      * inputs[0]: SparseTensor with shape = [2, 2]\n        [0, 0]: \"a\"\n        [1, 0]: \"b\"\n        [1, 1]: \"c\"\n      * inputs[1]: SparseTensor with shape = [2, 1]\n        [0, 0]: \"d\"\n        [1, 0]: \"e\"\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\n\n  then the output will be:\n\n      shape = [2, 2]\n      [0, 0]: FingerprintCat64(\n                  Fingerprint64(\"f\"), FingerprintCat64(\n                      Fingerprint64(\"d\"), Fingerprint64(\"a\")))\n      [1, 0]: FingerprintCat64(\n                  Fingerprint64(\"g\"), FingerprintCat64(\n                      Fingerprint64(\"e\"), Fingerprint64(\"b\")))\n      [1, 1]: FingerprintCat64(\n                  Fingerprint64(\"g\"), FingerprintCat64(\n                      Fingerprint64(\"e\"), Fingerprint64(\"c\")))\n\n  Args:\n    inputs: An iterable of `Tensor` or `SparseTensor`.\n    num_buckets: An `int` that is `>= 0`.\n      output = hashed_value%num_buckets if num_buckets > 0 else hashed_value.\n    hash_key: Integer hash_key that will be used by the `FingerprintCat64`\n      function. If not given, will use a default key.\n    name: Optional name for the op.\n\n  Returns:\n    A `SparseTensor` of type `int64`.\n  \"\"\"\n    return _sparse_cross_internal(inputs=inputs, hashed_output=True, num_buckets=num_buckets, hash_key=hash_key, name=name)",
        "mutated": [
            "@tf_export('sparse.cross_hashed')\ndef sparse_cross_hashed(inputs, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n    'Generates hashed sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: FingerprintCat64(\\n                  Fingerprint64(\"f\"), FingerprintCat64(\\n                      Fingerprint64(\"d\"), Fingerprint64(\"a\")))\\n      [1, 0]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"b\")))\\n      [1, 1]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"c\")))\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    num_buckets: An `int` that is `>= 0`.\\n      output = hashed_value%num_buckets if num_buckets > 0 else hashed_value.\\n    hash_key: Integer hash_key that will be used by the `FingerprintCat64`\\n      function. If not given, will use a default key.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    A `SparseTensor` of type `int64`.\\n  '\n    return _sparse_cross_internal(inputs=inputs, hashed_output=True, num_buckets=num_buckets, hash_key=hash_key, name=name)",
            "@tf_export('sparse.cross_hashed')\ndef sparse_cross_hashed(inputs, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generates hashed sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: FingerprintCat64(\\n                  Fingerprint64(\"f\"), FingerprintCat64(\\n                      Fingerprint64(\"d\"), Fingerprint64(\"a\")))\\n      [1, 0]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"b\")))\\n      [1, 1]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"c\")))\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    num_buckets: An `int` that is `>= 0`.\\n      output = hashed_value%num_buckets if num_buckets > 0 else hashed_value.\\n    hash_key: Integer hash_key that will be used by the `FingerprintCat64`\\n      function. If not given, will use a default key.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    A `SparseTensor` of type `int64`.\\n  '\n    return _sparse_cross_internal(inputs=inputs, hashed_output=True, num_buckets=num_buckets, hash_key=hash_key, name=name)",
            "@tf_export('sparse.cross_hashed')\ndef sparse_cross_hashed(inputs, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generates hashed sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: FingerprintCat64(\\n                  Fingerprint64(\"f\"), FingerprintCat64(\\n                      Fingerprint64(\"d\"), Fingerprint64(\"a\")))\\n      [1, 0]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"b\")))\\n      [1, 1]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"c\")))\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    num_buckets: An `int` that is `>= 0`.\\n      output = hashed_value%num_buckets if num_buckets > 0 else hashed_value.\\n    hash_key: Integer hash_key that will be used by the `FingerprintCat64`\\n      function. If not given, will use a default key.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    A `SparseTensor` of type `int64`.\\n  '\n    return _sparse_cross_internal(inputs=inputs, hashed_output=True, num_buckets=num_buckets, hash_key=hash_key, name=name)",
            "@tf_export('sparse.cross_hashed')\ndef sparse_cross_hashed(inputs, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generates hashed sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: FingerprintCat64(\\n                  Fingerprint64(\"f\"), FingerprintCat64(\\n                      Fingerprint64(\"d\"), Fingerprint64(\"a\")))\\n      [1, 0]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"b\")))\\n      [1, 1]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"c\")))\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    num_buckets: An `int` that is `>= 0`.\\n      output = hashed_value%num_buckets if num_buckets > 0 else hashed_value.\\n    hash_key: Integer hash_key that will be used by the `FingerprintCat64`\\n      function. If not given, will use a default key.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    A `SparseTensor` of type `int64`.\\n  '\n    return _sparse_cross_internal(inputs=inputs, hashed_output=True, num_buckets=num_buckets, hash_key=hash_key, name=name)",
            "@tf_export('sparse.cross_hashed')\ndef sparse_cross_hashed(inputs, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generates hashed sparse cross from a list of sparse and dense tensors.\\n\\n  For example, if the inputs are\\n\\n      * inputs[0]: SparseTensor with shape = [2, 2]\\n        [0, 0]: \"a\"\\n        [1, 0]: \"b\"\\n        [1, 1]: \"c\"\\n      * inputs[1]: SparseTensor with shape = [2, 1]\\n        [0, 0]: \"d\"\\n        [1, 0]: \"e\"\\n      * inputs[2]: Tensor [[\"f\"], [\"g\"]]\\n\\n  then the output will be:\\n\\n      shape = [2, 2]\\n      [0, 0]: FingerprintCat64(\\n                  Fingerprint64(\"f\"), FingerprintCat64(\\n                      Fingerprint64(\"d\"), Fingerprint64(\"a\")))\\n      [1, 0]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"b\")))\\n      [1, 1]: FingerprintCat64(\\n                  Fingerprint64(\"g\"), FingerprintCat64(\\n                      Fingerprint64(\"e\"), Fingerprint64(\"c\")))\\n\\n  Args:\\n    inputs: An iterable of `Tensor` or `SparseTensor`.\\n    num_buckets: An `int` that is `>= 0`.\\n      output = hashed_value%num_buckets if num_buckets > 0 else hashed_value.\\n    hash_key: Integer hash_key that will be used by the `FingerprintCat64`\\n      function. If not given, will use a default key.\\n    name: Optional name for the op.\\n\\n  Returns:\\n    A `SparseTensor` of type `int64`.\\n  '\n    return _sparse_cross_internal(inputs=inputs, hashed_output=True, num_buckets=num_buckets, hash_key=hash_key, name=name)"
        ]
    },
    {
        "func_name": "_sparse_cross_internal_v2",
        "original": "def _sparse_cross_internal_v2(inputs):\n    \"\"\"See gen_sparse_ops.sparse_cross_v2.\"\"\"\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be Tensor or SparseTensor.')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n    return (indices, values, shapes, dense_inputs)",
        "mutated": [
            "def _sparse_cross_internal_v2(inputs):\n    if False:\n        i = 10\n    'See gen_sparse_ops.sparse_cross_v2.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be Tensor or SparseTensor.')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n    return (indices, values, shapes, dense_inputs)",
            "def _sparse_cross_internal_v2(inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'See gen_sparse_ops.sparse_cross_v2.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be Tensor or SparseTensor.')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n    return (indices, values, shapes, dense_inputs)",
            "def _sparse_cross_internal_v2(inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'See gen_sparse_ops.sparse_cross_v2.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be Tensor or SparseTensor.')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n    return (indices, values, shapes, dense_inputs)",
            "def _sparse_cross_internal_v2(inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'See gen_sparse_ops.sparse_cross_v2.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be Tensor or SparseTensor.')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n    return (indices, values, shapes, dense_inputs)",
            "def _sparse_cross_internal_v2(inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'See gen_sparse_ops.sparse_cross_v2.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be Tensor or SparseTensor.')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n    return (indices, values, shapes, dense_inputs)"
        ]
    },
    {
        "func_name": "_sparse_cross_internal",
        "original": "def _sparse_cross_internal(inputs, hashed_output=False, num_buckets=0, hash_key=None, name=None):\n    \"\"\"See gen_sparse_ops.sparse_cross.\"\"\"\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be SparseTensors')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    out_type = dtypes.int64 if hashed_output else dtypes.string\n    internal_type = dtypes.string\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n            internal_type = dtypes.int64\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n            internal_type = dtypes.int64\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, hashed_output=hashed_output, num_buckets=num_buckets, hash_key=hash_key or _DEFAULT_HASH_KEY, out_type=out_type, internal_type=internal_type, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
        "mutated": [
            "def _sparse_cross_internal(inputs, hashed_output=False, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n    'See gen_sparse_ops.sparse_cross.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be SparseTensors')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    out_type = dtypes.int64 if hashed_output else dtypes.string\n    internal_type = dtypes.string\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n            internal_type = dtypes.int64\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n            internal_type = dtypes.int64\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, hashed_output=hashed_output, num_buckets=num_buckets, hash_key=hash_key or _DEFAULT_HASH_KEY, out_type=out_type, internal_type=internal_type, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
            "def _sparse_cross_internal(inputs, hashed_output=False, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'See gen_sparse_ops.sparse_cross.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be SparseTensors')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    out_type = dtypes.int64 if hashed_output else dtypes.string\n    internal_type = dtypes.string\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n            internal_type = dtypes.int64\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n            internal_type = dtypes.int64\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, hashed_output=hashed_output, num_buckets=num_buckets, hash_key=hash_key or _DEFAULT_HASH_KEY, out_type=out_type, internal_type=internal_type, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
            "def _sparse_cross_internal(inputs, hashed_output=False, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'See gen_sparse_ops.sparse_cross.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be SparseTensors')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    out_type = dtypes.int64 if hashed_output else dtypes.string\n    internal_type = dtypes.string\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n            internal_type = dtypes.int64\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n            internal_type = dtypes.int64\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, hashed_output=hashed_output, num_buckets=num_buckets, hash_key=hash_key or _DEFAULT_HASH_KEY, out_type=out_type, internal_type=internal_type, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
            "def _sparse_cross_internal(inputs, hashed_output=False, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'See gen_sparse_ops.sparse_cross.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be SparseTensors')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    out_type = dtypes.int64 if hashed_output else dtypes.string\n    internal_type = dtypes.string\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n            internal_type = dtypes.int64\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n            internal_type = dtypes.int64\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, hashed_output=hashed_output, num_buckets=num_buckets, hash_key=hash_key or _DEFAULT_HASH_KEY, out_type=out_type, internal_type=internal_type, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)",
            "def _sparse_cross_internal(inputs, hashed_output=False, num_buckets=0, hash_key=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'See gen_sparse_ops.sparse_cross.'\n    if not isinstance(inputs, (tuple, list)):\n        raise TypeError('Inputs must be a list')\n    if not all((isinstance(i, sparse_tensor.SparseTensor) or isinstance(i, tensor_lib.Tensor) for i in inputs)):\n        raise TypeError('All inputs must be SparseTensors')\n    sparse_inputs = [i for i in inputs if isinstance(i, sparse_tensor.SparseTensor)]\n    dense_inputs = [i for i in inputs if not isinstance(i, sparse_tensor.SparseTensor)]\n    indices = [sp_input.indices for sp_input in sparse_inputs]\n    values = [sp_input.values for sp_input in sparse_inputs]\n    shapes = [sp_input.dense_shape for sp_input in sparse_inputs]\n    out_type = dtypes.int64 if hashed_output else dtypes.string\n    internal_type = dtypes.string\n    for i in range(len(values)):\n        if values[i].dtype != dtypes.string:\n            values[i] = math_ops.cast(values[i], dtypes.int64)\n            internal_type = dtypes.int64\n    for i in range(len(dense_inputs)):\n        if dense_inputs[i].dtype != dtypes.string:\n            dense_inputs[i] = math_ops.cast(dense_inputs[i], dtypes.int64)\n            internal_type = dtypes.int64\n    (indices_out, values_out, shape_out) = gen_sparse_ops.sparse_cross(indices=indices, values=values, shapes=shapes, dense_inputs=dense_inputs, hashed_output=hashed_output, num_buckets=num_buckets, hash_key=hash_key or _DEFAULT_HASH_KEY, out_type=out_type, internal_type=internal_type, name=name)\n    return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)"
        ]
    },
    {
        "func_name": "sparse_dense_cwise_add",
        "original": "def sparse_dense_cwise_add(sp_t, dense_t):\n    \"\"\"Adds up a SparseTensor and a dense Tensor, using these special rules:\n\n  (1) Broadcasts the dense side to have the same shape as the sparse side, if\n      eligible;\n  (2) Then, only the dense values pointed to by the indices of the SparseTensor\n      participate in the cwise addition.\n\n  By the rules, the result is a logical SparseTensor with exactly the same\n  indices and shape, but possibly with different non-zero values.  The output of\n  this Op is the resultant non-zero values.\n\n  Args:\n    sp_t: the SparseTensor operand.\n    dense_t: the dense Tensor operand; must have the same dtype and a\n      broadcast-compatible shape as `sp_t`.\n\n  Returns:\n    output: the SparseTensor output.\n  \"\"\"\n    result = gen_sparse_ops.sparse_dense_cwise_add(sp_t.indices, sp_t.values, sp_t.dense_shape, dense_t)\n    return sparse_tensor.SparseTensor(sp_t.indices, result, sp_t.dense_shape)",
        "mutated": [
            "def sparse_dense_cwise_add(sp_t, dense_t):\n    if False:\n        i = 10\n    'Adds up a SparseTensor and a dense Tensor, using these special rules:\\n\\n  (1) Broadcasts the dense side to have the same shape as the sparse side, if\\n      eligible;\\n  (2) Then, only the dense values pointed to by the indices of the SparseTensor\\n      participate in the cwise addition.\\n\\n  By the rules, the result is a logical SparseTensor with exactly the same\\n  indices and shape, but possibly with different non-zero values.  The output of\\n  this Op is the resultant non-zero values.\\n\\n  Args:\\n    sp_t: the SparseTensor operand.\\n    dense_t: the dense Tensor operand; must have the same dtype and a\\n      broadcast-compatible shape as `sp_t`.\\n\\n  Returns:\\n    output: the SparseTensor output.\\n  '\n    result = gen_sparse_ops.sparse_dense_cwise_add(sp_t.indices, sp_t.values, sp_t.dense_shape, dense_t)\n    return sparse_tensor.SparseTensor(sp_t.indices, result, sp_t.dense_shape)",
            "def sparse_dense_cwise_add(sp_t, dense_t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Adds up a SparseTensor and a dense Tensor, using these special rules:\\n\\n  (1) Broadcasts the dense side to have the same shape as the sparse side, if\\n      eligible;\\n  (2) Then, only the dense values pointed to by the indices of the SparseTensor\\n      participate in the cwise addition.\\n\\n  By the rules, the result is a logical SparseTensor with exactly the same\\n  indices and shape, but possibly with different non-zero values.  The output of\\n  this Op is the resultant non-zero values.\\n\\n  Args:\\n    sp_t: the SparseTensor operand.\\n    dense_t: the dense Tensor operand; must have the same dtype and a\\n      broadcast-compatible shape as `sp_t`.\\n\\n  Returns:\\n    output: the SparseTensor output.\\n  '\n    result = gen_sparse_ops.sparse_dense_cwise_add(sp_t.indices, sp_t.values, sp_t.dense_shape, dense_t)\n    return sparse_tensor.SparseTensor(sp_t.indices, result, sp_t.dense_shape)",
            "def sparse_dense_cwise_add(sp_t, dense_t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Adds up a SparseTensor and a dense Tensor, using these special rules:\\n\\n  (1) Broadcasts the dense side to have the same shape as the sparse side, if\\n      eligible;\\n  (2) Then, only the dense values pointed to by the indices of the SparseTensor\\n      participate in the cwise addition.\\n\\n  By the rules, the result is a logical SparseTensor with exactly the same\\n  indices and shape, but possibly with different non-zero values.  The output of\\n  this Op is the resultant non-zero values.\\n\\n  Args:\\n    sp_t: the SparseTensor operand.\\n    dense_t: the dense Tensor operand; must have the same dtype and a\\n      broadcast-compatible shape as `sp_t`.\\n\\n  Returns:\\n    output: the SparseTensor output.\\n  '\n    result = gen_sparse_ops.sparse_dense_cwise_add(sp_t.indices, sp_t.values, sp_t.dense_shape, dense_t)\n    return sparse_tensor.SparseTensor(sp_t.indices, result, sp_t.dense_shape)",
            "def sparse_dense_cwise_add(sp_t, dense_t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Adds up a SparseTensor and a dense Tensor, using these special rules:\\n\\n  (1) Broadcasts the dense side to have the same shape as the sparse side, if\\n      eligible;\\n  (2) Then, only the dense values pointed to by the indices of the SparseTensor\\n      participate in the cwise addition.\\n\\n  By the rules, the result is a logical SparseTensor with exactly the same\\n  indices and shape, but possibly with different non-zero values.  The output of\\n  this Op is the resultant non-zero values.\\n\\n  Args:\\n    sp_t: the SparseTensor operand.\\n    dense_t: the dense Tensor operand; must have the same dtype and a\\n      broadcast-compatible shape as `sp_t`.\\n\\n  Returns:\\n    output: the SparseTensor output.\\n  '\n    result = gen_sparse_ops.sparse_dense_cwise_add(sp_t.indices, sp_t.values, sp_t.dense_shape, dense_t)\n    return sparse_tensor.SparseTensor(sp_t.indices, result, sp_t.dense_shape)",
            "def sparse_dense_cwise_add(sp_t, dense_t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Adds up a SparseTensor and a dense Tensor, using these special rules:\\n\\n  (1) Broadcasts the dense side to have the same shape as the sparse side, if\\n      eligible;\\n  (2) Then, only the dense values pointed to by the indices of the SparseTensor\\n      participate in the cwise addition.\\n\\n  By the rules, the result is a logical SparseTensor with exactly the same\\n  indices and shape, but possibly with different non-zero values.  The output of\\n  this Op is the resultant non-zero values.\\n\\n  Args:\\n    sp_t: the SparseTensor operand.\\n    dense_t: the dense Tensor operand; must have the same dtype and a\\n      broadcast-compatible shape as `sp_t`.\\n\\n  Returns:\\n    output: the SparseTensor output.\\n  '\n    result = gen_sparse_ops.sparse_dense_cwise_add(sp_t.indices, sp_t.values, sp_t.dense_shape, dense_t)\n    return sparse_tensor.SparseTensor(sp_t.indices, result, sp_t.dense_shape)"
        ]
    },
    {
        "func_name": "sparse_reorder",
        "original": "@tf_export('sparse.reorder', v1=['sparse.reorder', 'sparse_reorder'])\n@deprecation.deprecated_endpoints('sparse_reorder')\ndef sparse_reorder(sp_input, name=None):\n    \"\"\"Reorders a `SparseTensor` into the canonical, row-major ordering.\n\n  Note that by convention, all sparse ops preserve the canonical ordering\n  along increasing dimension number. The only time ordering can be violated\n  is during manual manipulation of the indices and values to add entries.\n\n  Reordering does not affect the shape of the `SparseTensor`.\n\n  For example, if `sp_input` has shape `[4, 5]` and `indices` / `values`:\n\n      [0, 3]: b\n      [0, 1]: a\n      [3, 1]: d\n      [2, 0]: c\n\n  then the output will be a `SparseTensor` of shape `[4, 5]` and\n  `indices` / `values`:\n\n      [0, 1]: a\n      [0, 3]: b\n      [2, 0]: c\n      [3, 1]: d\n\n  Args:\n    sp_input: The input `SparseTensor`.\n    name: A name prefix for the returned tensors (optional)\n\n  Returns:\n    A `SparseTensor` with the same shape and non-empty values, but in\n    canonical ordering.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (reordered_ind, reordered_val) = gen_sparse_ops.sparse_reorder(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name)\n    if sp_input.get_shape().is_fully_defined():\n        dense_shape = sp_input.get_shape().as_list()\n        return sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n    else:\n        dense_shape = array_ops.identity(sp_input.dense_shape)\n        sp_output = sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n        sp_output.set_shape(sp_input.shape)\n        return sp_output",
        "mutated": [
            "@tf_export('sparse.reorder', v1=['sparse.reorder', 'sparse_reorder'])\n@deprecation.deprecated_endpoints('sparse_reorder')\ndef sparse_reorder(sp_input, name=None):\n    if False:\n        i = 10\n    'Reorders a `SparseTensor` into the canonical, row-major ordering.\\n\\n  Note that by convention, all sparse ops preserve the canonical ordering\\n  along increasing dimension number. The only time ordering can be violated\\n  is during manual manipulation of the indices and values to add entries.\\n\\n  Reordering does not affect the shape of the `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and `indices` / `values`:\\n\\n      [0, 3]: b\\n      [0, 1]: a\\n      [3, 1]: d\\n      [2, 0]: c\\n\\n  then the output will be a `SparseTensor` of shape `[4, 5]` and\\n  `indices` / `values`:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same shape and non-empty values, but in\\n    canonical ordering.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (reordered_ind, reordered_val) = gen_sparse_ops.sparse_reorder(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name)\n    if sp_input.get_shape().is_fully_defined():\n        dense_shape = sp_input.get_shape().as_list()\n        return sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n    else:\n        dense_shape = array_ops.identity(sp_input.dense_shape)\n        sp_output = sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n        sp_output.set_shape(sp_input.shape)\n        return sp_output",
            "@tf_export('sparse.reorder', v1=['sparse.reorder', 'sparse_reorder'])\n@deprecation.deprecated_endpoints('sparse_reorder')\ndef sparse_reorder(sp_input, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Reorders a `SparseTensor` into the canonical, row-major ordering.\\n\\n  Note that by convention, all sparse ops preserve the canonical ordering\\n  along increasing dimension number. The only time ordering can be violated\\n  is during manual manipulation of the indices and values to add entries.\\n\\n  Reordering does not affect the shape of the `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and `indices` / `values`:\\n\\n      [0, 3]: b\\n      [0, 1]: a\\n      [3, 1]: d\\n      [2, 0]: c\\n\\n  then the output will be a `SparseTensor` of shape `[4, 5]` and\\n  `indices` / `values`:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same shape and non-empty values, but in\\n    canonical ordering.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (reordered_ind, reordered_val) = gen_sparse_ops.sparse_reorder(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name)\n    if sp_input.get_shape().is_fully_defined():\n        dense_shape = sp_input.get_shape().as_list()\n        return sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n    else:\n        dense_shape = array_ops.identity(sp_input.dense_shape)\n        sp_output = sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n        sp_output.set_shape(sp_input.shape)\n        return sp_output",
            "@tf_export('sparse.reorder', v1=['sparse.reorder', 'sparse_reorder'])\n@deprecation.deprecated_endpoints('sparse_reorder')\ndef sparse_reorder(sp_input, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Reorders a `SparseTensor` into the canonical, row-major ordering.\\n\\n  Note that by convention, all sparse ops preserve the canonical ordering\\n  along increasing dimension number. The only time ordering can be violated\\n  is during manual manipulation of the indices and values to add entries.\\n\\n  Reordering does not affect the shape of the `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and `indices` / `values`:\\n\\n      [0, 3]: b\\n      [0, 1]: a\\n      [3, 1]: d\\n      [2, 0]: c\\n\\n  then the output will be a `SparseTensor` of shape `[4, 5]` and\\n  `indices` / `values`:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same shape and non-empty values, but in\\n    canonical ordering.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (reordered_ind, reordered_val) = gen_sparse_ops.sparse_reorder(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name)\n    if sp_input.get_shape().is_fully_defined():\n        dense_shape = sp_input.get_shape().as_list()\n        return sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n    else:\n        dense_shape = array_ops.identity(sp_input.dense_shape)\n        sp_output = sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n        sp_output.set_shape(sp_input.shape)\n        return sp_output",
            "@tf_export('sparse.reorder', v1=['sparse.reorder', 'sparse_reorder'])\n@deprecation.deprecated_endpoints('sparse_reorder')\ndef sparse_reorder(sp_input, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Reorders a `SparseTensor` into the canonical, row-major ordering.\\n\\n  Note that by convention, all sparse ops preserve the canonical ordering\\n  along increasing dimension number. The only time ordering can be violated\\n  is during manual manipulation of the indices and values to add entries.\\n\\n  Reordering does not affect the shape of the `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and `indices` / `values`:\\n\\n      [0, 3]: b\\n      [0, 1]: a\\n      [3, 1]: d\\n      [2, 0]: c\\n\\n  then the output will be a `SparseTensor` of shape `[4, 5]` and\\n  `indices` / `values`:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same shape and non-empty values, but in\\n    canonical ordering.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (reordered_ind, reordered_val) = gen_sparse_ops.sparse_reorder(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name)\n    if sp_input.get_shape().is_fully_defined():\n        dense_shape = sp_input.get_shape().as_list()\n        return sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n    else:\n        dense_shape = array_ops.identity(sp_input.dense_shape)\n        sp_output = sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n        sp_output.set_shape(sp_input.shape)\n        return sp_output",
            "@tf_export('sparse.reorder', v1=['sparse.reorder', 'sparse_reorder'])\n@deprecation.deprecated_endpoints('sparse_reorder')\ndef sparse_reorder(sp_input, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Reorders a `SparseTensor` into the canonical, row-major ordering.\\n\\n  Note that by convention, all sparse ops preserve the canonical ordering\\n  along increasing dimension number. The only time ordering can be violated\\n  is during manual manipulation of the indices and values to add entries.\\n\\n  Reordering does not affect the shape of the `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and `indices` / `values`:\\n\\n      [0, 3]: b\\n      [0, 1]: a\\n      [3, 1]: d\\n      [2, 0]: c\\n\\n  then the output will be a `SparseTensor` of shape `[4, 5]` and\\n  `indices` / `values`:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same shape and non-empty values, but in\\n    canonical ordering.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (reordered_ind, reordered_val) = gen_sparse_ops.sparse_reorder(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name)\n    if sp_input.get_shape().is_fully_defined():\n        dense_shape = sp_input.get_shape().as_list()\n        return sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n    else:\n        dense_shape = array_ops.identity(sp_input.dense_shape)\n        sp_output = sparse_tensor.SparseTensor(reordered_ind, reordered_val, dense_shape)\n        sp_output.set_shape(sp_input.shape)\n        return sp_output"
        ]
    },
    {
        "func_name": "sparse_reshape",
        "original": "@tf_export('sparse.reshape', v1=['sparse.reshape', 'sparse_reshape'])\n@deprecation.deprecated_endpoints('sparse_reshape')\n@dispatch.add_dispatch_support\ndef sparse_reshape(sp_input, shape, name=None):\n    \"\"\"Reshapes a `SparseTensor` to represent values in a new dense shape.\n\n  This operation has the same semantics as `reshape` on the represented dense\n  tensor.  The indices of non-empty values in `sp_input` are recomputed based\n  on the new dense shape, and a new `SparseTensor` is returned containing the\n  new indices and new shape.  The order of non-empty values in `sp_input` is\n  unchanged.\n\n  If one component of `shape` is the special value -1, the size of that\n  dimension is computed so that the total dense size remains constant.  At\n  most one component of `shape` can be -1.  The number of dense elements\n  implied by `shape` must be the same as the number of dense elements\n  originally represented by `sp_input`.\n\n  For example, if `sp_input` has shape `[2, 3, 6]` and `indices` / `values`:\n\n      [0, 0, 0]: a\n      [0, 0, 1]: b\n      [0, 1, 0]: c\n      [1, 0, 0]: d\n      [1, 2, 3]: e\n\n  and `shape` is `[9, -1]`, then the output will be a `SparseTensor` of\n  shape `[9, 4]` and `indices` / `values`:\n\n      [0, 0]: a\n      [0, 1]: b\n      [1, 2]: c\n      [4, 2]: d\n      [8, 1]: e\n\n  Args:\n    sp_input: The input `SparseTensor`.\n    shape: A 1-D (vector) int64 `Tensor` specifying the new dense shape of the\n      represented `SparseTensor`.\n    name: A name prefix for the returned tensors (optional)\n\n  Returns:\n    A `SparseTensor` with the same non-empty values but with indices calculated\n    by the new dense shape.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n    ValueError:  If argument `shape` requests a `SparseTensor` with a different\n      number of elements than `sp_input`.\n    ValueError:  If `shape` has more than one inferred (== -1) dimension.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    shape = math_ops.cast(shape, dtype=dtypes.int64)\n    with ops.name_scope(name, 'SparseReshape', [sp_input]) as name:\n        (reshaped_ind, reshaped_shape) = gen_sparse_ops.sparse_reshape(sp_input.indices, sp_input.dense_shape, shape, name=name)\n        reshaped_shape_const = tensor_util.constant_value_as_shape(shape)\n        reshaped_shape_const = reshaped_shape_const.as_list() if reshaped_shape_const.ndims is not None else None\n        if reshaped_shape_const is not None and sp_input.shape.is_fully_defined():\n            shape_const_by_user = tensor_util.constant_value(shape)\n            if shape_const_by_user is not None:\n                num_implied_by_user = sum((d == -1 for d in shape_const_by_user))\n                if num_implied_by_user > 1:\n                    raise ValueError('At most one dimension can be inferred (-1). Found: %s' % shape_const_by_user)\n            original_reshaped_shape = list(reshaped_shape_const)\n            in_shape_size = np.prod(sp_input.shape.as_list())\n            num_implied = sum((dim is None for dim in reshaped_shape_const))\n            if num_implied == 1 and 0 not in reshaped_shape_const:\n                implied_idx = original_reshaped_shape.index(None)\n                non_implied_idx = original_reshaped_shape[:implied_idx] + original_reshaped_shape[implied_idx + 1:]\n                reshaped_shape_const[implied_idx] = int(in_shape_size // np.prod(non_implied_idx))\n            if num_implied == 0 or (num_implied == 1 and 0 not in reshaped_shape_const):\n                reshaped_size = np.prod(reshaped_shape_const)\n                if reshaped_size != in_shape_size:\n                    raise ValueError('Cannot reshape a tensor with %d elements to shape %s (%d elements).' % (in_shape_size, original_reshaped_shape, reshaped_size))\n                reshaped_shape = constant_op.constant(reshaped_shape_const, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(reshaped_ind, array_ops.identity(sp_input.values), reshaped_shape)",
        "mutated": [
            "@tf_export('sparse.reshape', v1=['sparse.reshape', 'sparse_reshape'])\n@deprecation.deprecated_endpoints('sparse_reshape')\n@dispatch.add_dispatch_support\ndef sparse_reshape(sp_input, shape, name=None):\n    if False:\n        i = 10\n    'Reshapes a `SparseTensor` to represent values in a new dense shape.\\n\\n  This operation has the same semantics as `reshape` on the represented dense\\n  tensor.  The indices of non-empty values in `sp_input` are recomputed based\\n  on the new dense shape, and a new `SparseTensor` is returned containing the\\n  new indices and new shape.  The order of non-empty values in `sp_input` is\\n  unchanged.\\n\\n  If one component of `shape` is the special value -1, the size of that\\n  dimension is computed so that the total dense size remains constant.  At\\n  most one component of `shape` can be -1.  The number of dense elements\\n  implied by `shape` must be the same as the number of dense elements\\n  originally represented by `sp_input`.\\n\\n  For example, if `sp_input` has shape `[2, 3, 6]` and `indices` / `values`:\\n\\n      [0, 0, 0]: a\\n      [0, 0, 1]: b\\n      [0, 1, 0]: c\\n      [1, 0, 0]: d\\n      [1, 2, 3]: e\\n\\n  and `shape` is `[9, -1]`, then the output will be a `SparseTensor` of\\n  shape `[9, 4]` and `indices` / `values`:\\n\\n      [0, 0]: a\\n      [0, 1]: b\\n      [1, 2]: c\\n      [4, 2]: d\\n      [8, 1]: e\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    shape: A 1-D (vector) int64 `Tensor` specifying the new dense shape of the\\n      represented `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same non-empty values but with indices calculated\\n    by the new dense shape.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError:  If argument `shape` requests a `SparseTensor` with a different\\n      number of elements than `sp_input`.\\n    ValueError:  If `shape` has more than one inferred (== -1) dimension.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    shape = math_ops.cast(shape, dtype=dtypes.int64)\n    with ops.name_scope(name, 'SparseReshape', [sp_input]) as name:\n        (reshaped_ind, reshaped_shape) = gen_sparse_ops.sparse_reshape(sp_input.indices, sp_input.dense_shape, shape, name=name)\n        reshaped_shape_const = tensor_util.constant_value_as_shape(shape)\n        reshaped_shape_const = reshaped_shape_const.as_list() if reshaped_shape_const.ndims is not None else None\n        if reshaped_shape_const is not None and sp_input.shape.is_fully_defined():\n            shape_const_by_user = tensor_util.constant_value(shape)\n            if shape_const_by_user is not None:\n                num_implied_by_user = sum((d == -1 for d in shape_const_by_user))\n                if num_implied_by_user > 1:\n                    raise ValueError('At most one dimension can be inferred (-1). Found: %s' % shape_const_by_user)\n            original_reshaped_shape = list(reshaped_shape_const)\n            in_shape_size = np.prod(sp_input.shape.as_list())\n            num_implied = sum((dim is None for dim in reshaped_shape_const))\n            if num_implied == 1 and 0 not in reshaped_shape_const:\n                implied_idx = original_reshaped_shape.index(None)\n                non_implied_idx = original_reshaped_shape[:implied_idx] + original_reshaped_shape[implied_idx + 1:]\n                reshaped_shape_const[implied_idx] = int(in_shape_size // np.prod(non_implied_idx))\n            if num_implied == 0 or (num_implied == 1 and 0 not in reshaped_shape_const):\n                reshaped_size = np.prod(reshaped_shape_const)\n                if reshaped_size != in_shape_size:\n                    raise ValueError('Cannot reshape a tensor with %d elements to shape %s (%d elements).' % (in_shape_size, original_reshaped_shape, reshaped_size))\n                reshaped_shape = constant_op.constant(reshaped_shape_const, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(reshaped_ind, array_ops.identity(sp_input.values), reshaped_shape)",
            "@tf_export('sparse.reshape', v1=['sparse.reshape', 'sparse_reshape'])\n@deprecation.deprecated_endpoints('sparse_reshape')\n@dispatch.add_dispatch_support\ndef sparse_reshape(sp_input, shape, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Reshapes a `SparseTensor` to represent values in a new dense shape.\\n\\n  This operation has the same semantics as `reshape` on the represented dense\\n  tensor.  The indices of non-empty values in `sp_input` are recomputed based\\n  on the new dense shape, and a new `SparseTensor` is returned containing the\\n  new indices and new shape.  The order of non-empty values in `sp_input` is\\n  unchanged.\\n\\n  If one component of `shape` is the special value -1, the size of that\\n  dimension is computed so that the total dense size remains constant.  At\\n  most one component of `shape` can be -1.  The number of dense elements\\n  implied by `shape` must be the same as the number of dense elements\\n  originally represented by `sp_input`.\\n\\n  For example, if `sp_input` has shape `[2, 3, 6]` and `indices` / `values`:\\n\\n      [0, 0, 0]: a\\n      [0, 0, 1]: b\\n      [0, 1, 0]: c\\n      [1, 0, 0]: d\\n      [1, 2, 3]: e\\n\\n  and `shape` is `[9, -1]`, then the output will be a `SparseTensor` of\\n  shape `[9, 4]` and `indices` / `values`:\\n\\n      [0, 0]: a\\n      [0, 1]: b\\n      [1, 2]: c\\n      [4, 2]: d\\n      [8, 1]: e\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    shape: A 1-D (vector) int64 `Tensor` specifying the new dense shape of the\\n      represented `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same non-empty values but with indices calculated\\n    by the new dense shape.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError:  If argument `shape` requests a `SparseTensor` with a different\\n      number of elements than `sp_input`.\\n    ValueError:  If `shape` has more than one inferred (== -1) dimension.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    shape = math_ops.cast(shape, dtype=dtypes.int64)\n    with ops.name_scope(name, 'SparseReshape', [sp_input]) as name:\n        (reshaped_ind, reshaped_shape) = gen_sparse_ops.sparse_reshape(sp_input.indices, sp_input.dense_shape, shape, name=name)\n        reshaped_shape_const = tensor_util.constant_value_as_shape(shape)\n        reshaped_shape_const = reshaped_shape_const.as_list() if reshaped_shape_const.ndims is not None else None\n        if reshaped_shape_const is not None and sp_input.shape.is_fully_defined():\n            shape_const_by_user = tensor_util.constant_value(shape)\n            if shape_const_by_user is not None:\n                num_implied_by_user = sum((d == -1 for d in shape_const_by_user))\n                if num_implied_by_user > 1:\n                    raise ValueError('At most one dimension can be inferred (-1). Found: %s' % shape_const_by_user)\n            original_reshaped_shape = list(reshaped_shape_const)\n            in_shape_size = np.prod(sp_input.shape.as_list())\n            num_implied = sum((dim is None for dim in reshaped_shape_const))\n            if num_implied == 1 and 0 not in reshaped_shape_const:\n                implied_idx = original_reshaped_shape.index(None)\n                non_implied_idx = original_reshaped_shape[:implied_idx] + original_reshaped_shape[implied_idx + 1:]\n                reshaped_shape_const[implied_idx] = int(in_shape_size // np.prod(non_implied_idx))\n            if num_implied == 0 or (num_implied == 1 and 0 not in reshaped_shape_const):\n                reshaped_size = np.prod(reshaped_shape_const)\n                if reshaped_size != in_shape_size:\n                    raise ValueError('Cannot reshape a tensor with %d elements to shape %s (%d elements).' % (in_shape_size, original_reshaped_shape, reshaped_size))\n                reshaped_shape = constant_op.constant(reshaped_shape_const, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(reshaped_ind, array_ops.identity(sp_input.values), reshaped_shape)",
            "@tf_export('sparse.reshape', v1=['sparse.reshape', 'sparse_reshape'])\n@deprecation.deprecated_endpoints('sparse_reshape')\n@dispatch.add_dispatch_support\ndef sparse_reshape(sp_input, shape, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Reshapes a `SparseTensor` to represent values in a new dense shape.\\n\\n  This operation has the same semantics as `reshape` on the represented dense\\n  tensor.  The indices of non-empty values in `sp_input` are recomputed based\\n  on the new dense shape, and a new `SparseTensor` is returned containing the\\n  new indices and new shape.  The order of non-empty values in `sp_input` is\\n  unchanged.\\n\\n  If one component of `shape` is the special value -1, the size of that\\n  dimension is computed so that the total dense size remains constant.  At\\n  most one component of `shape` can be -1.  The number of dense elements\\n  implied by `shape` must be the same as the number of dense elements\\n  originally represented by `sp_input`.\\n\\n  For example, if `sp_input` has shape `[2, 3, 6]` and `indices` / `values`:\\n\\n      [0, 0, 0]: a\\n      [0, 0, 1]: b\\n      [0, 1, 0]: c\\n      [1, 0, 0]: d\\n      [1, 2, 3]: e\\n\\n  and `shape` is `[9, -1]`, then the output will be a `SparseTensor` of\\n  shape `[9, 4]` and `indices` / `values`:\\n\\n      [0, 0]: a\\n      [0, 1]: b\\n      [1, 2]: c\\n      [4, 2]: d\\n      [8, 1]: e\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    shape: A 1-D (vector) int64 `Tensor` specifying the new dense shape of the\\n      represented `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same non-empty values but with indices calculated\\n    by the new dense shape.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError:  If argument `shape` requests a `SparseTensor` with a different\\n      number of elements than `sp_input`.\\n    ValueError:  If `shape` has more than one inferred (== -1) dimension.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    shape = math_ops.cast(shape, dtype=dtypes.int64)\n    with ops.name_scope(name, 'SparseReshape', [sp_input]) as name:\n        (reshaped_ind, reshaped_shape) = gen_sparse_ops.sparse_reshape(sp_input.indices, sp_input.dense_shape, shape, name=name)\n        reshaped_shape_const = tensor_util.constant_value_as_shape(shape)\n        reshaped_shape_const = reshaped_shape_const.as_list() if reshaped_shape_const.ndims is not None else None\n        if reshaped_shape_const is not None and sp_input.shape.is_fully_defined():\n            shape_const_by_user = tensor_util.constant_value(shape)\n            if shape_const_by_user is not None:\n                num_implied_by_user = sum((d == -1 for d in shape_const_by_user))\n                if num_implied_by_user > 1:\n                    raise ValueError('At most one dimension can be inferred (-1). Found: %s' % shape_const_by_user)\n            original_reshaped_shape = list(reshaped_shape_const)\n            in_shape_size = np.prod(sp_input.shape.as_list())\n            num_implied = sum((dim is None for dim in reshaped_shape_const))\n            if num_implied == 1 and 0 not in reshaped_shape_const:\n                implied_idx = original_reshaped_shape.index(None)\n                non_implied_idx = original_reshaped_shape[:implied_idx] + original_reshaped_shape[implied_idx + 1:]\n                reshaped_shape_const[implied_idx] = int(in_shape_size // np.prod(non_implied_idx))\n            if num_implied == 0 or (num_implied == 1 and 0 not in reshaped_shape_const):\n                reshaped_size = np.prod(reshaped_shape_const)\n                if reshaped_size != in_shape_size:\n                    raise ValueError('Cannot reshape a tensor with %d elements to shape %s (%d elements).' % (in_shape_size, original_reshaped_shape, reshaped_size))\n                reshaped_shape = constant_op.constant(reshaped_shape_const, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(reshaped_ind, array_ops.identity(sp_input.values), reshaped_shape)",
            "@tf_export('sparse.reshape', v1=['sparse.reshape', 'sparse_reshape'])\n@deprecation.deprecated_endpoints('sparse_reshape')\n@dispatch.add_dispatch_support\ndef sparse_reshape(sp_input, shape, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Reshapes a `SparseTensor` to represent values in a new dense shape.\\n\\n  This operation has the same semantics as `reshape` on the represented dense\\n  tensor.  The indices of non-empty values in `sp_input` are recomputed based\\n  on the new dense shape, and a new `SparseTensor` is returned containing the\\n  new indices and new shape.  The order of non-empty values in `sp_input` is\\n  unchanged.\\n\\n  If one component of `shape` is the special value -1, the size of that\\n  dimension is computed so that the total dense size remains constant.  At\\n  most one component of `shape` can be -1.  The number of dense elements\\n  implied by `shape` must be the same as the number of dense elements\\n  originally represented by `sp_input`.\\n\\n  For example, if `sp_input` has shape `[2, 3, 6]` and `indices` / `values`:\\n\\n      [0, 0, 0]: a\\n      [0, 0, 1]: b\\n      [0, 1, 0]: c\\n      [1, 0, 0]: d\\n      [1, 2, 3]: e\\n\\n  and `shape` is `[9, -1]`, then the output will be a `SparseTensor` of\\n  shape `[9, 4]` and `indices` / `values`:\\n\\n      [0, 0]: a\\n      [0, 1]: b\\n      [1, 2]: c\\n      [4, 2]: d\\n      [8, 1]: e\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    shape: A 1-D (vector) int64 `Tensor` specifying the new dense shape of the\\n      represented `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same non-empty values but with indices calculated\\n    by the new dense shape.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError:  If argument `shape` requests a `SparseTensor` with a different\\n      number of elements than `sp_input`.\\n    ValueError:  If `shape` has more than one inferred (== -1) dimension.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    shape = math_ops.cast(shape, dtype=dtypes.int64)\n    with ops.name_scope(name, 'SparseReshape', [sp_input]) as name:\n        (reshaped_ind, reshaped_shape) = gen_sparse_ops.sparse_reshape(sp_input.indices, sp_input.dense_shape, shape, name=name)\n        reshaped_shape_const = tensor_util.constant_value_as_shape(shape)\n        reshaped_shape_const = reshaped_shape_const.as_list() if reshaped_shape_const.ndims is not None else None\n        if reshaped_shape_const is not None and sp_input.shape.is_fully_defined():\n            shape_const_by_user = tensor_util.constant_value(shape)\n            if shape_const_by_user is not None:\n                num_implied_by_user = sum((d == -1 for d in shape_const_by_user))\n                if num_implied_by_user > 1:\n                    raise ValueError('At most one dimension can be inferred (-1). Found: %s' % shape_const_by_user)\n            original_reshaped_shape = list(reshaped_shape_const)\n            in_shape_size = np.prod(sp_input.shape.as_list())\n            num_implied = sum((dim is None for dim in reshaped_shape_const))\n            if num_implied == 1 and 0 not in reshaped_shape_const:\n                implied_idx = original_reshaped_shape.index(None)\n                non_implied_idx = original_reshaped_shape[:implied_idx] + original_reshaped_shape[implied_idx + 1:]\n                reshaped_shape_const[implied_idx] = int(in_shape_size // np.prod(non_implied_idx))\n            if num_implied == 0 or (num_implied == 1 and 0 not in reshaped_shape_const):\n                reshaped_size = np.prod(reshaped_shape_const)\n                if reshaped_size != in_shape_size:\n                    raise ValueError('Cannot reshape a tensor with %d elements to shape %s (%d elements).' % (in_shape_size, original_reshaped_shape, reshaped_size))\n                reshaped_shape = constant_op.constant(reshaped_shape_const, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(reshaped_ind, array_ops.identity(sp_input.values), reshaped_shape)",
            "@tf_export('sparse.reshape', v1=['sparse.reshape', 'sparse_reshape'])\n@deprecation.deprecated_endpoints('sparse_reshape')\n@dispatch.add_dispatch_support\ndef sparse_reshape(sp_input, shape, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Reshapes a `SparseTensor` to represent values in a new dense shape.\\n\\n  This operation has the same semantics as `reshape` on the represented dense\\n  tensor.  The indices of non-empty values in `sp_input` are recomputed based\\n  on the new dense shape, and a new `SparseTensor` is returned containing the\\n  new indices and new shape.  The order of non-empty values in `sp_input` is\\n  unchanged.\\n\\n  If one component of `shape` is the special value -1, the size of that\\n  dimension is computed so that the total dense size remains constant.  At\\n  most one component of `shape` can be -1.  The number of dense elements\\n  implied by `shape` must be the same as the number of dense elements\\n  originally represented by `sp_input`.\\n\\n  For example, if `sp_input` has shape `[2, 3, 6]` and `indices` / `values`:\\n\\n      [0, 0, 0]: a\\n      [0, 0, 1]: b\\n      [0, 1, 0]: c\\n      [1, 0, 0]: d\\n      [1, 2, 3]: e\\n\\n  and `shape` is `[9, -1]`, then the output will be a `SparseTensor` of\\n  shape `[9, 4]` and `indices` / `values`:\\n\\n      [0, 0]: a\\n      [0, 1]: b\\n      [1, 2]: c\\n      [4, 2]: d\\n      [8, 1]: e\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    shape: A 1-D (vector) int64 `Tensor` specifying the new dense shape of the\\n      represented `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` with the same non-empty values but with indices calculated\\n    by the new dense shape.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError:  If argument `shape` requests a `SparseTensor` with a different\\n      number of elements than `sp_input`.\\n    ValueError:  If `shape` has more than one inferred (== -1) dimension.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    shape = math_ops.cast(shape, dtype=dtypes.int64)\n    with ops.name_scope(name, 'SparseReshape', [sp_input]) as name:\n        (reshaped_ind, reshaped_shape) = gen_sparse_ops.sparse_reshape(sp_input.indices, sp_input.dense_shape, shape, name=name)\n        reshaped_shape_const = tensor_util.constant_value_as_shape(shape)\n        reshaped_shape_const = reshaped_shape_const.as_list() if reshaped_shape_const.ndims is not None else None\n        if reshaped_shape_const is not None and sp_input.shape.is_fully_defined():\n            shape_const_by_user = tensor_util.constant_value(shape)\n            if shape_const_by_user is not None:\n                num_implied_by_user = sum((d == -1 for d in shape_const_by_user))\n                if num_implied_by_user > 1:\n                    raise ValueError('At most one dimension can be inferred (-1). Found: %s' % shape_const_by_user)\n            original_reshaped_shape = list(reshaped_shape_const)\n            in_shape_size = np.prod(sp_input.shape.as_list())\n            num_implied = sum((dim is None for dim in reshaped_shape_const))\n            if num_implied == 1 and 0 not in reshaped_shape_const:\n                implied_idx = original_reshaped_shape.index(None)\n                non_implied_idx = original_reshaped_shape[:implied_idx] + original_reshaped_shape[implied_idx + 1:]\n                reshaped_shape_const[implied_idx] = int(in_shape_size // np.prod(non_implied_idx))\n            if num_implied == 0 or (num_implied == 1 and 0 not in reshaped_shape_const):\n                reshaped_size = np.prod(reshaped_shape_const)\n                if reshaped_size != in_shape_size:\n                    raise ValueError('Cannot reshape a tensor with %d elements to shape %s (%d elements).' % (in_shape_size, original_reshaped_shape, reshaped_size))\n                reshaped_shape = constant_op.constant(reshaped_shape_const, dtype=dtypes.int64)\n        return sparse_tensor.SparseTensor(reshaped_ind, array_ops.identity(sp_input.values), reshaped_shape)"
        ]
    },
    {
        "func_name": "__repr__",
        "original": "def __repr__(self):\n    return 'KeywordRequired()'",
        "mutated": [
            "def __repr__(self):\n    if False:\n        i = 10\n    return 'KeywordRequired()'",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 'KeywordRequired()'",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 'KeywordRequired()'",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 'KeywordRequired()'",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 'KeywordRequired()'"
        ]
    },
    {
        "func_name": "sparse_split",
        "original": "@tf_export(v1=['sparse.split', 'sparse_split'])\n@deprecation.deprecated_endpoints('sparse_split')\n@deprecation.deprecated_args(None, 'split_dim is deprecated, use axis instead', 'split_dim')\ndef sparse_split(keyword_required=KeywordRequired(), sp_input=None, num_split=None, axis=None, name=None, split_dim=None):\n    \"\"\"Split a `SparseTensor` into `num_split` tensors along `axis`.\n\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\n  dimension. For example, if `axis = 1` and `num_split = 2` and the\n  input is:\n\n      input_tensor = shape = [2, 7]\n      [    a   d e  ]\n      [b c          ]\n\n  Graphically the output tensors are:\n\n      output_tensor[0] =\n      [    a   ]\n      [b c     ]\n\n      output_tensor[1] =\n      [ d e  ]\n      [      ]\n\n  Args:\n    keyword_required: Python 2 standin for * (temporary for argument reorder)\n    sp_input: The `SparseTensor` to split.\n    num_split: A Python integer. The number of ways to split.\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\n      range [-rank, rank), where rank is the number of dimensions in the input\n      `SparseTensor`.\n    name: A name for the operation (optional).\n    split_dim: Deprecated old name for axis.\n\n  Returns:\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n    ValueError: If the deprecated `split_dim` and `axis` are both non None.\n  \"\"\"\n    if not isinstance(keyword_required, KeywordRequired):\n        raise ValueError('Keyword arguments are required for this function.')\n    if sp_input is None:\n        raise ValueError('sp_input is required')\n    if num_split is None:\n        raise ValueError('num_split is required')\n    if axis is None:\n        raise ValueError('axis is required')\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'split_dim', split_dim)\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (output_inds, output_vals, output_shapes) = gen_sparse_ops.sparse_split(axis, sp_input.indices, sp_input.values, sp_input.dense_shape, num_split, name=name)\n    sparse_tensors = []\n    for i in range(0, num_split):\n        sparse_tensors.append(sparse_tensor.SparseTensor(output_inds[i], output_vals[i], output_shapes[i]))\n    return sparse_tensors",
        "mutated": [
            "@tf_export(v1=['sparse.split', 'sparse_split'])\n@deprecation.deprecated_endpoints('sparse_split')\n@deprecation.deprecated_args(None, 'split_dim is deprecated, use axis instead', 'split_dim')\ndef sparse_split(keyword_required=KeywordRequired(), sp_input=None, num_split=None, axis=None, name=None, split_dim=None):\n    if False:\n        i = 10\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example, if `axis = 1` and `num_split = 2` and the\\n  input is:\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      output_tensor[0] =\\n      [    a   ]\\n      [b c     ]\\n\\n      output_tensor[1] =\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    keyword_required: Python 2 standin for * (temporary for argument reorder)\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n    split_dim: Deprecated old name for axis.\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If the deprecated `split_dim` and `axis` are both non None.\\n  '\n    if not isinstance(keyword_required, KeywordRequired):\n        raise ValueError('Keyword arguments are required for this function.')\n    if sp_input is None:\n        raise ValueError('sp_input is required')\n    if num_split is None:\n        raise ValueError('num_split is required')\n    if axis is None:\n        raise ValueError('axis is required')\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'split_dim', split_dim)\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (output_inds, output_vals, output_shapes) = gen_sparse_ops.sparse_split(axis, sp_input.indices, sp_input.values, sp_input.dense_shape, num_split, name=name)\n    sparse_tensors = []\n    for i in range(0, num_split):\n        sparse_tensors.append(sparse_tensor.SparseTensor(output_inds[i], output_vals[i], output_shapes[i]))\n    return sparse_tensors",
            "@tf_export(v1=['sparse.split', 'sparse_split'])\n@deprecation.deprecated_endpoints('sparse_split')\n@deprecation.deprecated_args(None, 'split_dim is deprecated, use axis instead', 'split_dim')\ndef sparse_split(keyword_required=KeywordRequired(), sp_input=None, num_split=None, axis=None, name=None, split_dim=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example, if `axis = 1` and `num_split = 2` and the\\n  input is:\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      output_tensor[0] =\\n      [    a   ]\\n      [b c     ]\\n\\n      output_tensor[1] =\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    keyword_required: Python 2 standin for * (temporary for argument reorder)\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n    split_dim: Deprecated old name for axis.\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If the deprecated `split_dim` and `axis` are both non None.\\n  '\n    if not isinstance(keyword_required, KeywordRequired):\n        raise ValueError('Keyword arguments are required for this function.')\n    if sp_input is None:\n        raise ValueError('sp_input is required')\n    if num_split is None:\n        raise ValueError('num_split is required')\n    if axis is None:\n        raise ValueError('axis is required')\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'split_dim', split_dim)\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (output_inds, output_vals, output_shapes) = gen_sparse_ops.sparse_split(axis, sp_input.indices, sp_input.values, sp_input.dense_shape, num_split, name=name)\n    sparse_tensors = []\n    for i in range(0, num_split):\n        sparse_tensors.append(sparse_tensor.SparseTensor(output_inds[i], output_vals[i], output_shapes[i]))\n    return sparse_tensors",
            "@tf_export(v1=['sparse.split', 'sparse_split'])\n@deprecation.deprecated_endpoints('sparse_split')\n@deprecation.deprecated_args(None, 'split_dim is deprecated, use axis instead', 'split_dim')\ndef sparse_split(keyword_required=KeywordRequired(), sp_input=None, num_split=None, axis=None, name=None, split_dim=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example, if `axis = 1` and `num_split = 2` and the\\n  input is:\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      output_tensor[0] =\\n      [    a   ]\\n      [b c     ]\\n\\n      output_tensor[1] =\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    keyword_required: Python 2 standin for * (temporary for argument reorder)\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n    split_dim: Deprecated old name for axis.\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If the deprecated `split_dim` and `axis` are both non None.\\n  '\n    if not isinstance(keyword_required, KeywordRequired):\n        raise ValueError('Keyword arguments are required for this function.')\n    if sp_input is None:\n        raise ValueError('sp_input is required')\n    if num_split is None:\n        raise ValueError('num_split is required')\n    if axis is None:\n        raise ValueError('axis is required')\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'split_dim', split_dim)\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (output_inds, output_vals, output_shapes) = gen_sparse_ops.sparse_split(axis, sp_input.indices, sp_input.values, sp_input.dense_shape, num_split, name=name)\n    sparse_tensors = []\n    for i in range(0, num_split):\n        sparse_tensors.append(sparse_tensor.SparseTensor(output_inds[i], output_vals[i], output_shapes[i]))\n    return sparse_tensors",
            "@tf_export(v1=['sparse.split', 'sparse_split'])\n@deprecation.deprecated_endpoints('sparse_split')\n@deprecation.deprecated_args(None, 'split_dim is deprecated, use axis instead', 'split_dim')\ndef sparse_split(keyword_required=KeywordRequired(), sp_input=None, num_split=None, axis=None, name=None, split_dim=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example, if `axis = 1` and `num_split = 2` and the\\n  input is:\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      output_tensor[0] =\\n      [    a   ]\\n      [b c     ]\\n\\n      output_tensor[1] =\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    keyword_required: Python 2 standin for * (temporary for argument reorder)\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n    split_dim: Deprecated old name for axis.\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If the deprecated `split_dim` and `axis` are both non None.\\n  '\n    if not isinstance(keyword_required, KeywordRequired):\n        raise ValueError('Keyword arguments are required for this function.')\n    if sp_input is None:\n        raise ValueError('sp_input is required')\n    if num_split is None:\n        raise ValueError('num_split is required')\n    if axis is None:\n        raise ValueError('axis is required')\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'split_dim', split_dim)\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (output_inds, output_vals, output_shapes) = gen_sparse_ops.sparse_split(axis, sp_input.indices, sp_input.values, sp_input.dense_shape, num_split, name=name)\n    sparse_tensors = []\n    for i in range(0, num_split):\n        sparse_tensors.append(sparse_tensor.SparseTensor(output_inds[i], output_vals[i], output_shapes[i]))\n    return sparse_tensors",
            "@tf_export(v1=['sparse.split', 'sparse_split'])\n@deprecation.deprecated_endpoints('sparse_split')\n@deprecation.deprecated_args(None, 'split_dim is deprecated, use axis instead', 'split_dim')\ndef sparse_split(keyword_required=KeywordRequired(), sp_input=None, num_split=None, axis=None, name=None, split_dim=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example, if `axis = 1` and `num_split = 2` and the\\n  input is:\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      output_tensor[0] =\\n      [    a   ]\\n      [b c     ]\\n\\n      output_tensor[1] =\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    keyword_required: Python 2 standin for * (temporary for argument reorder)\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n    split_dim: Deprecated old name for axis.\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If the deprecated `split_dim` and `axis` are both non None.\\n  '\n    if not isinstance(keyword_required, KeywordRequired):\n        raise ValueError('Keyword arguments are required for this function.')\n    if sp_input is None:\n        raise ValueError('sp_input is required')\n    if num_split is None:\n        raise ValueError('num_split is required')\n    if axis is None:\n        raise ValueError('axis is required')\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'split_dim', split_dim)\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    (output_inds, output_vals, output_shapes) = gen_sparse_ops.sparse_split(axis, sp_input.indices, sp_input.values, sp_input.dense_shape, num_split, name=name)\n    sparse_tensors = []\n    for i in range(0, num_split):\n        sparse_tensors.append(sparse_tensor.SparseTensor(output_inds[i], output_vals[i], output_shapes[i]))\n    return sparse_tensors"
        ]
    },
    {
        "func_name": "sparse_split_v2",
        "original": "@tf_export('sparse.split', v1=[])\ndef sparse_split_v2(sp_input=None, num_split=None, axis=None, name=None):\n    \"\"\"Split a `SparseTensor` into `num_split` tensors along `axis`.\n\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\n  dimension. For example:\n\n  >>> indices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]\n  >>> values = [1, 2, 3, 4, 5]\n  >>> t = tf.sparse.SparseTensor(indices=indices, values=values,\n  ...                            dense_shape=[2, 7])\n  >>> tf.sparse.to_dense(t)\n  <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\n  array([[0, 0, 1, 0, 2, 3, 0],\n         [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>\n\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=1)\n  >>> tf.sparse.to_dense(output[0])\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n  array([[0, 0, 1, 0],\n         [4, 5, 0, 0]], dtype=int32)>\n  >>> tf.sparse.to_dense(output[1])\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[2, 3, 0],\n         [0, 0, 0]], dtype=int32)>\n\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=0)\n  >>> tf.sparse.to_dense(output[0])\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],\n  dtype=int32)>\n  >>> tf.sparse.to_dense(output[1])\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],\n  dtype=int32)>\n\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=-1)\n  >>> tf.sparse.to_dense(output[0])\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n  array([[0, 0, 1, 0],\n         [4, 5, 0, 0]], dtype=int32)>\n  >>> tf.sparse.to_dense(output[1])\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[2, 3, 0],\n         [0, 0, 0]], dtype=int32)>\n\n  Args:\n    sp_input: The `SparseTensor` to split.\n    num_split: A Python integer. The number of ways to split.\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\n      range [-rank, rank), where rank is the number of dimensions in the input\n      `SparseTensor`.\n    name: A name for the operation (optional).\n\n  Returns:\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    return sparse_split(sp_input=sp_input, num_split=num_split, axis=axis, name=name, split_dim=None)",
        "mutated": [
            "@tf_export('sparse.split', v1=[])\ndef sparse_split_v2(sp_input=None, num_split=None, axis=None, name=None):\n    if False:\n        i = 10\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example:\\n\\n  >>> indices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]\\n  >>> values = [1, 2, 3, 4, 5]\\n  >>> t = tf.sparse.SparseTensor(indices=indices, values=values,\\n  ...                            dense_shape=[2, 7])\\n  >>> tf.sparse.to_dense(t)\\n  <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\\n  array([[0, 0, 1, 0, 2, 3, 0],\\n         [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=0)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],\\n  dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],\\n  dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=-1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    return sparse_split(sp_input=sp_input, num_split=num_split, axis=axis, name=name, split_dim=None)",
            "@tf_export('sparse.split', v1=[])\ndef sparse_split_v2(sp_input=None, num_split=None, axis=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example:\\n\\n  >>> indices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]\\n  >>> values = [1, 2, 3, 4, 5]\\n  >>> t = tf.sparse.SparseTensor(indices=indices, values=values,\\n  ...                            dense_shape=[2, 7])\\n  >>> tf.sparse.to_dense(t)\\n  <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\\n  array([[0, 0, 1, 0, 2, 3, 0],\\n         [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=0)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],\\n  dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],\\n  dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=-1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    return sparse_split(sp_input=sp_input, num_split=num_split, axis=axis, name=name, split_dim=None)",
            "@tf_export('sparse.split', v1=[])\ndef sparse_split_v2(sp_input=None, num_split=None, axis=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example:\\n\\n  >>> indices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]\\n  >>> values = [1, 2, 3, 4, 5]\\n  >>> t = tf.sparse.SparseTensor(indices=indices, values=values,\\n  ...                            dense_shape=[2, 7])\\n  >>> tf.sparse.to_dense(t)\\n  <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\\n  array([[0, 0, 1, 0, 2, 3, 0],\\n         [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=0)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],\\n  dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],\\n  dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=-1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    return sparse_split(sp_input=sp_input, num_split=num_split, axis=axis, name=name, split_dim=None)",
            "@tf_export('sparse.split', v1=[])\ndef sparse_split_v2(sp_input=None, num_split=None, axis=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example:\\n\\n  >>> indices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]\\n  >>> values = [1, 2, 3, 4, 5]\\n  >>> t = tf.sparse.SparseTensor(indices=indices, values=values,\\n  ...                            dense_shape=[2, 7])\\n  >>> tf.sparse.to_dense(t)\\n  <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\\n  array([[0, 0, 1, 0, 2, 3, 0],\\n         [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=0)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],\\n  dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],\\n  dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=-1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    return sparse_split(sp_input=sp_input, num_split=num_split, axis=axis, name=name, split_dim=None)",
            "@tf_export('sparse.split', v1=[])\ndef sparse_split_v2(sp_input=None, num_split=None, axis=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Split a `SparseTensor` into `num_split` tensors along `axis`.\\n\\n  If the `sp_input.dense_shape[axis]` is not an integer multiple of `num_split`\\n  each slice starting from 0:`shape[axis] % num_split` gets extra one\\n  dimension. For example:\\n\\n  >>> indices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]\\n  >>> values = [1, 2, 3, 4, 5]\\n  >>> t = tf.sparse.SparseTensor(indices=indices, values=values,\\n  ...                            dense_shape=[2, 7])\\n  >>> tf.sparse.to_dense(t)\\n  <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\\n  array([[0, 0, 1, 0, 2, 3, 0],\\n         [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=0)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],\\n  dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],\\n  dtype=int32)>\\n\\n  >>> output = tf.sparse.split(sp_input=t, num_split=2, axis=-1)\\n  >>> tf.sparse.to_dense(output[0])\\n  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\\n  array([[0, 0, 1, 0],\\n         [4, 5, 0, 0]], dtype=int32)>\\n  >>> tf.sparse.to_dense(output[1])\\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\\n  array([[2, 3, 0],\\n         [0, 0, 0]], dtype=int32)>\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    num_split: A Python integer. The number of ways to split.\\n    axis: A 0-D `int32` `Tensor`. The dimension along which to split. Must be in\\n      range [-rank, rank), where rank is the number of dimensions in the input\\n      `SparseTensor`.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    `num_split` `SparseTensor` objects resulting from splitting `value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    return sparse_split(sp_input=sp_input, num_split=num_split, axis=axis, name=name, split_dim=None)"
        ]
    },
    {
        "func_name": "sparse_slice",
        "original": "@tf_export('sparse.slice', v1=['sparse.slice', 'sparse_slice'])\n@deprecation.deprecated_endpoints('sparse_slice')\ndef sparse_slice(sp_input, start, size, name=None):\n    \"\"\"Slice a `SparseTensor` based on the `start` and `size`.\n\n  For example, if the input is\n\n      input_tensor = shape = [2, 7]\n      [    a   d e  ]\n      [b c          ]\n\n  Graphically the output tensors are:\n\n      sparse.slice([0, 0], [2, 4]) = shape = [2, 4]\n      [    a  ]\n      [b c    ]\n\n      sparse.slice([0, 4], [2, 3]) = shape = [2, 3]\n      [ d e  ]\n      [      ]\n\n  Args:\n    sp_input: The `SparseTensor` to split.\n    start: 1-D. tensor represents the start of the slice.\n    size: 1-D. tensor represents the size of the slice.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `SparseTensor` objects resulting from splicing.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    start = ops.convert_to_tensor(start, dtypes.int64)\n    size = ops.convert_to_tensor(size, dtypes.int64)\n    with ops.name_scope(name, 'SparseSlice', [sp_input]) as name:\n        (output_indices, output_values, output_shape) = gen_sparse_ops.sparse_slice(sp_input.indices, sp_input.values, sp_input.dense_shape, start, size, name=name)\n        return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
        "mutated": [
            "@tf_export('sparse.slice', v1=['sparse.slice', 'sparse_slice'])\n@deprecation.deprecated_endpoints('sparse_slice')\ndef sparse_slice(sp_input, start, size, name=None):\n    if False:\n        i = 10\n    'Slice a `SparseTensor` based on the `start` and `size`.\\n\\n  For example, if the input is\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      sparse.slice([0, 0], [2, 4]) = shape = [2, 4]\\n      [    a  ]\\n      [b c    ]\\n\\n      sparse.slice([0, 4], [2, 3]) = shape = [2, 3]\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    start: 1-D. tensor represents the start of the slice.\\n    size: 1-D. tensor represents the size of the slice.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A `SparseTensor` objects resulting from splicing.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    start = ops.convert_to_tensor(start, dtypes.int64)\n    size = ops.convert_to_tensor(size, dtypes.int64)\n    with ops.name_scope(name, 'SparseSlice', [sp_input]) as name:\n        (output_indices, output_values, output_shape) = gen_sparse_ops.sparse_slice(sp_input.indices, sp_input.values, sp_input.dense_shape, start, size, name=name)\n        return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "@tf_export('sparse.slice', v1=['sparse.slice', 'sparse_slice'])\n@deprecation.deprecated_endpoints('sparse_slice')\ndef sparse_slice(sp_input, start, size, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Slice a `SparseTensor` based on the `start` and `size`.\\n\\n  For example, if the input is\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      sparse.slice([0, 0], [2, 4]) = shape = [2, 4]\\n      [    a  ]\\n      [b c    ]\\n\\n      sparse.slice([0, 4], [2, 3]) = shape = [2, 3]\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    start: 1-D. tensor represents the start of the slice.\\n    size: 1-D. tensor represents the size of the slice.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A `SparseTensor` objects resulting from splicing.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    start = ops.convert_to_tensor(start, dtypes.int64)\n    size = ops.convert_to_tensor(size, dtypes.int64)\n    with ops.name_scope(name, 'SparseSlice', [sp_input]) as name:\n        (output_indices, output_values, output_shape) = gen_sparse_ops.sparse_slice(sp_input.indices, sp_input.values, sp_input.dense_shape, start, size, name=name)\n        return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "@tf_export('sparse.slice', v1=['sparse.slice', 'sparse_slice'])\n@deprecation.deprecated_endpoints('sparse_slice')\ndef sparse_slice(sp_input, start, size, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Slice a `SparseTensor` based on the `start` and `size`.\\n\\n  For example, if the input is\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      sparse.slice([0, 0], [2, 4]) = shape = [2, 4]\\n      [    a  ]\\n      [b c    ]\\n\\n      sparse.slice([0, 4], [2, 3]) = shape = [2, 3]\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    start: 1-D. tensor represents the start of the slice.\\n    size: 1-D. tensor represents the size of the slice.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A `SparseTensor` objects resulting from splicing.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    start = ops.convert_to_tensor(start, dtypes.int64)\n    size = ops.convert_to_tensor(size, dtypes.int64)\n    with ops.name_scope(name, 'SparseSlice', [sp_input]) as name:\n        (output_indices, output_values, output_shape) = gen_sparse_ops.sparse_slice(sp_input.indices, sp_input.values, sp_input.dense_shape, start, size, name=name)\n        return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "@tf_export('sparse.slice', v1=['sparse.slice', 'sparse_slice'])\n@deprecation.deprecated_endpoints('sparse_slice')\ndef sparse_slice(sp_input, start, size, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Slice a `SparseTensor` based on the `start` and `size`.\\n\\n  For example, if the input is\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      sparse.slice([0, 0], [2, 4]) = shape = [2, 4]\\n      [    a  ]\\n      [b c    ]\\n\\n      sparse.slice([0, 4], [2, 3]) = shape = [2, 3]\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    start: 1-D. tensor represents the start of the slice.\\n    size: 1-D. tensor represents the size of the slice.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A `SparseTensor` objects resulting from splicing.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    start = ops.convert_to_tensor(start, dtypes.int64)\n    size = ops.convert_to_tensor(size, dtypes.int64)\n    with ops.name_scope(name, 'SparseSlice', [sp_input]) as name:\n        (output_indices, output_values, output_shape) = gen_sparse_ops.sparse_slice(sp_input.indices, sp_input.values, sp_input.dense_shape, start, size, name=name)\n        return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "@tf_export('sparse.slice', v1=['sparse.slice', 'sparse_slice'])\n@deprecation.deprecated_endpoints('sparse_slice')\ndef sparse_slice(sp_input, start, size, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Slice a `SparseTensor` based on the `start` and `size`.\\n\\n  For example, if the input is\\n\\n      input_tensor = shape = [2, 7]\\n      [    a   d e  ]\\n      [b c          ]\\n\\n  Graphically the output tensors are:\\n\\n      sparse.slice([0, 0], [2, 4]) = shape = [2, 4]\\n      [    a  ]\\n      [b c    ]\\n\\n      sparse.slice([0, 4], [2, 3]) = shape = [2, 3]\\n      [ d e  ]\\n      [      ]\\n\\n  Args:\\n    sp_input: The `SparseTensor` to split.\\n    start: 1-D. tensor represents the start of the slice.\\n    size: 1-D. tensor represents the size of the slice.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A `SparseTensor` objects resulting from splicing.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    start = ops.convert_to_tensor(start, dtypes.int64)\n    size = ops.convert_to_tensor(size, dtypes.int64)\n    with ops.name_scope(name, 'SparseSlice', [sp_input]) as name:\n        (output_indices, output_values, output_shape) = gen_sparse_ops.sparse_slice(sp_input.indices, sp_input.values, sp_input.dense_shape, start, size, name=name)\n        return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)"
        ]
    },
    {
        "func_name": "sparse_to_dense",
        "original": "@tf_export(v1=['sparse_to_dense'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated(None, 'Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.')\ndef sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0, validate_indices=True, name=None):\n    \"\"\"Converts a sparse representation into a dense tensor.\n\n  Builds an array `dense` with shape `output_shape` such that\n\n  ```python\n  # If sparse_indices is scalar\n  dense[i] = (i == sparse_indices ? sparse_values : default_value)\n\n  # If sparse_indices is a vector, then for each i\n  dense[sparse_indices[i]] = sparse_values[i]\n\n  # If sparse_indices is an n by d matrix, then for each i in [0, n)\n  dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\n  ```\n\n  All other values in `dense` are set to `default_value`.  If `sparse_values`\n  is a scalar, all sparse indices are set to this single value.\n\n  Indices should be sorted in lexicographic order, and indices must not\n  contain any repeats. If `validate_indices` is True, these properties\n  are checked during execution.\n\n  Args:\n    sparse_indices: A 0-D, 1-D, or 2-D `Tensor` of type `int32` or `int64`.\n      `sparse_indices[i]` contains the complete index where `sparse_values[i]`\n      will be placed.\n    output_shape: A 1-D `Tensor` of the same type as `sparse_indices`.  Shape\n      of the dense output tensor.\n    sparse_values: A 0-D or 1-D `Tensor`.  Values corresponding to each row of\n      `sparse_indices`, or a scalar value to be used for all sparse indices.\n    default_value: A 0-D `Tensor` of the same type as `sparse_values`.  Value\n      to set for indices not specified in `sparse_indices`.  Defaults to zero.\n    validate_indices: A boolean value.  If True, indices are checked to make\n      sure they are sorted in lexicographic order and that there are no repeats.\n    name: A name for the operation (optional).\n\n  Returns:\n    Dense `Tensor` of shape `output_shape`.  Has the same type as\n    `sparse_values`.\n  \"\"\"\n    return gen_sparse_ops.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=default_value, validate_indices=validate_indices, name=name)",
        "mutated": [
            "@tf_export(v1=['sparse_to_dense'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated(None, 'Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.')\ndef sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0, validate_indices=True, name=None):\n    if False:\n        i = 10\n    'Converts a sparse representation into a dense tensor.\\n\\n  Builds an array `dense` with shape `output_shape` such that\\n\\n  ```python\\n  # If sparse_indices is scalar\\n  dense[i] = (i == sparse_indices ? sparse_values : default_value)\\n\\n  # If sparse_indices is a vector, then for each i\\n  dense[sparse_indices[i]] = sparse_values[i]\\n\\n  # If sparse_indices is an n by d matrix, then for each i in [0, n)\\n  dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\\n  ```\\n\\n  All other values in `dense` are set to `default_value`.  If `sparse_values`\\n  is a scalar, all sparse indices are set to this single value.\\n\\n  Indices should be sorted in lexicographic order, and indices must not\\n  contain any repeats. If `validate_indices` is True, these properties\\n  are checked during execution.\\n\\n  Args:\\n    sparse_indices: A 0-D, 1-D, or 2-D `Tensor` of type `int32` or `int64`.\\n      `sparse_indices[i]` contains the complete index where `sparse_values[i]`\\n      will be placed.\\n    output_shape: A 1-D `Tensor` of the same type as `sparse_indices`.  Shape\\n      of the dense output tensor.\\n    sparse_values: A 0-D or 1-D `Tensor`.  Values corresponding to each row of\\n      `sparse_indices`, or a scalar value to be used for all sparse indices.\\n    default_value: A 0-D `Tensor` of the same type as `sparse_values`.  Value\\n      to set for indices not specified in `sparse_indices`.  Defaults to zero.\\n    validate_indices: A boolean value.  If True, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    Dense `Tensor` of shape `output_shape`.  Has the same type as\\n    `sparse_values`.\\n  '\n    return gen_sparse_ops.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=default_value, validate_indices=validate_indices, name=name)",
            "@tf_export(v1=['sparse_to_dense'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated(None, 'Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.')\ndef sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0, validate_indices=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Converts a sparse representation into a dense tensor.\\n\\n  Builds an array `dense` with shape `output_shape` such that\\n\\n  ```python\\n  # If sparse_indices is scalar\\n  dense[i] = (i == sparse_indices ? sparse_values : default_value)\\n\\n  # If sparse_indices is a vector, then for each i\\n  dense[sparse_indices[i]] = sparse_values[i]\\n\\n  # If sparse_indices is an n by d matrix, then for each i in [0, n)\\n  dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\\n  ```\\n\\n  All other values in `dense` are set to `default_value`.  If `sparse_values`\\n  is a scalar, all sparse indices are set to this single value.\\n\\n  Indices should be sorted in lexicographic order, and indices must not\\n  contain any repeats. If `validate_indices` is True, these properties\\n  are checked during execution.\\n\\n  Args:\\n    sparse_indices: A 0-D, 1-D, or 2-D `Tensor` of type `int32` or `int64`.\\n      `sparse_indices[i]` contains the complete index where `sparse_values[i]`\\n      will be placed.\\n    output_shape: A 1-D `Tensor` of the same type as `sparse_indices`.  Shape\\n      of the dense output tensor.\\n    sparse_values: A 0-D or 1-D `Tensor`.  Values corresponding to each row of\\n      `sparse_indices`, or a scalar value to be used for all sparse indices.\\n    default_value: A 0-D `Tensor` of the same type as `sparse_values`.  Value\\n      to set for indices not specified in `sparse_indices`.  Defaults to zero.\\n    validate_indices: A boolean value.  If True, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    Dense `Tensor` of shape `output_shape`.  Has the same type as\\n    `sparse_values`.\\n  '\n    return gen_sparse_ops.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=default_value, validate_indices=validate_indices, name=name)",
            "@tf_export(v1=['sparse_to_dense'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated(None, 'Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.')\ndef sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0, validate_indices=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Converts a sparse representation into a dense tensor.\\n\\n  Builds an array `dense` with shape `output_shape` such that\\n\\n  ```python\\n  # If sparse_indices is scalar\\n  dense[i] = (i == sparse_indices ? sparse_values : default_value)\\n\\n  # If sparse_indices is a vector, then for each i\\n  dense[sparse_indices[i]] = sparse_values[i]\\n\\n  # If sparse_indices is an n by d matrix, then for each i in [0, n)\\n  dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\\n  ```\\n\\n  All other values in `dense` are set to `default_value`.  If `sparse_values`\\n  is a scalar, all sparse indices are set to this single value.\\n\\n  Indices should be sorted in lexicographic order, and indices must not\\n  contain any repeats. If `validate_indices` is True, these properties\\n  are checked during execution.\\n\\n  Args:\\n    sparse_indices: A 0-D, 1-D, or 2-D `Tensor` of type `int32` or `int64`.\\n      `sparse_indices[i]` contains the complete index where `sparse_values[i]`\\n      will be placed.\\n    output_shape: A 1-D `Tensor` of the same type as `sparse_indices`.  Shape\\n      of the dense output tensor.\\n    sparse_values: A 0-D or 1-D `Tensor`.  Values corresponding to each row of\\n      `sparse_indices`, or a scalar value to be used for all sparse indices.\\n    default_value: A 0-D `Tensor` of the same type as `sparse_values`.  Value\\n      to set for indices not specified in `sparse_indices`.  Defaults to zero.\\n    validate_indices: A boolean value.  If True, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    Dense `Tensor` of shape `output_shape`.  Has the same type as\\n    `sparse_values`.\\n  '\n    return gen_sparse_ops.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=default_value, validate_indices=validate_indices, name=name)",
            "@tf_export(v1=['sparse_to_dense'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated(None, 'Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.')\ndef sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0, validate_indices=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Converts a sparse representation into a dense tensor.\\n\\n  Builds an array `dense` with shape `output_shape` such that\\n\\n  ```python\\n  # If sparse_indices is scalar\\n  dense[i] = (i == sparse_indices ? sparse_values : default_value)\\n\\n  # If sparse_indices is a vector, then for each i\\n  dense[sparse_indices[i]] = sparse_values[i]\\n\\n  # If sparse_indices is an n by d matrix, then for each i in [0, n)\\n  dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\\n  ```\\n\\n  All other values in `dense` are set to `default_value`.  If `sparse_values`\\n  is a scalar, all sparse indices are set to this single value.\\n\\n  Indices should be sorted in lexicographic order, and indices must not\\n  contain any repeats. If `validate_indices` is True, these properties\\n  are checked during execution.\\n\\n  Args:\\n    sparse_indices: A 0-D, 1-D, or 2-D `Tensor` of type `int32` or `int64`.\\n      `sparse_indices[i]` contains the complete index where `sparse_values[i]`\\n      will be placed.\\n    output_shape: A 1-D `Tensor` of the same type as `sparse_indices`.  Shape\\n      of the dense output tensor.\\n    sparse_values: A 0-D or 1-D `Tensor`.  Values corresponding to each row of\\n      `sparse_indices`, or a scalar value to be used for all sparse indices.\\n    default_value: A 0-D `Tensor` of the same type as `sparse_values`.  Value\\n      to set for indices not specified in `sparse_indices`.  Defaults to zero.\\n    validate_indices: A boolean value.  If True, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    Dense `Tensor` of shape `output_shape`.  Has the same type as\\n    `sparse_values`.\\n  '\n    return gen_sparse_ops.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=default_value, validate_indices=validate_indices, name=name)",
            "@tf_export(v1=['sparse_to_dense'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated(None, 'Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.')\ndef sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0, validate_indices=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Converts a sparse representation into a dense tensor.\\n\\n  Builds an array `dense` with shape `output_shape` such that\\n\\n  ```python\\n  # If sparse_indices is scalar\\n  dense[i] = (i == sparse_indices ? sparse_values : default_value)\\n\\n  # If sparse_indices is a vector, then for each i\\n  dense[sparse_indices[i]] = sparse_values[i]\\n\\n  # If sparse_indices is an n by d matrix, then for each i in [0, n)\\n  dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\\n  ```\\n\\n  All other values in `dense` are set to `default_value`.  If `sparse_values`\\n  is a scalar, all sparse indices are set to this single value.\\n\\n  Indices should be sorted in lexicographic order, and indices must not\\n  contain any repeats. If `validate_indices` is True, these properties\\n  are checked during execution.\\n\\n  Args:\\n    sparse_indices: A 0-D, 1-D, or 2-D `Tensor` of type `int32` or `int64`.\\n      `sparse_indices[i]` contains the complete index where `sparse_values[i]`\\n      will be placed.\\n    output_shape: A 1-D `Tensor` of the same type as `sparse_indices`.  Shape\\n      of the dense output tensor.\\n    sparse_values: A 0-D or 1-D `Tensor`.  Values corresponding to each row of\\n      `sparse_indices`, or a scalar value to be used for all sparse indices.\\n    default_value: A 0-D `Tensor` of the same type as `sparse_values`.  Value\\n      to set for indices not specified in `sparse_indices`.  Defaults to zero.\\n    validate_indices: A boolean value.  If True, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    Dense `Tensor` of shape `output_shape`.  Has the same type as\\n    `sparse_values`.\\n  '\n    return gen_sparse_ops.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=default_value, validate_indices=validate_indices, name=name)"
        ]
    },
    {
        "func_name": "sparse_reduce_max_v2",
        "original": "@tf_export('sparse.reduce_max', v1=[])\ndef sparse_reduce_max_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    \"\"\"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\n\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\n\n  This Op takes a SparseTensor and is the sparse counterpart to\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\n  is `True`.\n\n  Note: A gradient is not defined for this function, so it can't be used\n  in training models that need gradient descent.\n\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keepdims` is true, the reduced dimensions are retained\n  with length 1.\n\n  If `axis` has no entries, all dimensions are reduced, and a tensor\n  with a single element is returned.  Additionally, the axes can be negative,\n  similar to the indexing rules in Python.\n\n  The values not defined in `sp_input` don't participate in the reduce max,\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\n  for sparse `axis`. But, in case there are no values in\n  `axis`, it will reduce to 0. See second example below.\n\n  For example:\n\n    # 'x' represents [[1, ?, 2]\n    #                 [?, 3, ?]]\n    # where ? is implicitly-zero.\n\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\n    >>> tf.sparse.reduce_max(x)\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\n    >>> tf.sparse.reduce_max(x, 0)\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\n    >>> tf.sparse.reduce_max(x, 1)\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n    array([[2],\n           [3]], dtype=int32)>\n    >>> tf.sparse.reduce_max(x, [0, 1])\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\n\n    # 'y' represents [[-7, ?]\n    #                 [ 4, 3]\n    #                 [ ?, ?]\n\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\n    ... [3, 2])\n    >>> tf.sparse.reduce_max(y, 1)\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\n\n  Args:\n    sp_input: The SparseTensor to reduce. Should have numeric type.\n    axis: The dimensions to reduce; list or scalar. If `None` (the\n      default), reduces all dimensions.\n    keepdims: If true, retain reduced dimensions with length 1.\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\n      `Tensor` (the default).\n    name: A name for the operation (optional).\n\n  Returns:\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\n    True.\n  \"\"\"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
        "mutated": [
            "@tf_export('sparse.reduce_max', v1=[])\ndef sparse_reduce_max_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `axis`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `axis`. But, in case there are no values in\\n  `axis`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
            "@tf_export('sparse.reduce_max', v1=[])\ndef sparse_reduce_max_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `axis`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `axis`. But, in case there are no values in\\n  `axis`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
            "@tf_export('sparse.reduce_max', v1=[])\ndef sparse_reduce_max_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `axis`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `axis`. But, in case there are no values in\\n  `axis`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
            "@tf_export('sparse.reduce_max', v1=[])\ndef sparse_reduce_max_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `axis`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `axis`. But, in case there are no values in\\n  `axis`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
            "@tf_export('sparse.reduce_max', v1=[])\ndef sparse_reduce_max_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `axis`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `axis`. But, in case there are no values in\\n  `axis`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)"
        ]
    },
    {
        "func_name": "sparse_reduce_max",
        "original": "@tf_export(v1=['sparse.reduce_max', 'sparse_reduce_max'])\n@deprecation.deprecated_endpoints('sparse_reduce_max')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_max(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    \"\"\"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\n\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\n\n  This Op takes a SparseTensor and is the sparse counterpart to\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\n  instead of a sparse one.\n\n  Note: A gradient is not defined for this function, so it can't be used\n  in training models that need gradient descent.\n\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\n  with length 1.\n\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\n  with a single element is returned.  Additionally, the axes can be negative,\n  similar to the indexing rules in Python.\n\n  The values not defined in `sp_input` don't participate in the reduce max,\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\n  for sparse `reduction_axes`. But, in case there are no values in\n  `reduction_axes`, it will reduce to 0. See second example below.\n\n  For example:\n\n    # 'x' represents [[1, ?, 2]\n    #                 [?, 3, ?]]\n    # where ? is implicitly-zero.\n\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\n    >>> tf.sparse.reduce_max(x)\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\n    >>> tf.sparse.reduce_max(x, 0)\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\n    >>> tf.sparse.reduce_max(x, 1)\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n    array([[2],\n           [3]], dtype=int32)>\n    >>> tf.sparse.reduce_max(x, [0, 1])\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\n\n    # 'y' represents [[-7, ?]\n    #                 [ 4, 3]\n    #                 [ ?, ?]\n\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\n    ... [3, 2])\n    >>> tf.sparse.reduce_max(y, 1)\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\n\n  Args:\n    sp_input: The SparseTensor to reduce. Should have numeric type.\n    axis: The dimensions to reduce; list or scalar. If `None` (the\n      default), reduces all dimensions.\n    keepdims: If true, retain reduced dimensions with length 1.\n    reduction_axes: Deprecated name of `axis`.\n    keep_dims:  Deprecated alias for `keepdims`.\n\n  Returns:\n    The reduced Tensor.\n  \"\"\"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
        "mutated": [
            "@tf_export(v1=['sparse.reduce_max', 'sparse_reduce_max'])\n@deprecation.deprecated_endpoints('sparse_reduce_max')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_max(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `reduction_axes`. But, in case there are no values in\\n  `reduction_axes`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims:  Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
            "@tf_export(v1=['sparse.reduce_max', 'sparse_reduce_max'])\n@deprecation.deprecated_endpoints('sparse_reduce_max')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_max(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `reduction_axes`. But, in case there are no values in\\n  `reduction_axes`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims:  Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
            "@tf_export(v1=['sparse.reduce_max', 'sparse_reduce_max'])\n@deprecation.deprecated_endpoints('sparse_reduce_max')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_max(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `reduction_axes`. But, in case there are no values in\\n  `reduction_axes`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims:  Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
            "@tf_export(v1=['sparse.reduce_max', 'sparse_reduce_max'])\n@deprecation.deprecated_endpoints('sparse_reduce_max')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_max(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `reduction_axes`. But, in case there are no values in\\n  `reduction_axes`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims:  Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
            "@tf_export(v1=['sparse.reduce_max', 'sparse_reduce_max'])\n@deprecation.deprecated_endpoints('sparse_reduce_max')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_max(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes `tf.sparse.maximum` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.maximum` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  The values not defined in `sp_input` don't participate in the reduce max,\\n  as opposed to be implicitly assumed 0 -- hence it can return negative values\\n  for sparse `reduction_axes`. But, in case there are no values in\\n  `reduction_axes`, it will reduce to 0. See second example below.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 2]\\n    #                 [?, 3, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])\\n    >>> tf.sparse.reduce_max(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_max(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1)\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [3]], dtype=int32)>\\n    >>> tf.sparse.reduce_max(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n    # 'y' represents [[-7, ?]\\n    #                 [ 4, 3]\\n    #                 [ ?, ?]\\n\\n    >>> y = tf.sparse.SparseTensor([[0, 0,], [1, 0], [1, 1]], [-7, 4, 3],\\n    ... [3, 2])\\n    >>> tf.sparse.reduce_max(y, 1)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([-7,  4,  0], dtype=int32)>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims:  Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_max(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)"
        ]
    },
    {
        "func_name": "sparse_reduce_max_sparse",
        "original": "@tf_export(v1=['sparse.reduce_max_sparse', 'sparse_reduce_max_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_max_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_max_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    \"\"\"Computes the max of elements across dimensions of a SparseTensor.\n\n  This Op takes a SparseTensor and is the sparse counterpart to\n  `tf.reduce_max()`.  In contrast to SparseReduceSum, this Op returns a\n  SparseTensor.\n\n  Note: A gradient is not defined for this function, so it can't be used\n  in training models that need gradient descent.\n\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\n  with length 1.\n\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\n  with a single element is returned.  Additionally, the axes can be negative,\n  which are interpreted according to the indexing rules in Python.\n\n  Args:\n    sp_input: The SparseTensor to reduce. Should have numeric type.\n    axis: The dimensions to reduce; list or scalar. If `None` (the\n      default), reduces all dimensions.\n    keepdims: If true, retain reduced dimensions with length 1.\n    reduction_axes: Deprecated name of axis.\n    keep_dims: Deprecated alias for `keepdims`.\n\n  Returns:\n    The reduced SparseTensor.\n  \"\"\"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
        "mutated": [
            "@tf_export(v1=['sparse.reduce_max_sparse', 'sparse_reduce_max_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_max_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_max_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n    \"Computes the max of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
            "@tf_export(v1=['sparse.reduce_max_sparse', 'sparse_reduce_max_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_max_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_max_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes the max of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
            "@tf_export(v1=['sparse.reduce_max_sparse', 'sparse_reduce_max_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_max_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_max_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes the max of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
            "@tf_export(v1=['sparse.reduce_max_sparse', 'sparse_reduce_max_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_max_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_max_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes the max of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
            "@tf_export(v1=['sparse.reduce_max_sparse', 'sparse_reduce_max_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_max_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_max_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes the max of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_max()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_max_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)"
        ]
    },
    {
        "func_name": "sparse_reduce_sum_v2",
        "original": "@tf_export('sparse.reduce_sum', v1=[])\ndef sparse_reduce_sum_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    \"\"\"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\n\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\n\n  This Op takes a SparseTensor and is the sparse counterpart to\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\n  is `True`.\n\n  Note: if `output_is_sparse` is True, a gradient is not defined for this\n  function, so it can't be used in training models that need gradient descent.\n\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless `keepdims` is\n  true, the rank of the tensor is reduced by 1 for each entry in `axis`. If\n  `keepdims` is true, the reduced dimensions are retained with length 1.\n\n  If `axis` has no entries, all dimensions are reduced, and a tensor\n  with a single element is returned.  Additionally, the axes can be negative,\n  similar to the indexing rules in Python.\n\n  For example:\n\n    # 'x' represents [[1, ?, 1]\n    #                 [?, 1, ?]]\n    # where ? is implicitly-zero.\n\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\n    >>> tf.sparse.reduce_sum(x)\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\n    >>> tf.sparse.reduce_sum(x, 0)\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n    array([[2],\n           [1]], dtype=int32)>\n    >>> tf.sparse.reduce_sum(x, [0, 1])\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\n\n  Args:\n    sp_input: The SparseTensor to reduce. Should have numeric type.\n    axis: The dimensions to reduce; list or scalar. If `None` (the\n      default), reduces all dimensions.\n    keepdims: If true, retain reduced dimensions with length 1.\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\n      `Tensor` (the default).\n    name: A name for the operation (optional).\n\n  Returns:\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\n    True.\n  \"\"\"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
        "mutated": [
            "@tf_export('sparse.reduce_sum', v1=[])\ndef sparse_reduce_sum_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: if `output_is_sparse` is True, a gradient is not defined for this\\n  function, so it can't be used in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless `keepdims` is\\n  true, the rank of the tensor is reduced by 1 for each entry in `axis`. If\\n  `keepdims` is true, the reduced dimensions are retained with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
            "@tf_export('sparse.reduce_sum', v1=[])\ndef sparse_reduce_sum_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: if `output_is_sparse` is True, a gradient is not defined for this\\n  function, so it can't be used in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless `keepdims` is\\n  true, the rank of the tensor is reduced by 1 for each entry in `axis`. If\\n  `keepdims` is true, the reduced dimensions are retained with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
            "@tf_export('sparse.reduce_sum', v1=[])\ndef sparse_reduce_sum_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: if `output_is_sparse` is True, a gradient is not defined for this\\n  function, so it can't be used in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless `keepdims` is\\n  true, the rank of the tensor is reduced by 1 for each entry in `axis`. If\\n  `keepdims` is true, the reduced dimensions are retained with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
            "@tf_export('sparse.reduce_sum', v1=[])\ndef sparse_reduce_sum_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: if `output_is_sparse` is True, a gradient is not defined for this\\n  function, so it can't be used in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless `keepdims` is\\n  true, the rank of the tensor is reduced by 1 for each entry in `axis`. If\\n  `keepdims` is true, the reduced dimensions are retained with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)",
            "@tf_export('sparse.reduce_sum', v1=[])\ndef sparse_reduce_sum_v2(sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  if `output_is_sparse` is `False`, or a `SparseTensor` if `output_is_sparse`\\n  is `True`.\\n\\n  Note: if `output_is_sparse` is True, a gradient is not defined for this\\n  function, so it can't be used in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `axis`.  Unless `keepdims` is\\n  true, the rank of the tensor is reduced by 1 for each entry in `axis`. If\\n  `keepdims` is true, the reduced dimensions are retained with length 1.\\n\\n  If `axis` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    output_is_sparse: If true, returns a `SparseTensor` instead of a dense\\n      `Tensor` (the default).\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    The reduced Tensor or the reduced SparseTensor if `output_is_sparse` is\\n    True.\\n  \"\n    if keepdims is None:\n        keepdims = False\n    if output_is_sparse:\n        (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)\n        return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims, name=name)"
        ]
    },
    {
        "func_name": "sparse_reduce_sum",
        "original": "@tf_export(v1=['sparse.reduce_sum', 'sparse_reduce_sum'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_sum(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    \"\"\"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\n\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\n\n  This Op takes a SparseTensor and is the sparse counterpart to\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\n  instead of a sparse one.\n\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\n  with length 1.\n\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\n  with a single element is returned.  Additionally, the axes can be negative,\n  similar to the indexing rules in Python.\n\n  For example:\n\n    # 'x' represents [[1, ?, 1]\n    #                 [?, 1, ?]]\n    # where ? is implicitly-zero.\n\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\n    >>> tf.sparse.reduce_sum(x)\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\n    >>> tf.sparse.reduce_sum(x, 0)\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n    array([[2],\n           [1]], dtype=int32)>\n    >>> tf.sparse.reduce_sum(x, [0, 1])\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\n\n  Args:\n    sp_input: The SparseTensor to reduce. Should have numeric type.\n    axis: The dimensions to reduce; list or scalar. If `None` (the\n      default), reduces all dimensions.\n    keepdims: If true, retain reduced dimensions with length 1.\n    reduction_axes: Deprecated name of `axis`.\n    keep_dims: Deprecated alias for `keepdims`.\n\n  Returns:\n    The reduced Tensor.\n  \"\"\"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
        "mutated": [
            "@tf_export(v1=['sparse.reduce_sum', 'sparse_reduce_sum'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_sum(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
            "@tf_export(v1=['sparse.reduce_sum', 'sparse_reduce_sum'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_sum(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
            "@tf_export(v1=['sparse.reduce_sum', 'sparse_reduce_sum'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_sum(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
            "@tf_export(v1=['sparse.reduce_sum', 'sparse_reduce_sum'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_sum(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)",
            "@tf_export(v1=['sparse.reduce_sum', 'sparse_reduce_sum'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\n@deprecation.deprecated_args(None, 'reduction_axes is deprecated, use axis instead', 'reduction_axes')\ndef sparse_reduce_sum(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes `tf.sparse.add` of elements across dimensions of a SparseTensor.\\n\\n  This is the reduction operation for the elementwise `tf.sparse.add` op.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\\n  instead of a sparse one.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  similar to the indexing rules in Python.\\n\\n  For example:\\n\\n    # 'x' represents [[1, ?, 1]\\n    #                 [?, 1, ?]]\\n    # where ? is implicitly-zero.\\n\\n    >>> x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])\\n    >>> tf.sparse.reduce_sum(x)\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n    >>> tf.sparse.reduce_sum(x, 0)\\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, 1, keepdims=True)\\n    <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\\n    array([[2],\\n           [1]], dtype=int32)>\\n    >>> tf.sparse.reduce_sum(x, [0, 1])\\n    <tf.Tensor: shape=(), dtype=int32, numpy=3>\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of `axis`.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced Tensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    return gen_sparse_ops.sparse_reduce_sum(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)"
        ]
    },
    {
        "func_name": "sparse_reduce_sum_sparse",
        "original": "@tf_export(v1=['sparse.reduce_sum_sparse', 'sparse_reduce_sum_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_sum_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    \"\"\"Computes the sum of elements across dimensions of a SparseTensor.\n\n  This Op takes a SparseTensor and is the sparse counterpart to\n  `tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a\n  SparseTensor.\n\n  Note: A gradient is not defined for this function, so it can't be used\n  in training models that need gradient descent.\n\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\n  with length 1.\n\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\n  with a single element is returned.  Additionally, the axes can be negative,\n  which are interpreted according to the indexing rules in Python.\n\n  Args:\n    sp_input: The SparseTensor to reduce. Should have numeric type.\n    axis: The dimensions to reduce; list or scalar. If `None` (the\n      default), reduces all dimensions.\n    keepdims: If true, retain reduced dimensions with length 1.\n    reduction_axes: Deprecated name of axis.\n    keep_dims: Deprecated alias for `keepdims`.\n\n  Returns:\n    The reduced SparseTensor.\n  \"\"\"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
        "mutated": [
            "@tf_export(v1=['sparse.reduce_sum_sparse', 'sparse_reduce_sum_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_sum_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n    \"Computes the sum of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
            "@tf_export(v1=['sparse.reduce_sum_sparse', 'sparse_reduce_sum_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_sum_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes the sum of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
            "@tf_export(v1=['sparse.reduce_sum_sparse', 'sparse_reduce_sum_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_sum_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes the sum of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
            "@tf_export(v1=['sparse.reduce_sum_sparse', 'sparse_reduce_sum_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_sum_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes the sum of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)",
            "@tf_export(v1=['sparse.reduce_sum_sparse', 'sparse_reduce_sum_sparse'])\n@deprecation.deprecated_endpoints('sparse_reduce_sum_sparse')\n@deprecation.deprecated_args(None, 'keep_dims is deprecated, use keepdims instead', 'keep_dims')\ndef sparse_reduce_sum_sparse(sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes the sum of elements across dimensions of a SparseTensor.\\n\\n  This Op takes a SparseTensor and is the sparse counterpart to\\n  `tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a\\n  SparseTensor.\\n\\n  Note: A gradient is not defined for this function, so it can't be used\\n  in training models that need gradient descent.\\n\\n  Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\\n  `keepdims` is true, the rank of the tensor is reduced by 1 for each entry in\\n  `reduction_axes`. If `keepdims` is true, the reduced dimensions are retained\\n  with length 1.\\n\\n  If `reduction_axes` has no entries, all dimensions are reduced, and a tensor\\n  with a single element is returned.  Additionally, the axes can be negative,\\n  which are interpreted according to the indexing rules in Python.\\n\\n  Args:\\n    sp_input: The SparseTensor to reduce. Should have numeric type.\\n    axis: The dimensions to reduce; list or scalar. If `None` (the\\n      default), reduces all dimensions.\\n    keepdims: If true, retain reduced dimensions with length 1.\\n    reduction_axes: Deprecated name of axis.\\n    keep_dims: Deprecated alias for `keepdims`.\\n\\n  Returns:\\n    The reduced SparseTensor.\\n  \"\n    keepdims = deprecation.deprecated_argument_lookup('keepdims', keepdims, 'keep_dims', keep_dims)\n    axis = deprecation.deprecated_argument_lookup('axis', axis, 'reduction_axes', reduction_axes)\n    if keepdims is None:\n        keepdims = False\n    (output_ind, output_val, output_shape) = gen_sparse_ops.sparse_reduce_sum_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, math_ops._ReductionDims(sp_input, axis), keepdims)\n    return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)"
        ]
    },
    {
        "func_name": "sparse_tensor_to_dense",
        "original": "@tf_export('sparse.to_dense', v1=['sparse.to_dense', 'sparse_tensor_to_dense'])\n@deprecation.deprecated_endpoints('sparse_tensor_to_dense')\ndef sparse_tensor_to_dense(sp_input, default_value=None, validate_indices=True, name=None):\n    \"\"\"Converts a `SparseTensor` into a dense tensor.\n\n  For this sparse tensor with three non-empty values:\n\n  >>> sp_input = tf.sparse.SparseTensor(\n  ...   dense_shape=[3, 5],\n  ...   values=[7, 8, 9],\n  ...   indices =[[0, 1],\n  ...             [0, 3],\n  ...             [2, 0]])\n\n  The output will be a dense `[3, 5]` tensor with values:\n\n  >>> tf.sparse.to_dense(sp_input).numpy()\n  array([[0, 7, 0, 8, 0],\n         [0, 0, 0, 0, 0],\n         [9, 0, 0, 0, 0]], dtype=int32)\n\n  Note: Indices must be without repeats.  This is only tested if\n  `validate_indices` is `True`.\n\n  Args:\n    sp_input: The input `SparseTensor`.\n    default_value: Scalar value to set for indices not specified in\n      `sp_input`.  Defaults to zero.\n    validate_indices: A boolean value.  If `True`, indices are checked to make\n      sure they are sorted in lexicographic order and that there are no repeats.\n    name: A name prefix for the returned tensors (optional).\n\n  Returns:\n    A dense tensor with shape `sp_input.dense_shape` and values specified by\n    the non-empty values in `sp_input`. Indices not in `sp_input` are assigned\n    `default_value`.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    if default_value is None:\n        default_value = array_ops.zeros([], dtype=sp_input.dtype)\n    return gen_sparse_ops.sparse_to_dense(sp_input.indices, sp_input.dense_shape, sp_input.values, default_value=default_value, validate_indices=validate_indices, name=name)",
        "mutated": [
            "@tf_export('sparse.to_dense', v1=['sparse.to_dense', 'sparse_tensor_to_dense'])\n@deprecation.deprecated_endpoints('sparse_tensor_to_dense')\ndef sparse_tensor_to_dense(sp_input, default_value=None, validate_indices=True, name=None):\n    if False:\n        i = 10\n    'Converts a `SparseTensor` into a dense tensor.\\n\\n  For this sparse tensor with three non-empty values:\\n\\n  >>> sp_input = tf.sparse.SparseTensor(\\n  ...   dense_shape=[3, 5],\\n  ...   values=[7, 8, 9],\\n  ...   indices =[[0, 1],\\n  ...             [0, 3],\\n  ...             [2, 0]])\\n\\n  The output will be a dense `[3, 5]` tensor with values:\\n\\n  >>> tf.sparse.to_dense(sp_input).numpy()\\n  array([[0, 7, 0, 8, 0],\\n         [0, 0, 0, 0, 0],\\n         [9, 0, 0, 0, 0]], dtype=int32)\\n\\n  Note: Indices must be without repeats.  This is only tested if\\n  `validate_indices` is `True`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    default_value: Scalar value to set for indices not specified in\\n      `sp_input`.  Defaults to zero.\\n    validate_indices: A boolean value.  If `True`, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A dense tensor with shape `sp_input.dense_shape` and values specified by\\n    the non-empty values in `sp_input`. Indices not in `sp_input` are assigned\\n    `default_value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    if default_value is None:\n        default_value = array_ops.zeros([], dtype=sp_input.dtype)\n    return gen_sparse_ops.sparse_to_dense(sp_input.indices, sp_input.dense_shape, sp_input.values, default_value=default_value, validate_indices=validate_indices, name=name)",
            "@tf_export('sparse.to_dense', v1=['sparse.to_dense', 'sparse_tensor_to_dense'])\n@deprecation.deprecated_endpoints('sparse_tensor_to_dense')\ndef sparse_tensor_to_dense(sp_input, default_value=None, validate_indices=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Converts a `SparseTensor` into a dense tensor.\\n\\n  For this sparse tensor with three non-empty values:\\n\\n  >>> sp_input = tf.sparse.SparseTensor(\\n  ...   dense_shape=[3, 5],\\n  ...   values=[7, 8, 9],\\n  ...   indices =[[0, 1],\\n  ...             [0, 3],\\n  ...             [2, 0]])\\n\\n  The output will be a dense `[3, 5]` tensor with values:\\n\\n  >>> tf.sparse.to_dense(sp_input).numpy()\\n  array([[0, 7, 0, 8, 0],\\n         [0, 0, 0, 0, 0],\\n         [9, 0, 0, 0, 0]], dtype=int32)\\n\\n  Note: Indices must be without repeats.  This is only tested if\\n  `validate_indices` is `True`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    default_value: Scalar value to set for indices not specified in\\n      `sp_input`.  Defaults to zero.\\n    validate_indices: A boolean value.  If `True`, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A dense tensor with shape `sp_input.dense_shape` and values specified by\\n    the non-empty values in `sp_input`. Indices not in `sp_input` are assigned\\n    `default_value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    if default_value is None:\n        default_value = array_ops.zeros([], dtype=sp_input.dtype)\n    return gen_sparse_ops.sparse_to_dense(sp_input.indices, sp_input.dense_shape, sp_input.values, default_value=default_value, validate_indices=validate_indices, name=name)",
            "@tf_export('sparse.to_dense', v1=['sparse.to_dense', 'sparse_tensor_to_dense'])\n@deprecation.deprecated_endpoints('sparse_tensor_to_dense')\ndef sparse_tensor_to_dense(sp_input, default_value=None, validate_indices=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Converts a `SparseTensor` into a dense tensor.\\n\\n  For this sparse tensor with three non-empty values:\\n\\n  >>> sp_input = tf.sparse.SparseTensor(\\n  ...   dense_shape=[3, 5],\\n  ...   values=[7, 8, 9],\\n  ...   indices =[[0, 1],\\n  ...             [0, 3],\\n  ...             [2, 0]])\\n\\n  The output will be a dense `[3, 5]` tensor with values:\\n\\n  >>> tf.sparse.to_dense(sp_input).numpy()\\n  array([[0, 7, 0, 8, 0],\\n         [0, 0, 0, 0, 0],\\n         [9, 0, 0, 0, 0]], dtype=int32)\\n\\n  Note: Indices must be without repeats.  This is only tested if\\n  `validate_indices` is `True`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    default_value: Scalar value to set for indices not specified in\\n      `sp_input`.  Defaults to zero.\\n    validate_indices: A boolean value.  If `True`, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A dense tensor with shape `sp_input.dense_shape` and values specified by\\n    the non-empty values in `sp_input`. Indices not in `sp_input` are assigned\\n    `default_value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    if default_value is None:\n        default_value = array_ops.zeros([], dtype=sp_input.dtype)\n    return gen_sparse_ops.sparse_to_dense(sp_input.indices, sp_input.dense_shape, sp_input.values, default_value=default_value, validate_indices=validate_indices, name=name)",
            "@tf_export('sparse.to_dense', v1=['sparse.to_dense', 'sparse_tensor_to_dense'])\n@deprecation.deprecated_endpoints('sparse_tensor_to_dense')\ndef sparse_tensor_to_dense(sp_input, default_value=None, validate_indices=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Converts a `SparseTensor` into a dense tensor.\\n\\n  For this sparse tensor with three non-empty values:\\n\\n  >>> sp_input = tf.sparse.SparseTensor(\\n  ...   dense_shape=[3, 5],\\n  ...   values=[7, 8, 9],\\n  ...   indices =[[0, 1],\\n  ...             [0, 3],\\n  ...             [2, 0]])\\n\\n  The output will be a dense `[3, 5]` tensor with values:\\n\\n  >>> tf.sparse.to_dense(sp_input).numpy()\\n  array([[0, 7, 0, 8, 0],\\n         [0, 0, 0, 0, 0],\\n         [9, 0, 0, 0, 0]], dtype=int32)\\n\\n  Note: Indices must be without repeats.  This is only tested if\\n  `validate_indices` is `True`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    default_value: Scalar value to set for indices not specified in\\n      `sp_input`.  Defaults to zero.\\n    validate_indices: A boolean value.  If `True`, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A dense tensor with shape `sp_input.dense_shape` and values specified by\\n    the non-empty values in `sp_input`. Indices not in `sp_input` are assigned\\n    `default_value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    if default_value is None:\n        default_value = array_ops.zeros([], dtype=sp_input.dtype)\n    return gen_sparse_ops.sparse_to_dense(sp_input.indices, sp_input.dense_shape, sp_input.values, default_value=default_value, validate_indices=validate_indices, name=name)",
            "@tf_export('sparse.to_dense', v1=['sparse.to_dense', 'sparse_tensor_to_dense'])\n@deprecation.deprecated_endpoints('sparse_tensor_to_dense')\ndef sparse_tensor_to_dense(sp_input, default_value=None, validate_indices=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Converts a `SparseTensor` into a dense tensor.\\n\\n  For this sparse tensor with three non-empty values:\\n\\n  >>> sp_input = tf.sparse.SparseTensor(\\n  ...   dense_shape=[3, 5],\\n  ...   values=[7, 8, 9],\\n  ...   indices =[[0, 1],\\n  ...             [0, 3],\\n  ...             [2, 0]])\\n\\n  The output will be a dense `[3, 5]` tensor with values:\\n\\n  >>> tf.sparse.to_dense(sp_input).numpy()\\n  array([[0, 7, 0, 8, 0],\\n         [0, 0, 0, 0, 0],\\n         [9, 0, 0, 0, 0]], dtype=int32)\\n\\n  Note: Indices must be without repeats.  This is only tested if\\n  `validate_indices` is `True`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    default_value: Scalar value to set for indices not specified in\\n      `sp_input`.  Defaults to zero.\\n    validate_indices: A boolean value.  If `True`, indices are checked to make\\n      sure they are sorted in lexicographic order and that there are no repeats.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A dense tensor with shape `sp_input.dense_shape` and values specified by\\n    the non-empty values in `sp_input`. Indices not in `sp_input` are assigned\\n    `default_value`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    if default_value is None:\n        default_value = array_ops.zeros([], dtype=sp_input.dtype)\n    return gen_sparse_ops.sparse_to_dense(sp_input.indices, sp_input.dense_shape, sp_input.values, default_value=default_value, validate_indices=validate_indices, name=name)"
        ]
    },
    {
        "func_name": "sparse_to_indicator",
        "original": "@tf_export('sparse.to_indicator', v1=['sparse.to_indicator', 'sparse_to_indicator'])\n@deprecation.deprecated_endpoints('sparse_to_indicator')\ndef sparse_to_indicator(sp_input, vocab_size, name=None):\n    \"\"\"Converts a `SparseTensor` of ids into a dense bool indicator tensor.\n\n  The last dimension of `sp_input.indices` is discarded and replaced with\n  the values of `sp_input`.  If `sp_input.dense_shape = [D0, D1, ..., Dn, K]`,\n  then `output.shape = [D0, D1, ..., Dn, vocab_size]`, where\n\n      output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True\n\n  and False elsewhere in `output`.\n\n  For example, if `sp_input.dense_shape = [2, 3, 4]` with non-empty values:\n\n      [0, 0, 0]: 0\n      [0, 1, 0]: 10\n      [1, 0, 3]: 103\n      [1, 1, 1]: 150\n      [1, 1, 2]: 149\n      [1, 1, 3]: 150\n      [1, 2, 1]: 121\n\n  and `vocab_size = 200`, then the output will be a `[2, 3, 200]` dense bool\n  tensor with False everywhere except at positions\n\n      (0, 0, 0), (0, 1, 10), (1, 0, 103), (1, 1, 149), (1, 1, 150),\n      (1, 2, 121).\n\n  Note that repeats are allowed in the input SparseTensor.\n  This op is useful for converting `SparseTensor`s into dense formats for\n  compatibility with ops that expect dense tensors.\n\n  The input `SparseTensor` must be in row-major order.\n\n  Args:\n    sp_input: A `SparseTensor` with `values` property of type `int32` or\n      `int64`.\n    vocab_size: A scalar int64 Tensor (or Python int) containing the new size\n      of the last dimension, `all(0 <= sp_input.values < vocab_size)`.\n    name: A name prefix for the returned tensors (optional)\n\n  Returns:\n    A dense bool indicator tensor representing the indices with specified value.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseToIndicator', [sp_input]) as name:\n        num_entries = array_ops.shape(sp_input.indices)[0]\n        new_values = array_ops.fill(array_ops.expand_dims(num_entries, 0), True)\n        sp_values = sparse_tensor.SparseTensor(sp_input.indices, new_values, sp_input.dense_shape)\n        sp_new = sparse_merge_impl(sp_input, sp_values, vocab_size, name)\n        return sparse_tensor_to_dense(sp_new, default_value=False, validate_indices=False, name=name)",
        "mutated": [
            "@tf_export('sparse.to_indicator', v1=['sparse.to_indicator', 'sparse_to_indicator'])\n@deprecation.deprecated_endpoints('sparse_to_indicator')\ndef sparse_to_indicator(sp_input, vocab_size, name=None):\n    if False:\n        i = 10\n    'Converts a `SparseTensor` of ids into a dense bool indicator tensor.\\n\\n  The last dimension of `sp_input.indices` is discarded and replaced with\\n  the values of `sp_input`.  If `sp_input.dense_shape = [D0, D1, ..., Dn, K]`,\\n  then `output.shape = [D0, D1, ..., Dn, vocab_size]`, where\\n\\n      output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True\\n\\n  and False elsewhere in `output`.\\n\\n  For example, if `sp_input.dense_shape = [2, 3, 4]` with non-empty values:\\n\\n      [0, 0, 0]: 0\\n      [0, 1, 0]: 10\\n      [1, 0, 3]: 103\\n      [1, 1, 1]: 150\\n      [1, 1, 2]: 149\\n      [1, 1, 3]: 150\\n      [1, 2, 1]: 121\\n\\n  and `vocab_size = 200`, then the output will be a `[2, 3, 200]` dense bool\\n  tensor with False everywhere except at positions\\n\\n      (0, 0, 0), (0, 1, 10), (1, 0, 103), (1, 1, 149), (1, 1, 150),\\n      (1, 2, 121).\\n\\n  Note that repeats are allowed in the input SparseTensor.\\n  This op is useful for converting `SparseTensor`s into dense formats for\\n  compatibility with ops that expect dense tensors.\\n\\n  The input `SparseTensor` must be in row-major order.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with `values` property of type `int32` or\\n      `int64`.\\n    vocab_size: A scalar int64 Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_input.values < vocab_size)`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense bool indicator tensor representing the indices with specified value.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseToIndicator', [sp_input]) as name:\n        num_entries = array_ops.shape(sp_input.indices)[0]\n        new_values = array_ops.fill(array_ops.expand_dims(num_entries, 0), True)\n        sp_values = sparse_tensor.SparseTensor(sp_input.indices, new_values, sp_input.dense_shape)\n        sp_new = sparse_merge_impl(sp_input, sp_values, vocab_size, name)\n        return sparse_tensor_to_dense(sp_new, default_value=False, validate_indices=False, name=name)",
            "@tf_export('sparse.to_indicator', v1=['sparse.to_indicator', 'sparse_to_indicator'])\n@deprecation.deprecated_endpoints('sparse_to_indicator')\ndef sparse_to_indicator(sp_input, vocab_size, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Converts a `SparseTensor` of ids into a dense bool indicator tensor.\\n\\n  The last dimension of `sp_input.indices` is discarded and replaced with\\n  the values of `sp_input`.  If `sp_input.dense_shape = [D0, D1, ..., Dn, K]`,\\n  then `output.shape = [D0, D1, ..., Dn, vocab_size]`, where\\n\\n      output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True\\n\\n  and False elsewhere in `output`.\\n\\n  For example, if `sp_input.dense_shape = [2, 3, 4]` with non-empty values:\\n\\n      [0, 0, 0]: 0\\n      [0, 1, 0]: 10\\n      [1, 0, 3]: 103\\n      [1, 1, 1]: 150\\n      [1, 1, 2]: 149\\n      [1, 1, 3]: 150\\n      [1, 2, 1]: 121\\n\\n  and `vocab_size = 200`, then the output will be a `[2, 3, 200]` dense bool\\n  tensor with False everywhere except at positions\\n\\n      (0, 0, 0), (0, 1, 10), (1, 0, 103), (1, 1, 149), (1, 1, 150),\\n      (1, 2, 121).\\n\\n  Note that repeats are allowed in the input SparseTensor.\\n  This op is useful for converting `SparseTensor`s into dense formats for\\n  compatibility with ops that expect dense tensors.\\n\\n  The input `SparseTensor` must be in row-major order.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with `values` property of type `int32` or\\n      `int64`.\\n    vocab_size: A scalar int64 Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_input.values < vocab_size)`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense bool indicator tensor representing the indices with specified value.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseToIndicator', [sp_input]) as name:\n        num_entries = array_ops.shape(sp_input.indices)[0]\n        new_values = array_ops.fill(array_ops.expand_dims(num_entries, 0), True)\n        sp_values = sparse_tensor.SparseTensor(sp_input.indices, new_values, sp_input.dense_shape)\n        sp_new = sparse_merge_impl(sp_input, sp_values, vocab_size, name)\n        return sparse_tensor_to_dense(sp_new, default_value=False, validate_indices=False, name=name)",
            "@tf_export('sparse.to_indicator', v1=['sparse.to_indicator', 'sparse_to_indicator'])\n@deprecation.deprecated_endpoints('sparse_to_indicator')\ndef sparse_to_indicator(sp_input, vocab_size, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Converts a `SparseTensor` of ids into a dense bool indicator tensor.\\n\\n  The last dimension of `sp_input.indices` is discarded and replaced with\\n  the values of `sp_input`.  If `sp_input.dense_shape = [D0, D1, ..., Dn, K]`,\\n  then `output.shape = [D0, D1, ..., Dn, vocab_size]`, where\\n\\n      output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True\\n\\n  and False elsewhere in `output`.\\n\\n  For example, if `sp_input.dense_shape = [2, 3, 4]` with non-empty values:\\n\\n      [0, 0, 0]: 0\\n      [0, 1, 0]: 10\\n      [1, 0, 3]: 103\\n      [1, 1, 1]: 150\\n      [1, 1, 2]: 149\\n      [1, 1, 3]: 150\\n      [1, 2, 1]: 121\\n\\n  and `vocab_size = 200`, then the output will be a `[2, 3, 200]` dense bool\\n  tensor with False everywhere except at positions\\n\\n      (0, 0, 0), (0, 1, 10), (1, 0, 103), (1, 1, 149), (1, 1, 150),\\n      (1, 2, 121).\\n\\n  Note that repeats are allowed in the input SparseTensor.\\n  This op is useful for converting `SparseTensor`s into dense formats for\\n  compatibility with ops that expect dense tensors.\\n\\n  The input `SparseTensor` must be in row-major order.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with `values` property of type `int32` or\\n      `int64`.\\n    vocab_size: A scalar int64 Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_input.values < vocab_size)`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense bool indicator tensor representing the indices with specified value.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseToIndicator', [sp_input]) as name:\n        num_entries = array_ops.shape(sp_input.indices)[0]\n        new_values = array_ops.fill(array_ops.expand_dims(num_entries, 0), True)\n        sp_values = sparse_tensor.SparseTensor(sp_input.indices, new_values, sp_input.dense_shape)\n        sp_new = sparse_merge_impl(sp_input, sp_values, vocab_size, name)\n        return sparse_tensor_to_dense(sp_new, default_value=False, validate_indices=False, name=name)",
            "@tf_export('sparse.to_indicator', v1=['sparse.to_indicator', 'sparse_to_indicator'])\n@deprecation.deprecated_endpoints('sparse_to_indicator')\ndef sparse_to_indicator(sp_input, vocab_size, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Converts a `SparseTensor` of ids into a dense bool indicator tensor.\\n\\n  The last dimension of `sp_input.indices` is discarded and replaced with\\n  the values of `sp_input`.  If `sp_input.dense_shape = [D0, D1, ..., Dn, K]`,\\n  then `output.shape = [D0, D1, ..., Dn, vocab_size]`, where\\n\\n      output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True\\n\\n  and False elsewhere in `output`.\\n\\n  For example, if `sp_input.dense_shape = [2, 3, 4]` with non-empty values:\\n\\n      [0, 0, 0]: 0\\n      [0, 1, 0]: 10\\n      [1, 0, 3]: 103\\n      [1, 1, 1]: 150\\n      [1, 1, 2]: 149\\n      [1, 1, 3]: 150\\n      [1, 2, 1]: 121\\n\\n  and `vocab_size = 200`, then the output will be a `[2, 3, 200]` dense bool\\n  tensor with False everywhere except at positions\\n\\n      (0, 0, 0), (0, 1, 10), (1, 0, 103), (1, 1, 149), (1, 1, 150),\\n      (1, 2, 121).\\n\\n  Note that repeats are allowed in the input SparseTensor.\\n  This op is useful for converting `SparseTensor`s into dense formats for\\n  compatibility with ops that expect dense tensors.\\n\\n  The input `SparseTensor` must be in row-major order.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with `values` property of type `int32` or\\n      `int64`.\\n    vocab_size: A scalar int64 Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_input.values < vocab_size)`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense bool indicator tensor representing the indices with specified value.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseToIndicator', [sp_input]) as name:\n        num_entries = array_ops.shape(sp_input.indices)[0]\n        new_values = array_ops.fill(array_ops.expand_dims(num_entries, 0), True)\n        sp_values = sparse_tensor.SparseTensor(sp_input.indices, new_values, sp_input.dense_shape)\n        sp_new = sparse_merge_impl(sp_input, sp_values, vocab_size, name)\n        return sparse_tensor_to_dense(sp_new, default_value=False, validate_indices=False, name=name)",
            "@tf_export('sparse.to_indicator', v1=['sparse.to_indicator', 'sparse_to_indicator'])\n@deprecation.deprecated_endpoints('sparse_to_indicator')\ndef sparse_to_indicator(sp_input, vocab_size, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Converts a `SparseTensor` of ids into a dense bool indicator tensor.\\n\\n  The last dimension of `sp_input.indices` is discarded and replaced with\\n  the values of `sp_input`.  If `sp_input.dense_shape = [D0, D1, ..., Dn, K]`,\\n  then `output.shape = [D0, D1, ..., Dn, vocab_size]`, where\\n\\n      output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True\\n\\n  and False elsewhere in `output`.\\n\\n  For example, if `sp_input.dense_shape = [2, 3, 4]` with non-empty values:\\n\\n      [0, 0, 0]: 0\\n      [0, 1, 0]: 10\\n      [1, 0, 3]: 103\\n      [1, 1, 1]: 150\\n      [1, 1, 2]: 149\\n      [1, 1, 3]: 150\\n      [1, 2, 1]: 121\\n\\n  and `vocab_size = 200`, then the output will be a `[2, 3, 200]` dense bool\\n  tensor with False everywhere except at positions\\n\\n      (0, 0, 0), (0, 1, 10), (1, 0, 103), (1, 1, 149), (1, 1, 150),\\n      (1, 2, 121).\\n\\n  Note that repeats are allowed in the input SparseTensor.\\n  This op is useful for converting `SparseTensor`s into dense formats for\\n  compatibility with ops that expect dense tensors.\\n\\n  The input `SparseTensor` must be in row-major order.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with `values` property of type `int32` or\\n      `int64`.\\n    vocab_size: A scalar int64 Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_input.values < vocab_size)`.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense bool indicator tensor representing the indices with specified value.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseToIndicator', [sp_input]) as name:\n        num_entries = array_ops.shape(sp_input.indices)[0]\n        new_values = array_ops.fill(array_ops.expand_dims(num_entries, 0), True)\n        sp_values = sparse_tensor.SparseTensor(sp_input.indices, new_values, sp_input.dense_shape)\n        sp_new = sparse_merge_impl(sp_input, sp_values, vocab_size, name)\n        return sparse_tensor_to_dense(sp_new, default_value=False, validate_indices=False, name=name)"
        ]
    },
    {
        "func_name": "sparse_merge",
        "original": "@tf_export(v1=['sparse.merge', 'sparse_merge'])\n@deprecation.deprecated(None, 'No similar op available at this time.')\ndef sparse_merge(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    \"\"\"Combines a batch of feature ids and values into a single `SparseTensor`.\n\n  The most common use case for this function occurs when feature ids and\n  their corresponding values are stored in `Example` protos on disk.\n  `parse_example` will return a batch of ids and a batch of values, and this\n  function joins them into a single logical `SparseTensor` for use in\n  functions such as `sparse_tensor_dense_matmul`, `sparse_to_dense`, etc.\n\n  The `SparseTensor` returned by this function has the following properties:\n\n    - `indices` is equivalent to `sp_ids.indices` with the last\n      dimension discarded and replaced with `sp_ids.values`.\n    - `values` is simply `sp_values.values`.\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\n      `output.shape = [D0, D1, ..., Dn, vocab_size]`.\n\n  For example, consider the following feature vectors:\n\n  ```python\n    vector1 = [-3, 0, 0, 0, 0, 0]\n    vector2 = [ 0, 1, 0, 4, 1, 0]\n    vector3 = [ 5, 0, 0, 9, 0, 0]\n  ```\n\n  These might be stored sparsely in the following Example protos by storing\n  only the feature ids (column number if the vectors are treated as a matrix)\n  of the non-zero elements and the corresponding values:\n\n  ```python\n    examples = [Example(features={\n                    \"ids\": Feature(int64_list=Int64List(value=[0])),\n                    \"values\": Feature(float_list=FloatList(value=[-3]))}),\n                Example(features={\n                    \"ids\": Feature(int64_list=Int64List(value=[1, 4, 3])),\n                    \"values\": Feature(float_list=FloatList(value=[1, 1, 4]))}),\n                Example(features={\n                    \"ids\": Feature(int64_list=Int64List(value=[0, 3])),\n                    \"values\": Feature(float_list=FloatList(value=[5, 9]))})]\n  ```\n\n  The result of calling parse_example on these examples will produce a\n  dictionary with entries for \"ids\" and \"values\". Passing those two objects\n  to this function along with vocab_size=6, will produce a `SparseTensor` that\n  sparsely represents all three instances. Namely, the `indices` property will\n  contain the coordinates of the non-zero entries in the feature matrix (the\n  first dimension is the row number in the matrix, i.e., the index within the\n  batch, and the second dimension is the column number, i.e., the feature id);\n  `values` will contain the actual values. `shape` will be the shape of the\n  original matrix, i.e., (3, 6). For our example above, the output will be\n  equal to:\n\n  ```python\n    SparseTensor(indices=[[0, 0], [1, 1], [1, 3], [1, 4], [2, 0], [2, 3]],\n                 values=[-3, 1, 4, 1, 5, 9],\n                 dense_shape=[3, 6])\n  ```\n\n  This method generalizes to higher-dimensions by simply providing a list for\n  both the sp_ids as well as the vocab_size.\n  In this case the resulting `SparseTensor` has the following properties:\n    - `indices` is equivalent to `sp_ids[0].indices` with the last\n      dimension discarded and concatenated with\n      `sp_ids[0].values, sp_ids[1].values, ...`.\n    - `values` is simply `sp_values.values`.\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\n      `output.shape = [D0, D1, ..., Dn] + vocab_size`.\n\n  Args:\n    sp_ids: A single `SparseTensor` with `values` property of type `int32`\n      or `int64` or a Python list of such `SparseTensor`s or a list thereof.\n    sp_values: A `SparseTensor` of any type.\n    vocab_size: A scalar `int64` Tensor (or Python int) containing the new size\n      of the last dimension, `all(0 <= sp_ids.values < vocab_size)`.\n      Or a list thereof with `all(0 <= sp_ids[i].values < vocab_size[i])` for\n      all `i`.\n    name: A name prefix for the returned tensors (optional)\n    already_sorted: A boolean to specify whether the per-batch values in\n     `sp_values` are already sorted. If so skip sorting, False by default\n     (optional).\n\n  Returns:\n    A `SparseTensor` compactly representing a batch of feature ids and values,\n    useful for passing to functions that expect such a `SparseTensor`.\n\n  Raises:\n    TypeError: If `sp_values` is not a `SparseTensor`. Or if `sp_ids` is neither\n      a `SparseTensor` nor a list thereof. Or if `vocab_size` is not a\n      `Tensor` or a Python int and `sp_ids` is a `SparseTensor`. Or if\n      `vocab_size` is not a or list thereof and `sp_ids` is a list.\n    ValueError: If `sp_ids` and `vocab_size` are lists of different lengths.\n  \"\"\"\n    return sparse_merge_impl(sp_ids, sp_values, vocab_size, name, already_sorted)",
        "mutated": [
            "@tf_export(v1=['sparse.merge', 'sparse_merge'])\n@deprecation.deprecated(None, 'No similar op available at this time.')\ndef sparse_merge(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n    'Combines a batch of feature ids and values into a single `SparseTensor`.\\n\\n  The most common use case for this function occurs when feature ids and\\n  their corresponding values are stored in `Example` protos on disk.\\n  `parse_example` will return a batch of ids and a batch of values, and this\\n  function joins them into a single logical `SparseTensor` for use in\\n  functions such as `sparse_tensor_dense_matmul`, `sparse_to_dense`, etc.\\n\\n  The `SparseTensor` returned by this function has the following properties:\\n\\n    - `indices` is equivalent to `sp_ids.indices` with the last\\n      dimension discarded and replaced with `sp_ids.values`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn, vocab_size]`.\\n\\n  For example, consider the following feature vectors:\\n\\n  ```python\\n    vector1 = [-3, 0, 0, 0, 0, 0]\\n    vector2 = [ 0, 1, 0, 4, 1, 0]\\n    vector3 = [ 5, 0, 0, 9, 0, 0]\\n  ```\\n\\n  These might be stored sparsely in the following Example protos by storing\\n  only the feature ids (column number if the vectors are treated as a matrix)\\n  of the non-zero elements and the corresponding values:\\n\\n  ```python\\n    examples = [Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0])),\\n                    \"values\": Feature(float_list=FloatList(value=[-3]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[1, 4, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[1, 1, 4]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[5, 9]))})]\\n  ```\\n\\n  The result of calling parse_example on these examples will produce a\\n  dictionary with entries for \"ids\" and \"values\". Passing those two objects\\n  to this function along with vocab_size=6, will produce a `SparseTensor` that\\n  sparsely represents all three instances. Namely, the `indices` property will\\n  contain the coordinates of the non-zero entries in the feature matrix (the\\n  first dimension is the row number in the matrix, i.e., the index within the\\n  batch, and the second dimension is the column number, i.e., the feature id);\\n  `values` will contain the actual values. `shape` will be the shape of the\\n  original matrix, i.e., (3, 6). For our example above, the output will be\\n  equal to:\\n\\n  ```python\\n    SparseTensor(indices=[[0, 0], [1, 1], [1, 3], [1, 4], [2, 0], [2, 3]],\\n                 values=[-3, 1, 4, 1, 5, 9],\\n                 dense_shape=[3, 6])\\n  ```\\n\\n  This method generalizes to higher-dimensions by simply providing a list for\\n  both the sp_ids as well as the vocab_size.\\n  In this case the resulting `SparseTensor` has the following properties:\\n    - `indices` is equivalent to `sp_ids[0].indices` with the last\\n      dimension discarded and concatenated with\\n      `sp_ids[0].values, sp_ids[1].values, ...`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn] + vocab_size`.\\n\\n  Args:\\n    sp_ids: A single `SparseTensor` with `values` property of type `int32`\\n      or `int64` or a Python list of such `SparseTensor`s or a list thereof.\\n    sp_values: A `SparseTensor` of any type.\\n    vocab_size: A scalar `int64` Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_ids.values < vocab_size)`.\\n      Or a list thereof with `all(0 <= sp_ids[i].values < vocab_size[i])` for\\n      all `i`.\\n    name: A name prefix for the returned tensors (optional)\\n    already_sorted: A boolean to specify whether the per-batch values in\\n     `sp_values` are already sorted. If so skip sorting, False by default\\n     (optional).\\n\\n  Returns:\\n    A `SparseTensor` compactly representing a batch of feature ids and values,\\n    useful for passing to functions that expect such a `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_values` is not a `SparseTensor`. Or if `sp_ids` is neither\\n      a `SparseTensor` nor a list thereof. Or if `vocab_size` is not a\\n      `Tensor` or a Python int and `sp_ids` is a `SparseTensor`. Or if\\n      `vocab_size` is not a or list thereof and `sp_ids` is a list.\\n    ValueError: If `sp_ids` and `vocab_size` are lists of different lengths.\\n  '\n    return sparse_merge_impl(sp_ids, sp_values, vocab_size, name, already_sorted)",
            "@tf_export(v1=['sparse.merge', 'sparse_merge'])\n@deprecation.deprecated(None, 'No similar op available at this time.')\ndef sparse_merge(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Combines a batch of feature ids and values into a single `SparseTensor`.\\n\\n  The most common use case for this function occurs when feature ids and\\n  their corresponding values are stored in `Example` protos on disk.\\n  `parse_example` will return a batch of ids and a batch of values, and this\\n  function joins them into a single logical `SparseTensor` for use in\\n  functions such as `sparse_tensor_dense_matmul`, `sparse_to_dense`, etc.\\n\\n  The `SparseTensor` returned by this function has the following properties:\\n\\n    - `indices` is equivalent to `sp_ids.indices` with the last\\n      dimension discarded and replaced with `sp_ids.values`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn, vocab_size]`.\\n\\n  For example, consider the following feature vectors:\\n\\n  ```python\\n    vector1 = [-3, 0, 0, 0, 0, 0]\\n    vector2 = [ 0, 1, 0, 4, 1, 0]\\n    vector3 = [ 5, 0, 0, 9, 0, 0]\\n  ```\\n\\n  These might be stored sparsely in the following Example protos by storing\\n  only the feature ids (column number if the vectors are treated as a matrix)\\n  of the non-zero elements and the corresponding values:\\n\\n  ```python\\n    examples = [Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0])),\\n                    \"values\": Feature(float_list=FloatList(value=[-3]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[1, 4, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[1, 1, 4]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[5, 9]))})]\\n  ```\\n\\n  The result of calling parse_example on these examples will produce a\\n  dictionary with entries for \"ids\" and \"values\". Passing those two objects\\n  to this function along with vocab_size=6, will produce a `SparseTensor` that\\n  sparsely represents all three instances. Namely, the `indices` property will\\n  contain the coordinates of the non-zero entries in the feature matrix (the\\n  first dimension is the row number in the matrix, i.e., the index within the\\n  batch, and the second dimension is the column number, i.e., the feature id);\\n  `values` will contain the actual values. `shape` will be the shape of the\\n  original matrix, i.e., (3, 6). For our example above, the output will be\\n  equal to:\\n\\n  ```python\\n    SparseTensor(indices=[[0, 0], [1, 1], [1, 3], [1, 4], [2, 0], [2, 3]],\\n                 values=[-3, 1, 4, 1, 5, 9],\\n                 dense_shape=[3, 6])\\n  ```\\n\\n  This method generalizes to higher-dimensions by simply providing a list for\\n  both the sp_ids as well as the vocab_size.\\n  In this case the resulting `SparseTensor` has the following properties:\\n    - `indices` is equivalent to `sp_ids[0].indices` with the last\\n      dimension discarded and concatenated with\\n      `sp_ids[0].values, sp_ids[1].values, ...`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn] + vocab_size`.\\n\\n  Args:\\n    sp_ids: A single `SparseTensor` with `values` property of type `int32`\\n      or `int64` or a Python list of such `SparseTensor`s or a list thereof.\\n    sp_values: A `SparseTensor` of any type.\\n    vocab_size: A scalar `int64` Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_ids.values < vocab_size)`.\\n      Or a list thereof with `all(0 <= sp_ids[i].values < vocab_size[i])` for\\n      all `i`.\\n    name: A name prefix for the returned tensors (optional)\\n    already_sorted: A boolean to specify whether the per-batch values in\\n     `sp_values` are already sorted. If so skip sorting, False by default\\n     (optional).\\n\\n  Returns:\\n    A `SparseTensor` compactly representing a batch of feature ids and values,\\n    useful for passing to functions that expect such a `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_values` is not a `SparseTensor`. Or if `sp_ids` is neither\\n      a `SparseTensor` nor a list thereof. Or if `vocab_size` is not a\\n      `Tensor` or a Python int and `sp_ids` is a `SparseTensor`. Or if\\n      `vocab_size` is not a or list thereof and `sp_ids` is a list.\\n    ValueError: If `sp_ids` and `vocab_size` are lists of different lengths.\\n  '\n    return sparse_merge_impl(sp_ids, sp_values, vocab_size, name, already_sorted)",
            "@tf_export(v1=['sparse.merge', 'sparse_merge'])\n@deprecation.deprecated(None, 'No similar op available at this time.')\ndef sparse_merge(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Combines a batch of feature ids and values into a single `SparseTensor`.\\n\\n  The most common use case for this function occurs when feature ids and\\n  their corresponding values are stored in `Example` protos on disk.\\n  `parse_example` will return a batch of ids and a batch of values, and this\\n  function joins them into a single logical `SparseTensor` for use in\\n  functions such as `sparse_tensor_dense_matmul`, `sparse_to_dense`, etc.\\n\\n  The `SparseTensor` returned by this function has the following properties:\\n\\n    - `indices` is equivalent to `sp_ids.indices` with the last\\n      dimension discarded and replaced with `sp_ids.values`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn, vocab_size]`.\\n\\n  For example, consider the following feature vectors:\\n\\n  ```python\\n    vector1 = [-3, 0, 0, 0, 0, 0]\\n    vector2 = [ 0, 1, 0, 4, 1, 0]\\n    vector3 = [ 5, 0, 0, 9, 0, 0]\\n  ```\\n\\n  These might be stored sparsely in the following Example protos by storing\\n  only the feature ids (column number if the vectors are treated as a matrix)\\n  of the non-zero elements and the corresponding values:\\n\\n  ```python\\n    examples = [Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0])),\\n                    \"values\": Feature(float_list=FloatList(value=[-3]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[1, 4, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[1, 1, 4]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[5, 9]))})]\\n  ```\\n\\n  The result of calling parse_example on these examples will produce a\\n  dictionary with entries for \"ids\" and \"values\". Passing those two objects\\n  to this function along with vocab_size=6, will produce a `SparseTensor` that\\n  sparsely represents all three instances. Namely, the `indices` property will\\n  contain the coordinates of the non-zero entries in the feature matrix (the\\n  first dimension is the row number in the matrix, i.e., the index within the\\n  batch, and the second dimension is the column number, i.e., the feature id);\\n  `values` will contain the actual values. `shape` will be the shape of the\\n  original matrix, i.e., (3, 6). For our example above, the output will be\\n  equal to:\\n\\n  ```python\\n    SparseTensor(indices=[[0, 0], [1, 1], [1, 3], [1, 4], [2, 0], [2, 3]],\\n                 values=[-3, 1, 4, 1, 5, 9],\\n                 dense_shape=[3, 6])\\n  ```\\n\\n  This method generalizes to higher-dimensions by simply providing a list for\\n  both the sp_ids as well as the vocab_size.\\n  In this case the resulting `SparseTensor` has the following properties:\\n    - `indices` is equivalent to `sp_ids[0].indices` with the last\\n      dimension discarded and concatenated with\\n      `sp_ids[0].values, sp_ids[1].values, ...`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn] + vocab_size`.\\n\\n  Args:\\n    sp_ids: A single `SparseTensor` with `values` property of type `int32`\\n      or `int64` or a Python list of such `SparseTensor`s or a list thereof.\\n    sp_values: A `SparseTensor` of any type.\\n    vocab_size: A scalar `int64` Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_ids.values < vocab_size)`.\\n      Or a list thereof with `all(0 <= sp_ids[i].values < vocab_size[i])` for\\n      all `i`.\\n    name: A name prefix for the returned tensors (optional)\\n    already_sorted: A boolean to specify whether the per-batch values in\\n     `sp_values` are already sorted. If so skip sorting, False by default\\n     (optional).\\n\\n  Returns:\\n    A `SparseTensor` compactly representing a batch of feature ids and values,\\n    useful for passing to functions that expect such a `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_values` is not a `SparseTensor`. Or if `sp_ids` is neither\\n      a `SparseTensor` nor a list thereof. Or if `vocab_size` is not a\\n      `Tensor` or a Python int and `sp_ids` is a `SparseTensor`. Or if\\n      `vocab_size` is not a or list thereof and `sp_ids` is a list.\\n    ValueError: If `sp_ids` and `vocab_size` are lists of different lengths.\\n  '\n    return sparse_merge_impl(sp_ids, sp_values, vocab_size, name, already_sorted)",
            "@tf_export(v1=['sparse.merge', 'sparse_merge'])\n@deprecation.deprecated(None, 'No similar op available at this time.')\ndef sparse_merge(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Combines a batch of feature ids and values into a single `SparseTensor`.\\n\\n  The most common use case for this function occurs when feature ids and\\n  their corresponding values are stored in `Example` protos on disk.\\n  `parse_example` will return a batch of ids and a batch of values, and this\\n  function joins them into a single logical `SparseTensor` for use in\\n  functions such as `sparse_tensor_dense_matmul`, `sparse_to_dense`, etc.\\n\\n  The `SparseTensor` returned by this function has the following properties:\\n\\n    - `indices` is equivalent to `sp_ids.indices` with the last\\n      dimension discarded and replaced with `sp_ids.values`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn, vocab_size]`.\\n\\n  For example, consider the following feature vectors:\\n\\n  ```python\\n    vector1 = [-3, 0, 0, 0, 0, 0]\\n    vector2 = [ 0, 1, 0, 4, 1, 0]\\n    vector3 = [ 5, 0, 0, 9, 0, 0]\\n  ```\\n\\n  These might be stored sparsely in the following Example protos by storing\\n  only the feature ids (column number if the vectors are treated as a matrix)\\n  of the non-zero elements and the corresponding values:\\n\\n  ```python\\n    examples = [Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0])),\\n                    \"values\": Feature(float_list=FloatList(value=[-3]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[1, 4, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[1, 1, 4]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[5, 9]))})]\\n  ```\\n\\n  The result of calling parse_example on these examples will produce a\\n  dictionary with entries for \"ids\" and \"values\". Passing those two objects\\n  to this function along with vocab_size=6, will produce a `SparseTensor` that\\n  sparsely represents all three instances. Namely, the `indices` property will\\n  contain the coordinates of the non-zero entries in the feature matrix (the\\n  first dimension is the row number in the matrix, i.e., the index within the\\n  batch, and the second dimension is the column number, i.e., the feature id);\\n  `values` will contain the actual values. `shape` will be the shape of the\\n  original matrix, i.e., (3, 6). For our example above, the output will be\\n  equal to:\\n\\n  ```python\\n    SparseTensor(indices=[[0, 0], [1, 1], [1, 3], [1, 4], [2, 0], [2, 3]],\\n                 values=[-3, 1, 4, 1, 5, 9],\\n                 dense_shape=[3, 6])\\n  ```\\n\\n  This method generalizes to higher-dimensions by simply providing a list for\\n  both the sp_ids as well as the vocab_size.\\n  In this case the resulting `SparseTensor` has the following properties:\\n    - `indices` is equivalent to `sp_ids[0].indices` with the last\\n      dimension discarded and concatenated with\\n      `sp_ids[0].values, sp_ids[1].values, ...`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn] + vocab_size`.\\n\\n  Args:\\n    sp_ids: A single `SparseTensor` with `values` property of type `int32`\\n      or `int64` or a Python list of such `SparseTensor`s or a list thereof.\\n    sp_values: A `SparseTensor` of any type.\\n    vocab_size: A scalar `int64` Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_ids.values < vocab_size)`.\\n      Or a list thereof with `all(0 <= sp_ids[i].values < vocab_size[i])` for\\n      all `i`.\\n    name: A name prefix for the returned tensors (optional)\\n    already_sorted: A boolean to specify whether the per-batch values in\\n     `sp_values` are already sorted. If so skip sorting, False by default\\n     (optional).\\n\\n  Returns:\\n    A `SparseTensor` compactly representing a batch of feature ids and values,\\n    useful for passing to functions that expect such a `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_values` is not a `SparseTensor`. Or if `sp_ids` is neither\\n      a `SparseTensor` nor a list thereof. Or if `vocab_size` is not a\\n      `Tensor` or a Python int and `sp_ids` is a `SparseTensor`. Or if\\n      `vocab_size` is not a or list thereof and `sp_ids` is a list.\\n    ValueError: If `sp_ids` and `vocab_size` are lists of different lengths.\\n  '\n    return sparse_merge_impl(sp_ids, sp_values, vocab_size, name, already_sorted)",
            "@tf_export(v1=['sparse.merge', 'sparse_merge'])\n@deprecation.deprecated(None, 'No similar op available at this time.')\ndef sparse_merge(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Combines a batch of feature ids and values into a single `SparseTensor`.\\n\\n  The most common use case for this function occurs when feature ids and\\n  their corresponding values are stored in `Example` protos on disk.\\n  `parse_example` will return a batch of ids and a batch of values, and this\\n  function joins them into a single logical `SparseTensor` for use in\\n  functions such as `sparse_tensor_dense_matmul`, `sparse_to_dense`, etc.\\n\\n  The `SparseTensor` returned by this function has the following properties:\\n\\n    - `indices` is equivalent to `sp_ids.indices` with the last\\n      dimension discarded and replaced with `sp_ids.values`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn, vocab_size]`.\\n\\n  For example, consider the following feature vectors:\\n\\n  ```python\\n    vector1 = [-3, 0, 0, 0, 0, 0]\\n    vector2 = [ 0, 1, 0, 4, 1, 0]\\n    vector3 = [ 5, 0, 0, 9, 0, 0]\\n  ```\\n\\n  These might be stored sparsely in the following Example protos by storing\\n  only the feature ids (column number if the vectors are treated as a matrix)\\n  of the non-zero elements and the corresponding values:\\n\\n  ```python\\n    examples = [Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0])),\\n                    \"values\": Feature(float_list=FloatList(value=[-3]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[1, 4, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[1, 1, 4]))}),\\n                Example(features={\\n                    \"ids\": Feature(int64_list=Int64List(value=[0, 3])),\\n                    \"values\": Feature(float_list=FloatList(value=[5, 9]))})]\\n  ```\\n\\n  The result of calling parse_example on these examples will produce a\\n  dictionary with entries for \"ids\" and \"values\". Passing those two objects\\n  to this function along with vocab_size=6, will produce a `SparseTensor` that\\n  sparsely represents all three instances. Namely, the `indices` property will\\n  contain the coordinates of the non-zero entries in the feature matrix (the\\n  first dimension is the row number in the matrix, i.e., the index within the\\n  batch, and the second dimension is the column number, i.e., the feature id);\\n  `values` will contain the actual values. `shape` will be the shape of the\\n  original matrix, i.e., (3, 6). For our example above, the output will be\\n  equal to:\\n\\n  ```python\\n    SparseTensor(indices=[[0, 0], [1, 1], [1, 3], [1, 4], [2, 0], [2, 3]],\\n                 values=[-3, 1, 4, 1, 5, 9],\\n                 dense_shape=[3, 6])\\n  ```\\n\\n  This method generalizes to higher-dimensions by simply providing a list for\\n  both the sp_ids as well as the vocab_size.\\n  In this case the resulting `SparseTensor` has the following properties:\\n    - `indices` is equivalent to `sp_ids[0].indices` with the last\\n      dimension discarded and concatenated with\\n      `sp_ids[0].values, sp_ids[1].values, ...`.\\n    - `values` is simply `sp_values.values`.\\n    - If `sp_ids.dense_shape = [D0, D1, ..., Dn, K]`, then\\n      `output.shape = [D0, D1, ..., Dn] + vocab_size`.\\n\\n  Args:\\n    sp_ids: A single `SparseTensor` with `values` property of type `int32`\\n      or `int64` or a Python list of such `SparseTensor`s or a list thereof.\\n    sp_values: A `SparseTensor` of any type.\\n    vocab_size: A scalar `int64` Tensor (or Python int) containing the new size\\n      of the last dimension, `all(0 <= sp_ids.values < vocab_size)`.\\n      Or a list thereof with `all(0 <= sp_ids[i].values < vocab_size[i])` for\\n      all `i`.\\n    name: A name prefix for the returned tensors (optional)\\n    already_sorted: A boolean to specify whether the per-batch values in\\n     `sp_values` are already sorted. If so skip sorting, False by default\\n     (optional).\\n\\n  Returns:\\n    A `SparseTensor` compactly representing a batch of feature ids and values,\\n    useful for passing to functions that expect such a `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_values` is not a `SparseTensor`. Or if `sp_ids` is neither\\n      a `SparseTensor` nor a list thereof. Or if `vocab_size` is not a\\n      `Tensor` or a Python int and `sp_ids` is a `SparseTensor`. Or if\\n      `vocab_size` is not a or list thereof and `sp_ids` is a list.\\n    ValueError: If `sp_ids` and `vocab_size` are lists of different lengths.\\n  '\n    return sparse_merge_impl(sp_ids, sp_values, vocab_size, name, already_sorted)"
        ]
    },
    {
        "func_name": "sparse_merge_impl",
        "original": "def sparse_merge_impl(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    \"\"\"Internal implementation for sparse_merge to avoid deprecation warnings.\"\"\"\n    if isinstance(sp_ids, sparse_tensor.SparseTensorValue) or isinstance(sp_ids, sparse_tensor.SparseTensor):\n        sp_ids = [sp_ids]\n        if not (isinstance(vocab_size, tensor_lib.Tensor) or isinstance(vocab_size, numbers.Integral)):\n            raise TypeError('vocab_size has to be a Tensor or Python int. Found %s' % type(vocab_size))\n        vocab_size = [vocab_size]\n    else:\n        if not isinstance(sp_ids, collections_abc.Iterable):\n            raise TypeError('sp_ids has to be a SparseTensor or list thereof. Found %s' % type(sp_ids))\n        if not isinstance(vocab_size, collections_abc.Iterable):\n            raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(vocab_size))\n        for dim in vocab_size:\n            if not (isinstance(dim, tensor_lib.Tensor) or isinstance(dim, numbers.Integral)):\n                raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(dim))\n    if len(sp_ids) != len(vocab_size):\n        raise ValueError('sp_ids and vocab_size have to have equal lengths.')\n    with ops.name_scope(name, 'SparseMerge', [sp_ids, sp_values]):\n        sp_ids = [_convert_to_sparse_tensor(sp_ids_dim) for sp_ids_dim in sp_ids]\n        sp_values = _convert_to_sparse_tensor(sp_values)\n        ids = []\n        for sp_ids_dim in sp_ids:\n            ids_dim = sp_ids_dim.values\n            if sp_ids_dim.dtype != dtypes.int64:\n                ids_dim = math_ops.cast(ids_dim, dtypes.int64)\n            ids += [array_ops.expand_dims(ids_dim, axis=1)]\n        vocab_size = [math_ops.cast(x, dtypes.int64) for x in vocab_size]\n        indices_columns_to_preserve = sp_ids[0].indices[:, :-1]\n        new_indices = array_ops.concat([indices_columns_to_preserve] + ids, 1)\n        new_values = sp_values.values\n        new_shape = array_ops.concat([sp_ids[0].dense_shape[:-1], vocab_size], 0)\n        result = sparse_tensor.SparseTensor(new_indices, new_values, new_shape)\n        if already_sorted:\n            return result\n        sorted_result = sparse_reorder(result)\n        return sparse_tensor.SparseTensor(sorted_result.indices, sorted_result.values, new_shape)",
        "mutated": [
            "def sparse_merge_impl(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n    'Internal implementation for sparse_merge to avoid deprecation warnings.'\n    if isinstance(sp_ids, sparse_tensor.SparseTensorValue) or isinstance(sp_ids, sparse_tensor.SparseTensor):\n        sp_ids = [sp_ids]\n        if not (isinstance(vocab_size, tensor_lib.Tensor) or isinstance(vocab_size, numbers.Integral)):\n            raise TypeError('vocab_size has to be a Tensor or Python int. Found %s' % type(vocab_size))\n        vocab_size = [vocab_size]\n    else:\n        if not isinstance(sp_ids, collections_abc.Iterable):\n            raise TypeError('sp_ids has to be a SparseTensor or list thereof. Found %s' % type(sp_ids))\n        if not isinstance(vocab_size, collections_abc.Iterable):\n            raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(vocab_size))\n        for dim in vocab_size:\n            if not (isinstance(dim, tensor_lib.Tensor) or isinstance(dim, numbers.Integral)):\n                raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(dim))\n    if len(sp_ids) != len(vocab_size):\n        raise ValueError('sp_ids and vocab_size have to have equal lengths.')\n    with ops.name_scope(name, 'SparseMerge', [sp_ids, sp_values]):\n        sp_ids = [_convert_to_sparse_tensor(sp_ids_dim) for sp_ids_dim in sp_ids]\n        sp_values = _convert_to_sparse_tensor(sp_values)\n        ids = []\n        for sp_ids_dim in sp_ids:\n            ids_dim = sp_ids_dim.values\n            if sp_ids_dim.dtype != dtypes.int64:\n                ids_dim = math_ops.cast(ids_dim, dtypes.int64)\n            ids += [array_ops.expand_dims(ids_dim, axis=1)]\n        vocab_size = [math_ops.cast(x, dtypes.int64) for x in vocab_size]\n        indices_columns_to_preserve = sp_ids[0].indices[:, :-1]\n        new_indices = array_ops.concat([indices_columns_to_preserve] + ids, 1)\n        new_values = sp_values.values\n        new_shape = array_ops.concat([sp_ids[0].dense_shape[:-1], vocab_size], 0)\n        result = sparse_tensor.SparseTensor(new_indices, new_values, new_shape)\n        if already_sorted:\n            return result\n        sorted_result = sparse_reorder(result)\n        return sparse_tensor.SparseTensor(sorted_result.indices, sorted_result.values, new_shape)",
            "def sparse_merge_impl(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Internal implementation for sparse_merge to avoid deprecation warnings.'\n    if isinstance(sp_ids, sparse_tensor.SparseTensorValue) or isinstance(sp_ids, sparse_tensor.SparseTensor):\n        sp_ids = [sp_ids]\n        if not (isinstance(vocab_size, tensor_lib.Tensor) or isinstance(vocab_size, numbers.Integral)):\n            raise TypeError('vocab_size has to be a Tensor or Python int. Found %s' % type(vocab_size))\n        vocab_size = [vocab_size]\n    else:\n        if not isinstance(sp_ids, collections_abc.Iterable):\n            raise TypeError('sp_ids has to be a SparseTensor or list thereof. Found %s' % type(sp_ids))\n        if not isinstance(vocab_size, collections_abc.Iterable):\n            raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(vocab_size))\n        for dim in vocab_size:\n            if not (isinstance(dim, tensor_lib.Tensor) or isinstance(dim, numbers.Integral)):\n                raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(dim))\n    if len(sp_ids) != len(vocab_size):\n        raise ValueError('sp_ids and vocab_size have to have equal lengths.')\n    with ops.name_scope(name, 'SparseMerge', [sp_ids, sp_values]):\n        sp_ids = [_convert_to_sparse_tensor(sp_ids_dim) for sp_ids_dim in sp_ids]\n        sp_values = _convert_to_sparse_tensor(sp_values)\n        ids = []\n        for sp_ids_dim in sp_ids:\n            ids_dim = sp_ids_dim.values\n            if sp_ids_dim.dtype != dtypes.int64:\n                ids_dim = math_ops.cast(ids_dim, dtypes.int64)\n            ids += [array_ops.expand_dims(ids_dim, axis=1)]\n        vocab_size = [math_ops.cast(x, dtypes.int64) for x in vocab_size]\n        indices_columns_to_preserve = sp_ids[0].indices[:, :-1]\n        new_indices = array_ops.concat([indices_columns_to_preserve] + ids, 1)\n        new_values = sp_values.values\n        new_shape = array_ops.concat([sp_ids[0].dense_shape[:-1], vocab_size], 0)\n        result = sparse_tensor.SparseTensor(new_indices, new_values, new_shape)\n        if already_sorted:\n            return result\n        sorted_result = sparse_reorder(result)\n        return sparse_tensor.SparseTensor(sorted_result.indices, sorted_result.values, new_shape)",
            "def sparse_merge_impl(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Internal implementation for sparse_merge to avoid deprecation warnings.'\n    if isinstance(sp_ids, sparse_tensor.SparseTensorValue) or isinstance(sp_ids, sparse_tensor.SparseTensor):\n        sp_ids = [sp_ids]\n        if not (isinstance(vocab_size, tensor_lib.Tensor) or isinstance(vocab_size, numbers.Integral)):\n            raise TypeError('vocab_size has to be a Tensor or Python int. Found %s' % type(vocab_size))\n        vocab_size = [vocab_size]\n    else:\n        if not isinstance(sp_ids, collections_abc.Iterable):\n            raise TypeError('sp_ids has to be a SparseTensor or list thereof. Found %s' % type(sp_ids))\n        if not isinstance(vocab_size, collections_abc.Iterable):\n            raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(vocab_size))\n        for dim in vocab_size:\n            if not (isinstance(dim, tensor_lib.Tensor) or isinstance(dim, numbers.Integral)):\n                raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(dim))\n    if len(sp_ids) != len(vocab_size):\n        raise ValueError('sp_ids and vocab_size have to have equal lengths.')\n    with ops.name_scope(name, 'SparseMerge', [sp_ids, sp_values]):\n        sp_ids = [_convert_to_sparse_tensor(sp_ids_dim) for sp_ids_dim in sp_ids]\n        sp_values = _convert_to_sparse_tensor(sp_values)\n        ids = []\n        for sp_ids_dim in sp_ids:\n            ids_dim = sp_ids_dim.values\n            if sp_ids_dim.dtype != dtypes.int64:\n                ids_dim = math_ops.cast(ids_dim, dtypes.int64)\n            ids += [array_ops.expand_dims(ids_dim, axis=1)]\n        vocab_size = [math_ops.cast(x, dtypes.int64) for x in vocab_size]\n        indices_columns_to_preserve = sp_ids[0].indices[:, :-1]\n        new_indices = array_ops.concat([indices_columns_to_preserve] + ids, 1)\n        new_values = sp_values.values\n        new_shape = array_ops.concat([sp_ids[0].dense_shape[:-1], vocab_size], 0)\n        result = sparse_tensor.SparseTensor(new_indices, new_values, new_shape)\n        if already_sorted:\n            return result\n        sorted_result = sparse_reorder(result)\n        return sparse_tensor.SparseTensor(sorted_result.indices, sorted_result.values, new_shape)",
            "def sparse_merge_impl(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Internal implementation for sparse_merge to avoid deprecation warnings.'\n    if isinstance(sp_ids, sparse_tensor.SparseTensorValue) or isinstance(sp_ids, sparse_tensor.SparseTensor):\n        sp_ids = [sp_ids]\n        if not (isinstance(vocab_size, tensor_lib.Tensor) or isinstance(vocab_size, numbers.Integral)):\n            raise TypeError('vocab_size has to be a Tensor or Python int. Found %s' % type(vocab_size))\n        vocab_size = [vocab_size]\n    else:\n        if not isinstance(sp_ids, collections_abc.Iterable):\n            raise TypeError('sp_ids has to be a SparseTensor or list thereof. Found %s' % type(sp_ids))\n        if not isinstance(vocab_size, collections_abc.Iterable):\n            raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(vocab_size))\n        for dim in vocab_size:\n            if not (isinstance(dim, tensor_lib.Tensor) or isinstance(dim, numbers.Integral)):\n                raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(dim))\n    if len(sp_ids) != len(vocab_size):\n        raise ValueError('sp_ids and vocab_size have to have equal lengths.')\n    with ops.name_scope(name, 'SparseMerge', [sp_ids, sp_values]):\n        sp_ids = [_convert_to_sparse_tensor(sp_ids_dim) for sp_ids_dim in sp_ids]\n        sp_values = _convert_to_sparse_tensor(sp_values)\n        ids = []\n        for sp_ids_dim in sp_ids:\n            ids_dim = sp_ids_dim.values\n            if sp_ids_dim.dtype != dtypes.int64:\n                ids_dim = math_ops.cast(ids_dim, dtypes.int64)\n            ids += [array_ops.expand_dims(ids_dim, axis=1)]\n        vocab_size = [math_ops.cast(x, dtypes.int64) for x in vocab_size]\n        indices_columns_to_preserve = sp_ids[0].indices[:, :-1]\n        new_indices = array_ops.concat([indices_columns_to_preserve] + ids, 1)\n        new_values = sp_values.values\n        new_shape = array_ops.concat([sp_ids[0].dense_shape[:-1], vocab_size], 0)\n        result = sparse_tensor.SparseTensor(new_indices, new_values, new_shape)\n        if already_sorted:\n            return result\n        sorted_result = sparse_reorder(result)\n        return sparse_tensor.SparseTensor(sorted_result.indices, sorted_result.values, new_shape)",
            "def sparse_merge_impl(sp_ids, sp_values, vocab_size, name=None, already_sorted=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Internal implementation for sparse_merge to avoid deprecation warnings.'\n    if isinstance(sp_ids, sparse_tensor.SparseTensorValue) or isinstance(sp_ids, sparse_tensor.SparseTensor):\n        sp_ids = [sp_ids]\n        if not (isinstance(vocab_size, tensor_lib.Tensor) or isinstance(vocab_size, numbers.Integral)):\n            raise TypeError('vocab_size has to be a Tensor or Python int. Found %s' % type(vocab_size))\n        vocab_size = [vocab_size]\n    else:\n        if not isinstance(sp_ids, collections_abc.Iterable):\n            raise TypeError('sp_ids has to be a SparseTensor or list thereof. Found %s' % type(sp_ids))\n        if not isinstance(vocab_size, collections_abc.Iterable):\n            raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(vocab_size))\n        for dim in vocab_size:\n            if not (isinstance(dim, tensor_lib.Tensor) or isinstance(dim, numbers.Integral)):\n                raise TypeError('vocab_size has to be a list of Tensors or Python ints. Found %s' % type(dim))\n    if len(sp_ids) != len(vocab_size):\n        raise ValueError('sp_ids and vocab_size have to have equal lengths.')\n    with ops.name_scope(name, 'SparseMerge', [sp_ids, sp_values]):\n        sp_ids = [_convert_to_sparse_tensor(sp_ids_dim) for sp_ids_dim in sp_ids]\n        sp_values = _convert_to_sparse_tensor(sp_values)\n        ids = []\n        for sp_ids_dim in sp_ids:\n            ids_dim = sp_ids_dim.values\n            if sp_ids_dim.dtype != dtypes.int64:\n                ids_dim = math_ops.cast(ids_dim, dtypes.int64)\n            ids += [array_ops.expand_dims(ids_dim, axis=1)]\n        vocab_size = [math_ops.cast(x, dtypes.int64) for x in vocab_size]\n        indices_columns_to_preserve = sp_ids[0].indices[:, :-1]\n        new_indices = array_ops.concat([indices_columns_to_preserve] + ids, 1)\n        new_values = sp_values.values\n        new_shape = array_ops.concat([sp_ids[0].dense_shape[:-1], vocab_size], 0)\n        result = sparse_tensor.SparseTensor(new_indices, new_values, new_shape)\n        if already_sorted:\n            return result\n        sorted_result = sparse_reorder(result)\n        return sparse_tensor.SparseTensor(sorted_result.indices, sorted_result.values, new_shape)"
        ]
    },
    {
        "func_name": "sparse_retain",
        "original": "@tf_export('sparse.retain', v1=['sparse.retain', 'sparse_retain'])\n@deprecation.deprecated_endpoints('sparse_retain')\ndef sparse_retain(sp_input, to_retain):\n    \"\"\"Retains specified non-empty values within a `SparseTensor`.\n\n  For example, if `sp_input` has shape `[4, 5]` and 4 non-empty string values:\n\n      [0, 1]: a\n      [0, 3]: b\n      [2, 0]: c\n      [3, 1]: d\n\n  and `to_retain = [True, False, False, True]`, then the output will\n  be a `SparseTensor` of shape `[4, 5]` with 2 non-empty values:\n\n      [0, 1]: a\n      [3, 1]: d\n\n  Args:\n    sp_input: The input `SparseTensor` with `N` non-empty elements.\n    to_retain: A bool vector of length `N` with `M` true values.\n\n  Returns:\n    A `SparseTensor` with the same shape as the input and `M` non-empty\n    elements corresponding to the true positions in `to_retain`.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    to_retain = ops.convert_to_tensor(to_retain)\n    retain_shape = to_retain.get_shape()\n    retain_shape.assert_has_rank(1)\n    if sp_input.values.get_shape().dims is not None:\n        sp_input.values.get_shape().dims[0].assert_is_compatible_with(tensor_shape.dimension_at_index(retain_shape, 0))\n    where_true = array_ops.reshape(array_ops.where_v2(to_retain), [-1])\n    new_indices = array_ops.gather(sp_input.indices, where_true)\n    new_values = array_ops.gather(sp_input.values, where_true)\n    return sparse_tensor.SparseTensor(new_indices, new_values, array_ops.identity(sp_input.dense_shape))",
        "mutated": [
            "@tf_export('sparse.retain', v1=['sparse.retain', 'sparse_retain'])\n@deprecation.deprecated_endpoints('sparse_retain')\ndef sparse_retain(sp_input, to_retain):\n    if False:\n        i = 10\n    'Retains specified non-empty values within a `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and 4 non-empty string values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  and `to_retain = [True, False, False, True]`, then the output will\\n  be a `SparseTensor` of shape `[4, 5]` with 2 non-empty values:\\n\\n      [0, 1]: a\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor` with `N` non-empty elements.\\n    to_retain: A bool vector of length `N` with `M` true values.\\n\\n  Returns:\\n    A `SparseTensor` with the same shape as the input and `M` non-empty\\n    elements corresponding to the true positions in `to_retain`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    to_retain = ops.convert_to_tensor(to_retain)\n    retain_shape = to_retain.get_shape()\n    retain_shape.assert_has_rank(1)\n    if sp_input.values.get_shape().dims is not None:\n        sp_input.values.get_shape().dims[0].assert_is_compatible_with(tensor_shape.dimension_at_index(retain_shape, 0))\n    where_true = array_ops.reshape(array_ops.where_v2(to_retain), [-1])\n    new_indices = array_ops.gather(sp_input.indices, where_true)\n    new_values = array_ops.gather(sp_input.values, where_true)\n    return sparse_tensor.SparseTensor(new_indices, new_values, array_ops.identity(sp_input.dense_shape))",
            "@tf_export('sparse.retain', v1=['sparse.retain', 'sparse_retain'])\n@deprecation.deprecated_endpoints('sparse_retain')\ndef sparse_retain(sp_input, to_retain):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Retains specified non-empty values within a `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and 4 non-empty string values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  and `to_retain = [True, False, False, True]`, then the output will\\n  be a `SparseTensor` of shape `[4, 5]` with 2 non-empty values:\\n\\n      [0, 1]: a\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor` with `N` non-empty elements.\\n    to_retain: A bool vector of length `N` with `M` true values.\\n\\n  Returns:\\n    A `SparseTensor` with the same shape as the input and `M` non-empty\\n    elements corresponding to the true positions in `to_retain`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    to_retain = ops.convert_to_tensor(to_retain)\n    retain_shape = to_retain.get_shape()\n    retain_shape.assert_has_rank(1)\n    if sp_input.values.get_shape().dims is not None:\n        sp_input.values.get_shape().dims[0].assert_is_compatible_with(tensor_shape.dimension_at_index(retain_shape, 0))\n    where_true = array_ops.reshape(array_ops.where_v2(to_retain), [-1])\n    new_indices = array_ops.gather(sp_input.indices, where_true)\n    new_values = array_ops.gather(sp_input.values, where_true)\n    return sparse_tensor.SparseTensor(new_indices, new_values, array_ops.identity(sp_input.dense_shape))",
            "@tf_export('sparse.retain', v1=['sparse.retain', 'sparse_retain'])\n@deprecation.deprecated_endpoints('sparse_retain')\ndef sparse_retain(sp_input, to_retain):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Retains specified non-empty values within a `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and 4 non-empty string values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  and `to_retain = [True, False, False, True]`, then the output will\\n  be a `SparseTensor` of shape `[4, 5]` with 2 non-empty values:\\n\\n      [0, 1]: a\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor` with `N` non-empty elements.\\n    to_retain: A bool vector of length `N` with `M` true values.\\n\\n  Returns:\\n    A `SparseTensor` with the same shape as the input and `M` non-empty\\n    elements corresponding to the true positions in `to_retain`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    to_retain = ops.convert_to_tensor(to_retain)\n    retain_shape = to_retain.get_shape()\n    retain_shape.assert_has_rank(1)\n    if sp_input.values.get_shape().dims is not None:\n        sp_input.values.get_shape().dims[0].assert_is_compatible_with(tensor_shape.dimension_at_index(retain_shape, 0))\n    where_true = array_ops.reshape(array_ops.where_v2(to_retain), [-1])\n    new_indices = array_ops.gather(sp_input.indices, where_true)\n    new_values = array_ops.gather(sp_input.values, where_true)\n    return sparse_tensor.SparseTensor(new_indices, new_values, array_ops.identity(sp_input.dense_shape))",
            "@tf_export('sparse.retain', v1=['sparse.retain', 'sparse_retain'])\n@deprecation.deprecated_endpoints('sparse_retain')\ndef sparse_retain(sp_input, to_retain):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Retains specified non-empty values within a `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and 4 non-empty string values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  and `to_retain = [True, False, False, True]`, then the output will\\n  be a `SparseTensor` of shape `[4, 5]` with 2 non-empty values:\\n\\n      [0, 1]: a\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor` with `N` non-empty elements.\\n    to_retain: A bool vector of length `N` with `M` true values.\\n\\n  Returns:\\n    A `SparseTensor` with the same shape as the input and `M` non-empty\\n    elements corresponding to the true positions in `to_retain`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    to_retain = ops.convert_to_tensor(to_retain)\n    retain_shape = to_retain.get_shape()\n    retain_shape.assert_has_rank(1)\n    if sp_input.values.get_shape().dims is not None:\n        sp_input.values.get_shape().dims[0].assert_is_compatible_with(tensor_shape.dimension_at_index(retain_shape, 0))\n    where_true = array_ops.reshape(array_ops.where_v2(to_retain), [-1])\n    new_indices = array_ops.gather(sp_input.indices, where_true)\n    new_values = array_ops.gather(sp_input.values, where_true)\n    return sparse_tensor.SparseTensor(new_indices, new_values, array_ops.identity(sp_input.dense_shape))",
            "@tf_export('sparse.retain', v1=['sparse.retain', 'sparse_retain'])\n@deprecation.deprecated_endpoints('sparse_retain')\ndef sparse_retain(sp_input, to_retain):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Retains specified non-empty values within a `SparseTensor`.\\n\\n  For example, if `sp_input` has shape `[4, 5]` and 4 non-empty string values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  and `to_retain = [True, False, False, True]`, then the output will\\n  be a `SparseTensor` of shape `[4, 5]` with 2 non-empty values:\\n\\n      [0, 1]: a\\n      [3, 1]: d\\n\\n  Args:\\n    sp_input: The input `SparseTensor` with `N` non-empty elements.\\n    to_retain: A bool vector of length `N` with `M` true values.\\n\\n  Returns:\\n    A `SparseTensor` with the same shape as the input and `M` non-empty\\n    elements corresponding to the true positions in `to_retain`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    to_retain = ops.convert_to_tensor(to_retain)\n    retain_shape = to_retain.get_shape()\n    retain_shape.assert_has_rank(1)\n    if sp_input.values.get_shape().dims is not None:\n        sp_input.values.get_shape().dims[0].assert_is_compatible_with(tensor_shape.dimension_at_index(retain_shape, 0))\n    where_true = array_ops.reshape(array_ops.where_v2(to_retain), [-1])\n    new_indices = array_ops.gather(sp_input.indices, where_true)\n    new_values = array_ops.gather(sp_input.values, where_true)\n    return sparse_tensor.SparseTensor(new_indices, new_values, array_ops.identity(sp_input.dense_shape))"
        ]
    },
    {
        "func_name": "sparse_reset_shape",
        "original": "@tf_export('sparse.reset_shape', v1=['sparse.reset_shape', 'sparse_reset_shape'])\n@deprecation.deprecated_endpoints('sparse_reset_shape')\ndef sparse_reset_shape(sp_input, new_shape=None):\n    \"\"\"Resets the shape of a `SparseTensor` with indices and values unchanged.\n\n  If `new_shape` is None, returns a copy of `sp_input` with its shape reset\n  to the tight bounding box of `sp_input`. This will be a shape consisting of\n  all zeros if sp_input has no values.\n\n  If `new_shape` is provided, then it must be larger or equal in all dimensions\n  compared to the shape of `sp_input`. When this condition is met, the returned\n  SparseTensor will have its shape reset to `new_shape` and its indices and\n  values unchanged from that of `sp_input.`\n\n  For example:\n\n    Consider a `sp_input` with shape [2, 3, 5]:\n\n      [0, 0, 1]: a\n      [0, 1, 0]: b\n      [0, 2, 2]: c\n      [1, 0, 3]: d\n\n    - It is an error to set `new_shape` as [3, 7] since this represents a\n      rank-2 tensor while `sp_input` is rank-3. This is either a ValueError\n      during graph construction (if both shapes are known) or an OpError during\n      run time.\n\n    - Setting `new_shape` as [2, 3, 6] will be fine as this shape is larger or\n      equal in every dimension compared to the original shape [2, 3, 5].\n\n    - On the other hand, setting new_shape as [2, 3, 4] is also an error: The\n      third dimension is smaller than the original shape [2, 3, 5] (and an\n      `InvalidArgumentError` will be raised).\n\n    - If `new_shape` is None, the returned SparseTensor will have a shape\n      [2, 3, 4], which is the tight bounding box of `sp_input`.\n\n  Args:\n    sp_input: The input `SparseTensor`.\n    new_shape: None or a vector representing the new shape for the returned\n      `SparseTensor`.\n\n  Returns:\n    A `SparseTensor` indices and values unchanged from `sp_input`. Its shape is\n      `new_shape` if that is set. Otherwise it is the tight bounding box of\n       `sp_input`\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n    ValueError: If `new_shape` represents a tensor with a different rank from\n      that of `sp_input` (if shapes are known when graph is constructed).\n    ValueError:  If `new_shape` is determined during graph build to have\n      dimension sizes that are too small.\n    OpError:\n      - If `new_shape` has dimension sizes that are too small.\n      - If shapes are not known during graph construction time, and during run\n        time it is found out that the ranks do not match.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    in_indices = array_ops.identity(sp_input.indices)\n    in_values = array_ops.identity(sp_input.values)\n    in_shape = array_ops.identity(sp_input.dense_shape)\n    if new_shape is None:\n        dim_low_bound = math_ops.reduce_max(in_indices, axis=0)\n        output_shape_tensor = math_ops.maximum(array_ops.constant(0, dtype=dtypes.int64), math_ops.add(dim_low_bound, array_ops.ones_like(in_shape)))\n    else:\n        output_shape_tensor = ops.convert_to_tensor(new_shape)\n        output_shape_tensor.get_shape().assert_has_rank(1)\n        output_shape_tensor = math_ops.cast(output_shape_tensor, dtypes.int64)\n        if output_shape_tensor.get_shape().rank is not None:\n            output_shape_tensor.get_shape().dims[0].assert_is_compatible_with(in_shape.get_shape().dims[0])\n        output_shape_tensor_const = tensor_util.constant_value(output_shape_tensor)\n        if output_shape_tensor_const is not None and sp_input.get_shape().is_fully_defined():\n            in_shape_const = np.array(sp_input.get_shape().as_list())\n            if not np.all(in_shape_const <= output_shape_tensor_const):\n                raise ValueError('Requested new_shape should have dimension sizes >= sp_input.shape.  Found new_shape (%s), sp_input.shape (%s).' % (in_shape_const, output_shape_tensor_const))\n            output_shape_tensor = output_shape_tensor_const\n        else:\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_equal(array_ops.shape(in_shape), array_ops.shape(output_shape_tensor))], output_shape_tensor)\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_less_equal(in_shape, output_shape_tensor)], output_shape_tensor)\n    return sparse_tensor.SparseTensor(in_indices, in_values, output_shape_tensor)",
        "mutated": [
            "@tf_export('sparse.reset_shape', v1=['sparse.reset_shape', 'sparse_reset_shape'])\n@deprecation.deprecated_endpoints('sparse_reset_shape')\ndef sparse_reset_shape(sp_input, new_shape=None):\n    if False:\n        i = 10\n    'Resets the shape of a `SparseTensor` with indices and values unchanged.\\n\\n  If `new_shape` is None, returns a copy of `sp_input` with its shape reset\\n  to the tight bounding box of `sp_input`. This will be a shape consisting of\\n  all zeros if sp_input has no values.\\n\\n  If `new_shape` is provided, then it must be larger or equal in all dimensions\\n  compared to the shape of `sp_input`. When this condition is met, the returned\\n  SparseTensor will have its shape reset to `new_shape` and its indices and\\n  values unchanged from that of `sp_input.`\\n\\n  For example:\\n\\n    Consider a `sp_input` with shape [2, 3, 5]:\\n\\n      [0, 0, 1]: a\\n      [0, 1, 0]: b\\n      [0, 2, 2]: c\\n      [1, 0, 3]: d\\n\\n    - It is an error to set `new_shape` as [3, 7] since this represents a\\n      rank-2 tensor while `sp_input` is rank-3. This is either a ValueError\\n      during graph construction (if both shapes are known) or an OpError during\\n      run time.\\n\\n    - Setting `new_shape` as [2, 3, 6] will be fine as this shape is larger or\\n      equal in every dimension compared to the original shape [2, 3, 5].\\n\\n    - On the other hand, setting new_shape as [2, 3, 4] is also an error: The\\n      third dimension is smaller than the original shape [2, 3, 5] (and an\\n      `InvalidArgumentError` will be raised).\\n\\n    - If `new_shape` is None, the returned SparseTensor will have a shape\\n      [2, 3, 4], which is the tight bounding box of `sp_input`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    new_shape: None or a vector representing the new shape for the returned\\n      `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` indices and values unchanged from `sp_input`. Its shape is\\n      `new_shape` if that is set. Otherwise it is the tight bounding box of\\n       `sp_input`\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If `new_shape` represents a tensor with a different rank from\\n      that of `sp_input` (if shapes are known when graph is constructed).\\n    ValueError:  If `new_shape` is determined during graph build to have\\n      dimension sizes that are too small.\\n    OpError:\\n      - If `new_shape` has dimension sizes that are too small.\\n      - If shapes are not known during graph construction time, and during run\\n        time it is found out that the ranks do not match.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    in_indices = array_ops.identity(sp_input.indices)\n    in_values = array_ops.identity(sp_input.values)\n    in_shape = array_ops.identity(sp_input.dense_shape)\n    if new_shape is None:\n        dim_low_bound = math_ops.reduce_max(in_indices, axis=0)\n        output_shape_tensor = math_ops.maximum(array_ops.constant(0, dtype=dtypes.int64), math_ops.add(dim_low_bound, array_ops.ones_like(in_shape)))\n    else:\n        output_shape_tensor = ops.convert_to_tensor(new_shape)\n        output_shape_tensor.get_shape().assert_has_rank(1)\n        output_shape_tensor = math_ops.cast(output_shape_tensor, dtypes.int64)\n        if output_shape_tensor.get_shape().rank is not None:\n            output_shape_tensor.get_shape().dims[0].assert_is_compatible_with(in_shape.get_shape().dims[0])\n        output_shape_tensor_const = tensor_util.constant_value(output_shape_tensor)\n        if output_shape_tensor_const is not None and sp_input.get_shape().is_fully_defined():\n            in_shape_const = np.array(sp_input.get_shape().as_list())\n            if not np.all(in_shape_const <= output_shape_tensor_const):\n                raise ValueError('Requested new_shape should have dimension sizes >= sp_input.shape.  Found new_shape (%s), sp_input.shape (%s).' % (in_shape_const, output_shape_tensor_const))\n            output_shape_tensor = output_shape_tensor_const\n        else:\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_equal(array_ops.shape(in_shape), array_ops.shape(output_shape_tensor))], output_shape_tensor)\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_less_equal(in_shape, output_shape_tensor)], output_shape_tensor)\n    return sparse_tensor.SparseTensor(in_indices, in_values, output_shape_tensor)",
            "@tf_export('sparse.reset_shape', v1=['sparse.reset_shape', 'sparse_reset_shape'])\n@deprecation.deprecated_endpoints('sparse_reset_shape')\ndef sparse_reset_shape(sp_input, new_shape=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Resets the shape of a `SparseTensor` with indices and values unchanged.\\n\\n  If `new_shape` is None, returns a copy of `sp_input` with its shape reset\\n  to the tight bounding box of `sp_input`. This will be a shape consisting of\\n  all zeros if sp_input has no values.\\n\\n  If `new_shape` is provided, then it must be larger or equal in all dimensions\\n  compared to the shape of `sp_input`. When this condition is met, the returned\\n  SparseTensor will have its shape reset to `new_shape` and its indices and\\n  values unchanged from that of `sp_input.`\\n\\n  For example:\\n\\n    Consider a `sp_input` with shape [2, 3, 5]:\\n\\n      [0, 0, 1]: a\\n      [0, 1, 0]: b\\n      [0, 2, 2]: c\\n      [1, 0, 3]: d\\n\\n    - It is an error to set `new_shape` as [3, 7] since this represents a\\n      rank-2 tensor while `sp_input` is rank-3. This is either a ValueError\\n      during graph construction (if both shapes are known) or an OpError during\\n      run time.\\n\\n    - Setting `new_shape` as [2, 3, 6] will be fine as this shape is larger or\\n      equal in every dimension compared to the original shape [2, 3, 5].\\n\\n    - On the other hand, setting new_shape as [2, 3, 4] is also an error: The\\n      third dimension is smaller than the original shape [2, 3, 5] (and an\\n      `InvalidArgumentError` will be raised).\\n\\n    - If `new_shape` is None, the returned SparseTensor will have a shape\\n      [2, 3, 4], which is the tight bounding box of `sp_input`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    new_shape: None or a vector representing the new shape for the returned\\n      `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` indices and values unchanged from `sp_input`. Its shape is\\n      `new_shape` if that is set. Otherwise it is the tight bounding box of\\n       `sp_input`\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If `new_shape` represents a tensor with a different rank from\\n      that of `sp_input` (if shapes are known when graph is constructed).\\n    ValueError:  If `new_shape` is determined during graph build to have\\n      dimension sizes that are too small.\\n    OpError:\\n      - If `new_shape` has dimension sizes that are too small.\\n      - If shapes are not known during graph construction time, and during run\\n        time it is found out that the ranks do not match.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    in_indices = array_ops.identity(sp_input.indices)\n    in_values = array_ops.identity(sp_input.values)\n    in_shape = array_ops.identity(sp_input.dense_shape)\n    if new_shape is None:\n        dim_low_bound = math_ops.reduce_max(in_indices, axis=0)\n        output_shape_tensor = math_ops.maximum(array_ops.constant(0, dtype=dtypes.int64), math_ops.add(dim_low_bound, array_ops.ones_like(in_shape)))\n    else:\n        output_shape_tensor = ops.convert_to_tensor(new_shape)\n        output_shape_tensor.get_shape().assert_has_rank(1)\n        output_shape_tensor = math_ops.cast(output_shape_tensor, dtypes.int64)\n        if output_shape_tensor.get_shape().rank is not None:\n            output_shape_tensor.get_shape().dims[0].assert_is_compatible_with(in_shape.get_shape().dims[0])\n        output_shape_tensor_const = tensor_util.constant_value(output_shape_tensor)\n        if output_shape_tensor_const is not None and sp_input.get_shape().is_fully_defined():\n            in_shape_const = np.array(sp_input.get_shape().as_list())\n            if not np.all(in_shape_const <= output_shape_tensor_const):\n                raise ValueError('Requested new_shape should have dimension sizes >= sp_input.shape.  Found new_shape (%s), sp_input.shape (%s).' % (in_shape_const, output_shape_tensor_const))\n            output_shape_tensor = output_shape_tensor_const\n        else:\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_equal(array_ops.shape(in_shape), array_ops.shape(output_shape_tensor))], output_shape_tensor)\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_less_equal(in_shape, output_shape_tensor)], output_shape_tensor)\n    return sparse_tensor.SparseTensor(in_indices, in_values, output_shape_tensor)",
            "@tf_export('sparse.reset_shape', v1=['sparse.reset_shape', 'sparse_reset_shape'])\n@deprecation.deprecated_endpoints('sparse_reset_shape')\ndef sparse_reset_shape(sp_input, new_shape=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Resets the shape of a `SparseTensor` with indices and values unchanged.\\n\\n  If `new_shape` is None, returns a copy of `sp_input` with its shape reset\\n  to the tight bounding box of `sp_input`. This will be a shape consisting of\\n  all zeros if sp_input has no values.\\n\\n  If `new_shape` is provided, then it must be larger or equal in all dimensions\\n  compared to the shape of `sp_input`. When this condition is met, the returned\\n  SparseTensor will have its shape reset to `new_shape` and its indices and\\n  values unchanged from that of `sp_input.`\\n\\n  For example:\\n\\n    Consider a `sp_input` with shape [2, 3, 5]:\\n\\n      [0, 0, 1]: a\\n      [0, 1, 0]: b\\n      [0, 2, 2]: c\\n      [1, 0, 3]: d\\n\\n    - It is an error to set `new_shape` as [3, 7] since this represents a\\n      rank-2 tensor while `sp_input` is rank-3. This is either a ValueError\\n      during graph construction (if both shapes are known) or an OpError during\\n      run time.\\n\\n    - Setting `new_shape` as [2, 3, 6] will be fine as this shape is larger or\\n      equal in every dimension compared to the original shape [2, 3, 5].\\n\\n    - On the other hand, setting new_shape as [2, 3, 4] is also an error: The\\n      third dimension is smaller than the original shape [2, 3, 5] (and an\\n      `InvalidArgumentError` will be raised).\\n\\n    - If `new_shape` is None, the returned SparseTensor will have a shape\\n      [2, 3, 4], which is the tight bounding box of `sp_input`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    new_shape: None or a vector representing the new shape for the returned\\n      `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` indices and values unchanged from `sp_input`. Its shape is\\n      `new_shape` if that is set. Otherwise it is the tight bounding box of\\n       `sp_input`\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If `new_shape` represents a tensor with a different rank from\\n      that of `sp_input` (if shapes are known when graph is constructed).\\n    ValueError:  If `new_shape` is determined during graph build to have\\n      dimension sizes that are too small.\\n    OpError:\\n      - If `new_shape` has dimension sizes that are too small.\\n      - If shapes are not known during graph construction time, and during run\\n        time it is found out that the ranks do not match.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    in_indices = array_ops.identity(sp_input.indices)\n    in_values = array_ops.identity(sp_input.values)\n    in_shape = array_ops.identity(sp_input.dense_shape)\n    if new_shape is None:\n        dim_low_bound = math_ops.reduce_max(in_indices, axis=0)\n        output_shape_tensor = math_ops.maximum(array_ops.constant(0, dtype=dtypes.int64), math_ops.add(dim_low_bound, array_ops.ones_like(in_shape)))\n    else:\n        output_shape_tensor = ops.convert_to_tensor(new_shape)\n        output_shape_tensor.get_shape().assert_has_rank(1)\n        output_shape_tensor = math_ops.cast(output_shape_tensor, dtypes.int64)\n        if output_shape_tensor.get_shape().rank is not None:\n            output_shape_tensor.get_shape().dims[0].assert_is_compatible_with(in_shape.get_shape().dims[0])\n        output_shape_tensor_const = tensor_util.constant_value(output_shape_tensor)\n        if output_shape_tensor_const is not None and sp_input.get_shape().is_fully_defined():\n            in_shape_const = np.array(sp_input.get_shape().as_list())\n            if not np.all(in_shape_const <= output_shape_tensor_const):\n                raise ValueError('Requested new_shape should have dimension sizes >= sp_input.shape.  Found new_shape (%s), sp_input.shape (%s).' % (in_shape_const, output_shape_tensor_const))\n            output_shape_tensor = output_shape_tensor_const\n        else:\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_equal(array_ops.shape(in_shape), array_ops.shape(output_shape_tensor))], output_shape_tensor)\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_less_equal(in_shape, output_shape_tensor)], output_shape_tensor)\n    return sparse_tensor.SparseTensor(in_indices, in_values, output_shape_tensor)",
            "@tf_export('sparse.reset_shape', v1=['sparse.reset_shape', 'sparse_reset_shape'])\n@deprecation.deprecated_endpoints('sparse_reset_shape')\ndef sparse_reset_shape(sp_input, new_shape=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Resets the shape of a `SparseTensor` with indices and values unchanged.\\n\\n  If `new_shape` is None, returns a copy of `sp_input` with its shape reset\\n  to the tight bounding box of `sp_input`. This will be a shape consisting of\\n  all zeros if sp_input has no values.\\n\\n  If `new_shape` is provided, then it must be larger or equal in all dimensions\\n  compared to the shape of `sp_input`. When this condition is met, the returned\\n  SparseTensor will have its shape reset to `new_shape` and its indices and\\n  values unchanged from that of `sp_input.`\\n\\n  For example:\\n\\n    Consider a `sp_input` with shape [2, 3, 5]:\\n\\n      [0, 0, 1]: a\\n      [0, 1, 0]: b\\n      [0, 2, 2]: c\\n      [1, 0, 3]: d\\n\\n    - It is an error to set `new_shape` as [3, 7] since this represents a\\n      rank-2 tensor while `sp_input` is rank-3. This is either a ValueError\\n      during graph construction (if both shapes are known) or an OpError during\\n      run time.\\n\\n    - Setting `new_shape` as [2, 3, 6] will be fine as this shape is larger or\\n      equal in every dimension compared to the original shape [2, 3, 5].\\n\\n    - On the other hand, setting new_shape as [2, 3, 4] is also an error: The\\n      third dimension is smaller than the original shape [2, 3, 5] (and an\\n      `InvalidArgumentError` will be raised).\\n\\n    - If `new_shape` is None, the returned SparseTensor will have a shape\\n      [2, 3, 4], which is the tight bounding box of `sp_input`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    new_shape: None or a vector representing the new shape for the returned\\n      `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` indices and values unchanged from `sp_input`. Its shape is\\n      `new_shape` if that is set. Otherwise it is the tight bounding box of\\n       `sp_input`\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If `new_shape` represents a tensor with a different rank from\\n      that of `sp_input` (if shapes are known when graph is constructed).\\n    ValueError:  If `new_shape` is determined during graph build to have\\n      dimension sizes that are too small.\\n    OpError:\\n      - If `new_shape` has dimension sizes that are too small.\\n      - If shapes are not known during graph construction time, and during run\\n        time it is found out that the ranks do not match.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    in_indices = array_ops.identity(sp_input.indices)\n    in_values = array_ops.identity(sp_input.values)\n    in_shape = array_ops.identity(sp_input.dense_shape)\n    if new_shape is None:\n        dim_low_bound = math_ops.reduce_max(in_indices, axis=0)\n        output_shape_tensor = math_ops.maximum(array_ops.constant(0, dtype=dtypes.int64), math_ops.add(dim_low_bound, array_ops.ones_like(in_shape)))\n    else:\n        output_shape_tensor = ops.convert_to_tensor(new_shape)\n        output_shape_tensor.get_shape().assert_has_rank(1)\n        output_shape_tensor = math_ops.cast(output_shape_tensor, dtypes.int64)\n        if output_shape_tensor.get_shape().rank is not None:\n            output_shape_tensor.get_shape().dims[0].assert_is_compatible_with(in_shape.get_shape().dims[0])\n        output_shape_tensor_const = tensor_util.constant_value(output_shape_tensor)\n        if output_shape_tensor_const is not None and sp_input.get_shape().is_fully_defined():\n            in_shape_const = np.array(sp_input.get_shape().as_list())\n            if not np.all(in_shape_const <= output_shape_tensor_const):\n                raise ValueError('Requested new_shape should have dimension sizes >= sp_input.shape.  Found new_shape (%s), sp_input.shape (%s).' % (in_shape_const, output_shape_tensor_const))\n            output_shape_tensor = output_shape_tensor_const\n        else:\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_equal(array_ops.shape(in_shape), array_ops.shape(output_shape_tensor))], output_shape_tensor)\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_less_equal(in_shape, output_shape_tensor)], output_shape_tensor)\n    return sparse_tensor.SparseTensor(in_indices, in_values, output_shape_tensor)",
            "@tf_export('sparse.reset_shape', v1=['sparse.reset_shape', 'sparse_reset_shape'])\n@deprecation.deprecated_endpoints('sparse_reset_shape')\ndef sparse_reset_shape(sp_input, new_shape=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Resets the shape of a `SparseTensor` with indices and values unchanged.\\n\\n  If `new_shape` is None, returns a copy of `sp_input` with its shape reset\\n  to the tight bounding box of `sp_input`. This will be a shape consisting of\\n  all zeros if sp_input has no values.\\n\\n  If `new_shape` is provided, then it must be larger or equal in all dimensions\\n  compared to the shape of `sp_input`. When this condition is met, the returned\\n  SparseTensor will have its shape reset to `new_shape` and its indices and\\n  values unchanged from that of `sp_input.`\\n\\n  For example:\\n\\n    Consider a `sp_input` with shape [2, 3, 5]:\\n\\n      [0, 0, 1]: a\\n      [0, 1, 0]: b\\n      [0, 2, 2]: c\\n      [1, 0, 3]: d\\n\\n    - It is an error to set `new_shape` as [3, 7] since this represents a\\n      rank-2 tensor while `sp_input` is rank-3. This is either a ValueError\\n      during graph construction (if both shapes are known) or an OpError during\\n      run time.\\n\\n    - Setting `new_shape` as [2, 3, 6] will be fine as this shape is larger or\\n      equal in every dimension compared to the original shape [2, 3, 5].\\n\\n    - On the other hand, setting new_shape as [2, 3, 4] is also an error: The\\n      third dimension is smaller than the original shape [2, 3, 5] (and an\\n      `InvalidArgumentError` will be raised).\\n\\n    - If `new_shape` is None, the returned SparseTensor will have a shape\\n      [2, 3, 4], which is the tight bounding box of `sp_input`.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    new_shape: None or a vector representing the new shape for the returned\\n      `SparseTensor`.\\n\\n  Returns:\\n    A `SparseTensor` indices and values unchanged from `sp_input`. Its shape is\\n      `new_shape` if that is set. Otherwise it is the tight bounding box of\\n       `sp_input`\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n    ValueError: If `new_shape` represents a tensor with a different rank from\\n      that of `sp_input` (if shapes are known when graph is constructed).\\n    ValueError:  If `new_shape` is determined during graph build to have\\n      dimension sizes that are too small.\\n    OpError:\\n      - If `new_shape` has dimension sizes that are too small.\\n      - If shapes are not known during graph construction time, and during run\\n        time it is found out that the ranks do not match.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    in_indices = array_ops.identity(sp_input.indices)\n    in_values = array_ops.identity(sp_input.values)\n    in_shape = array_ops.identity(sp_input.dense_shape)\n    if new_shape is None:\n        dim_low_bound = math_ops.reduce_max(in_indices, axis=0)\n        output_shape_tensor = math_ops.maximum(array_ops.constant(0, dtype=dtypes.int64), math_ops.add(dim_low_bound, array_ops.ones_like(in_shape)))\n    else:\n        output_shape_tensor = ops.convert_to_tensor(new_shape)\n        output_shape_tensor.get_shape().assert_has_rank(1)\n        output_shape_tensor = math_ops.cast(output_shape_tensor, dtypes.int64)\n        if output_shape_tensor.get_shape().rank is not None:\n            output_shape_tensor.get_shape().dims[0].assert_is_compatible_with(in_shape.get_shape().dims[0])\n        output_shape_tensor_const = tensor_util.constant_value(output_shape_tensor)\n        if output_shape_tensor_const is not None and sp_input.get_shape().is_fully_defined():\n            in_shape_const = np.array(sp_input.get_shape().as_list())\n            if not np.all(in_shape_const <= output_shape_tensor_const):\n                raise ValueError('Requested new_shape should have dimension sizes >= sp_input.shape.  Found new_shape (%s), sp_input.shape (%s).' % (in_shape_const, output_shape_tensor_const))\n            output_shape_tensor = output_shape_tensor_const\n        else:\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_equal(array_ops.shape(in_shape), array_ops.shape(output_shape_tensor))], output_shape_tensor)\n            output_shape_tensor = control_flow_ops.with_dependencies([check_ops.assert_less_equal(in_shape, output_shape_tensor)], output_shape_tensor)\n    return sparse_tensor.SparseTensor(in_indices, in_values, output_shape_tensor)"
        ]
    },
    {
        "func_name": "sparse_fill_empty_rows",
        "original": "@tf_export('sparse.fill_empty_rows', v1=['sparse.fill_empty_rows', 'sparse_fill_empty_rows'])\n@deprecation.deprecated_endpoints('sparse_fill_empty_rows')\ndef sparse_fill_empty_rows(sp_input, default_value, name=None):\n    \"\"\"Fills empty rows in the input 2-D `SparseTensor` with a default value.\n\n  This op adds entries with the specified `default_value` at index\n  `[row, 0]` for any row in the input that does not already have a value.\n\n  For example, suppose `sp_input` has shape `[5, 6]` and non-empty values:\n\n      [0, 1]: a\n      [0, 3]: b\n      [2, 0]: c\n      [3, 1]: d\n\n  Rows 1 and 4 are empty, so the output will be of shape `[5, 6]` with values:\n\n      [0, 1]: a\n      [0, 3]: b\n      [1, 0]: default_value\n      [2, 0]: c\n      [3, 1]: d\n      [4, 0]: default_value\n\n  Note that the input may have empty columns at the end, with no effect on\n  this op.\n\n  The output `SparseTensor` will be in row-major order and will have the\n  same shape as the input.\n\n  This op also returns an indicator vector such that\n\n      empty_row_indicator[i] = True iff row i was an empty row.\n\n  Args:\n    sp_input: A `SparseTensor` with shape `[N, M]`.\n    default_value: The value to fill for empty rows, with the same type as\n      `sp_input.`\n    name: A name prefix for the returned tensors (optional)\n\n  Returns:\n    sp_ordered_output: A `SparseTensor` with shape `[N, M]`, and with all empty\n      rows filled in with `default_value`.\n    empty_row_indicator: A bool vector of length `N` indicating whether each\n      input row was empty.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseFillEmptyRows', [sp_input]):\n        default_value = ops.convert_to_tensor(default_value, dtype=sp_input.values.dtype)\n        (output_indices, output_values, empty_row_indicator, unused_reverse_index_map) = gen_sparse_ops.sparse_fill_empty_rows(indices=sp_input.indices, values=sp_input.values, dense_shape=sp_input.dense_shape, default_value=default_value)\n        return (sparse_tensor.SparseTensor(indices=output_indices, values=output_values, dense_shape=sp_input.dense_shape), empty_row_indicator)",
        "mutated": [
            "@tf_export('sparse.fill_empty_rows', v1=['sparse.fill_empty_rows', 'sparse_fill_empty_rows'])\n@deprecation.deprecated_endpoints('sparse_fill_empty_rows')\ndef sparse_fill_empty_rows(sp_input, default_value, name=None):\n    if False:\n        i = 10\n    'Fills empty rows in the input 2-D `SparseTensor` with a default value.\\n\\n  This op adds entries with the specified `default_value` at index\\n  `[row, 0]` for any row in the input that does not already have a value.\\n\\n  For example, suppose `sp_input` has shape `[5, 6]` and non-empty values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Rows 1 and 4 are empty, so the output will be of shape `[5, 6]` with values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [1, 0]: default_value\\n      [2, 0]: c\\n      [3, 1]: d\\n      [4, 0]: default_value\\n\\n  Note that the input may have empty columns at the end, with no effect on\\n  this op.\\n\\n  The output `SparseTensor` will be in row-major order and will have the\\n  same shape as the input.\\n\\n  This op also returns an indicator vector such that\\n\\n      empty_row_indicator[i] = True iff row i was an empty row.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with shape `[N, M]`.\\n    default_value: The value to fill for empty rows, with the same type as\\n      `sp_input.`\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    sp_ordered_output: A `SparseTensor` with shape `[N, M]`, and with all empty\\n      rows filled in with `default_value`.\\n    empty_row_indicator: A bool vector of length `N` indicating whether each\\n      input row was empty.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseFillEmptyRows', [sp_input]):\n        default_value = ops.convert_to_tensor(default_value, dtype=sp_input.values.dtype)\n        (output_indices, output_values, empty_row_indicator, unused_reverse_index_map) = gen_sparse_ops.sparse_fill_empty_rows(indices=sp_input.indices, values=sp_input.values, dense_shape=sp_input.dense_shape, default_value=default_value)\n        return (sparse_tensor.SparseTensor(indices=output_indices, values=output_values, dense_shape=sp_input.dense_shape), empty_row_indicator)",
            "@tf_export('sparse.fill_empty_rows', v1=['sparse.fill_empty_rows', 'sparse_fill_empty_rows'])\n@deprecation.deprecated_endpoints('sparse_fill_empty_rows')\ndef sparse_fill_empty_rows(sp_input, default_value, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Fills empty rows in the input 2-D `SparseTensor` with a default value.\\n\\n  This op adds entries with the specified `default_value` at index\\n  `[row, 0]` for any row in the input that does not already have a value.\\n\\n  For example, suppose `sp_input` has shape `[5, 6]` and non-empty values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Rows 1 and 4 are empty, so the output will be of shape `[5, 6]` with values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [1, 0]: default_value\\n      [2, 0]: c\\n      [3, 1]: d\\n      [4, 0]: default_value\\n\\n  Note that the input may have empty columns at the end, with no effect on\\n  this op.\\n\\n  The output `SparseTensor` will be in row-major order and will have the\\n  same shape as the input.\\n\\n  This op also returns an indicator vector such that\\n\\n      empty_row_indicator[i] = True iff row i was an empty row.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with shape `[N, M]`.\\n    default_value: The value to fill for empty rows, with the same type as\\n      `sp_input.`\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    sp_ordered_output: A `SparseTensor` with shape `[N, M]`, and with all empty\\n      rows filled in with `default_value`.\\n    empty_row_indicator: A bool vector of length `N` indicating whether each\\n      input row was empty.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseFillEmptyRows', [sp_input]):\n        default_value = ops.convert_to_tensor(default_value, dtype=sp_input.values.dtype)\n        (output_indices, output_values, empty_row_indicator, unused_reverse_index_map) = gen_sparse_ops.sparse_fill_empty_rows(indices=sp_input.indices, values=sp_input.values, dense_shape=sp_input.dense_shape, default_value=default_value)\n        return (sparse_tensor.SparseTensor(indices=output_indices, values=output_values, dense_shape=sp_input.dense_shape), empty_row_indicator)",
            "@tf_export('sparse.fill_empty_rows', v1=['sparse.fill_empty_rows', 'sparse_fill_empty_rows'])\n@deprecation.deprecated_endpoints('sparse_fill_empty_rows')\ndef sparse_fill_empty_rows(sp_input, default_value, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Fills empty rows in the input 2-D `SparseTensor` with a default value.\\n\\n  This op adds entries with the specified `default_value` at index\\n  `[row, 0]` for any row in the input that does not already have a value.\\n\\n  For example, suppose `sp_input` has shape `[5, 6]` and non-empty values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Rows 1 and 4 are empty, so the output will be of shape `[5, 6]` with values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [1, 0]: default_value\\n      [2, 0]: c\\n      [3, 1]: d\\n      [4, 0]: default_value\\n\\n  Note that the input may have empty columns at the end, with no effect on\\n  this op.\\n\\n  The output `SparseTensor` will be in row-major order and will have the\\n  same shape as the input.\\n\\n  This op also returns an indicator vector such that\\n\\n      empty_row_indicator[i] = True iff row i was an empty row.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with shape `[N, M]`.\\n    default_value: The value to fill for empty rows, with the same type as\\n      `sp_input.`\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    sp_ordered_output: A `SparseTensor` with shape `[N, M]`, and with all empty\\n      rows filled in with `default_value`.\\n    empty_row_indicator: A bool vector of length `N` indicating whether each\\n      input row was empty.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseFillEmptyRows', [sp_input]):\n        default_value = ops.convert_to_tensor(default_value, dtype=sp_input.values.dtype)\n        (output_indices, output_values, empty_row_indicator, unused_reverse_index_map) = gen_sparse_ops.sparse_fill_empty_rows(indices=sp_input.indices, values=sp_input.values, dense_shape=sp_input.dense_shape, default_value=default_value)\n        return (sparse_tensor.SparseTensor(indices=output_indices, values=output_values, dense_shape=sp_input.dense_shape), empty_row_indicator)",
            "@tf_export('sparse.fill_empty_rows', v1=['sparse.fill_empty_rows', 'sparse_fill_empty_rows'])\n@deprecation.deprecated_endpoints('sparse_fill_empty_rows')\ndef sparse_fill_empty_rows(sp_input, default_value, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Fills empty rows in the input 2-D `SparseTensor` with a default value.\\n\\n  This op adds entries with the specified `default_value` at index\\n  `[row, 0]` for any row in the input that does not already have a value.\\n\\n  For example, suppose `sp_input` has shape `[5, 6]` and non-empty values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Rows 1 and 4 are empty, so the output will be of shape `[5, 6]` with values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [1, 0]: default_value\\n      [2, 0]: c\\n      [3, 1]: d\\n      [4, 0]: default_value\\n\\n  Note that the input may have empty columns at the end, with no effect on\\n  this op.\\n\\n  The output `SparseTensor` will be in row-major order and will have the\\n  same shape as the input.\\n\\n  This op also returns an indicator vector such that\\n\\n      empty_row_indicator[i] = True iff row i was an empty row.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with shape `[N, M]`.\\n    default_value: The value to fill for empty rows, with the same type as\\n      `sp_input.`\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    sp_ordered_output: A `SparseTensor` with shape `[N, M]`, and with all empty\\n      rows filled in with `default_value`.\\n    empty_row_indicator: A bool vector of length `N` indicating whether each\\n      input row was empty.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseFillEmptyRows', [sp_input]):\n        default_value = ops.convert_to_tensor(default_value, dtype=sp_input.values.dtype)\n        (output_indices, output_values, empty_row_indicator, unused_reverse_index_map) = gen_sparse_ops.sparse_fill_empty_rows(indices=sp_input.indices, values=sp_input.values, dense_shape=sp_input.dense_shape, default_value=default_value)\n        return (sparse_tensor.SparseTensor(indices=output_indices, values=output_values, dense_shape=sp_input.dense_shape), empty_row_indicator)",
            "@tf_export('sparse.fill_empty_rows', v1=['sparse.fill_empty_rows', 'sparse_fill_empty_rows'])\n@deprecation.deprecated_endpoints('sparse_fill_empty_rows')\ndef sparse_fill_empty_rows(sp_input, default_value, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Fills empty rows in the input 2-D `SparseTensor` with a default value.\\n\\n  This op adds entries with the specified `default_value` at index\\n  `[row, 0]` for any row in the input that does not already have a value.\\n\\n  For example, suppose `sp_input` has shape `[5, 6]` and non-empty values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [2, 0]: c\\n      [3, 1]: d\\n\\n  Rows 1 and 4 are empty, so the output will be of shape `[5, 6]` with values:\\n\\n      [0, 1]: a\\n      [0, 3]: b\\n      [1, 0]: default_value\\n      [2, 0]: c\\n      [3, 1]: d\\n      [4, 0]: default_value\\n\\n  Note that the input may have empty columns at the end, with no effect on\\n  this op.\\n\\n  The output `SparseTensor` will be in row-major order and will have the\\n  same shape as the input.\\n\\n  This op also returns an indicator vector such that\\n\\n      empty_row_indicator[i] = True iff row i was an empty row.\\n\\n  Args:\\n    sp_input: A `SparseTensor` with shape `[N, M]`.\\n    default_value: The value to fill for empty rows, with the same type as\\n      `sp_input.`\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    sp_ordered_output: A `SparseTensor` with shape `[N, M]`, and with all empty\\n      rows filled in with `default_value`.\\n    empty_row_indicator: A bool vector of length `N` indicating whether each\\n      input row was empty.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    with ops.name_scope(name, 'SparseFillEmptyRows', [sp_input]):\n        default_value = ops.convert_to_tensor(default_value, dtype=sp_input.values.dtype)\n        (output_indices, output_values, empty_row_indicator, unused_reverse_index_map) = gen_sparse_ops.sparse_fill_empty_rows(indices=sp_input.indices, values=sp_input.values, dense_shape=sp_input.dense_shape, default_value=default_value)\n        return (sparse_tensor.SparseTensor(indices=output_indices, values=output_values, dense_shape=sp_input.dense_shape), empty_row_indicator)"
        ]
    },
    {
        "func_name": "serialize_sparse",
        "original": "@tf_export(v1=['io.serialize_sparse', 'serialize_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_sparse')\ndef serialize_sparse(sp_input, name=None, out_type=dtypes.string):\n    \"\"\"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\n\n  Args:\n    sp_input: The input `SparseTensor`.\n    name: A name prefix for the returned tensors (optional).\n    out_type: The `dtype` to use for serialization.\n\n  Returns:\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\n    `SparseTensor`'s indices, values, and shape (respectively).\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    return serialize_sparse_v2(sp_input, out_type, name)",
        "mutated": [
            "@tf_export(v1=['io.serialize_sparse', 'serialize_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_sparse')\ndef serialize_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_sparse_v2(sp_input, out_type, name)",
            "@tf_export(v1=['io.serialize_sparse', 'serialize_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_sparse')\ndef serialize_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_sparse_v2(sp_input, out_type, name)",
            "@tf_export(v1=['io.serialize_sparse', 'serialize_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_sparse')\ndef serialize_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_sparse_v2(sp_input, out_type, name)",
            "@tf_export(v1=['io.serialize_sparse', 'serialize_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_sparse')\ndef serialize_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_sparse_v2(sp_input, out_type, name)",
            "@tf_export(v1=['io.serialize_sparse', 'serialize_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_sparse')\ndef serialize_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_sparse_v2(sp_input, out_type, name)"
        ]
    },
    {
        "func_name": "serialize_sparse_v2",
        "original": "@tf_export('io.serialize_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    \"\"\"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\n\n  Args:\n    sp_input: The input `SparseTensor`.\n    out_type: The `dtype` to use for serialization.\n    name: A name prefix for the returned tensors (optional).\n\n  Returns:\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\n    `SparseTensor`'s indices, values, and shape (respectively).\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
        "mutated": [
            "@tf_export('io.serialize_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
            "@tf_export('io.serialize_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
            "@tf_export('io.serialize_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
            "@tf_export('io.serialize_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
            "@tf_export('io.serialize_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A 3-vector (1-D `Tensor`), with each column representing the serialized\\n    `SparseTensor`'s indices, values, and shape (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)"
        ]
    },
    {
        "func_name": "serialize_many_sparse",
        "original": "@tf_export(v1=['io.serialize_many_sparse', 'serialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_many_sparse')\ndef serialize_many_sparse(sp_input, name=None, out_type=dtypes.string):\n    \"\"\"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\n\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\n  must be sorted in increasing order of this first dimension.  The serialized\n  `SparseTensor` objects going into each row of the output `Tensor` will have\n  rank `R-1`.\n\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\n\n  Args:\n    sp_input: The input rank `R` `SparseTensor`.\n    name: A name prefix for the returned tensors (optional).\n    out_type: The `dtype` to use for serialization.\n\n  Returns:\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\n    represents serialized `SparseTensor`'s indices, values, and shape\n    (respectively).\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    return serialize_many_sparse_v2(sp_input, out_type, name)",
        "mutated": [
            "@tf_export(v1=['io.serialize_many_sparse', 'serialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_many_sparse')\ndef serialize_many_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_many_sparse_v2(sp_input, out_type, name)",
            "@tf_export(v1=['io.serialize_many_sparse', 'serialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_many_sparse')\ndef serialize_many_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_many_sparse_v2(sp_input, out_type, name)",
            "@tf_export(v1=['io.serialize_many_sparse', 'serialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_many_sparse')\ndef serialize_many_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_many_sparse_v2(sp_input, out_type, name)",
            "@tf_export(v1=['io.serialize_many_sparse', 'serialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_many_sparse')\ndef serialize_many_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_many_sparse_v2(sp_input, out_type, name)",
            "@tf_export(v1=['io.serialize_many_sparse', 'serialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('serialize_many_sparse')\ndef serialize_many_sparse(sp_input, name=None, out_type=dtypes.string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    name: A name prefix for the returned tensors (optional).\\n    out_type: The `dtype` to use for serialization.\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    return serialize_many_sparse_v2(sp_input, out_type, name)"
        ]
    },
    {
        "func_name": "serialize_many_sparse_v2",
        "original": "@tf_export('io.serialize_many_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_many_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    \"\"\"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\n\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\n  must be sorted in increasing order of this first dimension.  The serialized\n  `SparseTensor` objects going into each row of the output `Tensor` will have\n  rank `R-1`.\n\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\n\n  Args:\n    sp_input: The input rank `R` `SparseTensor`.\n    out_type: The `dtype` to use for serialization.\n    name: A name prefix for the returned tensors (optional).\n\n  Returns:\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\n    represents serialized `SparseTensor`'s indices, values, and shape\n    (respectively).\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_many_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
        "mutated": [
            "@tf_export('io.serialize_many_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_many_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_many_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
            "@tf_export('io.serialize_many_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_many_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_many_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
            "@tf_export('io.serialize_many_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_many_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_many_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
            "@tf_export('io.serialize_many_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_many_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_many_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)",
            "@tf_export('io.serialize_many_sparse', v1=[])\n@dispatch.add_dispatch_support\ndef serialize_many_sparse_v2(sp_input, out_type=dtypes.string, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    out_type: The `dtype` to use for serialization.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A matrix (2-D `Tensor`) with `N` rows and `3` columns. Each column\\n    represents serialized `SparseTensor`'s indices, values, and shape\\n    (respectively).\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.serialize_many_sparse(sp_input.indices, sp_input.values, sp_input.dense_shape, name=name, out_type=out_type)"
        ]
    },
    {
        "func_name": "deserialize_sparse",
        "original": "def deserialize_sparse(serialized_sparse, dtype, rank=None, name=None):\n    \"\"\"Deserialize `SparseTensor` objects.\n\n  The input `serialized_sparse` must have the shape `[?, ?, ..., ?, 3]` where\n  the last dimension stores serialized `SparseTensor` objects and the other N\n  dimensions (N >= 0) correspond to a batch. The ranks of the original\n  `SparseTensor` objects must all match. When the final `SparseTensor` is\n  created, its rank is the rank of the incoming `SparseTensor` objects plus N;\n  the sparse tensors have been concatenated along new dimensions, one for each\n  batch.\n\n  The output `SparseTensor` object's shape values for the original dimensions\n  are the max across the input `SparseTensor` objects' shape values for the\n  corresponding dimensions. The new dimensions match the size of the batch.\n\n  The input `SparseTensor` objects' indices are assumed ordered in\n  standard lexicographic order.  If this is not the case, after this\n  step run `SparseReorder` to restore index ordering.\n\n  For example, if the serialized input is a `[2 x 3]` matrix representing two\n  original `SparseTensor` objects:\n\n      index = [ 0]\n              [10]\n              [20]\n      values = [1, 2, 3]\n      shape = [50]\n\n  and\n\n      index = [ 2]\n              [10]\n      values = [4, 5]\n      shape = [30]\n\n  then the final deserialized `SparseTensor` will be:\n\n      index = [0  0]\n              [0 10]\n              [0 20]\n              [1  2]\n              [1 10]\n      values = [1, 2, 3, 4, 5]\n      shape = [2 50]\n\n  Args:\n    serialized_sparse: The serialized `SparseTensor` objects.\n      The last dimension must have 3 columns.\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\n    name: A name prefix for the returned tensors (optional).\n\n  Returns:\n    A `SparseTensor` representing the deserialized `SparseTensor` objects.\n\n  \"\"\"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
        "mutated": [
            "def deserialize_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n    \"Deserialize `SparseTensor` objects.\\n\\n  The input `serialized_sparse` must have the shape `[?, ?, ..., ?, 3]` where\\n  the last dimension stores serialized `SparseTensor` objects and the other N\\n  dimensions (N >= 0) correspond to a batch. The ranks of the original\\n  `SparseTensor` objects must all match. When the final `SparseTensor` is\\n  created, its rank is the rank of the incoming `SparseTensor` objects plus N;\\n  the sparse tensors have been concatenated along new dimensions, one for each\\n  batch.\\n\\n  The output `SparseTensor` object's shape values for the original dimensions\\n  are the max across the input `SparseTensor` objects' shape values for the\\n  corresponding dimensions. The new dimensions match the size of the batch.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `SparseReorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2 x 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: The serialized `SparseTensor` objects.\\n      The last dimension must have 3 columns.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor` objects.\\n\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "def deserialize_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Deserialize `SparseTensor` objects.\\n\\n  The input `serialized_sparse` must have the shape `[?, ?, ..., ?, 3]` where\\n  the last dimension stores serialized `SparseTensor` objects and the other N\\n  dimensions (N >= 0) correspond to a batch. The ranks of the original\\n  `SparseTensor` objects must all match. When the final `SparseTensor` is\\n  created, its rank is the rank of the incoming `SparseTensor` objects plus N;\\n  the sparse tensors have been concatenated along new dimensions, one for each\\n  batch.\\n\\n  The output `SparseTensor` object's shape values for the original dimensions\\n  are the max across the input `SparseTensor` objects' shape values for the\\n  corresponding dimensions. The new dimensions match the size of the batch.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `SparseReorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2 x 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: The serialized `SparseTensor` objects.\\n      The last dimension must have 3 columns.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor` objects.\\n\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "def deserialize_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Deserialize `SparseTensor` objects.\\n\\n  The input `serialized_sparse` must have the shape `[?, ?, ..., ?, 3]` where\\n  the last dimension stores serialized `SparseTensor` objects and the other N\\n  dimensions (N >= 0) correspond to a batch. The ranks of the original\\n  `SparseTensor` objects must all match. When the final `SparseTensor` is\\n  created, its rank is the rank of the incoming `SparseTensor` objects plus N;\\n  the sparse tensors have been concatenated along new dimensions, one for each\\n  batch.\\n\\n  The output `SparseTensor` object's shape values for the original dimensions\\n  are the max across the input `SparseTensor` objects' shape values for the\\n  corresponding dimensions. The new dimensions match the size of the batch.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `SparseReorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2 x 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: The serialized `SparseTensor` objects.\\n      The last dimension must have 3 columns.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor` objects.\\n\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "def deserialize_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Deserialize `SparseTensor` objects.\\n\\n  The input `serialized_sparse` must have the shape `[?, ?, ..., ?, 3]` where\\n  the last dimension stores serialized `SparseTensor` objects and the other N\\n  dimensions (N >= 0) correspond to a batch. The ranks of the original\\n  `SparseTensor` objects must all match. When the final `SparseTensor` is\\n  created, its rank is the rank of the incoming `SparseTensor` objects plus N;\\n  the sparse tensors have been concatenated along new dimensions, one for each\\n  batch.\\n\\n  The output `SparseTensor` object's shape values for the original dimensions\\n  are the max across the input `SparseTensor` objects' shape values for the\\n  corresponding dimensions. The new dimensions match the size of the batch.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `SparseReorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2 x 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: The serialized `SparseTensor` objects.\\n      The last dimension must have 3 columns.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor` objects.\\n\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "def deserialize_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Deserialize `SparseTensor` objects.\\n\\n  The input `serialized_sparse` must have the shape `[?, ?, ..., ?, 3]` where\\n  the last dimension stores serialized `SparseTensor` objects and the other N\\n  dimensions (N >= 0) correspond to a batch. The ranks of the original\\n  `SparseTensor` objects must all match. When the final `SparseTensor` is\\n  created, its rank is the rank of the incoming `SparseTensor` objects plus N;\\n  the sparse tensors have been concatenated along new dimensions, one for each\\n  batch.\\n\\n  The output `SparseTensor` object's shape values for the original dimensions\\n  are the max across the input `SparseTensor` objects' shape values for the\\n  corresponding dimensions. The new dimensions match the size of the batch.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `SparseReorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2 x 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: The serialized `SparseTensor` objects.\\n      The last dimension must have 3 columns.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor` objects.\\n\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)"
        ]
    },
    {
        "func_name": "deserialize_many_sparse",
        "original": "@tf_export('io.deserialize_many_sparse', v1=['io.deserialize_many_sparse', 'deserialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('deserialize_many_sparse')\ndef deserialize_many_sparse(serialized_sparse, dtype, rank=None, name=None):\n    \"\"\"Deserialize and concatenate `SparseTensors` from a serialized minibatch.\n\n  The input `serialized_sparse` must be a string matrix of shape `[N x 3]` where\n  `N` is the minibatch size and the rows correspond to packed outputs of\n  `serialize_sparse`.  The ranks of the original `SparseTensor` objects\n  must all match.  When the final `SparseTensor` is created, it has rank one\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\n  concatenated along a new row dimension).\n\n  The output `SparseTensor` object's shape values for all dimensions but the\n  first are the max across the input `SparseTensor` objects' shape values\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\n  size.\n\n  The input `SparseTensor` objects' indices are assumed ordered in\n  standard lexicographic order.  If this is not the case, after this\n  step run `sparse.reorder` to restore index ordering.\n\n  For example, if the serialized input is a `[2, 3]` matrix representing two\n  original `SparseTensor` objects:\n\n      index = [ 0]\n              [10]\n              [20]\n      values = [1, 2, 3]\n      shape = [50]\n\n  and\n\n      index = [ 2]\n              [10]\n      values = [4, 5]\n      shape = [30]\n\n  then the final deserialized `SparseTensor` will be:\n\n      index = [0  0]\n              [0 10]\n              [0 20]\n              [1  2]\n              [1 10]\n      values = [1, 2, 3, 4, 5]\n      shape = [2 50]\n\n  Args:\n    serialized_sparse: 2-D `Tensor` of type `string` of shape `[N, 3]`.\n      The serialized and packed `SparseTensor` objects.\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\n    name: A name prefix for the returned tensors (optional)\n\n  Returns:\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\n    concatenated along the `SparseTensor`s' first dimension.\n\n    All of the serialized `SparseTensor`s must have had the same rank and type.\n  \"\"\"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_many_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
        "mutated": [
            "@tf_export('io.deserialize_many_sparse', v1=['io.deserialize_many_sparse', 'deserialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('deserialize_many_sparse')\ndef deserialize_many_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n    \"Deserialize and concatenate `SparseTensors` from a serialized minibatch.\\n\\n  The input `serialized_sparse` must be a string matrix of shape `[N x 3]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `serialize_sparse`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: 2-D `Tensor` of type `string` of shape `[N, 3]`.\\n      The serialized and packed `SparseTensor` objects.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_many_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "@tf_export('io.deserialize_many_sparse', v1=['io.deserialize_many_sparse', 'deserialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('deserialize_many_sparse')\ndef deserialize_many_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Deserialize and concatenate `SparseTensors` from a serialized minibatch.\\n\\n  The input `serialized_sparse` must be a string matrix of shape `[N x 3]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `serialize_sparse`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: 2-D `Tensor` of type `string` of shape `[N, 3]`.\\n      The serialized and packed `SparseTensor` objects.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_many_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "@tf_export('io.deserialize_many_sparse', v1=['io.deserialize_many_sparse', 'deserialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('deserialize_many_sparse')\ndef deserialize_many_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Deserialize and concatenate `SparseTensors` from a serialized minibatch.\\n\\n  The input `serialized_sparse` must be a string matrix of shape `[N x 3]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `serialize_sparse`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: 2-D `Tensor` of type `string` of shape `[N, 3]`.\\n      The serialized and packed `SparseTensor` objects.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_many_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "@tf_export('io.deserialize_many_sparse', v1=['io.deserialize_many_sparse', 'deserialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('deserialize_many_sparse')\ndef deserialize_many_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Deserialize and concatenate `SparseTensors` from a serialized minibatch.\\n\\n  The input `serialized_sparse` must be a string matrix of shape `[N x 3]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `serialize_sparse`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: 2-D `Tensor` of type `string` of shape `[N, 3]`.\\n      The serialized and packed `SparseTensor` objects.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_many_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "@tf_export('io.deserialize_many_sparse', v1=['io.deserialize_many_sparse', 'deserialize_many_sparse'])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints('deserialize_many_sparse')\ndef deserialize_many_sparse(serialized_sparse, dtype, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Deserialize and concatenate `SparseTensors` from a serialized minibatch.\\n\\n  The input `serialized_sparse` must be a string matrix of shape `[N x 3]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `serialize_sparse`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    serialized_sparse: 2-D `Tensor` of type `string` of shape `[N, 3]`.\\n      The serialized and packed `SparseTensor` objects.\\n    dtype: The `dtype` of the serialized `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    (output_indices, output_values, output_shape) = gen_sparse_ops.deserialize_many_sparse(serialized_sparse, dtype, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)"
        ]
    },
    {
        "func_name": "sparse_tensor_dense_matmul",
        "original": "@tf_export('sparse.sparse_dense_matmul', v1=['sparse.sparse_dense_matmul', 'sparse.matmul', 'sparse_tensor_dense_matmul'])\n@deprecation.deprecated_endpoints('sparse_tensor_dense_matmul')\ndef sparse_tensor_dense_matmul(sp_a, b, adjoint_a=False, adjoint_b=False, name=None):\n    \"\"\"Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix\n\n  (or SparseTensor) \"B\". Please note that one and only one of the inputs MUST\n  be a SparseTensor and the other MUST be a dense matrix.\n\n  The following input format is recommended (but not required) for optimal\n  performance:\n\n  * If `adjoint_a == false`: `A` should be sorted in lexicographically\n    increasing order.  Use `sparse.reorder` if you're not sure.\n  * If `adjoint_a == true`: `A` should be sorted in order of increasing\n    dimension 1 (i.e., \"column major\" order instead of \"row major\" order).\n\n  Args:\n    sp_a: SparseTensor (or dense Matrix) A, of rank 2.\n    b: dense Matrix (or SparseTensor) B, with the same dtype as sp_a.\n    adjoint_a: Use the adjoint of A in the matrix multiply.  If A is complex,\n      this is transpose(conj(A)).  Otherwise it's transpose(A).\n    adjoint_b: Use the adjoint of B in the matrix multiply.  If B is complex,\n      this is transpose(conj(B)).  Otherwise it's transpose(B).\n    name: A name prefix for the returned tensors (optional)\n\n  Returns:\n    A dense matrix (pseudo-code in dense np.matrix notation):\n      `A = A.H if adjoint_a else A`\n      `B = B.H if adjoint_b else B`\n      `return A*B`\n\n  Notes:\n\n  Using `tf.nn.embedding_lookup_sparse` for sparse multiplication:\n\n  It's not obvious but you can consider `embedding_lookup_sparse` as another\n  sparse and dense multiplication. In some situations, you may prefer to use\n  `embedding_lookup_sparse` even though you're not dealing with embeddings.\n\n  There are two questions to ask in the decision process: Do you need gradients\n  computed as sparse too? Is your sparse data represented as two\n  `SparseTensor`s: ids and values? There is more explanation about data format\n  below. If you answer any of these questions as yes, consider using\n  `tf.nn.embedding_lookup_sparse`.\n\n  Following explains differences between the expected SparseTensors:\n  For example if dense form of your sparse data has shape `[3, 5]` and values:\n\n      [[  a      ]\n       [b       c]\n       [    d    ]]\n\n\n  `SparseTensor` format expected by `sparse_tensor_dense_matmul`:\n   `sp_a` (indices, values):\n\n      [0, 1]: a\n      [1, 0]: b\n      [1, 4]: c\n      [2, 2]: d\n\n  `SparseTensor` format expected by `embedding_lookup_sparse`:\n   `sp_ids`                 `sp_weights`\n\n      [0, 0]: 1                [0, 0]: a\n      [1, 0]: 0                [1, 0]: b\n      [1, 1]: 4                [1, 1]: c\n      [2, 0]: 2                [2, 0]: d\n\n\n  Deciding when to use `sparse_tensor_dense_matmul` vs.\n  `matmul`(a_is_sparse=True):\n\n  There are a number of questions to ask in the decision process, including:\n\n  * Will the SparseTensor `A` fit in memory if densified?\n  * Is the column count of the product large (>> 1)?\n  * Is the density of `A` larger than approximately 15%?\n\n  If the answer to several of these questions is yes, consider\n  converting the `SparseTensor` to a dense one and using `tf.matmul` with\n  `a_is_sparse=True`.\n\n  This operation tends to perform well when `A` is more sparse, if the column\n  size of the product is small (e.g. matrix-vector multiplication), if\n  `sp_a.dense_shape` takes on large values.\n\n  Below is a rough speed comparison between `sparse_tensor_dense_matmul`,\n  labeled 'sparse', and `matmul`(a_is_sparse=True), labeled 'dense'.  For\n  purposes of the comparison, the time spent converting from a `SparseTensor` to\n  a dense `Tensor` is not included, so it is overly conservative with respect to\n  the time ratio.\n\n  Benchmark system:\n  CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB\n  GPU: NVidia Tesla k40c\n\n  Compiled with:\n  `-c opt --config=cuda --copt=-mavx`\n\n  ```\n  tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks\n  A sparse [m, k] with % nonzero values between 1% and 80%\n  B dense [k, n]\n\n  % nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)/dt(dense)\n  0.01   1   True  100   100   0.000221166   0.00010154   0.459112\n  0.01   1   True  100   1000  0.00033858    0.000109275  0.322745\n  0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385\n  0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669\n  0.01   1   False 100   100   0.000208085   0.000107603  0.51711\n  0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762\n  0.01   1   False 1000  100   0.000308222   0.00010345   0.335635\n  0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124\n  0.01   10  True  100   100   0.000218522   0.000105537  0.482958\n  0.01   10  True  100   1000  0.000340882   0.000111641  0.327506\n  0.01   10  True  1000  100   0.000315472   0.000117376  0.372064\n  0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128\n  0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354\n  0.01   10  False 100   1000  0.000330552   0.000112615  0.340687\n  0.01   10  False 1000  100   0.000341277   0.000114097  0.334324\n  0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549\n  0.01   25  True  100   100   0.000207806   0.000105977  0.509981\n  0.01   25  True  100   1000  0.000322879   0.00012921   0.400181\n  0.01   25  True  1000  100   0.00038262    0.00014158   0.370035\n  0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504\n  0.01   25  False 100   100   0.000209401   0.000104696  0.499979\n  0.01   25  False 100   1000  0.000321161   0.000130737  0.407076\n  0.01   25  False 1000  100   0.000377012   0.000136801  0.362856\n  0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413\n  0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833\n  0.2    1   True  100   1000  0.000348674   0.000147475  0.422959\n  0.2    1   True  1000  100   0.000336908   0.00010122   0.300439\n  0.2    1   True  1000  1000  0.001022      0.000203274  0.198898\n  0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746\n  0.2    1   False 100   1000  0.000356127   0.000146824  0.41228\n  0.2    1   False 1000  100   0.000322664   0.000100918  0.312764\n  0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648\n  0.2    10  True  100   100   0.000211692   0.000109903  0.519165\n  0.2    10  True  100   1000  0.000372819   0.000164321  0.440753\n  0.2    10  True  1000  100   0.000338651   0.000144806  0.427596\n  0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064\n  0.2    10  False 100   100   0.000215727   0.000110502  0.512231\n  0.2    10  False 100   1000  0.000375419   0.0001613    0.429653\n  0.2    10  False 1000  100   0.000336999   0.000145628  0.432132\n  0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618\n  0.2    25  True  100   100   0.000218705   0.000129913  0.594009\n  0.2    25  True  100   1000  0.000394794   0.00029428   0.745402\n  0.2    25  True  1000  100   0.000404483   0.0002693    0.665788\n  0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052\n  0.2    25  False 100   100   0.000221494   0.0001306    0.589632\n  0.2    25  False 100   1000  0.000396436   0.000297204  0.74969\n  0.2    25  False 1000  100   0.000409346   0.000270068  0.659754\n  0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046\n  0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836\n  0.5    1   True  100   1000  0.000415328   0.000223073  0.537101\n  0.5    1   True  1000  100   0.000358324   0.00011269   0.314492\n  0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851\n  0.5    1   False 100   100   0.000224196   0.000101423  0.452386\n  0.5    1   False 100   1000  0.000400987   0.000223286  0.556841\n  0.5    1   False 1000  100   0.000368825   0.00011224   0.304318\n  0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563\n  0.5    10  True  100   100   0.000222125   0.000112308  0.505608\n  0.5    10  True  100   1000  0.000461088   0.00032357   0.701753\n  0.5    10  True  1000  100   0.000394624   0.000225497  0.571422\n  0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801\n  0.5    10  False 100   100   0.000232083   0.000114978  0.495418\n  0.5    10  False 100   1000  0.000454574   0.000324632  0.714146\n  0.5    10  False 1000  100   0.000379097   0.000227768  0.600817\n  0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638\n  0.5    25  True  100   100   0.00023429    0.000151703  0.647501\n  0.5    25  True  100   1000  0.000497462   0.000598873  1.20386\n  0.5    25  True  1000  100   0.000460778   0.000557038  1.20891\n  0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845\n  0.5    25  False 100   100   0.000228981   0.000155334  0.678371\n  0.5    25  False 100   1000  0.000496139   0.000620789  1.25124\n  0.5    25  False 1000  100   0.00045473    0.000551528  1.21287\n  0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927\n  0.8    1   True  100   100   0.000222037   0.000105301  0.47425\n  0.8    1   True  100   1000  0.000410804   0.000329327  0.801664\n  0.8    1   True  1000  100   0.000349735   0.000131225  0.375212\n  0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633\n  0.8    1   False 100   100   0.000214079   0.000107486  0.502085\n  0.8    1   False 100   1000  0.000413746   0.000323244  0.781261\n  0.8    1   False 1000  100   0.000348983   0.000131983  0.378193\n  0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282\n  0.8    10  True  100   100   0.000229159   0.00011825   0.516017\n  0.8    10  True  100   1000  0.000498845   0.000532618  1.0677\n  0.8    10  True  1000  100   0.000383126   0.00029935   0.781336\n  0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689\n  0.8    10  False 100   100   0.000230783   0.000124958  0.541452\n  0.8    10  False 100   1000  0.000493393   0.000550654  1.11606\n  0.8    10  False 1000  100   0.000377167   0.000298581  0.791642\n  0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024\n  0.8    25  True  100   100   0.000233496   0.000175241  0.75051\n  0.8    25  True  100   1000  0.00055654    0.00102658   1.84458\n  0.8    25  True  1000  100   0.000463814   0.000783267  1.68875\n  0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132\n  0.8    25  False 100   100   0.000240243   0.000175047  0.728625\n  0.8    25  False 100   1000  0.000578102   0.00104499   1.80763\n  0.8    25  False 1000  100   0.000485113   0.000776849  1.60138\n  0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992\n  ```\n\n  \"\"\"\n    if isinstance(b, sparse_tensor.SparseTensor) or isinstance(b, sparse_tensor.SparseTensorValue):\n        if adjoint_a != adjoint_b:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a, adjoint_b))\n        else:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a=not adjoint_a, adjoint_b=not adjoint_b))\n    else:\n        sp_a = _convert_to_sparse_tensor(sp_a)\n        with ops.name_scope(name, 'SparseTensorDenseMatMul', [sp_a.indices, sp_a.values, b]) as name:\n            b = ops.convert_to_tensor(b, name='b')\n            return gen_sparse_ops.sparse_tensor_dense_mat_mul(a_indices=sp_a.indices, a_values=sp_a.values, a_shape=sp_a.dense_shape, b=b, adjoint_a=adjoint_a, adjoint_b=adjoint_b)",
        "mutated": [
            "@tf_export('sparse.sparse_dense_matmul', v1=['sparse.sparse_dense_matmul', 'sparse.matmul', 'sparse_tensor_dense_matmul'])\n@deprecation.deprecated_endpoints('sparse_tensor_dense_matmul')\ndef sparse_tensor_dense_matmul(sp_a, b, adjoint_a=False, adjoint_b=False, name=None):\n    if False:\n        i = 10\n    'Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix\\n\\n  (or SparseTensor) \"B\". Please note that one and only one of the inputs MUST\\n  be a SparseTensor and the other MUST be a dense matrix.\\n\\n  The following input format is recommended (but not required) for optimal\\n  performance:\\n\\n  * If `adjoint_a == false`: `A` should be sorted in lexicographically\\n    increasing order.  Use `sparse.reorder` if you\\'re not sure.\\n  * If `adjoint_a == true`: `A` should be sorted in order of increasing\\n    dimension 1 (i.e., \"column major\" order instead of \"row major\" order).\\n\\n  Args:\\n    sp_a: SparseTensor (or dense Matrix) A, of rank 2.\\n    b: dense Matrix (or SparseTensor) B, with the same dtype as sp_a.\\n    adjoint_a: Use the adjoint of A in the matrix multiply.  If A is complex,\\n      this is transpose(conj(A)).  Otherwise it\\'s transpose(A).\\n    adjoint_b: Use the adjoint of B in the matrix multiply.  If B is complex,\\n      this is transpose(conj(B)).  Otherwise it\\'s transpose(B).\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense matrix (pseudo-code in dense np.matrix notation):\\n      `A = A.H if adjoint_a else A`\\n      `B = B.H if adjoint_b else B`\\n      `return A*B`\\n\\n  Notes:\\n\\n  Using `tf.nn.embedding_lookup_sparse` for sparse multiplication:\\n\\n  It\\'s not obvious but you can consider `embedding_lookup_sparse` as another\\n  sparse and dense multiplication. In some situations, you may prefer to use\\n  `embedding_lookup_sparse` even though you\\'re not dealing with embeddings.\\n\\n  There are two questions to ask in the decision process: Do you need gradients\\n  computed as sparse too? Is your sparse data represented as two\\n  `SparseTensor`s: ids and values? There is more explanation about data format\\n  below. If you answer any of these questions as yes, consider using\\n  `tf.nn.embedding_lookup_sparse`.\\n\\n  Following explains differences between the expected SparseTensors:\\n  For example if dense form of your sparse data has shape `[3, 5]` and values:\\n\\n      [[  a      ]\\n       [b       c]\\n       [    d    ]]\\n\\n\\n  `SparseTensor` format expected by `sparse_tensor_dense_matmul`:\\n   `sp_a` (indices, values):\\n\\n      [0, 1]: a\\n      [1, 0]: b\\n      [1, 4]: c\\n      [2, 2]: d\\n\\n  `SparseTensor` format expected by `embedding_lookup_sparse`:\\n   `sp_ids`                 `sp_weights`\\n\\n      [0, 0]: 1                [0, 0]: a\\n      [1, 0]: 0                [1, 0]: b\\n      [1, 1]: 4                [1, 1]: c\\n      [2, 0]: 2                [2, 0]: d\\n\\n\\n  Deciding when to use `sparse_tensor_dense_matmul` vs.\\n  `matmul`(a_is_sparse=True):\\n\\n  There are a number of questions to ask in the decision process, including:\\n\\n  * Will the SparseTensor `A` fit in memory if densified?\\n  * Is the column count of the product large (>> 1)?\\n  * Is the density of `A` larger than approximately 15%?\\n\\n  If the answer to several of these questions is yes, consider\\n  converting the `SparseTensor` to a dense one and using `tf.matmul` with\\n  `a_is_sparse=True`.\\n\\n  This operation tends to perform well when `A` is more sparse, if the column\\n  size of the product is small (e.g. matrix-vector multiplication), if\\n  `sp_a.dense_shape` takes on large values.\\n\\n  Below is a rough speed comparison between `sparse_tensor_dense_matmul`,\\n  labeled \\'sparse\\', and `matmul`(a_is_sparse=True), labeled \\'dense\\'.  For\\n  purposes of the comparison, the time spent converting from a `SparseTensor` to\\n  a dense `Tensor` is not included, so it is overly conservative with respect to\\n  the time ratio.\\n\\n  Benchmark system:\\n  CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB\\n  GPU: NVidia Tesla k40c\\n\\n  Compiled with:\\n  `-c opt --config=cuda --copt=-mavx`\\n\\n  ```\\n  tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks\\n  A sparse [m, k] with % nonzero values between 1% and 80%\\n  B dense [k, n]\\n\\n  % nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)/dt(dense)\\n  0.01   1   True  100   100   0.000221166   0.00010154   0.459112\\n  0.01   1   True  100   1000  0.00033858    0.000109275  0.322745\\n  0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385\\n  0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669\\n  0.01   1   False 100   100   0.000208085   0.000107603  0.51711\\n  0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762\\n  0.01   1   False 1000  100   0.000308222   0.00010345   0.335635\\n  0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124\\n  0.01   10  True  100   100   0.000218522   0.000105537  0.482958\\n  0.01   10  True  100   1000  0.000340882   0.000111641  0.327506\\n  0.01   10  True  1000  100   0.000315472   0.000117376  0.372064\\n  0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128\\n  0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354\\n  0.01   10  False 100   1000  0.000330552   0.000112615  0.340687\\n  0.01   10  False 1000  100   0.000341277   0.000114097  0.334324\\n  0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549\\n  0.01   25  True  100   100   0.000207806   0.000105977  0.509981\\n  0.01   25  True  100   1000  0.000322879   0.00012921   0.400181\\n  0.01   25  True  1000  100   0.00038262    0.00014158   0.370035\\n  0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504\\n  0.01   25  False 100   100   0.000209401   0.000104696  0.499979\\n  0.01   25  False 100   1000  0.000321161   0.000130737  0.407076\\n  0.01   25  False 1000  100   0.000377012   0.000136801  0.362856\\n  0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413\\n  0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833\\n  0.2    1   True  100   1000  0.000348674   0.000147475  0.422959\\n  0.2    1   True  1000  100   0.000336908   0.00010122   0.300439\\n  0.2    1   True  1000  1000  0.001022      0.000203274  0.198898\\n  0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746\\n  0.2    1   False 100   1000  0.000356127   0.000146824  0.41228\\n  0.2    1   False 1000  100   0.000322664   0.000100918  0.312764\\n  0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648\\n  0.2    10  True  100   100   0.000211692   0.000109903  0.519165\\n  0.2    10  True  100   1000  0.000372819   0.000164321  0.440753\\n  0.2    10  True  1000  100   0.000338651   0.000144806  0.427596\\n  0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064\\n  0.2    10  False 100   100   0.000215727   0.000110502  0.512231\\n  0.2    10  False 100   1000  0.000375419   0.0001613    0.429653\\n  0.2    10  False 1000  100   0.000336999   0.000145628  0.432132\\n  0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618\\n  0.2    25  True  100   100   0.000218705   0.000129913  0.594009\\n  0.2    25  True  100   1000  0.000394794   0.00029428   0.745402\\n  0.2    25  True  1000  100   0.000404483   0.0002693    0.665788\\n  0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052\\n  0.2    25  False 100   100   0.000221494   0.0001306    0.589632\\n  0.2    25  False 100   1000  0.000396436   0.000297204  0.74969\\n  0.2    25  False 1000  100   0.000409346   0.000270068  0.659754\\n  0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046\\n  0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836\\n  0.5    1   True  100   1000  0.000415328   0.000223073  0.537101\\n  0.5    1   True  1000  100   0.000358324   0.00011269   0.314492\\n  0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851\\n  0.5    1   False 100   100   0.000224196   0.000101423  0.452386\\n  0.5    1   False 100   1000  0.000400987   0.000223286  0.556841\\n  0.5    1   False 1000  100   0.000368825   0.00011224   0.304318\\n  0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563\\n  0.5    10  True  100   100   0.000222125   0.000112308  0.505608\\n  0.5    10  True  100   1000  0.000461088   0.00032357   0.701753\\n  0.5    10  True  1000  100   0.000394624   0.000225497  0.571422\\n  0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801\\n  0.5    10  False 100   100   0.000232083   0.000114978  0.495418\\n  0.5    10  False 100   1000  0.000454574   0.000324632  0.714146\\n  0.5    10  False 1000  100   0.000379097   0.000227768  0.600817\\n  0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638\\n  0.5    25  True  100   100   0.00023429    0.000151703  0.647501\\n  0.5    25  True  100   1000  0.000497462   0.000598873  1.20386\\n  0.5    25  True  1000  100   0.000460778   0.000557038  1.20891\\n  0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845\\n  0.5    25  False 100   100   0.000228981   0.000155334  0.678371\\n  0.5    25  False 100   1000  0.000496139   0.000620789  1.25124\\n  0.5    25  False 1000  100   0.00045473    0.000551528  1.21287\\n  0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927\\n  0.8    1   True  100   100   0.000222037   0.000105301  0.47425\\n  0.8    1   True  100   1000  0.000410804   0.000329327  0.801664\\n  0.8    1   True  1000  100   0.000349735   0.000131225  0.375212\\n  0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633\\n  0.8    1   False 100   100   0.000214079   0.000107486  0.502085\\n  0.8    1   False 100   1000  0.000413746   0.000323244  0.781261\\n  0.8    1   False 1000  100   0.000348983   0.000131983  0.378193\\n  0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282\\n  0.8    10  True  100   100   0.000229159   0.00011825   0.516017\\n  0.8    10  True  100   1000  0.000498845   0.000532618  1.0677\\n  0.8    10  True  1000  100   0.000383126   0.00029935   0.781336\\n  0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689\\n  0.8    10  False 100   100   0.000230783   0.000124958  0.541452\\n  0.8    10  False 100   1000  0.000493393   0.000550654  1.11606\\n  0.8    10  False 1000  100   0.000377167   0.000298581  0.791642\\n  0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024\\n  0.8    25  True  100   100   0.000233496   0.000175241  0.75051\\n  0.8    25  True  100   1000  0.00055654    0.00102658   1.84458\\n  0.8    25  True  1000  100   0.000463814   0.000783267  1.68875\\n  0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132\\n  0.8    25  False 100   100   0.000240243   0.000175047  0.728625\\n  0.8    25  False 100   1000  0.000578102   0.00104499   1.80763\\n  0.8    25  False 1000  100   0.000485113   0.000776849  1.60138\\n  0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992\\n  ```\\n\\n  '\n    if isinstance(b, sparse_tensor.SparseTensor) or isinstance(b, sparse_tensor.SparseTensorValue):\n        if adjoint_a != adjoint_b:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a, adjoint_b))\n        else:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a=not adjoint_a, adjoint_b=not adjoint_b))\n    else:\n        sp_a = _convert_to_sparse_tensor(sp_a)\n        with ops.name_scope(name, 'SparseTensorDenseMatMul', [sp_a.indices, sp_a.values, b]) as name:\n            b = ops.convert_to_tensor(b, name='b')\n            return gen_sparse_ops.sparse_tensor_dense_mat_mul(a_indices=sp_a.indices, a_values=sp_a.values, a_shape=sp_a.dense_shape, b=b, adjoint_a=adjoint_a, adjoint_b=adjoint_b)",
            "@tf_export('sparse.sparse_dense_matmul', v1=['sparse.sparse_dense_matmul', 'sparse.matmul', 'sparse_tensor_dense_matmul'])\n@deprecation.deprecated_endpoints('sparse_tensor_dense_matmul')\ndef sparse_tensor_dense_matmul(sp_a, b, adjoint_a=False, adjoint_b=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix\\n\\n  (or SparseTensor) \"B\". Please note that one and only one of the inputs MUST\\n  be a SparseTensor and the other MUST be a dense matrix.\\n\\n  The following input format is recommended (but not required) for optimal\\n  performance:\\n\\n  * If `adjoint_a == false`: `A` should be sorted in lexicographically\\n    increasing order.  Use `sparse.reorder` if you\\'re not sure.\\n  * If `adjoint_a == true`: `A` should be sorted in order of increasing\\n    dimension 1 (i.e., \"column major\" order instead of \"row major\" order).\\n\\n  Args:\\n    sp_a: SparseTensor (or dense Matrix) A, of rank 2.\\n    b: dense Matrix (or SparseTensor) B, with the same dtype as sp_a.\\n    adjoint_a: Use the adjoint of A in the matrix multiply.  If A is complex,\\n      this is transpose(conj(A)).  Otherwise it\\'s transpose(A).\\n    adjoint_b: Use the adjoint of B in the matrix multiply.  If B is complex,\\n      this is transpose(conj(B)).  Otherwise it\\'s transpose(B).\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense matrix (pseudo-code in dense np.matrix notation):\\n      `A = A.H if adjoint_a else A`\\n      `B = B.H if adjoint_b else B`\\n      `return A*B`\\n\\n  Notes:\\n\\n  Using `tf.nn.embedding_lookup_sparse` for sparse multiplication:\\n\\n  It\\'s not obvious but you can consider `embedding_lookup_sparse` as another\\n  sparse and dense multiplication. In some situations, you may prefer to use\\n  `embedding_lookup_sparse` even though you\\'re not dealing with embeddings.\\n\\n  There are two questions to ask in the decision process: Do you need gradients\\n  computed as sparse too? Is your sparse data represented as two\\n  `SparseTensor`s: ids and values? There is more explanation about data format\\n  below. If you answer any of these questions as yes, consider using\\n  `tf.nn.embedding_lookup_sparse`.\\n\\n  Following explains differences between the expected SparseTensors:\\n  For example if dense form of your sparse data has shape `[3, 5]` and values:\\n\\n      [[  a      ]\\n       [b       c]\\n       [    d    ]]\\n\\n\\n  `SparseTensor` format expected by `sparse_tensor_dense_matmul`:\\n   `sp_a` (indices, values):\\n\\n      [0, 1]: a\\n      [1, 0]: b\\n      [1, 4]: c\\n      [2, 2]: d\\n\\n  `SparseTensor` format expected by `embedding_lookup_sparse`:\\n   `sp_ids`                 `sp_weights`\\n\\n      [0, 0]: 1                [0, 0]: a\\n      [1, 0]: 0                [1, 0]: b\\n      [1, 1]: 4                [1, 1]: c\\n      [2, 0]: 2                [2, 0]: d\\n\\n\\n  Deciding when to use `sparse_tensor_dense_matmul` vs.\\n  `matmul`(a_is_sparse=True):\\n\\n  There are a number of questions to ask in the decision process, including:\\n\\n  * Will the SparseTensor `A` fit in memory if densified?\\n  * Is the column count of the product large (>> 1)?\\n  * Is the density of `A` larger than approximately 15%?\\n\\n  If the answer to several of these questions is yes, consider\\n  converting the `SparseTensor` to a dense one and using `tf.matmul` with\\n  `a_is_sparse=True`.\\n\\n  This operation tends to perform well when `A` is more sparse, if the column\\n  size of the product is small (e.g. matrix-vector multiplication), if\\n  `sp_a.dense_shape` takes on large values.\\n\\n  Below is a rough speed comparison between `sparse_tensor_dense_matmul`,\\n  labeled \\'sparse\\', and `matmul`(a_is_sparse=True), labeled \\'dense\\'.  For\\n  purposes of the comparison, the time spent converting from a `SparseTensor` to\\n  a dense `Tensor` is not included, so it is overly conservative with respect to\\n  the time ratio.\\n\\n  Benchmark system:\\n  CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB\\n  GPU: NVidia Tesla k40c\\n\\n  Compiled with:\\n  `-c opt --config=cuda --copt=-mavx`\\n\\n  ```\\n  tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks\\n  A sparse [m, k] with % nonzero values between 1% and 80%\\n  B dense [k, n]\\n\\n  % nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)/dt(dense)\\n  0.01   1   True  100   100   0.000221166   0.00010154   0.459112\\n  0.01   1   True  100   1000  0.00033858    0.000109275  0.322745\\n  0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385\\n  0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669\\n  0.01   1   False 100   100   0.000208085   0.000107603  0.51711\\n  0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762\\n  0.01   1   False 1000  100   0.000308222   0.00010345   0.335635\\n  0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124\\n  0.01   10  True  100   100   0.000218522   0.000105537  0.482958\\n  0.01   10  True  100   1000  0.000340882   0.000111641  0.327506\\n  0.01   10  True  1000  100   0.000315472   0.000117376  0.372064\\n  0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128\\n  0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354\\n  0.01   10  False 100   1000  0.000330552   0.000112615  0.340687\\n  0.01   10  False 1000  100   0.000341277   0.000114097  0.334324\\n  0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549\\n  0.01   25  True  100   100   0.000207806   0.000105977  0.509981\\n  0.01   25  True  100   1000  0.000322879   0.00012921   0.400181\\n  0.01   25  True  1000  100   0.00038262    0.00014158   0.370035\\n  0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504\\n  0.01   25  False 100   100   0.000209401   0.000104696  0.499979\\n  0.01   25  False 100   1000  0.000321161   0.000130737  0.407076\\n  0.01   25  False 1000  100   0.000377012   0.000136801  0.362856\\n  0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413\\n  0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833\\n  0.2    1   True  100   1000  0.000348674   0.000147475  0.422959\\n  0.2    1   True  1000  100   0.000336908   0.00010122   0.300439\\n  0.2    1   True  1000  1000  0.001022      0.000203274  0.198898\\n  0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746\\n  0.2    1   False 100   1000  0.000356127   0.000146824  0.41228\\n  0.2    1   False 1000  100   0.000322664   0.000100918  0.312764\\n  0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648\\n  0.2    10  True  100   100   0.000211692   0.000109903  0.519165\\n  0.2    10  True  100   1000  0.000372819   0.000164321  0.440753\\n  0.2    10  True  1000  100   0.000338651   0.000144806  0.427596\\n  0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064\\n  0.2    10  False 100   100   0.000215727   0.000110502  0.512231\\n  0.2    10  False 100   1000  0.000375419   0.0001613    0.429653\\n  0.2    10  False 1000  100   0.000336999   0.000145628  0.432132\\n  0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618\\n  0.2    25  True  100   100   0.000218705   0.000129913  0.594009\\n  0.2    25  True  100   1000  0.000394794   0.00029428   0.745402\\n  0.2    25  True  1000  100   0.000404483   0.0002693    0.665788\\n  0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052\\n  0.2    25  False 100   100   0.000221494   0.0001306    0.589632\\n  0.2    25  False 100   1000  0.000396436   0.000297204  0.74969\\n  0.2    25  False 1000  100   0.000409346   0.000270068  0.659754\\n  0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046\\n  0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836\\n  0.5    1   True  100   1000  0.000415328   0.000223073  0.537101\\n  0.5    1   True  1000  100   0.000358324   0.00011269   0.314492\\n  0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851\\n  0.5    1   False 100   100   0.000224196   0.000101423  0.452386\\n  0.5    1   False 100   1000  0.000400987   0.000223286  0.556841\\n  0.5    1   False 1000  100   0.000368825   0.00011224   0.304318\\n  0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563\\n  0.5    10  True  100   100   0.000222125   0.000112308  0.505608\\n  0.5    10  True  100   1000  0.000461088   0.00032357   0.701753\\n  0.5    10  True  1000  100   0.000394624   0.000225497  0.571422\\n  0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801\\n  0.5    10  False 100   100   0.000232083   0.000114978  0.495418\\n  0.5    10  False 100   1000  0.000454574   0.000324632  0.714146\\n  0.5    10  False 1000  100   0.000379097   0.000227768  0.600817\\n  0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638\\n  0.5    25  True  100   100   0.00023429    0.000151703  0.647501\\n  0.5    25  True  100   1000  0.000497462   0.000598873  1.20386\\n  0.5    25  True  1000  100   0.000460778   0.000557038  1.20891\\n  0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845\\n  0.5    25  False 100   100   0.000228981   0.000155334  0.678371\\n  0.5    25  False 100   1000  0.000496139   0.000620789  1.25124\\n  0.5    25  False 1000  100   0.00045473    0.000551528  1.21287\\n  0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927\\n  0.8    1   True  100   100   0.000222037   0.000105301  0.47425\\n  0.8    1   True  100   1000  0.000410804   0.000329327  0.801664\\n  0.8    1   True  1000  100   0.000349735   0.000131225  0.375212\\n  0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633\\n  0.8    1   False 100   100   0.000214079   0.000107486  0.502085\\n  0.8    1   False 100   1000  0.000413746   0.000323244  0.781261\\n  0.8    1   False 1000  100   0.000348983   0.000131983  0.378193\\n  0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282\\n  0.8    10  True  100   100   0.000229159   0.00011825   0.516017\\n  0.8    10  True  100   1000  0.000498845   0.000532618  1.0677\\n  0.8    10  True  1000  100   0.000383126   0.00029935   0.781336\\n  0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689\\n  0.8    10  False 100   100   0.000230783   0.000124958  0.541452\\n  0.8    10  False 100   1000  0.000493393   0.000550654  1.11606\\n  0.8    10  False 1000  100   0.000377167   0.000298581  0.791642\\n  0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024\\n  0.8    25  True  100   100   0.000233496   0.000175241  0.75051\\n  0.8    25  True  100   1000  0.00055654    0.00102658   1.84458\\n  0.8    25  True  1000  100   0.000463814   0.000783267  1.68875\\n  0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132\\n  0.8    25  False 100   100   0.000240243   0.000175047  0.728625\\n  0.8    25  False 100   1000  0.000578102   0.00104499   1.80763\\n  0.8    25  False 1000  100   0.000485113   0.000776849  1.60138\\n  0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992\\n  ```\\n\\n  '\n    if isinstance(b, sparse_tensor.SparseTensor) or isinstance(b, sparse_tensor.SparseTensorValue):\n        if adjoint_a != adjoint_b:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a, adjoint_b))\n        else:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a=not adjoint_a, adjoint_b=not adjoint_b))\n    else:\n        sp_a = _convert_to_sparse_tensor(sp_a)\n        with ops.name_scope(name, 'SparseTensorDenseMatMul', [sp_a.indices, sp_a.values, b]) as name:\n            b = ops.convert_to_tensor(b, name='b')\n            return gen_sparse_ops.sparse_tensor_dense_mat_mul(a_indices=sp_a.indices, a_values=sp_a.values, a_shape=sp_a.dense_shape, b=b, adjoint_a=adjoint_a, adjoint_b=adjoint_b)",
            "@tf_export('sparse.sparse_dense_matmul', v1=['sparse.sparse_dense_matmul', 'sparse.matmul', 'sparse_tensor_dense_matmul'])\n@deprecation.deprecated_endpoints('sparse_tensor_dense_matmul')\ndef sparse_tensor_dense_matmul(sp_a, b, adjoint_a=False, adjoint_b=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix\\n\\n  (or SparseTensor) \"B\". Please note that one and only one of the inputs MUST\\n  be a SparseTensor and the other MUST be a dense matrix.\\n\\n  The following input format is recommended (but not required) for optimal\\n  performance:\\n\\n  * If `adjoint_a == false`: `A` should be sorted in lexicographically\\n    increasing order.  Use `sparse.reorder` if you\\'re not sure.\\n  * If `adjoint_a == true`: `A` should be sorted in order of increasing\\n    dimension 1 (i.e., \"column major\" order instead of \"row major\" order).\\n\\n  Args:\\n    sp_a: SparseTensor (or dense Matrix) A, of rank 2.\\n    b: dense Matrix (or SparseTensor) B, with the same dtype as sp_a.\\n    adjoint_a: Use the adjoint of A in the matrix multiply.  If A is complex,\\n      this is transpose(conj(A)).  Otherwise it\\'s transpose(A).\\n    adjoint_b: Use the adjoint of B in the matrix multiply.  If B is complex,\\n      this is transpose(conj(B)).  Otherwise it\\'s transpose(B).\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense matrix (pseudo-code in dense np.matrix notation):\\n      `A = A.H if adjoint_a else A`\\n      `B = B.H if adjoint_b else B`\\n      `return A*B`\\n\\n  Notes:\\n\\n  Using `tf.nn.embedding_lookup_sparse` for sparse multiplication:\\n\\n  It\\'s not obvious but you can consider `embedding_lookup_sparse` as another\\n  sparse and dense multiplication. In some situations, you may prefer to use\\n  `embedding_lookup_sparse` even though you\\'re not dealing with embeddings.\\n\\n  There are two questions to ask in the decision process: Do you need gradients\\n  computed as sparse too? Is your sparse data represented as two\\n  `SparseTensor`s: ids and values? There is more explanation about data format\\n  below. If you answer any of these questions as yes, consider using\\n  `tf.nn.embedding_lookup_sparse`.\\n\\n  Following explains differences between the expected SparseTensors:\\n  For example if dense form of your sparse data has shape `[3, 5]` and values:\\n\\n      [[  a      ]\\n       [b       c]\\n       [    d    ]]\\n\\n\\n  `SparseTensor` format expected by `sparse_tensor_dense_matmul`:\\n   `sp_a` (indices, values):\\n\\n      [0, 1]: a\\n      [1, 0]: b\\n      [1, 4]: c\\n      [2, 2]: d\\n\\n  `SparseTensor` format expected by `embedding_lookup_sparse`:\\n   `sp_ids`                 `sp_weights`\\n\\n      [0, 0]: 1                [0, 0]: a\\n      [1, 0]: 0                [1, 0]: b\\n      [1, 1]: 4                [1, 1]: c\\n      [2, 0]: 2                [2, 0]: d\\n\\n\\n  Deciding when to use `sparse_tensor_dense_matmul` vs.\\n  `matmul`(a_is_sparse=True):\\n\\n  There are a number of questions to ask in the decision process, including:\\n\\n  * Will the SparseTensor `A` fit in memory if densified?\\n  * Is the column count of the product large (>> 1)?\\n  * Is the density of `A` larger than approximately 15%?\\n\\n  If the answer to several of these questions is yes, consider\\n  converting the `SparseTensor` to a dense one and using `tf.matmul` with\\n  `a_is_sparse=True`.\\n\\n  This operation tends to perform well when `A` is more sparse, if the column\\n  size of the product is small (e.g. matrix-vector multiplication), if\\n  `sp_a.dense_shape` takes on large values.\\n\\n  Below is a rough speed comparison between `sparse_tensor_dense_matmul`,\\n  labeled \\'sparse\\', and `matmul`(a_is_sparse=True), labeled \\'dense\\'.  For\\n  purposes of the comparison, the time spent converting from a `SparseTensor` to\\n  a dense `Tensor` is not included, so it is overly conservative with respect to\\n  the time ratio.\\n\\n  Benchmark system:\\n  CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB\\n  GPU: NVidia Tesla k40c\\n\\n  Compiled with:\\n  `-c opt --config=cuda --copt=-mavx`\\n\\n  ```\\n  tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks\\n  A sparse [m, k] with % nonzero values between 1% and 80%\\n  B dense [k, n]\\n\\n  % nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)/dt(dense)\\n  0.01   1   True  100   100   0.000221166   0.00010154   0.459112\\n  0.01   1   True  100   1000  0.00033858    0.000109275  0.322745\\n  0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385\\n  0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669\\n  0.01   1   False 100   100   0.000208085   0.000107603  0.51711\\n  0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762\\n  0.01   1   False 1000  100   0.000308222   0.00010345   0.335635\\n  0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124\\n  0.01   10  True  100   100   0.000218522   0.000105537  0.482958\\n  0.01   10  True  100   1000  0.000340882   0.000111641  0.327506\\n  0.01   10  True  1000  100   0.000315472   0.000117376  0.372064\\n  0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128\\n  0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354\\n  0.01   10  False 100   1000  0.000330552   0.000112615  0.340687\\n  0.01   10  False 1000  100   0.000341277   0.000114097  0.334324\\n  0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549\\n  0.01   25  True  100   100   0.000207806   0.000105977  0.509981\\n  0.01   25  True  100   1000  0.000322879   0.00012921   0.400181\\n  0.01   25  True  1000  100   0.00038262    0.00014158   0.370035\\n  0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504\\n  0.01   25  False 100   100   0.000209401   0.000104696  0.499979\\n  0.01   25  False 100   1000  0.000321161   0.000130737  0.407076\\n  0.01   25  False 1000  100   0.000377012   0.000136801  0.362856\\n  0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413\\n  0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833\\n  0.2    1   True  100   1000  0.000348674   0.000147475  0.422959\\n  0.2    1   True  1000  100   0.000336908   0.00010122   0.300439\\n  0.2    1   True  1000  1000  0.001022      0.000203274  0.198898\\n  0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746\\n  0.2    1   False 100   1000  0.000356127   0.000146824  0.41228\\n  0.2    1   False 1000  100   0.000322664   0.000100918  0.312764\\n  0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648\\n  0.2    10  True  100   100   0.000211692   0.000109903  0.519165\\n  0.2    10  True  100   1000  0.000372819   0.000164321  0.440753\\n  0.2    10  True  1000  100   0.000338651   0.000144806  0.427596\\n  0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064\\n  0.2    10  False 100   100   0.000215727   0.000110502  0.512231\\n  0.2    10  False 100   1000  0.000375419   0.0001613    0.429653\\n  0.2    10  False 1000  100   0.000336999   0.000145628  0.432132\\n  0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618\\n  0.2    25  True  100   100   0.000218705   0.000129913  0.594009\\n  0.2    25  True  100   1000  0.000394794   0.00029428   0.745402\\n  0.2    25  True  1000  100   0.000404483   0.0002693    0.665788\\n  0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052\\n  0.2    25  False 100   100   0.000221494   0.0001306    0.589632\\n  0.2    25  False 100   1000  0.000396436   0.000297204  0.74969\\n  0.2    25  False 1000  100   0.000409346   0.000270068  0.659754\\n  0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046\\n  0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836\\n  0.5    1   True  100   1000  0.000415328   0.000223073  0.537101\\n  0.5    1   True  1000  100   0.000358324   0.00011269   0.314492\\n  0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851\\n  0.5    1   False 100   100   0.000224196   0.000101423  0.452386\\n  0.5    1   False 100   1000  0.000400987   0.000223286  0.556841\\n  0.5    1   False 1000  100   0.000368825   0.00011224   0.304318\\n  0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563\\n  0.5    10  True  100   100   0.000222125   0.000112308  0.505608\\n  0.5    10  True  100   1000  0.000461088   0.00032357   0.701753\\n  0.5    10  True  1000  100   0.000394624   0.000225497  0.571422\\n  0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801\\n  0.5    10  False 100   100   0.000232083   0.000114978  0.495418\\n  0.5    10  False 100   1000  0.000454574   0.000324632  0.714146\\n  0.5    10  False 1000  100   0.000379097   0.000227768  0.600817\\n  0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638\\n  0.5    25  True  100   100   0.00023429    0.000151703  0.647501\\n  0.5    25  True  100   1000  0.000497462   0.000598873  1.20386\\n  0.5    25  True  1000  100   0.000460778   0.000557038  1.20891\\n  0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845\\n  0.5    25  False 100   100   0.000228981   0.000155334  0.678371\\n  0.5    25  False 100   1000  0.000496139   0.000620789  1.25124\\n  0.5    25  False 1000  100   0.00045473    0.000551528  1.21287\\n  0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927\\n  0.8    1   True  100   100   0.000222037   0.000105301  0.47425\\n  0.8    1   True  100   1000  0.000410804   0.000329327  0.801664\\n  0.8    1   True  1000  100   0.000349735   0.000131225  0.375212\\n  0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633\\n  0.8    1   False 100   100   0.000214079   0.000107486  0.502085\\n  0.8    1   False 100   1000  0.000413746   0.000323244  0.781261\\n  0.8    1   False 1000  100   0.000348983   0.000131983  0.378193\\n  0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282\\n  0.8    10  True  100   100   0.000229159   0.00011825   0.516017\\n  0.8    10  True  100   1000  0.000498845   0.000532618  1.0677\\n  0.8    10  True  1000  100   0.000383126   0.00029935   0.781336\\n  0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689\\n  0.8    10  False 100   100   0.000230783   0.000124958  0.541452\\n  0.8    10  False 100   1000  0.000493393   0.000550654  1.11606\\n  0.8    10  False 1000  100   0.000377167   0.000298581  0.791642\\n  0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024\\n  0.8    25  True  100   100   0.000233496   0.000175241  0.75051\\n  0.8    25  True  100   1000  0.00055654    0.00102658   1.84458\\n  0.8    25  True  1000  100   0.000463814   0.000783267  1.68875\\n  0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132\\n  0.8    25  False 100   100   0.000240243   0.000175047  0.728625\\n  0.8    25  False 100   1000  0.000578102   0.00104499   1.80763\\n  0.8    25  False 1000  100   0.000485113   0.000776849  1.60138\\n  0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992\\n  ```\\n\\n  '\n    if isinstance(b, sparse_tensor.SparseTensor) or isinstance(b, sparse_tensor.SparseTensorValue):\n        if adjoint_a != adjoint_b:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a, adjoint_b))\n        else:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a=not adjoint_a, adjoint_b=not adjoint_b))\n    else:\n        sp_a = _convert_to_sparse_tensor(sp_a)\n        with ops.name_scope(name, 'SparseTensorDenseMatMul', [sp_a.indices, sp_a.values, b]) as name:\n            b = ops.convert_to_tensor(b, name='b')\n            return gen_sparse_ops.sparse_tensor_dense_mat_mul(a_indices=sp_a.indices, a_values=sp_a.values, a_shape=sp_a.dense_shape, b=b, adjoint_a=adjoint_a, adjoint_b=adjoint_b)",
            "@tf_export('sparse.sparse_dense_matmul', v1=['sparse.sparse_dense_matmul', 'sparse.matmul', 'sparse_tensor_dense_matmul'])\n@deprecation.deprecated_endpoints('sparse_tensor_dense_matmul')\ndef sparse_tensor_dense_matmul(sp_a, b, adjoint_a=False, adjoint_b=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix\\n\\n  (or SparseTensor) \"B\". Please note that one and only one of the inputs MUST\\n  be a SparseTensor and the other MUST be a dense matrix.\\n\\n  The following input format is recommended (but not required) for optimal\\n  performance:\\n\\n  * If `adjoint_a == false`: `A` should be sorted in lexicographically\\n    increasing order.  Use `sparse.reorder` if you\\'re not sure.\\n  * If `adjoint_a == true`: `A` should be sorted in order of increasing\\n    dimension 1 (i.e., \"column major\" order instead of \"row major\" order).\\n\\n  Args:\\n    sp_a: SparseTensor (or dense Matrix) A, of rank 2.\\n    b: dense Matrix (or SparseTensor) B, with the same dtype as sp_a.\\n    adjoint_a: Use the adjoint of A in the matrix multiply.  If A is complex,\\n      this is transpose(conj(A)).  Otherwise it\\'s transpose(A).\\n    adjoint_b: Use the adjoint of B in the matrix multiply.  If B is complex,\\n      this is transpose(conj(B)).  Otherwise it\\'s transpose(B).\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense matrix (pseudo-code in dense np.matrix notation):\\n      `A = A.H if adjoint_a else A`\\n      `B = B.H if adjoint_b else B`\\n      `return A*B`\\n\\n  Notes:\\n\\n  Using `tf.nn.embedding_lookup_sparse` for sparse multiplication:\\n\\n  It\\'s not obvious but you can consider `embedding_lookup_sparse` as another\\n  sparse and dense multiplication. In some situations, you may prefer to use\\n  `embedding_lookup_sparse` even though you\\'re not dealing with embeddings.\\n\\n  There are two questions to ask in the decision process: Do you need gradients\\n  computed as sparse too? Is your sparse data represented as two\\n  `SparseTensor`s: ids and values? There is more explanation about data format\\n  below. If you answer any of these questions as yes, consider using\\n  `tf.nn.embedding_lookup_sparse`.\\n\\n  Following explains differences between the expected SparseTensors:\\n  For example if dense form of your sparse data has shape `[3, 5]` and values:\\n\\n      [[  a      ]\\n       [b       c]\\n       [    d    ]]\\n\\n\\n  `SparseTensor` format expected by `sparse_tensor_dense_matmul`:\\n   `sp_a` (indices, values):\\n\\n      [0, 1]: a\\n      [1, 0]: b\\n      [1, 4]: c\\n      [2, 2]: d\\n\\n  `SparseTensor` format expected by `embedding_lookup_sparse`:\\n   `sp_ids`                 `sp_weights`\\n\\n      [0, 0]: 1                [0, 0]: a\\n      [1, 0]: 0                [1, 0]: b\\n      [1, 1]: 4                [1, 1]: c\\n      [2, 0]: 2                [2, 0]: d\\n\\n\\n  Deciding when to use `sparse_tensor_dense_matmul` vs.\\n  `matmul`(a_is_sparse=True):\\n\\n  There are a number of questions to ask in the decision process, including:\\n\\n  * Will the SparseTensor `A` fit in memory if densified?\\n  * Is the column count of the product large (>> 1)?\\n  * Is the density of `A` larger than approximately 15%?\\n\\n  If the answer to several of these questions is yes, consider\\n  converting the `SparseTensor` to a dense one and using `tf.matmul` with\\n  `a_is_sparse=True`.\\n\\n  This operation tends to perform well when `A` is more sparse, if the column\\n  size of the product is small (e.g. matrix-vector multiplication), if\\n  `sp_a.dense_shape` takes on large values.\\n\\n  Below is a rough speed comparison between `sparse_tensor_dense_matmul`,\\n  labeled \\'sparse\\', and `matmul`(a_is_sparse=True), labeled \\'dense\\'.  For\\n  purposes of the comparison, the time spent converting from a `SparseTensor` to\\n  a dense `Tensor` is not included, so it is overly conservative with respect to\\n  the time ratio.\\n\\n  Benchmark system:\\n  CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB\\n  GPU: NVidia Tesla k40c\\n\\n  Compiled with:\\n  `-c opt --config=cuda --copt=-mavx`\\n\\n  ```\\n  tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks\\n  A sparse [m, k] with % nonzero values between 1% and 80%\\n  B dense [k, n]\\n\\n  % nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)/dt(dense)\\n  0.01   1   True  100   100   0.000221166   0.00010154   0.459112\\n  0.01   1   True  100   1000  0.00033858    0.000109275  0.322745\\n  0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385\\n  0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669\\n  0.01   1   False 100   100   0.000208085   0.000107603  0.51711\\n  0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762\\n  0.01   1   False 1000  100   0.000308222   0.00010345   0.335635\\n  0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124\\n  0.01   10  True  100   100   0.000218522   0.000105537  0.482958\\n  0.01   10  True  100   1000  0.000340882   0.000111641  0.327506\\n  0.01   10  True  1000  100   0.000315472   0.000117376  0.372064\\n  0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128\\n  0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354\\n  0.01   10  False 100   1000  0.000330552   0.000112615  0.340687\\n  0.01   10  False 1000  100   0.000341277   0.000114097  0.334324\\n  0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549\\n  0.01   25  True  100   100   0.000207806   0.000105977  0.509981\\n  0.01   25  True  100   1000  0.000322879   0.00012921   0.400181\\n  0.01   25  True  1000  100   0.00038262    0.00014158   0.370035\\n  0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504\\n  0.01   25  False 100   100   0.000209401   0.000104696  0.499979\\n  0.01   25  False 100   1000  0.000321161   0.000130737  0.407076\\n  0.01   25  False 1000  100   0.000377012   0.000136801  0.362856\\n  0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413\\n  0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833\\n  0.2    1   True  100   1000  0.000348674   0.000147475  0.422959\\n  0.2    1   True  1000  100   0.000336908   0.00010122   0.300439\\n  0.2    1   True  1000  1000  0.001022      0.000203274  0.198898\\n  0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746\\n  0.2    1   False 100   1000  0.000356127   0.000146824  0.41228\\n  0.2    1   False 1000  100   0.000322664   0.000100918  0.312764\\n  0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648\\n  0.2    10  True  100   100   0.000211692   0.000109903  0.519165\\n  0.2    10  True  100   1000  0.000372819   0.000164321  0.440753\\n  0.2    10  True  1000  100   0.000338651   0.000144806  0.427596\\n  0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064\\n  0.2    10  False 100   100   0.000215727   0.000110502  0.512231\\n  0.2    10  False 100   1000  0.000375419   0.0001613    0.429653\\n  0.2    10  False 1000  100   0.000336999   0.000145628  0.432132\\n  0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618\\n  0.2    25  True  100   100   0.000218705   0.000129913  0.594009\\n  0.2    25  True  100   1000  0.000394794   0.00029428   0.745402\\n  0.2    25  True  1000  100   0.000404483   0.0002693    0.665788\\n  0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052\\n  0.2    25  False 100   100   0.000221494   0.0001306    0.589632\\n  0.2    25  False 100   1000  0.000396436   0.000297204  0.74969\\n  0.2    25  False 1000  100   0.000409346   0.000270068  0.659754\\n  0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046\\n  0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836\\n  0.5    1   True  100   1000  0.000415328   0.000223073  0.537101\\n  0.5    1   True  1000  100   0.000358324   0.00011269   0.314492\\n  0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851\\n  0.5    1   False 100   100   0.000224196   0.000101423  0.452386\\n  0.5    1   False 100   1000  0.000400987   0.000223286  0.556841\\n  0.5    1   False 1000  100   0.000368825   0.00011224   0.304318\\n  0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563\\n  0.5    10  True  100   100   0.000222125   0.000112308  0.505608\\n  0.5    10  True  100   1000  0.000461088   0.00032357   0.701753\\n  0.5    10  True  1000  100   0.000394624   0.000225497  0.571422\\n  0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801\\n  0.5    10  False 100   100   0.000232083   0.000114978  0.495418\\n  0.5    10  False 100   1000  0.000454574   0.000324632  0.714146\\n  0.5    10  False 1000  100   0.000379097   0.000227768  0.600817\\n  0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638\\n  0.5    25  True  100   100   0.00023429    0.000151703  0.647501\\n  0.5    25  True  100   1000  0.000497462   0.000598873  1.20386\\n  0.5    25  True  1000  100   0.000460778   0.000557038  1.20891\\n  0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845\\n  0.5    25  False 100   100   0.000228981   0.000155334  0.678371\\n  0.5    25  False 100   1000  0.000496139   0.000620789  1.25124\\n  0.5    25  False 1000  100   0.00045473    0.000551528  1.21287\\n  0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927\\n  0.8    1   True  100   100   0.000222037   0.000105301  0.47425\\n  0.8    1   True  100   1000  0.000410804   0.000329327  0.801664\\n  0.8    1   True  1000  100   0.000349735   0.000131225  0.375212\\n  0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633\\n  0.8    1   False 100   100   0.000214079   0.000107486  0.502085\\n  0.8    1   False 100   1000  0.000413746   0.000323244  0.781261\\n  0.8    1   False 1000  100   0.000348983   0.000131983  0.378193\\n  0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282\\n  0.8    10  True  100   100   0.000229159   0.00011825   0.516017\\n  0.8    10  True  100   1000  0.000498845   0.000532618  1.0677\\n  0.8    10  True  1000  100   0.000383126   0.00029935   0.781336\\n  0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689\\n  0.8    10  False 100   100   0.000230783   0.000124958  0.541452\\n  0.8    10  False 100   1000  0.000493393   0.000550654  1.11606\\n  0.8    10  False 1000  100   0.000377167   0.000298581  0.791642\\n  0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024\\n  0.8    25  True  100   100   0.000233496   0.000175241  0.75051\\n  0.8    25  True  100   1000  0.00055654    0.00102658   1.84458\\n  0.8    25  True  1000  100   0.000463814   0.000783267  1.68875\\n  0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132\\n  0.8    25  False 100   100   0.000240243   0.000175047  0.728625\\n  0.8    25  False 100   1000  0.000578102   0.00104499   1.80763\\n  0.8    25  False 1000  100   0.000485113   0.000776849  1.60138\\n  0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992\\n  ```\\n\\n  '\n    if isinstance(b, sparse_tensor.SparseTensor) or isinstance(b, sparse_tensor.SparseTensorValue):\n        if adjoint_a != adjoint_b:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a, adjoint_b))\n        else:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a=not adjoint_a, adjoint_b=not adjoint_b))\n    else:\n        sp_a = _convert_to_sparse_tensor(sp_a)\n        with ops.name_scope(name, 'SparseTensorDenseMatMul', [sp_a.indices, sp_a.values, b]) as name:\n            b = ops.convert_to_tensor(b, name='b')\n            return gen_sparse_ops.sparse_tensor_dense_mat_mul(a_indices=sp_a.indices, a_values=sp_a.values, a_shape=sp_a.dense_shape, b=b, adjoint_a=adjoint_a, adjoint_b=adjoint_b)",
            "@tf_export('sparse.sparse_dense_matmul', v1=['sparse.sparse_dense_matmul', 'sparse.matmul', 'sparse_tensor_dense_matmul'])\n@deprecation.deprecated_endpoints('sparse_tensor_dense_matmul')\ndef sparse_tensor_dense_matmul(sp_a, b, adjoint_a=False, adjoint_b=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix\\n\\n  (or SparseTensor) \"B\". Please note that one and only one of the inputs MUST\\n  be a SparseTensor and the other MUST be a dense matrix.\\n\\n  The following input format is recommended (but not required) for optimal\\n  performance:\\n\\n  * If `adjoint_a == false`: `A` should be sorted in lexicographically\\n    increasing order.  Use `sparse.reorder` if you\\'re not sure.\\n  * If `adjoint_a == true`: `A` should be sorted in order of increasing\\n    dimension 1 (i.e., \"column major\" order instead of \"row major\" order).\\n\\n  Args:\\n    sp_a: SparseTensor (or dense Matrix) A, of rank 2.\\n    b: dense Matrix (or SparseTensor) B, with the same dtype as sp_a.\\n    adjoint_a: Use the adjoint of A in the matrix multiply.  If A is complex,\\n      this is transpose(conj(A)).  Otherwise it\\'s transpose(A).\\n    adjoint_b: Use the adjoint of B in the matrix multiply.  If B is complex,\\n      this is transpose(conj(B)).  Otherwise it\\'s transpose(B).\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A dense matrix (pseudo-code in dense np.matrix notation):\\n      `A = A.H if adjoint_a else A`\\n      `B = B.H if adjoint_b else B`\\n      `return A*B`\\n\\n  Notes:\\n\\n  Using `tf.nn.embedding_lookup_sparse` for sparse multiplication:\\n\\n  It\\'s not obvious but you can consider `embedding_lookup_sparse` as another\\n  sparse and dense multiplication. In some situations, you may prefer to use\\n  `embedding_lookup_sparse` even though you\\'re not dealing with embeddings.\\n\\n  There are two questions to ask in the decision process: Do you need gradients\\n  computed as sparse too? Is your sparse data represented as two\\n  `SparseTensor`s: ids and values? There is more explanation about data format\\n  below. If you answer any of these questions as yes, consider using\\n  `tf.nn.embedding_lookup_sparse`.\\n\\n  Following explains differences between the expected SparseTensors:\\n  For example if dense form of your sparse data has shape `[3, 5]` and values:\\n\\n      [[  a      ]\\n       [b       c]\\n       [    d    ]]\\n\\n\\n  `SparseTensor` format expected by `sparse_tensor_dense_matmul`:\\n   `sp_a` (indices, values):\\n\\n      [0, 1]: a\\n      [1, 0]: b\\n      [1, 4]: c\\n      [2, 2]: d\\n\\n  `SparseTensor` format expected by `embedding_lookup_sparse`:\\n   `sp_ids`                 `sp_weights`\\n\\n      [0, 0]: 1                [0, 0]: a\\n      [1, 0]: 0                [1, 0]: b\\n      [1, 1]: 4                [1, 1]: c\\n      [2, 0]: 2                [2, 0]: d\\n\\n\\n  Deciding when to use `sparse_tensor_dense_matmul` vs.\\n  `matmul`(a_is_sparse=True):\\n\\n  There are a number of questions to ask in the decision process, including:\\n\\n  * Will the SparseTensor `A` fit in memory if densified?\\n  * Is the column count of the product large (>> 1)?\\n  * Is the density of `A` larger than approximately 15%?\\n\\n  If the answer to several of these questions is yes, consider\\n  converting the `SparseTensor` to a dense one and using `tf.matmul` with\\n  `a_is_sparse=True`.\\n\\n  This operation tends to perform well when `A` is more sparse, if the column\\n  size of the product is small (e.g. matrix-vector multiplication), if\\n  `sp_a.dense_shape` takes on large values.\\n\\n  Below is a rough speed comparison between `sparse_tensor_dense_matmul`,\\n  labeled \\'sparse\\', and `matmul`(a_is_sparse=True), labeled \\'dense\\'.  For\\n  purposes of the comparison, the time spent converting from a `SparseTensor` to\\n  a dense `Tensor` is not included, so it is overly conservative with respect to\\n  the time ratio.\\n\\n  Benchmark system:\\n  CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB\\n  GPU: NVidia Tesla k40c\\n\\n  Compiled with:\\n  `-c opt --config=cuda --copt=-mavx`\\n\\n  ```\\n  tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks\\n  A sparse [m, k] with % nonzero values between 1% and 80%\\n  B dense [k, n]\\n\\n  % nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)/dt(dense)\\n  0.01   1   True  100   100   0.000221166   0.00010154   0.459112\\n  0.01   1   True  100   1000  0.00033858    0.000109275  0.322745\\n  0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385\\n  0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669\\n  0.01   1   False 100   100   0.000208085   0.000107603  0.51711\\n  0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762\\n  0.01   1   False 1000  100   0.000308222   0.00010345   0.335635\\n  0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124\\n  0.01   10  True  100   100   0.000218522   0.000105537  0.482958\\n  0.01   10  True  100   1000  0.000340882   0.000111641  0.327506\\n  0.01   10  True  1000  100   0.000315472   0.000117376  0.372064\\n  0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128\\n  0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354\\n  0.01   10  False 100   1000  0.000330552   0.000112615  0.340687\\n  0.01   10  False 1000  100   0.000341277   0.000114097  0.334324\\n  0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549\\n  0.01   25  True  100   100   0.000207806   0.000105977  0.509981\\n  0.01   25  True  100   1000  0.000322879   0.00012921   0.400181\\n  0.01   25  True  1000  100   0.00038262    0.00014158   0.370035\\n  0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504\\n  0.01   25  False 100   100   0.000209401   0.000104696  0.499979\\n  0.01   25  False 100   1000  0.000321161   0.000130737  0.407076\\n  0.01   25  False 1000  100   0.000377012   0.000136801  0.362856\\n  0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413\\n  0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833\\n  0.2    1   True  100   1000  0.000348674   0.000147475  0.422959\\n  0.2    1   True  1000  100   0.000336908   0.00010122   0.300439\\n  0.2    1   True  1000  1000  0.001022      0.000203274  0.198898\\n  0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746\\n  0.2    1   False 100   1000  0.000356127   0.000146824  0.41228\\n  0.2    1   False 1000  100   0.000322664   0.000100918  0.312764\\n  0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648\\n  0.2    10  True  100   100   0.000211692   0.000109903  0.519165\\n  0.2    10  True  100   1000  0.000372819   0.000164321  0.440753\\n  0.2    10  True  1000  100   0.000338651   0.000144806  0.427596\\n  0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064\\n  0.2    10  False 100   100   0.000215727   0.000110502  0.512231\\n  0.2    10  False 100   1000  0.000375419   0.0001613    0.429653\\n  0.2    10  False 1000  100   0.000336999   0.000145628  0.432132\\n  0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618\\n  0.2    25  True  100   100   0.000218705   0.000129913  0.594009\\n  0.2    25  True  100   1000  0.000394794   0.00029428   0.745402\\n  0.2    25  True  1000  100   0.000404483   0.0002693    0.665788\\n  0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052\\n  0.2    25  False 100   100   0.000221494   0.0001306    0.589632\\n  0.2    25  False 100   1000  0.000396436   0.000297204  0.74969\\n  0.2    25  False 1000  100   0.000409346   0.000270068  0.659754\\n  0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046\\n  0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836\\n  0.5    1   True  100   1000  0.000415328   0.000223073  0.537101\\n  0.5    1   True  1000  100   0.000358324   0.00011269   0.314492\\n  0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851\\n  0.5    1   False 100   100   0.000224196   0.000101423  0.452386\\n  0.5    1   False 100   1000  0.000400987   0.000223286  0.556841\\n  0.5    1   False 1000  100   0.000368825   0.00011224   0.304318\\n  0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563\\n  0.5    10  True  100   100   0.000222125   0.000112308  0.505608\\n  0.5    10  True  100   1000  0.000461088   0.00032357   0.701753\\n  0.5    10  True  1000  100   0.000394624   0.000225497  0.571422\\n  0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801\\n  0.5    10  False 100   100   0.000232083   0.000114978  0.495418\\n  0.5    10  False 100   1000  0.000454574   0.000324632  0.714146\\n  0.5    10  False 1000  100   0.000379097   0.000227768  0.600817\\n  0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638\\n  0.5    25  True  100   100   0.00023429    0.000151703  0.647501\\n  0.5    25  True  100   1000  0.000497462   0.000598873  1.20386\\n  0.5    25  True  1000  100   0.000460778   0.000557038  1.20891\\n  0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845\\n  0.5    25  False 100   100   0.000228981   0.000155334  0.678371\\n  0.5    25  False 100   1000  0.000496139   0.000620789  1.25124\\n  0.5    25  False 1000  100   0.00045473    0.000551528  1.21287\\n  0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927\\n  0.8    1   True  100   100   0.000222037   0.000105301  0.47425\\n  0.8    1   True  100   1000  0.000410804   0.000329327  0.801664\\n  0.8    1   True  1000  100   0.000349735   0.000131225  0.375212\\n  0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633\\n  0.8    1   False 100   100   0.000214079   0.000107486  0.502085\\n  0.8    1   False 100   1000  0.000413746   0.000323244  0.781261\\n  0.8    1   False 1000  100   0.000348983   0.000131983  0.378193\\n  0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282\\n  0.8    10  True  100   100   0.000229159   0.00011825   0.516017\\n  0.8    10  True  100   1000  0.000498845   0.000532618  1.0677\\n  0.8    10  True  1000  100   0.000383126   0.00029935   0.781336\\n  0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689\\n  0.8    10  False 100   100   0.000230783   0.000124958  0.541452\\n  0.8    10  False 100   1000  0.000493393   0.000550654  1.11606\\n  0.8    10  False 1000  100   0.000377167   0.000298581  0.791642\\n  0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024\\n  0.8    25  True  100   100   0.000233496   0.000175241  0.75051\\n  0.8    25  True  100   1000  0.00055654    0.00102658   1.84458\\n  0.8    25  True  1000  100   0.000463814   0.000783267  1.68875\\n  0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132\\n  0.8    25  False 100   100   0.000240243   0.000175047  0.728625\\n  0.8    25  False 100   1000  0.000578102   0.00104499   1.80763\\n  0.8    25  False 1000  100   0.000485113   0.000776849  1.60138\\n  0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992\\n  ```\\n\\n  '\n    if isinstance(b, sparse_tensor.SparseTensor) or isinstance(b, sparse_tensor.SparseTensorValue):\n        if adjoint_a != adjoint_b:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a, adjoint_b))\n        else:\n            return array_ops.transpose(sparse_tensor_dense_matmul(b, sp_a, adjoint_a=not adjoint_a, adjoint_b=not adjoint_b))\n    else:\n        sp_a = _convert_to_sparse_tensor(sp_a)\n        with ops.name_scope(name, 'SparseTensorDenseMatMul', [sp_a.indices, sp_a.values, b]) as name:\n            b = ops.convert_to_tensor(b, name='b')\n            return gen_sparse_ops.sparse_tensor_dense_mat_mul(a_indices=sp_a.indices, a_values=sp_a.values, a_shape=sp_a.dense_shape, b=b, adjoint_a=adjoint_a, adjoint_b=adjoint_b)"
        ]
    },
    {
        "func_name": "sparse_softmax",
        "original": "@tf_export('sparse.softmax', v1=['sparse.softmax', 'sparse_softmax'])\n@deprecation.deprecated_endpoints('sparse_softmax')\ndef sparse_softmax(sp_input, name=None):\n    \"\"\"Applies softmax to a batched N-D `SparseTensor`.\n\n  The inputs represent an N-D SparseTensor with logical shape `[..., B, C]`\n  (where `N >= 2`), and with indices sorted in the canonical lexicographic\n  order.\n\n  This op is equivalent to applying the normal `tf.nn.softmax()` to each\n  innermost logical submatrix with shape `[B, C]`, but with the catch that *the\n  implicitly zero elements do not participate*.  Specifically, the algorithm is\n  equivalent to:\n\n    (1) Applies `tf.nn.softmax()` to a densified view of each innermost\n        submatrix with shape `[B, C]`, along the size-C dimension;\n    (2) Masks out the original implicitly-zero locations;\n    (3) Renormalizes the remaining elements.\n\n  Hence, the `SparseTensor` result has exactly the same non-zero indices and\n  shape.\n\n  Example using a 3-D SparseTensor:\n\n    >>> st = tf.sparse.from_dense(\n    ...   [[[0., np.e],\n    ...     [1., 0.]],\n    ...\n    ...    [[np.e, 0.],\n    ...     [np.e, np.e]]])\n    >>> res = tf.sparse.softmax(st)\n    >>> res.indices\n    <tf.Tensor: shape=(5, 3), dtype=int64, numpy=\n    array([[0, 0, 1],\n           [0, 1, 0],\n           [1, 0, 0],\n           [1, 1, 0],\n           [1, 1, 1]])>\n    >>> res.values\n    <tf.Tensor: ... numpy=array([1. , 1. , 1. , 0.5, 0.5], dtype=float32)>\n    >>> res.dense_shape\n    <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 2])>\n    >>> tf.sparse.to_dense(res)\n    <tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\n    array([[[0. , 1. ],\n            [1. , 0. ]],\n           [[1. , 0. ],\n            [0.5, 0.5]]], dtype=float32)>\n\n  Args:\n    sp_input: N-D `SparseTensor`, where `N >= 2`.\n    name: optional name of the operation.\n  Returns:\n    output: N-D `SparseTensor` representing the results.\n  \"\"\"\n    with ops.name_scope(name, 'SparseSoftmax', [sp_input.indices, sp_input.values]) as name:\n        out_vals = gen_sparse_ops.sparse_softmax(sp_input.indices, sp_input.values, sp_input.dense_shape)\n        return sparse_tensor.SparseTensor(sp_input.indices, out_vals, sp_input.dense_shape)",
        "mutated": [
            "@tf_export('sparse.softmax', v1=['sparse.softmax', 'sparse_softmax'])\n@deprecation.deprecated_endpoints('sparse_softmax')\ndef sparse_softmax(sp_input, name=None):\n    if False:\n        i = 10\n    'Applies softmax to a batched N-D `SparseTensor`.\\n\\n  The inputs represent an N-D SparseTensor with logical shape `[..., B, C]`\\n  (where `N >= 2`), and with indices sorted in the canonical lexicographic\\n  order.\\n\\n  This op is equivalent to applying the normal `tf.nn.softmax()` to each\\n  innermost logical submatrix with shape `[B, C]`, but with the catch that *the\\n  implicitly zero elements do not participate*.  Specifically, the algorithm is\\n  equivalent to:\\n\\n    (1) Applies `tf.nn.softmax()` to a densified view of each innermost\\n        submatrix with shape `[B, C]`, along the size-C dimension;\\n    (2) Masks out the original implicitly-zero locations;\\n    (3) Renormalizes the remaining elements.\\n\\n  Hence, the `SparseTensor` result has exactly the same non-zero indices and\\n  shape.\\n\\n  Example using a 3-D SparseTensor:\\n\\n    >>> st = tf.sparse.from_dense(\\n    ...   [[[0., np.e],\\n    ...     [1., 0.]],\\n    ...\\n    ...    [[np.e, 0.],\\n    ...     [np.e, np.e]]])\\n    >>> res = tf.sparse.softmax(st)\\n    >>> res.indices\\n    <tf.Tensor: shape=(5, 3), dtype=int64, numpy=\\n    array([[0, 0, 1],\\n           [0, 1, 0],\\n           [1, 0, 0],\\n           [1, 1, 0],\\n           [1, 1, 1]])>\\n    >>> res.values\\n    <tf.Tensor: ... numpy=array([1. , 1. , 1. , 0.5, 0.5], dtype=float32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 2])>\\n    >>> tf.sparse.to_dense(res)\\n    <tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\\n    array([[[0. , 1. ],\\n            [1. , 0. ]],\\n           [[1. , 0. ],\\n            [0.5, 0.5]]], dtype=float32)>\\n\\n  Args:\\n    sp_input: N-D `SparseTensor`, where `N >= 2`.\\n    name: optional name of the operation.\\n  Returns:\\n    output: N-D `SparseTensor` representing the results.\\n  '\n    with ops.name_scope(name, 'SparseSoftmax', [sp_input.indices, sp_input.values]) as name:\n        out_vals = gen_sparse_ops.sparse_softmax(sp_input.indices, sp_input.values, sp_input.dense_shape)\n        return sparse_tensor.SparseTensor(sp_input.indices, out_vals, sp_input.dense_shape)",
            "@tf_export('sparse.softmax', v1=['sparse.softmax', 'sparse_softmax'])\n@deprecation.deprecated_endpoints('sparse_softmax')\ndef sparse_softmax(sp_input, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Applies softmax to a batched N-D `SparseTensor`.\\n\\n  The inputs represent an N-D SparseTensor with logical shape `[..., B, C]`\\n  (where `N >= 2`), and with indices sorted in the canonical lexicographic\\n  order.\\n\\n  This op is equivalent to applying the normal `tf.nn.softmax()` to each\\n  innermost logical submatrix with shape `[B, C]`, but with the catch that *the\\n  implicitly zero elements do not participate*.  Specifically, the algorithm is\\n  equivalent to:\\n\\n    (1) Applies `tf.nn.softmax()` to a densified view of each innermost\\n        submatrix with shape `[B, C]`, along the size-C dimension;\\n    (2) Masks out the original implicitly-zero locations;\\n    (3) Renormalizes the remaining elements.\\n\\n  Hence, the `SparseTensor` result has exactly the same non-zero indices and\\n  shape.\\n\\n  Example using a 3-D SparseTensor:\\n\\n    >>> st = tf.sparse.from_dense(\\n    ...   [[[0., np.e],\\n    ...     [1., 0.]],\\n    ...\\n    ...    [[np.e, 0.],\\n    ...     [np.e, np.e]]])\\n    >>> res = tf.sparse.softmax(st)\\n    >>> res.indices\\n    <tf.Tensor: shape=(5, 3), dtype=int64, numpy=\\n    array([[0, 0, 1],\\n           [0, 1, 0],\\n           [1, 0, 0],\\n           [1, 1, 0],\\n           [1, 1, 1]])>\\n    >>> res.values\\n    <tf.Tensor: ... numpy=array([1. , 1. , 1. , 0.5, 0.5], dtype=float32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 2])>\\n    >>> tf.sparse.to_dense(res)\\n    <tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\\n    array([[[0. , 1. ],\\n            [1. , 0. ]],\\n           [[1. , 0. ],\\n            [0.5, 0.5]]], dtype=float32)>\\n\\n  Args:\\n    sp_input: N-D `SparseTensor`, where `N >= 2`.\\n    name: optional name of the operation.\\n  Returns:\\n    output: N-D `SparseTensor` representing the results.\\n  '\n    with ops.name_scope(name, 'SparseSoftmax', [sp_input.indices, sp_input.values]) as name:\n        out_vals = gen_sparse_ops.sparse_softmax(sp_input.indices, sp_input.values, sp_input.dense_shape)\n        return sparse_tensor.SparseTensor(sp_input.indices, out_vals, sp_input.dense_shape)",
            "@tf_export('sparse.softmax', v1=['sparse.softmax', 'sparse_softmax'])\n@deprecation.deprecated_endpoints('sparse_softmax')\ndef sparse_softmax(sp_input, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Applies softmax to a batched N-D `SparseTensor`.\\n\\n  The inputs represent an N-D SparseTensor with logical shape `[..., B, C]`\\n  (where `N >= 2`), and with indices sorted in the canonical lexicographic\\n  order.\\n\\n  This op is equivalent to applying the normal `tf.nn.softmax()` to each\\n  innermost logical submatrix with shape `[B, C]`, but with the catch that *the\\n  implicitly zero elements do not participate*.  Specifically, the algorithm is\\n  equivalent to:\\n\\n    (1) Applies `tf.nn.softmax()` to a densified view of each innermost\\n        submatrix with shape `[B, C]`, along the size-C dimension;\\n    (2) Masks out the original implicitly-zero locations;\\n    (3) Renormalizes the remaining elements.\\n\\n  Hence, the `SparseTensor` result has exactly the same non-zero indices and\\n  shape.\\n\\n  Example using a 3-D SparseTensor:\\n\\n    >>> st = tf.sparse.from_dense(\\n    ...   [[[0., np.e],\\n    ...     [1., 0.]],\\n    ...\\n    ...    [[np.e, 0.],\\n    ...     [np.e, np.e]]])\\n    >>> res = tf.sparse.softmax(st)\\n    >>> res.indices\\n    <tf.Tensor: shape=(5, 3), dtype=int64, numpy=\\n    array([[0, 0, 1],\\n           [0, 1, 0],\\n           [1, 0, 0],\\n           [1, 1, 0],\\n           [1, 1, 1]])>\\n    >>> res.values\\n    <tf.Tensor: ... numpy=array([1. , 1. , 1. , 0.5, 0.5], dtype=float32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 2])>\\n    >>> tf.sparse.to_dense(res)\\n    <tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\\n    array([[[0. , 1. ],\\n            [1. , 0. ]],\\n           [[1. , 0. ],\\n            [0.5, 0.5]]], dtype=float32)>\\n\\n  Args:\\n    sp_input: N-D `SparseTensor`, where `N >= 2`.\\n    name: optional name of the operation.\\n  Returns:\\n    output: N-D `SparseTensor` representing the results.\\n  '\n    with ops.name_scope(name, 'SparseSoftmax', [sp_input.indices, sp_input.values]) as name:\n        out_vals = gen_sparse_ops.sparse_softmax(sp_input.indices, sp_input.values, sp_input.dense_shape)\n        return sparse_tensor.SparseTensor(sp_input.indices, out_vals, sp_input.dense_shape)",
            "@tf_export('sparse.softmax', v1=['sparse.softmax', 'sparse_softmax'])\n@deprecation.deprecated_endpoints('sparse_softmax')\ndef sparse_softmax(sp_input, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Applies softmax to a batched N-D `SparseTensor`.\\n\\n  The inputs represent an N-D SparseTensor with logical shape `[..., B, C]`\\n  (where `N >= 2`), and with indices sorted in the canonical lexicographic\\n  order.\\n\\n  This op is equivalent to applying the normal `tf.nn.softmax()` to each\\n  innermost logical submatrix with shape `[B, C]`, but with the catch that *the\\n  implicitly zero elements do not participate*.  Specifically, the algorithm is\\n  equivalent to:\\n\\n    (1) Applies `tf.nn.softmax()` to a densified view of each innermost\\n        submatrix with shape `[B, C]`, along the size-C dimension;\\n    (2) Masks out the original implicitly-zero locations;\\n    (3) Renormalizes the remaining elements.\\n\\n  Hence, the `SparseTensor` result has exactly the same non-zero indices and\\n  shape.\\n\\n  Example using a 3-D SparseTensor:\\n\\n    >>> st = tf.sparse.from_dense(\\n    ...   [[[0., np.e],\\n    ...     [1., 0.]],\\n    ...\\n    ...    [[np.e, 0.],\\n    ...     [np.e, np.e]]])\\n    >>> res = tf.sparse.softmax(st)\\n    >>> res.indices\\n    <tf.Tensor: shape=(5, 3), dtype=int64, numpy=\\n    array([[0, 0, 1],\\n           [0, 1, 0],\\n           [1, 0, 0],\\n           [1, 1, 0],\\n           [1, 1, 1]])>\\n    >>> res.values\\n    <tf.Tensor: ... numpy=array([1. , 1. , 1. , 0.5, 0.5], dtype=float32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 2])>\\n    >>> tf.sparse.to_dense(res)\\n    <tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\\n    array([[[0. , 1. ],\\n            [1. , 0. ]],\\n           [[1. , 0. ],\\n            [0.5, 0.5]]], dtype=float32)>\\n\\n  Args:\\n    sp_input: N-D `SparseTensor`, where `N >= 2`.\\n    name: optional name of the operation.\\n  Returns:\\n    output: N-D `SparseTensor` representing the results.\\n  '\n    with ops.name_scope(name, 'SparseSoftmax', [sp_input.indices, sp_input.values]) as name:\n        out_vals = gen_sparse_ops.sparse_softmax(sp_input.indices, sp_input.values, sp_input.dense_shape)\n        return sparse_tensor.SparseTensor(sp_input.indices, out_vals, sp_input.dense_shape)",
            "@tf_export('sparse.softmax', v1=['sparse.softmax', 'sparse_softmax'])\n@deprecation.deprecated_endpoints('sparse_softmax')\ndef sparse_softmax(sp_input, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Applies softmax to a batched N-D `SparseTensor`.\\n\\n  The inputs represent an N-D SparseTensor with logical shape `[..., B, C]`\\n  (where `N >= 2`), and with indices sorted in the canonical lexicographic\\n  order.\\n\\n  This op is equivalent to applying the normal `tf.nn.softmax()` to each\\n  innermost logical submatrix with shape `[B, C]`, but with the catch that *the\\n  implicitly zero elements do not participate*.  Specifically, the algorithm is\\n  equivalent to:\\n\\n    (1) Applies `tf.nn.softmax()` to a densified view of each innermost\\n        submatrix with shape `[B, C]`, along the size-C dimension;\\n    (2) Masks out the original implicitly-zero locations;\\n    (3) Renormalizes the remaining elements.\\n\\n  Hence, the `SparseTensor` result has exactly the same non-zero indices and\\n  shape.\\n\\n  Example using a 3-D SparseTensor:\\n\\n    >>> st = tf.sparse.from_dense(\\n    ...   [[[0., np.e],\\n    ...     [1., 0.]],\\n    ...\\n    ...    [[np.e, 0.],\\n    ...     [np.e, np.e]]])\\n    >>> res = tf.sparse.softmax(st)\\n    >>> res.indices\\n    <tf.Tensor: shape=(5, 3), dtype=int64, numpy=\\n    array([[0, 0, 1],\\n           [0, 1, 0],\\n           [1, 0, 0],\\n           [1, 1, 0],\\n           [1, 1, 1]])>\\n    >>> res.values\\n    <tf.Tensor: ... numpy=array([1. , 1. , 1. , 0.5, 0.5], dtype=float32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 2])>\\n    >>> tf.sparse.to_dense(res)\\n    <tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\\n    array([[[0. , 1. ],\\n            [1. , 0. ]],\\n           [[1. , 0. ],\\n            [0.5, 0.5]]], dtype=float32)>\\n\\n  Args:\\n    sp_input: N-D `SparseTensor`, where `N >= 2`.\\n    name: optional name of the operation.\\n  Returns:\\n    output: N-D `SparseTensor` representing the results.\\n  '\n    with ops.name_scope(name, 'SparseSoftmax', [sp_input.indices, sp_input.values]) as name:\n        out_vals = gen_sparse_ops.sparse_softmax(sp_input.indices, sp_input.values, sp_input.dense_shape)\n        return sparse_tensor.SparseTensor(sp_input.indices, out_vals, sp_input.dense_shape)"
        ]
    },
    {
        "func_name": "sparse_maximum",
        "original": "@tf_export('sparse.maximum', v1=['sparse.maximum', 'sparse_maximum'])\n@deprecation.deprecated_endpoints('sparse_maximum')\ndef sparse_maximum(sp_a, sp_b, name=None):\n    \"\"\"Returns the element-wise max of two SparseTensors.\n\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\n\n  Example:\n\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\n    >>> res = tf.sparse.maximum(sp_zero, sp_one)\n    >>> res.indices\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\n    array([[0],\n           [1]])>\n    >>> res.values\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>\n    >>> res.dense_shape\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\n\n  The reduction version of this elementwise operation is `tf.sparse.reduce_max`\n\n  Args:\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\n      lexicographically ordered.\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\n      same shape).\n    name: optional name of the operation.\n  Returns:\n    output: the output SparseTensor.\n  \"\"\"\n    with ops.name_scope(name, 'SparseSparseMaximum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_maximum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
        "mutated": [
            "@tf_export('sparse.maximum', v1=['sparse.maximum', 'sparse_maximum'])\n@deprecation.deprecated_endpoints('sparse_maximum')\ndef sparse_maximum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n    'Returns the element-wise max of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.maximum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  The reduction version of this elementwise operation is `tf.sparse.reduce_max`\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMaximum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_maximum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
            "@tf_export('sparse.maximum', v1=['sparse.maximum', 'sparse_maximum'])\n@deprecation.deprecated_endpoints('sparse_maximum')\ndef sparse_maximum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the element-wise max of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.maximum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  The reduction version of this elementwise operation is `tf.sparse.reduce_max`\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMaximum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_maximum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
            "@tf_export('sparse.maximum', v1=['sparse.maximum', 'sparse_maximum'])\n@deprecation.deprecated_endpoints('sparse_maximum')\ndef sparse_maximum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the element-wise max of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.maximum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  The reduction version of this elementwise operation is `tf.sparse.reduce_max`\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMaximum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_maximum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
            "@tf_export('sparse.maximum', v1=['sparse.maximum', 'sparse_maximum'])\n@deprecation.deprecated_endpoints('sparse_maximum')\ndef sparse_maximum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the element-wise max of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.maximum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  The reduction version of this elementwise operation is `tf.sparse.reduce_max`\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMaximum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_maximum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
            "@tf_export('sparse.maximum', v1=['sparse.maximum', 'sparse_maximum'])\n@deprecation.deprecated_endpoints('sparse_maximum')\ndef sparse_maximum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the element-wise max of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.maximum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  The reduction version of this elementwise operation is `tf.sparse.reduce_max`\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMaximum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_maximum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)"
        ]
    },
    {
        "func_name": "sparse_minimum",
        "original": "@tf_export('sparse.minimum', v1=['sparse.minimum', 'sparse_minimum'])\n@deprecation.deprecated_endpoints('sparse_minimum')\ndef sparse_minimum(sp_a, sp_b, name=None):\n    \"\"\"Returns the element-wise min of two SparseTensors.\n\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\n\n  Example:\n\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\n    >>> res = tf.sparse.minimum(sp_zero, sp_one)\n    >>> res.indices\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\n    array([[0],\n           [1]])>\n    >>> res.values\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>\n    >>> res.dense_shape\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\n\n  Args:\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\n      lexicographically ordered.\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\n      same shape).\n    name: optional name of the operation.\n  Returns:\n    output: the output SparseTensor.\n  \"\"\"\n    with ops.name_scope(name, 'SparseSparseMinimum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_minimum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
        "mutated": [
            "@tf_export('sparse.minimum', v1=['sparse.minimum', 'sparse_minimum'])\n@deprecation.deprecated_endpoints('sparse_minimum')\ndef sparse_minimum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n    'Returns the element-wise min of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.minimum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMinimum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_minimum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
            "@tf_export('sparse.minimum', v1=['sparse.minimum', 'sparse_minimum'])\n@deprecation.deprecated_endpoints('sparse_minimum')\ndef sparse_minimum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the element-wise min of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.minimum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMinimum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_minimum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
            "@tf_export('sparse.minimum', v1=['sparse.minimum', 'sparse_minimum'])\n@deprecation.deprecated_endpoints('sparse_minimum')\ndef sparse_minimum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the element-wise min of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.minimum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMinimum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_minimum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
            "@tf_export('sparse.minimum', v1=['sparse.minimum', 'sparse_minimum'])\n@deprecation.deprecated_endpoints('sparse_minimum')\ndef sparse_minimum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the element-wise min of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.minimum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMinimum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_minimum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)",
            "@tf_export('sparse.minimum', v1=['sparse.minimum', 'sparse_minimum'])\n@deprecation.deprecated_endpoints('sparse_minimum')\ndef sparse_minimum(sp_a, sp_b, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the element-wise min of two SparseTensors.\\n\\n  Assumes the two SparseTensors have the same shape, i.e., no broadcasting.\\n\\n  Example:\\n\\n    >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])\\n    >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])\\n    >>> res = tf.sparse.minimum(sp_zero, sp_one)\\n    >>> res.indices\\n    <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\\n    array([[0],\\n           [1]])>\\n    >>> res.values\\n    <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>\\n    >>> res.dense_shape\\n    <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>\\n\\n  Args:\\n    sp_a: a `SparseTensor` operand whose dtype is real, and indices\\n      lexicographically ordered.\\n    sp_b: the other `SparseTensor` operand with the same requirements (and the\\n      same shape).\\n    name: optional name of the operation.\\n  Returns:\\n    output: the output SparseTensor.\\n  '\n    with ops.name_scope(name, 'SparseSparseMinimum', [sp_a.indices, sp_a.values, sp_b.indices, sp_b.values]) as name:\n        (out_indices, out_values) = gen_sparse_ops.sparse_sparse_minimum(sp_a.indices, sp_a.values, sp_a.dense_shape, sp_b.indices, sp_b.values, sp_b.dense_shape, name=name)\n    return sparse_tensor.SparseTensor(out_indices, out_values, sp_a.dense_shape)"
        ]
    },
    {
        "func_name": "sparse_transpose",
        "original": "@tf_export('sparse.transpose', v1=['sparse.transpose', 'sparse_transpose'])\n@deprecation.deprecated_endpoints('sparse_transpose')\ndef sparse_transpose(sp_input, perm=None, name=None):\n    \"\"\"Transposes a `SparseTensor`.\n\n  Permutes the dimensions according to the value of `perm`.  This is the sparse\n  version of `tf.transpose`.\n\n  The returned tensor's dimension `i` will correspond to the input dimension\n  `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\n  of the input tensor. Hence, by default, this operation performs a regular\n  matrix transpose on 2-D input Tensors.\n\n  For example:\n\n  >>> x = tf.SparseTensor(indices=[[0, 1], [0, 3], [2, 3], [3, 1]],\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\n  ...                     dense_shape=[4, 5])\n  >>> print('x =', tf.sparse.to_dense(x))\n  x = tf.Tensor(\n  [[0.  1.1 0.  2.2 0. ]\n  [0.  0.  0.  0.  0. ]\n  [0.  0.  0.  3.3 0. ]\n  [0.  4.4 0.  0.  0. ]], shape=(4, 5), dtype=float32)\n\n  >>> x_transpose = tf.sparse.transpose(x)\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\n  x_transpose = tf.Tensor(\n  [[0.  0.  0.  0. ]\n  [1.1 0.  0.  4.4]\n  [0.  0.  0.  0. ]\n  [2.2 0.  3.3 0. ]\n  [0.  0.  0.  0. ]], shape=(5, 4), dtype=float32)\n\n  Equivalently, you could call `tf.sparse.transpose(x, perm=[1, 0])`.  The\n  `perm` argument is more useful for n-dimensional tensors where n > 2.\n\n  >>> x = tf.SparseTensor(indices=[[0, 0, 1], [0, 0, 3], [1, 2, 3], [1, 3, 1]],\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\n  ...                     dense_shape=[2, 4, 5])\n  >>> print('x =', tf.sparse.to_dense(x))\n  x = tf.Tensor(\n  [[[0.  1.1 0.  2.2 0. ]\n    [0.  0.  0.  0.  0. ]\n    [0.  0.  0.  0.  0. ]\n    [0.  0.  0.  0.  0. ]]\n  [[0.  0.  0.  0.  0. ]\n    [0.  0.  0.  0.  0. ]\n    [0.  0.  0.  3.3 0. ]\n    [0.  4.4 0.  0.  0. ]]], shape=(2, 4, 5), dtype=float32)\n\n  As above, simply calling `tf.sparse.transpose` will default to `perm=[2,1,0]`.\n\n  To take the transpose of a batch of sparse matrices, where 0 is the batch\n  dimension, you would set `perm=[0,2,1]`.\n\n  >>> x_transpose = tf.sparse.transpose(x, perm=[0, 2, 1])\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\n  x_transpose = tf.Tensor(\n  [[[0.  0.  0.  0. ]\n    [1.1 0.  0.  0. ]\n    [0.  0.  0.  0. ]\n    [2.2 0.  0.  0. ]\n    [0.  0.  0.  0. ]]\n  [[0.  0.  0.  0. ]\n    [0.  0.  0.  4.4]\n    [0.  0.  0.  0. ]\n    [0.  0.  3.3 0. ]\n    [0.  0.  0.  0. ]]], shape=(2, 5, 4), dtype=float32)\n\n  Args:\n    sp_input: The input `SparseTensor`.\n    perm: A permutation vector of the dimensions of `sp_input`.\n    name: A name prefix for the returned tensors (optional).\n\n  Returns:\n    A transposed `SparseTensor`.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    with ops.name_scope(name, 'SparseTranspose', [sp_input]) as name:\n        if perm is None:\n            if sp_input.shape.rank is not None:\n                rank = sp_input.shape.rank\n                perm = rank - 1 - np.arange(0, rank, 1)\n            else:\n                rank = array_ops.rank(sp_input)\n                perm = rank - 1 - math_ops.range(0, rank, 1)\n        indices = sp_input.indices\n        transposed_indices = array_ops.transpose(array_ops.gather(array_ops.transpose(indices), perm))\n        perm_ = tensor_util.constant_value(ops.convert_to_tensor(perm))\n        if perm_ is not None and sp_input.get_shape().is_fully_defined():\n            old_shape_ = sp_input.get_shape().as_list()\n            transposed_dense_shape = list(old_shape_)\n            for (i, p) in enumerate(perm_):\n                transposed_dense_shape[i] = old_shape_[p]\n        else:\n            dense_shape = sp_input.dense_shape\n            transposed_dense_shape = array_ops.gather(dense_shape, perm)\n        transposed_st = sparse_tensor.SparseTensor(transposed_indices, sp_input.values, transposed_dense_shape)\n        transposed_st = sparse_reorder(transposed_st)\n        return transposed_st",
        "mutated": [
            "@tf_export('sparse.transpose', v1=['sparse.transpose', 'sparse_transpose'])\n@deprecation.deprecated_endpoints('sparse_transpose')\ndef sparse_transpose(sp_input, perm=None, name=None):\n    if False:\n        i = 10\n    \"Transposes a `SparseTensor`.\\n\\n  Permutes the dimensions according to the value of `perm`.  This is the sparse\\n  version of `tf.transpose`.\\n\\n  The returned tensor's dimension `i` will correspond to the input dimension\\n  `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\\n  of the input tensor. Hence, by default, this operation performs a regular\\n  matrix transpose on 2-D input Tensors.\\n\\n  For example:\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 1], [0, 3], [2, 3], [3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[0.  1.1 0.  2.2 0. ]\\n  [0.  0.  0.  0.  0. ]\\n  [0.  0.  0.  3.3 0. ]\\n  [0.  4.4 0.  0.  0. ]], shape=(4, 5), dtype=float32)\\n\\n  >>> x_transpose = tf.sparse.transpose(x)\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[0.  0.  0.  0. ]\\n  [1.1 0.  0.  4.4]\\n  [0.  0.  0.  0. ]\\n  [2.2 0.  3.3 0. ]\\n  [0.  0.  0.  0. ]], shape=(5, 4), dtype=float32)\\n\\n  Equivalently, you could call `tf.sparse.transpose(x, perm=[1, 0])`.  The\\n  `perm` argument is more useful for n-dimensional tensors where n > 2.\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 0, 1], [0, 0, 3], [1, 2, 3], [1, 3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[2, 4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[[0.  1.1 0.  2.2 0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  3.3 0. ]\\n    [0.  4.4 0.  0.  0. ]]], shape=(2, 4, 5), dtype=float32)\\n\\n  As above, simply calling `tf.sparse.transpose` will default to `perm=[2,1,0]`.\\n\\n  To take the transpose of a batch of sparse matrices, where 0 is the batch\\n  dimension, you would set `perm=[0,2,1]`.\\n\\n  >>> x_transpose = tf.sparse.transpose(x, perm=[0, 2, 1])\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[[0.  0.  0.  0. ]\\n    [1.1 0.  0.  0. ]\\n    [0.  0.  0.  0. ]\\n    [2.2 0.  0.  0. ]\\n    [0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0. ]\\n    [0.  0.  0.  4.4]\\n    [0.  0.  0.  0. ]\\n    [0.  0.  3.3 0. ]\\n    [0.  0.  0.  0. ]]], shape=(2, 5, 4), dtype=float32)\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    perm: A permutation vector of the dimensions of `sp_input`.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A transposed `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    with ops.name_scope(name, 'SparseTranspose', [sp_input]) as name:\n        if perm is None:\n            if sp_input.shape.rank is not None:\n                rank = sp_input.shape.rank\n                perm = rank - 1 - np.arange(0, rank, 1)\n            else:\n                rank = array_ops.rank(sp_input)\n                perm = rank - 1 - math_ops.range(0, rank, 1)\n        indices = sp_input.indices\n        transposed_indices = array_ops.transpose(array_ops.gather(array_ops.transpose(indices), perm))\n        perm_ = tensor_util.constant_value(ops.convert_to_tensor(perm))\n        if perm_ is not None and sp_input.get_shape().is_fully_defined():\n            old_shape_ = sp_input.get_shape().as_list()\n            transposed_dense_shape = list(old_shape_)\n            for (i, p) in enumerate(perm_):\n                transposed_dense_shape[i] = old_shape_[p]\n        else:\n            dense_shape = sp_input.dense_shape\n            transposed_dense_shape = array_ops.gather(dense_shape, perm)\n        transposed_st = sparse_tensor.SparseTensor(transposed_indices, sp_input.values, transposed_dense_shape)\n        transposed_st = sparse_reorder(transposed_st)\n        return transposed_st",
            "@tf_export('sparse.transpose', v1=['sparse.transpose', 'sparse_transpose'])\n@deprecation.deprecated_endpoints('sparse_transpose')\ndef sparse_transpose(sp_input, perm=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Transposes a `SparseTensor`.\\n\\n  Permutes the dimensions according to the value of `perm`.  This is the sparse\\n  version of `tf.transpose`.\\n\\n  The returned tensor's dimension `i` will correspond to the input dimension\\n  `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\\n  of the input tensor. Hence, by default, this operation performs a regular\\n  matrix transpose on 2-D input Tensors.\\n\\n  For example:\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 1], [0, 3], [2, 3], [3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[0.  1.1 0.  2.2 0. ]\\n  [0.  0.  0.  0.  0. ]\\n  [0.  0.  0.  3.3 0. ]\\n  [0.  4.4 0.  0.  0. ]], shape=(4, 5), dtype=float32)\\n\\n  >>> x_transpose = tf.sparse.transpose(x)\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[0.  0.  0.  0. ]\\n  [1.1 0.  0.  4.4]\\n  [0.  0.  0.  0. ]\\n  [2.2 0.  3.3 0. ]\\n  [0.  0.  0.  0. ]], shape=(5, 4), dtype=float32)\\n\\n  Equivalently, you could call `tf.sparse.transpose(x, perm=[1, 0])`.  The\\n  `perm` argument is more useful for n-dimensional tensors where n > 2.\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 0, 1], [0, 0, 3], [1, 2, 3], [1, 3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[2, 4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[[0.  1.1 0.  2.2 0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  3.3 0. ]\\n    [0.  4.4 0.  0.  0. ]]], shape=(2, 4, 5), dtype=float32)\\n\\n  As above, simply calling `tf.sparse.transpose` will default to `perm=[2,1,0]`.\\n\\n  To take the transpose of a batch of sparse matrices, where 0 is the batch\\n  dimension, you would set `perm=[0,2,1]`.\\n\\n  >>> x_transpose = tf.sparse.transpose(x, perm=[0, 2, 1])\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[[0.  0.  0.  0. ]\\n    [1.1 0.  0.  0. ]\\n    [0.  0.  0.  0. ]\\n    [2.2 0.  0.  0. ]\\n    [0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0. ]\\n    [0.  0.  0.  4.4]\\n    [0.  0.  0.  0. ]\\n    [0.  0.  3.3 0. ]\\n    [0.  0.  0.  0. ]]], shape=(2, 5, 4), dtype=float32)\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    perm: A permutation vector of the dimensions of `sp_input`.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A transposed `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    with ops.name_scope(name, 'SparseTranspose', [sp_input]) as name:\n        if perm is None:\n            if sp_input.shape.rank is not None:\n                rank = sp_input.shape.rank\n                perm = rank - 1 - np.arange(0, rank, 1)\n            else:\n                rank = array_ops.rank(sp_input)\n                perm = rank - 1 - math_ops.range(0, rank, 1)\n        indices = sp_input.indices\n        transposed_indices = array_ops.transpose(array_ops.gather(array_ops.transpose(indices), perm))\n        perm_ = tensor_util.constant_value(ops.convert_to_tensor(perm))\n        if perm_ is not None and sp_input.get_shape().is_fully_defined():\n            old_shape_ = sp_input.get_shape().as_list()\n            transposed_dense_shape = list(old_shape_)\n            for (i, p) in enumerate(perm_):\n                transposed_dense_shape[i] = old_shape_[p]\n        else:\n            dense_shape = sp_input.dense_shape\n            transposed_dense_shape = array_ops.gather(dense_shape, perm)\n        transposed_st = sparse_tensor.SparseTensor(transposed_indices, sp_input.values, transposed_dense_shape)\n        transposed_st = sparse_reorder(transposed_st)\n        return transposed_st",
            "@tf_export('sparse.transpose', v1=['sparse.transpose', 'sparse_transpose'])\n@deprecation.deprecated_endpoints('sparse_transpose')\ndef sparse_transpose(sp_input, perm=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Transposes a `SparseTensor`.\\n\\n  Permutes the dimensions according to the value of `perm`.  This is the sparse\\n  version of `tf.transpose`.\\n\\n  The returned tensor's dimension `i` will correspond to the input dimension\\n  `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\\n  of the input tensor. Hence, by default, this operation performs a regular\\n  matrix transpose on 2-D input Tensors.\\n\\n  For example:\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 1], [0, 3], [2, 3], [3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[0.  1.1 0.  2.2 0. ]\\n  [0.  0.  0.  0.  0. ]\\n  [0.  0.  0.  3.3 0. ]\\n  [0.  4.4 0.  0.  0. ]], shape=(4, 5), dtype=float32)\\n\\n  >>> x_transpose = tf.sparse.transpose(x)\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[0.  0.  0.  0. ]\\n  [1.1 0.  0.  4.4]\\n  [0.  0.  0.  0. ]\\n  [2.2 0.  3.3 0. ]\\n  [0.  0.  0.  0. ]], shape=(5, 4), dtype=float32)\\n\\n  Equivalently, you could call `tf.sparse.transpose(x, perm=[1, 0])`.  The\\n  `perm` argument is more useful for n-dimensional tensors where n > 2.\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 0, 1], [0, 0, 3], [1, 2, 3], [1, 3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[2, 4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[[0.  1.1 0.  2.2 0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  3.3 0. ]\\n    [0.  4.4 0.  0.  0. ]]], shape=(2, 4, 5), dtype=float32)\\n\\n  As above, simply calling `tf.sparse.transpose` will default to `perm=[2,1,0]`.\\n\\n  To take the transpose of a batch of sparse matrices, where 0 is the batch\\n  dimension, you would set `perm=[0,2,1]`.\\n\\n  >>> x_transpose = tf.sparse.transpose(x, perm=[0, 2, 1])\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[[0.  0.  0.  0. ]\\n    [1.1 0.  0.  0. ]\\n    [0.  0.  0.  0. ]\\n    [2.2 0.  0.  0. ]\\n    [0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0. ]\\n    [0.  0.  0.  4.4]\\n    [0.  0.  0.  0. ]\\n    [0.  0.  3.3 0. ]\\n    [0.  0.  0.  0. ]]], shape=(2, 5, 4), dtype=float32)\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    perm: A permutation vector of the dimensions of `sp_input`.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A transposed `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    with ops.name_scope(name, 'SparseTranspose', [sp_input]) as name:\n        if perm is None:\n            if sp_input.shape.rank is not None:\n                rank = sp_input.shape.rank\n                perm = rank - 1 - np.arange(0, rank, 1)\n            else:\n                rank = array_ops.rank(sp_input)\n                perm = rank - 1 - math_ops.range(0, rank, 1)\n        indices = sp_input.indices\n        transposed_indices = array_ops.transpose(array_ops.gather(array_ops.transpose(indices), perm))\n        perm_ = tensor_util.constant_value(ops.convert_to_tensor(perm))\n        if perm_ is not None and sp_input.get_shape().is_fully_defined():\n            old_shape_ = sp_input.get_shape().as_list()\n            transposed_dense_shape = list(old_shape_)\n            for (i, p) in enumerate(perm_):\n                transposed_dense_shape[i] = old_shape_[p]\n        else:\n            dense_shape = sp_input.dense_shape\n            transposed_dense_shape = array_ops.gather(dense_shape, perm)\n        transposed_st = sparse_tensor.SparseTensor(transposed_indices, sp_input.values, transposed_dense_shape)\n        transposed_st = sparse_reorder(transposed_st)\n        return transposed_st",
            "@tf_export('sparse.transpose', v1=['sparse.transpose', 'sparse_transpose'])\n@deprecation.deprecated_endpoints('sparse_transpose')\ndef sparse_transpose(sp_input, perm=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Transposes a `SparseTensor`.\\n\\n  Permutes the dimensions according to the value of `perm`.  This is the sparse\\n  version of `tf.transpose`.\\n\\n  The returned tensor's dimension `i` will correspond to the input dimension\\n  `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\\n  of the input tensor. Hence, by default, this operation performs a regular\\n  matrix transpose on 2-D input Tensors.\\n\\n  For example:\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 1], [0, 3], [2, 3], [3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[0.  1.1 0.  2.2 0. ]\\n  [0.  0.  0.  0.  0. ]\\n  [0.  0.  0.  3.3 0. ]\\n  [0.  4.4 0.  0.  0. ]], shape=(4, 5), dtype=float32)\\n\\n  >>> x_transpose = tf.sparse.transpose(x)\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[0.  0.  0.  0. ]\\n  [1.1 0.  0.  4.4]\\n  [0.  0.  0.  0. ]\\n  [2.2 0.  3.3 0. ]\\n  [0.  0.  0.  0. ]], shape=(5, 4), dtype=float32)\\n\\n  Equivalently, you could call `tf.sparse.transpose(x, perm=[1, 0])`.  The\\n  `perm` argument is more useful for n-dimensional tensors where n > 2.\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 0, 1], [0, 0, 3], [1, 2, 3], [1, 3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[2, 4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[[0.  1.1 0.  2.2 0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  3.3 0. ]\\n    [0.  4.4 0.  0.  0. ]]], shape=(2, 4, 5), dtype=float32)\\n\\n  As above, simply calling `tf.sparse.transpose` will default to `perm=[2,1,0]`.\\n\\n  To take the transpose of a batch of sparse matrices, where 0 is the batch\\n  dimension, you would set `perm=[0,2,1]`.\\n\\n  >>> x_transpose = tf.sparse.transpose(x, perm=[0, 2, 1])\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[[0.  0.  0.  0. ]\\n    [1.1 0.  0.  0. ]\\n    [0.  0.  0.  0. ]\\n    [2.2 0.  0.  0. ]\\n    [0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0. ]\\n    [0.  0.  0.  4.4]\\n    [0.  0.  0.  0. ]\\n    [0.  0.  3.3 0. ]\\n    [0.  0.  0.  0. ]]], shape=(2, 5, 4), dtype=float32)\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    perm: A permutation vector of the dimensions of `sp_input`.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A transposed `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    with ops.name_scope(name, 'SparseTranspose', [sp_input]) as name:\n        if perm is None:\n            if sp_input.shape.rank is not None:\n                rank = sp_input.shape.rank\n                perm = rank - 1 - np.arange(0, rank, 1)\n            else:\n                rank = array_ops.rank(sp_input)\n                perm = rank - 1 - math_ops.range(0, rank, 1)\n        indices = sp_input.indices\n        transposed_indices = array_ops.transpose(array_ops.gather(array_ops.transpose(indices), perm))\n        perm_ = tensor_util.constant_value(ops.convert_to_tensor(perm))\n        if perm_ is not None and sp_input.get_shape().is_fully_defined():\n            old_shape_ = sp_input.get_shape().as_list()\n            transposed_dense_shape = list(old_shape_)\n            for (i, p) in enumerate(perm_):\n                transposed_dense_shape[i] = old_shape_[p]\n        else:\n            dense_shape = sp_input.dense_shape\n            transposed_dense_shape = array_ops.gather(dense_shape, perm)\n        transposed_st = sparse_tensor.SparseTensor(transposed_indices, sp_input.values, transposed_dense_shape)\n        transposed_st = sparse_reorder(transposed_st)\n        return transposed_st",
            "@tf_export('sparse.transpose', v1=['sparse.transpose', 'sparse_transpose'])\n@deprecation.deprecated_endpoints('sparse_transpose')\ndef sparse_transpose(sp_input, perm=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Transposes a `SparseTensor`.\\n\\n  Permutes the dimensions according to the value of `perm`.  This is the sparse\\n  version of `tf.transpose`.\\n\\n  The returned tensor's dimension `i` will correspond to the input dimension\\n  `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\\n  of the input tensor. Hence, by default, this operation performs a regular\\n  matrix transpose on 2-D input Tensors.\\n\\n  For example:\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 1], [0, 3], [2, 3], [3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[0.  1.1 0.  2.2 0. ]\\n  [0.  0.  0.  0.  0. ]\\n  [0.  0.  0.  3.3 0. ]\\n  [0.  4.4 0.  0.  0. ]], shape=(4, 5), dtype=float32)\\n\\n  >>> x_transpose = tf.sparse.transpose(x)\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[0.  0.  0.  0. ]\\n  [1.1 0.  0.  4.4]\\n  [0.  0.  0.  0. ]\\n  [2.2 0.  3.3 0. ]\\n  [0.  0.  0.  0. ]], shape=(5, 4), dtype=float32)\\n\\n  Equivalently, you could call `tf.sparse.transpose(x, perm=[1, 0])`.  The\\n  `perm` argument is more useful for n-dimensional tensors where n > 2.\\n\\n  >>> x = tf.SparseTensor(indices=[[0, 0, 1], [0, 0, 3], [1, 2, 3], [1, 3, 1]],\\n  ...                     values=[1.1, 2.2, 3.3, 4.4],\\n  ...                     dense_shape=[2, 4, 5])\\n  >>> print('x =', tf.sparse.to_dense(x))\\n  x = tf.Tensor(\\n  [[[0.  1.1 0.  2.2 0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  0.  0. ]\\n    [0.  0.  0.  3.3 0. ]\\n    [0.  4.4 0.  0.  0. ]]], shape=(2, 4, 5), dtype=float32)\\n\\n  As above, simply calling `tf.sparse.transpose` will default to `perm=[2,1,0]`.\\n\\n  To take the transpose of a batch of sparse matrices, where 0 is the batch\\n  dimension, you would set `perm=[0,2,1]`.\\n\\n  >>> x_transpose = tf.sparse.transpose(x, perm=[0, 2, 1])\\n  >>> print('x_transpose =', tf.sparse.to_dense(x_transpose))\\n  x_transpose = tf.Tensor(\\n  [[[0.  0.  0.  0. ]\\n    [1.1 0.  0.  0. ]\\n    [0.  0.  0.  0. ]\\n    [2.2 0.  0.  0. ]\\n    [0.  0.  0.  0. ]]\\n  [[0.  0.  0.  0. ]\\n    [0.  0.  0.  4.4]\\n    [0.  0.  0.  0. ]\\n    [0.  0.  3.3 0. ]\\n    [0.  0.  0.  0. ]]], shape=(2, 5, 4), dtype=float32)\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    perm: A permutation vector of the dimensions of `sp_input`.\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A transposed `SparseTensor`.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  \"\n    with ops.name_scope(name, 'SparseTranspose', [sp_input]) as name:\n        if perm is None:\n            if sp_input.shape.rank is not None:\n                rank = sp_input.shape.rank\n                perm = rank - 1 - np.arange(0, rank, 1)\n            else:\n                rank = array_ops.rank(sp_input)\n                perm = rank - 1 - math_ops.range(0, rank, 1)\n        indices = sp_input.indices\n        transposed_indices = array_ops.transpose(array_ops.gather(array_ops.transpose(indices), perm))\n        perm_ = tensor_util.constant_value(ops.convert_to_tensor(perm))\n        if perm_ is not None and sp_input.get_shape().is_fully_defined():\n            old_shape_ = sp_input.get_shape().as_list()\n            transposed_dense_shape = list(old_shape_)\n            for (i, p) in enumerate(perm_):\n                transposed_dense_shape[i] = old_shape_[p]\n        else:\n            dense_shape = sp_input.dense_shape\n            transposed_dense_shape = array_ops.gather(dense_shape, perm)\n        transposed_st = sparse_tensor.SparseTensor(transposed_indices, sp_input.values, transposed_dense_shape)\n        transposed_st = sparse_reorder(transposed_st)\n        return transposed_st"
        ]
    },
    {
        "func_name": "map_values",
        "original": "@tf_export('sparse.map_values', v1=[])\n@dispatch.add_dispatch_support\ndef map_values(op, *args, **kwargs):\n    \"\"\"Applies `op` to the `.values` tensor of one or more `SparseTensor`s.\n\n  Replaces any `SparseTensor` in `args` or `kwargs` with its `values`\n  tensor (which contains the non-default values for the SparseTensor),\n  and then calls `op`.  Returns a `SparseTensor` that is constructed\n  from the input `SparseTensor`s' `indices`, `dense_shape`, and the\n  value returned by the `op`.\n\n  If the input arguments contain multiple `SparseTensor`s, then they must have\n  equal `indices` and dense shapes.\n\n  Examples:\n\n  >>> s = tf.sparse.from_dense([[1, 2, 0],\n  ...                           [0, 4, 0],\n  ...                           [1, 0, 0]])\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()\n  array([[1, 1, 0],\n         [0, 1, 0],\n         [1, 0, 0]], dtype=int32)\n\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.multiply, s, s)).numpy()\n  array([[ 1,  4,  0],\n         [ 0, 16,  0],\n         [ 1,  0,  0]], dtype=int32)\n\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.add, s, 5)).numpy()\n  array([[6, 7, 0],\n         [0, 9, 0],\n         [6, 0, 0]], dtype=int32)\n\n  Note: even though `tf.add(0, 5) != 0`, implicit zeros\n  will remain unchanged. However, if the sparse tensor contains any explicit\n  zeros, these will be affected by the mapping!\n\n  Args:\n    op: The operation that should be applied to the SparseTensor `values`. `op`\n      is typically an element-wise operation (such as math_ops.add), but any\n      operation that preserves the shape can be used.\n    *args: Arguments for `op`.\n    **kwargs: Keyword arguments for `op`.\n\n  Returns:\n    A `SparseTensor` whose `indices` and `dense_shape` matches the `indices`\n    and `dense_shape` of all input `SparseTensor`s.\n  Raises:\n    ValueError: If args contains no `SparseTensor`, or if the `indices`\n      or `dense_shape`s of the input `SparseTensor`s are not equal.\n  \"\"\"\n    sparse_list = []\n    inner_args = _replace_sparse_with_values(args, sparse_list)\n    inner_kwargs = _replace_sparse_with_values(kwargs, sparse_list)\n    if not sparse_list:\n        raise ValueError('No SparseTensor in argument list of map_values')\n    with ops.control_dependencies(_assert_sparse_compatible(sparse_list)):\n        return sparse_tensor.SparseTensor(sparse_list[0].indices, op(*inner_args, **inner_kwargs), sparse_list[0].dense_shape)",
        "mutated": [
            "@tf_export('sparse.map_values', v1=[])\n@dispatch.add_dispatch_support\ndef map_values(op, *args, **kwargs):\n    if False:\n        i = 10\n    \"Applies `op` to the `.values` tensor of one or more `SparseTensor`s.\\n\\n  Replaces any `SparseTensor` in `args` or `kwargs` with its `values`\\n  tensor (which contains the non-default values for the SparseTensor),\\n  and then calls `op`.  Returns a `SparseTensor` that is constructed\\n  from the input `SparseTensor`s' `indices`, `dense_shape`, and the\\n  value returned by the `op`.\\n\\n  If the input arguments contain multiple `SparseTensor`s, then they must have\\n  equal `indices` and dense shapes.\\n\\n  Examples:\\n\\n  >>> s = tf.sparse.from_dense([[1, 2, 0],\\n  ...                           [0, 4, 0],\\n  ...                           [1, 0, 0]])\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()\\n  array([[1, 1, 0],\\n         [0, 1, 0],\\n         [1, 0, 0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.multiply, s, s)).numpy()\\n  array([[ 1,  4,  0],\\n         [ 0, 16,  0],\\n         [ 1,  0,  0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.add, s, 5)).numpy()\\n  array([[6, 7, 0],\\n         [0, 9, 0],\\n         [6, 0, 0]], dtype=int32)\\n\\n  Note: even though `tf.add(0, 5) != 0`, implicit zeros\\n  will remain unchanged. However, if the sparse tensor contains any explicit\\n  zeros, these will be affected by the mapping!\\n\\n  Args:\\n    op: The operation that should be applied to the SparseTensor `values`. `op`\\n      is typically an element-wise operation (such as math_ops.add), but any\\n      operation that preserves the shape can be used.\\n    *args: Arguments for `op`.\\n    **kwargs: Keyword arguments for `op`.\\n\\n  Returns:\\n    A `SparseTensor` whose `indices` and `dense_shape` matches the `indices`\\n    and `dense_shape` of all input `SparseTensor`s.\\n  Raises:\\n    ValueError: If args contains no `SparseTensor`, or if the `indices`\\n      or `dense_shape`s of the input `SparseTensor`s are not equal.\\n  \"\n    sparse_list = []\n    inner_args = _replace_sparse_with_values(args, sparse_list)\n    inner_kwargs = _replace_sparse_with_values(kwargs, sparse_list)\n    if not sparse_list:\n        raise ValueError('No SparseTensor in argument list of map_values')\n    with ops.control_dependencies(_assert_sparse_compatible(sparse_list)):\n        return sparse_tensor.SparseTensor(sparse_list[0].indices, op(*inner_args, **inner_kwargs), sparse_list[0].dense_shape)",
            "@tf_export('sparse.map_values', v1=[])\n@dispatch.add_dispatch_support\ndef map_values(op, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Applies `op` to the `.values` tensor of one or more `SparseTensor`s.\\n\\n  Replaces any `SparseTensor` in `args` or `kwargs` with its `values`\\n  tensor (which contains the non-default values for the SparseTensor),\\n  and then calls `op`.  Returns a `SparseTensor` that is constructed\\n  from the input `SparseTensor`s' `indices`, `dense_shape`, and the\\n  value returned by the `op`.\\n\\n  If the input arguments contain multiple `SparseTensor`s, then they must have\\n  equal `indices` and dense shapes.\\n\\n  Examples:\\n\\n  >>> s = tf.sparse.from_dense([[1, 2, 0],\\n  ...                           [0, 4, 0],\\n  ...                           [1, 0, 0]])\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()\\n  array([[1, 1, 0],\\n         [0, 1, 0],\\n         [1, 0, 0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.multiply, s, s)).numpy()\\n  array([[ 1,  4,  0],\\n         [ 0, 16,  0],\\n         [ 1,  0,  0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.add, s, 5)).numpy()\\n  array([[6, 7, 0],\\n         [0, 9, 0],\\n         [6, 0, 0]], dtype=int32)\\n\\n  Note: even though `tf.add(0, 5) != 0`, implicit zeros\\n  will remain unchanged. However, if the sparse tensor contains any explicit\\n  zeros, these will be affected by the mapping!\\n\\n  Args:\\n    op: The operation that should be applied to the SparseTensor `values`. `op`\\n      is typically an element-wise operation (such as math_ops.add), but any\\n      operation that preserves the shape can be used.\\n    *args: Arguments for `op`.\\n    **kwargs: Keyword arguments for `op`.\\n\\n  Returns:\\n    A `SparseTensor` whose `indices` and `dense_shape` matches the `indices`\\n    and `dense_shape` of all input `SparseTensor`s.\\n  Raises:\\n    ValueError: If args contains no `SparseTensor`, or if the `indices`\\n      or `dense_shape`s of the input `SparseTensor`s are not equal.\\n  \"\n    sparse_list = []\n    inner_args = _replace_sparse_with_values(args, sparse_list)\n    inner_kwargs = _replace_sparse_with_values(kwargs, sparse_list)\n    if not sparse_list:\n        raise ValueError('No SparseTensor in argument list of map_values')\n    with ops.control_dependencies(_assert_sparse_compatible(sparse_list)):\n        return sparse_tensor.SparseTensor(sparse_list[0].indices, op(*inner_args, **inner_kwargs), sparse_list[0].dense_shape)",
            "@tf_export('sparse.map_values', v1=[])\n@dispatch.add_dispatch_support\ndef map_values(op, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Applies `op` to the `.values` tensor of one or more `SparseTensor`s.\\n\\n  Replaces any `SparseTensor` in `args` or `kwargs` with its `values`\\n  tensor (which contains the non-default values for the SparseTensor),\\n  and then calls `op`.  Returns a `SparseTensor` that is constructed\\n  from the input `SparseTensor`s' `indices`, `dense_shape`, and the\\n  value returned by the `op`.\\n\\n  If the input arguments contain multiple `SparseTensor`s, then they must have\\n  equal `indices` and dense shapes.\\n\\n  Examples:\\n\\n  >>> s = tf.sparse.from_dense([[1, 2, 0],\\n  ...                           [0, 4, 0],\\n  ...                           [1, 0, 0]])\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()\\n  array([[1, 1, 0],\\n         [0, 1, 0],\\n         [1, 0, 0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.multiply, s, s)).numpy()\\n  array([[ 1,  4,  0],\\n         [ 0, 16,  0],\\n         [ 1,  0,  0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.add, s, 5)).numpy()\\n  array([[6, 7, 0],\\n         [0, 9, 0],\\n         [6, 0, 0]], dtype=int32)\\n\\n  Note: even though `tf.add(0, 5) != 0`, implicit zeros\\n  will remain unchanged. However, if the sparse tensor contains any explicit\\n  zeros, these will be affected by the mapping!\\n\\n  Args:\\n    op: The operation that should be applied to the SparseTensor `values`. `op`\\n      is typically an element-wise operation (such as math_ops.add), but any\\n      operation that preserves the shape can be used.\\n    *args: Arguments for `op`.\\n    **kwargs: Keyword arguments for `op`.\\n\\n  Returns:\\n    A `SparseTensor` whose `indices` and `dense_shape` matches the `indices`\\n    and `dense_shape` of all input `SparseTensor`s.\\n  Raises:\\n    ValueError: If args contains no `SparseTensor`, or if the `indices`\\n      or `dense_shape`s of the input `SparseTensor`s are not equal.\\n  \"\n    sparse_list = []\n    inner_args = _replace_sparse_with_values(args, sparse_list)\n    inner_kwargs = _replace_sparse_with_values(kwargs, sparse_list)\n    if not sparse_list:\n        raise ValueError('No SparseTensor in argument list of map_values')\n    with ops.control_dependencies(_assert_sparse_compatible(sparse_list)):\n        return sparse_tensor.SparseTensor(sparse_list[0].indices, op(*inner_args, **inner_kwargs), sparse_list[0].dense_shape)",
            "@tf_export('sparse.map_values', v1=[])\n@dispatch.add_dispatch_support\ndef map_values(op, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Applies `op` to the `.values` tensor of one or more `SparseTensor`s.\\n\\n  Replaces any `SparseTensor` in `args` or `kwargs` with its `values`\\n  tensor (which contains the non-default values for the SparseTensor),\\n  and then calls `op`.  Returns a `SparseTensor` that is constructed\\n  from the input `SparseTensor`s' `indices`, `dense_shape`, and the\\n  value returned by the `op`.\\n\\n  If the input arguments contain multiple `SparseTensor`s, then they must have\\n  equal `indices` and dense shapes.\\n\\n  Examples:\\n\\n  >>> s = tf.sparse.from_dense([[1, 2, 0],\\n  ...                           [0, 4, 0],\\n  ...                           [1, 0, 0]])\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()\\n  array([[1, 1, 0],\\n         [0, 1, 0],\\n         [1, 0, 0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.multiply, s, s)).numpy()\\n  array([[ 1,  4,  0],\\n         [ 0, 16,  0],\\n         [ 1,  0,  0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.add, s, 5)).numpy()\\n  array([[6, 7, 0],\\n         [0, 9, 0],\\n         [6, 0, 0]], dtype=int32)\\n\\n  Note: even though `tf.add(0, 5) != 0`, implicit zeros\\n  will remain unchanged. However, if the sparse tensor contains any explicit\\n  zeros, these will be affected by the mapping!\\n\\n  Args:\\n    op: The operation that should be applied to the SparseTensor `values`. `op`\\n      is typically an element-wise operation (such as math_ops.add), but any\\n      operation that preserves the shape can be used.\\n    *args: Arguments for `op`.\\n    **kwargs: Keyword arguments for `op`.\\n\\n  Returns:\\n    A `SparseTensor` whose `indices` and `dense_shape` matches the `indices`\\n    and `dense_shape` of all input `SparseTensor`s.\\n  Raises:\\n    ValueError: If args contains no `SparseTensor`, or if the `indices`\\n      or `dense_shape`s of the input `SparseTensor`s are not equal.\\n  \"\n    sparse_list = []\n    inner_args = _replace_sparse_with_values(args, sparse_list)\n    inner_kwargs = _replace_sparse_with_values(kwargs, sparse_list)\n    if not sparse_list:\n        raise ValueError('No SparseTensor in argument list of map_values')\n    with ops.control_dependencies(_assert_sparse_compatible(sparse_list)):\n        return sparse_tensor.SparseTensor(sparse_list[0].indices, op(*inner_args, **inner_kwargs), sparse_list[0].dense_shape)",
            "@tf_export('sparse.map_values', v1=[])\n@dispatch.add_dispatch_support\ndef map_values(op, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Applies `op` to the `.values` tensor of one or more `SparseTensor`s.\\n\\n  Replaces any `SparseTensor` in `args` or `kwargs` with its `values`\\n  tensor (which contains the non-default values for the SparseTensor),\\n  and then calls `op`.  Returns a `SparseTensor` that is constructed\\n  from the input `SparseTensor`s' `indices`, `dense_shape`, and the\\n  value returned by the `op`.\\n\\n  If the input arguments contain multiple `SparseTensor`s, then they must have\\n  equal `indices` and dense shapes.\\n\\n  Examples:\\n\\n  >>> s = tf.sparse.from_dense([[1, 2, 0],\\n  ...                           [0, 4, 0],\\n  ...                           [1, 0, 0]])\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()\\n  array([[1, 1, 0],\\n         [0, 1, 0],\\n         [1, 0, 0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.multiply, s, s)).numpy()\\n  array([[ 1,  4,  0],\\n         [ 0, 16,  0],\\n         [ 1,  0,  0]], dtype=int32)\\n\\n  >>> tf.sparse.to_dense(tf.sparse.map_values(tf.add, s, 5)).numpy()\\n  array([[6, 7, 0],\\n         [0, 9, 0],\\n         [6, 0, 0]], dtype=int32)\\n\\n  Note: even though `tf.add(0, 5) != 0`, implicit zeros\\n  will remain unchanged. However, if the sparse tensor contains any explicit\\n  zeros, these will be affected by the mapping!\\n\\n  Args:\\n    op: The operation that should be applied to the SparseTensor `values`. `op`\\n      is typically an element-wise operation (such as math_ops.add), but any\\n      operation that preserves the shape can be used.\\n    *args: Arguments for `op`.\\n    **kwargs: Keyword arguments for `op`.\\n\\n  Returns:\\n    A `SparseTensor` whose `indices` and `dense_shape` matches the `indices`\\n    and `dense_shape` of all input `SparseTensor`s.\\n  Raises:\\n    ValueError: If args contains no `SparseTensor`, or if the `indices`\\n      or `dense_shape`s of the input `SparseTensor`s are not equal.\\n  \"\n    sparse_list = []\n    inner_args = _replace_sparse_with_values(args, sparse_list)\n    inner_kwargs = _replace_sparse_with_values(kwargs, sparse_list)\n    if not sparse_list:\n        raise ValueError('No SparseTensor in argument list of map_values')\n    with ops.control_dependencies(_assert_sparse_compatible(sparse_list)):\n        return sparse_tensor.SparseTensor(sparse_list[0].indices, op(*inner_args, **inner_kwargs), sparse_list[0].dense_shape)"
        ]
    },
    {
        "func_name": "bincount",
        "original": "@dispatch.dispatch_for_api(bincount_ops.bincount)\ndef bincount(arr: sparse_tensor.SparseTensor, weights=None, minlength=None, maxlength=None, dtype=dtypes.int32, name=None, axis=None, binary_output=False):\n    \"\"\"Counts the number of occurrences of each value in an integer array.\n\n  Only the values in the SparseTensor's `values` tensor are counted,\n  missing zeros are ignored.\n\n  If `minlength` and `maxlength` are not given, returns a vector with length\n  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.\n\n  >>> data = tf.sparse.SparseTensor(\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\n  ...     values=[1,1,2,3,2,4,4,5],\n  ...     dense_shape=[8, 10])\n  >>> tf.math.bincount(data)\n  <tf.Tensor: ... numpy=array([0, 2, 2, 1, 2, 1], dtype=int32)>\n\n  Vector length = Maximum element in vector `values` is 5. Adding 1, which is 6\n                  will be the vector length.\n\n  Each bin value in the output indicates number of occurrences of the particular\n  index. Here, index 1 in output has a value 2. This indicates value 1 occurs\n  two times in `values`.\n\n  **Bin-counting with weights**\n\n  >>> indices=[[0, 3], [1, 7], [2, 4], [3, 0], [4, 9], [5, 1], [6, 8], [7, 2]]\n  >>> data = tf.sparse.SparseTensor(\n  ...     indices=indices,\n  ...     values=[1,1,2,3,2,4,4,5],\n  ...     dense_shape=[8, 10])\n  >>> weights = tf.sparse.SparseTensor(\n  ...     indices=indices,\n  ...     values=[1,5,0,1,0,5,4,5],\n  ...     dense_shape=[8, 10])\n  >>> tf.math.bincount(data, weights=weights)\n  <tf.Tensor: ... numpy=array([0, 6, 0, 1, 9, 5], dtype=int32)>\n\n  When `weights` is specified, bins will be incremented by the corresponding\n  weight instead of 1. Here, index 1 in output has a value 6. This is the\n  summation of `weights` corresponding to the value in `values` (i.e. for index\n  1, the first two data values are 1 so the first two weights, 1 and 5, are\n  summed).\n\n  On GPU, `bincount` with weights is only supported when `axis=0` and XLA is\n  enabled (typically when a function decorated with\n  `@tf.function(jit_compile=True)`).\n\n  **Bin-counting matrix rows independently**\n\n  This example uses `axis=-1` with a 2 dimensional input and returns a\n  `Tensor` with bincounting where axis 0 is **not** flattened, i.e. an\n  independent bincount for each matrix row.\n\n  >>> data = tf.sparse.SparseTensor(\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\n  ...     values=[1,1,2,3,2,4,4,5],\n  ...     dense_shape=[3, 10])\n  >>> tf.math.bincount(data, axis=-1)\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\n    array([[0, 2, 0, 0, 0, 0],\n           [0, 0, 2, 1, 0, 0],\n           [0, 0, 0, 0, 2, 1]], dtype=int32)>\n\n  **Bin-counting with binary_output**\n\n  This example gives binary output instead of counting the occurrence.\n\n  >>> data = tf.sparse.SparseTensor(\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\n  ...     values=[1,1,2,3,2,4,4,5],\n  ...     dense_shape=[3, 10])\n  >>> tf.math.bincount(data, axis=-1, binary_output=True)\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\n    array([[0, 1, 0, 0, 0, 0],\n           [0, 0, 1, 1, 0, 0],\n           [0, 0, 0, 0, 1, 1]], dtype=int32)>\n\n  **Missing zeros in SparseTensor**\n\n  Note that missing zeros (implict zeros) in SparseTensor are **NOT** counted.\n  This supports cases such as `0` in the values tensor indicates that index/id\n  `0`is present and a missing zero indicates that no index/id is present.\n\n  If counting missing zeros is desired, there are workarounds.\n  For the `axis=0` case, the number of missing zeros can computed by subtracting\n  the number of elements in the SparseTensor's `values` tensor from the\n  number of elements in the dense shape, and this difference can be added to the\n  first element of the output of `bincount`. For all cases, the SparseTensor\n  can be converted to a dense Tensor with `tf.sparse.to_dense` before calling\n  `tf.math.bincount`.\n\n  >>> data = tf.sparse.SparseTensor(\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\n  ...     values=[1,1,2,3,2,4,4,5],\n  ...     dense_shape=[8, 10])\n  >>> counts = tf.math.bincount(data, dtype=tf.int64)\n  >>> dense_size = tf.math.reduce_prod(data.dense_shape)\n  >>> missing_zeros = dense_size - tf.size(data.values, out_type=tf.int64)\n  >>> tf.concat([[counts[0] + missing_zeros], counts[1:]], 0)\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\n\n  >>> data = tf.sparse.SparseTensor(\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\n  ...     values=[1,1,2,3,2,4,4,5],\n  ...     dense_shape=[8, 10])\n  >>> tf.math.bincount(tf.sparse.to_dense(data), dtype=tf.int64)\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\n\n\n  Args:\n    arr: A SparseTensor whose values should be counted.\n      These tensors must have a rank of 2 if `axis=-1`.\n    weights: If non-None, must be a SparseTensor with the same dense shape and\n      same indices as `arr`. For each value in `arr`, the bin will be\n      incremented by the corresponding weight instead of 1. If non-None,\n      `binary_output` must be False.\n    minlength: If given, ensures the output has length at least `minlength`,\n      padding with zeros at the end if necessary.\n    maxlength: If given, skips values in `arr` that are equal or greater than\n      `maxlength`, ensuring that the output has length at most `maxlength`.\n    dtype: If `weights` is None, determines the type of the output bins.\n    name: A name scope for the associated operations (optional).\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\n      all axes will be flattened (identical to passing `0`). XLA does not\n      support `axis=-1`.\n    binary_output: If True, this op will output 1 instead of the number of times\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\n      reduce_add). Defaults to False.\n\n  Returns:\n    A vector with the same dtype as `weights` or the given `dtype` containing\n    the bincount values.\n\n  Raises:\n    `InvalidArgumentError` if negative values are provided as an input.\n\n  \"\"\"\n    name = 'bincount' if name is None else name\n    with ops.name_scope(name):\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if not arr.dtype.is_integer:\n            arr = math_ops.cast(arr, dtypes.int32)\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        total_size = array_ops.size(arr)\n        array_is_nonempty = total_size > 0\n        max_value = math_ops.maximum(math_ops.reduce_max(arr.values), -1)\n        output_size = math_ops.cast(array_is_nonempty, arr.dtype) * (max_value + 1)\n        if minlength is not None:\n            minlength = ops.convert_to_tensor(minlength, name='minlength', dtype=arr.dtype)\n            output_size = gen_math_ops.maximum(minlength, output_size)\n        if maxlength is not None:\n            maxlength = ops.convert_to_tensor(maxlength, name='maxlength', dtype=arr.dtype)\n            output_size = gen_math_ops.minimum(maxlength, output_size)\n        if axis == 0:\n            if weights is not None:\n                weights = validate_sparse_weights(arr, weights, dtype)\n            arr = arr.values\n        if isinstance(arr, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(arr, weights, dtype)\n            return gen_math_ops.sparse_bincount(indices=arr.indices, values=arr.values, dense_shape=arr.dense_shape, size=output_size, weights=weights, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(arr, weights, dtype)\n            return gen_math_ops.dense_bincount(input=arr, size=output_size, weights=weights, binary_output=binary_output)",
        "mutated": [
            "@dispatch.dispatch_for_api(bincount_ops.bincount)\ndef bincount(arr: sparse_tensor.SparseTensor, weights=None, minlength=None, maxlength=None, dtype=dtypes.int32, name=None, axis=None, binary_output=False):\n    if False:\n        i = 10\n    \"Counts the number of occurrences of each value in an integer array.\\n\\n  Only the values in the SparseTensor's `values` tensor are counted,\\n  missing zeros are ignored.\\n\\n  If `minlength` and `maxlength` are not given, returns a vector with length\\n  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data)\\n  <tf.Tensor: ... numpy=array([0, 2, 2, 1, 2, 1], dtype=int32)>\\n\\n  Vector length = Maximum element in vector `values` is 5. Adding 1, which is 6\\n                  will be the vector length.\\n\\n  Each bin value in the output indicates number of occurrences of the particular\\n  index. Here, index 1 in output has a value 2. This indicates value 1 occurs\\n  two times in `values`.\\n\\n  **Bin-counting with weights**\\n\\n  >>> indices=[[0, 3], [1, 7], [2, 4], [3, 0], [4, 9], [5, 1], [6, 8], [7, 2]]\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> weights = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,5,0,1,0,5,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data, weights=weights)\\n  <tf.Tensor: ... numpy=array([0, 6, 0, 1, 9, 5], dtype=int32)>\\n\\n  When `weights` is specified, bins will be incremented by the corresponding\\n  weight instead of 1. Here, index 1 in output has a value 6. This is the\\n  summation of `weights` corresponding to the value in `values` (i.e. for index\\n  1, the first two data values are 1 so the first two weights, 1 and 5, are\\n  summed).\\n\\n  On GPU, `bincount` with weights is only supported when `axis=0` and XLA is\\n  enabled (typically when a function decorated with\\n  `@tf.function(jit_compile=True)`).\\n\\n  **Bin-counting matrix rows independently**\\n\\n  This example uses `axis=-1` with a 2 dimensional input and returns a\\n  `Tensor` with bincounting where axis 0 is **not** flattened, i.e. an\\n  independent bincount for each matrix row.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 2, 0, 0, 0, 0],\\n           [0, 0, 2, 1, 0, 0],\\n           [0, 0, 0, 0, 2, 1]], dtype=int32)>\\n\\n  **Bin-counting with binary_output**\\n\\n  This example gives binary output instead of counting the occurrence.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1, binary_output=True)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 1, 0, 0, 0, 0],\\n           [0, 0, 1, 1, 0, 0],\\n           [0, 0, 0, 0, 1, 1]], dtype=int32)>\\n\\n  **Missing zeros in SparseTensor**\\n\\n  Note that missing zeros (implict zeros) in SparseTensor are **NOT** counted.\\n  This supports cases such as `0` in the values tensor indicates that index/id\\n  `0`is present and a missing zero indicates that no index/id is present.\\n\\n  If counting missing zeros is desired, there are workarounds.\\n  For the `axis=0` case, the number of missing zeros can computed by subtracting\\n  the number of elements in the SparseTensor's `values` tensor from the\\n  number of elements in the dense shape, and this difference can be added to the\\n  first element of the output of `bincount`. For all cases, the SparseTensor\\n  can be converted to a dense Tensor with `tf.sparse.to_dense` before calling\\n  `tf.math.bincount`.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> counts = tf.math.bincount(data, dtype=tf.int64)\\n  >>> dense_size = tf.math.reduce_prod(data.dense_shape)\\n  >>> missing_zeros = dense_size - tf.size(data.values, out_type=tf.int64)\\n  >>> tf.concat([[counts[0] + missing_zeros], counts[1:]], 0)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(tf.sparse.to_dense(data), dtype=tf.int64)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n\\n  Args:\\n    arr: A SparseTensor whose values should be counted.\\n      These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be a SparseTensor with the same dense shape and\\n      same indices as `arr`. For each value in `arr`, the bin will be\\n      incremented by the corresponding weight instead of 1. If non-None,\\n      `binary_output` must be False.\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `arr` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    dtype: If `weights` is None, determines the type of the output bins.\\n    name: A name scope for the associated operations (optional).\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`). XLA does not\\n      support `axis=-1`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n\\n  Returns:\\n    A vector with the same dtype as `weights` or the given `dtype` containing\\n    the bincount values.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  \"\n    name = 'bincount' if name is None else name\n    with ops.name_scope(name):\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if not arr.dtype.is_integer:\n            arr = math_ops.cast(arr, dtypes.int32)\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        total_size = array_ops.size(arr)\n        array_is_nonempty = total_size > 0\n        max_value = math_ops.maximum(math_ops.reduce_max(arr.values), -1)\n        output_size = math_ops.cast(array_is_nonempty, arr.dtype) * (max_value + 1)\n        if minlength is not None:\n            minlength = ops.convert_to_tensor(minlength, name='minlength', dtype=arr.dtype)\n            output_size = gen_math_ops.maximum(minlength, output_size)\n        if maxlength is not None:\n            maxlength = ops.convert_to_tensor(maxlength, name='maxlength', dtype=arr.dtype)\n            output_size = gen_math_ops.minimum(maxlength, output_size)\n        if axis == 0:\n            if weights is not None:\n                weights = validate_sparse_weights(arr, weights, dtype)\n            arr = arr.values\n        if isinstance(arr, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(arr, weights, dtype)\n            return gen_math_ops.sparse_bincount(indices=arr.indices, values=arr.values, dense_shape=arr.dense_shape, size=output_size, weights=weights, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(arr, weights, dtype)\n            return gen_math_ops.dense_bincount(input=arr, size=output_size, weights=weights, binary_output=binary_output)",
            "@dispatch.dispatch_for_api(bincount_ops.bincount)\ndef bincount(arr: sparse_tensor.SparseTensor, weights=None, minlength=None, maxlength=None, dtype=dtypes.int32, name=None, axis=None, binary_output=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Counts the number of occurrences of each value in an integer array.\\n\\n  Only the values in the SparseTensor's `values` tensor are counted,\\n  missing zeros are ignored.\\n\\n  If `minlength` and `maxlength` are not given, returns a vector with length\\n  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data)\\n  <tf.Tensor: ... numpy=array([0, 2, 2, 1, 2, 1], dtype=int32)>\\n\\n  Vector length = Maximum element in vector `values` is 5. Adding 1, which is 6\\n                  will be the vector length.\\n\\n  Each bin value in the output indicates number of occurrences of the particular\\n  index. Here, index 1 in output has a value 2. This indicates value 1 occurs\\n  two times in `values`.\\n\\n  **Bin-counting with weights**\\n\\n  >>> indices=[[0, 3], [1, 7], [2, 4], [3, 0], [4, 9], [5, 1], [6, 8], [7, 2]]\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> weights = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,5,0,1,0,5,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data, weights=weights)\\n  <tf.Tensor: ... numpy=array([0, 6, 0, 1, 9, 5], dtype=int32)>\\n\\n  When `weights` is specified, bins will be incremented by the corresponding\\n  weight instead of 1. Here, index 1 in output has a value 6. This is the\\n  summation of `weights` corresponding to the value in `values` (i.e. for index\\n  1, the first two data values are 1 so the first two weights, 1 and 5, are\\n  summed).\\n\\n  On GPU, `bincount` with weights is only supported when `axis=0` and XLA is\\n  enabled (typically when a function decorated with\\n  `@tf.function(jit_compile=True)`).\\n\\n  **Bin-counting matrix rows independently**\\n\\n  This example uses `axis=-1` with a 2 dimensional input and returns a\\n  `Tensor` with bincounting where axis 0 is **not** flattened, i.e. an\\n  independent bincount for each matrix row.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 2, 0, 0, 0, 0],\\n           [0, 0, 2, 1, 0, 0],\\n           [0, 0, 0, 0, 2, 1]], dtype=int32)>\\n\\n  **Bin-counting with binary_output**\\n\\n  This example gives binary output instead of counting the occurrence.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1, binary_output=True)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 1, 0, 0, 0, 0],\\n           [0, 0, 1, 1, 0, 0],\\n           [0, 0, 0, 0, 1, 1]], dtype=int32)>\\n\\n  **Missing zeros in SparseTensor**\\n\\n  Note that missing zeros (implict zeros) in SparseTensor are **NOT** counted.\\n  This supports cases such as `0` in the values tensor indicates that index/id\\n  `0`is present and a missing zero indicates that no index/id is present.\\n\\n  If counting missing zeros is desired, there are workarounds.\\n  For the `axis=0` case, the number of missing zeros can computed by subtracting\\n  the number of elements in the SparseTensor's `values` tensor from the\\n  number of elements in the dense shape, and this difference can be added to the\\n  first element of the output of `bincount`. For all cases, the SparseTensor\\n  can be converted to a dense Tensor with `tf.sparse.to_dense` before calling\\n  `tf.math.bincount`.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> counts = tf.math.bincount(data, dtype=tf.int64)\\n  >>> dense_size = tf.math.reduce_prod(data.dense_shape)\\n  >>> missing_zeros = dense_size - tf.size(data.values, out_type=tf.int64)\\n  >>> tf.concat([[counts[0] + missing_zeros], counts[1:]], 0)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(tf.sparse.to_dense(data), dtype=tf.int64)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n\\n  Args:\\n    arr: A SparseTensor whose values should be counted.\\n      These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be a SparseTensor with the same dense shape and\\n      same indices as `arr`. For each value in `arr`, the bin will be\\n      incremented by the corresponding weight instead of 1. If non-None,\\n      `binary_output` must be False.\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `arr` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    dtype: If `weights` is None, determines the type of the output bins.\\n    name: A name scope for the associated operations (optional).\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`). XLA does not\\n      support `axis=-1`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n\\n  Returns:\\n    A vector with the same dtype as `weights` or the given `dtype` containing\\n    the bincount values.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  \"\n    name = 'bincount' if name is None else name\n    with ops.name_scope(name):\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if not arr.dtype.is_integer:\n            arr = math_ops.cast(arr, dtypes.int32)\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        total_size = array_ops.size(arr)\n        array_is_nonempty = total_size > 0\n        max_value = math_ops.maximum(math_ops.reduce_max(arr.values), -1)\n        output_size = math_ops.cast(array_is_nonempty, arr.dtype) * (max_value + 1)\n        if minlength is not None:\n            minlength = ops.convert_to_tensor(minlength, name='minlength', dtype=arr.dtype)\n            output_size = gen_math_ops.maximum(minlength, output_size)\n        if maxlength is not None:\n            maxlength = ops.convert_to_tensor(maxlength, name='maxlength', dtype=arr.dtype)\n            output_size = gen_math_ops.minimum(maxlength, output_size)\n        if axis == 0:\n            if weights is not None:\n                weights = validate_sparse_weights(arr, weights, dtype)\n            arr = arr.values\n        if isinstance(arr, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(arr, weights, dtype)\n            return gen_math_ops.sparse_bincount(indices=arr.indices, values=arr.values, dense_shape=arr.dense_shape, size=output_size, weights=weights, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(arr, weights, dtype)\n            return gen_math_ops.dense_bincount(input=arr, size=output_size, weights=weights, binary_output=binary_output)",
            "@dispatch.dispatch_for_api(bincount_ops.bincount)\ndef bincount(arr: sparse_tensor.SparseTensor, weights=None, minlength=None, maxlength=None, dtype=dtypes.int32, name=None, axis=None, binary_output=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Counts the number of occurrences of each value in an integer array.\\n\\n  Only the values in the SparseTensor's `values` tensor are counted,\\n  missing zeros are ignored.\\n\\n  If `minlength` and `maxlength` are not given, returns a vector with length\\n  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data)\\n  <tf.Tensor: ... numpy=array([0, 2, 2, 1, 2, 1], dtype=int32)>\\n\\n  Vector length = Maximum element in vector `values` is 5. Adding 1, which is 6\\n                  will be the vector length.\\n\\n  Each bin value in the output indicates number of occurrences of the particular\\n  index. Here, index 1 in output has a value 2. This indicates value 1 occurs\\n  two times in `values`.\\n\\n  **Bin-counting with weights**\\n\\n  >>> indices=[[0, 3], [1, 7], [2, 4], [3, 0], [4, 9], [5, 1], [6, 8], [7, 2]]\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> weights = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,5,0,1,0,5,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data, weights=weights)\\n  <tf.Tensor: ... numpy=array([0, 6, 0, 1, 9, 5], dtype=int32)>\\n\\n  When `weights` is specified, bins will be incremented by the corresponding\\n  weight instead of 1. Here, index 1 in output has a value 6. This is the\\n  summation of `weights` corresponding to the value in `values` (i.e. for index\\n  1, the first two data values are 1 so the first two weights, 1 and 5, are\\n  summed).\\n\\n  On GPU, `bincount` with weights is only supported when `axis=0` and XLA is\\n  enabled (typically when a function decorated with\\n  `@tf.function(jit_compile=True)`).\\n\\n  **Bin-counting matrix rows independently**\\n\\n  This example uses `axis=-1` with a 2 dimensional input and returns a\\n  `Tensor` with bincounting where axis 0 is **not** flattened, i.e. an\\n  independent bincount for each matrix row.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 2, 0, 0, 0, 0],\\n           [0, 0, 2, 1, 0, 0],\\n           [0, 0, 0, 0, 2, 1]], dtype=int32)>\\n\\n  **Bin-counting with binary_output**\\n\\n  This example gives binary output instead of counting the occurrence.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1, binary_output=True)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 1, 0, 0, 0, 0],\\n           [0, 0, 1, 1, 0, 0],\\n           [0, 0, 0, 0, 1, 1]], dtype=int32)>\\n\\n  **Missing zeros in SparseTensor**\\n\\n  Note that missing zeros (implict zeros) in SparseTensor are **NOT** counted.\\n  This supports cases such as `0` in the values tensor indicates that index/id\\n  `0`is present and a missing zero indicates that no index/id is present.\\n\\n  If counting missing zeros is desired, there are workarounds.\\n  For the `axis=0` case, the number of missing zeros can computed by subtracting\\n  the number of elements in the SparseTensor's `values` tensor from the\\n  number of elements in the dense shape, and this difference can be added to the\\n  first element of the output of `bincount`. For all cases, the SparseTensor\\n  can be converted to a dense Tensor with `tf.sparse.to_dense` before calling\\n  `tf.math.bincount`.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> counts = tf.math.bincount(data, dtype=tf.int64)\\n  >>> dense_size = tf.math.reduce_prod(data.dense_shape)\\n  >>> missing_zeros = dense_size - tf.size(data.values, out_type=tf.int64)\\n  >>> tf.concat([[counts[0] + missing_zeros], counts[1:]], 0)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(tf.sparse.to_dense(data), dtype=tf.int64)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n\\n  Args:\\n    arr: A SparseTensor whose values should be counted.\\n      These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be a SparseTensor with the same dense shape and\\n      same indices as `arr`. For each value in `arr`, the bin will be\\n      incremented by the corresponding weight instead of 1. If non-None,\\n      `binary_output` must be False.\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `arr` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    dtype: If `weights` is None, determines the type of the output bins.\\n    name: A name scope for the associated operations (optional).\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`). XLA does not\\n      support `axis=-1`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n\\n  Returns:\\n    A vector with the same dtype as `weights` or the given `dtype` containing\\n    the bincount values.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  \"\n    name = 'bincount' if name is None else name\n    with ops.name_scope(name):\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if not arr.dtype.is_integer:\n            arr = math_ops.cast(arr, dtypes.int32)\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        total_size = array_ops.size(arr)\n        array_is_nonempty = total_size > 0\n        max_value = math_ops.maximum(math_ops.reduce_max(arr.values), -1)\n        output_size = math_ops.cast(array_is_nonempty, arr.dtype) * (max_value + 1)\n        if minlength is not None:\n            minlength = ops.convert_to_tensor(minlength, name='minlength', dtype=arr.dtype)\n            output_size = gen_math_ops.maximum(minlength, output_size)\n        if maxlength is not None:\n            maxlength = ops.convert_to_tensor(maxlength, name='maxlength', dtype=arr.dtype)\n            output_size = gen_math_ops.minimum(maxlength, output_size)\n        if axis == 0:\n            if weights is not None:\n                weights = validate_sparse_weights(arr, weights, dtype)\n            arr = arr.values\n        if isinstance(arr, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(arr, weights, dtype)\n            return gen_math_ops.sparse_bincount(indices=arr.indices, values=arr.values, dense_shape=arr.dense_shape, size=output_size, weights=weights, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(arr, weights, dtype)\n            return gen_math_ops.dense_bincount(input=arr, size=output_size, weights=weights, binary_output=binary_output)",
            "@dispatch.dispatch_for_api(bincount_ops.bincount)\ndef bincount(arr: sparse_tensor.SparseTensor, weights=None, minlength=None, maxlength=None, dtype=dtypes.int32, name=None, axis=None, binary_output=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Counts the number of occurrences of each value in an integer array.\\n\\n  Only the values in the SparseTensor's `values` tensor are counted,\\n  missing zeros are ignored.\\n\\n  If `minlength` and `maxlength` are not given, returns a vector with length\\n  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data)\\n  <tf.Tensor: ... numpy=array([0, 2, 2, 1, 2, 1], dtype=int32)>\\n\\n  Vector length = Maximum element in vector `values` is 5. Adding 1, which is 6\\n                  will be the vector length.\\n\\n  Each bin value in the output indicates number of occurrences of the particular\\n  index. Here, index 1 in output has a value 2. This indicates value 1 occurs\\n  two times in `values`.\\n\\n  **Bin-counting with weights**\\n\\n  >>> indices=[[0, 3], [1, 7], [2, 4], [3, 0], [4, 9], [5, 1], [6, 8], [7, 2]]\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> weights = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,5,0,1,0,5,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data, weights=weights)\\n  <tf.Tensor: ... numpy=array([0, 6, 0, 1, 9, 5], dtype=int32)>\\n\\n  When `weights` is specified, bins will be incremented by the corresponding\\n  weight instead of 1. Here, index 1 in output has a value 6. This is the\\n  summation of `weights` corresponding to the value in `values` (i.e. for index\\n  1, the first two data values are 1 so the first two weights, 1 and 5, are\\n  summed).\\n\\n  On GPU, `bincount` with weights is only supported when `axis=0` and XLA is\\n  enabled (typically when a function decorated with\\n  `@tf.function(jit_compile=True)`).\\n\\n  **Bin-counting matrix rows independently**\\n\\n  This example uses `axis=-1` with a 2 dimensional input and returns a\\n  `Tensor` with bincounting where axis 0 is **not** flattened, i.e. an\\n  independent bincount for each matrix row.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 2, 0, 0, 0, 0],\\n           [0, 0, 2, 1, 0, 0],\\n           [0, 0, 0, 0, 2, 1]], dtype=int32)>\\n\\n  **Bin-counting with binary_output**\\n\\n  This example gives binary output instead of counting the occurrence.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1, binary_output=True)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 1, 0, 0, 0, 0],\\n           [0, 0, 1, 1, 0, 0],\\n           [0, 0, 0, 0, 1, 1]], dtype=int32)>\\n\\n  **Missing zeros in SparseTensor**\\n\\n  Note that missing zeros (implict zeros) in SparseTensor are **NOT** counted.\\n  This supports cases such as `0` in the values tensor indicates that index/id\\n  `0`is present and a missing zero indicates that no index/id is present.\\n\\n  If counting missing zeros is desired, there are workarounds.\\n  For the `axis=0` case, the number of missing zeros can computed by subtracting\\n  the number of elements in the SparseTensor's `values` tensor from the\\n  number of elements in the dense shape, and this difference can be added to the\\n  first element of the output of `bincount`. For all cases, the SparseTensor\\n  can be converted to a dense Tensor with `tf.sparse.to_dense` before calling\\n  `tf.math.bincount`.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> counts = tf.math.bincount(data, dtype=tf.int64)\\n  >>> dense_size = tf.math.reduce_prod(data.dense_shape)\\n  >>> missing_zeros = dense_size - tf.size(data.values, out_type=tf.int64)\\n  >>> tf.concat([[counts[0] + missing_zeros], counts[1:]], 0)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(tf.sparse.to_dense(data), dtype=tf.int64)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n\\n  Args:\\n    arr: A SparseTensor whose values should be counted.\\n      These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be a SparseTensor with the same dense shape and\\n      same indices as `arr`. For each value in `arr`, the bin will be\\n      incremented by the corresponding weight instead of 1. If non-None,\\n      `binary_output` must be False.\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `arr` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    dtype: If `weights` is None, determines the type of the output bins.\\n    name: A name scope for the associated operations (optional).\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`). XLA does not\\n      support `axis=-1`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n\\n  Returns:\\n    A vector with the same dtype as `weights` or the given `dtype` containing\\n    the bincount values.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  \"\n    name = 'bincount' if name is None else name\n    with ops.name_scope(name):\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if not arr.dtype.is_integer:\n            arr = math_ops.cast(arr, dtypes.int32)\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        total_size = array_ops.size(arr)\n        array_is_nonempty = total_size > 0\n        max_value = math_ops.maximum(math_ops.reduce_max(arr.values), -1)\n        output_size = math_ops.cast(array_is_nonempty, arr.dtype) * (max_value + 1)\n        if minlength is not None:\n            minlength = ops.convert_to_tensor(minlength, name='minlength', dtype=arr.dtype)\n            output_size = gen_math_ops.maximum(minlength, output_size)\n        if maxlength is not None:\n            maxlength = ops.convert_to_tensor(maxlength, name='maxlength', dtype=arr.dtype)\n            output_size = gen_math_ops.minimum(maxlength, output_size)\n        if axis == 0:\n            if weights is not None:\n                weights = validate_sparse_weights(arr, weights, dtype)\n            arr = arr.values\n        if isinstance(arr, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(arr, weights, dtype)\n            return gen_math_ops.sparse_bincount(indices=arr.indices, values=arr.values, dense_shape=arr.dense_shape, size=output_size, weights=weights, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(arr, weights, dtype)\n            return gen_math_ops.dense_bincount(input=arr, size=output_size, weights=weights, binary_output=binary_output)",
            "@dispatch.dispatch_for_api(bincount_ops.bincount)\ndef bincount(arr: sparse_tensor.SparseTensor, weights=None, minlength=None, maxlength=None, dtype=dtypes.int32, name=None, axis=None, binary_output=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Counts the number of occurrences of each value in an integer array.\\n\\n  Only the values in the SparseTensor's `values` tensor are counted,\\n  missing zeros are ignored.\\n\\n  If `minlength` and `maxlength` are not given, returns a vector with length\\n  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data)\\n  <tf.Tensor: ... numpy=array([0, 2, 2, 1, 2, 1], dtype=int32)>\\n\\n  Vector length = Maximum element in vector `values` is 5. Adding 1, which is 6\\n                  will be the vector length.\\n\\n  Each bin value in the output indicates number of occurrences of the particular\\n  index. Here, index 1 in output has a value 2. This indicates value 1 occurs\\n  two times in `values`.\\n\\n  **Bin-counting with weights**\\n\\n  >>> indices=[[0, 3], [1, 7], [2, 4], [3, 0], [4, 9], [5, 1], [6, 8], [7, 2]]\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> weights = tf.sparse.SparseTensor(\\n  ...     indices=indices,\\n  ...     values=[1,5,0,1,0,5,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(data, weights=weights)\\n  <tf.Tensor: ... numpy=array([0, 6, 0, 1, 9, 5], dtype=int32)>\\n\\n  When `weights` is specified, bins will be incremented by the corresponding\\n  weight instead of 1. Here, index 1 in output has a value 6. This is the\\n  summation of `weights` corresponding to the value in `values` (i.e. for index\\n  1, the first two data values are 1 so the first two weights, 1 and 5, are\\n  summed).\\n\\n  On GPU, `bincount` with weights is only supported when `axis=0` and XLA is\\n  enabled (typically when a function decorated with\\n  `@tf.function(jit_compile=True)`).\\n\\n  **Bin-counting matrix rows independently**\\n\\n  This example uses `axis=-1` with a 2 dimensional input and returns a\\n  `Tensor` with bincounting where axis 0 is **not** flattened, i.e. an\\n  independent bincount for each matrix row.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 2, 0, 0, 0, 0],\\n           [0, 0, 2, 1, 0, 0],\\n           [0, 0, 0, 0, 2, 1]], dtype=int32)>\\n\\n  **Bin-counting with binary_output**\\n\\n  This example gives binary output instead of counting the occurrence.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [1, 4], [1, 0],\\n  ...              [1, 9], [2, 1], [2, 8], [2, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[3, 10])\\n  >>> tf.math.bincount(data, axis=-1, binary_output=True)\\n    <tf.Tensor: shape=(3, 6), dtype=int32, numpy=\\n    array([[0, 1, 0, 0, 0, 0],\\n           [0, 0, 1, 1, 0, 0],\\n           [0, 0, 0, 0, 1, 1]], dtype=int32)>\\n\\n  **Missing zeros in SparseTensor**\\n\\n  Note that missing zeros (implict zeros) in SparseTensor are **NOT** counted.\\n  This supports cases such as `0` in the values tensor indicates that index/id\\n  `0`is present and a missing zero indicates that no index/id is present.\\n\\n  If counting missing zeros is desired, there are workarounds.\\n  For the `axis=0` case, the number of missing zeros can computed by subtracting\\n  the number of elements in the SparseTensor's `values` tensor from the\\n  number of elements in the dense shape, and this difference can be added to the\\n  first element of the output of `bincount`. For all cases, the SparseTensor\\n  can be converted to a dense Tensor with `tf.sparse.to_dense` before calling\\n  `tf.math.bincount`.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> counts = tf.math.bincount(data, dtype=tf.int64)\\n  >>> dense_size = tf.math.reduce_prod(data.dense_shape)\\n  >>> missing_zeros = dense_size - tf.size(data.values, out_type=tf.int64)\\n  >>> tf.concat([[counts[0] + missing_zeros], counts[1:]], 0)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [1, 7], [2, 4], [3, 0],\\n  ...              [4, 9], [5, 1], [6, 8], [7, 2]],\\n  ...     values=[1,1,2,3,2,4,4,5],\\n  ...     dense_shape=[8, 10])\\n  >>> tf.math.bincount(tf.sparse.to_dense(data), dtype=tf.int64)\\n  <tf.Tensor: ... numpy=array([72, 2, 2, 1, 2, 1])>\\n\\n\\n  Args:\\n    arr: A SparseTensor whose values should be counted.\\n      These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be a SparseTensor with the same dense shape and\\n      same indices as `arr`. For each value in `arr`, the bin will be\\n      incremented by the corresponding weight instead of 1. If non-None,\\n      `binary_output` must be False.\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `arr` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    dtype: If `weights` is None, determines the type of the output bins.\\n    name: A name scope for the associated operations (optional).\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`). XLA does not\\n      support `axis=-1`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n\\n  Returns:\\n    A vector with the same dtype as `weights` or the given `dtype` containing\\n    the bincount values.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  \"\n    name = 'bincount' if name is None else name\n    with ops.name_scope(name):\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if not arr.dtype.is_integer:\n            arr = math_ops.cast(arr, dtypes.int32)\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        total_size = array_ops.size(arr)\n        array_is_nonempty = total_size > 0\n        max_value = math_ops.maximum(math_ops.reduce_max(arr.values), -1)\n        output_size = math_ops.cast(array_is_nonempty, arr.dtype) * (max_value + 1)\n        if minlength is not None:\n            minlength = ops.convert_to_tensor(minlength, name='minlength', dtype=arr.dtype)\n            output_size = gen_math_ops.maximum(minlength, output_size)\n        if maxlength is not None:\n            maxlength = ops.convert_to_tensor(maxlength, name='maxlength', dtype=arr.dtype)\n            output_size = gen_math_ops.minimum(maxlength, output_size)\n        if axis == 0:\n            if weights is not None:\n                weights = validate_sparse_weights(arr, weights, dtype)\n            arr = arr.values\n        if isinstance(arr, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(arr, weights, dtype)\n            return gen_math_ops.sparse_bincount(indices=arr.indices, values=arr.values, dense_shape=arr.dense_shape, size=output_size, weights=weights, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(arr, weights, dtype)\n            return gen_math_ops.dense_bincount(input=arr, size=output_size, weights=weights, binary_output=binary_output)"
        ]
    },
    {
        "func_name": "sparse_bincount",
        "original": "@tf_export('sparse.bincount')\n@dispatch.add_dispatch_support\ndef sparse_bincount(values, weights=None, axis=0, minlength=None, maxlength=None, binary_output=False, name=None):\n    \"\"\"Count the number of times an integer value appears in a tensor.\n\n  This op takes an N-dimensional `Tensor`, `RaggedTensor`, or `SparseTensor`,\n  and returns an N-dimensional int64 SparseTensor where element\n  `[i0...i[axis], j]` contains the number of times the value `j` appears in\n  slice `[i0...i[axis], :]` of the input tensor.  Currently, only N=0 and\n  N=-1 are supported.\n\n  Args:\n    values: A Tensor, RaggedTensor, or SparseTensor whose values should be\n      counted. These tensors must have a rank of 2 if `axis=-1`.\n    weights: If non-None, must be the same shape as `arr`. If `arr` is a\n      SparseTensor, `weights` must be a SparseTensor with the same dense shape\n      and same indices as `arr`. For each value in `value`, the bin will be\n      incremented by the corresponding weight instead of 1.\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\n      all axes will be flattened (identical to passing `0`).\n    minlength: If given, ensures the output has length at least `minlength`,\n      padding with zeros at the end if necessary.\n    maxlength: If given, skips values in `values` that are equal or greater than\n      `maxlength`, ensuring that the output has length at most `maxlength`.\n    binary_output: If True, this op will output 1 instead of the number of times\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\n      reduce_add). Defaults to False.\n    name: A name for this op.\n\n  Returns:\n    A SparseTensor with `output.shape = values.shape[:axis] + [N]`, where `N` is\n      * `maxlength` (if set);\n      * `minlength` (if set, and `minlength > reduce_max(values)`);\n      * `0` (if `values` is empty);\n      * `reduce_max(values) + 1` otherwise.\n\n  Raises:\n    `InvalidArgumentError` if negative values are provided as an input.\n\n  Examples:\n\n  **Bin-counting every item in individual batches**\n\n  This example takes an input (which could be a Tensor, RaggedTensor, or\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\n  number of times value j appears in batch i.\n\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\n  >>> tf.sparse.bincount(data, axis=-1)\n  SparseTensor(indices=tf.Tensor(\n  [[    0    10]\n   [    0    20]\n   [    0    30]\n   [    1    11]\n   [    1   101]\n   [    1 10001]], shape=(6, 2), dtype=int64),\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\n\n  This example shows a sparse tensor input. Missing zeros are not counted.\n\n  >>> data = tf.sparse.SparseTensor(\n  ...     indices=[[0, 3], [0, 7], [0, 8], [0, 11],\n  ...              [1, 9], [1, 11], [1, 18], [1, 27]],\n  ...     values=[10, 20, 30, 20, 11, 101, 11, 10001],\n  ...     dense_shape=[2, 30])\n  >>> tf.sparse.bincount(data, axis=-1)\n  SparseTensor(indices=tf.Tensor(\n  [[    0    10]\n   [    0    20]\n   [    0    30]\n   [    1    11]\n   [    1   101]\n   [    1 10001]], shape=(6, 2), dtype=int64),\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int32),\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\n\n  **Bin-counting with defined output shape**\n\n  This example takes an input (which could be a Tensor, RaggedTensor, or\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\n  number of times value j appears in batch i. However, all values of j\n  above 'maxlength' are ignored. The dense_shape of the output sparse tensor\n  is set to 'minlength'. Note that, while the input is identical to the\n  example above, the value '10001' in batch item 2 is dropped, and the\n  dense shape is [2, 500] instead of [2,10002] or [2, 102].\n\n  >>> minlength = maxlength = 500\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\n  >>> tf.sparse.bincount(\n  ...    data, axis=-1, minlength=minlength, maxlength=maxlength)\n  SparseTensor(indices=tf.Tensor(\n  [[  0  10]\n   [  0  20]\n   [  0  30]\n   [  1  11]\n   [  1 101]], shape=(5, 2), dtype=int64),\n   values=tf.Tensor([1 2 1 2 1], shape=(5,), dtype=int64),\n   dense_shape=tf.Tensor([  2 500], shape=(2,), dtype=int64))\n\n  **Binary bin-counting**\n\n  This example takes an input (which could be a Tensor, RaggedTensor, or\n  SparseTensor) and returns a SparseTensor where (i,j) is 1 if the value j\n  appears in batch i at least once and is 0 otherwise. Note that, even though\n  some values (like 20 in batch 1 and 11 in batch 2) appear more than once,\n  the 'values' tensor is all 1s.\n\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\n  >>> tf.sparse.bincount(data, binary_output=True, axis=-1)\n  SparseTensor(indices=tf.Tensor(\n  [[    0    10]\n   [    0    20]\n   [    0    30]\n   [    1    11]\n   [    1   101]\n   [    1 10001]], shape=(6, 2), dtype=int64),\n   values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64),\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\n\n  **Weighted bin-counting**\n\n  This example takes two inputs - a values tensor and a weights tensor. These\n  tensors must be identically shaped, and have the same row splits or indices\n  in the case of RaggedTensors or SparseTensors. When performing a weighted\n  count, the op will output a SparseTensor where the value of (i, j) is the\n  sum of the values in the weight tensor's batch i in the locations where\n  the values tensor has the value j. In this case, the output dtype is the\n  same as the dtype of the weights tensor.\n\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\n  >>> weights = [[2, 0.25, 15, 0.5], [2, 17, 3, 0.9]]\n  >>> tf.sparse.bincount(data, weights=weights, axis=-1)\n  SparseTensor(indices=tf.Tensor(\n  [[    0    10]\n   [    0    20]\n   [    0    30]\n   [    1    11]\n   [    1   101]\n   [    1 10001]], shape=(6, 2), dtype=int64),\n   values=tf.Tensor([2. 0.75 15. 5. 17. 0.9], shape=(6,), dtype=float32),\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\n\n  \"\"\"\n    with ops.name_scope(name, 'count', [values, weights]):\n        if not isinstance(values, sparse_tensor.SparseTensor):\n            values = tensor_conversion.convert_to_tensor_v2_with_dispatch(values, name='values')\n        if weights is not None:\n            if not isinstance(weights, composite_tensor.CompositeTensor):\n                weights = tensor_conversion.convert_to_tensor_v2_with_dispatch(weights, name='weights')\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        minlength_value = minlength if minlength is not None else -1\n        maxlength_value = maxlength if maxlength is not None else -1\n        if axis == 0:\n            if isinstance(values, sparse_tensor.SparseTensor):\n                if weights is not None:\n                    weights = validate_sparse_weights(values, weights)\n                values = values.values\n            else:\n                if weights is not None:\n                    weights = array_ops.reshape(weights, [-1])\n                values = array_ops.reshape(values, [-1])\n        if isinstance(values, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.sparse_count_sparse_output(values.indices, values.values, values.dense_shape, weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.dense_count_sparse_output(values, weights=weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        return sparse_tensor.SparseTensor(c_ind, c_val, c_shape)",
        "mutated": [
            "@tf_export('sparse.bincount')\n@dispatch.add_dispatch_support\ndef sparse_bincount(values, weights=None, axis=0, minlength=None, maxlength=None, binary_output=False, name=None):\n    if False:\n        i = 10\n    \"Count the number of times an integer value appears in a tensor.\\n\\n  This op takes an N-dimensional `Tensor`, `RaggedTensor`, or `SparseTensor`,\\n  and returns an N-dimensional int64 SparseTensor where element\\n  `[i0...i[axis], j]` contains the number of times the value `j` appears in\\n  slice `[i0...i[axis], :]` of the input tensor.  Currently, only N=0 and\\n  N=-1 are supported.\\n\\n  Args:\\n    values: A Tensor, RaggedTensor, or SparseTensor whose values should be\\n      counted. These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be the same shape as `arr`. If `arr` is a\\n      SparseTensor, `weights` must be a SparseTensor with the same dense shape\\n      and same indices as `arr`. For each value in `value`, the bin will be\\n      incremented by the corresponding weight instead of 1.\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`).\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `values` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n    name: A name for this op.\\n\\n  Returns:\\n    A SparseTensor with `output.shape = values.shape[:axis] + [N]`, where `N` is\\n      * `maxlength` (if set);\\n      * `minlength` (if set, and `minlength > reduce_max(values)`);\\n      * `0` (if `values` is empty);\\n      * `reduce_max(values) + 1` otherwise.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  Examples:\\n\\n  **Bin-counting every item in individual batches**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  This example shows a sparse tensor input. Missing zeros are not counted.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [0, 8], [0, 11],\\n  ...              [1, 9], [1, 11], [1, 18], [1, 27]],\\n  ...     values=[10, 20, 30, 20, 11, 101, 11, 10001],\\n  ...     dense_shape=[2, 30])\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Bin-counting with defined output shape**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i. However, all values of j\\n  above 'maxlength' are ignored. The dense_shape of the output sparse tensor\\n  is set to 'minlength'. Note that, while the input is identical to the\\n  example above, the value '10001' in batch item 2 is dropped, and the\\n  dense shape is [2, 500] instead of [2,10002] or [2, 102].\\n\\n  >>> minlength = maxlength = 500\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(\\n  ...    data, axis=-1, minlength=minlength, maxlength=maxlength)\\n  SparseTensor(indices=tf.Tensor(\\n  [[  0  10]\\n   [  0  20]\\n   [  0  30]\\n   [  1  11]\\n   [  1 101]], shape=(5, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1], shape=(5,), dtype=int64),\\n   dense_shape=tf.Tensor([  2 500], shape=(2,), dtype=int64))\\n\\n  **Binary bin-counting**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where (i,j) is 1 if the value j\\n  appears in batch i at least once and is 0 otherwise. Note that, even though\\n  some values (like 20 in batch 1 and 11 in batch 2) appear more than once,\\n  the 'values' tensor is all 1s.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, binary_output=True, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Weighted bin-counting**\\n\\n  This example takes two inputs - a values tensor and a weights tensor. These\\n  tensors must be identically shaped, and have the same row splits or indices\\n  in the case of RaggedTensors or SparseTensors. When performing a weighted\\n  count, the op will output a SparseTensor where the value of (i, j) is the\\n  sum of the values in the weight tensor's batch i in the locations where\\n  the values tensor has the value j. In this case, the output dtype is the\\n  same as the dtype of the weights tensor.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> weights = [[2, 0.25, 15, 0.5], [2, 17, 3, 0.9]]\\n  >>> tf.sparse.bincount(data, weights=weights, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([2. 0.75 15. 5. 17. 0.9], shape=(6,), dtype=float32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  \"\n    with ops.name_scope(name, 'count', [values, weights]):\n        if not isinstance(values, sparse_tensor.SparseTensor):\n            values = tensor_conversion.convert_to_tensor_v2_with_dispatch(values, name='values')\n        if weights is not None:\n            if not isinstance(weights, composite_tensor.CompositeTensor):\n                weights = tensor_conversion.convert_to_tensor_v2_with_dispatch(weights, name='weights')\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        minlength_value = minlength if minlength is not None else -1\n        maxlength_value = maxlength if maxlength is not None else -1\n        if axis == 0:\n            if isinstance(values, sparse_tensor.SparseTensor):\n                if weights is not None:\n                    weights = validate_sparse_weights(values, weights)\n                values = values.values\n            else:\n                if weights is not None:\n                    weights = array_ops.reshape(weights, [-1])\n                values = array_ops.reshape(values, [-1])\n        if isinstance(values, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.sparse_count_sparse_output(values.indices, values.values, values.dense_shape, weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.dense_count_sparse_output(values, weights=weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        return sparse_tensor.SparseTensor(c_ind, c_val, c_shape)",
            "@tf_export('sparse.bincount')\n@dispatch.add_dispatch_support\ndef sparse_bincount(values, weights=None, axis=0, minlength=None, maxlength=None, binary_output=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Count the number of times an integer value appears in a tensor.\\n\\n  This op takes an N-dimensional `Tensor`, `RaggedTensor`, or `SparseTensor`,\\n  and returns an N-dimensional int64 SparseTensor where element\\n  `[i0...i[axis], j]` contains the number of times the value `j` appears in\\n  slice `[i0...i[axis], :]` of the input tensor.  Currently, only N=0 and\\n  N=-1 are supported.\\n\\n  Args:\\n    values: A Tensor, RaggedTensor, or SparseTensor whose values should be\\n      counted. These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be the same shape as `arr`. If `arr` is a\\n      SparseTensor, `weights` must be a SparseTensor with the same dense shape\\n      and same indices as `arr`. For each value in `value`, the bin will be\\n      incremented by the corresponding weight instead of 1.\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`).\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `values` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n    name: A name for this op.\\n\\n  Returns:\\n    A SparseTensor with `output.shape = values.shape[:axis] + [N]`, where `N` is\\n      * `maxlength` (if set);\\n      * `minlength` (if set, and `minlength > reduce_max(values)`);\\n      * `0` (if `values` is empty);\\n      * `reduce_max(values) + 1` otherwise.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  Examples:\\n\\n  **Bin-counting every item in individual batches**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  This example shows a sparse tensor input. Missing zeros are not counted.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [0, 8], [0, 11],\\n  ...              [1, 9], [1, 11], [1, 18], [1, 27]],\\n  ...     values=[10, 20, 30, 20, 11, 101, 11, 10001],\\n  ...     dense_shape=[2, 30])\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Bin-counting with defined output shape**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i. However, all values of j\\n  above 'maxlength' are ignored. The dense_shape of the output sparse tensor\\n  is set to 'minlength'. Note that, while the input is identical to the\\n  example above, the value '10001' in batch item 2 is dropped, and the\\n  dense shape is [2, 500] instead of [2,10002] or [2, 102].\\n\\n  >>> minlength = maxlength = 500\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(\\n  ...    data, axis=-1, minlength=minlength, maxlength=maxlength)\\n  SparseTensor(indices=tf.Tensor(\\n  [[  0  10]\\n   [  0  20]\\n   [  0  30]\\n   [  1  11]\\n   [  1 101]], shape=(5, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1], shape=(5,), dtype=int64),\\n   dense_shape=tf.Tensor([  2 500], shape=(2,), dtype=int64))\\n\\n  **Binary bin-counting**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where (i,j) is 1 if the value j\\n  appears in batch i at least once and is 0 otherwise. Note that, even though\\n  some values (like 20 in batch 1 and 11 in batch 2) appear more than once,\\n  the 'values' tensor is all 1s.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, binary_output=True, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Weighted bin-counting**\\n\\n  This example takes two inputs - a values tensor and a weights tensor. These\\n  tensors must be identically shaped, and have the same row splits or indices\\n  in the case of RaggedTensors or SparseTensors. When performing a weighted\\n  count, the op will output a SparseTensor where the value of (i, j) is the\\n  sum of the values in the weight tensor's batch i in the locations where\\n  the values tensor has the value j. In this case, the output dtype is the\\n  same as the dtype of the weights tensor.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> weights = [[2, 0.25, 15, 0.5], [2, 17, 3, 0.9]]\\n  >>> tf.sparse.bincount(data, weights=weights, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([2. 0.75 15. 5. 17. 0.9], shape=(6,), dtype=float32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  \"\n    with ops.name_scope(name, 'count', [values, weights]):\n        if not isinstance(values, sparse_tensor.SparseTensor):\n            values = tensor_conversion.convert_to_tensor_v2_with_dispatch(values, name='values')\n        if weights is not None:\n            if not isinstance(weights, composite_tensor.CompositeTensor):\n                weights = tensor_conversion.convert_to_tensor_v2_with_dispatch(weights, name='weights')\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        minlength_value = minlength if minlength is not None else -1\n        maxlength_value = maxlength if maxlength is not None else -1\n        if axis == 0:\n            if isinstance(values, sparse_tensor.SparseTensor):\n                if weights is not None:\n                    weights = validate_sparse_weights(values, weights)\n                values = values.values\n            else:\n                if weights is not None:\n                    weights = array_ops.reshape(weights, [-1])\n                values = array_ops.reshape(values, [-1])\n        if isinstance(values, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.sparse_count_sparse_output(values.indices, values.values, values.dense_shape, weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.dense_count_sparse_output(values, weights=weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        return sparse_tensor.SparseTensor(c_ind, c_val, c_shape)",
            "@tf_export('sparse.bincount')\n@dispatch.add_dispatch_support\ndef sparse_bincount(values, weights=None, axis=0, minlength=None, maxlength=None, binary_output=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Count the number of times an integer value appears in a tensor.\\n\\n  This op takes an N-dimensional `Tensor`, `RaggedTensor`, or `SparseTensor`,\\n  and returns an N-dimensional int64 SparseTensor where element\\n  `[i0...i[axis], j]` contains the number of times the value `j` appears in\\n  slice `[i0...i[axis], :]` of the input tensor.  Currently, only N=0 and\\n  N=-1 are supported.\\n\\n  Args:\\n    values: A Tensor, RaggedTensor, or SparseTensor whose values should be\\n      counted. These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be the same shape as `arr`. If `arr` is a\\n      SparseTensor, `weights` must be a SparseTensor with the same dense shape\\n      and same indices as `arr`. For each value in `value`, the bin will be\\n      incremented by the corresponding weight instead of 1.\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`).\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `values` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n    name: A name for this op.\\n\\n  Returns:\\n    A SparseTensor with `output.shape = values.shape[:axis] + [N]`, where `N` is\\n      * `maxlength` (if set);\\n      * `minlength` (if set, and `minlength > reduce_max(values)`);\\n      * `0` (if `values` is empty);\\n      * `reduce_max(values) + 1` otherwise.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  Examples:\\n\\n  **Bin-counting every item in individual batches**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  This example shows a sparse tensor input. Missing zeros are not counted.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [0, 8], [0, 11],\\n  ...              [1, 9], [1, 11], [1, 18], [1, 27]],\\n  ...     values=[10, 20, 30, 20, 11, 101, 11, 10001],\\n  ...     dense_shape=[2, 30])\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Bin-counting with defined output shape**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i. However, all values of j\\n  above 'maxlength' are ignored. The dense_shape of the output sparse tensor\\n  is set to 'minlength'. Note that, while the input is identical to the\\n  example above, the value '10001' in batch item 2 is dropped, and the\\n  dense shape is [2, 500] instead of [2,10002] or [2, 102].\\n\\n  >>> minlength = maxlength = 500\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(\\n  ...    data, axis=-1, minlength=minlength, maxlength=maxlength)\\n  SparseTensor(indices=tf.Tensor(\\n  [[  0  10]\\n   [  0  20]\\n   [  0  30]\\n   [  1  11]\\n   [  1 101]], shape=(5, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1], shape=(5,), dtype=int64),\\n   dense_shape=tf.Tensor([  2 500], shape=(2,), dtype=int64))\\n\\n  **Binary bin-counting**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where (i,j) is 1 if the value j\\n  appears in batch i at least once and is 0 otherwise. Note that, even though\\n  some values (like 20 in batch 1 and 11 in batch 2) appear more than once,\\n  the 'values' tensor is all 1s.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, binary_output=True, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Weighted bin-counting**\\n\\n  This example takes two inputs - a values tensor and a weights tensor. These\\n  tensors must be identically shaped, and have the same row splits or indices\\n  in the case of RaggedTensors or SparseTensors. When performing a weighted\\n  count, the op will output a SparseTensor where the value of (i, j) is the\\n  sum of the values in the weight tensor's batch i in the locations where\\n  the values tensor has the value j. In this case, the output dtype is the\\n  same as the dtype of the weights tensor.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> weights = [[2, 0.25, 15, 0.5], [2, 17, 3, 0.9]]\\n  >>> tf.sparse.bincount(data, weights=weights, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([2. 0.75 15. 5. 17. 0.9], shape=(6,), dtype=float32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  \"\n    with ops.name_scope(name, 'count', [values, weights]):\n        if not isinstance(values, sparse_tensor.SparseTensor):\n            values = tensor_conversion.convert_to_tensor_v2_with_dispatch(values, name='values')\n        if weights is not None:\n            if not isinstance(weights, composite_tensor.CompositeTensor):\n                weights = tensor_conversion.convert_to_tensor_v2_with_dispatch(weights, name='weights')\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        minlength_value = minlength if minlength is not None else -1\n        maxlength_value = maxlength if maxlength is not None else -1\n        if axis == 0:\n            if isinstance(values, sparse_tensor.SparseTensor):\n                if weights is not None:\n                    weights = validate_sparse_weights(values, weights)\n                values = values.values\n            else:\n                if weights is not None:\n                    weights = array_ops.reshape(weights, [-1])\n                values = array_ops.reshape(values, [-1])\n        if isinstance(values, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.sparse_count_sparse_output(values.indices, values.values, values.dense_shape, weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.dense_count_sparse_output(values, weights=weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        return sparse_tensor.SparseTensor(c_ind, c_val, c_shape)",
            "@tf_export('sparse.bincount')\n@dispatch.add_dispatch_support\ndef sparse_bincount(values, weights=None, axis=0, minlength=None, maxlength=None, binary_output=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Count the number of times an integer value appears in a tensor.\\n\\n  This op takes an N-dimensional `Tensor`, `RaggedTensor`, or `SparseTensor`,\\n  and returns an N-dimensional int64 SparseTensor where element\\n  `[i0...i[axis], j]` contains the number of times the value `j` appears in\\n  slice `[i0...i[axis], :]` of the input tensor.  Currently, only N=0 and\\n  N=-1 are supported.\\n\\n  Args:\\n    values: A Tensor, RaggedTensor, or SparseTensor whose values should be\\n      counted. These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be the same shape as `arr`. If `arr` is a\\n      SparseTensor, `weights` must be a SparseTensor with the same dense shape\\n      and same indices as `arr`. For each value in `value`, the bin will be\\n      incremented by the corresponding weight instead of 1.\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`).\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `values` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n    name: A name for this op.\\n\\n  Returns:\\n    A SparseTensor with `output.shape = values.shape[:axis] + [N]`, where `N` is\\n      * `maxlength` (if set);\\n      * `minlength` (if set, and `minlength > reduce_max(values)`);\\n      * `0` (if `values` is empty);\\n      * `reduce_max(values) + 1` otherwise.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  Examples:\\n\\n  **Bin-counting every item in individual batches**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  This example shows a sparse tensor input. Missing zeros are not counted.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [0, 8], [0, 11],\\n  ...              [1, 9], [1, 11], [1, 18], [1, 27]],\\n  ...     values=[10, 20, 30, 20, 11, 101, 11, 10001],\\n  ...     dense_shape=[2, 30])\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Bin-counting with defined output shape**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i. However, all values of j\\n  above 'maxlength' are ignored. The dense_shape of the output sparse tensor\\n  is set to 'minlength'. Note that, while the input is identical to the\\n  example above, the value '10001' in batch item 2 is dropped, and the\\n  dense shape is [2, 500] instead of [2,10002] or [2, 102].\\n\\n  >>> minlength = maxlength = 500\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(\\n  ...    data, axis=-1, minlength=minlength, maxlength=maxlength)\\n  SparseTensor(indices=tf.Tensor(\\n  [[  0  10]\\n   [  0  20]\\n   [  0  30]\\n   [  1  11]\\n   [  1 101]], shape=(5, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1], shape=(5,), dtype=int64),\\n   dense_shape=tf.Tensor([  2 500], shape=(2,), dtype=int64))\\n\\n  **Binary bin-counting**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where (i,j) is 1 if the value j\\n  appears in batch i at least once and is 0 otherwise. Note that, even though\\n  some values (like 20 in batch 1 and 11 in batch 2) appear more than once,\\n  the 'values' tensor is all 1s.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, binary_output=True, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Weighted bin-counting**\\n\\n  This example takes two inputs - a values tensor and a weights tensor. These\\n  tensors must be identically shaped, and have the same row splits or indices\\n  in the case of RaggedTensors or SparseTensors. When performing a weighted\\n  count, the op will output a SparseTensor where the value of (i, j) is the\\n  sum of the values in the weight tensor's batch i in the locations where\\n  the values tensor has the value j. In this case, the output dtype is the\\n  same as the dtype of the weights tensor.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> weights = [[2, 0.25, 15, 0.5], [2, 17, 3, 0.9]]\\n  >>> tf.sparse.bincount(data, weights=weights, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([2. 0.75 15. 5. 17. 0.9], shape=(6,), dtype=float32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  \"\n    with ops.name_scope(name, 'count', [values, weights]):\n        if not isinstance(values, sparse_tensor.SparseTensor):\n            values = tensor_conversion.convert_to_tensor_v2_with_dispatch(values, name='values')\n        if weights is not None:\n            if not isinstance(weights, composite_tensor.CompositeTensor):\n                weights = tensor_conversion.convert_to_tensor_v2_with_dispatch(weights, name='weights')\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        minlength_value = minlength if minlength is not None else -1\n        maxlength_value = maxlength if maxlength is not None else -1\n        if axis == 0:\n            if isinstance(values, sparse_tensor.SparseTensor):\n                if weights is not None:\n                    weights = validate_sparse_weights(values, weights)\n                values = values.values\n            else:\n                if weights is not None:\n                    weights = array_ops.reshape(weights, [-1])\n                values = array_ops.reshape(values, [-1])\n        if isinstance(values, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.sparse_count_sparse_output(values.indices, values.values, values.dense_shape, weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.dense_count_sparse_output(values, weights=weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        return sparse_tensor.SparseTensor(c_ind, c_val, c_shape)",
            "@tf_export('sparse.bincount')\n@dispatch.add_dispatch_support\ndef sparse_bincount(values, weights=None, axis=0, minlength=None, maxlength=None, binary_output=False, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Count the number of times an integer value appears in a tensor.\\n\\n  This op takes an N-dimensional `Tensor`, `RaggedTensor`, or `SparseTensor`,\\n  and returns an N-dimensional int64 SparseTensor where element\\n  `[i0...i[axis], j]` contains the number of times the value `j` appears in\\n  slice `[i0...i[axis], :]` of the input tensor.  Currently, only N=0 and\\n  N=-1 are supported.\\n\\n  Args:\\n    values: A Tensor, RaggedTensor, or SparseTensor whose values should be\\n      counted. These tensors must have a rank of 2 if `axis=-1`.\\n    weights: If non-None, must be the same shape as `arr`. If `arr` is a\\n      SparseTensor, `weights` must be a SparseTensor with the same dense shape\\n      and same indices as `arr`. For each value in `value`, the bin will be\\n      incremented by the corresponding weight instead of 1.\\n    axis: The axis to slice over. Axes at and below `axis` will be flattened\\n      before bin counting. Currently, only `0`, and `-1` are supported. If None,\\n      all axes will be flattened (identical to passing `0`).\\n    minlength: If given, ensures the output has length at least `minlength`,\\n      padding with zeros at the end if necessary.\\n    maxlength: If given, skips values in `values` that are equal or greater than\\n      `maxlength`, ensuring that the output has length at most `maxlength`.\\n    binary_output: If True, this op will output 1 instead of the number of times\\n      a token appears (equivalent to one_hot + reduce_any instead of one_hot +\\n      reduce_add). Defaults to False.\\n    name: A name for this op.\\n\\n  Returns:\\n    A SparseTensor with `output.shape = values.shape[:axis] + [N]`, where `N` is\\n      * `maxlength` (if set);\\n      * `minlength` (if set, and `minlength > reduce_max(values)`);\\n      * `0` (if `values` is empty);\\n      * `reduce_max(values) + 1` otherwise.\\n\\n  Raises:\\n    `InvalidArgumentError` if negative values are provided as an input.\\n\\n  Examples:\\n\\n  **Bin-counting every item in individual batches**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  This example shows a sparse tensor input. Missing zeros are not counted.\\n\\n  >>> data = tf.sparse.SparseTensor(\\n  ...     indices=[[0, 3], [0, 7], [0, 8], [0, 11],\\n  ...              [1, 9], [1, 11], [1, 18], [1, 27]],\\n  ...     values=[10, 20, 30, 20, 11, 101, 11, 10001],\\n  ...     dense_shape=[2, 30])\\n  >>> tf.sparse.bincount(data, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Bin-counting with defined output shape**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where the value of (i,j) is the\\n  number of times value j appears in batch i. However, all values of j\\n  above 'maxlength' are ignored. The dense_shape of the output sparse tensor\\n  is set to 'minlength'. Note that, while the input is identical to the\\n  example above, the value '10001' in batch item 2 is dropped, and the\\n  dense shape is [2, 500] instead of [2,10002] or [2, 102].\\n\\n  >>> minlength = maxlength = 500\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(\\n  ...    data, axis=-1, minlength=minlength, maxlength=maxlength)\\n  SparseTensor(indices=tf.Tensor(\\n  [[  0  10]\\n   [  0  20]\\n   [  0  30]\\n   [  1  11]\\n   [  1 101]], shape=(5, 2), dtype=int64),\\n   values=tf.Tensor([1 2 1 2 1], shape=(5,), dtype=int64),\\n   dense_shape=tf.Tensor([  2 500], shape=(2,), dtype=int64))\\n\\n  **Binary bin-counting**\\n\\n  This example takes an input (which could be a Tensor, RaggedTensor, or\\n  SparseTensor) and returns a SparseTensor where (i,j) is 1 if the value j\\n  appears in batch i at least once and is 0 otherwise. Note that, even though\\n  some values (like 20 in batch 1 and 11 in batch 2) appear more than once,\\n  the 'values' tensor is all 1s.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> tf.sparse.bincount(data, binary_output=True, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int64),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  **Weighted bin-counting**\\n\\n  This example takes two inputs - a values tensor and a weights tensor. These\\n  tensors must be identically shaped, and have the same row splits or indices\\n  in the case of RaggedTensors or SparseTensors. When performing a weighted\\n  count, the op will output a SparseTensor where the value of (i, j) is the\\n  sum of the values in the weight tensor's batch i in the locations where\\n  the values tensor has the value j. In this case, the output dtype is the\\n  same as the dtype of the weights tensor.\\n\\n  >>> data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)\\n  >>> weights = [[2, 0.25, 15, 0.5], [2, 17, 3, 0.9]]\\n  >>> tf.sparse.bincount(data, weights=weights, axis=-1)\\n  SparseTensor(indices=tf.Tensor(\\n  [[    0    10]\\n   [    0    20]\\n   [    0    30]\\n   [    1    11]\\n   [    1   101]\\n   [    1 10001]], shape=(6, 2), dtype=int64),\\n   values=tf.Tensor([2. 0.75 15. 5. 17. 0.9], shape=(6,), dtype=float32),\\n   dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))\\n\\n  \"\n    with ops.name_scope(name, 'count', [values, weights]):\n        if not isinstance(values, sparse_tensor.SparseTensor):\n            values = tensor_conversion.convert_to_tensor_v2_with_dispatch(values, name='values')\n        if weights is not None:\n            if not isinstance(weights, composite_tensor.CompositeTensor):\n                weights = tensor_conversion.convert_to_tensor_v2_with_dispatch(weights, name='weights')\n        if weights is not None and binary_output:\n            raise ValueError('Arguments `binary_output` and `weights` are mutually exclusive. Please specify only one.')\n        if axis is None:\n            axis = 0\n        if axis not in [0, -1]:\n            raise ValueError(f'Unsupported value for argument axis={axis}. Only 0 and -1 are currently supported.')\n        minlength_value = minlength if minlength is not None else -1\n        maxlength_value = maxlength if maxlength is not None else -1\n        if axis == 0:\n            if isinstance(values, sparse_tensor.SparseTensor):\n                if weights is not None:\n                    weights = validate_sparse_weights(values, weights)\n                values = values.values\n            else:\n                if weights is not None:\n                    weights = array_ops.reshape(weights, [-1])\n                values = array_ops.reshape(values, [-1])\n        if isinstance(values, sparse_tensor.SparseTensor):\n            weights = validate_sparse_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.sparse_count_sparse_output(values.indices, values.values, values.dense_shape, weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        else:\n            weights = bincount_ops.validate_dense_weights(values, weights)\n            (c_ind, c_val, c_shape) = gen_count_ops.dense_count_sparse_output(values, weights=weights, minlength=minlength_value, maxlength=maxlength_value, binary_output=binary_output)\n        return sparse_tensor.SparseTensor(c_ind, c_val, c_shape)"
        ]
    },
    {
        "func_name": "validate_sparse_weights",
        "original": "def validate_sparse_weights(values, weights, dtype=None):\n    \"\"\"Validates the passed weight tensor or creates an empty one.\"\"\"\n    if weights is None:\n        if dtype:\n            return array_ops.constant([], dtype=dtype)\n        return array_ops.constant([], dtype=values.values.dtype)\n    if not isinstance(weights, sparse_tensor.SparseTensor):\n        raise ValueError(f'Argument `weights` must be a SparseTensor if `values` is a SparseTensor. Received weights={weights} of type: {type(weights).__name__}')\n    checks = []\n    if weights.dense_shape is not values.dense_shape:\n        checks.append(check_ops.assert_equal(weights.dense_shape, values.dense_shape, message=\"'weights' and 'values' must have the same dense shape.\"))\n    if weights.indices is not values.indices:\n        checks.append(check_ops.assert_equal(weights.indices, values.indices, message=\"'weights' and 'values' must have the same indices.\"))\n    if checks:\n        with ops.control_dependencies(checks):\n            weights = array_ops.identity(weights.values)\n    else:\n        weights = weights.values\n    return weights",
        "mutated": [
            "def validate_sparse_weights(values, weights, dtype=None):\n    if False:\n        i = 10\n    'Validates the passed weight tensor or creates an empty one.'\n    if weights is None:\n        if dtype:\n            return array_ops.constant([], dtype=dtype)\n        return array_ops.constant([], dtype=values.values.dtype)\n    if not isinstance(weights, sparse_tensor.SparseTensor):\n        raise ValueError(f'Argument `weights` must be a SparseTensor if `values` is a SparseTensor. Received weights={weights} of type: {type(weights).__name__}')\n    checks = []\n    if weights.dense_shape is not values.dense_shape:\n        checks.append(check_ops.assert_equal(weights.dense_shape, values.dense_shape, message=\"'weights' and 'values' must have the same dense shape.\"))\n    if weights.indices is not values.indices:\n        checks.append(check_ops.assert_equal(weights.indices, values.indices, message=\"'weights' and 'values' must have the same indices.\"))\n    if checks:\n        with ops.control_dependencies(checks):\n            weights = array_ops.identity(weights.values)\n    else:\n        weights = weights.values\n    return weights",
            "def validate_sparse_weights(values, weights, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Validates the passed weight tensor or creates an empty one.'\n    if weights is None:\n        if dtype:\n            return array_ops.constant([], dtype=dtype)\n        return array_ops.constant([], dtype=values.values.dtype)\n    if not isinstance(weights, sparse_tensor.SparseTensor):\n        raise ValueError(f'Argument `weights` must be a SparseTensor if `values` is a SparseTensor. Received weights={weights} of type: {type(weights).__name__}')\n    checks = []\n    if weights.dense_shape is not values.dense_shape:\n        checks.append(check_ops.assert_equal(weights.dense_shape, values.dense_shape, message=\"'weights' and 'values' must have the same dense shape.\"))\n    if weights.indices is not values.indices:\n        checks.append(check_ops.assert_equal(weights.indices, values.indices, message=\"'weights' and 'values' must have the same indices.\"))\n    if checks:\n        with ops.control_dependencies(checks):\n            weights = array_ops.identity(weights.values)\n    else:\n        weights = weights.values\n    return weights",
            "def validate_sparse_weights(values, weights, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Validates the passed weight tensor or creates an empty one.'\n    if weights is None:\n        if dtype:\n            return array_ops.constant([], dtype=dtype)\n        return array_ops.constant([], dtype=values.values.dtype)\n    if not isinstance(weights, sparse_tensor.SparseTensor):\n        raise ValueError(f'Argument `weights` must be a SparseTensor if `values` is a SparseTensor. Received weights={weights} of type: {type(weights).__name__}')\n    checks = []\n    if weights.dense_shape is not values.dense_shape:\n        checks.append(check_ops.assert_equal(weights.dense_shape, values.dense_shape, message=\"'weights' and 'values' must have the same dense shape.\"))\n    if weights.indices is not values.indices:\n        checks.append(check_ops.assert_equal(weights.indices, values.indices, message=\"'weights' and 'values' must have the same indices.\"))\n    if checks:\n        with ops.control_dependencies(checks):\n            weights = array_ops.identity(weights.values)\n    else:\n        weights = weights.values\n    return weights",
            "def validate_sparse_weights(values, weights, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Validates the passed weight tensor or creates an empty one.'\n    if weights is None:\n        if dtype:\n            return array_ops.constant([], dtype=dtype)\n        return array_ops.constant([], dtype=values.values.dtype)\n    if not isinstance(weights, sparse_tensor.SparseTensor):\n        raise ValueError(f'Argument `weights` must be a SparseTensor if `values` is a SparseTensor. Received weights={weights} of type: {type(weights).__name__}')\n    checks = []\n    if weights.dense_shape is not values.dense_shape:\n        checks.append(check_ops.assert_equal(weights.dense_shape, values.dense_shape, message=\"'weights' and 'values' must have the same dense shape.\"))\n    if weights.indices is not values.indices:\n        checks.append(check_ops.assert_equal(weights.indices, values.indices, message=\"'weights' and 'values' must have the same indices.\"))\n    if checks:\n        with ops.control_dependencies(checks):\n            weights = array_ops.identity(weights.values)\n    else:\n        weights = weights.values\n    return weights",
            "def validate_sparse_weights(values, weights, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Validates the passed weight tensor or creates an empty one.'\n    if weights is None:\n        if dtype:\n            return array_ops.constant([], dtype=dtype)\n        return array_ops.constant([], dtype=values.values.dtype)\n    if not isinstance(weights, sparse_tensor.SparseTensor):\n        raise ValueError(f'Argument `weights` must be a SparseTensor if `values` is a SparseTensor. Received weights={weights} of type: {type(weights).__name__}')\n    checks = []\n    if weights.dense_shape is not values.dense_shape:\n        checks.append(check_ops.assert_equal(weights.dense_shape, values.dense_shape, message=\"'weights' and 'values' must have the same dense shape.\"))\n    if weights.indices is not values.indices:\n        checks.append(check_ops.assert_equal(weights.indices, values.indices, message=\"'weights' and 'values' must have the same indices.\"))\n    if checks:\n        with ops.control_dependencies(checks):\n            weights = array_ops.identity(weights.values)\n    else:\n        weights = weights.values\n    return weights"
        ]
    },
    {
        "func_name": "_assert_sparse_compatible",
        "original": "def _assert_sparse_compatible(sparse_tensors):\n    \"\"\"Check that all of `sparse_tensors` have same `indices` and `dense_shape`.\n\n  Args:\n    sparse_tensors: A list of sparse tensors.\n\n  Returns:\n    An op to be used as a control dependency.\n  \"\"\"\n    checks = []\n    first = sparse_tensors[0]\n    for t in sparse_tensors[1:]:\n        checks.append(check_ops.assert_equal(first.dense_shape, t.dense_shape, message='Mismatched shapes!'))\n        checks.append(check_ops.assert_equal(first.indices, t.indices, message='Mismatched indices!'))\n    return checks",
        "mutated": [
            "def _assert_sparse_compatible(sparse_tensors):\n    if False:\n        i = 10\n    'Check that all of `sparse_tensors` have same `indices` and `dense_shape`.\\n\\n  Args:\\n    sparse_tensors: A list of sparse tensors.\\n\\n  Returns:\\n    An op to be used as a control dependency.\\n  '\n    checks = []\n    first = sparse_tensors[0]\n    for t in sparse_tensors[1:]:\n        checks.append(check_ops.assert_equal(first.dense_shape, t.dense_shape, message='Mismatched shapes!'))\n        checks.append(check_ops.assert_equal(first.indices, t.indices, message='Mismatched indices!'))\n    return checks",
            "def _assert_sparse_compatible(sparse_tensors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check that all of `sparse_tensors` have same `indices` and `dense_shape`.\\n\\n  Args:\\n    sparse_tensors: A list of sparse tensors.\\n\\n  Returns:\\n    An op to be used as a control dependency.\\n  '\n    checks = []\n    first = sparse_tensors[0]\n    for t in sparse_tensors[1:]:\n        checks.append(check_ops.assert_equal(first.dense_shape, t.dense_shape, message='Mismatched shapes!'))\n        checks.append(check_ops.assert_equal(first.indices, t.indices, message='Mismatched indices!'))\n    return checks",
            "def _assert_sparse_compatible(sparse_tensors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check that all of `sparse_tensors` have same `indices` and `dense_shape`.\\n\\n  Args:\\n    sparse_tensors: A list of sparse tensors.\\n\\n  Returns:\\n    An op to be used as a control dependency.\\n  '\n    checks = []\n    first = sparse_tensors[0]\n    for t in sparse_tensors[1:]:\n        checks.append(check_ops.assert_equal(first.dense_shape, t.dense_shape, message='Mismatched shapes!'))\n        checks.append(check_ops.assert_equal(first.indices, t.indices, message='Mismatched indices!'))\n    return checks",
            "def _assert_sparse_compatible(sparse_tensors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check that all of `sparse_tensors` have same `indices` and `dense_shape`.\\n\\n  Args:\\n    sparse_tensors: A list of sparse tensors.\\n\\n  Returns:\\n    An op to be used as a control dependency.\\n  '\n    checks = []\n    first = sparse_tensors[0]\n    for t in sparse_tensors[1:]:\n        checks.append(check_ops.assert_equal(first.dense_shape, t.dense_shape, message='Mismatched shapes!'))\n        checks.append(check_ops.assert_equal(first.indices, t.indices, message='Mismatched indices!'))\n    return checks",
            "def _assert_sparse_compatible(sparse_tensors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check that all of `sparse_tensors` have same `indices` and `dense_shape`.\\n\\n  Args:\\n    sparse_tensors: A list of sparse tensors.\\n\\n  Returns:\\n    An op to be used as a control dependency.\\n  '\n    checks = []\n    first = sparse_tensors[0]\n    for t in sparse_tensors[1:]:\n        checks.append(check_ops.assert_equal(first.dense_shape, t.dense_shape, message='Mismatched shapes!'))\n        checks.append(check_ops.assert_equal(first.indices, t.indices, message='Mismatched indices!'))\n    return checks"
        ]
    },
    {
        "func_name": "_replace_sparse_with_values",
        "original": "def _replace_sparse_with_values(value, sparse_list):\n    \"\"\"Replace `SparseTensor`s with their values in `value`\n\n  Each `SparseTensor` in `value` is replaced by its `values` tensor, and\n  collects all `SparseTensor`s in `sparse_list`.\n\n  Args:\n    value: A structure of `Tensor`s and `SparseTensor`s\n    sparse_list: A list. Output parameter that collects all `SparseTensor`s in\n      `value`.\n\n  Returns:\n    `value` with each SparseTensor replaced by its `.value` attribute.\n  \"\"\"\n    flat_vals = nest.flatten(value, expand_composites=False)\n    new_vals = []\n    for v in flat_vals:\n        if isinstance(v, sparse_tensor.SparseTensor):\n            sparse_list.append(v)\n            new_vals.append(v.values)\n        else:\n            new_vals.append(v)\n    return nest.pack_sequence_as(value, new_vals, expand_composites=False)",
        "mutated": [
            "def _replace_sparse_with_values(value, sparse_list):\n    if False:\n        i = 10\n    'Replace `SparseTensor`s with their values in `value`\\n\\n  Each `SparseTensor` in `value` is replaced by its `values` tensor, and\\n  collects all `SparseTensor`s in `sparse_list`.\\n\\n  Args:\\n    value: A structure of `Tensor`s and `SparseTensor`s\\n    sparse_list: A list. Output parameter that collects all `SparseTensor`s in\\n      `value`.\\n\\n  Returns:\\n    `value` with each SparseTensor replaced by its `.value` attribute.\\n  '\n    flat_vals = nest.flatten(value, expand_composites=False)\n    new_vals = []\n    for v in flat_vals:\n        if isinstance(v, sparse_tensor.SparseTensor):\n            sparse_list.append(v)\n            new_vals.append(v.values)\n        else:\n            new_vals.append(v)\n    return nest.pack_sequence_as(value, new_vals, expand_composites=False)",
            "def _replace_sparse_with_values(value, sparse_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Replace `SparseTensor`s with their values in `value`\\n\\n  Each `SparseTensor` in `value` is replaced by its `values` tensor, and\\n  collects all `SparseTensor`s in `sparse_list`.\\n\\n  Args:\\n    value: A structure of `Tensor`s and `SparseTensor`s\\n    sparse_list: A list. Output parameter that collects all `SparseTensor`s in\\n      `value`.\\n\\n  Returns:\\n    `value` with each SparseTensor replaced by its `.value` attribute.\\n  '\n    flat_vals = nest.flatten(value, expand_composites=False)\n    new_vals = []\n    for v in flat_vals:\n        if isinstance(v, sparse_tensor.SparseTensor):\n            sparse_list.append(v)\n            new_vals.append(v.values)\n        else:\n            new_vals.append(v)\n    return nest.pack_sequence_as(value, new_vals, expand_composites=False)",
            "def _replace_sparse_with_values(value, sparse_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Replace `SparseTensor`s with their values in `value`\\n\\n  Each `SparseTensor` in `value` is replaced by its `values` tensor, and\\n  collects all `SparseTensor`s in `sparse_list`.\\n\\n  Args:\\n    value: A structure of `Tensor`s and `SparseTensor`s\\n    sparse_list: A list. Output parameter that collects all `SparseTensor`s in\\n      `value`.\\n\\n  Returns:\\n    `value` with each SparseTensor replaced by its `.value` attribute.\\n  '\n    flat_vals = nest.flatten(value, expand_composites=False)\n    new_vals = []\n    for v in flat_vals:\n        if isinstance(v, sparse_tensor.SparseTensor):\n            sparse_list.append(v)\n            new_vals.append(v.values)\n        else:\n            new_vals.append(v)\n    return nest.pack_sequence_as(value, new_vals, expand_composites=False)",
            "def _replace_sparse_with_values(value, sparse_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Replace `SparseTensor`s with their values in `value`\\n\\n  Each `SparseTensor` in `value` is replaced by its `values` tensor, and\\n  collects all `SparseTensor`s in `sparse_list`.\\n\\n  Args:\\n    value: A structure of `Tensor`s and `SparseTensor`s\\n    sparse_list: A list. Output parameter that collects all `SparseTensor`s in\\n      `value`.\\n\\n  Returns:\\n    `value` with each SparseTensor replaced by its `.value` attribute.\\n  '\n    flat_vals = nest.flatten(value, expand_composites=False)\n    new_vals = []\n    for v in flat_vals:\n        if isinstance(v, sparse_tensor.SparseTensor):\n            sparse_list.append(v)\n            new_vals.append(v.values)\n        else:\n            new_vals.append(v)\n    return nest.pack_sequence_as(value, new_vals, expand_composites=False)",
            "def _replace_sparse_with_values(value, sparse_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Replace `SparseTensor`s with their values in `value`\\n\\n  Each `SparseTensor` in `value` is replaced by its `values` tensor, and\\n  collects all `SparseTensor`s in `sparse_list`.\\n\\n  Args:\\n    value: A structure of `Tensor`s and `SparseTensor`s\\n    sparse_list: A list. Output parameter that collects all `SparseTensor`s in\\n      `value`.\\n\\n  Returns:\\n    `value` with each SparseTensor replaced by its `.value` attribute.\\n  '\n    flat_vals = nest.flatten(value, expand_composites=False)\n    new_vals = []\n    for v in flat_vals:\n        if isinstance(v, sparse_tensor.SparseTensor):\n            sparse_list.append(v)\n            new_vals.append(v.values)\n        else:\n            new_vals.append(v)\n    return nest.pack_sequence_as(value, new_vals, expand_composites=False)"
        ]
    },
    {
        "func_name": "_add_sparse_to_tensors_map",
        "original": "def _add_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    \"\"\"Add a `SparseTensor` to a `SparseTensorsMap` and return its handle.\n\n  Args:\n    sp_input: The input `SparseTensor`.\n    container: The container for the underlying `SparseTensorsMap` (optional).\n    shared_name: The shared name for the underlying `SparseTensorsMap`\n      (optional, defaults to the name of the newly created op).\n    name: A name prefix for the returned tensors (optional).\n\n  Returns:\n    A string 1-vector (1D `Tensor`), with the single element representing the\n    a unique handle to a `SparseTensor` stored by the `SparseTensorMap`\n    underlying this op.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
        "mutated": [
            "def _add_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n    'Add a `SparseTensor` to a `SparseTensorsMap` and return its handle.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string 1-vector (1D `Tensor`), with the single element representing the\\n    a unique handle to a `SparseTensor` stored by the `SparseTensorMap`\\n    underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
            "def _add_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Add a `SparseTensor` to a `SparseTensorsMap` and return its handle.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string 1-vector (1D `Tensor`), with the single element representing the\\n    a unique handle to a `SparseTensor` stored by the `SparseTensorMap`\\n    underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
            "def _add_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Add a `SparseTensor` to a `SparseTensorsMap` and return its handle.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string 1-vector (1D `Tensor`), with the single element representing the\\n    a unique handle to a `SparseTensor` stored by the `SparseTensorMap`\\n    underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
            "def _add_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Add a `SparseTensor` to a `SparseTensorsMap` and return its handle.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string 1-vector (1D `Tensor`), with the single element representing the\\n    a unique handle to a `SparseTensor` stored by the `SparseTensorMap`\\n    underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
            "def _add_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Add a `SparseTensor` to a `SparseTensorsMap` and return its handle.\\n\\n  Args:\\n    sp_input: The input `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string 1-vector (1D `Tensor`), with the single element representing the\\n    a unique handle to a `SparseTensor` stored by the `SparseTensorMap`\\n    underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)"
        ]
    },
    {
        "func_name": "_add_many_sparse_to_tensors_map",
        "original": "def _add_many_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    \"\"\"Add a minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.\n\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\n  must be sorted in increasing order of this first dimension.  The serialized\n  `SparseTensor` objects going into each row of the output `Tensor` will have\n  rank `R-1`.\n\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\n\n  Args:\n    sp_input: The input rank `R` `SparseTensor`.\n    container: The container for the underlying `SparseTensorsMap` (optional).\n    shared_name: The shared name for the underlying `SparseTensorsMap`\n      (optional, defaults to the name of the newly created op).\n    name: A name prefix for the returned tensors (optional).\n\n  Returns:\n    A string matrix (2-D `Tensor`) with `N` rows and `1` column.\n    Each row represents a unique handle to a `SparseTensor` stored by\n    the `SparseTensorMap` underlying this op.\n\n  Raises:\n    TypeError: If `sp_input` is not a `SparseTensor`.\n  \"\"\"\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_many_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
        "mutated": [
            "def _add_many_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n    'Add a minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string matrix (2-D `Tensor`) with `N` rows and `1` column.\\n    Each row represents a unique handle to a `SparseTensor` stored by\\n    the `SparseTensorMap` underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_many_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
            "def _add_many_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Add a minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string matrix (2-D `Tensor`) with `N` rows and `1` column.\\n    Each row represents a unique handle to a `SparseTensor` stored by\\n    the `SparseTensorMap` underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_many_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
            "def _add_many_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Add a minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string matrix (2-D `Tensor`) with `N` rows and `1` column.\\n    Each row represents a unique handle to a `SparseTensor` stored by\\n    the `SparseTensorMap` underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_many_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
            "def _add_many_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Add a minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string matrix (2-D `Tensor`) with `N` rows and `1` column.\\n    Each row represents a unique handle to a `SparseTensor` stored by\\n    the `SparseTensorMap` underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_many_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)",
            "def _add_many_sparse_to_tensors_map(sp_input, container=None, shared_name=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Add a minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.\\n\\n  The `SparseTensor` must have rank `R` greater than 1, and the first dimension\\n  is treated as the minibatch dimension.  Elements of the `SparseTensor`\\n  must be sorted in increasing order of this first dimension.  The serialized\\n  `SparseTensor` objects going into each row of the output `Tensor` will have\\n  rank `R-1`.\\n\\n  The minibatch size `N` is extracted from `sparse_shape[0]`.\\n\\n  Args:\\n    sp_input: The input rank `R` `SparseTensor`.\\n    container: The container for the underlying `SparseTensorsMap` (optional).\\n    shared_name: The shared name for the underlying `SparseTensorsMap`\\n      (optional, defaults to the name of the newly created op).\\n    name: A name prefix for the returned tensors (optional).\\n\\n  Returns:\\n    A string matrix (2-D `Tensor`) with `N` rows and `1` column.\\n    Each row represents a unique handle to a `SparseTensor` stored by\\n    the `SparseTensorMap` underlying this op.\\n\\n  Raises:\\n    TypeError: If `sp_input` is not a `SparseTensor`.\\n  '\n    sp_input = _convert_to_sparse_tensor(sp_input)\n    return gen_sparse_ops.add_many_sparse_to_tensors_map(sp_input.indices, sp_input.values, sp_input.dense_shape, container=container, shared_name=shared_name, name=name)"
        ]
    },
    {
        "func_name": "_take_many_sparse_from_tensors_map",
        "original": "def _take_many_sparse_from_tensors_map(sparse_map_op, sparse_handles, rank=None, name=None):\n    \"\"\"Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.\n\n  The input `sparse_handles` must be a string matrix of shape `[N, 1]` where\n  `N` is the minibatch size and the rows correspond to packed outputs of\n  `add_sparse_to_tensors_map`.  The ranks of the original `SparseTensor` objects\n  must all match.  When the final `SparseTensor` is created, it has rank one\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\n  concatenated along a new row dimension).\n\n  The output `SparseTensor` object's shape values for all dimensions but the\n  first are the max across the input `SparseTensor` objects' shape values\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\n  size.\n\n  The input `SparseTensor` objects' indices are assumed ordered in\n  standard lexicographic order.  If this is not the case, after this\n  step run `sparse.reorder` to restore index ordering.\n\n  For example, if the serialized input is a `[2, 3]` matrix representing two\n  original `SparseTensor` objects:\n\n      index = [ 0]\n              [10]\n              [20]\n      values = [1, 2, 3]\n      shape = [50]\n\n  and\n\n      index = [ 2]\n              [10]\n      values = [4, 5]\n      shape = [30]\n\n  then the final deserialized `SparseTensor` will be:\n\n      index = [0  0]\n              [0 10]\n              [0 20]\n              [1  2]\n              [1 10]\n      values = [1, 2, 3, 4, 5]\n      shape = [2 50]\n\n  Args:\n    sparse_map_op: The `Operation` that created the original handles.\n      Usually this is, e.g., `add_sparse_to_tensors_map(...).op`.\n    sparse_handles: 2-D `Tensor` of type `string` of shape `[N, 1]`.\n      The serialized and packed `SparseTensor` objects.\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\n    name: A name prefix for the returned tensors (optional)\n\n  Returns:\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\n    concatenated along the `SparseTensor`s' first dimension.\n\n    All of the serialized `SparseTensor`s must have had the same rank and type.\n  \"\"\"\n    if not isinstance(sparse_map_op, ops.Operation):\n        raise TypeError('sparse_map_op be an Operation')\n    if sparse_map_op.type not in ('AddSparseToTensorsMap', 'AddManySparseToTensorsMap'):\n        raise TypeError('sparse_map_op must be one of AddSparseToTensorsMap or AddSparseToTensorsMap. Instead, found `%s`.' % sparse_map_op.type)\n    with ops.colocate_with(sparse_map_op):\n        shared_name = sparse_map_op.get_attr('shared_name') or sparse_map_op.name\n        (output_indices, output_values, output_shape) = gen_sparse_ops.take_many_sparse_from_tensors_map(sparse_handles, dtype=sparse_map_op.get_attr('T'), container=sparse_map_op.get_attr('container'), shared_name=shared_name, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
        "mutated": [
            "def _take_many_sparse_from_tensors_map(sparse_map_op, sparse_handles, rank=None, name=None):\n    if False:\n        i = 10\n    \"Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.\\n\\n  The input `sparse_handles` must be a string matrix of shape `[N, 1]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `add_sparse_to_tensors_map`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    sparse_map_op: The `Operation` that created the original handles.\\n      Usually this is, e.g., `add_sparse_to_tensors_map(...).op`.\\n    sparse_handles: 2-D `Tensor` of type `string` of shape `[N, 1]`.\\n      The serialized and packed `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    if not isinstance(sparse_map_op, ops.Operation):\n        raise TypeError('sparse_map_op be an Operation')\n    if sparse_map_op.type not in ('AddSparseToTensorsMap', 'AddManySparseToTensorsMap'):\n        raise TypeError('sparse_map_op must be one of AddSparseToTensorsMap or AddSparseToTensorsMap. Instead, found `%s`.' % sparse_map_op.type)\n    with ops.colocate_with(sparse_map_op):\n        shared_name = sparse_map_op.get_attr('shared_name') or sparse_map_op.name\n        (output_indices, output_values, output_shape) = gen_sparse_ops.take_many_sparse_from_tensors_map(sparse_handles, dtype=sparse_map_op.get_attr('T'), container=sparse_map_op.get_attr('container'), shared_name=shared_name, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "def _take_many_sparse_from_tensors_map(sparse_map_op, sparse_handles, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.\\n\\n  The input `sparse_handles` must be a string matrix of shape `[N, 1]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `add_sparse_to_tensors_map`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    sparse_map_op: The `Operation` that created the original handles.\\n      Usually this is, e.g., `add_sparse_to_tensors_map(...).op`.\\n    sparse_handles: 2-D `Tensor` of type `string` of shape `[N, 1]`.\\n      The serialized and packed `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    if not isinstance(sparse_map_op, ops.Operation):\n        raise TypeError('sparse_map_op be an Operation')\n    if sparse_map_op.type not in ('AddSparseToTensorsMap', 'AddManySparseToTensorsMap'):\n        raise TypeError('sparse_map_op must be one of AddSparseToTensorsMap or AddSparseToTensorsMap. Instead, found `%s`.' % sparse_map_op.type)\n    with ops.colocate_with(sparse_map_op):\n        shared_name = sparse_map_op.get_attr('shared_name') or sparse_map_op.name\n        (output_indices, output_values, output_shape) = gen_sparse_ops.take_many_sparse_from_tensors_map(sparse_handles, dtype=sparse_map_op.get_attr('T'), container=sparse_map_op.get_attr('container'), shared_name=shared_name, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "def _take_many_sparse_from_tensors_map(sparse_map_op, sparse_handles, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.\\n\\n  The input `sparse_handles` must be a string matrix of shape `[N, 1]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `add_sparse_to_tensors_map`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    sparse_map_op: The `Operation` that created the original handles.\\n      Usually this is, e.g., `add_sparse_to_tensors_map(...).op`.\\n    sparse_handles: 2-D `Tensor` of type `string` of shape `[N, 1]`.\\n      The serialized and packed `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    if not isinstance(sparse_map_op, ops.Operation):\n        raise TypeError('sparse_map_op be an Operation')\n    if sparse_map_op.type not in ('AddSparseToTensorsMap', 'AddManySparseToTensorsMap'):\n        raise TypeError('sparse_map_op must be one of AddSparseToTensorsMap or AddSparseToTensorsMap. Instead, found `%s`.' % sparse_map_op.type)\n    with ops.colocate_with(sparse_map_op):\n        shared_name = sparse_map_op.get_attr('shared_name') or sparse_map_op.name\n        (output_indices, output_values, output_shape) = gen_sparse_ops.take_many_sparse_from_tensors_map(sparse_handles, dtype=sparse_map_op.get_attr('T'), container=sparse_map_op.get_attr('container'), shared_name=shared_name, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "def _take_many_sparse_from_tensors_map(sparse_map_op, sparse_handles, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.\\n\\n  The input `sparse_handles` must be a string matrix of shape `[N, 1]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `add_sparse_to_tensors_map`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    sparse_map_op: The `Operation` that created the original handles.\\n      Usually this is, e.g., `add_sparse_to_tensors_map(...).op`.\\n    sparse_handles: 2-D `Tensor` of type `string` of shape `[N, 1]`.\\n      The serialized and packed `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    if not isinstance(sparse_map_op, ops.Operation):\n        raise TypeError('sparse_map_op be an Operation')\n    if sparse_map_op.type not in ('AddSparseToTensorsMap', 'AddManySparseToTensorsMap'):\n        raise TypeError('sparse_map_op must be one of AddSparseToTensorsMap or AddSparseToTensorsMap. Instead, found `%s`.' % sparse_map_op.type)\n    with ops.colocate_with(sparse_map_op):\n        shared_name = sparse_map_op.get_attr('shared_name') or sparse_map_op.name\n        (output_indices, output_values, output_shape) = gen_sparse_ops.take_many_sparse_from_tensors_map(sparse_handles, dtype=sparse_map_op.get_attr('T'), container=sparse_map_op.get_attr('container'), shared_name=shared_name, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)",
            "def _take_many_sparse_from_tensors_map(sparse_map_op, sparse_handles, rank=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.\\n\\n  The input `sparse_handles` must be a string matrix of shape `[N, 1]` where\\n  `N` is the minibatch size and the rows correspond to packed outputs of\\n  `add_sparse_to_tensors_map`.  The ranks of the original `SparseTensor` objects\\n  must all match.  When the final `SparseTensor` is created, it has rank one\\n  higher than the ranks of the incoming `SparseTensor` objects (they have been\\n  concatenated along a new row dimension).\\n\\n  The output `SparseTensor` object's shape values for all dimensions but the\\n  first are the max across the input `SparseTensor` objects' shape values\\n  for the corresponding dimensions.  Its first shape value is `N`, the minibatch\\n  size.\\n\\n  The input `SparseTensor` objects' indices are assumed ordered in\\n  standard lexicographic order.  If this is not the case, after this\\n  step run `sparse.reorder` to restore index ordering.\\n\\n  For example, if the serialized input is a `[2, 3]` matrix representing two\\n  original `SparseTensor` objects:\\n\\n      index = [ 0]\\n              [10]\\n              [20]\\n      values = [1, 2, 3]\\n      shape = [50]\\n\\n  and\\n\\n      index = [ 2]\\n              [10]\\n      values = [4, 5]\\n      shape = [30]\\n\\n  then the final deserialized `SparseTensor` will be:\\n\\n      index = [0  0]\\n              [0 10]\\n              [0 20]\\n              [1  2]\\n              [1 10]\\n      values = [1, 2, 3, 4, 5]\\n      shape = [2 50]\\n\\n  Args:\\n    sparse_map_op: The `Operation` that created the original handles.\\n      Usually this is, e.g., `add_sparse_to_tensors_map(...).op`.\\n    sparse_handles: 2-D `Tensor` of type `string` of shape `[N, 1]`.\\n      The serialized and packed `SparseTensor` objects.\\n    rank: (optional) Python int, the rank of the `SparseTensor` objects.\\n    name: A name prefix for the returned tensors (optional)\\n\\n  Returns:\\n    A `SparseTensor` representing the deserialized `SparseTensor`s,\\n    concatenated along the `SparseTensor`s' first dimension.\\n\\n    All of the serialized `SparseTensor`s must have had the same rank and type.\\n  \"\n    if not isinstance(sparse_map_op, ops.Operation):\n        raise TypeError('sparse_map_op be an Operation')\n    if sparse_map_op.type not in ('AddSparseToTensorsMap', 'AddManySparseToTensorsMap'):\n        raise TypeError('sparse_map_op must be one of AddSparseToTensorsMap or AddSparseToTensorsMap. Instead, found `%s`.' % sparse_map_op.type)\n    with ops.colocate_with(sparse_map_op):\n        shared_name = sparse_map_op.get_attr('shared_name') or sparse_map_op.name\n        (output_indices, output_values, output_shape) = gen_sparse_ops.take_many_sparse_from_tensors_map(sparse_handles, dtype=sparse_map_op.get_attr('T'), container=sparse_map_op.get_attr('container'), shared_name=shared_name, name=name)\n    output_indices.set_shape([None, rank])\n    output_shape.set_shape([rank])\n    return sparse_tensor.SparseTensor(output_indices, output_values, output_shape)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, original_func):\n    self._original_func = original_func\n    func_name = get_canonical_name_for_symbol(original_func)\n    arg_names = tf_inspect.getfullargspec(original_func)[0]\n    self._x = arg_names[0]\n    original_func.__doc__ = original_func.__doc__.rstrip() + '\\n\\n' + '    If `{x}` is a `SparseTensor`, returns\\n    `SparseTensor({x}.indices, tf.{func}({x}.values, ...), {x}.dense_shape)`'.format(x=self._x, func=func_name)",
        "mutated": [
            "def __init__(self, original_func):\n    if False:\n        i = 10\n    self._original_func = original_func\n    func_name = get_canonical_name_for_symbol(original_func)\n    arg_names = tf_inspect.getfullargspec(original_func)[0]\n    self._x = arg_names[0]\n    original_func.__doc__ = original_func.__doc__.rstrip() + '\\n\\n' + '    If `{x}` is a `SparseTensor`, returns\\n    `SparseTensor({x}.indices, tf.{func}({x}.values, ...), {x}.dense_shape)`'.format(x=self._x, func=func_name)",
            "def __init__(self, original_func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self._original_func = original_func\n    func_name = get_canonical_name_for_symbol(original_func)\n    arg_names = tf_inspect.getfullargspec(original_func)[0]\n    self._x = arg_names[0]\n    original_func.__doc__ = original_func.__doc__.rstrip() + '\\n\\n' + '    If `{x}` is a `SparseTensor`, returns\\n    `SparseTensor({x}.indices, tf.{func}({x}.values, ...), {x}.dense_shape)`'.format(x=self._x, func=func_name)",
            "def __init__(self, original_func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self._original_func = original_func\n    func_name = get_canonical_name_for_symbol(original_func)\n    arg_names = tf_inspect.getfullargspec(original_func)[0]\n    self._x = arg_names[0]\n    original_func.__doc__ = original_func.__doc__.rstrip() + '\\n\\n' + '    If `{x}` is a `SparseTensor`, returns\\n    `SparseTensor({x}.indices, tf.{func}({x}.values, ...), {x}.dense_shape)`'.format(x=self._x, func=func_name)",
            "def __init__(self, original_func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self._original_func = original_func\n    func_name = get_canonical_name_for_symbol(original_func)\n    arg_names = tf_inspect.getfullargspec(original_func)[0]\n    self._x = arg_names[0]\n    original_func.__doc__ = original_func.__doc__.rstrip() + '\\n\\n' + '    If `{x}` is a `SparseTensor`, returns\\n    `SparseTensor({x}.indices, tf.{func}({x}.values, ...), {x}.dense_shape)`'.format(x=self._x, func=func_name)",
            "def __init__(self, original_func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self._original_func = original_func\n    func_name = get_canonical_name_for_symbol(original_func)\n    arg_names = tf_inspect.getfullargspec(original_func)[0]\n    self._x = arg_names[0]\n    original_func.__doc__ = original_func.__doc__.rstrip() + '\\n\\n' + '    If `{x}` is a `SparseTensor`, returns\\n    `SparseTensor({x}.indices, tf.{func}({x}.values, ...), {x}.dense_shape)`'.format(x=self._x, func=func_name)"
        ]
    },
    {
        "func_name": "handle",
        "original": "def handle(self, args, kwargs):\n    if args:\n        (x, args) = (args[0], args[1:])\n    else:\n        kwargs = kwargs.copy()\n        x = kwargs.pop(self._x, None)\n    if isinstance(x, sparse_tensor.SparseTensor):\n        return sparse_tensor.SparseTensor(indices=x.indices, values=self._original_func(x.values, *args, **kwargs), dense_shape=x.dense_shape)\n    else:\n        return self.NOT_SUPPORTED",
        "mutated": [
            "def handle(self, args, kwargs):\n    if False:\n        i = 10\n    if args:\n        (x, args) = (args[0], args[1:])\n    else:\n        kwargs = kwargs.copy()\n        x = kwargs.pop(self._x, None)\n    if isinstance(x, sparse_tensor.SparseTensor):\n        return sparse_tensor.SparseTensor(indices=x.indices, values=self._original_func(x.values, *args, **kwargs), dense_shape=x.dense_shape)\n    else:\n        return self.NOT_SUPPORTED",
            "def handle(self, args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if args:\n        (x, args) = (args[0], args[1:])\n    else:\n        kwargs = kwargs.copy()\n        x = kwargs.pop(self._x, None)\n    if isinstance(x, sparse_tensor.SparseTensor):\n        return sparse_tensor.SparseTensor(indices=x.indices, values=self._original_func(x.values, *args, **kwargs), dense_shape=x.dense_shape)\n    else:\n        return self.NOT_SUPPORTED",
            "def handle(self, args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if args:\n        (x, args) = (args[0], args[1:])\n    else:\n        kwargs = kwargs.copy()\n        x = kwargs.pop(self._x, None)\n    if isinstance(x, sparse_tensor.SparseTensor):\n        return sparse_tensor.SparseTensor(indices=x.indices, values=self._original_func(x.values, *args, **kwargs), dense_shape=x.dense_shape)\n    else:\n        return self.NOT_SUPPORTED",
            "def handle(self, args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if args:\n        (x, args) = (args[0], args[1:])\n    else:\n        kwargs = kwargs.copy()\n        x = kwargs.pop(self._x, None)\n    if isinstance(x, sparse_tensor.SparseTensor):\n        return sparse_tensor.SparseTensor(indices=x.indices, values=self._original_func(x.values, *args, **kwargs), dense_shape=x.dense_shape)\n    else:\n        return self.NOT_SUPPORTED",
            "def handle(self, args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if args:\n        (x, args) = (args[0], args[1:])\n    else:\n        kwargs = kwargs.copy()\n        x = kwargs.pop(self._x, None)\n    if isinstance(x, sparse_tensor.SparseTensor):\n        return sparse_tensor.SparseTensor(indices=x.indices, values=self._original_func(x.values, *args, **kwargs), dense_shape=x.dense_shape)\n    else:\n        return self.NOT_SUPPORTED"
        ]
    }
]