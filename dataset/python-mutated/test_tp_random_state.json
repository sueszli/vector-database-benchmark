[
    {
        "func_name": "get_tensor_slice",
        "original": "def get_tensor_slice(self, idx, n, large_tensor):\n    shape = large_tensor.shape\n    assert shape[0] % n == 0\n    local_shape = [shape[0] // n, shape[1]]\n    slice_idx = [slice(idx * local_shape[0], (idx + 1) * local_shape[0]), slice(local_shape[1])]\n    return large_tensor[slice_idx]",
        "mutated": [
            "def get_tensor_slice(self, idx, n, large_tensor):\n    if False:\n        i = 10\n    shape = large_tensor.shape\n    assert shape[0] % n == 0\n    local_shape = [shape[0] // n, shape[1]]\n    slice_idx = [slice(idx * local_shape[0], (idx + 1) * local_shape[0]), slice(local_shape[1])]\n    return large_tensor[slice_idx]",
            "def get_tensor_slice(self, idx, n, large_tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    shape = large_tensor.shape\n    assert shape[0] % n == 0\n    local_shape = [shape[0] // n, shape[1]]\n    slice_idx = [slice(idx * local_shape[0], (idx + 1) * local_shape[0]), slice(local_shape[1])]\n    return large_tensor[slice_idx]",
            "def get_tensor_slice(self, idx, n, large_tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    shape = large_tensor.shape\n    assert shape[0] % n == 0\n    local_shape = [shape[0] // n, shape[1]]\n    slice_idx = [slice(idx * local_shape[0], (idx + 1) * local_shape[0]), slice(local_shape[1])]\n    return large_tensor[slice_idx]",
            "def get_tensor_slice(self, idx, n, large_tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    shape = large_tensor.shape\n    assert shape[0] % n == 0\n    local_shape = [shape[0] // n, shape[1]]\n    slice_idx = [slice(idx * local_shape[0], (idx + 1) * local_shape[0]), slice(local_shape[1])]\n    return large_tensor[slice_idx]",
            "def get_tensor_slice(self, idx, n, large_tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    shape = large_tensor.shape\n    assert shape[0] % n == 0\n    local_shape = [shape[0] // n, shape[1]]\n    slice_idx = [slice(idx * local_shape[0], (idx + 1) * local_shape[0]), slice(local_shape[1])]\n    return large_tensor[slice_idx]"
        ]
    },
    {
        "func_name": "check_gathered_tensors",
        "original": "def check_gathered_tensors(self, self_rank, size, gathered_tensors, assertFunc):\n    for other_rank in range(size):\n        if self_rank != other_rank:\n            assertFunc(self.get_tensor_slice(self_rank, size, gathered_tensors), self.get_tensor_slice(other_rank, size, gathered_tensors))",
        "mutated": [
            "def check_gathered_tensors(self, self_rank, size, gathered_tensors, assertFunc):\n    if False:\n        i = 10\n    for other_rank in range(size):\n        if self_rank != other_rank:\n            assertFunc(self.get_tensor_slice(self_rank, size, gathered_tensors), self.get_tensor_slice(other_rank, size, gathered_tensors))",
            "def check_gathered_tensors(self, self_rank, size, gathered_tensors, assertFunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for other_rank in range(size):\n        if self_rank != other_rank:\n            assertFunc(self.get_tensor_slice(self_rank, size, gathered_tensors), self.get_tensor_slice(other_rank, size, gathered_tensors))",
            "def check_gathered_tensors(self, self_rank, size, gathered_tensors, assertFunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for other_rank in range(size):\n        if self_rank != other_rank:\n            assertFunc(self.get_tensor_slice(self_rank, size, gathered_tensors), self.get_tensor_slice(other_rank, size, gathered_tensors))",
            "def check_gathered_tensors(self, self_rank, size, gathered_tensors, assertFunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for other_rank in range(size):\n        if self_rank != other_rank:\n            assertFunc(self.get_tensor_slice(self_rank, size, gathered_tensors), self.get_tensor_slice(other_rank, size, gathered_tensors))",
            "def check_gathered_tensors(self, self_rank, size, gathered_tensors, assertFunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for other_rank in range(size):\n        if self_rank != other_rank:\n            assertFunc(self.get_tensor_slice(self_rank, size, gathered_tensors), self.get_tensor_slice(other_rank, size, gathered_tensors))"
        ]
    },
    {
        "func_name": "tp_weights_assert",
        "original": "def tp_weights_assert(tensor1, tensor2):\n    if enable_distribute_flag:\n        self.assertNotEqual(tensor1, tensor2)\n    else:\n        self.assertEqual(tensor1, tensor2)",
        "mutated": [
            "def tp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n    if enable_distribute_flag:\n        self.assertNotEqual(tensor1, tensor2)\n    else:\n        self.assertEqual(tensor1, tensor2)",
            "def tp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if enable_distribute_flag:\n        self.assertNotEqual(tensor1, tensor2)\n    else:\n        self.assertEqual(tensor1, tensor2)",
            "def tp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if enable_distribute_flag:\n        self.assertNotEqual(tensor1, tensor2)\n    else:\n        self.assertEqual(tensor1, tensor2)",
            "def tp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if enable_distribute_flag:\n        self.assertNotEqual(tensor1, tensor2)\n    else:\n        self.assertEqual(tensor1, tensor2)",
            "def tp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if enable_distribute_flag:\n        self.assertNotEqual(tensor1, tensor2)\n    else:\n        self.assertEqual(tensor1, tensor2)"
        ]
    },
    {
        "func_name": "dp_weights_assert",
        "original": "def dp_weights_assert(tensor1, tensor2):\n    if enable_distribute_flag:\n        self.assertEqual(tensor1, tensor2)\n    else:\n        self.assertNotEqual(tensor1, tensor2)",
        "mutated": [
            "def dp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n    if enable_distribute_flag:\n        self.assertEqual(tensor1, tensor2)\n    else:\n        self.assertNotEqual(tensor1, tensor2)",
            "def dp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if enable_distribute_flag:\n        self.assertEqual(tensor1, tensor2)\n    else:\n        self.assertNotEqual(tensor1, tensor2)",
            "def dp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if enable_distribute_flag:\n        self.assertEqual(tensor1, tensor2)\n    else:\n        self.assertNotEqual(tensor1, tensor2)",
            "def dp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if enable_distribute_flag:\n        self.assertEqual(tensor1, tensor2)\n    else:\n        self.assertNotEqual(tensor1, tensor2)",
            "def dp_weights_assert(tensor1, tensor2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if enable_distribute_flag:\n        self.assertEqual(tensor1, tensor2)\n    else:\n        self.assertNotEqual(tensor1, tensor2)"
        ]
    },
    {
        "func_name": "test_model_init",
        "original": "@with_comms\n@skip_if_lt_x_gpu(4)\ndef test_model_init(self):\n    mesh = torch.arange(self.world_size).reshape(2, 2)\n    device_mesh = DeviceMesh(self.device_type, mesh)\n    tp_rank = device_mesh.get_coordinate()[0]\n    dp_rank = device_mesh.get_coordinate()[1]\n    for enable_distribute_flag in [False, True]:\n        model = MLPModule(device='meta')\n        model_tp = parallelize_module(model, device_mesh, {'net1': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d), 'net2': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d)})\n        torch.cuda.manual_seed(dp_rank)\n        random._rng_tracker.distribute_region_enabled = enable_distribute_flag\n        self.assertTrue(model_tp.net1.weight.is_meta)\n        model_tp.to_empty(device=self.device_type)\n        model_tp.reset_parameters()\n        for dtensor in [model_tp.net1.weight, model_tp.net2.weight]:\n            _1d_mesh = dtensor.device_mesh\n            assert _1d_mesh.ndim == 1\n            tensor_local = dtensor.to_local()\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 0))\n            self.assertEqual(_1d_mesh.get_coordinate()[0], tp_rank)\n\n            def tp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertNotEqual(tensor1, tensor2)\n                else:\n                    self.assertEqual(tensor1, tensor2)\n            self.check_gathered_tensors(tp_rank, 2, tensor_gather, tp_weights_assert)\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 1))\n\n            def dp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertEqual(tensor1, tensor2)\n                else:\n                    self.assertNotEqual(tensor1, tensor2)\n            self.check_gathered_tensors(dp_rank, 2, tensor_gather, dp_weights_assert)",
        "mutated": [
            "@with_comms\n@skip_if_lt_x_gpu(4)\ndef test_model_init(self):\n    if False:\n        i = 10\n    mesh = torch.arange(self.world_size).reshape(2, 2)\n    device_mesh = DeviceMesh(self.device_type, mesh)\n    tp_rank = device_mesh.get_coordinate()[0]\n    dp_rank = device_mesh.get_coordinate()[1]\n    for enable_distribute_flag in [False, True]:\n        model = MLPModule(device='meta')\n        model_tp = parallelize_module(model, device_mesh, {'net1': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d), 'net2': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d)})\n        torch.cuda.manual_seed(dp_rank)\n        random._rng_tracker.distribute_region_enabled = enable_distribute_flag\n        self.assertTrue(model_tp.net1.weight.is_meta)\n        model_tp.to_empty(device=self.device_type)\n        model_tp.reset_parameters()\n        for dtensor in [model_tp.net1.weight, model_tp.net2.weight]:\n            _1d_mesh = dtensor.device_mesh\n            assert _1d_mesh.ndim == 1\n            tensor_local = dtensor.to_local()\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 0))\n            self.assertEqual(_1d_mesh.get_coordinate()[0], tp_rank)\n\n            def tp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertNotEqual(tensor1, tensor2)\n                else:\n                    self.assertEqual(tensor1, tensor2)\n            self.check_gathered_tensors(tp_rank, 2, tensor_gather, tp_weights_assert)\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 1))\n\n            def dp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertEqual(tensor1, tensor2)\n                else:\n                    self.assertNotEqual(tensor1, tensor2)\n            self.check_gathered_tensors(dp_rank, 2, tensor_gather, dp_weights_assert)",
            "@with_comms\n@skip_if_lt_x_gpu(4)\ndef test_model_init(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mesh = torch.arange(self.world_size).reshape(2, 2)\n    device_mesh = DeviceMesh(self.device_type, mesh)\n    tp_rank = device_mesh.get_coordinate()[0]\n    dp_rank = device_mesh.get_coordinate()[1]\n    for enable_distribute_flag in [False, True]:\n        model = MLPModule(device='meta')\n        model_tp = parallelize_module(model, device_mesh, {'net1': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d), 'net2': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d)})\n        torch.cuda.manual_seed(dp_rank)\n        random._rng_tracker.distribute_region_enabled = enable_distribute_flag\n        self.assertTrue(model_tp.net1.weight.is_meta)\n        model_tp.to_empty(device=self.device_type)\n        model_tp.reset_parameters()\n        for dtensor in [model_tp.net1.weight, model_tp.net2.weight]:\n            _1d_mesh = dtensor.device_mesh\n            assert _1d_mesh.ndim == 1\n            tensor_local = dtensor.to_local()\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 0))\n            self.assertEqual(_1d_mesh.get_coordinate()[0], tp_rank)\n\n            def tp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertNotEqual(tensor1, tensor2)\n                else:\n                    self.assertEqual(tensor1, tensor2)\n            self.check_gathered_tensors(tp_rank, 2, tensor_gather, tp_weights_assert)\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 1))\n\n            def dp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertEqual(tensor1, tensor2)\n                else:\n                    self.assertNotEqual(tensor1, tensor2)\n            self.check_gathered_tensors(dp_rank, 2, tensor_gather, dp_weights_assert)",
            "@with_comms\n@skip_if_lt_x_gpu(4)\ndef test_model_init(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mesh = torch.arange(self.world_size).reshape(2, 2)\n    device_mesh = DeviceMesh(self.device_type, mesh)\n    tp_rank = device_mesh.get_coordinate()[0]\n    dp_rank = device_mesh.get_coordinate()[1]\n    for enable_distribute_flag in [False, True]:\n        model = MLPModule(device='meta')\n        model_tp = parallelize_module(model, device_mesh, {'net1': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d), 'net2': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d)})\n        torch.cuda.manual_seed(dp_rank)\n        random._rng_tracker.distribute_region_enabled = enable_distribute_flag\n        self.assertTrue(model_tp.net1.weight.is_meta)\n        model_tp.to_empty(device=self.device_type)\n        model_tp.reset_parameters()\n        for dtensor in [model_tp.net1.weight, model_tp.net2.weight]:\n            _1d_mesh = dtensor.device_mesh\n            assert _1d_mesh.ndim == 1\n            tensor_local = dtensor.to_local()\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 0))\n            self.assertEqual(_1d_mesh.get_coordinate()[0], tp_rank)\n\n            def tp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertNotEqual(tensor1, tensor2)\n                else:\n                    self.assertEqual(tensor1, tensor2)\n            self.check_gathered_tensors(tp_rank, 2, tensor_gather, tp_weights_assert)\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 1))\n\n            def dp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertEqual(tensor1, tensor2)\n                else:\n                    self.assertNotEqual(tensor1, tensor2)\n            self.check_gathered_tensors(dp_rank, 2, tensor_gather, dp_weights_assert)",
            "@with_comms\n@skip_if_lt_x_gpu(4)\ndef test_model_init(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mesh = torch.arange(self.world_size).reshape(2, 2)\n    device_mesh = DeviceMesh(self.device_type, mesh)\n    tp_rank = device_mesh.get_coordinate()[0]\n    dp_rank = device_mesh.get_coordinate()[1]\n    for enable_distribute_flag in [False, True]:\n        model = MLPModule(device='meta')\n        model_tp = parallelize_module(model, device_mesh, {'net1': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d), 'net2': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d)})\n        torch.cuda.manual_seed(dp_rank)\n        random._rng_tracker.distribute_region_enabled = enable_distribute_flag\n        self.assertTrue(model_tp.net1.weight.is_meta)\n        model_tp.to_empty(device=self.device_type)\n        model_tp.reset_parameters()\n        for dtensor in [model_tp.net1.weight, model_tp.net2.weight]:\n            _1d_mesh = dtensor.device_mesh\n            assert _1d_mesh.ndim == 1\n            tensor_local = dtensor.to_local()\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 0))\n            self.assertEqual(_1d_mesh.get_coordinate()[0], tp_rank)\n\n            def tp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertNotEqual(tensor1, tensor2)\n                else:\n                    self.assertEqual(tensor1, tensor2)\n            self.check_gathered_tensors(tp_rank, 2, tensor_gather, tp_weights_assert)\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 1))\n\n            def dp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertEqual(tensor1, tensor2)\n                else:\n                    self.assertNotEqual(tensor1, tensor2)\n            self.check_gathered_tensors(dp_rank, 2, tensor_gather, dp_weights_assert)",
            "@with_comms\n@skip_if_lt_x_gpu(4)\ndef test_model_init(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mesh = torch.arange(self.world_size).reshape(2, 2)\n    device_mesh = DeviceMesh(self.device_type, mesh)\n    tp_rank = device_mesh.get_coordinate()[0]\n    dp_rank = device_mesh.get_coordinate()[1]\n    for enable_distribute_flag in [False, True]:\n        model = MLPModule(device='meta')\n        model_tp = parallelize_module(model, device_mesh, {'net1': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d), 'net2': ColwiseParallel(make_input_replicate_1d, make_output_replicate_1d)})\n        torch.cuda.manual_seed(dp_rank)\n        random._rng_tracker.distribute_region_enabled = enable_distribute_flag\n        self.assertTrue(model_tp.net1.weight.is_meta)\n        model_tp.to_empty(device=self.device_type)\n        model_tp.reset_parameters()\n        for dtensor in [model_tp.net1.weight, model_tp.net2.weight]:\n            _1d_mesh = dtensor.device_mesh\n            assert _1d_mesh.ndim == 1\n            tensor_local = dtensor.to_local()\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 0))\n            self.assertEqual(_1d_mesh.get_coordinate()[0], tp_rank)\n\n            def tp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertNotEqual(tensor1, tensor2)\n                else:\n                    self.assertEqual(tensor1, tensor2)\n            self.check_gathered_tensors(tp_rank, 2, tensor_gather, tp_weights_assert)\n            tensor_gather = funcol.all_gather_tensor(tensor_local, gather_dim=0, group=(_1d_mesh, 1))\n\n            def dp_weights_assert(tensor1, tensor2):\n                if enable_distribute_flag:\n                    self.assertEqual(tensor1, tensor2)\n                else:\n                    self.assertNotEqual(tensor1, tensor2)\n            self.check_gathered_tensors(dp_rank, 2, tensor_gather, dp_weights_assert)"
        ]
    }
]