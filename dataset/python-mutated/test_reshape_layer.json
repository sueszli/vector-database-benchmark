[
    {
        "func_name": "pytest_generate_tests",
        "original": "def pytest_generate_tests(metafunc):\n    if metafunc.config.option.all:\n        bsz_rng = [32, 64]\n    else:\n        bsz_rng = [32]\n    if 'fargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            nin_rng = [10, 30]\n            nout_rng = [5, 10]\n        else:\n            nin_rng = [30]\n            nout_rng = [10]\n        fargs = itt.product(nin_rng, nout_rng, bsz_rng)\n        metafunc.parametrize('fargs', fargs)",
        "mutated": [
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n    if metafunc.config.option.all:\n        bsz_rng = [32, 64]\n    else:\n        bsz_rng = [32]\n    if 'fargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            nin_rng = [10, 30]\n            nout_rng = [5, 10]\n        else:\n            nin_rng = [30]\n            nout_rng = [10]\n        fargs = itt.product(nin_rng, nout_rng, bsz_rng)\n        metafunc.parametrize('fargs', fargs)",
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if metafunc.config.option.all:\n        bsz_rng = [32, 64]\n    else:\n        bsz_rng = [32]\n    if 'fargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            nin_rng = [10, 30]\n            nout_rng = [5, 10]\n        else:\n            nin_rng = [30]\n            nout_rng = [10]\n        fargs = itt.product(nin_rng, nout_rng, bsz_rng)\n        metafunc.parametrize('fargs', fargs)",
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if metafunc.config.option.all:\n        bsz_rng = [32, 64]\n    else:\n        bsz_rng = [32]\n    if 'fargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            nin_rng = [10, 30]\n            nout_rng = [5, 10]\n        else:\n            nin_rng = [30]\n            nout_rng = [10]\n        fargs = itt.product(nin_rng, nout_rng, bsz_rng)\n        metafunc.parametrize('fargs', fargs)",
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if metafunc.config.option.all:\n        bsz_rng = [32, 64]\n    else:\n        bsz_rng = [32]\n    if 'fargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            nin_rng = [10, 30]\n            nout_rng = [5, 10]\n        else:\n            nin_rng = [30]\n            nout_rng = [10]\n        fargs = itt.product(nin_rng, nout_rng, bsz_rng)\n        metafunc.parametrize('fargs', fargs)",
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if metafunc.config.option.all:\n        bsz_rng = [32, 64]\n    else:\n        bsz_rng = [32]\n    if 'fargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            nin_rng = [10, 30]\n            nout_rng = [5, 10]\n        else:\n            nin_rng = [30]\n            nout_rng = [10]\n        fargs = itt.product(nin_rng, nout_rng, bsz_rng)\n        metafunc.parametrize('fargs', fargs)"
        ]
    },
    {
        "func_name": "test_reshape_configure",
        "original": "def test_reshape_configure(backend_default):\n    \"\"\"\n    test cases:\n    - reshape with 0\n    - reshape with -1\n    - reshape with collapsing dimensions\n    - reshape with expanding dimensions\n    \"\"\"\n    bsz = backend_default.bsz\n    reshape_0 = Reshape((0, 10, 10))\n    reshape_0.configure((10, 2, 50))\n    assert reshape_0.out_shape == (10, 10, 10)\n    reshape_1 = Reshape((10, 25, -1))\n    reshape_1.configure((10, 2, 50))\n    assert reshape_1.out_shape == (10, 25, 4)\n    reshape_2 = Reshape((5, -1))\n    reshape_2.configure((10, 2, 25))\n    assert reshape_2.out_shape == (5, 100)\n    assert reshape_2.out_shape_t == (5, 100 * bsz)\n    reshape_3 = Reshape((5, -1, 5))\n    reshape_3.configure((10, 25))\n    assert reshape_3.out_shape == (5, 10, 5)",
        "mutated": [
            "def test_reshape_configure(backend_default):\n    if False:\n        i = 10\n    '\\n    test cases:\\n    - reshape with 0\\n    - reshape with -1\\n    - reshape with collapsing dimensions\\n    - reshape with expanding dimensions\\n    '\n    bsz = backend_default.bsz\n    reshape_0 = Reshape((0, 10, 10))\n    reshape_0.configure((10, 2, 50))\n    assert reshape_0.out_shape == (10, 10, 10)\n    reshape_1 = Reshape((10, 25, -1))\n    reshape_1.configure((10, 2, 50))\n    assert reshape_1.out_shape == (10, 25, 4)\n    reshape_2 = Reshape((5, -1))\n    reshape_2.configure((10, 2, 25))\n    assert reshape_2.out_shape == (5, 100)\n    assert reshape_2.out_shape_t == (5, 100 * bsz)\n    reshape_3 = Reshape((5, -1, 5))\n    reshape_3.configure((10, 25))\n    assert reshape_3.out_shape == (5, 10, 5)",
            "def test_reshape_configure(backend_default):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    test cases:\\n    - reshape with 0\\n    - reshape with -1\\n    - reshape with collapsing dimensions\\n    - reshape with expanding dimensions\\n    '\n    bsz = backend_default.bsz\n    reshape_0 = Reshape((0, 10, 10))\n    reshape_0.configure((10, 2, 50))\n    assert reshape_0.out_shape == (10, 10, 10)\n    reshape_1 = Reshape((10, 25, -1))\n    reshape_1.configure((10, 2, 50))\n    assert reshape_1.out_shape == (10, 25, 4)\n    reshape_2 = Reshape((5, -1))\n    reshape_2.configure((10, 2, 25))\n    assert reshape_2.out_shape == (5, 100)\n    assert reshape_2.out_shape_t == (5, 100 * bsz)\n    reshape_3 = Reshape((5, -1, 5))\n    reshape_3.configure((10, 25))\n    assert reshape_3.out_shape == (5, 10, 5)",
            "def test_reshape_configure(backend_default):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    test cases:\\n    - reshape with 0\\n    - reshape with -1\\n    - reshape with collapsing dimensions\\n    - reshape with expanding dimensions\\n    '\n    bsz = backend_default.bsz\n    reshape_0 = Reshape((0, 10, 10))\n    reshape_0.configure((10, 2, 50))\n    assert reshape_0.out_shape == (10, 10, 10)\n    reshape_1 = Reshape((10, 25, -1))\n    reshape_1.configure((10, 2, 50))\n    assert reshape_1.out_shape == (10, 25, 4)\n    reshape_2 = Reshape((5, -1))\n    reshape_2.configure((10, 2, 25))\n    assert reshape_2.out_shape == (5, 100)\n    assert reshape_2.out_shape_t == (5, 100 * bsz)\n    reshape_3 = Reshape((5, -1, 5))\n    reshape_3.configure((10, 25))\n    assert reshape_3.out_shape == (5, 10, 5)",
            "def test_reshape_configure(backend_default):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    test cases:\\n    - reshape with 0\\n    - reshape with -1\\n    - reshape with collapsing dimensions\\n    - reshape with expanding dimensions\\n    '\n    bsz = backend_default.bsz\n    reshape_0 = Reshape((0, 10, 10))\n    reshape_0.configure((10, 2, 50))\n    assert reshape_0.out_shape == (10, 10, 10)\n    reshape_1 = Reshape((10, 25, -1))\n    reshape_1.configure((10, 2, 50))\n    assert reshape_1.out_shape == (10, 25, 4)\n    reshape_2 = Reshape((5, -1))\n    reshape_2.configure((10, 2, 25))\n    assert reshape_2.out_shape == (5, 100)\n    assert reshape_2.out_shape_t == (5, 100 * bsz)\n    reshape_3 = Reshape((5, -1, 5))\n    reshape_3.configure((10, 25))\n    assert reshape_3.out_shape == (5, 10, 5)",
            "def test_reshape_configure(backend_default):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    test cases:\\n    - reshape with 0\\n    - reshape with -1\\n    - reshape with collapsing dimensions\\n    - reshape with expanding dimensions\\n    '\n    bsz = backend_default.bsz\n    reshape_0 = Reshape((0, 10, 10))\n    reshape_0.configure((10, 2, 50))\n    assert reshape_0.out_shape == (10, 10, 10)\n    reshape_1 = Reshape((10, 25, -1))\n    reshape_1.configure((10, 2, 50))\n    assert reshape_1.out_shape == (10, 25, 4)\n    reshape_2 = Reshape((5, -1))\n    reshape_2.configure((10, 2, 25))\n    assert reshape_2.out_shape == (5, 100)\n    assert reshape_2.out_shape_t == (5, 100 * bsz)\n    reshape_3 = Reshape((5, -1, 5))\n    reshape_3.configure((10, 25))\n    assert reshape_3.out_shape == (5, 10, 5)"
        ]
    },
    {
        "func_name": "test_reshape_layer_model",
        "original": "def test_reshape_layer_model(backend_default, fargs):\n    \"\"\"\n    test cases:\n    - conv before RNNs\n    - conv after RNNs\n    - conv after LUT\n    \"\"\"\n    np.random.seed(seed=0)\n    (nin, nout, bsz) = fargs\n    be = backend_default\n    be.bsz = bsz\n    input_size = (nin, be.bsz)\n    init = Uniform(-0.1, 0.1)\n    g_uni = GlorotUniform()\n    inp_np = np.random.rand(nin, be.bsz)\n    delta_np = np.random.rand(nout, be.bsz)\n    inp = be.array(inp_np)\n    delta = be.array(delta_np)\n    conv_lut_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Reshape(reshape=(4, 100, -1)), Conv((3, 3, 16), init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), RecurrentSum(), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_lut_2 = [LookupTable(vocab_size=1000, embedding_dim=400, init=init), Reshape(reshape=(4, 50, -1)), Conv((3, 3, 16), init=init), Pooling(2, strides=2), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    conv_rnn_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), Reshape(reshape=(4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_rnn_2 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Recurrent(64, g_uni, activation=Tanh(), reset_cells=True), Reshape(reshape=(4, -1, 32)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    lut_sum_1 = [LookupTable(vocab_size=1000, embedding_dim=128, init=init), RecurrentSum(), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    lut_birnn_1 = [LookupTable(vocab_size=1000, embedding_dim=200, init=init), DeepBiRNN(32, init=GlorotUniform(), batch_norm=True, activation=Tanh(), reset_cells=True, depth=1), Reshape((4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    layers_test = [conv_lut_1, conv_lut_2, conv_rnn_1, conv_rnn_2, lut_sum_1, lut_birnn_1]\n    for lg in layers_test:\n        model = Model(layers=lg)\n        cost = GeneralizedCost(costfunc=CrossEntropyBinary())\n        model.initialize(input_size, cost)\n        model.fprop(inp)\n        model.bprop(delta)",
        "mutated": [
            "def test_reshape_layer_model(backend_default, fargs):\n    if False:\n        i = 10\n    '\\n    test cases:\\n    - conv before RNNs\\n    - conv after RNNs\\n    - conv after LUT\\n    '\n    np.random.seed(seed=0)\n    (nin, nout, bsz) = fargs\n    be = backend_default\n    be.bsz = bsz\n    input_size = (nin, be.bsz)\n    init = Uniform(-0.1, 0.1)\n    g_uni = GlorotUniform()\n    inp_np = np.random.rand(nin, be.bsz)\n    delta_np = np.random.rand(nout, be.bsz)\n    inp = be.array(inp_np)\n    delta = be.array(delta_np)\n    conv_lut_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Reshape(reshape=(4, 100, -1)), Conv((3, 3, 16), init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), RecurrentSum(), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_lut_2 = [LookupTable(vocab_size=1000, embedding_dim=400, init=init), Reshape(reshape=(4, 50, -1)), Conv((3, 3, 16), init=init), Pooling(2, strides=2), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    conv_rnn_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), Reshape(reshape=(4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_rnn_2 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Recurrent(64, g_uni, activation=Tanh(), reset_cells=True), Reshape(reshape=(4, -1, 32)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    lut_sum_1 = [LookupTable(vocab_size=1000, embedding_dim=128, init=init), RecurrentSum(), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    lut_birnn_1 = [LookupTable(vocab_size=1000, embedding_dim=200, init=init), DeepBiRNN(32, init=GlorotUniform(), batch_norm=True, activation=Tanh(), reset_cells=True, depth=1), Reshape((4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    layers_test = [conv_lut_1, conv_lut_2, conv_rnn_1, conv_rnn_2, lut_sum_1, lut_birnn_1]\n    for lg in layers_test:\n        model = Model(layers=lg)\n        cost = GeneralizedCost(costfunc=CrossEntropyBinary())\n        model.initialize(input_size, cost)\n        model.fprop(inp)\n        model.bprop(delta)",
            "def test_reshape_layer_model(backend_default, fargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    test cases:\\n    - conv before RNNs\\n    - conv after RNNs\\n    - conv after LUT\\n    '\n    np.random.seed(seed=0)\n    (nin, nout, bsz) = fargs\n    be = backend_default\n    be.bsz = bsz\n    input_size = (nin, be.bsz)\n    init = Uniform(-0.1, 0.1)\n    g_uni = GlorotUniform()\n    inp_np = np.random.rand(nin, be.bsz)\n    delta_np = np.random.rand(nout, be.bsz)\n    inp = be.array(inp_np)\n    delta = be.array(delta_np)\n    conv_lut_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Reshape(reshape=(4, 100, -1)), Conv((3, 3, 16), init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), RecurrentSum(), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_lut_2 = [LookupTable(vocab_size=1000, embedding_dim=400, init=init), Reshape(reshape=(4, 50, -1)), Conv((3, 3, 16), init=init), Pooling(2, strides=2), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    conv_rnn_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), Reshape(reshape=(4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_rnn_2 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Recurrent(64, g_uni, activation=Tanh(), reset_cells=True), Reshape(reshape=(4, -1, 32)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    lut_sum_1 = [LookupTable(vocab_size=1000, embedding_dim=128, init=init), RecurrentSum(), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    lut_birnn_1 = [LookupTable(vocab_size=1000, embedding_dim=200, init=init), DeepBiRNN(32, init=GlorotUniform(), batch_norm=True, activation=Tanh(), reset_cells=True, depth=1), Reshape((4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    layers_test = [conv_lut_1, conv_lut_2, conv_rnn_1, conv_rnn_2, lut_sum_1, lut_birnn_1]\n    for lg in layers_test:\n        model = Model(layers=lg)\n        cost = GeneralizedCost(costfunc=CrossEntropyBinary())\n        model.initialize(input_size, cost)\n        model.fprop(inp)\n        model.bprop(delta)",
            "def test_reshape_layer_model(backend_default, fargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    test cases:\\n    - conv before RNNs\\n    - conv after RNNs\\n    - conv after LUT\\n    '\n    np.random.seed(seed=0)\n    (nin, nout, bsz) = fargs\n    be = backend_default\n    be.bsz = bsz\n    input_size = (nin, be.bsz)\n    init = Uniform(-0.1, 0.1)\n    g_uni = GlorotUniform()\n    inp_np = np.random.rand(nin, be.bsz)\n    delta_np = np.random.rand(nout, be.bsz)\n    inp = be.array(inp_np)\n    delta = be.array(delta_np)\n    conv_lut_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Reshape(reshape=(4, 100, -1)), Conv((3, 3, 16), init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), RecurrentSum(), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_lut_2 = [LookupTable(vocab_size=1000, embedding_dim=400, init=init), Reshape(reshape=(4, 50, -1)), Conv((3, 3, 16), init=init), Pooling(2, strides=2), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    conv_rnn_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), Reshape(reshape=(4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_rnn_2 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Recurrent(64, g_uni, activation=Tanh(), reset_cells=True), Reshape(reshape=(4, -1, 32)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    lut_sum_1 = [LookupTable(vocab_size=1000, embedding_dim=128, init=init), RecurrentSum(), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    lut_birnn_1 = [LookupTable(vocab_size=1000, embedding_dim=200, init=init), DeepBiRNN(32, init=GlorotUniform(), batch_norm=True, activation=Tanh(), reset_cells=True, depth=1), Reshape((4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    layers_test = [conv_lut_1, conv_lut_2, conv_rnn_1, conv_rnn_2, lut_sum_1, lut_birnn_1]\n    for lg in layers_test:\n        model = Model(layers=lg)\n        cost = GeneralizedCost(costfunc=CrossEntropyBinary())\n        model.initialize(input_size, cost)\n        model.fprop(inp)\n        model.bprop(delta)",
            "def test_reshape_layer_model(backend_default, fargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    test cases:\\n    - conv before RNNs\\n    - conv after RNNs\\n    - conv after LUT\\n    '\n    np.random.seed(seed=0)\n    (nin, nout, bsz) = fargs\n    be = backend_default\n    be.bsz = bsz\n    input_size = (nin, be.bsz)\n    init = Uniform(-0.1, 0.1)\n    g_uni = GlorotUniform()\n    inp_np = np.random.rand(nin, be.bsz)\n    delta_np = np.random.rand(nout, be.bsz)\n    inp = be.array(inp_np)\n    delta = be.array(delta_np)\n    conv_lut_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Reshape(reshape=(4, 100, -1)), Conv((3, 3, 16), init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), RecurrentSum(), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_lut_2 = [LookupTable(vocab_size=1000, embedding_dim=400, init=init), Reshape(reshape=(4, 50, -1)), Conv((3, 3, 16), init=init), Pooling(2, strides=2), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    conv_rnn_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), Reshape(reshape=(4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_rnn_2 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Recurrent(64, g_uni, activation=Tanh(), reset_cells=True), Reshape(reshape=(4, -1, 32)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    lut_sum_1 = [LookupTable(vocab_size=1000, embedding_dim=128, init=init), RecurrentSum(), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    lut_birnn_1 = [LookupTable(vocab_size=1000, embedding_dim=200, init=init), DeepBiRNN(32, init=GlorotUniform(), batch_norm=True, activation=Tanh(), reset_cells=True, depth=1), Reshape((4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    layers_test = [conv_lut_1, conv_lut_2, conv_rnn_1, conv_rnn_2, lut_sum_1, lut_birnn_1]\n    for lg in layers_test:\n        model = Model(layers=lg)\n        cost = GeneralizedCost(costfunc=CrossEntropyBinary())\n        model.initialize(input_size, cost)\n        model.fprop(inp)\n        model.bprop(delta)",
            "def test_reshape_layer_model(backend_default, fargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    test cases:\\n    - conv before RNNs\\n    - conv after RNNs\\n    - conv after LUT\\n    '\n    np.random.seed(seed=0)\n    (nin, nout, bsz) = fargs\n    be = backend_default\n    be.bsz = bsz\n    input_size = (nin, be.bsz)\n    init = Uniform(-0.1, 0.1)\n    g_uni = GlorotUniform()\n    inp_np = np.random.rand(nin, be.bsz)\n    delta_np = np.random.rand(nout, be.bsz)\n    inp = be.array(inp_np)\n    delta = be.array(delta_np)\n    conv_lut_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Reshape(reshape=(4, 100, -1)), Conv((3, 3, 16), init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), RecurrentSum(), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_lut_2 = [LookupTable(vocab_size=1000, embedding_dim=400, init=init), Reshape(reshape=(4, 50, -1)), Conv((3, 3, 16), init=init), Pooling(2, strides=2), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    conv_rnn_1 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), LSTM(64, g_uni, activation=Tanh(), gate_activation=Logistic(), reset_cells=True), Reshape(reshape=(4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    conv_rnn_2 = [LookupTable(vocab_size=2000, embedding_dim=400, init=init), Recurrent(64, g_uni, activation=Tanh(), reset_cells=True), Reshape(reshape=(4, -1, 32)), Conv((3, 3, 16), init=init), Affine(nout, init, bias=init, activation=Softmax())]\n    lut_sum_1 = [LookupTable(vocab_size=1000, embedding_dim=128, init=init), RecurrentSum(), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    lut_birnn_1 = [LookupTable(vocab_size=1000, embedding_dim=200, init=init), DeepBiRNN(32, init=GlorotUniform(), batch_norm=True, activation=Tanh(), reset_cells=True, depth=1), Reshape((4, 32, -1)), Conv((3, 3, 16), init=init), Affine(nout=nout, init=init, bias=init, activation=Softmax())]\n    layers_test = [conv_lut_1, conv_lut_2, conv_rnn_1, conv_rnn_2, lut_sum_1, lut_birnn_1]\n    for lg in layers_test:\n        model = Model(layers=lg)\n        cost = GeneralizedCost(costfunc=CrossEntropyBinary())\n        model.initialize(input_size, cost)\n        model.fprop(inp)\n        model.bprop(delta)"
        ]
    }
]