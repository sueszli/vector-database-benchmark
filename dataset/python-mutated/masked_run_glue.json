[
    {
        "func_name": "set_seed",
        "original": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
        "mutated": [
            "def set_seed(args):\n    if False:\n        i = 10\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
            "def set_seed(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
            "def set_seed(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
            "def set_seed(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
            "def set_seed(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)"
        ]
    },
    {
        "func_name": "schedule_threshold",
        "original": "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
        "mutated": [
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)"
        ]
    },
    {
        "func_name": "regularization",
        "original": "def regularization(model: nn.Module, mode: str):\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
        "mutated": [
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(args, train_dataset, model, tokenizer, teacher=None):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 0\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            global_step = int(args.model_name_or_path.split('-')[-1].split('/')[0])\n        except ValueError:\n            global_step = 0\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n        logger.info('  Continuing training from epoch %d', epochs_trained)\n        logger.info('  Continuing training from global step %d', global_step)\n        logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n            if args.model_type != 'distilbert':\n                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, logits_stu) = outputs\n            if teacher is not None:\n                if 'token_type_ids' not in inputs:\n                    inputs['token_type_ids'] = None if args.teacher_type == 'xlm' else batch[2]\n                with torch.no_grad():\n                    (logits_tea,) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_logits = nn.functional.kl_div(input=nn.functional.log_softmax(logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0 or (len(epoch_iterator) <= args.gradient_accumulation_steps and step + 1 == len(epoch_iterator)):\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    logs = {}\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            eval_key = 'eval_{}'.format(key)\n                            logs[eval_key] = value\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()\n                    logs['learning_rate'] = learning_rate_scalar[0]\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            logs[f'learning_rate/{idx + 1}'] = lr\n                    logs['loss'] = loss_scalar\n                    if teacher is not None:\n                        logs['loss/distil'] = loss_logits.item()\n                    if args.regularization is not None:\n                        logs['loss/regularization'] = regu_.item()\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            logs['loss/instant_ce'] = (loss.item() - regu_lambda * logs['loss/regularization'] - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        elif teacher is not None:\n                            logs['loss/instant_ce'] = (loss.item() - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        else:\n                            logs['loss/instant_ce'] = loss.item() - regu_lambda * logs['loss/regularization']\n                    logging_loss = tr_loss\n                    for (key, value) in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{'step': global_step}}))\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
        "mutated": [
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 0\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            global_step = int(args.model_name_or_path.split('-')[-1].split('/')[0])\n        except ValueError:\n            global_step = 0\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n        logger.info('  Continuing training from epoch %d', epochs_trained)\n        logger.info('  Continuing training from global step %d', global_step)\n        logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n            if args.model_type != 'distilbert':\n                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, logits_stu) = outputs\n            if teacher is not None:\n                if 'token_type_ids' not in inputs:\n                    inputs['token_type_ids'] = None if args.teacher_type == 'xlm' else batch[2]\n                with torch.no_grad():\n                    (logits_tea,) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_logits = nn.functional.kl_div(input=nn.functional.log_softmax(logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0 or (len(epoch_iterator) <= args.gradient_accumulation_steps and step + 1 == len(epoch_iterator)):\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    logs = {}\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            eval_key = 'eval_{}'.format(key)\n                            logs[eval_key] = value\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()\n                    logs['learning_rate'] = learning_rate_scalar[0]\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            logs[f'learning_rate/{idx + 1}'] = lr\n                    logs['loss'] = loss_scalar\n                    if teacher is not None:\n                        logs['loss/distil'] = loss_logits.item()\n                    if args.regularization is not None:\n                        logs['loss/regularization'] = regu_.item()\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            logs['loss/instant_ce'] = (loss.item() - regu_lambda * logs['loss/regularization'] - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        elif teacher is not None:\n                            logs['loss/instant_ce'] = (loss.item() - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        else:\n                            logs['loss/instant_ce'] = loss.item() - regu_lambda * logs['loss/regularization']\n                    logging_loss = tr_loss\n                    for (key, value) in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{'step': global_step}}))\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 0\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            global_step = int(args.model_name_or_path.split('-')[-1].split('/')[0])\n        except ValueError:\n            global_step = 0\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n        logger.info('  Continuing training from epoch %d', epochs_trained)\n        logger.info('  Continuing training from global step %d', global_step)\n        logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n            if args.model_type != 'distilbert':\n                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, logits_stu) = outputs\n            if teacher is not None:\n                if 'token_type_ids' not in inputs:\n                    inputs['token_type_ids'] = None if args.teacher_type == 'xlm' else batch[2]\n                with torch.no_grad():\n                    (logits_tea,) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_logits = nn.functional.kl_div(input=nn.functional.log_softmax(logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0 or (len(epoch_iterator) <= args.gradient_accumulation_steps and step + 1 == len(epoch_iterator)):\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    logs = {}\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            eval_key = 'eval_{}'.format(key)\n                            logs[eval_key] = value\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()\n                    logs['learning_rate'] = learning_rate_scalar[0]\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            logs[f'learning_rate/{idx + 1}'] = lr\n                    logs['loss'] = loss_scalar\n                    if teacher is not None:\n                        logs['loss/distil'] = loss_logits.item()\n                    if args.regularization is not None:\n                        logs['loss/regularization'] = regu_.item()\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            logs['loss/instant_ce'] = (loss.item() - regu_lambda * logs['loss/regularization'] - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        elif teacher is not None:\n                            logs['loss/instant_ce'] = (loss.item() - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        else:\n                            logs['loss/instant_ce'] = loss.item() - regu_lambda * logs['loss/regularization']\n                    logging_loss = tr_loss\n                    for (key, value) in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{'step': global_step}}))\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 0\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            global_step = int(args.model_name_or_path.split('-')[-1].split('/')[0])\n        except ValueError:\n            global_step = 0\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n        logger.info('  Continuing training from epoch %d', epochs_trained)\n        logger.info('  Continuing training from global step %d', global_step)\n        logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n            if args.model_type != 'distilbert':\n                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, logits_stu) = outputs\n            if teacher is not None:\n                if 'token_type_ids' not in inputs:\n                    inputs['token_type_ids'] = None if args.teacher_type == 'xlm' else batch[2]\n                with torch.no_grad():\n                    (logits_tea,) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_logits = nn.functional.kl_div(input=nn.functional.log_softmax(logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0 or (len(epoch_iterator) <= args.gradient_accumulation_steps and step + 1 == len(epoch_iterator)):\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    logs = {}\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            eval_key = 'eval_{}'.format(key)\n                            logs[eval_key] = value\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()\n                    logs['learning_rate'] = learning_rate_scalar[0]\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            logs[f'learning_rate/{idx + 1}'] = lr\n                    logs['loss'] = loss_scalar\n                    if teacher is not None:\n                        logs['loss/distil'] = loss_logits.item()\n                    if args.regularization is not None:\n                        logs['loss/regularization'] = regu_.item()\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            logs['loss/instant_ce'] = (loss.item() - regu_lambda * logs['loss/regularization'] - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        elif teacher is not None:\n                            logs['loss/instant_ce'] = (loss.item() - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        else:\n                            logs['loss/instant_ce'] = loss.item() - regu_lambda * logs['loss/regularization']\n                    logging_loss = tr_loss\n                    for (key, value) in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{'step': global_step}}))\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 0\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            global_step = int(args.model_name_or_path.split('-')[-1].split('/')[0])\n        except ValueError:\n            global_step = 0\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n        logger.info('  Continuing training from epoch %d', epochs_trained)\n        logger.info('  Continuing training from global step %d', global_step)\n        logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n            if args.model_type != 'distilbert':\n                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, logits_stu) = outputs\n            if teacher is not None:\n                if 'token_type_ids' not in inputs:\n                    inputs['token_type_ids'] = None if args.teacher_type == 'xlm' else batch[2]\n                with torch.no_grad():\n                    (logits_tea,) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_logits = nn.functional.kl_div(input=nn.functional.log_softmax(logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0 or (len(epoch_iterator) <= args.gradient_accumulation_steps and step + 1 == len(epoch_iterator)):\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    logs = {}\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            eval_key = 'eval_{}'.format(key)\n                            logs[eval_key] = value\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()\n                    logs['learning_rate'] = learning_rate_scalar[0]\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            logs[f'learning_rate/{idx + 1}'] = lr\n                    logs['loss'] = loss_scalar\n                    if teacher is not None:\n                        logs['loss/distil'] = loss_logits.item()\n                    if args.regularization is not None:\n                        logs['loss/regularization'] = regu_.item()\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            logs['loss/instant_ce'] = (loss.item() - regu_lambda * logs['loss/regularization'] - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        elif teacher is not None:\n                            logs['loss/instant_ce'] = (loss.item() - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        else:\n                            logs['loss/instant_ce'] = loss.item() - regu_lambda * logs['loss/regularization']\n                    logging_loss = tr_loss\n                    for (key, value) in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{'step': global_step}}))\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 0\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            global_step = int(args.model_name_or_path.split('-')[-1].split('/')[0])\n        except ValueError:\n            global_step = 0\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n        logger.info('  Continuing training from epoch %d', epochs_trained)\n        logger.info('  Continuing training from global step %d', global_step)\n        logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n            if args.model_type != 'distilbert':\n                inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, logits_stu) = outputs\n            if teacher is not None:\n                if 'token_type_ids' not in inputs:\n                    inputs['token_type_ids'] = None if args.teacher_type == 'xlm' else batch[2]\n                with torch.no_grad():\n                    (logits_tea,) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_logits = nn.functional.kl_div(input=nn.functional.log_softmax(logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0 or (len(epoch_iterator) <= args.gradient_accumulation_steps and step + 1 == len(epoch_iterator)):\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    logs = {}\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            eval_key = 'eval_{}'.format(key)\n                            logs[eval_key] = value\n                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n                    learning_rate_scalar = scheduler.get_lr()\n                    logs['learning_rate'] = learning_rate_scalar[0]\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            logs[f'learning_rate/{idx + 1}'] = lr\n                    logs['loss'] = loss_scalar\n                    if teacher is not None:\n                        logs['loss/distil'] = loss_logits.item()\n                    if args.regularization is not None:\n                        logs['loss/regularization'] = regu_.item()\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            logs['loss/instant_ce'] = (loss.item() - regu_lambda * logs['loss/regularization'] - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        elif teacher is not None:\n                            logs['loss/instant_ce'] = (loss.item() - args.alpha_distil * logs['loss/distil']) / args.alpha_ce\n                        else:\n                            logs['loss/instant_ce'] = loss.item() - regu_lambda * logs['loss/regularization']\n                    logging_loss = tr_loss\n                    for (key, value) in logs.items():\n                        tb_writer.add_scalar(key, value, global_step)\n                    print(json.dumps({**logs, **{'step': global_step}}))\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)"
        ]
    },
    {
        "func_name": "evaluate",
        "original": "def evaluate(args, model, tokenizer, prefix=''):\n    eval_task_names = ('mnli', 'mnli-mm') if args.task_name == 'mnli' else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '/MM') if args.task_name == 'mnli' else (args.output_dir,)\n    results = {}\n    for (eval_task, eval_output_dir) in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n        if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n            model = nn.DataParallel(model)\n        logger.info('***** Running evaluation {} *****'.format(prefix))\n        logger.info('  Num examples = %d', len(eval_dataset))\n        logger.info('  Batch size = %d', args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        if args.global_topk:\n            threshold_mem = None\n        for batch in tqdm(eval_dataloader, desc='Evaluating'):\n            model.eval()\n            batch = tuple((t.to(args.device) for t in batch))\n            with torch.no_grad():\n                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n                if 'masked' in args.model_type:\n                    inputs['threshold'] = args.final_threshold\n                    if args.global_topk:\n                        if threshold_mem is None:\n                            concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                            n = concat.numel()\n                            kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                            threshold_mem = concat.kthvalue(kth).values.item()\n                        inputs['threshold'] = threshold_mem\n                outputs = model(**inputs)\n                (tmp_eval_loss, logits) = outputs[:2]\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == 'classification':\n            from scipy.special import softmax\n            probs = softmax(preds, axis=-1)\n            entropy = np.exp((-probs * np.log(probs)).sum(axis=-1).mean())\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == 'regression':\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n        if entropy is not None:\n            result['eval_avg_entropy'] = entropy\n        output_eval_file = os.path.join(eval_output_dir, prefix, 'eval_results.txt')\n        with open(output_eval_file, 'w') as writer:\n            logger.info('***** Eval results {} *****'.format(prefix))\n            for key in sorted(result.keys()):\n                logger.info('  %s = %s', key, str(result[key]))\n                writer.write('%s = %s\\n' % (key, str(result[key])))\n    return results",
        "mutated": [
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n    eval_task_names = ('mnli', 'mnli-mm') if args.task_name == 'mnli' else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '/MM') if args.task_name == 'mnli' else (args.output_dir,)\n    results = {}\n    for (eval_task, eval_output_dir) in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n        if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n            model = nn.DataParallel(model)\n        logger.info('***** Running evaluation {} *****'.format(prefix))\n        logger.info('  Num examples = %d', len(eval_dataset))\n        logger.info('  Batch size = %d', args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        if args.global_topk:\n            threshold_mem = None\n        for batch in tqdm(eval_dataloader, desc='Evaluating'):\n            model.eval()\n            batch = tuple((t.to(args.device) for t in batch))\n            with torch.no_grad():\n                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n                if 'masked' in args.model_type:\n                    inputs['threshold'] = args.final_threshold\n                    if args.global_topk:\n                        if threshold_mem is None:\n                            concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                            n = concat.numel()\n                            kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                            threshold_mem = concat.kthvalue(kth).values.item()\n                        inputs['threshold'] = threshold_mem\n                outputs = model(**inputs)\n                (tmp_eval_loss, logits) = outputs[:2]\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == 'classification':\n            from scipy.special import softmax\n            probs = softmax(preds, axis=-1)\n            entropy = np.exp((-probs * np.log(probs)).sum(axis=-1).mean())\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == 'regression':\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n        if entropy is not None:\n            result['eval_avg_entropy'] = entropy\n        output_eval_file = os.path.join(eval_output_dir, prefix, 'eval_results.txt')\n        with open(output_eval_file, 'w') as writer:\n            logger.info('***** Eval results {} *****'.format(prefix))\n            for key in sorted(result.keys()):\n                logger.info('  %s = %s', key, str(result[key]))\n                writer.write('%s = %s\\n' % (key, str(result[key])))\n    return results",
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    eval_task_names = ('mnli', 'mnli-mm') if args.task_name == 'mnli' else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '/MM') if args.task_name == 'mnli' else (args.output_dir,)\n    results = {}\n    for (eval_task, eval_output_dir) in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n        if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n            model = nn.DataParallel(model)\n        logger.info('***** Running evaluation {} *****'.format(prefix))\n        logger.info('  Num examples = %d', len(eval_dataset))\n        logger.info('  Batch size = %d', args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        if args.global_topk:\n            threshold_mem = None\n        for batch in tqdm(eval_dataloader, desc='Evaluating'):\n            model.eval()\n            batch = tuple((t.to(args.device) for t in batch))\n            with torch.no_grad():\n                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n                if 'masked' in args.model_type:\n                    inputs['threshold'] = args.final_threshold\n                    if args.global_topk:\n                        if threshold_mem is None:\n                            concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                            n = concat.numel()\n                            kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                            threshold_mem = concat.kthvalue(kth).values.item()\n                        inputs['threshold'] = threshold_mem\n                outputs = model(**inputs)\n                (tmp_eval_loss, logits) = outputs[:2]\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == 'classification':\n            from scipy.special import softmax\n            probs = softmax(preds, axis=-1)\n            entropy = np.exp((-probs * np.log(probs)).sum(axis=-1).mean())\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == 'regression':\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n        if entropy is not None:\n            result['eval_avg_entropy'] = entropy\n        output_eval_file = os.path.join(eval_output_dir, prefix, 'eval_results.txt')\n        with open(output_eval_file, 'w') as writer:\n            logger.info('***** Eval results {} *****'.format(prefix))\n            for key in sorted(result.keys()):\n                logger.info('  %s = %s', key, str(result[key]))\n                writer.write('%s = %s\\n' % (key, str(result[key])))\n    return results",
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    eval_task_names = ('mnli', 'mnli-mm') if args.task_name == 'mnli' else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '/MM') if args.task_name == 'mnli' else (args.output_dir,)\n    results = {}\n    for (eval_task, eval_output_dir) in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n        if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n            model = nn.DataParallel(model)\n        logger.info('***** Running evaluation {} *****'.format(prefix))\n        logger.info('  Num examples = %d', len(eval_dataset))\n        logger.info('  Batch size = %d', args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        if args.global_topk:\n            threshold_mem = None\n        for batch in tqdm(eval_dataloader, desc='Evaluating'):\n            model.eval()\n            batch = tuple((t.to(args.device) for t in batch))\n            with torch.no_grad():\n                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n                if 'masked' in args.model_type:\n                    inputs['threshold'] = args.final_threshold\n                    if args.global_topk:\n                        if threshold_mem is None:\n                            concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                            n = concat.numel()\n                            kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                            threshold_mem = concat.kthvalue(kth).values.item()\n                        inputs['threshold'] = threshold_mem\n                outputs = model(**inputs)\n                (tmp_eval_loss, logits) = outputs[:2]\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == 'classification':\n            from scipy.special import softmax\n            probs = softmax(preds, axis=-1)\n            entropy = np.exp((-probs * np.log(probs)).sum(axis=-1).mean())\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == 'regression':\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n        if entropy is not None:\n            result['eval_avg_entropy'] = entropy\n        output_eval_file = os.path.join(eval_output_dir, prefix, 'eval_results.txt')\n        with open(output_eval_file, 'w') as writer:\n            logger.info('***** Eval results {} *****'.format(prefix))\n            for key in sorted(result.keys()):\n                logger.info('  %s = %s', key, str(result[key]))\n                writer.write('%s = %s\\n' % (key, str(result[key])))\n    return results",
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    eval_task_names = ('mnli', 'mnli-mm') if args.task_name == 'mnli' else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '/MM') if args.task_name == 'mnli' else (args.output_dir,)\n    results = {}\n    for (eval_task, eval_output_dir) in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n        if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n            model = nn.DataParallel(model)\n        logger.info('***** Running evaluation {} *****'.format(prefix))\n        logger.info('  Num examples = %d', len(eval_dataset))\n        logger.info('  Batch size = %d', args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        if args.global_topk:\n            threshold_mem = None\n        for batch in tqdm(eval_dataloader, desc='Evaluating'):\n            model.eval()\n            batch = tuple((t.to(args.device) for t in batch))\n            with torch.no_grad():\n                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n                if 'masked' in args.model_type:\n                    inputs['threshold'] = args.final_threshold\n                    if args.global_topk:\n                        if threshold_mem is None:\n                            concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                            n = concat.numel()\n                            kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                            threshold_mem = concat.kthvalue(kth).values.item()\n                        inputs['threshold'] = threshold_mem\n                outputs = model(**inputs)\n                (tmp_eval_loss, logits) = outputs[:2]\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == 'classification':\n            from scipy.special import softmax\n            probs = softmax(preds, axis=-1)\n            entropy = np.exp((-probs * np.log(probs)).sum(axis=-1).mean())\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == 'regression':\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n        if entropy is not None:\n            result['eval_avg_entropy'] = entropy\n        output_eval_file = os.path.join(eval_output_dir, prefix, 'eval_results.txt')\n        with open(output_eval_file, 'w') as writer:\n            logger.info('***** Eval results {} *****'.format(prefix))\n            for key in sorted(result.keys()):\n                logger.info('  %s = %s', key, str(result[key]))\n                writer.write('%s = %s\\n' % (key, str(result[key])))\n    return results",
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    eval_task_names = ('mnli', 'mnli-mm') if args.task_name == 'mnli' else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '/MM') if args.task_name == 'mnli' else (args.output_dir,)\n    results = {}\n    for (eval_task, eval_output_dir) in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n        if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n            model = nn.DataParallel(model)\n        logger.info('***** Running evaluation {} *****'.format(prefix))\n        logger.info('  Num examples = %d', len(eval_dataset))\n        logger.info('  Batch size = %d', args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        if args.global_topk:\n            threshold_mem = None\n        for batch in tqdm(eval_dataloader, desc='Evaluating'):\n            model.eval()\n            batch = tuple((t.to(args.device) for t in batch))\n            with torch.no_grad():\n                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'masked_bert', 'xlnet', 'albert'] else None\n                if 'masked' in args.model_type:\n                    inputs['threshold'] = args.final_threshold\n                    if args.global_topk:\n                        if threshold_mem is None:\n                            concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                            n = concat.numel()\n                            kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                            threshold_mem = concat.kthvalue(kth).values.item()\n                        inputs['threshold'] = threshold_mem\n                outputs = model(**inputs)\n                (tmp_eval_loss, logits) = outputs[:2]\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == 'classification':\n            from scipy.special import softmax\n            probs = softmax(preds, axis=-1)\n            entropy = np.exp((-probs * np.log(probs)).sum(axis=-1).mean())\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == 'regression':\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n        if entropy is not None:\n            result['eval_avg_entropy'] = entropy\n        output_eval_file = os.path.join(eval_output_dir, prefix, 'eval_results.txt')\n        with open(output_eval_file, 'w') as writer:\n            logger.info('***** Eval results {} *****'.format(prefix))\n            for key in sorted(result.keys()):\n                logger.info('  %s = %s', key, str(result[key]))\n                writer.write('%s = %s\\n' % (key, str(result[key])))\n    return results"
        ]
    },
    {
        "func_name": "load_and_cache_examples",
        "original": "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), str(task)))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info('Creating features from dataset file at %s', args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta', 'xlmroberta']:\n            (label_list[1], label_list[2]) = (label_list[2], label_list[1])\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=output_mode)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == 'classification':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == 'regression':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset",
        "mutated": [
            "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if False:\n        i = 10\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), str(task)))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info('Creating features from dataset file at %s', args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta', 'xlmroberta']:\n            (label_list[1], label_list[2]) = (label_list[2], label_list[1])\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=output_mode)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == 'classification':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == 'regression':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset",
            "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), str(task)))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info('Creating features from dataset file at %s', args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta', 'xlmroberta']:\n            (label_list[1], label_list[2]) = (label_list[2], label_list[1])\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=output_mode)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == 'classification':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == 'regression':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset",
            "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), str(task)))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info('Creating features from dataset file at %s', args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta', 'xlmroberta']:\n            (label_list[1], label_list[2]) = (label_list[2], label_list[1])\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=output_mode)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == 'classification':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == 'regression':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset",
            "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), str(task)))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info('Creating features from dataset file at %s', args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta', 'xlmroberta']:\n            (label_list[1], label_list[2]) = (label_list[2], label_list[1])\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=output_mode)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == 'classification':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == 'regression':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset",
            "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), str(task)))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info('Creating features from dataset file at %s', args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta', 'xlmroberta']:\n            (label_list[1], label_list[2]) = (label_list[2], label_list[1])\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=output_mode)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == 'classification':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == 'regression':\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--logging_steps', type=int, default=50, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=50, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Avoid using CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError(f'Output directory ({args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.')\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, do_lower_case=args.do_lower_case)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            result = {k + '_{}'.format(global_step): v for (k, v) in result.items()}\n            results.update(result)\n    return results",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--logging_steps', type=int, default=50, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=50, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Avoid using CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError(f'Output directory ({args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.')\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, do_lower_case=args.do_lower_case)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            result = {k + '_{}'.format(global_step): v for (k, v) in result.items()}\n            results.update(result)\n    return results",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--logging_steps', type=int, default=50, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=50, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Avoid using CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError(f'Output directory ({args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.')\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, do_lower_case=args.do_lower_case)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            result = {k + '_{}'.format(global_step): v for (k, v) in result.items()}\n            results.update(result)\n    return results",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--logging_steps', type=int, default=50, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=50, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Avoid using CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError(f'Output directory ({args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.')\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, do_lower_case=args.do_lower_case)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            result = {k + '_{}'.format(global_step): v for (k, v) in result.items()}\n            results.update(result)\n    return results",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--logging_steps', type=int, default=50, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=50, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Avoid using CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError(f'Output directory ({args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.')\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, do_lower_case=args.do_lower_case)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            result = {k + '_{}'.format(global_step): v for (k, v) in result.items()}\n            results.update(result)\n    return results",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--logging_steps', type=int, default=50, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=50, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Avoid using CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError(f'Output directory ({args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.')\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, do_lower_case=args.do_lower_case)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            result = {k + '_{}'.format(global_step): v for (k, v) in result.items()}\n            results.update(result)\n    return results"
        ]
    }
]