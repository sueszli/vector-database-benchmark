[
    {
        "func_name": "process_url",
        "original": "def process_url(url):\n    return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)",
        "mutated": [
            "def process_url(url):\n    if False:\n        i = 10\n    return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)",
            "def process_url(url):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)",
            "def process_url(url):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)",
            "def process_url(url):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)",
            "def process_url(url):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)"
        ]
    },
    {
        "func_name": "process_url",
        "original": "def process_url(url):\n    return url",
        "mutated": [
            "def process_url(url):\n    if False:\n        i = 10\n    return url",
            "def process_url(url):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return url",
            "def process_url(url):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return url",
            "def process_url(url):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return url",
            "def process_url(url):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return url"
        ]
    },
    {
        "func_name": "crack_and_chunk_and_embed",
        "original": "def crack_and_chunk_and_embed(logger, activity_logger, source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, use_rcts: bool=True, custom_loader: Optional[str]=None, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, verbosity: int=0) -> EmbeddingsContainer:\n    \"\"\"Crack and chunk and embed and index documents.\"\"\"\n    if embeddings_container is None:\n        connection_args = {}\n        if embeddings_connection is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            if isinstance(embeddings_connection, str):\n                connection_args['connection'] = {'id': embeddings_connection}\n            else:\n                from azure.ai.generative.index._utils.connections import get_id_from_connection\n                connection_args['connection'] = {'id': get_id_from_connection(embeddings_connection)}\n        embeddings_container = EmbeddingsContainer.from_uri(embeddings_model, **connection_args)\n    if citation_replacement_regex:\n        document_path_replacement = json.loads(citation_replacement_regex)\n        url_replacement_match = re.compile(document_path_replacement['match_pattern'])\n\n        def process_url(url):\n            return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)\n    else:\n\n        def process_url(url):\n            return url\n    source_documents = files_to_document_source(source_uri, source_glob, citation_url if citation_url is not None else DocumentChunksIterator._infer_base_url_from_git(source_uri), process_url)\n    filter_and_log_extensions = get_activity_logging_filter(activity_logger, source_glob)\n    sources_to_embed = OrderedDict()\n    reused_sources = OrderedDict()\n    for source_doc in filter_and_log_extensions(source_documents):\n        mtime = source_doc.mtime\n        existing_embedded_source = embeddings_container._document_sources.get(source_doc.filename)\n        if existing_embedded_source:\n            if existing_embedded_source.source.mtime is not None and existing_embedded_source.source.mtime >= mtime:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"REUSING: source '{source_doc.filename}' embedding as mtime {mtime} is older than existing mtime {existing_embedded_source.source.mtime}\")\n                reused_sources[source_doc.filename] = existing_embedded_source\n            else:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: source '{source_doc.filename}' as mtime {mtime} is newer than existing mtime {existing_embedded_source.source.mtime}\")\n                sources_to_embed[source_doc.filename] = source_doc\n            del embeddings_container._document_sources[source_doc.filename]\n        else:\n            if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                logger.info(f\"EMBEDDING: source '{source_doc.filename}' as no existing embedding found\")\n            sources_to_embed[source_doc.filename] = source_doc\n    deleted_sources = embeddings_container._document_sources\n    for deleted_source in deleted_sources.values():\n        if verbosity >= SOURCE_LOGGING_VERBOSITY:\n            logger.info(f\"REMOVING: source '{deleted_source.source.filename}' from EmbeddingsContainer as it no longer exists in source\")\n    embeddings_container._document_sources = OrderedDict()\n    embeddings_container._deleted_sources = deleted_sources\n    logger.info(f'Finished processing sources:\\nSources to embed: {len(sources_to_embed)}\\nSources reused: {len(reused_sources)}\\nSources deleted: {len(deleted_sources)}')\n    documents_embedded = OrderedDict()\n    for reused_source in reused_sources.values():\n        embeddings_container._document_sources[reused_source.source.filename] = reused_source\n        for doc_id in reused_source.document_ids:\n            if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                logger.info(f\"REUSING: chunk '{doc_id}' from source '{reused_source.source.filename}'\")\n            documents_embedded[doc_id] = embeddings_container._document_embeddings[doc_id]\n            del embeddings_container._document_embeddings[doc_id]\n    extension_loaders = copy.deepcopy(file_extension_loaders)\n    extension_splitters = copy.deepcopy(file_extension_splitters)\n    if custom_loader:\n        logger.info(f'Loading custom loader(s) from {custom_loader}', extra={'print': True})\n        for python_file_path in Path(custom_loader).glob('**/*.py'):\n            custom_loading(python_file_path, extension_loaders, extension_splitters)\n    splitter_args = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': use_rcts}\n    cracked_sources = crack_documents(sources_to_embed.values(), file_extension_loaders=extension_loaders)\n    chunked_docs = split_documents(cracked_sources, splitter_args=splitter_args, file_extension_splitters=extension_splitters)\n    documents_to_embed = []\n    for chunked_doc in chunked_docs:\n        logger.info(f'Processing chunks for source: {chunked_doc.source.filename}')\n        source_doc_ids = []\n        for document in chunked_doc.chunks:\n            import hashlib\n            document_data = document.load_data()\n            document_hash = hashlib.sha256(document_data.encode('utf-8')).hexdigest()\n            document.metadata['content_hash'] = document_hash\n            existing_embedded_document = embeddings_container._document_embeddings.get(document.document_id)\n            if existing_embedded_document:\n                if existing_embedded_document.document_hash == document_hash:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"SKIPPING: chunk '{document.document_id}' embedding as hash {document_hash} is the same as existing hash {existing_embedded_document.document_hash}\")\n                    documents_embedded[document.document_id] = existing_embedded_document\n                else:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"EMBEDDING: chunk '{document.document_id}' as hash {document_hash} is different than existing hash {existing_embedded_document.document_hash}\")\n                    documents_to_embed.append(document)\n                del embeddings_container._document_embeddings[document.document_id]\n            else:\n                if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: chunk '{document.document_id}' as no existing embedding found\")\n                documents_to_embed.append(document)\n            source_doc_ids.append(document.document_id)\n        embeddings_container._document_sources[chunked_doc.source.filename] = EmbeddedDocumentSource(chunked_doc.source, source_doc_ids)\n    deleted_documents = embeddings_container._document_embeddings\n    for document in deleted_documents.values():\n        if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n            logger.debug(f\"REMOVING: chunk '{document.document_id}' from EmbeddingsContainer as it no longer exists in source\")\n    logger.info(f'Finished determining Documents to embed:\\nDocuments to embed: {len(documents_to_embed)}\\nDocuments reused: {len(documents_embedded.keys())}\\nDocuments deleted: {len(deleted_documents.keys())}')\n    data_to_embed = [document.load_data() for document in documents_to_embed]\n    with track_activity(logger, 'Embeddings.embed', custom_dimensions={'documents_to_embed': len(documents_to_embed), 'reused_documents': len(documents_embedded.keys()), 'deleted_documents': len(deleted_documents.keys()), 'kind': embeddings_container.kind, 'model': embeddings_container.arguments.get('model', '')}) as activity_logger:\n        embeddings = embeddings_container._embed_fn(data_to_embed, activity_logger=activity_logger)\n    for (document, embedding) in zip(documents_to_embed, embeddings):\n        documents_embedded[document.document_id] = DataEmbeddedDocument(document.document_id, document.mtime, document.metadata['content_hash'], document.load_data(), embedding, document.metadata)\n    embeddings_container._document_embeddings = documents_embedded\n    embeddings_container._deleted_documents = deleted_documents\n    file_count = len(sources_to_embed) + len(reused_sources)\n    logger.info(f'Processed {file_count} files')\n    activity_logger.activity_info['file_count'] = str(file_count)\n    if file_count == 0:\n        logger.info(f'No chunked documents found in {source_uri} with glob {source_glob}')\n        activity_logger.activity_info['error'] = 'No chunks found'\n        activity_logger.activity_info['glob'] = source_glob if re.match('^[*/\\\\\"\\']+$', source_glob) is not None else '[REDACTED]'\n        raise ValueError(f'No chunked documents found in {source_uri} with glob {source_glob}.')\n    return embeddings_container",
        "mutated": [
            "def crack_and_chunk_and_embed(logger, activity_logger, source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, use_rcts: bool=True, custom_loader: Optional[str]=None, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, verbosity: int=0) -> EmbeddingsContainer:\n    if False:\n        i = 10\n    'Crack and chunk and embed and index documents.'\n    if embeddings_container is None:\n        connection_args = {}\n        if embeddings_connection is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            if isinstance(embeddings_connection, str):\n                connection_args['connection'] = {'id': embeddings_connection}\n            else:\n                from azure.ai.generative.index._utils.connections import get_id_from_connection\n                connection_args['connection'] = {'id': get_id_from_connection(embeddings_connection)}\n        embeddings_container = EmbeddingsContainer.from_uri(embeddings_model, **connection_args)\n    if citation_replacement_regex:\n        document_path_replacement = json.loads(citation_replacement_regex)\n        url_replacement_match = re.compile(document_path_replacement['match_pattern'])\n\n        def process_url(url):\n            return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)\n    else:\n\n        def process_url(url):\n            return url\n    source_documents = files_to_document_source(source_uri, source_glob, citation_url if citation_url is not None else DocumentChunksIterator._infer_base_url_from_git(source_uri), process_url)\n    filter_and_log_extensions = get_activity_logging_filter(activity_logger, source_glob)\n    sources_to_embed = OrderedDict()\n    reused_sources = OrderedDict()\n    for source_doc in filter_and_log_extensions(source_documents):\n        mtime = source_doc.mtime\n        existing_embedded_source = embeddings_container._document_sources.get(source_doc.filename)\n        if existing_embedded_source:\n            if existing_embedded_source.source.mtime is not None and existing_embedded_source.source.mtime >= mtime:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"REUSING: source '{source_doc.filename}' embedding as mtime {mtime} is older than existing mtime {existing_embedded_source.source.mtime}\")\n                reused_sources[source_doc.filename] = existing_embedded_source\n            else:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: source '{source_doc.filename}' as mtime {mtime} is newer than existing mtime {existing_embedded_source.source.mtime}\")\n                sources_to_embed[source_doc.filename] = source_doc\n            del embeddings_container._document_sources[source_doc.filename]\n        else:\n            if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                logger.info(f\"EMBEDDING: source '{source_doc.filename}' as no existing embedding found\")\n            sources_to_embed[source_doc.filename] = source_doc\n    deleted_sources = embeddings_container._document_sources\n    for deleted_source in deleted_sources.values():\n        if verbosity >= SOURCE_LOGGING_VERBOSITY:\n            logger.info(f\"REMOVING: source '{deleted_source.source.filename}' from EmbeddingsContainer as it no longer exists in source\")\n    embeddings_container._document_sources = OrderedDict()\n    embeddings_container._deleted_sources = deleted_sources\n    logger.info(f'Finished processing sources:\\nSources to embed: {len(sources_to_embed)}\\nSources reused: {len(reused_sources)}\\nSources deleted: {len(deleted_sources)}')\n    documents_embedded = OrderedDict()\n    for reused_source in reused_sources.values():\n        embeddings_container._document_sources[reused_source.source.filename] = reused_source\n        for doc_id in reused_source.document_ids:\n            if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                logger.info(f\"REUSING: chunk '{doc_id}' from source '{reused_source.source.filename}'\")\n            documents_embedded[doc_id] = embeddings_container._document_embeddings[doc_id]\n            del embeddings_container._document_embeddings[doc_id]\n    extension_loaders = copy.deepcopy(file_extension_loaders)\n    extension_splitters = copy.deepcopy(file_extension_splitters)\n    if custom_loader:\n        logger.info(f'Loading custom loader(s) from {custom_loader}', extra={'print': True})\n        for python_file_path in Path(custom_loader).glob('**/*.py'):\n            custom_loading(python_file_path, extension_loaders, extension_splitters)\n    splitter_args = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': use_rcts}\n    cracked_sources = crack_documents(sources_to_embed.values(), file_extension_loaders=extension_loaders)\n    chunked_docs = split_documents(cracked_sources, splitter_args=splitter_args, file_extension_splitters=extension_splitters)\n    documents_to_embed = []\n    for chunked_doc in chunked_docs:\n        logger.info(f'Processing chunks for source: {chunked_doc.source.filename}')\n        source_doc_ids = []\n        for document in chunked_doc.chunks:\n            import hashlib\n            document_data = document.load_data()\n            document_hash = hashlib.sha256(document_data.encode('utf-8')).hexdigest()\n            document.metadata['content_hash'] = document_hash\n            existing_embedded_document = embeddings_container._document_embeddings.get(document.document_id)\n            if existing_embedded_document:\n                if existing_embedded_document.document_hash == document_hash:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"SKIPPING: chunk '{document.document_id}' embedding as hash {document_hash} is the same as existing hash {existing_embedded_document.document_hash}\")\n                    documents_embedded[document.document_id] = existing_embedded_document\n                else:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"EMBEDDING: chunk '{document.document_id}' as hash {document_hash} is different than existing hash {existing_embedded_document.document_hash}\")\n                    documents_to_embed.append(document)\n                del embeddings_container._document_embeddings[document.document_id]\n            else:\n                if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: chunk '{document.document_id}' as no existing embedding found\")\n                documents_to_embed.append(document)\n            source_doc_ids.append(document.document_id)\n        embeddings_container._document_sources[chunked_doc.source.filename] = EmbeddedDocumentSource(chunked_doc.source, source_doc_ids)\n    deleted_documents = embeddings_container._document_embeddings\n    for document in deleted_documents.values():\n        if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n            logger.debug(f\"REMOVING: chunk '{document.document_id}' from EmbeddingsContainer as it no longer exists in source\")\n    logger.info(f'Finished determining Documents to embed:\\nDocuments to embed: {len(documents_to_embed)}\\nDocuments reused: {len(documents_embedded.keys())}\\nDocuments deleted: {len(deleted_documents.keys())}')\n    data_to_embed = [document.load_data() for document in documents_to_embed]\n    with track_activity(logger, 'Embeddings.embed', custom_dimensions={'documents_to_embed': len(documents_to_embed), 'reused_documents': len(documents_embedded.keys()), 'deleted_documents': len(deleted_documents.keys()), 'kind': embeddings_container.kind, 'model': embeddings_container.arguments.get('model', '')}) as activity_logger:\n        embeddings = embeddings_container._embed_fn(data_to_embed, activity_logger=activity_logger)\n    for (document, embedding) in zip(documents_to_embed, embeddings):\n        documents_embedded[document.document_id] = DataEmbeddedDocument(document.document_id, document.mtime, document.metadata['content_hash'], document.load_data(), embedding, document.metadata)\n    embeddings_container._document_embeddings = documents_embedded\n    embeddings_container._deleted_documents = deleted_documents\n    file_count = len(sources_to_embed) + len(reused_sources)\n    logger.info(f'Processed {file_count} files')\n    activity_logger.activity_info['file_count'] = str(file_count)\n    if file_count == 0:\n        logger.info(f'No chunked documents found in {source_uri} with glob {source_glob}')\n        activity_logger.activity_info['error'] = 'No chunks found'\n        activity_logger.activity_info['glob'] = source_glob if re.match('^[*/\\\\\"\\']+$', source_glob) is not None else '[REDACTED]'\n        raise ValueError(f'No chunked documents found in {source_uri} with glob {source_glob}.')\n    return embeddings_container",
            "def crack_and_chunk_and_embed(logger, activity_logger, source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, use_rcts: bool=True, custom_loader: Optional[str]=None, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, verbosity: int=0) -> EmbeddingsContainer:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Crack and chunk and embed and index documents.'\n    if embeddings_container is None:\n        connection_args = {}\n        if embeddings_connection is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            if isinstance(embeddings_connection, str):\n                connection_args['connection'] = {'id': embeddings_connection}\n            else:\n                from azure.ai.generative.index._utils.connections import get_id_from_connection\n                connection_args['connection'] = {'id': get_id_from_connection(embeddings_connection)}\n        embeddings_container = EmbeddingsContainer.from_uri(embeddings_model, **connection_args)\n    if citation_replacement_regex:\n        document_path_replacement = json.loads(citation_replacement_regex)\n        url_replacement_match = re.compile(document_path_replacement['match_pattern'])\n\n        def process_url(url):\n            return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)\n    else:\n\n        def process_url(url):\n            return url\n    source_documents = files_to_document_source(source_uri, source_glob, citation_url if citation_url is not None else DocumentChunksIterator._infer_base_url_from_git(source_uri), process_url)\n    filter_and_log_extensions = get_activity_logging_filter(activity_logger, source_glob)\n    sources_to_embed = OrderedDict()\n    reused_sources = OrderedDict()\n    for source_doc in filter_and_log_extensions(source_documents):\n        mtime = source_doc.mtime\n        existing_embedded_source = embeddings_container._document_sources.get(source_doc.filename)\n        if existing_embedded_source:\n            if existing_embedded_source.source.mtime is not None and existing_embedded_source.source.mtime >= mtime:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"REUSING: source '{source_doc.filename}' embedding as mtime {mtime} is older than existing mtime {existing_embedded_source.source.mtime}\")\n                reused_sources[source_doc.filename] = existing_embedded_source\n            else:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: source '{source_doc.filename}' as mtime {mtime} is newer than existing mtime {existing_embedded_source.source.mtime}\")\n                sources_to_embed[source_doc.filename] = source_doc\n            del embeddings_container._document_sources[source_doc.filename]\n        else:\n            if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                logger.info(f\"EMBEDDING: source '{source_doc.filename}' as no existing embedding found\")\n            sources_to_embed[source_doc.filename] = source_doc\n    deleted_sources = embeddings_container._document_sources\n    for deleted_source in deleted_sources.values():\n        if verbosity >= SOURCE_LOGGING_VERBOSITY:\n            logger.info(f\"REMOVING: source '{deleted_source.source.filename}' from EmbeddingsContainer as it no longer exists in source\")\n    embeddings_container._document_sources = OrderedDict()\n    embeddings_container._deleted_sources = deleted_sources\n    logger.info(f'Finished processing sources:\\nSources to embed: {len(sources_to_embed)}\\nSources reused: {len(reused_sources)}\\nSources deleted: {len(deleted_sources)}')\n    documents_embedded = OrderedDict()\n    for reused_source in reused_sources.values():\n        embeddings_container._document_sources[reused_source.source.filename] = reused_source\n        for doc_id in reused_source.document_ids:\n            if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                logger.info(f\"REUSING: chunk '{doc_id}' from source '{reused_source.source.filename}'\")\n            documents_embedded[doc_id] = embeddings_container._document_embeddings[doc_id]\n            del embeddings_container._document_embeddings[doc_id]\n    extension_loaders = copy.deepcopy(file_extension_loaders)\n    extension_splitters = copy.deepcopy(file_extension_splitters)\n    if custom_loader:\n        logger.info(f'Loading custom loader(s) from {custom_loader}', extra={'print': True})\n        for python_file_path in Path(custom_loader).glob('**/*.py'):\n            custom_loading(python_file_path, extension_loaders, extension_splitters)\n    splitter_args = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': use_rcts}\n    cracked_sources = crack_documents(sources_to_embed.values(), file_extension_loaders=extension_loaders)\n    chunked_docs = split_documents(cracked_sources, splitter_args=splitter_args, file_extension_splitters=extension_splitters)\n    documents_to_embed = []\n    for chunked_doc in chunked_docs:\n        logger.info(f'Processing chunks for source: {chunked_doc.source.filename}')\n        source_doc_ids = []\n        for document in chunked_doc.chunks:\n            import hashlib\n            document_data = document.load_data()\n            document_hash = hashlib.sha256(document_data.encode('utf-8')).hexdigest()\n            document.metadata['content_hash'] = document_hash\n            existing_embedded_document = embeddings_container._document_embeddings.get(document.document_id)\n            if existing_embedded_document:\n                if existing_embedded_document.document_hash == document_hash:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"SKIPPING: chunk '{document.document_id}' embedding as hash {document_hash} is the same as existing hash {existing_embedded_document.document_hash}\")\n                    documents_embedded[document.document_id] = existing_embedded_document\n                else:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"EMBEDDING: chunk '{document.document_id}' as hash {document_hash} is different than existing hash {existing_embedded_document.document_hash}\")\n                    documents_to_embed.append(document)\n                del embeddings_container._document_embeddings[document.document_id]\n            else:\n                if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: chunk '{document.document_id}' as no existing embedding found\")\n                documents_to_embed.append(document)\n            source_doc_ids.append(document.document_id)\n        embeddings_container._document_sources[chunked_doc.source.filename] = EmbeddedDocumentSource(chunked_doc.source, source_doc_ids)\n    deleted_documents = embeddings_container._document_embeddings\n    for document in deleted_documents.values():\n        if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n            logger.debug(f\"REMOVING: chunk '{document.document_id}' from EmbeddingsContainer as it no longer exists in source\")\n    logger.info(f'Finished determining Documents to embed:\\nDocuments to embed: {len(documents_to_embed)}\\nDocuments reused: {len(documents_embedded.keys())}\\nDocuments deleted: {len(deleted_documents.keys())}')\n    data_to_embed = [document.load_data() for document in documents_to_embed]\n    with track_activity(logger, 'Embeddings.embed', custom_dimensions={'documents_to_embed': len(documents_to_embed), 'reused_documents': len(documents_embedded.keys()), 'deleted_documents': len(deleted_documents.keys()), 'kind': embeddings_container.kind, 'model': embeddings_container.arguments.get('model', '')}) as activity_logger:\n        embeddings = embeddings_container._embed_fn(data_to_embed, activity_logger=activity_logger)\n    for (document, embedding) in zip(documents_to_embed, embeddings):\n        documents_embedded[document.document_id] = DataEmbeddedDocument(document.document_id, document.mtime, document.metadata['content_hash'], document.load_data(), embedding, document.metadata)\n    embeddings_container._document_embeddings = documents_embedded\n    embeddings_container._deleted_documents = deleted_documents\n    file_count = len(sources_to_embed) + len(reused_sources)\n    logger.info(f'Processed {file_count} files')\n    activity_logger.activity_info['file_count'] = str(file_count)\n    if file_count == 0:\n        logger.info(f'No chunked documents found in {source_uri} with glob {source_glob}')\n        activity_logger.activity_info['error'] = 'No chunks found'\n        activity_logger.activity_info['glob'] = source_glob if re.match('^[*/\\\\\"\\']+$', source_glob) is not None else '[REDACTED]'\n        raise ValueError(f'No chunked documents found in {source_uri} with glob {source_glob}.')\n    return embeddings_container",
            "def crack_and_chunk_and_embed(logger, activity_logger, source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, use_rcts: bool=True, custom_loader: Optional[str]=None, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, verbosity: int=0) -> EmbeddingsContainer:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Crack and chunk and embed and index documents.'\n    if embeddings_container is None:\n        connection_args = {}\n        if embeddings_connection is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            if isinstance(embeddings_connection, str):\n                connection_args['connection'] = {'id': embeddings_connection}\n            else:\n                from azure.ai.generative.index._utils.connections import get_id_from_connection\n                connection_args['connection'] = {'id': get_id_from_connection(embeddings_connection)}\n        embeddings_container = EmbeddingsContainer.from_uri(embeddings_model, **connection_args)\n    if citation_replacement_regex:\n        document_path_replacement = json.loads(citation_replacement_regex)\n        url_replacement_match = re.compile(document_path_replacement['match_pattern'])\n\n        def process_url(url):\n            return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)\n    else:\n\n        def process_url(url):\n            return url\n    source_documents = files_to_document_source(source_uri, source_glob, citation_url if citation_url is not None else DocumentChunksIterator._infer_base_url_from_git(source_uri), process_url)\n    filter_and_log_extensions = get_activity_logging_filter(activity_logger, source_glob)\n    sources_to_embed = OrderedDict()\n    reused_sources = OrderedDict()\n    for source_doc in filter_and_log_extensions(source_documents):\n        mtime = source_doc.mtime\n        existing_embedded_source = embeddings_container._document_sources.get(source_doc.filename)\n        if existing_embedded_source:\n            if existing_embedded_source.source.mtime is not None and existing_embedded_source.source.mtime >= mtime:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"REUSING: source '{source_doc.filename}' embedding as mtime {mtime} is older than existing mtime {existing_embedded_source.source.mtime}\")\n                reused_sources[source_doc.filename] = existing_embedded_source\n            else:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: source '{source_doc.filename}' as mtime {mtime} is newer than existing mtime {existing_embedded_source.source.mtime}\")\n                sources_to_embed[source_doc.filename] = source_doc\n            del embeddings_container._document_sources[source_doc.filename]\n        else:\n            if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                logger.info(f\"EMBEDDING: source '{source_doc.filename}' as no existing embedding found\")\n            sources_to_embed[source_doc.filename] = source_doc\n    deleted_sources = embeddings_container._document_sources\n    for deleted_source in deleted_sources.values():\n        if verbosity >= SOURCE_LOGGING_VERBOSITY:\n            logger.info(f\"REMOVING: source '{deleted_source.source.filename}' from EmbeddingsContainer as it no longer exists in source\")\n    embeddings_container._document_sources = OrderedDict()\n    embeddings_container._deleted_sources = deleted_sources\n    logger.info(f'Finished processing sources:\\nSources to embed: {len(sources_to_embed)}\\nSources reused: {len(reused_sources)}\\nSources deleted: {len(deleted_sources)}')\n    documents_embedded = OrderedDict()\n    for reused_source in reused_sources.values():\n        embeddings_container._document_sources[reused_source.source.filename] = reused_source\n        for doc_id in reused_source.document_ids:\n            if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                logger.info(f\"REUSING: chunk '{doc_id}' from source '{reused_source.source.filename}'\")\n            documents_embedded[doc_id] = embeddings_container._document_embeddings[doc_id]\n            del embeddings_container._document_embeddings[doc_id]\n    extension_loaders = copy.deepcopy(file_extension_loaders)\n    extension_splitters = copy.deepcopy(file_extension_splitters)\n    if custom_loader:\n        logger.info(f'Loading custom loader(s) from {custom_loader}', extra={'print': True})\n        for python_file_path in Path(custom_loader).glob('**/*.py'):\n            custom_loading(python_file_path, extension_loaders, extension_splitters)\n    splitter_args = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': use_rcts}\n    cracked_sources = crack_documents(sources_to_embed.values(), file_extension_loaders=extension_loaders)\n    chunked_docs = split_documents(cracked_sources, splitter_args=splitter_args, file_extension_splitters=extension_splitters)\n    documents_to_embed = []\n    for chunked_doc in chunked_docs:\n        logger.info(f'Processing chunks for source: {chunked_doc.source.filename}')\n        source_doc_ids = []\n        for document in chunked_doc.chunks:\n            import hashlib\n            document_data = document.load_data()\n            document_hash = hashlib.sha256(document_data.encode('utf-8')).hexdigest()\n            document.metadata['content_hash'] = document_hash\n            existing_embedded_document = embeddings_container._document_embeddings.get(document.document_id)\n            if existing_embedded_document:\n                if existing_embedded_document.document_hash == document_hash:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"SKIPPING: chunk '{document.document_id}' embedding as hash {document_hash} is the same as existing hash {existing_embedded_document.document_hash}\")\n                    documents_embedded[document.document_id] = existing_embedded_document\n                else:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"EMBEDDING: chunk '{document.document_id}' as hash {document_hash} is different than existing hash {existing_embedded_document.document_hash}\")\n                    documents_to_embed.append(document)\n                del embeddings_container._document_embeddings[document.document_id]\n            else:\n                if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: chunk '{document.document_id}' as no existing embedding found\")\n                documents_to_embed.append(document)\n            source_doc_ids.append(document.document_id)\n        embeddings_container._document_sources[chunked_doc.source.filename] = EmbeddedDocumentSource(chunked_doc.source, source_doc_ids)\n    deleted_documents = embeddings_container._document_embeddings\n    for document in deleted_documents.values():\n        if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n            logger.debug(f\"REMOVING: chunk '{document.document_id}' from EmbeddingsContainer as it no longer exists in source\")\n    logger.info(f'Finished determining Documents to embed:\\nDocuments to embed: {len(documents_to_embed)}\\nDocuments reused: {len(documents_embedded.keys())}\\nDocuments deleted: {len(deleted_documents.keys())}')\n    data_to_embed = [document.load_data() for document in documents_to_embed]\n    with track_activity(logger, 'Embeddings.embed', custom_dimensions={'documents_to_embed': len(documents_to_embed), 'reused_documents': len(documents_embedded.keys()), 'deleted_documents': len(deleted_documents.keys()), 'kind': embeddings_container.kind, 'model': embeddings_container.arguments.get('model', '')}) as activity_logger:\n        embeddings = embeddings_container._embed_fn(data_to_embed, activity_logger=activity_logger)\n    for (document, embedding) in zip(documents_to_embed, embeddings):\n        documents_embedded[document.document_id] = DataEmbeddedDocument(document.document_id, document.mtime, document.metadata['content_hash'], document.load_data(), embedding, document.metadata)\n    embeddings_container._document_embeddings = documents_embedded\n    embeddings_container._deleted_documents = deleted_documents\n    file_count = len(sources_to_embed) + len(reused_sources)\n    logger.info(f'Processed {file_count} files')\n    activity_logger.activity_info['file_count'] = str(file_count)\n    if file_count == 0:\n        logger.info(f'No chunked documents found in {source_uri} with glob {source_glob}')\n        activity_logger.activity_info['error'] = 'No chunks found'\n        activity_logger.activity_info['glob'] = source_glob if re.match('^[*/\\\\\"\\']+$', source_glob) is not None else '[REDACTED]'\n        raise ValueError(f'No chunked documents found in {source_uri} with glob {source_glob}.')\n    return embeddings_container",
            "def crack_and_chunk_and_embed(logger, activity_logger, source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, use_rcts: bool=True, custom_loader: Optional[str]=None, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, verbosity: int=0) -> EmbeddingsContainer:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Crack and chunk and embed and index documents.'\n    if embeddings_container is None:\n        connection_args = {}\n        if embeddings_connection is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            if isinstance(embeddings_connection, str):\n                connection_args['connection'] = {'id': embeddings_connection}\n            else:\n                from azure.ai.generative.index._utils.connections import get_id_from_connection\n                connection_args['connection'] = {'id': get_id_from_connection(embeddings_connection)}\n        embeddings_container = EmbeddingsContainer.from_uri(embeddings_model, **connection_args)\n    if citation_replacement_regex:\n        document_path_replacement = json.loads(citation_replacement_regex)\n        url_replacement_match = re.compile(document_path_replacement['match_pattern'])\n\n        def process_url(url):\n            return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)\n    else:\n\n        def process_url(url):\n            return url\n    source_documents = files_to_document_source(source_uri, source_glob, citation_url if citation_url is not None else DocumentChunksIterator._infer_base_url_from_git(source_uri), process_url)\n    filter_and_log_extensions = get_activity_logging_filter(activity_logger, source_glob)\n    sources_to_embed = OrderedDict()\n    reused_sources = OrderedDict()\n    for source_doc in filter_and_log_extensions(source_documents):\n        mtime = source_doc.mtime\n        existing_embedded_source = embeddings_container._document_sources.get(source_doc.filename)\n        if existing_embedded_source:\n            if existing_embedded_source.source.mtime is not None and existing_embedded_source.source.mtime >= mtime:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"REUSING: source '{source_doc.filename}' embedding as mtime {mtime} is older than existing mtime {existing_embedded_source.source.mtime}\")\n                reused_sources[source_doc.filename] = existing_embedded_source\n            else:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: source '{source_doc.filename}' as mtime {mtime} is newer than existing mtime {existing_embedded_source.source.mtime}\")\n                sources_to_embed[source_doc.filename] = source_doc\n            del embeddings_container._document_sources[source_doc.filename]\n        else:\n            if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                logger.info(f\"EMBEDDING: source '{source_doc.filename}' as no existing embedding found\")\n            sources_to_embed[source_doc.filename] = source_doc\n    deleted_sources = embeddings_container._document_sources\n    for deleted_source in deleted_sources.values():\n        if verbosity >= SOURCE_LOGGING_VERBOSITY:\n            logger.info(f\"REMOVING: source '{deleted_source.source.filename}' from EmbeddingsContainer as it no longer exists in source\")\n    embeddings_container._document_sources = OrderedDict()\n    embeddings_container._deleted_sources = deleted_sources\n    logger.info(f'Finished processing sources:\\nSources to embed: {len(sources_to_embed)}\\nSources reused: {len(reused_sources)}\\nSources deleted: {len(deleted_sources)}')\n    documents_embedded = OrderedDict()\n    for reused_source in reused_sources.values():\n        embeddings_container._document_sources[reused_source.source.filename] = reused_source\n        for doc_id in reused_source.document_ids:\n            if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                logger.info(f\"REUSING: chunk '{doc_id}' from source '{reused_source.source.filename}'\")\n            documents_embedded[doc_id] = embeddings_container._document_embeddings[doc_id]\n            del embeddings_container._document_embeddings[doc_id]\n    extension_loaders = copy.deepcopy(file_extension_loaders)\n    extension_splitters = copy.deepcopy(file_extension_splitters)\n    if custom_loader:\n        logger.info(f'Loading custom loader(s) from {custom_loader}', extra={'print': True})\n        for python_file_path in Path(custom_loader).glob('**/*.py'):\n            custom_loading(python_file_path, extension_loaders, extension_splitters)\n    splitter_args = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': use_rcts}\n    cracked_sources = crack_documents(sources_to_embed.values(), file_extension_loaders=extension_loaders)\n    chunked_docs = split_documents(cracked_sources, splitter_args=splitter_args, file_extension_splitters=extension_splitters)\n    documents_to_embed = []\n    for chunked_doc in chunked_docs:\n        logger.info(f'Processing chunks for source: {chunked_doc.source.filename}')\n        source_doc_ids = []\n        for document in chunked_doc.chunks:\n            import hashlib\n            document_data = document.load_data()\n            document_hash = hashlib.sha256(document_data.encode('utf-8')).hexdigest()\n            document.metadata['content_hash'] = document_hash\n            existing_embedded_document = embeddings_container._document_embeddings.get(document.document_id)\n            if existing_embedded_document:\n                if existing_embedded_document.document_hash == document_hash:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"SKIPPING: chunk '{document.document_id}' embedding as hash {document_hash} is the same as existing hash {existing_embedded_document.document_hash}\")\n                    documents_embedded[document.document_id] = existing_embedded_document\n                else:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"EMBEDDING: chunk '{document.document_id}' as hash {document_hash} is different than existing hash {existing_embedded_document.document_hash}\")\n                    documents_to_embed.append(document)\n                del embeddings_container._document_embeddings[document.document_id]\n            else:\n                if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: chunk '{document.document_id}' as no existing embedding found\")\n                documents_to_embed.append(document)\n            source_doc_ids.append(document.document_id)\n        embeddings_container._document_sources[chunked_doc.source.filename] = EmbeddedDocumentSource(chunked_doc.source, source_doc_ids)\n    deleted_documents = embeddings_container._document_embeddings\n    for document in deleted_documents.values():\n        if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n            logger.debug(f\"REMOVING: chunk '{document.document_id}' from EmbeddingsContainer as it no longer exists in source\")\n    logger.info(f'Finished determining Documents to embed:\\nDocuments to embed: {len(documents_to_embed)}\\nDocuments reused: {len(documents_embedded.keys())}\\nDocuments deleted: {len(deleted_documents.keys())}')\n    data_to_embed = [document.load_data() for document in documents_to_embed]\n    with track_activity(logger, 'Embeddings.embed', custom_dimensions={'documents_to_embed': len(documents_to_embed), 'reused_documents': len(documents_embedded.keys()), 'deleted_documents': len(deleted_documents.keys()), 'kind': embeddings_container.kind, 'model': embeddings_container.arguments.get('model', '')}) as activity_logger:\n        embeddings = embeddings_container._embed_fn(data_to_embed, activity_logger=activity_logger)\n    for (document, embedding) in zip(documents_to_embed, embeddings):\n        documents_embedded[document.document_id] = DataEmbeddedDocument(document.document_id, document.mtime, document.metadata['content_hash'], document.load_data(), embedding, document.metadata)\n    embeddings_container._document_embeddings = documents_embedded\n    embeddings_container._deleted_documents = deleted_documents\n    file_count = len(sources_to_embed) + len(reused_sources)\n    logger.info(f'Processed {file_count} files')\n    activity_logger.activity_info['file_count'] = str(file_count)\n    if file_count == 0:\n        logger.info(f'No chunked documents found in {source_uri} with glob {source_glob}')\n        activity_logger.activity_info['error'] = 'No chunks found'\n        activity_logger.activity_info['glob'] = source_glob if re.match('^[*/\\\\\"\\']+$', source_glob) is not None else '[REDACTED]'\n        raise ValueError(f'No chunked documents found in {source_uri} with glob {source_glob}.')\n    return embeddings_container",
            "def crack_and_chunk_and_embed(logger, activity_logger, source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, use_rcts: bool=True, custom_loader: Optional[str]=None, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, verbosity: int=0) -> EmbeddingsContainer:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Crack and chunk and embed and index documents.'\n    if embeddings_container is None:\n        connection_args = {}\n        if embeddings_connection is not None:\n            connection_args['connection_type'] = 'workspace_connection'\n            if isinstance(embeddings_connection, str):\n                connection_args['connection'] = {'id': embeddings_connection}\n            else:\n                from azure.ai.generative.index._utils.connections import get_id_from_connection\n                connection_args['connection'] = {'id': get_id_from_connection(embeddings_connection)}\n        embeddings_container = EmbeddingsContainer.from_uri(embeddings_model, **connection_args)\n    if citation_replacement_regex:\n        document_path_replacement = json.loads(citation_replacement_regex)\n        url_replacement_match = re.compile(document_path_replacement['match_pattern'])\n\n        def process_url(url):\n            return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)\n    else:\n\n        def process_url(url):\n            return url\n    source_documents = files_to_document_source(source_uri, source_glob, citation_url if citation_url is not None else DocumentChunksIterator._infer_base_url_from_git(source_uri), process_url)\n    filter_and_log_extensions = get_activity_logging_filter(activity_logger, source_glob)\n    sources_to_embed = OrderedDict()\n    reused_sources = OrderedDict()\n    for source_doc in filter_and_log_extensions(source_documents):\n        mtime = source_doc.mtime\n        existing_embedded_source = embeddings_container._document_sources.get(source_doc.filename)\n        if existing_embedded_source:\n            if existing_embedded_source.source.mtime is not None and existing_embedded_source.source.mtime >= mtime:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"REUSING: source '{source_doc.filename}' embedding as mtime {mtime} is older than existing mtime {existing_embedded_source.source.mtime}\")\n                reused_sources[source_doc.filename] = existing_embedded_source\n            else:\n                if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: source '{source_doc.filename}' as mtime {mtime} is newer than existing mtime {existing_embedded_source.source.mtime}\")\n                sources_to_embed[source_doc.filename] = source_doc\n            del embeddings_container._document_sources[source_doc.filename]\n        else:\n            if verbosity >= SOURCE_LOGGING_VERBOSITY:\n                logger.info(f\"EMBEDDING: source '{source_doc.filename}' as no existing embedding found\")\n            sources_to_embed[source_doc.filename] = source_doc\n    deleted_sources = embeddings_container._document_sources\n    for deleted_source in deleted_sources.values():\n        if verbosity >= SOURCE_LOGGING_VERBOSITY:\n            logger.info(f\"REMOVING: source '{deleted_source.source.filename}' from EmbeddingsContainer as it no longer exists in source\")\n    embeddings_container._document_sources = OrderedDict()\n    embeddings_container._deleted_sources = deleted_sources\n    logger.info(f'Finished processing sources:\\nSources to embed: {len(sources_to_embed)}\\nSources reused: {len(reused_sources)}\\nSources deleted: {len(deleted_sources)}')\n    documents_embedded = OrderedDict()\n    for reused_source in reused_sources.values():\n        embeddings_container._document_sources[reused_source.source.filename] = reused_source\n        for doc_id in reused_source.document_ids:\n            if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                logger.info(f\"REUSING: chunk '{doc_id}' from source '{reused_source.source.filename}'\")\n            documents_embedded[doc_id] = embeddings_container._document_embeddings[doc_id]\n            del embeddings_container._document_embeddings[doc_id]\n    extension_loaders = copy.deepcopy(file_extension_loaders)\n    extension_splitters = copy.deepcopy(file_extension_splitters)\n    if custom_loader:\n        logger.info(f'Loading custom loader(s) from {custom_loader}', extra={'print': True})\n        for python_file_path in Path(custom_loader).glob('**/*.py'):\n            custom_loading(python_file_path, extension_loaders, extension_splitters)\n    splitter_args = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': use_rcts}\n    cracked_sources = crack_documents(sources_to_embed.values(), file_extension_loaders=extension_loaders)\n    chunked_docs = split_documents(cracked_sources, splitter_args=splitter_args, file_extension_splitters=extension_splitters)\n    documents_to_embed = []\n    for chunked_doc in chunked_docs:\n        logger.info(f'Processing chunks for source: {chunked_doc.source.filename}')\n        source_doc_ids = []\n        for document in chunked_doc.chunks:\n            import hashlib\n            document_data = document.load_data()\n            document_hash = hashlib.sha256(document_data.encode('utf-8')).hexdigest()\n            document.metadata['content_hash'] = document_hash\n            existing_embedded_document = embeddings_container._document_embeddings.get(document.document_id)\n            if existing_embedded_document:\n                if existing_embedded_document.document_hash == document_hash:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"SKIPPING: chunk '{document.document_id}' embedding as hash {document_hash} is the same as existing hash {existing_embedded_document.document_hash}\")\n                    documents_embedded[document.document_id] = existing_embedded_document\n                else:\n                    if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                        logger.info(f\"EMBEDDING: chunk '{document.document_id}' as hash {document_hash} is different than existing hash {existing_embedded_document.document_hash}\")\n                    documents_to_embed.append(document)\n                del embeddings_container._document_embeddings[document.document_id]\n            else:\n                if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n                    logger.info(f\"EMBEDDING: chunk '{document.document_id}' as no existing embedding found\")\n                documents_to_embed.append(document)\n            source_doc_ids.append(document.document_id)\n        embeddings_container._document_sources[chunked_doc.source.filename] = EmbeddedDocumentSource(chunked_doc.source, source_doc_ids)\n    deleted_documents = embeddings_container._document_embeddings\n    for document in deleted_documents.values():\n        if verbosity >= DOCUMENT_LOGGING_VERBOSITY:\n            logger.debug(f\"REMOVING: chunk '{document.document_id}' from EmbeddingsContainer as it no longer exists in source\")\n    logger.info(f'Finished determining Documents to embed:\\nDocuments to embed: {len(documents_to_embed)}\\nDocuments reused: {len(documents_embedded.keys())}\\nDocuments deleted: {len(deleted_documents.keys())}')\n    data_to_embed = [document.load_data() for document in documents_to_embed]\n    with track_activity(logger, 'Embeddings.embed', custom_dimensions={'documents_to_embed': len(documents_to_embed), 'reused_documents': len(documents_embedded.keys()), 'deleted_documents': len(deleted_documents.keys()), 'kind': embeddings_container.kind, 'model': embeddings_container.arguments.get('model', '')}) as activity_logger:\n        embeddings = embeddings_container._embed_fn(data_to_embed, activity_logger=activity_logger)\n    for (document, embedding) in zip(documents_to_embed, embeddings):\n        documents_embedded[document.document_id] = DataEmbeddedDocument(document.document_id, document.mtime, document.metadata['content_hash'], document.load_data(), embedding, document.metadata)\n    embeddings_container._document_embeddings = documents_embedded\n    embeddings_container._deleted_documents = deleted_documents\n    file_count = len(sources_to_embed) + len(reused_sources)\n    logger.info(f'Processed {file_count} files')\n    activity_logger.activity_info['file_count'] = str(file_count)\n    if file_count == 0:\n        logger.info(f'No chunked documents found in {source_uri} with glob {source_glob}')\n        activity_logger.activity_info['error'] = 'No chunks found'\n        activity_logger.activity_info['glob'] = source_glob if re.match('^[*/\\\\\"\\']+$', source_glob) is not None else '[REDACTED]'\n        raise ValueError(f'No chunked documents found in {source_uri} with glob {source_glob}.')\n    return embeddings_container"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(args, logger, activity_logger):\n    \"\"\"Main function for crack_and_chunk_and_embed.\"\"\"\n    with EmbeddingsContainer.mount_and_load(args.embeddings_container, activity_logger) as embeddings_container:\n        embeddings_container = crack_and_chunk_and_embed(logger, activity_logger, source_uri=args.input_data, source_glob=args.input_glob, chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap, use_rcts=args.use_rcts, custom_loader=args.custom_loader, citation_url=args.citation_url, citation_replacement_regex=args.citation_replacement_regex, embeddings_model=args.embeddings_model, embeddings_connection=args.embeddings_connection_id, embeddings_container=embeddings_container, verbosity=args.verbosity)\n        if args.output_path is not None:\n            embeddings_container.save(args.output_path, with_metadata=True)",
        "mutated": [
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n    'Main function for crack_and_chunk_and_embed.'\n    with EmbeddingsContainer.mount_and_load(args.embeddings_container, activity_logger) as embeddings_container:\n        embeddings_container = crack_and_chunk_and_embed(logger, activity_logger, source_uri=args.input_data, source_glob=args.input_glob, chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap, use_rcts=args.use_rcts, custom_loader=args.custom_loader, citation_url=args.citation_url, citation_replacement_regex=args.citation_replacement_regex, embeddings_model=args.embeddings_model, embeddings_connection=args.embeddings_connection_id, embeddings_container=embeddings_container, verbosity=args.verbosity)\n        if args.output_path is not None:\n            embeddings_container.save(args.output_path, with_metadata=True)",
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Main function for crack_and_chunk_and_embed.'\n    with EmbeddingsContainer.mount_and_load(args.embeddings_container, activity_logger) as embeddings_container:\n        embeddings_container = crack_and_chunk_and_embed(logger, activity_logger, source_uri=args.input_data, source_glob=args.input_glob, chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap, use_rcts=args.use_rcts, custom_loader=args.custom_loader, citation_url=args.citation_url, citation_replacement_regex=args.citation_replacement_regex, embeddings_model=args.embeddings_model, embeddings_connection=args.embeddings_connection_id, embeddings_container=embeddings_container, verbosity=args.verbosity)\n        if args.output_path is not None:\n            embeddings_container.save(args.output_path, with_metadata=True)",
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Main function for crack_and_chunk_and_embed.'\n    with EmbeddingsContainer.mount_and_load(args.embeddings_container, activity_logger) as embeddings_container:\n        embeddings_container = crack_and_chunk_and_embed(logger, activity_logger, source_uri=args.input_data, source_glob=args.input_glob, chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap, use_rcts=args.use_rcts, custom_loader=args.custom_loader, citation_url=args.citation_url, citation_replacement_regex=args.citation_replacement_regex, embeddings_model=args.embeddings_model, embeddings_connection=args.embeddings_connection_id, embeddings_container=embeddings_container, verbosity=args.verbosity)\n        if args.output_path is not None:\n            embeddings_container.save(args.output_path, with_metadata=True)",
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Main function for crack_and_chunk_and_embed.'\n    with EmbeddingsContainer.mount_and_load(args.embeddings_container, activity_logger) as embeddings_container:\n        embeddings_container = crack_and_chunk_and_embed(logger, activity_logger, source_uri=args.input_data, source_glob=args.input_glob, chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap, use_rcts=args.use_rcts, custom_loader=args.custom_loader, citation_url=args.citation_url, citation_replacement_regex=args.citation_replacement_regex, embeddings_model=args.embeddings_model, embeddings_connection=args.embeddings_connection_id, embeddings_container=embeddings_container, verbosity=args.verbosity)\n        if args.output_path is not None:\n            embeddings_container.save(args.output_path, with_metadata=True)",
            "def main(args, logger, activity_logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Main function for crack_and_chunk_and_embed.'\n    with EmbeddingsContainer.mount_and_load(args.embeddings_container, activity_logger) as embeddings_container:\n        embeddings_container = crack_and_chunk_and_embed(logger, activity_logger, source_uri=args.input_data, source_glob=args.input_glob, chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap, use_rcts=args.use_rcts, custom_loader=args.custom_loader, citation_url=args.citation_url, citation_replacement_regex=args.citation_replacement_regex, embeddings_model=args.embeddings_model, embeddings_connection=args.embeddings_connection_id, embeddings_container=embeddings_container, verbosity=args.verbosity)\n        if args.output_path is not None:\n            embeddings_container.save(args.output_path, with_metadata=True)"
        ]
    },
    {
        "func_name": "main_wrapper",
        "original": "def main_wrapper(args, logger):\n    with track_activity(logger, 'crack_and_chunk_and_embed') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception as e:\n            activity_logger.error(f'crack_and_chunk failed with exception: {traceback.format_exc()}')\n            raise e",
        "mutated": [
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n    with track_activity(logger, 'crack_and_chunk_and_embed') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception as e:\n            activity_logger.error(f'crack_and_chunk failed with exception: {traceback.format_exc()}')\n            raise e",
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with track_activity(logger, 'crack_and_chunk_and_embed') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception as e:\n            activity_logger.error(f'crack_and_chunk failed with exception: {traceback.format_exc()}')\n            raise e",
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with track_activity(logger, 'crack_and_chunk_and_embed') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception as e:\n            activity_logger.error(f'crack_and_chunk failed with exception: {traceback.format_exc()}')\n            raise e",
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with track_activity(logger, 'crack_and_chunk_and_embed') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception as e:\n            activity_logger.error(f'crack_and_chunk failed with exception: {traceback.format_exc()}')\n            raise e",
            "def main_wrapper(args, logger):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with track_activity(logger, 'crack_and_chunk_and_embed') as activity_logger, safe_mlflow_start_run(logger=logger):\n        try:\n            main(args, logger, activity_logger)\n        except Exception as e:\n            activity_logger.error(f'crack_and_chunk failed with exception: {traceback.format_exc()}')\n            raise e"
        ]
    }
]