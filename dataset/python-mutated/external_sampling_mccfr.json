[
    {
        "func_name": "__init__",
        "original": "def __init__(self, game, average_type=AverageType.SIMPLE):\n    super().__init__(game)\n    self._average_type = average_type\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
        "mutated": [
            "def __init__(self, game, average_type=AverageType.SIMPLE):\n    if False:\n        i = 10\n    super().__init__(game)\n    self._average_type = average_type\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
            "def __init__(self, game, average_type=AverageType.SIMPLE):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(game)\n    self._average_type = average_type\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
            "def __init__(self, game, average_type=AverageType.SIMPLE):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(game)\n    self._average_type = average_type\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
            "def __init__(self, game, average_type=AverageType.SIMPLE):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(game)\n    self._average_type = average_type\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
            "def __init__(self, game, average_type=AverageType.SIMPLE):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(game)\n    self._average_type = average_type\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'"
        ]
    },
    {
        "func_name": "iteration",
        "original": "def iteration(self):\n    \"\"\"Performs one iteration of external sampling.\n\n    An iteration consists of one episode for each player as the update\n    player.\n    \"\"\"\n    for player in range(self._num_players):\n        self._update_regrets(self._game.new_initial_state(), player)\n    if self._average_type == AverageType.FULL:\n        reach_probs = np.ones(self._num_players, dtype=np.float64)\n        self._full_update_average(self._game.new_initial_state(), reach_probs)",
        "mutated": [
            "def iteration(self):\n    if False:\n        i = 10\n    'Performs one iteration of external sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for player in range(self._num_players):\n        self._update_regrets(self._game.new_initial_state(), player)\n    if self._average_type == AverageType.FULL:\n        reach_probs = np.ones(self._num_players, dtype=np.float64)\n        self._full_update_average(self._game.new_initial_state(), reach_probs)",
            "def iteration(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Performs one iteration of external sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for player in range(self._num_players):\n        self._update_regrets(self._game.new_initial_state(), player)\n    if self._average_type == AverageType.FULL:\n        reach_probs = np.ones(self._num_players, dtype=np.float64)\n        self._full_update_average(self._game.new_initial_state(), reach_probs)",
            "def iteration(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Performs one iteration of external sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for player in range(self._num_players):\n        self._update_regrets(self._game.new_initial_state(), player)\n    if self._average_type == AverageType.FULL:\n        reach_probs = np.ones(self._num_players, dtype=np.float64)\n        self._full_update_average(self._game.new_initial_state(), reach_probs)",
            "def iteration(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Performs one iteration of external sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for player in range(self._num_players):\n        self._update_regrets(self._game.new_initial_state(), player)\n    if self._average_type == AverageType.FULL:\n        reach_probs = np.ones(self._num_players, dtype=np.float64)\n        self._full_update_average(self._game.new_initial_state(), reach_probs)",
            "def iteration(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Performs one iteration of external sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for player in range(self._num_players):\n        self._update_regrets(self._game.new_initial_state(), player)\n    if self._average_type == AverageType.FULL:\n        reach_probs = np.ones(self._num_players, dtype=np.float64)\n        self._full_update_average(self._game.new_initial_state(), reach_probs)"
        ]
    },
    {
        "func_name": "_full_update_average",
        "original": "def _full_update_average(self, state, reach_probs):\n    \"\"\"Performs a full update average.\n\n    Args:\n      state: the open spiel state to run from\n      reach_probs: array containing the probability of reaching the state\n        from the players point of view\n    \"\"\"\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for action in state.legal_actions():\n            self._full_update_average(state.child(action), reach_probs)\n        return\n    sum_reach_probs = np.sum(reach_probs)\n    if sum_reach_probs == 0:\n        return\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    for action_idx in range(num_legal_actions):\n        new_reach_probs = np.copy(reach_probs)\n        new_reach_probs[cur_player] *= policy[action_idx]\n        self._full_update_average(state.child(legal_actions[action_idx]), new_reach_probs)\n    for action_idx in range(num_legal_actions):\n        self._add_avstrat(info_state_key, action_idx, reach_probs[cur_player] * policy[action_idx])",
        "mutated": [
            "def _full_update_average(self, state, reach_probs):\n    if False:\n        i = 10\n    'Performs a full update average.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      reach_probs: array containing the probability of reaching the state\\n        from the players point of view\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for action in state.legal_actions():\n            self._full_update_average(state.child(action), reach_probs)\n        return\n    sum_reach_probs = np.sum(reach_probs)\n    if sum_reach_probs == 0:\n        return\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    for action_idx in range(num_legal_actions):\n        new_reach_probs = np.copy(reach_probs)\n        new_reach_probs[cur_player] *= policy[action_idx]\n        self._full_update_average(state.child(legal_actions[action_idx]), new_reach_probs)\n    for action_idx in range(num_legal_actions):\n        self._add_avstrat(info_state_key, action_idx, reach_probs[cur_player] * policy[action_idx])",
            "def _full_update_average(self, state, reach_probs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Performs a full update average.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      reach_probs: array containing the probability of reaching the state\\n        from the players point of view\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for action in state.legal_actions():\n            self._full_update_average(state.child(action), reach_probs)\n        return\n    sum_reach_probs = np.sum(reach_probs)\n    if sum_reach_probs == 0:\n        return\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    for action_idx in range(num_legal_actions):\n        new_reach_probs = np.copy(reach_probs)\n        new_reach_probs[cur_player] *= policy[action_idx]\n        self._full_update_average(state.child(legal_actions[action_idx]), new_reach_probs)\n    for action_idx in range(num_legal_actions):\n        self._add_avstrat(info_state_key, action_idx, reach_probs[cur_player] * policy[action_idx])",
            "def _full_update_average(self, state, reach_probs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Performs a full update average.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      reach_probs: array containing the probability of reaching the state\\n        from the players point of view\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for action in state.legal_actions():\n            self._full_update_average(state.child(action), reach_probs)\n        return\n    sum_reach_probs = np.sum(reach_probs)\n    if sum_reach_probs == 0:\n        return\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    for action_idx in range(num_legal_actions):\n        new_reach_probs = np.copy(reach_probs)\n        new_reach_probs[cur_player] *= policy[action_idx]\n        self._full_update_average(state.child(legal_actions[action_idx]), new_reach_probs)\n    for action_idx in range(num_legal_actions):\n        self._add_avstrat(info_state_key, action_idx, reach_probs[cur_player] * policy[action_idx])",
            "def _full_update_average(self, state, reach_probs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Performs a full update average.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      reach_probs: array containing the probability of reaching the state\\n        from the players point of view\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for action in state.legal_actions():\n            self._full_update_average(state.child(action), reach_probs)\n        return\n    sum_reach_probs = np.sum(reach_probs)\n    if sum_reach_probs == 0:\n        return\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    for action_idx in range(num_legal_actions):\n        new_reach_probs = np.copy(reach_probs)\n        new_reach_probs[cur_player] *= policy[action_idx]\n        self._full_update_average(state.child(legal_actions[action_idx]), new_reach_probs)\n    for action_idx in range(num_legal_actions):\n        self._add_avstrat(info_state_key, action_idx, reach_probs[cur_player] * policy[action_idx])",
            "def _full_update_average(self, state, reach_probs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Performs a full update average.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      reach_probs: array containing the probability of reaching the state\\n        from the players point of view\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for action in state.legal_actions():\n            self._full_update_average(state.child(action), reach_probs)\n        return\n    sum_reach_probs = np.sum(reach_probs)\n    if sum_reach_probs == 0:\n        return\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    for action_idx in range(num_legal_actions):\n        new_reach_probs = np.copy(reach_probs)\n        new_reach_probs[cur_player] *= policy[action_idx]\n        self._full_update_average(state.child(legal_actions[action_idx]), new_reach_probs)\n    for action_idx in range(num_legal_actions):\n        self._add_avstrat(info_state_key, action_idx, reach_probs[cur_player] * policy[action_idx])"
        ]
    },
    {
        "func_name": "_update_regrets",
        "original": "def _update_regrets(self, state, player):\n    \"\"\"Runs an episode of external sampling.\n\n    Args:\n      state: the open spiel state to run from\n      player: the player to update regrets for\n\n    Returns:\n      value: is the value of the state in the game\n      obtained as the weighted average of the values\n      of the children\n    \"\"\"\n    if state.is_terminal():\n        return state.player_return(player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        outcome = np.random.choice(outcomes, p=probs)\n        return self._update_regrets(state.child(outcome), player)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    value = 0\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    if cur_player != player:\n        action_idx = np.random.choice(np.arange(num_legal_actions), p=policy)\n        value = self._update_regrets(state.child(legal_actions[action_idx]), player)\n    else:\n        for action_idx in range(num_legal_actions):\n            child_values[action_idx] = self._update_regrets(state.child(legal_actions[action_idx]), player)\n            value += policy[action_idx] * child_values[action_idx]\n    if cur_player == player:\n        for action_idx in range(num_legal_actions):\n            self._add_regret(info_state_key, action_idx, child_values[action_idx] - value)\n    if self._average_type == AverageType.SIMPLE and cur_player == (player + 1) % self._num_players:\n        for action_idx in range(num_legal_actions):\n            self._add_avstrat(info_state_key, action_idx, policy[action_idx])\n    return value",
        "mutated": [
            "def _update_regrets(self, state, player):\n    if False:\n        i = 10\n    'Runs an episode of external sampling.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      player: the player to update regrets for\\n\\n    Returns:\\n      value: is the value of the state in the game\\n      obtained as the weighted average of the values\\n      of the children\\n    '\n    if state.is_terminal():\n        return state.player_return(player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        outcome = np.random.choice(outcomes, p=probs)\n        return self._update_regrets(state.child(outcome), player)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    value = 0\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    if cur_player != player:\n        action_idx = np.random.choice(np.arange(num_legal_actions), p=policy)\n        value = self._update_regrets(state.child(legal_actions[action_idx]), player)\n    else:\n        for action_idx in range(num_legal_actions):\n            child_values[action_idx] = self._update_regrets(state.child(legal_actions[action_idx]), player)\n            value += policy[action_idx] * child_values[action_idx]\n    if cur_player == player:\n        for action_idx in range(num_legal_actions):\n            self._add_regret(info_state_key, action_idx, child_values[action_idx] - value)\n    if self._average_type == AverageType.SIMPLE and cur_player == (player + 1) % self._num_players:\n        for action_idx in range(num_legal_actions):\n            self._add_avstrat(info_state_key, action_idx, policy[action_idx])\n    return value",
            "def _update_regrets(self, state, player):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Runs an episode of external sampling.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      player: the player to update regrets for\\n\\n    Returns:\\n      value: is the value of the state in the game\\n      obtained as the weighted average of the values\\n      of the children\\n    '\n    if state.is_terminal():\n        return state.player_return(player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        outcome = np.random.choice(outcomes, p=probs)\n        return self._update_regrets(state.child(outcome), player)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    value = 0\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    if cur_player != player:\n        action_idx = np.random.choice(np.arange(num_legal_actions), p=policy)\n        value = self._update_regrets(state.child(legal_actions[action_idx]), player)\n    else:\n        for action_idx in range(num_legal_actions):\n            child_values[action_idx] = self._update_regrets(state.child(legal_actions[action_idx]), player)\n            value += policy[action_idx] * child_values[action_idx]\n    if cur_player == player:\n        for action_idx in range(num_legal_actions):\n            self._add_regret(info_state_key, action_idx, child_values[action_idx] - value)\n    if self._average_type == AverageType.SIMPLE and cur_player == (player + 1) % self._num_players:\n        for action_idx in range(num_legal_actions):\n            self._add_avstrat(info_state_key, action_idx, policy[action_idx])\n    return value",
            "def _update_regrets(self, state, player):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Runs an episode of external sampling.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      player: the player to update regrets for\\n\\n    Returns:\\n      value: is the value of the state in the game\\n      obtained as the weighted average of the values\\n      of the children\\n    '\n    if state.is_terminal():\n        return state.player_return(player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        outcome = np.random.choice(outcomes, p=probs)\n        return self._update_regrets(state.child(outcome), player)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    value = 0\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    if cur_player != player:\n        action_idx = np.random.choice(np.arange(num_legal_actions), p=policy)\n        value = self._update_regrets(state.child(legal_actions[action_idx]), player)\n    else:\n        for action_idx in range(num_legal_actions):\n            child_values[action_idx] = self._update_regrets(state.child(legal_actions[action_idx]), player)\n            value += policy[action_idx] * child_values[action_idx]\n    if cur_player == player:\n        for action_idx in range(num_legal_actions):\n            self._add_regret(info_state_key, action_idx, child_values[action_idx] - value)\n    if self._average_type == AverageType.SIMPLE and cur_player == (player + 1) % self._num_players:\n        for action_idx in range(num_legal_actions):\n            self._add_avstrat(info_state_key, action_idx, policy[action_idx])\n    return value",
            "def _update_regrets(self, state, player):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Runs an episode of external sampling.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      player: the player to update regrets for\\n\\n    Returns:\\n      value: is the value of the state in the game\\n      obtained as the weighted average of the values\\n      of the children\\n    '\n    if state.is_terminal():\n        return state.player_return(player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        outcome = np.random.choice(outcomes, p=probs)\n        return self._update_regrets(state.child(outcome), player)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    value = 0\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    if cur_player != player:\n        action_idx = np.random.choice(np.arange(num_legal_actions), p=policy)\n        value = self._update_regrets(state.child(legal_actions[action_idx]), player)\n    else:\n        for action_idx in range(num_legal_actions):\n            child_values[action_idx] = self._update_regrets(state.child(legal_actions[action_idx]), player)\n            value += policy[action_idx] * child_values[action_idx]\n    if cur_player == player:\n        for action_idx in range(num_legal_actions):\n            self._add_regret(info_state_key, action_idx, child_values[action_idx] - value)\n    if self._average_type == AverageType.SIMPLE and cur_player == (player + 1) % self._num_players:\n        for action_idx in range(num_legal_actions):\n            self._add_avstrat(info_state_key, action_idx, policy[action_idx])\n    return value",
            "def _update_regrets(self, state, player):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Runs an episode of external sampling.\\n\\n    Args:\\n      state: the open spiel state to run from\\n      player: the player to update regrets for\\n\\n    Returns:\\n      value: is the value of the state in the game\\n      obtained as the weighted average of the values\\n      of the children\\n    '\n    if state.is_terminal():\n        return state.player_return(player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        outcome = np.random.choice(outcomes, p=probs)\n        return self._update_regrets(state.child(outcome), player)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    value = 0\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    if cur_player != player:\n        action_idx = np.random.choice(np.arange(num_legal_actions), p=policy)\n        value = self._update_regrets(state.child(legal_actions[action_idx]), player)\n    else:\n        for action_idx in range(num_legal_actions):\n            child_values[action_idx] = self._update_regrets(state.child(legal_actions[action_idx]), player)\n            value += policy[action_idx] * child_values[action_idx]\n    if cur_player == player:\n        for action_idx in range(num_legal_actions):\n            self._add_regret(info_state_key, action_idx, child_values[action_idx] - value)\n    if self._average_type == AverageType.SIMPLE and cur_player == (player + 1) % self._num_players:\n        for action_idx in range(num_legal_actions):\n            self._add_avstrat(info_state_key, action_idx, policy[action_idx])\n    return value"
        ]
    }
]