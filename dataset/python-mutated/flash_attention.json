[
    {
        "func_name": "sdp_kernel",
        "original": "@signature_safe_contextmanager\ndef sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=True):\n    \"\"\"\n    With the sdp_kernel context manager, different algorithm implementations can\n    be selected for scaled_dot_product_attention.\n    \"\"\"\n    global g_enable_math, g_enable_flash, g_enable_mem_efficient\n    original_enable_math = g_enable_math\n    original_enable_flash = g_enable_math\n    original_enable_mem_efficient = g_enable_mem_efficient\n    g_enable_math = enable_math\n    g_enable_flash = enable_flash\n    g_enable_mem_efficient = enable_mem_efficient\n    try:\n        yield\n    finally:\n        g_enable_math = original_enable_math\n        g_enable_flash = original_enable_flash\n        g_enable_mem_efficient = original_enable_mem_efficient",
        "mutated": [
            "@signature_safe_contextmanager\ndef sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=True):\n    if False:\n        i = 10\n    '\\n    With the sdp_kernel context manager, different algorithm implementations can\\n    be selected for scaled_dot_product_attention.\\n    '\n    global g_enable_math, g_enable_flash, g_enable_mem_efficient\n    original_enable_math = g_enable_math\n    original_enable_flash = g_enable_math\n    original_enable_mem_efficient = g_enable_mem_efficient\n    g_enable_math = enable_math\n    g_enable_flash = enable_flash\n    g_enable_mem_efficient = enable_mem_efficient\n    try:\n        yield\n    finally:\n        g_enable_math = original_enable_math\n        g_enable_flash = original_enable_flash\n        g_enable_mem_efficient = original_enable_mem_efficient",
            "@signature_safe_contextmanager\ndef sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    With the sdp_kernel context manager, different algorithm implementations can\\n    be selected for scaled_dot_product_attention.\\n    '\n    global g_enable_math, g_enable_flash, g_enable_mem_efficient\n    original_enable_math = g_enable_math\n    original_enable_flash = g_enable_math\n    original_enable_mem_efficient = g_enable_mem_efficient\n    g_enable_math = enable_math\n    g_enable_flash = enable_flash\n    g_enable_mem_efficient = enable_mem_efficient\n    try:\n        yield\n    finally:\n        g_enable_math = original_enable_math\n        g_enable_flash = original_enable_flash\n        g_enable_mem_efficient = original_enable_mem_efficient",
            "@signature_safe_contextmanager\ndef sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    With the sdp_kernel context manager, different algorithm implementations can\\n    be selected for scaled_dot_product_attention.\\n    '\n    global g_enable_math, g_enable_flash, g_enable_mem_efficient\n    original_enable_math = g_enable_math\n    original_enable_flash = g_enable_math\n    original_enable_mem_efficient = g_enable_mem_efficient\n    g_enable_math = enable_math\n    g_enable_flash = enable_flash\n    g_enable_mem_efficient = enable_mem_efficient\n    try:\n        yield\n    finally:\n        g_enable_math = original_enable_math\n        g_enable_flash = original_enable_flash\n        g_enable_mem_efficient = original_enable_mem_efficient",
            "@signature_safe_contextmanager\ndef sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    With the sdp_kernel context manager, different algorithm implementations can\\n    be selected for scaled_dot_product_attention.\\n    '\n    global g_enable_math, g_enable_flash, g_enable_mem_efficient\n    original_enable_math = g_enable_math\n    original_enable_flash = g_enable_math\n    original_enable_mem_efficient = g_enable_mem_efficient\n    g_enable_math = enable_math\n    g_enable_flash = enable_flash\n    g_enable_mem_efficient = enable_mem_efficient\n    try:\n        yield\n    finally:\n        g_enable_math = original_enable_math\n        g_enable_flash = original_enable_flash\n        g_enable_mem_efficient = original_enable_mem_efficient",
            "@signature_safe_contextmanager\ndef sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    With the sdp_kernel context manager, different algorithm implementations can\\n    be selected for scaled_dot_product_attention.\\n    '\n    global g_enable_math, g_enable_flash, g_enable_mem_efficient\n    original_enable_math = g_enable_math\n    original_enable_flash = g_enable_math\n    original_enable_mem_efficient = g_enable_mem_efficient\n    g_enable_math = enable_math\n    g_enable_flash = enable_flash\n    g_enable_mem_efficient = enable_mem_efficient\n    try:\n        yield\n    finally:\n        g_enable_math = original_enable_math\n        g_enable_flash = original_enable_flash\n        g_enable_mem_efficient = original_enable_mem_efficient"
        ]
    },
    {
        "func_name": "get_triangle_upper_mask",
        "original": "def get_triangle_upper_mask(x):\n    mask = paddle.full_like(x, -10000.0)\n    mask.stop_gradient = True\n    mask = paddle.triu(mask, diagonal=1)\n    mask.stop_gradient = True\n    return mask",
        "mutated": [
            "def get_triangle_upper_mask(x):\n    if False:\n        i = 10\n    mask = paddle.full_like(x, -10000.0)\n    mask.stop_gradient = True\n    mask = paddle.triu(mask, diagonal=1)\n    mask.stop_gradient = True\n    return mask",
            "def get_triangle_upper_mask(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mask = paddle.full_like(x, -10000.0)\n    mask.stop_gradient = True\n    mask = paddle.triu(mask, diagonal=1)\n    mask.stop_gradient = True\n    return mask",
            "def get_triangle_upper_mask(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mask = paddle.full_like(x, -10000.0)\n    mask.stop_gradient = True\n    mask = paddle.triu(mask, diagonal=1)\n    mask.stop_gradient = True\n    return mask",
            "def get_triangle_upper_mask(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mask = paddle.full_like(x, -10000.0)\n    mask.stop_gradient = True\n    mask = paddle.triu(mask, diagonal=1)\n    mask.stop_gradient = True\n    return mask",
            "def get_triangle_upper_mask(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mask = paddle.full_like(x, -10000.0)\n    mask.stop_gradient = True\n    mask = paddle.triu(mask, diagonal=1)\n    mask.stop_gradient = True\n    return mask"
        ]
    },
    {
        "func_name": "_math_attention",
        "original": "def _math_attention(query, key, value, dropout_rate=0.0, causal=False, return_softmax=False, training=True):\n    \"\"\"\n    This is a basic implementation of scaled dot product attention composed of\n    combinations of fundamental components.\n    \"\"\"\n    head_dim = query.shape[-1]\n    query = paddle.transpose(query, [0, 2, 1, 3])\n    key = paddle.transpose(key, [0, 2, 1, 3])\n    value = paddle.transpose(value, [0, 2, 1, 3])\n    product = paddle.matmul(x=query * head_dim ** (-0.5), y=key, transpose_y=True)\n    if not causal:\n        weights = F.softmax(product)\n    else:\n        place = paddle.get_device()\n        if 'xpu' in place:\n            mask = get_triangle_upper_mask(product)\n            product = product + mask\n            weights = F.softmax(product)\n        else:\n            weights = paddle.incubate.softmax_mask_fuse_upper_triangle(product)\n    if dropout_rate > 0.0:\n        weights = F.dropout(weights, dropout_rate, training=training, mode='upscale_in_train')\n    out = paddle.matmul(weights, value)\n    out = paddle.transpose(out, [0, 2, 1, 3])\n    return (out, weights if return_softmax else None)",
        "mutated": [
            "def _math_attention(query, key, value, dropout_rate=0.0, causal=False, return_softmax=False, training=True):\n    if False:\n        i = 10\n    '\\n    This is a basic implementation of scaled dot product attention composed of\\n    combinations of fundamental components.\\n    '\n    head_dim = query.shape[-1]\n    query = paddle.transpose(query, [0, 2, 1, 3])\n    key = paddle.transpose(key, [0, 2, 1, 3])\n    value = paddle.transpose(value, [0, 2, 1, 3])\n    product = paddle.matmul(x=query * head_dim ** (-0.5), y=key, transpose_y=True)\n    if not causal:\n        weights = F.softmax(product)\n    else:\n        place = paddle.get_device()\n        if 'xpu' in place:\n            mask = get_triangle_upper_mask(product)\n            product = product + mask\n            weights = F.softmax(product)\n        else:\n            weights = paddle.incubate.softmax_mask_fuse_upper_triangle(product)\n    if dropout_rate > 0.0:\n        weights = F.dropout(weights, dropout_rate, training=training, mode='upscale_in_train')\n    out = paddle.matmul(weights, value)\n    out = paddle.transpose(out, [0, 2, 1, 3])\n    return (out, weights if return_softmax else None)",
            "def _math_attention(query, key, value, dropout_rate=0.0, causal=False, return_softmax=False, training=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This is a basic implementation of scaled dot product attention composed of\\n    combinations of fundamental components.\\n    '\n    head_dim = query.shape[-1]\n    query = paddle.transpose(query, [0, 2, 1, 3])\n    key = paddle.transpose(key, [0, 2, 1, 3])\n    value = paddle.transpose(value, [0, 2, 1, 3])\n    product = paddle.matmul(x=query * head_dim ** (-0.5), y=key, transpose_y=True)\n    if not causal:\n        weights = F.softmax(product)\n    else:\n        place = paddle.get_device()\n        if 'xpu' in place:\n            mask = get_triangle_upper_mask(product)\n            product = product + mask\n            weights = F.softmax(product)\n        else:\n            weights = paddle.incubate.softmax_mask_fuse_upper_triangle(product)\n    if dropout_rate > 0.0:\n        weights = F.dropout(weights, dropout_rate, training=training, mode='upscale_in_train')\n    out = paddle.matmul(weights, value)\n    out = paddle.transpose(out, [0, 2, 1, 3])\n    return (out, weights if return_softmax else None)",
            "def _math_attention(query, key, value, dropout_rate=0.0, causal=False, return_softmax=False, training=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This is a basic implementation of scaled dot product attention composed of\\n    combinations of fundamental components.\\n    '\n    head_dim = query.shape[-1]\n    query = paddle.transpose(query, [0, 2, 1, 3])\n    key = paddle.transpose(key, [0, 2, 1, 3])\n    value = paddle.transpose(value, [0, 2, 1, 3])\n    product = paddle.matmul(x=query * head_dim ** (-0.5), y=key, transpose_y=True)\n    if not causal:\n        weights = F.softmax(product)\n    else:\n        place = paddle.get_device()\n        if 'xpu' in place:\n            mask = get_triangle_upper_mask(product)\n            product = product + mask\n            weights = F.softmax(product)\n        else:\n            weights = paddle.incubate.softmax_mask_fuse_upper_triangle(product)\n    if dropout_rate > 0.0:\n        weights = F.dropout(weights, dropout_rate, training=training, mode='upscale_in_train')\n    out = paddle.matmul(weights, value)\n    out = paddle.transpose(out, [0, 2, 1, 3])\n    return (out, weights if return_softmax else None)",
            "def _math_attention(query, key, value, dropout_rate=0.0, causal=False, return_softmax=False, training=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This is a basic implementation of scaled dot product attention composed of\\n    combinations of fundamental components.\\n    '\n    head_dim = query.shape[-1]\n    query = paddle.transpose(query, [0, 2, 1, 3])\n    key = paddle.transpose(key, [0, 2, 1, 3])\n    value = paddle.transpose(value, [0, 2, 1, 3])\n    product = paddle.matmul(x=query * head_dim ** (-0.5), y=key, transpose_y=True)\n    if not causal:\n        weights = F.softmax(product)\n    else:\n        place = paddle.get_device()\n        if 'xpu' in place:\n            mask = get_triangle_upper_mask(product)\n            product = product + mask\n            weights = F.softmax(product)\n        else:\n            weights = paddle.incubate.softmax_mask_fuse_upper_triangle(product)\n    if dropout_rate > 0.0:\n        weights = F.dropout(weights, dropout_rate, training=training, mode='upscale_in_train')\n    out = paddle.matmul(weights, value)\n    out = paddle.transpose(out, [0, 2, 1, 3])\n    return (out, weights if return_softmax else None)",
            "def _math_attention(query, key, value, dropout_rate=0.0, causal=False, return_softmax=False, training=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This is a basic implementation of scaled dot product attention composed of\\n    combinations of fundamental components.\\n    '\n    head_dim = query.shape[-1]\n    query = paddle.transpose(query, [0, 2, 1, 3])\n    key = paddle.transpose(key, [0, 2, 1, 3])\n    value = paddle.transpose(value, [0, 2, 1, 3])\n    product = paddle.matmul(x=query * head_dim ** (-0.5), y=key, transpose_y=True)\n    if not causal:\n        weights = F.softmax(product)\n    else:\n        place = paddle.get_device()\n        if 'xpu' in place:\n            mask = get_triangle_upper_mask(product)\n            product = product + mask\n            weights = F.softmax(product)\n        else:\n            weights = paddle.incubate.softmax_mask_fuse_upper_triangle(product)\n    if dropout_rate > 0.0:\n        weights = F.dropout(weights, dropout_rate, training=training, mode='upscale_in_train')\n    out = paddle.matmul(weights, value)\n    out = paddle.transpose(out, [0, 2, 1, 3])\n    return (out, weights if return_softmax else None)"
        ]
    },
    {
        "func_name": "_select_sdp_cuda",
        "original": "def _select_sdp_cuda(head_dim):\n    if head_dim <= 256:\n        return 'flash_attn'\n    else:\n        return 'mem_efficient'",
        "mutated": [
            "def _select_sdp_cuda(head_dim):\n    if False:\n        i = 10\n    if head_dim <= 256:\n        return 'flash_attn'\n    else:\n        return 'mem_efficient'",
            "def _select_sdp_cuda(head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if head_dim <= 256:\n        return 'flash_attn'\n    else:\n        return 'mem_efficient'",
            "def _select_sdp_cuda(head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if head_dim <= 256:\n        return 'flash_attn'\n    else:\n        return 'mem_efficient'",
            "def _select_sdp_cuda(head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if head_dim <= 256:\n        return 'flash_attn'\n    else:\n        return 'mem_efficient'",
            "def _select_sdp_cuda(head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if head_dim <= 256:\n        return 'flash_attn'\n    else:\n        return 'mem_efficient'"
        ]
    },
    {
        "func_name": "_select_sdp",
        "original": "def _select_sdp(head_dim):\n    \"\"\"\n    There are currently three different implementation options available for\n    scaled dot product attention, and the chosen approach depends on whether it\n    is determined by the sdp_kernel configuration or specified through input values.\n    \"\"\"\n    place = paddle.get_device()\n    if g_enable_flash is None:\n        if 'gpu' not in place:\n            return 'math'\n        else:\n            return _select_sdp_cuda(head_dim)\n    if g_enable_math is False and g_enable_flash is False and (g_enable_mem_efficient is False):\n        raise AssertionError('No available backend for scaled_dot_product_attention was found.')\n    if g_enable_math is True:\n        if g_enable_flash is False and g_enable_mem_efficient is False:\n            return 'math'\n        if 'gpu' not in place:\n            return 'math'\n    if g_enable_flash is True and g_enable_mem_efficient is True:\n        return _select_sdp_cuda(head_dim)\n    if g_enable_flash is True:\n        return 'flash_attn'\n    return 'mem_efficient'",
        "mutated": [
            "def _select_sdp(head_dim):\n    if False:\n        i = 10\n    '\\n    There are currently three different implementation options available for\\n    scaled dot product attention, and the chosen approach depends on whether it\\n    is determined by the sdp_kernel configuration or specified through input values.\\n    '\n    place = paddle.get_device()\n    if g_enable_flash is None:\n        if 'gpu' not in place:\n            return 'math'\n        else:\n            return _select_sdp_cuda(head_dim)\n    if g_enable_math is False and g_enable_flash is False and (g_enable_mem_efficient is False):\n        raise AssertionError('No available backend for scaled_dot_product_attention was found.')\n    if g_enable_math is True:\n        if g_enable_flash is False and g_enable_mem_efficient is False:\n            return 'math'\n        if 'gpu' not in place:\n            return 'math'\n    if g_enable_flash is True and g_enable_mem_efficient is True:\n        return _select_sdp_cuda(head_dim)\n    if g_enable_flash is True:\n        return 'flash_attn'\n    return 'mem_efficient'",
            "def _select_sdp(head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    There are currently three different implementation options available for\\n    scaled dot product attention, and the chosen approach depends on whether it\\n    is determined by the sdp_kernel configuration or specified through input values.\\n    '\n    place = paddle.get_device()\n    if g_enable_flash is None:\n        if 'gpu' not in place:\n            return 'math'\n        else:\n            return _select_sdp_cuda(head_dim)\n    if g_enable_math is False and g_enable_flash is False and (g_enable_mem_efficient is False):\n        raise AssertionError('No available backend for scaled_dot_product_attention was found.')\n    if g_enable_math is True:\n        if g_enable_flash is False and g_enable_mem_efficient is False:\n            return 'math'\n        if 'gpu' not in place:\n            return 'math'\n    if g_enable_flash is True and g_enable_mem_efficient is True:\n        return _select_sdp_cuda(head_dim)\n    if g_enable_flash is True:\n        return 'flash_attn'\n    return 'mem_efficient'",
            "def _select_sdp(head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    There are currently three different implementation options available for\\n    scaled dot product attention, and the chosen approach depends on whether it\\n    is determined by the sdp_kernel configuration or specified through input values.\\n    '\n    place = paddle.get_device()\n    if g_enable_flash is None:\n        if 'gpu' not in place:\n            return 'math'\n        else:\n            return _select_sdp_cuda(head_dim)\n    if g_enable_math is False and g_enable_flash is False and (g_enable_mem_efficient is False):\n        raise AssertionError('No available backend for scaled_dot_product_attention was found.')\n    if g_enable_math is True:\n        if g_enable_flash is False and g_enable_mem_efficient is False:\n            return 'math'\n        if 'gpu' not in place:\n            return 'math'\n    if g_enable_flash is True and g_enable_mem_efficient is True:\n        return _select_sdp_cuda(head_dim)\n    if g_enable_flash is True:\n        return 'flash_attn'\n    return 'mem_efficient'",
            "def _select_sdp(head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    There are currently three different implementation options available for\\n    scaled dot product attention, and the chosen approach depends on whether it\\n    is determined by the sdp_kernel configuration or specified through input values.\\n    '\n    place = paddle.get_device()\n    if g_enable_flash is None:\n        if 'gpu' not in place:\n            return 'math'\n        else:\n            return _select_sdp_cuda(head_dim)\n    if g_enable_math is False and g_enable_flash is False and (g_enable_mem_efficient is False):\n        raise AssertionError('No available backend for scaled_dot_product_attention was found.')\n    if g_enable_math is True:\n        if g_enable_flash is False and g_enable_mem_efficient is False:\n            return 'math'\n        if 'gpu' not in place:\n            return 'math'\n    if g_enable_flash is True and g_enable_mem_efficient is True:\n        return _select_sdp_cuda(head_dim)\n    if g_enable_flash is True:\n        return 'flash_attn'\n    return 'mem_efficient'",
            "def _select_sdp(head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    There are currently three different implementation options available for\\n    scaled dot product attention, and the chosen approach depends on whether it\\n    is determined by the sdp_kernel configuration or specified through input values.\\n    '\n    place = paddle.get_device()\n    if g_enable_flash is None:\n        if 'gpu' not in place:\n            return 'math'\n        else:\n            return _select_sdp_cuda(head_dim)\n    if g_enable_math is False and g_enable_flash is False and (g_enable_mem_efficient is False):\n        raise AssertionError('No available backend for scaled_dot_product_attention was found.')\n    if g_enable_math is True:\n        if g_enable_flash is False and g_enable_mem_efficient is False:\n            return 'math'\n        if 'gpu' not in place:\n            return 'math'\n    if g_enable_flash is True and g_enable_mem_efficient is True:\n        return _select_sdp_cuda(head_dim)\n    if g_enable_flash is True:\n        return 'flash_attn'\n    return 'mem_efficient'"
        ]
    },
    {
        "func_name": "flash_attention",
        "original": "def flash_attention(query, key, value, dropout=0.0, causal=False, return_softmax=False, *, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    \"\"\"\n    The equation is:\n\n    .. math::\n\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\n\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\n    The dimensions of the three parameters are the same.\n    ``d`` represents the size of the last dimension of the three parameters.\n\n    Warning:\n        This API is only support inputs with dtype float16 and bfloat16.\n\n    Args:\n        query(Tensor): The query tensor in the Attention module.\n                        4-D tensor with shape:\n                        [batch_size, seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        key(Tensor): The key tensor in the Attention module.\n                        4-D tensor with shape:\n                        [batch_size, seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        value(Tensor): The value tensor in the Attention module.\n                        4-D tensor with shape:\n                        [batch_size, seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        dropout(float): The dropout ratio.\n        causal(bool): Whether enable causal mode.\n        return_softmax(bool): Whether to return softmax.\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\n        training(bool): Whether it is in the training phase.\n        rng_name(str): The name to select Generator.\n        name(str, optional): The default value is None. Normally there is no need for user\n                        to set this property. For more information, please refer to\n                        :ref:`api_guide_Name`.\n\n    Returns:\n        out(Tensor): The attention tensor.\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\n                    The dtype can be float16 or bfloat16.\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> paddle.seed(2023)\n            >>> q = paddle.rand((1, 128, 2, 16))\n\n            >>> output = paddle.nn.functional.flash_attention.flash_attention(q, q, q, 0.9, False, False)\n            >>> print(output)\n            (Tensor(shape=[1, 128, 2, 16], dtype=float32, place=Place(cpu), stop_gradient=True,\n            [[[[0.34992966, 0.34456208, 0.45826620, ..., 0.39883569,\n                0.42132431, 0.39157745],\n               [0.76687670, 0.65837246, 0.69117945, ..., 0.82817286,\n                0.76690865, 0.71485823]],\n              ...,\n              [[0.71662450, 0.57275224, 0.57053083, ..., 0.48108247,\n                0.53336465, 0.54540104],\n               [0.59137970, 0.51350880, 0.50449550, ..., 0.38860250,\n                0.40526697, 0.60541755]]]]), None)\n\n    \"\"\"\n    head_dim = query.shape[3]\n    sdp_func_name = _select_sdp(head_dim)\n    if sdp_func_name == 'flash_attn':\n        if in_dynamic_mode():\n            (result_attention, result_softmax) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, None, dropout, causal, return_softmax, not training, rng_name)\n            return (result_attention, result_softmax if return_softmax else None)\n        helper = LayerHelper('flash_attn', **locals())\n        dtype = helper.input_dtype(input_param_name='q')\n        out = helper.create_variable_for_type_inference(dtype)\n        softmax = helper.create_variable_for_type_inference(dtype)\n        softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n        seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n        inputs = {'q': query, 'k': key, 'v': value, 'fixed_seed_offset': fixed_seed_offset}\n        outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n        helper.append_op(type='flash_attn', inputs=inputs, outputs=outputs, attrs={'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n        return (out, softmax if return_softmax else None)\n    elif sdp_func_name == 'mem_efficient':\n        from paddle.incubate.nn.memory_efficient_attention import memory_efficient_attention\n        output = memory_efficient_attention(query, key, value, attn_bias=None, p=dropout, scale=None, training=training)\n        return (output, None)\n    else:\n        return _math_attention(query, key, value, dropout_rate=dropout, causal=causal, return_softmax=return_softmax, training=training)",
        "mutated": [
            "def flash_attention(query, key, value, dropout=0.0, causal=False, return_softmax=False, *, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n    '\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        training(bool): Whether it is in the training phase.\\n        rng_name(str): The name to select Generator.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((1, 128, 2, 16))\\n\\n            >>> output = paddle.nn.functional.flash_attention.flash_attention(q, q, q, 0.9, False, False)\\n            >>> print(output)\\n            (Tensor(shape=[1, 128, 2, 16], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[[[0.34992966, 0.34456208, 0.45826620, ..., 0.39883569,\\n                0.42132431, 0.39157745],\\n               [0.76687670, 0.65837246, 0.69117945, ..., 0.82817286,\\n                0.76690865, 0.71485823]],\\n              ...,\\n              [[0.71662450, 0.57275224, 0.57053083, ..., 0.48108247,\\n                0.53336465, 0.54540104],\\n               [0.59137970, 0.51350880, 0.50449550, ..., 0.38860250,\\n                0.40526697, 0.60541755]]]]), None)\\n\\n    '\n    head_dim = query.shape[3]\n    sdp_func_name = _select_sdp(head_dim)\n    if sdp_func_name == 'flash_attn':\n        if in_dynamic_mode():\n            (result_attention, result_softmax) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, None, dropout, causal, return_softmax, not training, rng_name)\n            return (result_attention, result_softmax if return_softmax else None)\n        helper = LayerHelper('flash_attn', **locals())\n        dtype = helper.input_dtype(input_param_name='q')\n        out = helper.create_variable_for_type_inference(dtype)\n        softmax = helper.create_variable_for_type_inference(dtype)\n        softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n        seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n        inputs = {'q': query, 'k': key, 'v': value, 'fixed_seed_offset': fixed_seed_offset}\n        outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n        helper.append_op(type='flash_attn', inputs=inputs, outputs=outputs, attrs={'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n        return (out, softmax if return_softmax else None)\n    elif sdp_func_name == 'mem_efficient':\n        from paddle.incubate.nn.memory_efficient_attention import memory_efficient_attention\n        output = memory_efficient_attention(query, key, value, attn_bias=None, p=dropout, scale=None, training=training)\n        return (output, None)\n    else:\n        return _math_attention(query, key, value, dropout_rate=dropout, causal=causal, return_softmax=return_softmax, training=training)",
            "def flash_attention(query, key, value, dropout=0.0, causal=False, return_softmax=False, *, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        training(bool): Whether it is in the training phase.\\n        rng_name(str): The name to select Generator.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((1, 128, 2, 16))\\n\\n            >>> output = paddle.nn.functional.flash_attention.flash_attention(q, q, q, 0.9, False, False)\\n            >>> print(output)\\n            (Tensor(shape=[1, 128, 2, 16], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[[[0.34992966, 0.34456208, 0.45826620, ..., 0.39883569,\\n                0.42132431, 0.39157745],\\n               [0.76687670, 0.65837246, 0.69117945, ..., 0.82817286,\\n                0.76690865, 0.71485823]],\\n              ...,\\n              [[0.71662450, 0.57275224, 0.57053083, ..., 0.48108247,\\n                0.53336465, 0.54540104],\\n               [0.59137970, 0.51350880, 0.50449550, ..., 0.38860250,\\n                0.40526697, 0.60541755]]]]), None)\\n\\n    '\n    head_dim = query.shape[3]\n    sdp_func_name = _select_sdp(head_dim)\n    if sdp_func_name == 'flash_attn':\n        if in_dynamic_mode():\n            (result_attention, result_softmax) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, None, dropout, causal, return_softmax, not training, rng_name)\n            return (result_attention, result_softmax if return_softmax else None)\n        helper = LayerHelper('flash_attn', **locals())\n        dtype = helper.input_dtype(input_param_name='q')\n        out = helper.create_variable_for_type_inference(dtype)\n        softmax = helper.create_variable_for_type_inference(dtype)\n        softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n        seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n        inputs = {'q': query, 'k': key, 'v': value, 'fixed_seed_offset': fixed_seed_offset}\n        outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n        helper.append_op(type='flash_attn', inputs=inputs, outputs=outputs, attrs={'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n        return (out, softmax if return_softmax else None)\n    elif sdp_func_name == 'mem_efficient':\n        from paddle.incubate.nn.memory_efficient_attention import memory_efficient_attention\n        output = memory_efficient_attention(query, key, value, attn_bias=None, p=dropout, scale=None, training=training)\n        return (output, None)\n    else:\n        return _math_attention(query, key, value, dropout_rate=dropout, causal=causal, return_softmax=return_softmax, training=training)",
            "def flash_attention(query, key, value, dropout=0.0, causal=False, return_softmax=False, *, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        training(bool): Whether it is in the training phase.\\n        rng_name(str): The name to select Generator.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((1, 128, 2, 16))\\n\\n            >>> output = paddle.nn.functional.flash_attention.flash_attention(q, q, q, 0.9, False, False)\\n            >>> print(output)\\n            (Tensor(shape=[1, 128, 2, 16], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[[[0.34992966, 0.34456208, 0.45826620, ..., 0.39883569,\\n                0.42132431, 0.39157745],\\n               [0.76687670, 0.65837246, 0.69117945, ..., 0.82817286,\\n                0.76690865, 0.71485823]],\\n              ...,\\n              [[0.71662450, 0.57275224, 0.57053083, ..., 0.48108247,\\n                0.53336465, 0.54540104],\\n               [0.59137970, 0.51350880, 0.50449550, ..., 0.38860250,\\n                0.40526697, 0.60541755]]]]), None)\\n\\n    '\n    head_dim = query.shape[3]\n    sdp_func_name = _select_sdp(head_dim)\n    if sdp_func_name == 'flash_attn':\n        if in_dynamic_mode():\n            (result_attention, result_softmax) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, None, dropout, causal, return_softmax, not training, rng_name)\n            return (result_attention, result_softmax if return_softmax else None)\n        helper = LayerHelper('flash_attn', **locals())\n        dtype = helper.input_dtype(input_param_name='q')\n        out = helper.create_variable_for_type_inference(dtype)\n        softmax = helper.create_variable_for_type_inference(dtype)\n        softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n        seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n        inputs = {'q': query, 'k': key, 'v': value, 'fixed_seed_offset': fixed_seed_offset}\n        outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n        helper.append_op(type='flash_attn', inputs=inputs, outputs=outputs, attrs={'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n        return (out, softmax if return_softmax else None)\n    elif sdp_func_name == 'mem_efficient':\n        from paddle.incubate.nn.memory_efficient_attention import memory_efficient_attention\n        output = memory_efficient_attention(query, key, value, attn_bias=None, p=dropout, scale=None, training=training)\n        return (output, None)\n    else:\n        return _math_attention(query, key, value, dropout_rate=dropout, causal=causal, return_softmax=return_softmax, training=training)",
            "def flash_attention(query, key, value, dropout=0.0, causal=False, return_softmax=False, *, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        training(bool): Whether it is in the training phase.\\n        rng_name(str): The name to select Generator.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((1, 128, 2, 16))\\n\\n            >>> output = paddle.nn.functional.flash_attention.flash_attention(q, q, q, 0.9, False, False)\\n            >>> print(output)\\n            (Tensor(shape=[1, 128, 2, 16], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[[[0.34992966, 0.34456208, 0.45826620, ..., 0.39883569,\\n                0.42132431, 0.39157745],\\n               [0.76687670, 0.65837246, 0.69117945, ..., 0.82817286,\\n                0.76690865, 0.71485823]],\\n              ...,\\n              [[0.71662450, 0.57275224, 0.57053083, ..., 0.48108247,\\n                0.53336465, 0.54540104],\\n               [0.59137970, 0.51350880, 0.50449550, ..., 0.38860250,\\n                0.40526697, 0.60541755]]]]), None)\\n\\n    '\n    head_dim = query.shape[3]\n    sdp_func_name = _select_sdp(head_dim)\n    if sdp_func_name == 'flash_attn':\n        if in_dynamic_mode():\n            (result_attention, result_softmax) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, None, dropout, causal, return_softmax, not training, rng_name)\n            return (result_attention, result_softmax if return_softmax else None)\n        helper = LayerHelper('flash_attn', **locals())\n        dtype = helper.input_dtype(input_param_name='q')\n        out = helper.create_variable_for_type_inference(dtype)\n        softmax = helper.create_variable_for_type_inference(dtype)\n        softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n        seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n        inputs = {'q': query, 'k': key, 'v': value, 'fixed_seed_offset': fixed_seed_offset}\n        outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n        helper.append_op(type='flash_attn', inputs=inputs, outputs=outputs, attrs={'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n        return (out, softmax if return_softmax else None)\n    elif sdp_func_name == 'mem_efficient':\n        from paddle.incubate.nn.memory_efficient_attention import memory_efficient_attention\n        output = memory_efficient_attention(query, key, value, attn_bias=None, p=dropout, scale=None, training=training)\n        return (output, None)\n    else:\n        return _math_attention(query, key, value, dropout_rate=dropout, causal=causal, return_softmax=return_softmax, training=training)",
            "def flash_attention(query, key, value, dropout=0.0, causal=False, return_softmax=False, *, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        training(bool): Whether it is in the training phase.\\n        rng_name(str): The name to select Generator.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((1, 128, 2, 16))\\n\\n            >>> output = paddle.nn.functional.flash_attention.flash_attention(q, q, q, 0.9, False, False)\\n            >>> print(output)\\n            (Tensor(shape=[1, 128, 2, 16], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[[[0.34992966, 0.34456208, 0.45826620, ..., 0.39883569,\\n                0.42132431, 0.39157745],\\n               [0.76687670, 0.65837246, 0.69117945, ..., 0.82817286,\\n                0.76690865, 0.71485823]],\\n              ...,\\n              [[0.71662450, 0.57275224, 0.57053083, ..., 0.48108247,\\n                0.53336465, 0.54540104],\\n               [0.59137970, 0.51350880, 0.50449550, ..., 0.38860250,\\n                0.40526697, 0.60541755]]]]), None)\\n\\n    '\n    head_dim = query.shape[3]\n    sdp_func_name = _select_sdp(head_dim)\n    if sdp_func_name == 'flash_attn':\n        if in_dynamic_mode():\n            (result_attention, result_softmax) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, None, dropout, causal, return_softmax, not training, rng_name)\n            return (result_attention, result_softmax if return_softmax else None)\n        helper = LayerHelper('flash_attn', **locals())\n        dtype = helper.input_dtype(input_param_name='q')\n        out = helper.create_variable_for_type_inference(dtype)\n        softmax = helper.create_variable_for_type_inference(dtype)\n        softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n        seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n        inputs = {'q': query, 'k': key, 'v': value, 'fixed_seed_offset': fixed_seed_offset}\n        outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n        helper.append_op(type='flash_attn', inputs=inputs, outputs=outputs, attrs={'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n        return (out, softmax if return_softmax else None)\n    elif sdp_func_name == 'mem_efficient':\n        from paddle.incubate.nn.memory_efficient_attention import memory_efficient_attention\n        output = memory_efficient_attention(query, key, value, attn_bias=None, p=dropout, scale=None, training=training)\n        return (output, None)\n    else:\n        return _math_attention(query, key, value, dropout_rate=dropout, causal=causal, return_softmax=return_softmax, training=training)"
        ]
    },
    {
        "func_name": "flash_attn_unpadded",
        "original": "def flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, scale, dropout=0.0, causal=False, return_softmax=False, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    \"\"\"\n    The equation is:\n\n    .. math::\n\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\n\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\n    The dimensions of the three parameters are the same.\n    ``d`` represents the size of the last dimension of the three parameters.\n\n    Warning:\n        This API is only support inputs with dtype float16 and bfloat16.\n\n    Args:\n        query(Tensor): The query tensor in the Attention module.\n                        3-D tensor with shape:\n                        [total_seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        key(Tensor): The key tensor in the Attention module.\n                        3-D tensor with shape:\n                        [total_seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        value(Tensor): The value tensor in the Attention module.\n                        3-D tensor with shape:\n                        [total_seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        cu_seqlens_q(Tensor): The cumulative sequence lengths of the sequences in the batch,\n                        used to index query.\n        cu_seqlens_k(Tensor): The cumulative sequence lengths of the sequences in the batch,\n                        used to index key and value.\n        max_seqlen_q(int): Maximum sequence length of query in the batch.\n        max_seqlen_k(int): Maximum sequence length of key/value in the batch.\n        scale(float): The scaling of QK^T before applying softmax.\n        dropout(float): The dropout ratio.\n        causal(bool): Whether enable causal mode.\n        return_softmax(bool): Whether to return softmax.\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\n        rng_name(str): The name to select Generator.\n        training(bool): Whether it is in the training phase.\n        name(str, optional): The default value is None. Normally there is no need for user\n                        to set this property. For more information, please refer to\n                        :ref:`api_guide_Name`.\n\n    Returns:\n        out(Tensor): The attention tensor.\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\n                    The dtype can be float16 or bfloat16.\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n            >>> paddle.seed(2023)\n            >>> q = paddle.rand((2, 128, 8, 16), dtype='float16')\n            >>> cu = paddle.arange(0, 384, 128, dtype='int32')\n            >>> qq = paddle.reshape(q, [256, 8, 16])\n            >>> output = paddle.nn.functional.flash_attention.flash_attn_unpadded(qq, qq, qq, cu, cu, 128, 128, 0.25, 0.0, False, False)\n\n    \"\"\"\n    if in_dynamic_mode():\n        (result_attention, result_softmax) = _C_ops.flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, fixed_seed_offset, None, max_seqlen_q, max_seqlen_k, scale, dropout, causal, return_softmax, not training, rng_name)\n        return (result_attention, result_softmax if return_softmax else None)\n    helper = LayerHelper('flash_attn_unpadded', **locals())\n    dtype = helper.input_dtype(input_param_name='q')\n    out = helper.create_variable_for_type_inference(dtype)\n    softmax = helper.create_variable_for_type_inference(dtype)\n    softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n    seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n    inputs = {'q': query, 'k': key, 'v': value, 'cu_seqlens_q': cu_seqlens_q, 'cu_seqlens_k': cu_seqlens_k, 'fixed_seed_offset': fixed_seed_offset}\n    outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n    helper.append_op(type='flash_attn_unpadded', inputs=inputs, outputs=outputs, attrs={'max_seqlen_q': max_seqlen_q, 'max_seqlen_k': max_seqlen_k, 'scale': scale, 'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n    return (out, softmax if return_softmax else None)",
        "mutated": [
            "def flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, scale, dropout=0.0, causal=False, return_softmax=False, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        cu_seqlens_q(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index query.\\n        cu_seqlens_k(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index key and value.\\n        max_seqlen_q(int): Maximum sequence length of query in the batch.\\n        max_seqlen_k(int): Maximum sequence length of key/value in the batch.\\n        scale(float): The scaling of QK^T before applying softmax.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        rng_name(str): The name to select Generator.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((2, 128, 8, 16), dtype='float16')\\n            >>> cu = paddle.arange(0, 384, 128, dtype='int32')\\n            >>> qq = paddle.reshape(q, [256, 8, 16])\\n            >>> output = paddle.nn.functional.flash_attention.flash_attn_unpadded(qq, qq, qq, cu, cu, 128, 128, 0.25, 0.0, False, False)\\n\\n    \"\n    if in_dynamic_mode():\n        (result_attention, result_softmax) = _C_ops.flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, fixed_seed_offset, None, max_seqlen_q, max_seqlen_k, scale, dropout, causal, return_softmax, not training, rng_name)\n        return (result_attention, result_softmax if return_softmax else None)\n    helper = LayerHelper('flash_attn_unpadded', **locals())\n    dtype = helper.input_dtype(input_param_name='q')\n    out = helper.create_variable_for_type_inference(dtype)\n    softmax = helper.create_variable_for_type_inference(dtype)\n    softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n    seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n    inputs = {'q': query, 'k': key, 'v': value, 'cu_seqlens_q': cu_seqlens_q, 'cu_seqlens_k': cu_seqlens_k, 'fixed_seed_offset': fixed_seed_offset}\n    outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n    helper.append_op(type='flash_attn_unpadded', inputs=inputs, outputs=outputs, attrs={'max_seqlen_q': max_seqlen_q, 'max_seqlen_k': max_seqlen_k, 'scale': scale, 'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n    return (out, softmax if return_softmax else None)",
            "def flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, scale, dropout=0.0, causal=False, return_softmax=False, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        cu_seqlens_q(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index query.\\n        cu_seqlens_k(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index key and value.\\n        max_seqlen_q(int): Maximum sequence length of query in the batch.\\n        max_seqlen_k(int): Maximum sequence length of key/value in the batch.\\n        scale(float): The scaling of QK^T before applying softmax.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        rng_name(str): The name to select Generator.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((2, 128, 8, 16), dtype='float16')\\n            >>> cu = paddle.arange(0, 384, 128, dtype='int32')\\n            >>> qq = paddle.reshape(q, [256, 8, 16])\\n            >>> output = paddle.nn.functional.flash_attention.flash_attn_unpadded(qq, qq, qq, cu, cu, 128, 128, 0.25, 0.0, False, False)\\n\\n    \"\n    if in_dynamic_mode():\n        (result_attention, result_softmax) = _C_ops.flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, fixed_seed_offset, None, max_seqlen_q, max_seqlen_k, scale, dropout, causal, return_softmax, not training, rng_name)\n        return (result_attention, result_softmax if return_softmax else None)\n    helper = LayerHelper('flash_attn_unpadded', **locals())\n    dtype = helper.input_dtype(input_param_name='q')\n    out = helper.create_variable_for_type_inference(dtype)\n    softmax = helper.create_variable_for_type_inference(dtype)\n    softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n    seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n    inputs = {'q': query, 'k': key, 'v': value, 'cu_seqlens_q': cu_seqlens_q, 'cu_seqlens_k': cu_seqlens_k, 'fixed_seed_offset': fixed_seed_offset}\n    outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n    helper.append_op(type='flash_attn_unpadded', inputs=inputs, outputs=outputs, attrs={'max_seqlen_q': max_seqlen_q, 'max_seqlen_k': max_seqlen_k, 'scale': scale, 'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n    return (out, softmax if return_softmax else None)",
            "def flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, scale, dropout=0.0, causal=False, return_softmax=False, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        cu_seqlens_q(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index query.\\n        cu_seqlens_k(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index key and value.\\n        max_seqlen_q(int): Maximum sequence length of query in the batch.\\n        max_seqlen_k(int): Maximum sequence length of key/value in the batch.\\n        scale(float): The scaling of QK^T before applying softmax.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        rng_name(str): The name to select Generator.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((2, 128, 8, 16), dtype='float16')\\n            >>> cu = paddle.arange(0, 384, 128, dtype='int32')\\n            >>> qq = paddle.reshape(q, [256, 8, 16])\\n            >>> output = paddle.nn.functional.flash_attention.flash_attn_unpadded(qq, qq, qq, cu, cu, 128, 128, 0.25, 0.0, False, False)\\n\\n    \"\n    if in_dynamic_mode():\n        (result_attention, result_softmax) = _C_ops.flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, fixed_seed_offset, None, max_seqlen_q, max_seqlen_k, scale, dropout, causal, return_softmax, not training, rng_name)\n        return (result_attention, result_softmax if return_softmax else None)\n    helper = LayerHelper('flash_attn_unpadded', **locals())\n    dtype = helper.input_dtype(input_param_name='q')\n    out = helper.create_variable_for_type_inference(dtype)\n    softmax = helper.create_variable_for_type_inference(dtype)\n    softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n    seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n    inputs = {'q': query, 'k': key, 'v': value, 'cu_seqlens_q': cu_seqlens_q, 'cu_seqlens_k': cu_seqlens_k, 'fixed_seed_offset': fixed_seed_offset}\n    outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n    helper.append_op(type='flash_attn_unpadded', inputs=inputs, outputs=outputs, attrs={'max_seqlen_q': max_seqlen_q, 'max_seqlen_k': max_seqlen_k, 'scale': scale, 'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n    return (out, softmax if return_softmax else None)",
            "def flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, scale, dropout=0.0, causal=False, return_softmax=False, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        cu_seqlens_q(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index query.\\n        cu_seqlens_k(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index key and value.\\n        max_seqlen_q(int): Maximum sequence length of query in the batch.\\n        max_seqlen_k(int): Maximum sequence length of key/value in the batch.\\n        scale(float): The scaling of QK^T before applying softmax.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        rng_name(str): The name to select Generator.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((2, 128, 8, 16), dtype='float16')\\n            >>> cu = paddle.arange(0, 384, 128, dtype='int32')\\n            >>> qq = paddle.reshape(q, [256, 8, 16])\\n            >>> output = paddle.nn.functional.flash_attention.flash_attn_unpadded(qq, qq, qq, cu, cu, 128, 128, 0.25, 0.0, False, False)\\n\\n    \"\n    if in_dynamic_mode():\n        (result_attention, result_softmax) = _C_ops.flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, fixed_seed_offset, None, max_seqlen_q, max_seqlen_k, scale, dropout, causal, return_softmax, not training, rng_name)\n        return (result_attention, result_softmax if return_softmax else None)\n    helper = LayerHelper('flash_attn_unpadded', **locals())\n    dtype = helper.input_dtype(input_param_name='q')\n    out = helper.create_variable_for_type_inference(dtype)\n    softmax = helper.create_variable_for_type_inference(dtype)\n    softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n    seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n    inputs = {'q': query, 'k': key, 'v': value, 'cu_seqlens_q': cu_seqlens_q, 'cu_seqlens_k': cu_seqlens_k, 'fixed_seed_offset': fixed_seed_offset}\n    outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n    helper.append_op(type='flash_attn_unpadded', inputs=inputs, outputs=outputs, attrs={'max_seqlen_q': max_seqlen_q, 'max_seqlen_k': max_seqlen_k, 'scale': scale, 'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n    return (out, softmax if return_softmax else None)",
            "def flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, scale, dropout=0.0, causal=False, return_softmax=False, fixed_seed_offset=None, rng_name='', training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API is only support inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        3-D tensor with shape:\\n                        [total_seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        cu_seqlens_q(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index query.\\n        cu_seqlens_k(Tensor): The cumulative sequence lengths of the sequences in the batch,\\n                        used to index key and value.\\n        max_seqlen_q(int): Maximum sequence length of query in the batch.\\n        max_seqlen_k(int): Maximum sequence length of key/value in the batch.\\n        scale(float): The scaling of QK^T before applying softmax.\\n        dropout(float): The dropout ratio.\\n        causal(bool): Whether enable causal mode.\\n        return_softmax(bool): Whether to return softmax.\\n        fixed_seed_offset(Tensor, optional): With fixed seed, offset for dropout mask.\\n        rng_name(str): The name to select Generator.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n        softmax(Tensor): The softmax tensor. None if return_softmax is False.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> paddle.seed(2023)\\n            >>> q = paddle.rand((2, 128, 8, 16), dtype='float16')\\n            >>> cu = paddle.arange(0, 384, 128, dtype='int32')\\n            >>> qq = paddle.reshape(q, [256, 8, 16])\\n            >>> output = paddle.nn.functional.flash_attention.flash_attn_unpadded(qq, qq, qq, cu, cu, 128, 128, 0.25, 0.0, False, False)\\n\\n    \"\n    if in_dynamic_mode():\n        (result_attention, result_softmax) = _C_ops.flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, fixed_seed_offset, None, max_seqlen_q, max_seqlen_k, scale, dropout, causal, return_softmax, not training, rng_name)\n        return (result_attention, result_softmax if return_softmax else None)\n    helper = LayerHelper('flash_attn_unpadded', **locals())\n    dtype = helper.input_dtype(input_param_name='q')\n    out = helper.create_variable_for_type_inference(dtype)\n    softmax = helper.create_variable_for_type_inference(dtype)\n    softmax_lse = helper.create_variable_for_type_inference(paddle.float32)\n    seed_offset = helper.create_variable_for_type_inference(paddle.int64)\n    inputs = {'q': query, 'k': key, 'v': value, 'cu_seqlens_q': cu_seqlens_q, 'cu_seqlens_k': cu_seqlens_k, 'fixed_seed_offset': fixed_seed_offset}\n    outputs = {'out': out, 'softmax': softmax, 'softmax_lse': softmax_lse, 'seed_offset': seed_offset}\n    helper.append_op(type='flash_attn_unpadded', inputs=inputs, outputs=outputs, attrs={'max_seqlen_q': max_seqlen_q, 'max_seqlen_k': max_seqlen_k, 'scale': scale, 'dropout': dropout, 'causal': causal, 'return_softmax': return_softmax, 'is_test': not training, 'rng_name': rng_name})\n    return (out, softmax if return_softmax else None)"
        ]
    },
    {
        "func_name": "scaled_dot_product_attention",
        "original": "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, training=True, name=None):\n    \"\"\"\n    The equation is:\n\n    .. math::\n\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\n\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\n    The dimensions of the three parameters are the same.\n    ``d`` represents the size of the last dimension of the three parameters.\n\n    Warning:\n        This API only supports inputs with dtype float16 and bfloat16.\n\n    Args:\n        query(Tensor): The query tensor in the Attention module.\n                        4-D tensor with shape:\n                        [batch_size, seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        key(Tensor): The key tensor in the Attention module.\n                        4-D tensor with shape:\n                        [batch_size, seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        value(Tensor): The value tensor in the Attention module.\n                        4-D tensor with shape:\n                        [batch_size, seq_len, num_heads, head_dim].\n                        The dtype can be float61 or bfloat16.\n        attn_mask(Tensor,optional): A float mask of the same type as query,\n                        key, value that is added to the attention score.\n        dropout_p(float): The dropout ratio.\n        is_causal(bool): Whether enable causal mode.\n        training(bool): Whether it is in the training phase.\n        name(str, optional): The default value is None. Normally there is no need for user\n                        to set this property. For more information, please refer to\n                        :ref:`api_guide_Name`.\n\n    Returns:\n        out(Tensor): The attention tensor.\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\n                    The dtype can be float16 or bfloat16.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +SKIP('bfloat need V100 compile')\n            >>> import paddle\n            >>> q = paddle.rand((1, 128, 2, 16), dtype=paddle.bfloat16)\n            >>> output = paddle.nn.functional.scaled_dot_product_attention(q, q, q, None, 0.9, False)\n            >>> print(output)\n            >>> # doctest: -SKIP\n    \"\"\"\n    if attn_mask is None:\n        (out, _) = flash_attention(query, key, value, dropout_p, is_causal)\n    else:\n        fixed_seed_offset = (None,)\n        return_softmax = False\n        rng_name = ''\n        (out, _) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, attn_mask, dropout_p, is_causal, return_softmax, not training, rng_name)\n    return out",
        "mutated": [
            "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, training=True, name=None):\n    if False:\n        i = 10\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API only supports inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        attn_mask(Tensor,optional): A float mask of the same type as query,\\n                        key, value that is added to the attention score.\\n        dropout_p(float): The dropout ratio.\\n        is_causal(bool): Whether enable causal mode.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('bfloat need V100 compile')\\n            >>> import paddle\\n            >>> q = paddle.rand((1, 128, 2, 16), dtype=paddle.bfloat16)\\n            >>> output = paddle.nn.functional.scaled_dot_product_attention(q, q, q, None, 0.9, False)\\n            >>> print(output)\\n            >>> # doctest: -SKIP\\n    \"\n    if attn_mask is None:\n        (out, _) = flash_attention(query, key, value, dropout_p, is_causal)\n    else:\n        fixed_seed_offset = (None,)\n        return_softmax = False\n        rng_name = ''\n        (out, _) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, attn_mask, dropout_p, is_causal, return_softmax, not training, rng_name)\n    return out",
            "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API only supports inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        attn_mask(Tensor,optional): A float mask of the same type as query,\\n                        key, value that is added to the attention score.\\n        dropout_p(float): The dropout ratio.\\n        is_causal(bool): Whether enable causal mode.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('bfloat need V100 compile')\\n            >>> import paddle\\n            >>> q = paddle.rand((1, 128, 2, 16), dtype=paddle.bfloat16)\\n            >>> output = paddle.nn.functional.scaled_dot_product_attention(q, q, q, None, 0.9, False)\\n            >>> print(output)\\n            >>> # doctest: -SKIP\\n    \"\n    if attn_mask is None:\n        (out, _) = flash_attention(query, key, value, dropout_p, is_causal)\n    else:\n        fixed_seed_offset = (None,)\n        return_softmax = False\n        rng_name = ''\n        (out, _) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, attn_mask, dropout_p, is_causal, return_softmax, not training, rng_name)\n    return out",
            "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API only supports inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        attn_mask(Tensor,optional): A float mask of the same type as query,\\n                        key, value that is added to the attention score.\\n        dropout_p(float): The dropout ratio.\\n        is_causal(bool): Whether enable causal mode.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('bfloat need V100 compile')\\n            >>> import paddle\\n            >>> q = paddle.rand((1, 128, 2, 16), dtype=paddle.bfloat16)\\n            >>> output = paddle.nn.functional.scaled_dot_product_attention(q, q, q, None, 0.9, False)\\n            >>> print(output)\\n            >>> # doctest: -SKIP\\n    \"\n    if attn_mask is None:\n        (out, _) = flash_attention(query, key, value, dropout_p, is_causal)\n    else:\n        fixed_seed_offset = (None,)\n        return_softmax = False\n        rng_name = ''\n        (out, _) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, attn_mask, dropout_p, is_causal, return_softmax, not training, rng_name)\n    return out",
            "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API only supports inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        attn_mask(Tensor,optional): A float mask of the same type as query,\\n                        key, value that is added to the attention score.\\n        dropout_p(float): The dropout ratio.\\n        is_causal(bool): Whether enable causal mode.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('bfloat need V100 compile')\\n            >>> import paddle\\n            >>> q = paddle.rand((1, 128, 2, 16), dtype=paddle.bfloat16)\\n            >>> output = paddle.nn.functional.scaled_dot_product_attention(q, q, q, None, 0.9, False)\\n            >>> print(output)\\n            >>> # doctest: -SKIP\\n    \"\n    if attn_mask is None:\n        (out, _) = flash_attention(query, key, value, dropout_p, is_causal)\n    else:\n        fixed_seed_offset = (None,)\n        return_softmax = False\n        rng_name = ''\n        (out, _) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, attn_mask, dropout_p, is_causal, return_softmax, not training, rng_name)\n    return out",
            "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, training=True, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    The equation is:\\n\\n    .. math::\\n\\n        result=softmax(\\\\frac{ Q * K^T }{\\\\sqrt{d}}) * V\\n\\n    where : ``Q``, ``K``, and ``V`` represent the three input parameters of the attention module.\\n    The dimensions of the three parameters are the same.\\n    ``d`` represents the size of the last dimension of the three parameters.\\n\\n    Warning:\\n        This API only supports inputs with dtype float16 and bfloat16.\\n\\n    Args:\\n        query(Tensor): The query tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        key(Tensor): The key tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        value(Tensor): The value tensor in the Attention module.\\n                        4-D tensor with shape:\\n                        [batch_size, seq_len, num_heads, head_dim].\\n                        The dtype can be float61 or bfloat16.\\n        attn_mask(Tensor,optional): A float mask of the same type as query,\\n                        key, value that is added to the attention score.\\n        dropout_p(float): The dropout ratio.\\n        is_causal(bool): Whether enable causal mode.\\n        training(bool): Whether it is in the training phase.\\n        name(str, optional): The default value is None. Normally there is no need for user\\n                        to set this property. For more information, please refer to\\n                        :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out(Tensor): The attention tensor.\\n                    4-D tensor with shape: [batch_size, seq_len, num_heads, head_dim].\\n                    The dtype can be float16 or bfloat16.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('bfloat need V100 compile')\\n            >>> import paddle\\n            >>> q = paddle.rand((1, 128, 2, 16), dtype=paddle.bfloat16)\\n            >>> output = paddle.nn.functional.scaled_dot_product_attention(q, q, q, None, 0.9, False)\\n            >>> print(output)\\n            >>> # doctest: -SKIP\\n    \"\n    if attn_mask is None:\n        (out, _) = flash_attention(query, key, value, dropout_p, is_causal)\n    else:\n        fixed_seed_offset = (None,)\n        return_softmax = False\n        rng_name = ''\n        (out, _) = _C_ops.flash_attn(query, key, value, fixed_seed_offset, attn_mask, dropout_p, is_causal, return_softmax, not training, rng_name)\n    return out"
        ]
    }
]