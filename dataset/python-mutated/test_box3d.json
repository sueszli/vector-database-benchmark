[
    {
        "func_name": "test_bbox3d_mapping_back",
        "original": "def test_bbox3d_mapping_back():\n    bboxes = BaseInstance3DBoxes([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]])\n    new_bboxes = bbox3d_mapping_back(bboxes, 1.1, True, True)\n    expected_new_bboxes = torch.tensor([[-4.7657, 36.3827, 0.2705, 1.8745, 4.0082, 1.4073, -1.488], [-24.2501, 5.0864, -0.8312, 0.3118, 0.4164, 0.7109, -4.6276], [-5.2816, 32.1902, 0.1826, 2.1782, 3.6082, 1.5745, -4.652], [-28.4624, 0.991, -0.1769, 1.7673, 3.5064, 1.5664, -2.8143]])\n    assert torch.allclose(new_bboxes.tensor, expected_new_bboxes, atol=0.0001)",
        "mutated": [
            "def test_bbox3d_mapping_back():\n    if False:\n        i = 10\n    bboxes = BaseInstance3DBoxes([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]])\n    new_bboxes = bbox3d_mapping_back(bboxes, 1.1, True, True)\n    expected_new_bboxes = torch.tensor([[-4.7657, 36.3827, 0.2705, 1.8745, 4.0082, 1.4073, -1.488], [-24.2501, 5.0864, -0.8312, 0.3118, 0.4164, 0.7109, -4.6276], [-5.2816, 32.1902, 0.1826, 2.1782, 3.6082, 1.5745, -4.652], [-28.4624, 0.991, -0.1769, 1.7673, 3.5064, 1.5664, -2.8143]])\n    assert torch.allclose(new_bboxes.tensor, expected_new_bboxes, atol=0.0001)",
            "def test_bbox3d_mapping_back():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bboxes = BaseInstance3DBoxes([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]])\n    new_bboxes = bbox3d_mapping_back(bboxes, 1.1, True, True)\n    expected_new_bboxes = torch.tensor([[-4.7657, 36.3827, 0.2705, 1.8745, 4.0082, 1.4073, -1.488], [-24.2501, 5.0864, -0.8312, 0.3118, 0.4164, 0.7109, -4.6276], [-5.2816, 32.1902, 0.1826, 2.1782, 3.6082, 1.5745, -4.652], [-28.4624, 0.991, -0.1769, 1.7673, 3.5064, 1.5664, -2.8143]])\n    assert torch.allclose(new_bboxes.tensor, expected_new_bboxes, atol=0.0001)",
            "def test_bbox3d_mapping_back():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bboxes = BaseInstance3DBoxes([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]])\n    new_bboxes = bbox3d_mapping_back(bboxes, 1.1, True, True)\n    expected_new_bboxes = torch.tensor([[-4.7657, 36.3827, 0.2705, 1.8745, 4.0082, 1.4073, -1.488], [-24.2501, 5.0864, -0.8312, 0.3118, 0.4164, 0.7109, -4.6276], [-5.2816, 32.1902, 0.1826, 2.1782, 3.6082, 1.5745, -4.652], [-28.4624, 0.991, -0.1769, 1.7673, 3.5064, 1.5664, -2.8143]])\n    assert torch.allclose(new_bboxes.tensor, expected_new_bboxes, atol=0.0001)",
            "def test_bbox3d_mapping_back():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bboxes = BaseInstance3DBoxes([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]])\n    new_bboxes = bbox3d_mapping_back(bboxes, 1.1, True, True)\n    expected_new_bboxes = torch.tensor([[-4.7657, 36.3827, 0.2705, 1.8745, 4.0082, 1.4073, -1.488], [-24.2501, 5.0864, -0.8312, 0.3118, 0.4164, 0.7109, -4.6276], [-5.2816, 32.1902, 0.1826, 2.1782, 3.6082, 1.5745, -4.652], [-28.4624, 0.991, -0.1769, 1.7673, 3.5064, 1.5664, -2.8143]])\n    assert torch.allclose(new_bboxes.tensor, expected_new_bboxes, atol=0.0001)",
            "def test_bbox3d_mapping_back():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bboxes = BaseInstance3DBoxes([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]])\n    new_bboxes = bbox3d_mapping_back(bboxes, 1.1, True, True)\n    expected_new_bboxes = torch.tensor([[-4.7657, 36.3827, 0.2705, 1.8745, 4.0082, 1.4073, -1.488], [-24.2501, 5.0864, -0.8312, 0.3118, 0.4164, 0.7109, -4.6276], [-5.2816, 32.1902, 0.1826, 2.1782, 3.6082, 1.5745, -4.652], [-28.4624, 0.991, -0.1769, 1.7673, 3.5064, 1.5664, -2.8143]])\n    assert torch.allclose(new_bboxes.tensor, expected_new_bboxes, atol=0.0001)"
        ]
    },
    {
        "func_name": "test_bbox3d2roi",
        "original": "def test_bbox3d2roi():\n    bbox_0 = torch.tensor([[-5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [-5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652]])\n    bbox_1 = torch.tensor([[-2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [-3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    bbox_list = [bbox_0, bbox_1]\n    rois = bbox3d2roi(bbox_list)\n    expected_rois = torch.tensor([[0.0, -5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [0.0, -5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652], [1.0, -2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [1.0, -3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    assert torch.all(torch.eq(rois, expected_rois))",
        "mutated": [
            "def test_bbox3d2roi():\n    if False:\n        i = 10\n    bbox_0 = torch.tensor([[-5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [-5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652]])\n    bbox_1 = torch.tensor([[-2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [-3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    bbox_list = [bbox_0, bbox_1]\n    rois = bbox3d2roi(bbox_list)\n    expected_rois = torch.tensor([[0.0, -5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [0.0, -5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652], [1.0, -2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [1.0, -3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    assert torch.all(torch.eq(rois, expected_rois))",
            "def test_bbox3d2roi():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bbox_0 = torch.tensor([[-5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [-5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652]])\n    bbox_1 = torch.tensor([[-2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [-3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    bbox_list = [bbox_0, bbox_1]\n    rois = bbox3d2roi(bbox_list)\n    expected_rois = torch.tensor([[0.0, -5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [0.0, -5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652], [1.0, -2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [1.0, -3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    assert torch.all(torch.eq(rois, expected_rois))",
            "def test_bbox3d2roi():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bbox_0 = torch.tensor([[-5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [-5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652]])\n    bbox_1 = torch.tensor([[-2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [-3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    bbox_list = [bbox_0, bbox_1]\n    rois = bbox3d2roi(bbox_list)\n    expected_rois = torch.tensor([[0.0, -5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [0.0, -5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652], [1.0, -2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [1.0, -3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    assert torch.all(torch.eq(rois, expected_rois))",
            "def test_bbox3d2roi():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bbox_0 = torch.tensor([[-5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [-5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652]])\n    bbox_1 = torch.tensor([[-2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [-3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    bbox_list = [bbox_0, bbox_1]\n    rois = bbox3d2roi(bbox_list)\n    expected_rois = torch.tensor([[0.0, -5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [0.0, -5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652], [1.0, -2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [1.0, -3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    assert torch.all(torch.eq(rois, expected_rois))",
            "def test_bbox3d2roi():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bbox_0 = torch.tensor([[-5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [-5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652]])\n    bbox_1 = torch.tensor([[-2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [-3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    bbox_list = [bbox_0, bbox_1]\n    rois = bbox3d2roi(bbox_list)\n    expected_rois = torch.tensor([[0.0, -5.2422, 4.002, 2.9757, 2.062, 4.409, 1.548, -1.488], [0.0, -5.8097, 3.5409, 2.0088, 2.396, 3.969, 1.732, -4.652], [1.0, -2.6675, 5.5949, -9.1434, 3.43, 4.58, 7.82, -4.6275], [1.0, -3.1308, 1.09, -1.9461, 1.944, 3.857, 1.723, -2.8142]])\n    assert torch.all(torch.eq(rois, expected_rois))"
        ]
    },
    {
        "func_name": "test_base_boxes3d",
        "original": "def test_base_boxes3d():\n    empty_boxes = []\n    boxes = BaseInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = BaseInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    assert bottom_center_box.yaw.shape[0] == 4",
        "mutated": [
            "def test_base_boxes3d():\n    if False:\n        i = 10\n    empty_boxes = []\n    boxes = BaseInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = BaseInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    assert bottom_center_box.yaw.shape[0] == 4",
            "def test_base_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    empty_boxes = []\n    boxes = BaseInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = BaseInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    assert bottom_center_box.yaw.shape[0] == 4",
            "def test_base_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    empty_boxes = []\n    boxes = BaseInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = BaseInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    assert bottom_center_box.yaw.shape[0] == 4",
            "def test_base_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    empty_boxes = []\n    boxes = BaseInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = BaseInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    assert bottom_center_box.yaw.shape[0] == 4",
            "def test_base_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    empty_boxes = []\n    boxes = BaseInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = BaseInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    assert bottom_center_box.yaw.shape[0] == 4"
        ]
    },
    {
        "func_name": "test_lidar_boxes3d",
        "original": "def test_lidar_boxes3d():\n    empty_boxes = []\n    boxes = LiDARInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = LiDARInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    expected_tensor = torch.tensor([[-5.24223238, 40.0209696, -0.476429619, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -1.30534586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, -0.665110112, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -1.05611211, 1.944, 3.857, 1.723, -2.81427027]])\n    assert torch.allclose(expected_tensor, bottom_center_box.tensor)\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4]], dtype=np.float32)\n    boxes_1 = LiDARInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    assert repr(boxes) == 'LiDARInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4]], dtype=torch.float32)\n    boxes_2 = LiDARInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4]])\n    boxes = LiDARInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = LiDARInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[1.2559, -0.6762, -1.4658], [4.7814, -0.8784, -1.3857], [6.7053, 0.2517, -0.9697], [0.6533, -0.552, -0.5265], [4.587, 0.5358, -1.4741]])\n    expected_tensor = torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 - np.pi + 0.13603681398218054 * 4], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 - np.pi + 0.13603681398218054 * 4], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 - np.pi + 0.13603681398218054 * 4], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 - np.pi + 0.13603681398218054 * 4], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 - np.pi + 0.13603681398218054 * 4]])\n    expected_points = torch.tensor([[1.2559, 0.6762, -1.4658], [4.7814, 0.8784, -1.3857], [6.7053, -0.2517, -0.9697], [0.6533, 0.552, -0.5265], [4.587, -0.5358, -1.4741]])\n    points = boxes.flip('horizontal', points)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[-1.7802, -2.5162, -1.7501, 1.75, 3.39, 1.65, -1.6616 + np.pi * 2 - 0.13603681398218054 * 4], [-8.9594, -2.4567, -1.6357, 1.54, 4.01, 1.57, -1.5216 + np.pi * 2 - 0.13603681398218054 * 4], [-28.2967, 0.5558, -1.3033, 1.47, 2.23, 1.48, -4.7116 + np.pi * 2 - 0.13603681398218054 * 4], [-26.669, -21.823, -1.7361, 1.56, 3.48, 1.4, -4.8316 + np.pi * 2 - 0.13603681398218054 * 4], [-31.3198, -8.1621, -1.6218, 1.74, 3.77, 1.48, -0.3516 + np.pi * 2 - 0.13603681398218054 * 4]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-1.2559, 0.6762, -1.4658], [-4.7814, 0.8784, -1.3857], [-6.7053, -0.2517, -0.9697], [-0.6533, 0.552, -0.5265], [-4.587, -0.5358, -1.4741]])\n    assert torch.allclose(boxes_flip_vert.tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 - np.pi + 0.13603681398218054 * 2], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 - np.pi + 0.13603681398218054 * 2], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 - np.pi + 0.13603681398218054 * 2], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 - np.pi + 0.13603681398218054 * 2], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 - np.pi + 0.13603681398218054 * 2]])\n    (points, rot_mat_T) = boxes.rotate(-0.13603681398218054, points)\n    expected_points = torch.tensor([[-1.1526, 0.8403, -1.4658], [-4.6181, 1.5187, -1.3857], [-6.6775, 0.66, -0.9697], [-0.5724, 0.6355, -0.5265], [-4.6173, 0.0912, -1.4741]])\n    expected_rot_mat_T = torch.tensor([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(0.13603681398218054, points)\n    rot_mat = np.array([[0.99076125, -0.13561762, 0.0], [0.13561762, 0.99076125, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[-1.028, 0.9888, -1.4658], [-4.3695, 2.131, -1.3857], [-6.5263, 1.5595, -0.9697], [-0.4809, 0.7073, -0.5265], [-4.5623, 0.7166, -1.4741]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.13603681398218054, points_np)\n    expected_points_np = np.array([[-0.8844, 1.1191, -1.4658], [-4.0401, 2.7039, -1.3857], [-6.2545, 2.4302, -0.9697], [-0.3805, 0.766, -0.5265], [-4.423, 1.3287, -1.4741]])\n    expected_rot_mat_T_np = np.array([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(0.13603681398218054, points_np)\n    lidar_points = LiDARPoints(points_np)\n    (lidar_points, rot_mat_T_np) = boxes.rotate(rot_mat, lidar_points)\n    points_np = lidar_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.scale(1.00559866335275)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.translate([0.0838056, -0.13246193, -0.15701613])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1282, -3.0508, 1.7598, 3.409, -1.2079], [8.0981, -4.9332, 1.5486, 4.0325, -1.3479], [27.6424, -7.2409, 1.4782, 2.2425, 1.8421], [20.0183, -28.4773, 1.5687, 3.4995, 1.9621], [28.2147, -16.502, 1.7497, 3.7911, -2.5179]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([0, -20, -2, 22, 2, 5])\n    assert (mask == expected_tensor).all()\n    index_boxes = boxes[2:5]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 3\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[2]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi]])\n    assert len(index_boxes) == 1\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[[2, 4]]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 2\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    for (i, box) in enumerate(index_boxes):\n        torch.allclose(box, expected_tensor[i])\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 0.5, 0]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[-0.77767, -2.8332, -1.9169], [-0.77767, -2.8332, -0.25769], [2.4093, -1.6232, -0.25769], [2.4093, -1.6232, -1.9169], [-0.15301, -4.4784, -1.9169], [-0.15301, -4.4784, -0.25769], [3.034, -3.2684, -0.25769], [3.034, -3.2684, -1.9169]], [[5.9606, -4.6237, -1.8019], [5.9606, -4.6237, -0.2231], [9.8933, -3.7324, -0.2231], [9.8933, -3.7324, -1.8019], [6.3029, -6.134, -1.8019], [6.3029, -6.134, -0.2231], [10.236, -5.2427, -0.2231], [10.236, -5.2427, -1.8019]], [[26.364, -6.8292, -1.4676], [26.364, -6.8292, 0.020648], [28.525, -6.2283, 0.020648], [28.525, -6.2283, -1.4676], [26.76, -8.2534, -1.4676], [26.76, -8.2534, 0.020648], [28.921, -7.6525, 0.020648], [28.921, -7.6525, -1.4676]], [[18.102, -28.42, -1.9028], [18.102, -28.42, -0.49495], [21.337, -27.085, -0.49495], [21.337, -27.085, -1.9028], [18.7, -29.87, -1.9028], [18.7, -29.87, -0.49495], [21.935, -28.535, -0.49495], [21.935, -28.535, -1.9028]], [[28.612, -18.552, -1.7879], [28.612, -18.552, -0.29959], [26.398, -15.474, -0.29959], [26.398, -15.474, -1.7879], [30.032, -17.53, -1.7879], [30.032, -17.53, -0.29959], [27.818, -14.452, -0.29959], [27.818, -14.452, -1.7879]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.0001, atol=1e-07)\n    new_box1 = boxes.new_box([[1, 2, 3, 4, 5, 6, 7]])\n    assert torch.allclose(new_box1.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    assert new_box1.device == boxes.device\n    assert new_box1.with_yaw == boxes.with_yaw\n    assert new_box1.box_dim == boxes.box_dim\n    new_box2 = boxes.new_box(np.array([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box2.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    new_box3 = boxes.new_box(torch.tensor([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box3.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))",
        "mutated": [
            "def test_lidar_boxes3d():\n    if False:\n        i = 10\n    empty_boxes = []\n    boxes = LiDARInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = LiDARInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    expected_tensor = torch.tensor([[-5.24223238, 40.0209696, -0.476429619, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -1.30534586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, -0.665110112, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -1.05611211, 1.944, 3.857, 1.723, -2.81427027]])\n    assert torch.allclose(expected_tensor, bottom_center_box.tensor)\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4]], dtype=np.float32)\n    boxes_1 = LiDARInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    assert repr(boxes) == 'LiDARInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4]], dtype=torch.float32)\n    boxes_2 = LiDARInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4]])\n    boxes = LiDARInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = LiDARInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[1.2559, -0.6762, -1.4658], [4.7814, -0.8784, -1.3857], [6.7053, 0.2517, -0.9697], [0.6533, -0.552, -0.5265], [4.587, 0.5358, -1.4741]])\n    expected_tensor = torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 - np.pi + 0.13603681398218054 * 4], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 - np.pi + 0.13603681398218054 * 4], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 - np.pi + 0.13603681398218054 * 4], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 - np.pi + 0.13603681398218054 * 4], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 - np.pi + 0.13603681398218054 * 4]])\n    expected_points = torch.tensor([[1.2559, 0.6762, -1.4658], [4.7814, 0.8784, -1.3857], [6.7053, -0.2517, -0.9697], [0.6533, 0.552, -0.5265], [4.587, -0.5358, -1.4741]])\n    points = boxes.flip('horizontal', points)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[-1.7802, -2.5162, -1.7501, 1.75, 3.39, 1.65, -1.6616 + np.pi * 2 - 0.13603681398218054 * 4], [-8.9594, -2.4567, -1.6357, 1.54, 4.01, 1.57, -1.5216 + np.pi * 2 - 0.13603681398218054 * 4], [-28.2967, 0.5558, -1.3033, 1.47, 2.23, 1.48, -4.7116 + np.pi * 2 - 0.13603681398218054 * 4], [-26.669, -21.823, -1.7361, 1.56, 3.48, 1.4, -4.8316 + np.pi * 2 - 0.13603681398218054 * 4], [-31.3198, -8.1621, -1.6218, 1.74, 3.77, 1.48, -0.3516 + np.pi * 2 - 0.13603681398218054 * 4]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-1.2559, 0.6762, -1.4658], [-4.7814, 0.8784, -1.3857], [-6.7053, -0.2517, -0.9697], [-0.6533, 0.552, -0.5265], [-4.587, -0.5358, -1.4741]])\n    assert torch.allclose(boxes_flip_vert.tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 - np.pi + 0.13603681398218054 * 2], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 - np.pi + 0.13603681398218054 * 2], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 - np.pi + 0.13603681398218054 * 2], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 - np.pi + 0.13603681398218054 * 2], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 - np.pi + 0.13603681398218054 * 2]])\n    (points, rot_mat_T) = boxes.rotate(-0.13603681398218054, points)\n    expected_points = torch.tensor([[-1.1526, 0.8403, -1.4658], [-4.6181, 1.5187, -1.3857], [-6.6775, 0.66, -0.9697], [-0.5724, 0.6355, -0.5265], [-4.6173, 0.0912, -1.4741]])\n    expected_rot_mat_T = torch.tensor([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(0.13603681398218054, points)\n    rot_mat = np.array([[0.99076125, -0.13561762, 0.0], [0.13561762, 0.99076125, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[-1.028, 0.9888, -1.4658], [-4.3695, 2.131, -1.3857], [-6.5263, 1.5595, -0.9697], [-0.4809, 0.7073, -0.5265], [-4.5623, 0.7166, -1.4741]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.13603681398218054, points_np)\n    expected_points_np = np.array([[-0.8844, 1.1191, -1.4658], [-4.0401, 2.7039, -1.3857], [-6.2545, 2.4302, -0.9697], [-0.3805, 0.766, -0.5265], [-4.423, 1.3287, -1.4741]])\n    expected_rot_mat_T_np = np.array([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(0.13603681398218054, points_np)\n    lidar_points = LiDARPoints(points_np)\n    (lidar_points, rot_mat_T_np) = boxes.rotate(rot_mat, lidar_points)\n    points_np = lidar_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.scale(1.00559866335275)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.translate([0.0838056, -0.13246193, -0.15701613])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1282, -3.0508, 1.7598, 3.409, -1.2079], [8.0981, -4.9332, 1.5486, 4.0325, -1.3479], [27.6424, -7.2409, 1.4782, 2.2425, 1.8421], [20.0183, -28.4773, 1.5687, 3.4995, 1.9621], [28.2147, -16.502, 1.7497, 3.7911, -2.5179]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([0, -20, -2, 22, 2, 5])\n    assert (mask == expected_tensor).all()\n    index_boxes = boxes[2:5]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 3\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[2]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi]])\n    assert len(index_boxes) == 1\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[[2, 4]]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 2\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    for (i, box) in enumerate(index_boxes):\n        torch.allclose(box, expected_tensor[i])\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 0.5, 0]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[-0.77767, -2.8332, -1.9169], [-0.77767, -2.8332, -0.25769], [2.4093, -1.6232, -0.25769], [2.4093, -1.6232, -1.9169], [-0.15301, -4.4784, -1.9169], [-0.15301, -4.4784, -0.25769], [3.034, -3.2684, -0.25769], [3.034, -3.2684, -1.9169]], [[5.9606, -4.6237, -1.8019], [5.9606, -4.6237, -0.2231], [9.8933, -3.7324, -0.2231], [9.8933, -3.7324, -1.8019], [6.3029, -6.134, -1.8019], [6.3029, -6.134, -0.2231], [10.236, -5.2427, -0.2231], [10.236, -5.2427, -1.8019]], [[26.364, -6.8292, -1.4676], [26.364, -6.8292, 0.020648], [28.525, -6.2283, 0.020648], [28.525, -6.2283, -1.4676], [26.76, -8.2534, -1.4676], [26.76, -8.2534, 0.020648], [28.921, -7.6525, 0.020648], [28.921, -7.6525, -1.4676]], [[18.102, -28.42, -1.9028], [18.102, -28.42, -0.49495], [21.337, -27.085, -0.49495], [21.337, -27.085, -1.9028], [18.7, -29.87, -1.9028], [18.7, -29.87, -0.49495], [21.935, -28.535, -0.49495], [21.935, -28.535, -1.9028]], [[28.612, -18.552, -1.7879], [28.612, -18.552, -0.29959], [26.398, -15.474, -0.29959], [26.398, -15.474, -1.7879], [30.032, -17.53, -1.7879], [30.032, -17.53, -0.29959], [27.818, -14.452, -0.29959], [27.818, -14.452, -1.7879]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.0001, atol=1e-07)\n    new_box1 = boxes.new_box([[1, 2, 3, 4, 5, 6, 7]])\n    assert torch.allclose(new_box1.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    assert new_box1.device == boxes.device\n    assert new_box1.with_yaw == boxes.with_yaw\n    assert new_box1.box_dim == boxes.box_dim\n    new_box2 = boxes.new_box(np.array([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box2.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    new_box3 = boxes.new_box(torch.tensor([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box3.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))",
            "def test_lidar_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    empty_boxes = []\n    boxes = LiDARInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = LiDARInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    expected_tensor = torch.tensor([[-5.24223238, 40.0209696, -0.476429619, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -1.30534586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, -0.665110112, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -1.05611211, 1.944, 3.857, 1.723, -2.81427027]])\n    assert torch.allclose(expected_tensor, bottom_center_box.tensor)\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4]], dtype=np.float32)\n    boxes_1 = LiDARInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    assert repr(boxes) == 'LiDARInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4]], dtype=torch.float32)\n    boxes_2 = LiDARInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4]])\n    boxes = LiDARInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = LiDARInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[1.2559, -0.6762, -1.4658], [4.7814, -0.8784, -1.3857], [6.7053, 0.2517, -0.9697], [0.6533, -0.552, -0.5265], [4.587, 0.5358, -1.4741]])\n    expected_tensor = torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 - np.pi + 0.13603681398218054 * 4], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 - np.pi + 0.13603681398218054 * 4], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 - np.pi + 0.13603681398218054 * 4], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 - np.pi + 0.13603681398218054 * 4], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 - np.pi + 0.13603681398218054 * 4]])\n    expected_points = torch.tensor([[1.2559, 0.6762, -1.4658], [4.7814, 0.8784, -1.3857], [6.7053, -0.2517, -0.9697], [0.6533, 0.552, -0.5265], [4.587, -0.5358, -1.4741]])\n    points = boxes.flip('horizontal', points)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[-1.7802, -2.5162, -1.7501, 1.75, 3.39, 1.65, -1.6616 + np.pi * 2 - 0.13603681398218054 * 4], [-8.9594, -2.4567, -1.6357, 1.54, 4.01, 1.57, -1.5216 + np.pi * 2 - 0.13603681398218054 * 4], [-28.2967, 0.5558, -1.3033, 1.47, 2.23, 1.48, -4.7116 + np.pi * 2 - 0.13603681398218054 * 4], [-26.669, -21.823, -1.7361, 1.56, 3.48, 1.4, -4.8316 + np.pi * 2 - 0.13603681398218054 * 4], [-31.3198, -8.1621, -1.6218, 1.74, 3.77, 1.48, -0.3516 + np.pi * 2 - 0.13603681398218054 * 4]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-1.2559, 0.6762, -1.4658], [-4.7814, 0.8784, -1.3857], [-6.7053, -0.2517, -0.9697], [-0.6533, 0.552, -0.5265], [-4.587, -0.5358, -1.4741]])\n    assert torch.allclose(boxes_flip_vert.tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 - np.pi + 0.13603681398218054 * 2], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 - np.pi + 0.13603681398218054 * 2], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 - np.pi + 0.13603681398218054 * 2], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 - np.pi + 0.13603681398218054 * 2], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 - np.pi + 0.13603681398218054 * 2]])\n    (points, rot_mat_T) = boxes.rotate(-0.13603681398218054, points)\n    expected_points = torch.tensor([[-1.1526, 0.8403, -1.4658], [-4.6181, 1.5187, -1.3857], [-6.6775, 0.66, -0.9697], [-0.5724, 0.6355, -0.5265], [-4.6173, 0.0912, -1.4741]])\n    expected_rot_mat_T = torch.tensor([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(0.13603681398218054, points)\n    rot_mat = np.array([[0.99076125, -0.13561762, 0.0], [0.13561762, 0.99076125, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[-1.028, 0.9888, -1.4658], [-4.3695, 2.131, -1.3857], [-6.5263, 1.5595, -0.9697], [-0.4809, 0.7073, -0.5265], [-4.5623, 0.7166, -1.4741]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.13603681398218054, points_np)\n    expected_points_np = np.array([[-0.8844, 1.1191, -1.4658], [-4.0401, 2.7039, -1.3857], [-6.2545, 2.4302, -0.9697], [-0.3805, 0.766, -0.5265], [-4.423, 1.3287, -1.4741]])\n    expected_rot_mat_T_np = np.array([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(0.13603681398218054, points_np)\n    lidar_points = LiDARPoints(points_np)\n    (lidar_points, rot_mat_T_np) = boxes.rotate(rot_mat, lidar_points)\n    points_np = lidar_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.scale(1.00559866335275)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.translate([0.0838056, -0.13246193, -0.15701613])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1282, -3.0508, 1.7598, 3.409, -1.2079], [8.0981, -4.9332, 1.5486, 4.0325, -1.3479], [27.6424, -7.2409, 1.4782, 2.2425, 1.8421], [20.0183, -28.4773, 1.5687, 3.4995, 1.9621], [28.2147, -16.502, 1.7497, 3.7911, -2.5179]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([0, -20, -2, 22, 2, 5])\n    assert (mask == expected_tensor).all()\n    index_boxes = boxes[2:5]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 3\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[2]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi]])\n    assert len(index_boxes) == 1\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[[2, 4]]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 2\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    for (i, box) in enumerate(index_boxes):\n        torch.allclose(box, expected_tensor[i])\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 0.5, 0]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[-0.77767, -2.8332, -1.9169], [-0.77767, -2.8332, -0.25769], [2.4093, -1.6232, -0.25769], [2.4093, -1.6232, -1.9169], [-0.15301, -4.4784, -1.9169], [-0.15301, -4.4784, -0.25769], [3.034, -3.2684, -0.25769], [3.034, -3.2684, -1.9169]], [[5.9606, -4.6237, -1.8019], [5.9606, -4.6237, -0.2231], [9.8933, -3.7324, -0.2231], [9.8933, -3.7324, -1.8019], [6.3029, -6.134, -1.8019], [6.3029, -6.134, -0.2231], [10.236, -5.2427, -0.2231], [10.236, -5.2427, -1.8019]], [[26.364, -6.8292, -1.4676], [26.364, -6.8292, 0.020648], [28.525, -6.2283, 0.020648], [28.525, -6.2283, -1.4676], [26.76, -8.2534, -1.4676], [26.76, -8.2534, 0.020648], [28.921, -7.6525, 0.020648], [28.921, -7.6525, -1.4676]], [[18.102, -28.42, -1.9028], [18.102, -28.42, -0.49495], [21.337, -27.085, -0.49495], [21.337, -27.085, -1.9028], [18.7, -29.87, -1.9028], [18.7, -29.87, -0.49495], [21.935, -28.535, -0.49495], [21.935, -28.535, -1.9028]], [[28.612, -18.552, -1.7879], [28.612, -18.552, -0.29959], [26.398, -15.474, -0.29959], [26.398, -15.474, -1.7879], [30.032, -17.53, -1.7879], [30.032, -17.53, -0.29959], [27.818, -14.452, -0.29959], [27.818, -14.452, -1.7879]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.0001, atol=1e-07)\n    new_box1 = boxes.new_box([[1, 2, 3, 4, 5, 6, 7]])\n    assert torch.allclose(new_box1.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    assert new_box1.device == boxes.device\n    assert new_box1.with_yaw == boxes.with_yaw\n    assert new_box1.box_dim == boxes.box_dim\n    new_box2 = boxes.new_box(np.array([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box2.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    new_box3 = boxes.new_box(torch.tensor([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box3.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))",
            "def test_lidar_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    empty_boxes = []\n    boxes = LiDARInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = LiDARInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    expected_tensor = torch.tensor([[-5.24223238, 40.0209696, -0.476429619, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -1.30534586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, -0.665110112, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -1.05611211, 1.944, 3.857, 1.723, -2.81427027]])\n    assert torch.allclose(expected_tensor, bottom_center_box.tensor)\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4]], dtype=np.float32)\n    boxes_1 = LiDARInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    assert repr(boxes) == 'LiDARInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4]], dtype=torch.float32)\n    boxes_2 = LiDARInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4]])\n    boxes = LiDARInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = LiDARInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[1.2559, -0.6762, -1.4658], [4.7814, -0.8784, -1.3857], [6.7053, 0.2517, -0.9697], [0.6533, -0.552, -0.5265], [4.587, 0.5358, -1.4741]])\n    expected_tensor = torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 - np.pi + 0.13603681398218054 * 4], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 - np.pi + 0.13603681398218054 * 4], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 - np.pi + 0.13603681398218054 * 4], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 - np.pi + 0.13603681398218054 * 4], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 - np.pi + 0.13603681398218054 * 4]])\n    expected_points = torch.tensor([[1.2559, 0.6762, -1.4658], [4.7814, 0.8784, -1.3857], [6.7053, -0.2517, -0.9697], [0.6533, 0.552, -0.5265], [4.587, -0.5358, -1.4741]])\n    points = boxes.flip('horizontal', points)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[-1.7802, -2.5162, -1.7501, 1.75, 3.39, 1.65, -1.6616 + np.pi * 2 - 0.13603681398218054 * 4], [-8.9594, -2.4567, -1.6357, 1.54, 4.01, 1.57, -1.5216 + np.pi * 2 - 0.13603681398218054 * 4], [-28.2967, 0.5558, -1.3033, 1.47, 2.23, 1.48, -4.7116 + np.pi * 2 - 0.13603681398218054 * 4], [-26.669, -21.823, -1.7361, 1.56, 3.48, 1.4, -4.8316 + np.pi * 2 - 0.13603681398218054 * 4], [-31.3198, -8.1621, -1.6218, 1.74, 3.77, 1.48, -0.3516 + np.pi * 2 - 0.13603681398218054 * 4]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-1.2559, 0.6762, -1.4658], [-4.7814, 0.8784, -1.3857], [-6.7053, -0.2517, -0.9697], [-0.6533, 0.552, -0.5265], [-4.587, -0.5358, -1.4741]])\n    assert torch.allclose(boxes_flip_vert.tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 - np.pi + 0.13603681398218054 * 2], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 - np.pi + 0.13603681398218054 * 2], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 - np.pi + 0.13603681398218054 * 2], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 - np.pi + 0.13603681398218054 * 2], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 - np.pi + 0.13603681398218054 * 2]])\n    (points, rot_mat_T) = boxes.rotate(-0.13603681398218054, points)\n    expected_points = torch.tensor([[-1.1526, 0.8403, -1.4658], [-4.6181, 1.5187, -1.3857], [-6.6775, 0.66, -0.9697], [-0.5724, 0.6355, -0.5265], [-4.6173, 0.0912, -1.4741]])\n    expected_rot_mat_T = torch.tensor([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(0.13603681398218054, points)\n    rot_mat = np.array([[0.99076125, -0.13561762, 0.0], [0.13561762, 0.99076125, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[-1.028, 0.9888, -1.4658], [-4.3695, 2.131, -1.3857], [-6.5263, 1.5595, -0.9697], [-0.4809, 0.7073, -0.5265], [-4.5623, 0.7166, -1.4741]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.13603681398218054, points_np)\n    expected_points_np = np.array([[-0.8844, 1.1191, -1.4658], [-4.0401, 2.7039, -1.3857], [-6.2545, 2.4302, -0.9697], [-0.3805, 0.766, -0.5265], [-4.423, 1.3287, -1.4741]])\n    expected_rot_mat_T_np = np.array([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(0.13603681398218054, points_np)\n    lidar_points = LiDARPoints(points_np)\n    (lidar_points, rot_mat_T_np) = boxes.rotate(rot_mat, lidar_points)\n    points_np = lidar_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.scale(1.00559866335275)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.translate([0.0838056, -0.13246193, -0.15701613])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1282, -3.0508, 1.7598, 3.409, -1.2079], [8.0981, -4.9332, 1.5486, 4.0325, -1.3479], [27.6424, -7.2409, 1.4782, 2.2425, 1.8421], [20.0183, -28.4773, 1.5687, 3.4995, 1.9621], [28.2147, -16.502, 1.7497, 3.7911, -2.5179]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([0, -20, -2, 22, 2, 5])\n    assert (mask == expected_tensor).all()\n    index_boxes = boxes[2:5]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 3\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[2]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi]])\n    assert len(index_boxes) == 1\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[[2, 4]]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 2\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    for (i, box) in enumerate(index_boxes):\n        torch.allclose(box, expected_tensor[i])\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 0.5, 0]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[-0.77767, -2.8332, -1.9169], [-0.77767, -2.8332, -0.25769], [2.4093, -1.6232, -0.25769], [2.4093, -1.6232, -1.9169], [-0.15301, -4.4784, -1.9169], [-0.15301, -4.4784, -0.25769], [3.034, -3.2684, -0.25769], [3.034, -3.2684, -1.9169]], [[5.9606, -4.6237, -1.8019], [5.9606, -4.6237, -0.2231], [9.8933, -3.7324, -0.2231], [9.8933, -3.7324, -1.8019], [6.3029, -6.134, -1.8019], [6.3029, -6.134, -0.2231], [10.236, -5.2427, -0.2231], [10.236, -5.2427, -1.8019]], [[26.364, -6.8292, -1.4676], [26.364, -6.8292, 0.020648], [28.525, -6.2283, 0.020648], [28.525, -6.2283, -1.4676], [26.76, -8.2534, -1.4676], [26.76, -8.2534, 0.020648], [28.921, -7.6525, 0.020648], [28.921, -7.6525, -1.4676]], [[18.102, -28.42, -1.9028], [18.102, -28.42, -0.49495], [21.337, -27.085, -0.49495], [21.337, -27.085, -1.9028], [18.7, -29.87, -1.9028], [18.7, -29.87, -0.49495], [21.935, -28.535, -0.49495], [21.935, -28.535, -1.9028]], [[28.612, -18.552, -1.7879], [28.612, -18.552, -0.29959], [26.398, -15.474, -0.29959], [26.398, -15.474, -1.7879], [30.032, -17.53, -1.7879], [30.032, -17.53, -0.29959], [27.818, -14.452, -0.29959], [27.818, -14.452, -1.7879]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.0001, atol=1e-07)\n    new_box1 = boxes.new_box([[1, 2, 3, 4, 5, 6, 7]])\n    assert torch.allclose(new_box1.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    assert new_box1.device == boxes.device\n    assert new_box1.with_yaw == boxes.with_yaw\n    assert new_box1.box_dim == boxes.box_dim\n    new_box2 = boxes.new_box(np.array([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box2.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    new_box3 = boxes.new_box(torch.tensor([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box3.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))",
            "def test_lidar_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    empty_boxes = []\n    boxes = LiDARInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = LiDARInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    expected_tensor = torch.tensor([[-5.24223238, 40.0209696, -0.476429619, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -1.30534586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, -0.665110112, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -1.05611211, 1.944, 3.857, 1.723, -2.81427027]])\n    assert torch.allclose(expected_tensor, bottom_center_box.tensor)\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4]], dtype=np.float32)\n    boxes_1 = LiDARInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    assert repr(boxes) == 'LiDARInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4]], dtype=torch.float32)\n    boxes_2 = LiDARInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4]])\n    boxes = LiDARInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = LiDARInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[1.2559, -0.6762, -1.4658], [4.7814, -0.8784, -1.3857], [6.7053, 0.2517, -0.9697], [0.6533, -0.552, -0.5265], [4.587, 0.5358, -1.4741]])\n    expected_tensor = torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 - np.pi + 0.13603681398218054 * 4], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 - np.pi + 0.13603681398218054 * 4], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 - np.pi + 0.13603681398218054 * 4], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 - np.pi + 0.13603681398218054 * 4], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 - np.pi + 0.13603681398218054 * 4]])\n    expected_points = torch.tensor([[1.2559, 0.6762, -1.4658], [4.7814, 0.8784, -1.3857], [6.7053, -0.2517, -0.9697], [0.6533, 0.552, -0.5265], [4.587, -0.5358, -1.4741]])\n    points = boxes.flip('horizontal', points)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[-1.7802, -2.5162, -1.7501, 1.75, 3.39, 1.65, -1.6616 + np.pi * 2 - 0.13603681398218054 * 4], [-8.9594, -2.4567, -1.6357, 1.54, 4.01, 1.57, -1.5216 + np.pi * 2 - 0.13603681398218054 * 4], [-28.2967, 0.5558, -1.3033, 1.47, 2.23, 1.48, -4.7116 + np.pi * 2 - 0.13603681398218054 * 4], [-26.669, -21.823, -1.7361, 1.56, 3.48, 1.4, -4.8316 + np.pi * 2 - 0.13603681398218054 * 4], [-31.3198, -8.1621, -1.6218, 1.74, 3.77, 1.48, -0.3516 + np.pi * 2 - 0.13603681398218054 * 4]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-1.2559, 0.6762, -1.4658], [-4.7814, 0.8784, -1.3857], [-6.7053, -0.2517, -0.9697], [-0.6533, 0.552, -0.5265], [-4.587, -0.5358, -1.4741]])\n    assert torch.allclose(boxes_flip_vert.tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 - np.pi + 0.13603681398218054 * 2], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 - np.pi + 0.13603681398218054 * 2], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 - np.pi + 0.13603681398218054 * 2], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 - np.pi + 0.13603681398218054 * 2], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 - np.pi + 0.13603681398218054 * 2]])\n    (points, rot_mat_T) = boxes.rotate(-0.13603681398218054, points)\n    expected_points = torch.tensor([[-1.1526, 0.8403, -1.4658], [-4.6181, 1.5187, -1.3857], [-6.6775, 0.66, -0.9697], [-0.5724, 0.6355, -0.5265], [-4.6173, 0.0912, -1.4741]])\n    expected_rot_mat_T = torch.tensor([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(0.13603681398218054, points)\n    rot_mat = np.array([[0.99076125, -0.13561762, 0.0], [0.13561762, 0.99076125, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[-1.028, 0.9888, -1.4658], [-4.3695, 2.131, -1.3857], [-6.5263, 1.5595, -0.9697], [-0.4809, 0.7073, -0.5265], [-4.5623, 0.7166, -1.4741]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.13603681398218054, points_np)\n    expected_points_np = np.array([[-0.8844, 1.1191, -1.4658], [-4.0401, 2.7039, -1.3857], [-6.2545, 2.4302, -0.9697], [-0.3805, 0.766, -0.5265], [-4.423, 1.3287, -1.4741]])\n    expected_rot_mat_T_np = np.array([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(0.13603681398218054, points_np)\n    lidar_points = LiDARPoints(points_np)\n    (lidar_points, rot_mat_T_np) = boxes.rotate(rot_mat, lidar_points)\n    points_np = lidar_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.scale(1.00559866335275)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.translate([0.0838056, -0.13246193, -0.15701613])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1282, -3.0508, 1.7598, 3.409, -1.2079], [8.0981, -4.9332, 1.5486, 4.0325, -1.3479], [27.6424, -7.2409, 1.4782, 2.2425, 1.8421], [20.0183, -28.4773, 1.5687, 3.4995, 1.9621], [28.2147, -16.502, 1.7497, 3.7911, -2.5179]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([0, -20, -2, 22, 2, 5])\n    assert (mask == expected_tensor).all()\n    index_boxes = boxes[2:5]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 3\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[2]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi]])\n    assert len(index_boxes) == 1\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[[2, 4]]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 2\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    for (i, box) in enumerate(index_boxes):\n        torch.allclose(box, expected_tensor[i])\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 0.5, 0]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[-0.77767, -2.8332, -1.9169], [-0.77767, -2.8332, -0.25769], [2.4093, -1.6232, -0.25769], [2.4093, -1.6232, -1.9169], [-0.15301, -4.4784, -1.9169], [-0.15301, -4.4784, -0.25769], [3.034, -3.2684, -0.25769], [3.034, -3.2684, -1.9169]], [[5.9606, -4.6237, -1.8019], [5.9606, -4.6237, -0.2231], [9.8933, -3.7324, -0.2231], [9.8933, -3.7324, -1.8019], [6.3029, -6.134, -1.8019], [6.3029, -6.134, -0.2231], [10.236, -5.2427, -0.2231], [10.236, -5.2427, -1.8019]], [[26.364, -6.8292, -1.4676], [26.364, -6.8292, 0.020648], [28.525, -6.2283, 0.020648], [28.525, -6.2283, -1.4676], [26.76, -8.2534, -1.4676], [26.76, -8.2534, 0.020648], [28.921, -7.6525, 0.020648], [28.921, -7.6525, -1.4676]], [[18.102, -28.42, -1.9028], [18.102, -28.42, -0.49495], [21.337, -27.085, -0.49495], [21.337, -27.085, -1.9028], [18.7, -29.87, -1.9028], [18.7, -29.87, -0.49495], [21.935, -28.535, -0.49495], [21.935, -28.535, -1.9028]], [[28.612, -18.552, -1.7879], [28.612, -18.552, -0.29959], [26.398, -15.474, -0.29959], [26.398, -15.474, -1.7879], [30.032, -17.53, -1.7879], [30.032, -17.53, -0.29959], [27.818, -14.452, -0.29959], [27.818, -14.452, -1.7879]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.0001, atol=1e-07)\n    new_box1 = boxes.new_box([[1, 2, 3, 4, 5, 6, 7]])\n    assert torch.allclose(new_box1.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    assert new_box1.device == boxes.device\n    assert new_box1.with_yaw == boxes.with_yaw\n    assert new_box1.box_dim == boxes.box_dim\n    new_box2 = boxes.new_box(np.array([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box2.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    new_box3 = boxes.new_box(torch.tensor([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box3.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))",
            "def test_lidar_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    empty_boxes = []\n    boxes = LiDARInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    gravity_center_box = np.array([[-5.24223238, 40.0209696, 0.297570381, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -0.91434586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, 0.200889888, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -0.194612112, 1.944, 3.857, 1.723, -2.81427027]], dtype=np.float32)\n    bottom_center_box = LiDARInstance3DBoxes(gravity_center_box, origin=(0.5, 0.5, 0.5))\n    expected_tensor = torch.tensor([[-5.24223238, 40.0209696, -0.476429619, 2.062, 4.409, 1.548, -1.48801203], [-26.6751588, 5.59499564, -1.30534586, 0.343, 0.458, 0.782, -4.62759755], [-5.80979675, 35.4092357, -0.665110112, 2.396, 3.969, 1.732, -4.65203216], [-31.3086877, 1.09007628, -1.05611211, 1.944, 3.857, 1.723, -2.81427027]])\n    assert torch.allclose(expected_tensor, bottom_center_box.tensor)\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4]], dtype=np.float32)\n    boxes_1 = LiDARInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    assert repr(boxes) == 'LiDARInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4]], dtype=torch.float32)\n    boxes_2 = LiDARInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4]])\n    boxes = LiDARInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = LiDARInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[1.2559, -0.6762, -1.4658], [4.7814, -0.8784, -1.3857], [6.7053, 0.2517, -0.9697], [0.6533, -0.552, -0.5265], [4.587, 0.5358, -1.4741]])\n    expected_tensor = torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 - np.pi + 0.13603681398218054 * 4], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 - np.pi + 0.13603681398218054 * 4], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 - np.pi + 0.13603681398218054 * 4], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 - np.pi + 0.13603681398218054 * 4], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 - np.pi + 0.13603681398218054 * 4]])\n    expected_points = torch.tensor([[1.2559, 0.6762, -1.4658], [4.7814, 0.8784, -1.3857], [6.7053, -0.2517, -0.9697], [0.6533, 0.552, -0.5265], [4.587, -0.5358, -1.4741]])\n    points = boxes.flip('horizontal', points)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[-1.7802, -2.5162, -1.7501, 1.75, 3.39, 1.65, -1.6616 + np.pi * 2 - 0.13603681398218054 * 4], [-8.9594, -2.4567, -1.6357, 1.54, 4.01, 1.57, -1.5216 + np.pi * 2 - 0.13603681398218054 * 4], [-28.2967, 0.5558, -1.3033, 1.47, 2.23, 1.48, -4.7116 + np.pi * 2 - 0.13603681398218054 * 4], [-26.669, -21.823, -1.7361, 1.56, 3.48, 1.4, -4.8316 + np.pi * 2 - 0.13603681398218054 * 4], [-31.3198, -8.1621, -1.6218, 1.74, 3.77, 1.48, -0.3516 + np.pi * 2 - 0.13603681398218054 * 4]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-1.2559, 0.6762, -1.4658], [-4.7814, 0.8784, -1.3857], [-6.7053, -0.2517, -0.9697], [-0.6533, 0.552, -0.5265], [-4.587, -0.5358, -1.4741]])\n    assert torch.allclose(boxes_flip_vert.tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 - np.pi + 0.13603681398218054 * 2], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 - np.pi + 0.13603681398218054 * 2], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 - np.pi + 0.13603681398218054 * 2], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 - np.pi + 0.13603681398218054 * 2], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 - np.pi + 0.13603681398218054 * 2]])\n    (points, rot_mat_T) = boxes.rotate(-0.13603681398218054, points)\n    expected_points = torch.tensor([[-1.1526, 0.8403, -1.4658], [-4.6181, 1.5187, -1.3857], [-6.6775, 0.66, -0.9697], [-0.5724, 0.6355, -0.5265], [-4.6173, 0.0912, -1.4741]])\n    expected_rot_mat_T = torch.tensor([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(0.13603681398218054, points)\n    rot_mat = np.array([[0.99076125, -0.13561762, 0.0], [0.13561762, 0.99076125, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[-1.028, 0.9888, -1.4658], [-4.3695, 2.131, -1.3857], [-6.5263, 1.5595, -0.9697], [-0.4809, 0.7073, -0.5265], [-4.5623, 0.7166, -1.4741]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.13603681398218054, points_np)\n    expected_points_np = np.array([[-0.8844, 1.1191, -1.4658], [-4.0401, 2.7039, -1.3857], [-6.2545, 2.4302, -0.9697], [-0.3805, 0.766, -0.5265], [-4.423, 1.3287, -1.4741]])\n    expected_rot_mat_T_np = np.array([[0.9908, -0.1356, 0.0], [0.1356, 0.9908, 0.0], [0.0, 0.0, 1.0]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(0.13603681398218054, points_np)\n    lidar_points = LiDARPoints(points_np)\n    (lidar_points, rot_mat_T_np) = boxes.rotate(rot_mat, lidar_points)\n    points_np = lidar_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.scale(1.00559866335275)\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    boxes.translate([0.0838056, -0.13246193, -0.15701613])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([[1.1282, -3.0508, 1.7598, 3.409, -1.2079], [8.0981, -4.9332, 1.5486, 4.0325, -1.3479], [27.6424, -7.2409, 1.4782, 2.2425, 1.8421], [20.0183, -28.4773, 1.5687, 3.4995, 1.9621], [28.2147, -16.502, 1.7497, 3.7911, -2.5179]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([0, -20, -2, 22, 2, 5])\n    assert (mask == expected_tensor).all()\n    index_boxes = boxes[2:5]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 3\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[2]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi]])\n    assert len(index_boxes) == 1\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    index_boxes = boxes[[2, 4]]\n    expected_tensor = torch.tensor([[27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]])\n    assert len(index_boxes) == 2\n    assert torch.allclose(index_boxes.tensor, expected_tensor)\n    for (i, box) in enumerate(index_boxes):\n        torch.allclose(box, expected_tensor[i])\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 0.5, 0]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[-0.77767, -2.8332, -1.9169], [-0.77767, -2.8332, -0.25769], [2.4093, -1.6232, -0.25769], [2.4093, -1.6232, -1.9169], [-0.15301, -4.4784, -1.9169], [-0.15301, -4.4784, -0.25769], [3.034, -3.2684, -0.25769], [3.034, -3.2684, -1.9169]], [[5.9606, -4.6237, -1.8019], [5.9606, -4.6237, -0.2231], [9.8933, -3.7324, -0.2231], [9.8933, -3.7324, -1.8019], [6.3029, -6.134, -1.8019], [6.3029, -6.134, -0.2231], [10.236, -5.2427, -0.2231], [10.236, -5.2427, -1.8019]], [[26.364, -6.8292, -1.4676], [26.364, -6.8292, 0.020648], [28.525, -6.2283, 0.020648], [28.525, -6.2283, -1.4676], [26.76, -8.2534, -1.4676], [26.76, -8.2534, 0.020648], [28.921, -7.6525, 0.020648], [28.921, -7.6525, -1.4676]], [[18.102, -28.42, -1.9028], [18.102, -28.42, -0.49495], [21.337, -27.085, -0.49495], [21.337, -27.085, -1.9028], [18.7, -29.87, -1.9028], [18.7, -29.87, -0.49495], [21.935, -28.535, -0.49495], [21.935, -28.535, -1.9028]], [[28.612, -18.552, -1.7879], [28.612, -18.552, -0.29959], [26.398, -15.474, -0.29959], [26.398, -15.474, -1.7879], [30.032, -17.53, -1.7879], [30.032, -17.53, -0.29959], [27.818, -14.452, -0.29959], [27.818, -14.452, -1.7879]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.0001, atol=1e-07)\n    new_box1 = boxes.new_box([[1, 2, 3, 4, 5, 6, 7]])\n    assert torch.allclose(new_box1.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    assert new_box1.device == boxes.device\n    assert new_box1.with_yaw == boxes.with_yaw\n    assert new_box1.box_dim == boxes.box_dim\n    new_box2 = boxes.new_box(np.array([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box2.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))\n    new_box3 = boxes.new_box(torch.tensor([[1, 2, 3, 4, 5, 6, 7]]))\n    assert torch.allclose(new_box3.tensor, torch.tensor([[1, 2, 3, 4, 5, 6, 7]], dtype=boxes.tensor.dtype))"
        ]
    },
    {
        "func_name": "test_boxes_conversion",
        "original": "def test_boxes_conversion():\n    \"\"\"Test the conversion of boxes between different modes.\n\n    CommandLine:\n        xdoctest tests/test_box3d.py::test_boxes_conversion zero\n    \"\"\"\n    lidar_boxes = LiDARInstance3DBoxes([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    cam_box_tensor = Box3DMode.convert(lidar_boxes.tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    expected_box = lidar_boxes.convert_to(Box3DMode.CAM)\n    assert torch.equal(expected_box.tensor, cam_box_tensor)\n    cam_boxes = CameraInstance3DBoxes(cam_box_tensor)\n    assert torch.equal(cam_boxes.height, lidar_boxes.height)\n    assert torch.equal(cam_boxes.top_height, -lidar_boxes.top_height)\n    assert torch.equal(cam_boxes.bottom_height, -lidar_boxes.bottom_height)\n    assert torch.allclose(cam_boxes.volume, lidar_boxes.volume)\n    lidar_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.LIDAR)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    assert torch.allclose(expected_tensor, lidar_box_tensor)\n    assert torch.allclose(lidar_boxes.tensor, lidar_box_tensor)\n    depth_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.DEPTH)\n    depth_to_cam_box_tensor = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.CAM)\n    assert torch.allclose(cam_box_tensor, depth_to_cam_box_tensor)\n    same_results = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.DEPTH)\n    assert torch.equal(same_results, depth_box_tensor)\n    camera_boxes = CameraInstance3DBoxes([[0.06, 1.77, 21.4, 3.2, 1.61, 1.66, -1.54], [6.59, 1.53, 6.76, 12.78, 3.66, 2.28, 1.55], [6.71, 1.59, 22.18, 14.73, 3.64, 2.32, 1.59], [7.11, 1.58, 34.54, 10.04, 3.61, 2.32, 1.61], [7.78, 1.65, 45.95, 12.83, 3.63, 2.34, 1.64]])\n    rect = torch.tensor([[0.9999239, 0.00983776, -0.00744505, 0.0], [-0.0098698, 0.9999421, -0.00427846, 0.0], [0.00740253, 0.00435161, 0.9999631, 0.0], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    Trv2c = torch.tensor([[0.007533745, -0.9999714, -0.000616602, -0.004069766], [0.01480249, 0.0007280733, -0.9998902, -0.07631618], [0.9998621, 0.00752379, 0.01480755, -0.2717806], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    expected_tensor = torch.tensor([[21.6902434, -0.0406038554, -1.61906639, 3.20000005, 1.65999997, 1.61000001, 1.53999996 - np.pi / 2], [7.05006905, -6.57459601, -1.60107949, 12.7799997, 2.27999997, 3.66000009, -1.54999995 - np.pi / 2], [22.4698818, -6.69203759, -1.50118145, 14.7299995, 2.31999993, 3.6400001, -1.59000003 + 3 * np.pi / 2], [34.8291965, -7.09058388, -1.36622983, 10.04, 2.31999993, 3.6099999, -1.61000001 + 3 * np.pi / 2], [46.2394617, -7.758388, -1.3240502, 12.8299999, 2.33999991, 3.63000011, -1.63999999 + 3 * np.pi / 2]], dtype=torch.float32)\n    rt_mat = rect @ Trv2c\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes, Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse())\n    assert torch.allclose(cam_to_lidar_box.tensor, expected_tensor)\n    lidar_to_cam_box = Box3DMode.convert(cam_to_lidar_box.tensor, Box3DMode.LIDAR, Box3DMode.CAM, rt_mat)\n    assert torch.allclose(lidar_to_cam_box, camera_boxes.tensor)\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor.numpy(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(cam_to_lidar_box, expected_tensor.numpy())\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor[0].numpy().tolist(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(np.array(cam_to_lidar_box), expected_tensor[0].numpy())\n    depth_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_to_lidar_box = depth_boxes.convert_to(Box3DMode.LIDAR)\n    expected_box = depth_to_lidar_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(depth_boxes.tensor, expected_box.tensor)\n    lidar_to_depth_box = Box3DMode.convert(depth_to_lidar_box, Box3DMode.LIDAR, Box3DMode.DEPTH)\n    assert torch.allclose(depth_boxes.tensor, lidar_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, lidar_to_depth_box.volume)\n    depth_to_cam_box = Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, Box3DMode.CAM)\n    cam_to_depth_box = Box3DMode.convert(depth_to_cam_box, Box3DMode.CAM, Box3DMode.DEPTH)\n    expected_tensor = depth_to_cam_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(expected_tensor.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, cam_to_depth_box.volume)\n    with pytest.raises(NotImplementedError):\n        Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, 3)",
        "mutated": [
            "def test_boxes_conversion():\n    if False:\n        i = 10\n    'Test the conversion of boxes between different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes_conversion zero\\n    '\n    lidar_boxes = LiDARInstance3DBoxes([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    cam_box_tensor = Box3DMode.convert(lidar_boxes.tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    expected_box = lidar_boxes.convert_to(Box3DMode.CAM)\n    assert torch.equal(expected_box.tensor, cam_box_tensor)\n    cam_boxes = CameraInstance3DBoxes(cam_box_tensor)\n    assert torch.equal(cam_boxes.height, lidar_boxes.height)\n    assert torch.equal(cam_boxes.top_height, -lidar_boxes.top_height)\n    assert torch.equal(cam_boxes.bottom_height, -lidar_boxes.bottom_height)\n    assert torch.allclose(cam_boxes.volume, lidar_boxes.volume)\n    lidar_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.LIDAR)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    assert torch.allclose(expected_tensor, lidar_box_tensor)\n    assert torch.allclose(lidar_boxes.tensor, lidar_box_tensor)\n    depth_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.DEPTH)\n    depth_to_cam_box_tensor = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.CAM)\n    assert torch.allclose(cam_box_tensor, depth_to_cam_box_tensor)\n    same_results = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.DEPTH)\n    assert torch.equal(same_results, depth_box_tensor)\n    camera_boxes = CameraInstance3DBoxes([[0.06, 1.77, 21.4, 3.2, 1.61, 1.66, -1.54], [6.59, 1.53, 6.76, 12.78, 3.66, 2.28, 1.55], [6.71, 1.59, 22.18, 14.73, 3.64, 2.32, 1.59], [7.11, 1.58, 34.54, 10.04, 3.61, 2.32, 1.61], [7.78, 1.65, 45.95, 12.83, 3.63, 2.34, 1.64]])\n    rect = torch.tensor([[0.9999239, 0.00983776, -0.00744505, 0.0], [-0.0098698, 0.9999421, -0.00427846, 0.0], [0.00740253, 0.00435161, 0.9999631, 0.0], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    Trv2c = torch.tensor([[0.007533745, -0.9999714, -0.000616602, -0.004069766], [0.01480249, 0.0007280733, -0.9998902, -0.07631618], [0.9998621, 0.00752379, 0.01480755, -0.2717806], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    expected_tensor = torch.tensor([[21.6902434, -0.0406038554, -1.61906639, 3.20000005, 1.65999997, 1.61000001, 1.53999996 - np.pi / 2], [7.05006905, -6.57459601, -1.60107949, 12.7799997, 2.27999997, 3.66000009, -1.54999995 - np.pi / 2], [22.4698818, -6.69203759, -1.50118145, 14.7299995, 2.31999993, 3.6400001, -1.59000003 + 3 * np.pi / 2], [34.8291965, -7.09058388, -1.36622983, 10.04, 2.31999993, 3.6099999, -1.61000001 + 3 * np.pi / 2], [46.2394617, -7.758388, -1.3240502, 12.8299999, 2.33999991, 3.63000011, -1.63999999 + 3 * np.pi / 2]], dtype=torch.float32)\n    rt_mat = rect @ Trv2c\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes, Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse())\n    assert torch.allclose(cam_to_lidar_box.tensor, expected_tensor)\n    lidar_to_cam_box = Box3DMode.convert(cam_to_lidar_box.tensor, Box3DMode.LIDAR, Box3DMode.CAM, rt_mat)\n    assert torch.allclose(lidar_to_cam_box, camera_boxes.tensor)\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor.numpy(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(cam_to_lidar_box, expected_tensor.numpy())\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor[0].numpy().tolist(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(np.array(cam_to_lidar_box), expected_tensor[0].numpy())\n    depth_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_to_lidar_box = depth_boxes.convert_to(Box3DMode.LIDAR)\n    expected_box = depth_to_lidar_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(depth_boxes.tensor, expected_box.tensor)\n    lidar_to_depth_box = Box3DMode.convert(depth_to_lidar_box, Box3DMode.LIDAR, Box3DMode.DEPTH)\n    assert torch.allclose(depth_boxes.tensor, lidar_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, lidar_to_depth_box.volume)\n    depth_to_cam_box = Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, Box3DMode.CAM)\n    cam_to_depth_box = Box3DMode.convert(depth_to_cam_box, Box3DMode.CAM, Box3DMode.DEPTH)\n    expected_tensor = depth_to_cam_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(expected_tensor.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, cam_to_depth_box.volume)\n    with pytest.raises(NotImplementedError):\n        Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, 3)",
            "def test_boxes_conversion():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test the conversion of boxes between different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes_conversion zero\\n    '\n    lidar_boxes = LiDARInstance3DBoxes([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    cam_box_tensor = Box3DMode.convert(lidar_boxes.tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    expected_box = lidar_boxes.convert_to(Box3DMode.CAM)\n    assert torch.equal(expected_box.tensor, cam_box_tensor)\n    cam_boxes = CameraInstance3DBoxes(cam_box_tensor)\n    assert torch.equal(cam_boxes.height, lidar_boxes.height)\n    assert torch.equal(cam_boxes.top_height, -lidar_boxes.top_height)\n    assert torch.equal(cam_boxes.bottom_height, -lidar_boxes.bottom_height)\n    assert torch.allclose(cam_boxes.volume, lidar_boxes.volume)\n    lidar_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.LIDAR)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    assert torch.allclose(expected_tensor, lidar_box_tensor)\n    assert torch.allclose(lidar_boxes.tensor, lidar_box_tensor)\n    depth_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.DEPTH)\n    depth_to_cam_box_tensor = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.CAM)\n    assert torch.allclose(cam_box_tensor, depth_to_cam_box_tensor)\n    same_results = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.DEPTH)\n    assert torch.equal(same_results, depth_box_tensor)\n    camera_boxes = CameraInstance3DBoxes([[0.06, 1.77, 21.4, 3.2, 1.61, 1.66, -1.54], [6.59, 1.53, 6.76, 12.78, 3.66, 2.28, 1.55], [6.71, 1.59, 22.18, 14.73, 3.64, 2.32, 1.59], [7.11, 1.58, 34.54, 10.04, 3.61, 2.32, 1.61], [7.78, 1.65, 45.95, 12.83, 3.63, 2.34, 1.64]])\n    rect = torch.tensor([[0.9999239, 0.00983776, -0.00744505, 0.0], [-0.0098698, 0.9999421, -0.00427846, 0.0], [0.00740253, 0.00435161, 0.9999631, 0.0], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    Trv2c = torch.tensor([[0.007533745, -0.9999714, -0.000616602, -0.004069766], [0.01480249, 0.0007280733, -0.9998902, -0.07631618], [0.9998621, 0.00752379, 0.01480755, -0.2717806], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    expected_tensor = torch.tensor([[21.6902434, -0.0406038554, -1.61906639, 3.20000005, 1.65999997, 1.61000001, 1.53999996 - np.pi / 2], [7.05006905, -6.57459601, -1.60107949, 12.7799997, 2.27999997, 3.66000009, -1.54999995 - np.pi / 2], [22.4698818, -6.69203759, -1.50118145, 14.7299995, 2.31999993, 3.6400001, -1.59000003 + 3 * np.pi / 2], [34.8291965, -7.09058388, -1.36622983, 10.04, 2.31999993, 3.6099999, -1.61000001 + 3 * np.pi / 2], [46.2394617, -7.758388, -1.3240502, 12.8299999, 2.33999991, 3.63000011, -1.63999999 + 3 * np.pi / 2]], dtype=torch.float32)\n    rt_mat = rect @ Trv2c\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes, Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse())\n    assert torch.allclose(cam_to_lidar_box.tensor, expected_tensor)\n    lidar_to_cam_box = Box3DMode.convert(cam_to_lidar_box.tensor, Box3DMode.LIDAR, Box3DMode.CAM, rt_mat)\n    assert torch.allclose(lidar_to_cam_box, camera_boxes.tensor)\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor.numpy(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(cam_to_lidar_box, expected_tensor.numpy())\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor[0].numpy().tolist(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(np.array(cam_to_lidar_box), expected_tensor[0].numpy())\n    depth_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_to_lidar_box = depth_boxes.convert_to(Box3DMode.LIDAR)\n    expected_box = depth_to_lidar_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(depth_boxes.tensor, expected_box.tensor)\n    lidar_to_depth_box = Box3DMode.convert(depth_to_lidar_box, Box3DMode.LIDAR, Box3DMode.DEPTH)\n    assert torch.allclose(depth_boxes.tensor, lidar_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, lidar_to_depth_box.volume)\n    depth_to_cam_box = Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, Box3DMode.CAM)\n    cam_to_depth_box = Box3DMode.convert(depth_to_cam_box, Box3DMode.CAM, Box3DMode.DEPTH)\n    expected_tensor = depth_to_cam_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(expected_tensor.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, cam_to_depth_box.volume)\n    with pytest.raises(NotImplementedError):\n        Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, 3)",
            "def test_boxes_conversion():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test the conversion of boxes between different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes_conversion zero\\n    '\n    lidar_boxes = LiDARInstance3DBoxes([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    cam_box_tensor = Box3DMode.convert(lidar_boxes.tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    expected_box = lidar_boxes.convert_to(Box3DMode.CAM)\n    assert torch.equal(expected_box.tensor, cam_box_tensor)\n    cam_boxes = CameraInstance3DBoxes(cam_box_tensor)\n    assert torch.equal(cam_boxes.height, lidar_boxes.height)\n    assert torch.equal(cam_boxes.top_height, -lidar_boxes.top_height)\n    assert torch.equal(cam_boxes.bottom_height, -lidar_boxes.bottom_height)\n    assert torch.allclose(cam_boxes.volume, lidar_boxes.volume)\n    lidar_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.LIDAR)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    assert torch.allclose(expected_tensor, lidar_box_tensor)\n    assert torch.allclose(lidar_boxes.tensor, lidar_box_tensor)\n    depth_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.DEPTH)\n    depth_to_cam_box_tensor = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.CAM)\n    assert torch.allclose(cam_box_tensor, depth_to_cam_box_tensor)\n    same_results = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.DEPTH)\n    assert torch.equal(same_results, depth_box_tensor)\n    camera_boxes = CameraInstance3DBoxes([[0.06, 1.77, 21.4, 3.2, 1.61, 1.66, -1.54], [6.59, 1.53, 6.76, 12.78, 3.66, 2.28, 1.55], [6.71, 1.59, 22.18, 14.73, 3.64, 2.32, 1.59], [7.11, 1.58, 34.54, 10.04, 3.61, 2.32, 1.61], [7.78, 1.65, 45.95, 12.83, 3.63, 2.34, 1.64]])\n    rect = torch.tensor([[0.9999239, 0.00983776, -0.00744505, 0.0], [-0.0098698, 0.9999421, -0.00427846, 0.0], [0.00740253, 0.00435161, 0.9999631, 0.0], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    Trv2c = torch.tensor([[0.007533745, -0.9999714, -0.000616602, -0.004069766], [0.01480249, 0.0007280733, -0.9998902, -0.07631618], [0.9998621, 0.00752379, 0.01480755, -0.2717806], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    expected_tensor = torch.tensor([[21.6902434, -0.0406038554, -1.61906639, 3.20000005, 1.65999997, 1.61000001, 1.53999996 - np.pi / 2], [7.05006905, -6.57459601, -1.60107949, 12.7799997, 2.27999997, 3.66000009, -1.54999995 - np.pi / 2], [22.4698818, -6.69203759, -1.50118145, 14.7299995, 2.31999993, 3.6400001, -1.59000003 + 3 * np.pi / 2], [34.8291965, -7.09058388, -1.36622983, 10.04, 2.31999993, 3.6099999, -1.61000001 + 3 * np.pi / 2], [46.2394617, -7.758388, -1.3240502, 12.8299999, 2.33999991, 3.63000011, -1.63999999 + 3 * np.pi / 2]], dtype=torch.float32)\n    rt_mat = rect @ Trv2c\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes, Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse())\n    assert torch.allclose(cam_to_lidar_box.tensor, expected_tensor)\n    lidar_to_cam_box = Box3DMode.convert(cam_to_lidar_box.tensor, Box3DMode.LIDAR, Box3DMode.CAM, rt_mat)\n    assert torch.allclose(lidar_to_cam_box, camera_boxes.tensor)\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor.numpy(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(cam_to_lidar_box, expected_tensor.numpy())\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor[0].numpy().tolist(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(np.array(cam_to_lidar_box), expected_tensor[0].numpy())\n    depth_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_to_lidar_box = depth_boxes.convert_to(Box3DMode.LIDAR)\n    expected_box = depth_to_lidar_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(depth_boxes.tensor, expected_box.tensor)\n    lidar_to_depth_box = Box3DMode.convert(depth_to_lidar_box, Box3DMode.LIDAR, Box3DMode.DEPTH)\n    assert torch.allclose(depth_boxes.tensor, lidar_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, lidar_to_depth_box.volume)\n    depth_to_cam_box = Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, Box3DMode.CAM)\n    cam_to_depth_box = Box3DMode.convert(depth_to_cam_box, Box3DMode.CAM, Box3DMode.DEPTH)\n    expected_tensor = depth_to_cam_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(expected_tensor.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, cam_to_depth_box.volume)\n    with pytest.raises(NotImplementedError):\n        Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, 3)",
            "def test_boxes_conversion():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test the conversion of boxes between different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes_conversion zero\\n    '\n    lidar_boxes = LiDARInstance3DBoxes([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    cam_box_tensor = Box3DMode.convert(lidar_boxes.tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    expected_box = lidar_boxes.convert_to(Box3DMode.CAM)\n    assert torch.equal(expected_box.tensor, cam_box_tensor)\n    cam_boxes = CameraInstance3DBoxes(cam_box_tensor)\n    assert torch.equal(cam_boxes.height, lidar_boxes.height)\n    assert torch.equal(cam_boxes.top_height, -lidar_boxes.top_height)\n    assert torch.equal(cam_boxes.bottom_height, -lidar_boxes.bottom_height)\n    assert torch.allclose(cam_boxes.volume, lidar_boxes.volume)\n    lidar_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.LIDAR)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    assert torch.allclose(expected_tensor, lidar_box_tensor)\n    assert torch.allclose(lidar_boxes.tensor, lidar_box_tensor)\n    depth_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.DEPTH)\n    depth_to_cam_box_tensor = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.CAM)\n    assert torch.allclose(cam_box_tensor, depth_to_cam_box_tensor)\n    same_results = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.DEPTH)\n    assert torch.equal(same_results, depth_box_tensor)\n    camera_boxes = CameraInstance3DBoxes([[0.06, 1.77, 21.4, 3.2, 1.61, 1.66, -1.54], [6.59, 1.53, 6.76, 12.78, 3.66, 2.28, 1.55], [6.71, 1.59, 22.18, 14.73, 3.64, 2.32, 1.59], [7.11, 1.58, 34.54, 10.04, 3.61, 2.32, 1.61], [7.78, 1.65, 45.95, 12.83, 3.63, 2.34, 1.64]])\n    rect = torch.tensor([[0.9999239, 0.00983776, -0.00744505, 0.0], [-0.0098698, 0.9999421, -0.00427846, 0.0], [0.00740253, 0.00435161, 0.9999631, 0.0], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    Trv2c = torch.tensor([[0.007533745, -0.9999714, -0.000616602, -0.004069766], [0.01480249, 0.0007280733, -0.9998902, -0.07631618], [0.9998621, 0.00752379, 0.01480755, -0.2717806], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    expected_tensor = torch.tensor([[21.6902434, -0.0406038554, -1.61906639, 3.20000005, 1.65999997, 1.61000001, 1.53999996 - np.pi / 2], [7.05006905, -6.57459601, -1.60107949, 12.7799997, 2.27999997, 3.66000009, -1.54999995 - np.pi / 2], [22.4698818, -6.69203759, -1.50118145, 14.7299995, 2.31999993, 3.6400001, -1.59000003 + 3 * np.pi / 2], [34.8291965, -7.09058388, -1.36622983, 10.04, 2.31999993, 3.6099999, -1.61000001 + 3 * np.pi / 2], [46.2394617, -7.758388, -1.3240502, 12.8299999, 2.33999991, 3.63000011, -1.63999999 + 3 * np.pi / 2]], dtype=torch.float32)\n    rt_mat = rect @ Trv2c\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes, Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse())\n    assert torch.allclose(cam_to_lidar_box.tensor, expected_tensor)\n    lidar_to_cam_box = Box3DMode.convert(cam_to_lidar_box.tensor, Box3DMode.LIDAR, Box3DMode.CAM, rt_mat)\n    assert torch.allclose(lidar_to_cam_box, camera_boxes.tensor)\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor.numpy(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(cam_to_lidar_box, expected_tensor.numpy())\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor[0].numpy().tolist(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(np.array(cam_to_lidar_box), expected_tensor[0].numpy())\n    depth_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_to_lidar_box = depth_boxes.convert_to(Box3DMode.LIDAR)\n    expected_box = depth_to_lidar_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(depth_boxes.tensor, expected_box.tensor)\n    lidar_to_depth_box = Box3DMode.convert(depth_to_lidar_box, Box3DMode.LIDAR, Box3DMode.DEPTH)\n    assert torch.allclose(depth_boxes.tensor, lidar_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, lidar_to_depth_box.volume)\n    depth_to_cam_box = Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, Box3DMode.CAM)\n    cam_to_depth_box = Box3DMode.convert(depth_to_cam_box, Box3DMode.CAM, Box3DMode.DEPTH)\n    expected_tensor = depth_to_cam_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(expected_tensor.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, cam_to_depth_box.volume)\n    with pytest.raises(NotImplementedError):\n        Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, 3)",
            "def test_boxes_conversion():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test the conversion of boxes between different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes_conversion zero\\n    '\n    lidar_boxes = LiDARInstance3DBoxes([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    cam_box_tensor = Box3DMode.convert(lidar_boxes.tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    expected_box = lidar_boxes.convert_to(Box3DMode.CAM)\n    assert torch.equal(expected_box.tensor, cam_box_tensor)\n    cam_boxes = CameraInstance3DBoxes(cam_box_tensor)\n    assert torch.equal(cam_boxes.height, lidar_boxes.height)\n    assert torch.equal(cam_boxes.top_height, -lidar_boxes.top_height)\n    assert torch.equal(cam_boxes.bottom_height, -lidar_boxes.bottom_height)\n    assert torch.allclose(cam_boxes.volume, lidar_boxes.volume)\n    lidar_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.LIDAR)\n    expected_tensor = torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79]])\n    assert torch.allclose(expected_tensor, lidar_box_tensor)\n    assert torch.allclose(lidar_boxes.tensor, lidar_box_tensor)\n    depth_box_tensor = Box3DMode.convert(cam_box_tensor, Box3DMode.CAM, Box3DMode.DEPTH)\n    depth_to_cam_box_tensor = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.CAM)\n    assert torch.allclose(cam_box_tensor, depth_to_cam_box_tensor)\n    same_results = Box3DMode.convert(depth_box_tensor, Box3DMode.DEPTH, Box3DMode.DEPTH)\n    assert torch.equal(same_results, depth_box_tensor)\n    camera_boxes = CameraInstance3DBoxes([[0.06, 1.77, 21.4, 3.2, 1.61, 1.66, -1.54], [6.59, 1.53, 6.76, 12.78, 3.66, 2.28, 1.55], [6.71, 1.59, 22.18, 14.73, 3.64, 2.32, 1.59], [7.11, 1.58, 34.54, 10.04, 3.61, 2.32, 1.61], [7.78, 1.65, 45.95, 12.83, 3.63, 2.34, 1.64]])\n    rect = torch.tensor([[0.9999239, 0.00983776, -0.00744505, 0.0], [-0.0098698, 0.9999421, -0.00427846, 0.0], [0.00740253, 0.00435161, 0.9999631, 0.0], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    Trv2c = torch.tensor([[0.007533745, -0.9999714, -0.000616602, -0.004069766], [0.01480249, 0.0007280733, -0.9998902, -0.07631618], [0.9998621, 0.00752379, 0.01480755, -0.2717806], [0.0, 0.0, 0.0, 1.0]], dtype=torch.float32)\n    expected_tensor = torch.tensor([[21.6902434, -0.0406038554, -1.61906639, 3.20000005, 1.65999997, 1.61000001, 1.53999996 - np.pi / 2], [7.05006905, -6.57459601, -1.60107949, 12.7799997, 2.27999997, 3.66000009, -1.54999995 - np.pi / 2], [22.4698818, -6.69203759, -1.50118145, 14.7299995, 2.31999993, 3.6400001, -1.59000003 + 3 * np.pi / 2], [34.8291965, -7.09058388, -1.36622983, 10.04, 2.31999993, 3.6099999, -1.61000001 + 3 * np.pi / 2], [46.2394617, -7.758388, -1.3240502, 12.8299999, 2.33999991, 3.63000011, -1.63999999 + 3 * np.pi / 2]], dtype=torch.float32)\n    rt_mat = rect @ Trv2c\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes, Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse())\n    assert torch.allclose(cam_to_lidar_box.tensor, expected_tensor)\n    lidar_to_cam_box = Box3DMode.convert(cam_to_lidar_box.tensor, Box3DMode.LIDAR, Box3DMode.CAM, rt_mat)\n    assert torch.allclose(lidar_to_cam_box, camera_boxes.tensor)\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor.numpy(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(cam_to_lidar_box, expected_tensor.numpy())\n    cam_to_lidar_box = Box3DMode.convert(camera_boxes.tensor[0].numpy().tolist(), Box3DMode.CAM, Box3DMode.LIDAR, rt_mat.inverse().numpy())\n    assert np.allclose(np.array(cam_to_lidar_box), expected_tensor[0].numpy())\n    depth_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_to_lidar_box = depth_boxes.convert_to(Box3DMode.LIDAR)\n    expected_box = depth_to_lidar_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(depth_boxes.tensor, expected_box.tensor)\n    lidar_to_depth_box = Box3DMode.convert(depth_to_lidar_box, Box3DMode.LIDAR, Box3DMode.DEPTH)\n    assert torch.allclose(depth_boxes.tensor, lidar_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, lidar_to_depth_box.volume)\n    depth_to_cam_box = Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, Box3DMode.CAM)\n    cam_to_depth_box = Box3DMode.convert(depth_to_cam_box, Box3DMode.CAM, Box3DMode.DEPTH)\n    expected_tensor = depth_to_cam_box.convert_to(Box3DMode.DEPTH)\n    assert torch.equal(expected_tensor.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.tensor, cam_to_depth_box.tensor)\n    assert torch.allclose(depth_boxes.volume, cam_to_depth_box.volume)\n    with pytest.raises(NotImplementedError):\n        Box3DMode.convert(depth_boxes, Box3DMode.DEPTH, 3)"
        ]
    },
    {
        "func_name": "test_camera_boxes3d",
        "original": "def test_camera_boxes3d():\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=np.float32)\n    boxes_1 = Box3DMode.convert(LiDARInstance3DBoxes(np_boxes), Box3DMode.LIDAR, Box3DMode.CAM)\n    assert isinstance(boxes_1, CameraInstance3DBoxes)\n    cam_np_boxes = Box3DMode.convert(np_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    assert torch.allclose(boxes_1.tensor, boxes_1.tensor.new_tensor(cam_np_boxes))\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=torch.float32)\n    cam_th_boxes = Box3DMode.convert(th_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes_2 = CameraInstance3DBoxes(cam_th_boxes)\n    assert torch.allclose(boxes_2.tensor, cam_th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4 - 2 * np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes = CameraInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    points = torch.tensor([[0.6762, 1.4658, 1.2559], [0.8784, 1.3857, 4.7814], [-0.2517, 0.9697, 6.7053], [0.552, 0.5265, 0.6533], [-0.5358, 1.4741, 4.587]])\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 + 0.13603681398218054 * 4 - np.pi], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 + 0.13603681398218054 * 4 - np.pi], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 + 0.13603681398218054 * 4 - np.pi], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 + 0.13603681398218054 * 4 - np.pi], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 + 0.13603681398218054 * 4 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    points = boxes.flip('horizontal', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, 1.2559], [-0.8784, 1.3857, 4.7814], [0.2517, 0.9697, 6.7053], [-0.552, 0.5265, 0.6533], [0.5358, 1.4741, 4.587]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[2.5162, 1.7501, -1.7802, 1.75, 1.65, 3.39, 1.6616 + 0.13603681398218054 * 4 - np.pi / 2], [2.4567, 1.6357, -8.9594, 1.54, 1.57, 4.01, 1.5216 + 0.13603681398218054 * 4 - np.pi / 2], [-0.5558, 1.3033, -28.2967, 1.47, 1.48, 2.23, 4.7116 + 0.13603681398218054 * 4 - np.pi / 2], [21.823, 1.7361, -26.669, 1.56, 1.4, 3.48, 4.8316 + 0.13603681398218054 * 4 - np.pi / 2], [8.1621, 1.6218, -31.3198, 1.74, 1.48, 3.77, 0.3516 + 0.13603681398218054 * 4 - np.pi / 2]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, -1.2559], [-0.8784, 1.3857, -4.7814], [0.2517, 0.9697, -6.7053], [-0.552, 0.5265, -0.6533], [0.5358, 1.4741, -4.587]])\n    yaw_normalized_tensor = boxes_flip_vert.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 + 0.13603681398218054 * 2 - np.pi], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 + 0.13603681398218054 * 2 - np.pi], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 + 0.13603681398218054 * 2 - np.pi], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 + 0.13603681398218054 * 2 - np.pi], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 + 0.13603681398218054 * 2 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(0.13603681398218054), points)\n    expected_points = torch.tensor([[-0.8403, 1.4658, -1.1526], [-1.5187, 1.3857, -4.6181], [-0.66, 0.9697, -6.6775], [-0.6355, 0.5265, -0.5724], [-0.0912, 1.4741, -4.6173]])\n    expected_rot_mat_T = torch.tensor([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(-0.13603681398218054), points)\n    rot_mat = np.array([[0.99076125, 0.0, -0.13561762], [0.0, 1.0, 0.0], [0.13561762, 0.0, 0.99076125]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(0.13603681398218054), points_np)\n    expected_points_np = np.array([[0.4712, 1.2559, -1.544, 2.5359], [0.6824, 4.7814, -1.492, 0.7167], [-0.3809, 6.7053, -0.9266, 0.5599], [0.4755, 0.6533, -0.5965, 1.0032], [-0.7308, 4.587, -1.3878, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(-0.13603681398218054), points_np)\n    camera_points = CameraPoints(points_np, points_dim=4)\n    (camera_points, rot_mat_T_np) = boxes.rotate(rot_mat, camera_points)\n    points_np = camera_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.scale(1.00559866335275)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.translate(torch.tensor([0.13246193, 0.15701613, 0.0838056]))\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([-2, -5, 0, 20, 2, 22])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[3.0508, 1.1282, 1.7598, 3.409, -5.9203], [4.9332, 8.0981, 1.5486, 4.0325, -6.0603], [7.2409, 27.6424, 1.4782, 2.2425, -2.8703], [28.4773, 20.0183, 1.5687, 3.4995, -2.7503], [16.502, 28.2147, 1.7497, 3.7911, -0.9471]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 1.0, 0.5]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    lidar_expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    expected_tensor = lidar_expected_tensor.clone()\n    expected_tensor[:, 0::2] = -lidar_expected_tensor[:, [3, 1]]\n    expected_tensor[:, 1::2] = lidar_expected_tensor[:, 0::2]\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[2.8332, 0.25769, -0.77767], [1.6232, 0.25769, 2.4093], [1.6232, 1.9169, 2.4093], [2.8332, 1.9169, -0.77767], [4.4784, 0.25769, -0.15302], [3.2684, 0.25769, 3.034], [3.2684, 1.9169, 3.034], [4.4784, 1.9169, -0.15302]], [[4.6237, 0.2231, 5.9606], [3.7324, 0.2231, 9.8933], [3.7324, 1.8019, 9.8933], [4.6237, 1.8019, 5.9606], [6.134, 0.2231, 6.3029], [5.2427, 0.2231, 10.236], [5.2427, 1.8019, 10.236], [6.134, 1.8019, 6.3029]], [[6.8292, -0.020648, 26.364], [6.2283, -0.020648, 28.525], [6.2283, 1.4676, 28.525], [6.8292, 1.4676, 26.364], [8.2534, -0.020648, 26.76], [7.6525, -0.020648, 28.921], [7.6525, 1.4676, 28.921], [8.2534, 1.4676, 26.76]], [[28.42, 0.49495, 18.102], [27.085, 0.49495, 21.337], [27.085, 1.9028, 21.337], [28.42, 1.9028, 18.102], [29.87, 0.49495, 18.7], [28.535, 0.49495, 21.935], [28.535, 1.9028, 21.935], [29.87, 1.9028, 18.7]], [[14.452, 0.29959, 27.818], [17.53, 0.29959, 30.032], [17.53, 1.7879, 30.032], [14.452, 1.7879, 27.818], [15.474, 0.29959, 26.398], [18.552, 0.29959, 28.612], [18.552, 1.7879, 28.612], [15.474, 1.7879, 26.398]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.001, atol=0.0001)\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996]], dtype=torch.float32)\n    boxes_origin_given = CameraInstance3DBoxes(th_boxes.clone(), box_dim=7, origin=(0.5, 0.5, 0.5))\n    expected_tensor = th_boxes.clone()\n    expected_tensor[:, :3] = th_boxes[:, :3] + th_boxes[:, 3:6] * (th_boxes.new_tensor((0.5, 1.0, 0.5)) - th_boxes.new_tensor((0.5, 0.5, 0.5)))\n    assert torch.allclose(boxes_origin_given.tensor, expected_tensor)",
        "mutated": [
            "def test_camera_boxes3d():\n    if False:\n        i = 10\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=np.float32)\n    boxes_1 = Box3DMode.convert(LiDARInstance3DBoxes(np_boxes), Box3DMode.LIDAR, Box3DMode.CAM)\n    assert isinstance(boxes_1, CameraInstance3DBoxes)\n    cam_np_boxes = Box3DMode.convert(np_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    assert torch.allclose(boxes_1.tensor, boxes_1.tensor.new_tensor(cam_np_boxes))\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=torch.float32)\n    cam_th_boxes = Box3DMode.convert(th_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes_2 = CameraInstance3DBoxes(cam_th_boxes)\n    assert torch.allclose(boxes_2.tensor, cam_th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4 - 2 * np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes = CameraInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    points = torch.tensor([[0.6762, 1.4658, 1.2559], [0.8784, 1.3857, 4.7814], [-0.2517, 0.9697, 6.7053], [0.552, 0.5265, 0.6533], [-0.5358, 1.4741, 4.587]])\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 + 0.13603681398218054 * 4 - np.pi], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 + 0.13603681398218054 * 4 - np.pi], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 + 0.13603681398218054 * 4 - np.pi], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 + 0.13603681398218054 * 4 - np.pi], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 + 0.13603681398218054 * 4 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    points = boxes.flip('horizontal', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, 1.2559], [-0.8784, 1.3857, 4.7814], [0.2517, 0.9697, 6.7053], [-0.552, 0.5265, 0.6533], [0.5358, 1.4741, 4.587]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[2.5162, 1.7501, -1.7802, 1.75, 1.65, 3.39, 1.6616 + 0.13603681398218054 * 4 - np.pi / 2], [2.4567, 1.6357, -8.9594, 1.54, 1.57, 4.01, 1.5216 + 0.13603681398218054 * 4 - np.pi / 2], [-0.5558, 1.3033, -28.2967, 1.47, 1.48, 2.23, 4.7116 + 0.13603681398218054 * 4 - np.pi / 2], [21.823, 1.7361, -26.669, 1.56, 1.4, 3.48, 4.8316 + 0.13603681398218054 * 4 - np.pi / 2], [8.1621, 1.6218, -31.3198, 1.74, 1.48, 3.77, 0.3516 + 0.13603681398218054 * 4 - np.pi / 2]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, -1.2559], [-0.8784, 1.3857, -4.7814], [0.2517, 0.9697, -6.7053], [-0.552, 0.5265, -0.6533], [0.5358, 1.4741, -4.587]])\n    yaw_normalized_tensor = boxes_flip_vert.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 + 0.13603681398218054 * 2 - np.pi], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 + 0.13603681398218054 * 2 - np.pi], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 + 0.13603681398218054 * 2 - np.pi], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 + 0.13603681398218054 * 2 - np.pi], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 + 0.13603681398218054 * 2 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(0.13603681398218054), points)\n    expected_points = torch.tensor([[-0.8403, 1.4658, -1.1526], [-1.5187, 1.3857, -4.6181], [-0.66, 0.9697, -6.6775], [-0.6355, 0.5265, -0.5724], [-0.0912, 1.4741, -4.6173]])\n    expected_rot_mat_T = torch.tensor([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(-0.13603681398218054), points)\n    rot_mat = np.array([[0.99076125, 0.0, -0.13561762], [0.0, 1.0, 0.0], [0.13561762, 0.0, 0.99076125]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(0.13603681398218054), points_np)\n    expected_points_np = np.array([[0.4712, 1.2559, -1.544, 2.5359], [0.6824, 4.7814, -1.492, 0.7167], [-0.3809, 6.7053, -0.9266, 0.5599], [0.4755, 0.6533, -0.5965, 1.0032], [-0.7308, 4.587, -1.3878, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(-0.13603681398218054), points_np)\n    camera_points = CameraPoints(points_np, points_dim=4)\n    (camera_points, rot_mat_T_np) = boxes.rotate(rot_mat, camera_points)\n    points_np = camera_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.scale(1.00559866335275)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.translate(torch.tensor([0.13246193, 0.15701613, 0.0838056]))\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([-2, -5, 0, 20, 2, 22])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[3.0508, 1.1282, 1.7598, 3.409, -5.9203], [4.9332, 8.0981, 1.5486, 4.0325, -6.0603], [7.2409, 27.6424, 1.4782, 2.2425, -2.8703], [28.4773, 20.0183, 1.5687, 3.4995, -2.7503], [16.502, 28.2147, 1.7497, 3.7911, -0.9471]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 1.0, 0.5]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    lidar_expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    expected_tensor = lidar_expected_tensor.clone()\n    expected_tensor[:, 0::2] = -lidar_expected_tensor[:, [3, 1]]\n    expected_tensor[:, 1::2] = lidar_expected_tensor[:, 0::2]\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[2.8332, 0.25769, -0.77767], [1.6232, 0.25769, 2.4093], [1.6232, 1.9169, 2.4093], [2.8332, 1.9169, -0.77767], [4.4784, 0.25769, -0.15302], [3.2684, 0.25769, 3.034], [3.2684, 1.9169, 3.034], [4.4784, 1.9169, -0.15302]], [[4.6237, 0.2231, 5.9606], [3.7324, 0.2231, 9.8933], [3.7324, 1.8019, 9.8933], [4.6237, 1.8019, 5.9606], [6.134, 0.2231, 6.3029], [5.2427, 0.2231, 10.236], [5.2427, 1.8019, 10.236], [6.134, 1.8019, 6.3029]], [[6.8292, -0.020648, 26.364], [6.2283, -0.020648, 28.525], [6.2283, 1.4676, 28.525], [6.8292, 1.4676, 26.364], [8.2534, -0.020648, 26.76], [7.6525, -0.020648, 28.921], [7.6525, 1.4676, 28.921], [8.2534, 1.4676, 26.76]], [[28.42, 0.49495, 18.102], [27.085, 0.49495, 21.337], [27.085, 1.9028, 21.337], [28.42, 1.9028, 18.102], [29.87, 0.49495, 18.7], [28.535, 0.49495, 21.935], [28.535, 1.9028, 21.935], [29.87, 1.9028, 18.7]], [[14.452, 0.29959, 27.818], [17.53, 0.29959, 30.032], [17.53, 1.7879, 30.032], [14.452, 1.7879, 27.818], [15.474, 0.29959, 26.398], [18.552, 0.29959, 28.612], [18.552, 1.7879, 28.612], [15.474, 1.7879, 26.398]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.001, atol=0.0001)\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996]], dtype=torch.float32)\n    boxes_origin_given = CameraInstance3DBoxes(th_boxes.clone(), box_dim=7, origin=(0.5, 0.5, 0.5))\n    expected_tensor = th_boxes.clone()\n    expected_tensor[:, :3] = th_boxes[:, :3] + th_boxes[:, 3:6] * (th_boxes.new_tensor((0.5, 1.0, 0.5)) - th_boxes.new_tensor((0.5, 0.5, 0.5)))\n    assert torch.allclose(boxes_origin_given.tensor, expected_tensor)",
            "def test_camera_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=np.float32)\n    boxes_1 = Box3DMode.convert(LiDARInstance3DBoxes(np_boxes), Box3DMode.LIDAR, Box3DMode.CAM)\n    assert isinstance(boxes_1, CameraInstance3DBoxes)\n    cam_np_boxes = Box3DMode.convert(np_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    assert torch.allclose(boxes_1.tensor, boxes_1.tensor.new_tensor(cam_np_boxes))\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=torch.float32)\n    cam_th_boxes = Box3DMode.convert(th_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes_2 = CameraInstance3DBoxes(cam_th_boxes)\n    assert torch.allclose(boxes_2.tensor, cam_th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4 - 2 * np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes = CameraInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    points = torch.tensor([[0.6762, 1.4658, 1.2559], [0.8784, 1.3857, 4.7814], [-0.2517, 0.9697, 6.7053], [0.552, 0.5265, 0.6533], [-0.5358, 1.4741, 4.587]])\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 + 0.13603681398218054 * 4 - np.pi], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 + 0.13603681398218054 * 4 - np.pi], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 + 0.13603681398218054 * 4 - np.pi], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 + 0.13603681398218054 * 4 - np.pi], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 + 0.13603681398218054 * 4 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    points = boxes.flip('horizontal', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, 1.2559], [-0.8784, 1.3857, 4.7814], [0.2517, 0.9697, 6.7053], [-0.552, 0.5265, 0.6533], [0.5358, 1.4741, 4.587]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[2.5162, 1.7501, -1.7802, 1.75, 1.65, 3.39, 1.6616 + 0.13603681398218054 * 4 - np.pi / 2], [2.4567, 1.6357, -8.9594, 1.54, 1.57, 4.01, 1.5216 + 0.13603681398218054 * 4 - np.pi / 2], [-0.5558, 1.3033, -28.2967, 1.47, 1.48, 2.23, 4.7116 + 0.13603681398218054 * 4 - np.pi / 2], [21.823, 1.7361, -26.669, 1.56, 1.4, 3.48, 4.8316 + 0.13603681398218054 * 4 - np.pi / 2], [8.1621, 1.6218, -31.3198, 1.74, 1.48, 3.77, 0.3516 + 0.13603681398218054 * 4 - np.pi / 2]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, -1.2559], [-0.8784, 1.3857, -4.7814], [0.2517, 0.9697, -6.7053], [-0.552, 0.5265, -0.6533], [0.5358, 1.4741, -4.587]])\n    yaw_normalized_tensor = boxes_flip_vert.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 + 0.13603681398218054 * 2 - np.pi], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 + 0.13603681398218054 * 2 - np.pi], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 + 0.13603681398218054 * 2 - np.pi], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 + 0.13603681398218054 * 2 - np.pi], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 + 0.13603681398218054 * 2 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(0.13603681398218054), points)\n    expected_points = torch.tensor([[-0.8403, 1.4658, -1.1526], [-1.5187, 1.3857, -4.6181], [-0.66, 0.9697, -6.6775], [-0.6355, 0.5265, -0.5724], [-0.0912, 1.4741, -4.6173]])\n    expected_rot_mat_T = torch.tensor([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(-0.13603681398218054), points)\n    rot_mat = np.array([[0.99076125, 0.0, -0.13561762], [0.0, 1.0, 0.0], [0.13561762, 0.0, 0.99076125]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(0.13603681398218054), points_np)\n    expected_points_np = np.array([[0.4712, 1.2559, -1.544, 2.5359], [0.6824, 4.7814, -1.492, 0.7167], [-0.3809, 6.7053, -0.9266, 0.5599], [0.4755, 0.6533, -0.5965, 1.0032], [-0.7308, 4.587, -1.3878, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(-0.13603681398218054), points_np)\n    camera_points = CameraPoints(points_np, points_dim=4)\n    (camera_points, rot_mat_T_np) = boxes.rotate(rot_mat, camera_points)\n    points_np = camera_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.scale(1.00559866335275)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.translate(torch.tensor([0.13246193, 0.15701613, 0.0838056]))\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([-2, -5, 0, 20, 2, 22])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[3.0508, 1.1282, 1.7598, 3.409, -5.9203], [4.9332, 8.0981, 1.5486, 4.0325, -6.0603], [7.2409, 27.6424, 1.4782, 2.2425, -2.8703], [28.4773, 20.0183, 1.5687, 3.4995, -2.7503], [16.502, 28.2147, 1.7497, 3.7911, -0.9471]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 1.0, 0.5]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    lidar_expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    expected_tensor = lidar_expected_tensor.clone()\n    expected_tensor[:, 0::2] = -lidar_expected_tensor[:, [3, 1]]\n    expected_tensor[:, 1::2] = lidar_expected_tensor[:, 0::2]\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[2.8332, 0.25769, -0.77767], [1.6232, 0.25769, 2.4093], [1.6232, 1.9169, 2.4093], [2.8332, 1.9169, -0.77767], [4.4784, 0.25769, -0.15302], [3.2684, 0.25769, 3.034], [3.2684, 1.9169, 3.034], [4.4784, 1.9169, -0.15302]], [[4.6237, 0.2231, 5.9606], [3.7324, 0.2231, 9.8933], [3.7324, 1.8019, 9.8933], [4.6237, 1.8019, 5.9606], [6.134, 0.2231, 6.3029], [5.2427, 0.2231, 10.236], [5.2427, 1.8019, 10.236], [6.134, 1.8019, 6.3029]], [[6.8292, -0.020648, 26.364], [6.2283, -0.020648, 28.525], [6.2283, 1.4676, 28.525], [6.8292, 1.4676, 26.364], [8.2534, -0.020648, 26.76], [7.6525, -0.020648, 28.921], [7.6525, 1.4676, 28.921], [8.2534, 1.4676, 26.76]], [[28.42, 0.49495, 18.102], [27.085, 0.49495, 21.337], [27.085, 1.9028, 21.337], [28.42, 1.9028, 18.102], [29.87, 0.49495, 18.7], [28.535, 0.49495, 21.935], [28.535, 1.9028, 21.935], [29.87, 1.9028, 18.7]], [[14.452, 0.29959, 27.818], [17.53, 0.29959, 30.032], [17.53, 1.7879, 30.032], [14.452, 1.7879, 27.818], [15.474, 0.29959, 26.398], [18.552, 0.29959, 28.612], [18.552, 1.7879, 28.612], [15.474, 1.7879, 26.398]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.001, atol=0.0001)\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996]], dtype=torch.float32)\n    boxes_origin_given = CameraInstance3DBoxes(th_boxes.clone(), box_dim=7, origin=(0.5, 0.5, 0.5))\n    expected_tensor = th_boxes.clone()\n    expected_tensor[:, :3] = th_boxes[:, :3] + th_boxes[:, 3:6] * (th_boxes.new_tensor((0.5, 1.0, 0.5)) - th_boxes.new_tensor((0.5, 0.5, 0.5)))\n    assert torch.allclose(boxes_origin_given.tensor, expected_tensor)",
            "def test_camera_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=np.float32)\n    boxes_1 = Box3DMode.convert(LiDARInstance3DBoxes(np_boxes), Box3DMode.LIDAR, Box3DMode.CAM)\n    assert isinstance(boxes_1, CameraInstance3DBoxes)\n    cam_np_boxes = Box3DMode.convert(np_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    assert torch.allclose(boxes_1.tensor, boxes_1.tensor.new_tensor(cam_np_boxes))\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=torch.float32)\n    cam_th_boxes = Box3DMode.convert(th_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes_2 = CameraInstance3DBoxes(cam_th_boxes)\n    assert torch.allclose(boxes_2.tensor, cam_th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4 - 2 * np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes = CameraInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    points = torch.tensor([[0.6762, 1.4658, 1.2559], [0.8784, 1.3857, 4.7814], [-0.2517, 0.9697, 6.7053], [0.552, 0.5265, 0.6533], [-0.5358, 1.4741, 4.587]])\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 + 0.13603681398218054 * 4 - np.pi], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 + 0.13603681398218054 * 4 - np.pi], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 + 0.13603681398218054 * 4 - np.pi], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 + 0.13603681398218054 * 4 - np.pi], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 + 0.13603681398218054 * 4 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    points = boxes.flip('horizontal', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, 1.2559], [-0.8784, 1.3857, 4.7814], [0.2517, 0.9697, 6.7053], [-0.552, 0.5265, 0.6533], [0.5358, 1.4741, 4.587]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[2.5162, 1.7501, -1.7802, 1.75, 1.65, 3.39, 1.6616 + 0.13603681398218054 * 4 - np.pi / 2], [2.4567, 1.6357, -8.9594, 1.54, 1.57, 4.01, 1.5216 + 0.13603681398218054 * 4 - np.pi / 2], [-0.5558, 1.3033, -28.2967, 1.47, 1.48, 2.23, 4.7116 + 0.13603681398218054 * 4 - np.pi / 2], [21.823, 1.7361, -26.669, 1.56, 1.4, 3.48, 4.8316 + 0.13603681398218054 * 4 - np.pi / 2], [8.1621, 1.6218, -31.3198, 1.74, 1.48, 3.77, 0.3516 + 0.13603681398218054 * 4 - np.pi / 2]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, -1.2559], [-0.8784, 1.3857, -4.7814], [0.2517, 0.9697, -6.7053], [-0.552, 0.5265, -0.6533], [0.5358, 1.4741, -4.587]])\n    yaw_normalized_tensor = boxes_flip_vert.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 + 0.13603681398218054 * 2 - np.pi], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 + 0.13603681398218054 * 2 - np.pi], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 + 0.13603681398218054 * 2 - np.pi], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 + 0.13603681398218054 * 2 - np.pi], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 + 0.13603681398218054 * 2 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(0.13603681398218054), points)\n    expected_points = torch.tensor([[-0.8403, 1.4658, -1.1526], [-1.5187, 1.3857, -4.6181], [-0.66, 0.9697, -6.6775], [-0.6355, 0.5265, -0.5724], [-0.0912, 1.4741, -4.6173]])\n    expected_rot_mat_T = torch.tensor([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(-0.13603681398218054), points)\n    rot_mat = np.array([[0.99076125, 0.0, -0.13561762], [0.0, 1.0, 0.0], [0.13561762, 0.0, 0.99076125]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(0.13603681398218054), points_np)\n    expected_points_np = np.array([[0.4712, 1.2559, -1.544, 2.5359], [0.6824, 4.7814, -1.492, 0.7167], [-0.3809, 6.7053, -0.9266, 0.5599], [0.4755, 0.6533, -0.5965, 1.0032], [-0.7308, 4.587, -1.3878, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(-0.13603681398218054), points_np)\n    camera_points = CameraPoints(points_np, points_dim=4)\n    (camera_points, rot_mat_T_np) = boxes.rotate(rot_mat, camera_points)\n    points_np = camera_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.scale(1.00559866335275)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.translate(torch.tensor([0.13246193, 0.15701613, 0.0838056]))\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([-2, -5, 0, 20, 2, 22])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[3.0508, 1.1282, 1.7598, 3.409, -5.9203], [4.9332, 8.0981, 1.5486, 4.0325, -6.0603], [7.2409, 27.6424, 1.4782, 2.2425, -2.8703], [28.4773, 20.0183, 1.5687, 3.4995, -2.7503], [16.502, 28.2147, 1.7497, 3.7911, -0.9471]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 1.0, 0.5]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    lidar_expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    expected_tensor = lidar_expected_tensor.clone()\n    expected_tensor[:, 0::2] = -lidar_expected_tensor[:, [3, 1]]\n    expected_tensor[:, 1::2] = lidar_expected_tensor[:, 0::2]\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[2.8332, 0.25769, -0.77767], [1.6232, 0.25769, 2.4093], [1.6232, 1.9169, 2.4093], [2.8332, 1.9169, -0.77767], [4.4784, 0.25769, -0.15302], [3.2684, 0.25769, 3.034], [3.2684, 1.9169, 3.034], [4.4784, 1.9169, -0.15302]], [[4.6237, 0.2231, 5.9606], [3.7324, 0.2231, 9.8933], [3.7324, 1.8019, 9.8933], [4.6237, 1.8019, 5.9606], [6.134, 0.2231, 6.3029], [5.2427, 0.2231, 10.236], [5.2427, 1.8019, 10.236], [6.134, 1.8019, 6.3029]], [[6.8292, -0.020648, 26.364], [6.2283, -0.020648, 28.525], [6.2283, 1.4676, 28.525], [6.8292, 1.4676, 26.364], [8.2534, -0.020648, 26.76], [7.6525, -0.020648, 28.921], [7.6525, 1.4676, 28.921], [8.2534, 1.4676, 26.76]], [[28.42, 0.49495, 18.102], [27.085, 0.49495, 21.337], [27.085, 1.9028, 21.337], [28.42, 1.9028, 18.102], [29.87, 0.49495, 18.7], [28.535, 0.49495, 21.935], [28.535, 1.9028, 21.935], [29.87, 1.9028, 18.7]], [[14.452, 0.29959, 27.818], [17.53, 0.29959, 30.032], [17.53, 1.7879, 30.032], [14.452, 1.7879, 27.818], [15.474, 0.29959, 26.398], [18.552, 0.29959, 28.612], [18.552, 1.7879, 28.612], [15.474, 1.7879, 26.398]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.001, atol=0.0001)\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996]], dtype=torch.float32)\n    boxes_origin_given = CameraInstance3DBoxes(th_boxes.clone(), box_dim=7, origin=(0.5, 0.5, 0.5))\n    expected_tensor = th_boxes.clone()\n    expected_tensor[:, :3] = th_boxes[:, :3] + th_boxes[:, 3:6] * (th_boxes.new_tensor((0.5, 1.0, 0.5)) - th_boxes.new_tensor((0.5, 0.5, 0.5)))\n    assert torch.allclose(boxes_origin_given.tensor, expected_tensor)",
            "def test_camera_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=np.float32)\n    boxes_1 = Box3DMode.convert(LiDARInstance3DBoxes(np_boxes), Box3DMode.LIDAR, Box3DMode.CAM)\n    assert isinstance(boxes_1, CameraInstance3DBoxes)\n    cam_np_boxes = Box3DMode.convert(np_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    assert torch.allclose(boxes_1.tensor, boxes_1.tensor.new_tensor(cam_np_boxes))\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=torch.float32)\n    cam_th_boxes = Box3DMode.convert(th_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes_2 = CameraInstance3DBoxes(cam_th_boxes)\n    assert torch.allclose(boxes_2.tensor, cam_th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4 - 2 * np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes = CameraInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    points = torch.tensor([[0.6762, 1.4658, 1.2559], [0.8784, 1.3857, 4.7814], [-0.2517, 0.9697, 6.7053], [0.552, 0.5265, 0.6533], [-0.5358, 1.4741, 4.587]])\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 + 0.13603681398218054 * 4 - np.pi], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 + 0.13603681398218054 * 4 - np.pi], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 + 0.13603681398218054 * 4 - np.pi], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 + 0.13603681398218054 * 4 - np.pi], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 + 0.13603681398218054 * 4 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    points = boxes.flip('horizontal', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, 1.2559], [-0.8784, 1.3857, 4.7814], [0.2517, 0.9697, 6.7053], [-0.552, 0.5265, 0.6533], [0.5358, 1.4741, 4.587]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[2.5162, 1.7501, -1.7802, 1.75, 1.65, 3.39, 1.6616 + 0.13603681398218054 * 4 - np.pi / 2], [2.4567, 1.6357, -8.9594, 1.54, 1.57, 4.01, 1.5216 + 0.13603681398218054 * 4 - np.pi / 2], [-0.5558, 1.3033, -28.2967, 1.47, 1.48, 2.23, 4.7116 + 0.13603681398218054 * 4 - np.pi / 2], [21.823, 1.7361, -26.669, 1.56, 1.4, 3.48, 4.8316 + 0.13603681398218054 * 4 - np.pi / 2], [8.1621, 1.6218, -31.3198, 1.74, 1.48, 3.77, 0.3516 + 0.13603681398218054 * 4 - np.pi / 2]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, -1.2559], [-0.8784, 1.3857, -4.7814], [0.2517, 0.9697, -6.7053], [-0.552, 0.5265, -0.6533], [0.5358, 1.4741, -4.587]])\n    yaw_normalized_tensor = boxes_flip_vert.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 + 0.13603681398218054 * 2 - np.pi], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 + 0.13603681398218054 * 2 - np.pi], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 + 0.13603681398218054 * 2 - np.pi], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 + 0.13603681398218054 * 2 - np.pi], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 + 0.13603681398218054 * 2 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(0.13603681398218054), points)\n    expected_points = torch.tensor([[-0.8403, 1.4658, -1.1526], [-1.5187, 1.3857, -4.6181], [-0.66, 0.9697, -6.6775], [-0.6355, 0.5265, -0.5724], [-0.0912, 1.4741, -4.6173]])\n    expected_rot_mat_T = torch.tensor([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(-0.13603681398218054), points)\n    rot_mat = np.array([[0.99076125, 0.0, -0.13561762], [0.0, 1.0, 0.0], [0.13561762, 0.0, 0.99076125]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(0.13603681398218054), points_np)\n    expected_points_np = np.array([[0.4712, 1.2559, -1.544, 2.5359], [0.6824, 4.7814, -1.492, 0.7167], [-0.3809, 6.7053, -0.9266, 0.5599], [0.4755, 0.6533, -0.5965, 1.0032], [-0.7308, 4.587, -1.3878, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(-0.13603681398218054), points_np)\n    camera_points = CameraPoints(points_np, points_dim=4)\n    (camera_points, rot_mat_T_np) = boxes.rotate(rot_mat, camera_points)\n    points_np = camera_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.scale(1.00559866335275)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.translate(torch.tensor([0.13246193, 0.15701613, 0.0838056]))\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([-2, -5, 0, 20, 2, 22])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[3.0508, 1.1282, 1.7598, 3.409, -5.9203], [4.9332, 8.0981, 1.5486, 4.0325, -6.0603], [7.2409, 27.6424, 1.4782, 2.2425, -2.8703], [28.4773, 20.0183, 1.5687, 3.4995, -2.7503], [16.502, 28.2147, 1.7497, 3.7911, -0.9471]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 1.0, 0.5]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    lidar_expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    expected_tensor = lidar_expected_tensor.clone()\n    expected_tensor[:, 0::2] = -lidar_expected_tensor[:, [3, 1]]\n    expected_tensor[:, 1::2] = lidar_expected_tensor[:, 0::2]\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[2.8332, 0.25769, -0.77767], [1.6232, 0.25769, 2.4093], [1.6232, 1.9169, 2.4093], [2.8332, 1.9169, -0.77767], [4.4784, 0.25769, -0.15302], [3.2684, 0.25769, 3.034], [3.2684, 1.9169, 3.034], [4.4784, 1.9169, -0.15302]], [[4.6237, 0.2231, 5.9606], [3.7324, 0.2231, 9.8933], [3.7324, 1.8019, 9.8933], [4.6237, 1.8019, 5.9606], [6.134, 0.2231, 6.3029], [5.2427, 0.2231, 10.236], [5.2427, 1.8019, 10.236], [6.134, 1.8019, 6.3029]], [[6.8292, -0.020648, 26.364], [6.2283, -0.020648, 28.525], [6.2283, 1.4676, 28.525], [6.8292, 1.4676, 26.364], [8.2534, -0.020648, 26.76], [7.6525, -0.020648, 28.921], [7.6525, 1.4676, 28.921], [8.2534, 1.4676, 26.76]], [[28.42, 0.49495, 18.102], [27.085, 0.49495, 21.337], [27.085, 1.9028, 21.337], [28.42, 1.9028, 18.102], [29.87, 0.49495, 18.7], [28.535, 0.49495, 21.935], [28.535, 1.9028, 21.935], [29.87, 1.9028, 18.7]], [[14.452, 0.29959, 27.818], [17.53, 0.29959, 30.032], [17.53, 1.7879, 30.032], [14.452, 1.7879, 27.818], [15.474, 0.29959, 26.398], [18.552, 0.29959, 28.612], [18.552, 1.7879, 28.612], [15.474, 1.7879, 26.398]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.001, atol=0.0001)\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996]], dtype=torch.float32)\n    boxes_origin_given = CameraInstance3DBoxes(th_boxes.clone(), box_dim=7, origin=(0.5, 0.5, 0.5))\n    expected_tensor = th_boxes.clone()\n    expected_tensor[:, :3] = th_boxes[:, :3] + th_boxes[:, 3:6] * (th_boxes.new_tensor((0.5, 1.0, 0.5)) - th_boxes.new_tensor((0.5, 0.5, 0.5)))\n    assert torch.allclose(boxes_origin_given.tensor, expected_tensor)",
            "def test_camera_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np_boxes = np.array([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=np.float32)\n    boxes_1 = Box3DMode.convert(LiDARInstance3DBoxes(np_boxes), Box3DMode.LIDAR, Box3DMode.CAM)\n    assert isinstance(boxes_1, CameraInstance3DBoxes)\n    cam_np_boxes = Box3DMode.convert(np_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    assert torch.allclose(boxes_1.tensor, boxes_1.tensor.new_tensor(cam_np_boxes))\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996 - 0.13603681398218054 * 4 - 2 * np.pi]], dtype=torch.float32)\n    cam_th_boxes = Box3DMode.convert(th_boxes, Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes_2 = CameraInstance3DBoxes(cam_th_boxes)\n    assert torch.allclose(boxes_2.tensor, cam_th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, 2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.48 - 0.13603681398218054 * 4 - 2 * np.pi], [8.959413, 2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.62 - 0.13603681398218054 * 4 - 2 * np.pi], [28.2967, -0.5557558, -1.303325, 1.47, 2.23, 1.48, -1.57 - 0.13603681398218054 * 4 - 2 * np.pi], [26.66902, 21.82302, -1.736057, 1.56, 3.48, 1.4, -1.69 - 0.13603681398218054 * 4 - 2 * np.pi], [31.31978, 8.162144, -1.6217787, 1.74, 3.77, 1.48, 2.79 - 0.13603681398218054 * 4 - 2 * np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes = CameraInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    points = torch.tensor([[0.6762, 1.4658, 1.2559], [0.8784, 1.3857, 4.7814], [-0.2517, 0.9697, 6.7053], [0.552, 0.5265, 0.6533], [-0.5358, 1.4741, 4.587]])\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.7802081, -2.516249, -1.7501148, 1.75, 3.39, 1.65, 1.6615927 + 0.13603681398218054 * 4 - np.pi], [8.959413, -2.4567227, -1.6357126, 1.54, 4.01, 1.57, 1.5215927 + 0.13603681398218054 * 4 - np.pi], [28.2967, 0.5557558, -1.303325, 1.47, 2.23, 1.48, 4.7115927 + 0.13603681398218054 * 4 - np.pi], [26.66902, -21.82302, -1.736057, 1.56, 3.48, 1.4, 4.8315926 + 0.13603681398218054 * 4 - np.pi], [31.31978, -8.162144, -1.6217787, 1.74, 3.77, 1.48, 0.35159278 + 0.13603681398218054 * 4 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    points = boxes.flip('horizontal', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, 1.2559], [-0.8784, 1.3857, 4.7814], [0.2517, 0.9697, 6.7053], [-0.552, 0.5265, 0.6533], [0.5358, 1.4741, 4.587]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    expected_tensor = torch.tensor([[2.5162, 1.7501, -1.7802, 1.75, 1.65, 3.39, 1.6616 + 0.13603681398218054 * 4 - np.pi / 2], [2.4567, 1.6357, -8.9594, 1.54, 1.57, 4.01, 1.5216 + 0.13603681398218054 * 4 - np.pi / 2], [-0.5558, 1.3033, -28.2967, 1.47, 1.48, 2.23, 4.7116 + 0.13603681398218054 * 4 - np.pi / 2], [21.823, 1.7361, -26.669, 1.56, 1.4, 3.48, 4.8316 + 0.13603681398218054 * 4 - np.pi / 2], [8.1621, 1.6218, -31.3198, 1.74, 1.48, 3.77, 0.3516 + 0.13603681398218054 * 4 - np.pi / 2]])\n    boxes_flip_vert = boxes.clone()\n    points = boxes_flip_vert.flip('vertical', points)\n    expected_points = torch.tensor([[-0.6762, 1.4658, -1.2559], [-0.8784, 1.3857, -4.7814], [0.2517, 0.9697, -6.7053], [-0.552, 0.5265, -0.6533], [0.5358, 1.4741, -4.587]])\n    yaw_normalized_tensor = boxes_flip_vert.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.0001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.4225, -2.7344, -1.7501, 1.75, 3.39, 1.65, 1.7976 + 0.13603681398218054 * 2 - np.pi], [8.5435, -3.6491, -1.6357, 1.54, 4.01, 1.57, 1.6576 + 0.13603681398218054 * 2 - np.pi], [28.1106, -3.2869, -1.3033, 1.47, 2.23, 1.48, 4.8476 + 0.13603681398218054 * 2 - np.pi], [23.463, -25.2382, -1.7361, 1.56, 3.48, 1.4, 4.9676 + 0.13603681398218054 * 2 - np.pi], [29.9235, -12.3342, -1.6218, 1.74, 3.77, 1.48, 0.4876 + 0.13603681398218054 * 2 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(0.13603681398218054), points)\n    expected_points = torch.tensor([[-0.8403, 1.4658, -1.1526], [-1.5187, 1.3857, -4.6181], [-0.66, 0.9697, -6.6775], [-0.6355, 0.5265, -0.5724], [-0.0912, 1.4741, -4.6173]])\n    expected_rot_mat_T = torch.tensor([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(torch.tensor(-0.13603681398218054), points)\n    rot_mat = np.array([[0.99076125, 0.0, -0.13561762], [0.0, 1.0, 0.0], [0.13561762, 0.0, 0.99076125]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(0.13603681398218054), points_np)\n    expected_points_np = np.array([[0.4712, 1.2559, -1.544, 2.5359], [0.6824, 4.7814, -1.492, 0.7167], [-0.3809, 6.7053, -0.9266, 0.5599], [0.4755, 0.6533, -0.5965, 1.0032], [-0.7308, 4.587, -1.3878, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.9908, 0.0, -0.1356], [0.0, 1.0, 0.0], [0.1356, 0.0, 0.9908]])\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(torch.tensor(-0.13603681398218054), points_np)\n    camera_points = CameraPoints(points_np, points_dim=4)\n    (camera_points, rot_mat_T_np) = boxes.rotate(rot_mat, camera_points)\n    points_np = camera_points.tensor.numpy()\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.0443488, -2.9183323, -1.7599131, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.014273, -4.8007393, -1.6448704, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.558605, -7.1084175, -1.310622, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [19.934517, -28.344835, -1.7457767, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.130915, -16.369587, -1.6308585, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.scale(1.00559866335275)\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = Box3DMode.convert(torch.tensor([[1.1281544, -3.0507944, -1.9169292, 1.7597977, 3.4089797, 1.6592377, 1.9336663 - np.pi], [8.098079, -4.9332013, -1.8018866, 1.5486219, 4.0324507, 1.57879, 1.7936664 - np.pi], [27.64241, -7.2408795, -1.4676381, 1.4782301, 2.242485, 1.488286, 4.9836664 - np.pi], [20.018322, -28.477297, -1.9027928, 1.5687338, 3.4994833, 1.4078381, 5.1036663 - np.pi], [28.21472, -16.502048, -1.7878747, 1.7497417, 3.791107, 1.488286, 0.6236664 - np.pi]]), Box3DMode.LIDAR, Box3DMode.CAM)\n    boxes.translate(torch.tensor([0.13246193, 0.15701613, 0.0838056]))\n    yaw_normalized_tensor = boxes.tensor.clone()\n    yaw_normalized_tensor[:, -1:] = limit_period(yaw_normalized_tensor[:, -1:], period=np.pi * 2)\n    expected_tensor[:, -1:] = limit_period(expected_tensor[:, -1:], period=np.pi * 2)\n    assert torch.allclose(yaw_normalized_tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1, 1, 1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([1, 1, 0, 0, 0], dtype=torch.bool)\n    mask = boxes.in_range_3d([-2, -5, 0, 20, 2, 22])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[3.0508, 1.1282, 1.7598, 3.409, -5.9203], [4.9332, 8.0981, 1.5486, 4.0325, -6.0603], [7.2409, 27.6424, 1.4782, 2.2425, -2.8703], [28.4773, 20.0183, 1.5687, 3.4995, -2.7503], [16.502, 28.2147, 1.7497, 3.7911, -0.9471]])\n    assert torch.allclose(boxes.bev, expected_tensor, atol=0.001)\n    assert torch.allclose(boxes.bottom_center, boxes.tensor[:, :3])\n    expected_tensor = boxes.tensor[:, :3] - boxes.tensor[:, 3:6] * (torch.tensor([0.5, 1.0, 0.5]) - torch.tensor([0.5, 0.5, 0.5]))\n    assert torch.allclose(boxes.gravity_center, expected_tensor)\n    boxes.limit_yaw()\n    assert (boxes.tensor[:, 6] <= np.pi / 2).all()\n    assert (boxes.tensor[:, 6] >= -np.pi / 2).all()\n    Box3DMode.convert(boxes, Box3DMode.LIDAR, Box3DMode.LIDAR)\n    expected_tensor = boxes.tensor.clone()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    boxes.flip()\n    boxes.flip()\n    boxes.limit_yaw()\n    assert torch.allclose(expected_tensor, boxes.tensor)\n    lidar_expected_tensor = torch.tensor([[-0.5763, -3.9307, 2.8326, -2.1709], [6.0819, -5.7075, 10.1143, -4.1589], [26.5212, -7.98, 28.7637, -6.5018], [18.2686, -29.2617, 21.7681, -27.6929], [27.3398, -18.3976, 29.0896, -14.6065]])\n    expected_tensor = lidar_expected_tensor.clone()\n    expected_tensor[:, 0::2] = -lidar_expected_tensor[:, [3, 1]]\n    expected_tensor[:, 1::2] = lidar_expected_tensor[:, 0::2]\n    assert torch.allclose(boxes.nearest_bev, expected_tensor, rtol=0.0001, atol=1e-07)\n    expected_tensor = torch.tensor([[[2.8332, 0.25769, -0.77767], [1.6232, 0.25769, 2.4093], [1.6232, 1.9169, 2.4093], [2.8332, 1.9169, -0.77767], [4.4784, 0.25769, -0.15302], [3.2684, 0.25769, 3.034], [3.2684, 1.9169, 3.034], [4.4784, 1.9169, -0.15302]], [[4.6237, 0.2231, 5.9606], [3.7324, 0.2231, 9.8933], [3.7324, 1.8019, 9.8933], [4.6237, 1.8019, 5.9606], [6.134, 0.2231, 6.3029], [5.2427, 0.2231, 10.236], [5.2427, 1.8019, 10.236], [6.134, 1.8019, 6.3029]], [[6.8292, -0.020648, 26.364], [6.2283, -0.020648, 28.525], [6.2283, 1.4676, 28.525], [6.8292, 1.4676, 26.364], [8.2534, -0.020648, 26.76], [7.6525, -0.020648, 28.921], [7.6525, 1.4676, 28.921], [8.2534, 1.4676, 26.76]], [[28.42, 0.49495, 18.102], [27.085, 0.49495, 21.337], [27.085, 1.9028, 21.337], [28.42, 1.9028, 18.102], [29.87, 0.49495, 18.7], [28.535, 0.49495, 21.935], [28.535, 1.9028, 21.935], [29.87, 1.9028, 18.7]], [[14.452, 0.29959, 27.818], [17.53, 0.29959, 30.032], [17.53, 1.7879, 30.032], [14.452, 1.7879, 27.818], [15.474, 0.29959, 26.398], [18.552, 0.29959, 28.612], [18.552, 1.7879, 28.612], [15.474, 1.7879, 26.398]]])\n    assert torch.allclose(boxes.corners, expected_tensor, rtol=0.001, atol=0.0001)\n    th_boxes = torch.tensor([[28.29669987, -0.5557558, -1.30332506, 1.47000003, 2.23000002, 1.48000002, -1.57000005], [26.66901946, 21.82302134, -1.73605708, 1.55999994, 3.48000002, 1.39999998, -1.69000006], [31.31977974, 8.16214412, -1.62177875, 1.74000001, 3.76999998, 1.48000002, 2.78999996]], dtype=torch.float32)\n    boxes_origin_given = CameraInstance3DBoxes(th_boxes.clone(), box_dim=7, origin=(0.5, 0.5, 0.5))\n    expected_tensor = th_boxes.clone()\n    expected_tensor[:, :3] = th_boxes[:, :3] + th_boxes[:, 3:6] * (th_boxes.new_tensor((0.5, 1.0, 0.5)) - th_boxes.new_tensor((0.5, 0.5, 0.5)))\n    assert torch.allclose(boxes_origin_given.tensor, expected_tensor)"
        ]
    },
    {
        "func_name": "test_boxes3d_overlaps",
        "original": "def test_boxes3d_overlaps():\n    \"\"\"Test the iou calculation of boxes in different modes.\n\n    CommandLine:\n        xdoctest tests/test_box3d.py::test_boxes3d_overlaps zero\n    \"\"\"\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    boxes1_tensor = torch.tensor([[1.8, -2.5, -1.8, 1.75, 3.39, 1.65, -1.6615927], [8.9, -2.5, -1.6, 1.54, 4.01, 1.57, -1.5215927], [28.3, 0.5, -1.3, 1.47, 2.23, 1.48, -4.7115927], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35]], device='cuda')\n    boxes1 = LiDARInstance3DBoxes(boxes1_tensor)\n    boxes2_tensor = torch.tensor([[1.2, -3.0, -1.9, 1.8, 3.4, 1.7, -1.9], [8.1, -2.9, -1.8, 1.5, 4.1, 1.6, -1.8], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35], [20.1, -28.5, -1.9, 1.6, 3.5, 1.4, -5.1]], device='cuda')\n    boxes2 = LiDARInstance3DBoxes(boxes2_tensor)\n    expected_iou_tensor = torch.tensor([[0.371, 0.0, 0.0, 0.0], [0.0, 0.3322, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iou = boxes1.overlaps(boxes1, boxes2)\n    assert torch.allclose(expected_iou_tensor, overlaps_3d_iou, rtol=0.0001, atol=1e-07)\n    expected_iof_tensor = torch.tensor([[0.5582, 0.0, 0.0, 0.0], [0.0, 0.5025, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iof = boxes1.overlaps(boxes1, boxes2, mode='iof')\n    assert torch.allclose(expected_iof_tensor, overlaps_3d_iof, rtol=0.0001, atol=1e-07)\n    empty_boxes = []\n    boxes3 = LiDARInstance3DBoxes(empty_boxes)\n    overlaps_3d_empty = boxes1.overlaps(boxes3, boxes2)\n    assert overlaps_3d_empty.shape[0] == 0\n    assert overlaps_3d_empty.shape[1] == 4\n    cam_boxes1_tensor = Box3DMode.convert(boxes1_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes1 = CameraInstance3DBoxes(cam_boxes1_tensor)\n    cam_boxes2_tensor = Box3DMode.convert(boxes2_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes2 = CameraInstance3DBoxes(cam_boxes2_tensor)\n    cam_overlaps_3d = cam_boxes1.overlaps(cam_boxes1, cam_boxes2)\n    assert torch.allclose(expected_iou_tensor, cam_overlaps_3d, rtol=0.001, atol=0.0001)\n    assert torch.allclose(cam_overlaps_3d, overlaps_3d_iou, rtol=0.001, atol=0.0001)\n    with pytest.raises(AssertionError):\n        cam_boxes1.overlaps(cam_boxes1, boxes1)\n    with pytest.raises(AssertionError):\n        boxes1.overlaps(cam_boxes1, boxes1)",
        "mutated": [
            "def test_boxes3d_overlaps():\n    if False:\n        i = 10\n    'Test the iou calculation of boxes in different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes3d_overlaps zero\\n    '\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    boxes1_tensor = torch.tensor([[1.8, -2.5, -1.8, 1.75, 3.39, 1.65, -1.6615927], [8.9, -2.5, -1.6, 1.54, 4.01, 1.57, -1.5215927], [28.3, 0.5, -1.3, 1.47, 2.23, 1.48, -4.7115927], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35]], device='cuda')\n    boxes1 = LiDARInstance3DBoxes(boxes1_tensor)\n    boxes2_tensor = torch.tensor([[1.2, -3.0, -1.9, 1.8, 3.4, 1.7, -1.9], [8.1, -2.9, -1.8, 1.5, 4.1, 1.6, -1.8], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35], [20.1, -28.5, -1.9, 1.6, 3.5, 1.4, -5.1]], device='cuda')\n    boxes2 = LiDARInstance3DBoxes(boxes2_tensor)\n    expected_iou_tensor = torch.tensor([[0.371, 0.0, 0.0, 0.0], [0.0, 0.3322, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iou = boxes1.overlaps(boxes1, boxes2)\n    assert torch.allclose(expected_iou_tensor, overlaps_3d_iou, rtol=0.0001, atol=1e-07)\n    expected_iof_tensor = torch.tensor([[0.5582, 0.0, 0.0, 0.0], [0.0, 0.5025, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iof = boxes1.overlaps(boxes1, boxes2, mode='iof')\n    assert torch.allclose(expected_iof_tensor, overlaps_3d_iof, rtol=0.0001, atol=1e-07)\n    empty_boxes = []\n    boxes3 = LiDARInstance3DBoxes(empty_boxes)\n    overlaps_3d_empty = boxes1.overlaps(boxes3, boxes2)\n    assert overlaps_3d_empty.shape[0] == 0\n    assert overlaps_3d_empty.shape[1] == 4\n    cam_boxes1_tensor = Box3DMode.convert(boxes1_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes1 = CameraInstance3DBoxes(cam_boxes1_tensor)\n    cam_boxes2_tensor = Box3DMode.convert(boxes2_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes2 = CameraInstance3DBoxes(cam_boxes2_tensor)\n    cam_overlaps_3d = cam_boxes1.overlaps(cam_boxes1, cam_boxes2)\n    assert torch.allclose(expected_iou_tensor, cam_overlaps_3d, rtol=0.001, atol=0.0001)\n    assert torch.allclose(cam_overlaps_3d, overlaps_3d_iou, rtol=0.001, atol=0.0001)\n    with pytest.raises(AssertionError):\n        cam_boxes1.overlaps(cam_boxes1, boxes1)\n    with pytest.raises(AssertionError):\n        boxes1.overlaps(cam_boxes1, boxes1)",
            "def test_boxes3d_overlaps():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test the iou calculation of boxes in different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes3d_overlaps zero\\n    '\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    boxes1_tensor = torch.tensor([[1.8, -2.5, -1.8, 1.75, 3.39, 1.65, -1.6615927], [8.9, -2.5, -1.6, 1.54, 4.01, 1.57, -1.5215927], [28.3, 0.5, -1.3, 1.47, 2.23, 1.48, -4.7115927], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35]], device='cuda')\n    boxes1 = LiDARInstance3DBoxes(boxes1_tensor)\n    boxes2_tensor = torch.tensor([[1.2, -3.0, -1.9, 1.8, 3.4, 1.7, -1.9], [8.1, -2.9, -1.8, 1.5, 4.1, 1.6, -1.8], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35], [20.1, -28.5, -1.9, 1.6, 3.5, 1.4, -5.1]], device='cuda')\n    boxes2 = LiDARInstance3DBoxes(boxes2_tensor)\n    expected_iou_tensor = torch.tensor([[0.371, 0.0, 0.0, 0.0], [0.0, 0.3322, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iou = boxes1.overlaps(boxes1, boxes2)\n    assert torch.allclose(expected_iou_tensor, overlaps_3d_iou, rtol=0.0001, atol=1e-07)\n    expected_iof_tensor = torch.tensor([[0.5582, 0.0, 0.0, 0.0], [0.0, 0.5025, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iof = boxes1.overlaps(boxes1, boxes2, mode='iof')\n    assert torch.allclose(expected_iof_tensor, overlaps_3d_iof, rtol=0.0001, atol=1e-07)\n    empty_boxes = []\n    boxes3 = LiDARInstance3DBoxes(empty_boxes)\n    overlaps_3d_empty = boxes1.overlaps(boxes3, boxes2)\n    assert overlaps_3d_empty.shape[0] == 0\n    assert overlaps_3d_empty.shape[1] == 4\n    cam_boxes1_tensor = Box3DMode.convert(boxes1_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes1 = CameraInstance3DBoxes(cam_boxes1_tensor)\n    cam_boxes2_tensor = Box3DMode.convert(boxes2_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes2 = CameraInstance3DBoxes(cam_boxes2_tensor)\n    cam_overlaps_3d = cam_boxes1.overlaps(cam_boxes1, cam_boxes2)\n    assert torch.allclose(expected_iou_tensor, cam_overlaps_3d, rtol=0.001, atol=0.0001)\n    assert torch.allclose(cam_overlaps_3d, overlaps_3d_iou, rtol=0.001, atol=0.0001)\n    with pytest.raises(AssertionError):\n        cam_boxes1.overlaps(cam_boxes1, boxes1)\n    with pytest.raises(AssertionError):\n        boxes1.overlaps(cam_boxes1, boxes1)",
            "def test_boxes3d_overlaps():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test the iou calculation of boxes in different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes3d_overlaps zero\\n    '\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    boxes1_tensor = torch.tensor([[1.8, -2.5, -1.8, 1.75, 3.39, 1.65, -1.6615927], [8.9, -2.5, -1.6, 1.54, 4.01, 1.57, -1.5215927], [28.3, 0.5, -1.3, 1.47, 2.23, 1.48, -4.7115927], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35]], device='cuda')\n    boxes1 = LiDARInstance3DBoxes(boxes1_tensor)\n    boxes2_tensor = torch.tensor([[1.2, -3.0, -1.9, 1.8, 3.4, 1.7, -1.9], [8.1, -2.9, -1.8, 1.5, 4.1, 1.6, -1.8], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35], [20.1, -28.5, -1.9, 1.6, 3.5, 1.4, -5.1]], device='cuda')\n    boxes2 = LiDARInstance3DBoxes(boxes2_tensor)\n    expected_iou_tensor = torch.tensor([[0.371, 0.0, 0.0, 0.0], [0.0, 0.3322, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iou = boxes1.overlaps(boxes1, boxes2)\n    assert torch.allclose(expected_iou_tensor, overlaps_3d_iou, rtol=0.0001, atol=1e-07)\n    expected_iof_tensor = torch.tensor([[0.5582, 0.0, 0.0, 0.0], [0.0, 0.5025, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iof = boxes1.overlaps(boxes1, boxes2, mode='iof')\n    assert torch.allclose(expected_iof_tensor, overlaps_3d_iof, rtol=0.0001, atol=1e-07)\n    empty_boxes = []\n    boxes3 = LiDARInstance3DBoxes(empty_boxes)\n    overlaps_3d_empty = boxes1.overlaps(boxes3, boxes2)\n    assert overlaps_3d_empty.shape[0] == 0\n    assert overlaps_3d_empty.shape[1] == 4\n    cam_boxes1_tensor = Box3DMode.convert(boxes1_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes1 = CameraInstance3DBoxes(cam_boxes1_tensor)\n    cam_boxes2_tensor = Box3DMode.convert(boxes2_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes2 = CameraInstance3DBoxes(cam_boxes2_tensor)\n    cam_overlaps_3d = cam_boxes1.overlaps(cam_boxes1, cam_boxes2)\n    assert torch.allclose(expected_iou_tensor, cam_overlaps_3d, rtol=0.001, atol=0.0001)\n    assert torch.allclose(cam_overlaps_3d, overlaps_3d_iou, rtol=0.001, atol=0.0001)\n    with pytest.raises(AssertionError):\n        cam_boxes1.overlaps(cam_boxes1, boxes1)\n    with pytest.raises(AssertionError):\n        boxes1.overlaps(cam_boxes1, boxes1)",
            "def test_boxes3d_overlaps():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test the iou calculation of boxes in different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes3d_overlaps zero\\n    '\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    boxes1_tensor = torch.tensor([[1.8, -2.5, -1.8, 1.75, 3.39, 1.65, -1.6615927], [8.9, -2.5, -1.6, 1.54, 4.01, 1.57, -1.5215927], [28.3, 0.5, -1.3, 1.47, 2.23, 1.48, -4.7115927], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35]], device='cuda')\n    boxes1 = LiDARInstance3DBoxes(boxes1_tensor)\n    boxes2_tensor = torch.tensor([[1.2, -3.0, -1.9, 1.8, 3.4, 1.7, -1.9], [8.1, -2.9, -1.8, 1.5, 4.1, 1.6, -1.8], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35], [20.1, -28.5, -1.9, 1.6, 3.5, 1.4, -5.1]], device='cuda')\n    boxes2 = LiDARInstance3DBoxes(boxes2_tensor)\n    expected_iou_tensor = torch.tensor([[0.371, 0.0, 0.0, 0.0], [0.0, 0.3322, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iou = boxes1.overlaps(boxes1, boxes2)\n    assert torch.allclose(expected_iou_tensor, overlaps_3d_iou, rtol=0.0001, atol=1e-07)\n    expected_iof_tensor = torch.tensor([[0.5582, 0.0, 0.0, 0.0], [0.0, 0.5025, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iof = boxes1.overlaps(boxes1, boxes2, mode='iof')\n    assert torch.allclose(expected_iof_tensor, overlaps_3d_iof, rtol=0.0001, atol=1e-07)\n    empty_boxes = []\n    boxes3 = LiDARInstance3DBoxes(empty_boxes)\n    overlaps_3d_empty = boxes1.overlaps(boxes3, boxes2)\n    assert overlaps_3d_empty.shape[0] == 0\n    assert overlaps_3d_empty.shape[1] == 4\n    cam_boxes1_tensor = Box3DMode.convert(boxes1_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes1 = CameraInstance3DBoxes(cam_boxes1_tensor)\n    cam_boxes2_tensor = Box3DMode.convert(boxes2_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes2 = CameraInstance3DBoxes(cam_boxes2_tensor)\n    cam_overlaps_3d = cam_boxes1.overlaps(cam_boxes1, cam_boxes2)\n    assert torch.allclose(expected_iou_tensor, cam_overlaps_3d, rtol=0.001, atol=0.0001)\n    assert torch.allclose(cam_overlaps_3d, overlaps_3d_iou, rtol=0.001, atol=0.0001)\n    with pytest.raises(AssertionError):\n        cam_boxes1.overlaps(cam_boxes1, boxes1)\n    with pytest.raises(AssertionError):\n        boxes1.overlaps(cam_boxes1, boxes1)",
            "def test_boxes3d_overlaps():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test the iou calculation of boxes in different modes.\\n\\n    CommandLine:\\n        xdoctest tests/test_box3d.py::test_boxes3d_overlaps zero\\n    '\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    boxes1_tensor = torch.tensor([[1.8, -2.5, -1.8, 1.75, 3.39, 1.65, -1.6615927], [8.9, -2.5, -1.6, 1.54, 4.01, 1.57, -1.5215927], [28.3, 0.5, -1.3, 1.47, 2.23, 1.48, -4.7115927], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35]], device='cuda')\n    boxes1 = LiDARInstance3DBoxes(boxes1_tensor)\n    boxes2_tensor = torch.tensor([[1.2, -3.0, -1.9, 1.8, 3.4, 1.7, -1.9], [8.1, -2.9, -1.8, 1.5, 4.1, 1.6, -1.8], [31.3, -8.2, -1.6, 1.74, 3.77, 1.48, -0.35], [20.1, -28.5, -1.9, 1.6, 3.5, 1.4, -5.1]], device='cuda')\n    boxes2 = LiDARInstance3DBoxes(boxes2_tensor)\n    expected_iou_tensor = torch.tensor([[0.371, 0.0, 0.0, 0.0], [0.0, 0.3322, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iou = boxes1.overlaps(boxes1, boxes2)\n    assert torch.allclose(expected_iou_tensor, overlaps_3d_iou, rtol=0.0001, atol=1e-07)\n    expected_iof_tensor = torch.tensor([[0.5582, 0.0, 0.0, 0.0], [0.0, 0.5025, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], device='cuda')\n    overlaps_3d_iof = boxes1.overlaps(boxes1, boxes2, mode='iof')\n    assert torch.allclose(expected_iof_tensor, overlaps_3d_iof, rtol=0.0001, atol=1e-07)\n    empty_boxes = []\n    boxes3 = LiDARInstance3DBoxes(empty_boxes)\n    overlaps_3d_empty = boxes1.overlaps(boxes3, boxes2)\n    assert overlaps_3d_empty.shape[0] == 0\n    assert overlaps_3d_empty.shape[1] == 4\n    cam_boxes1_tensor = Box3DMode.convert(boxes1_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes1 = CameraInstance3DBoxes(cam_boxes1_tensor)\n    cam_boxes2_tensor = Box3DMode.convert(boxes2_tensor, Box3DMode.LIDAR, Box3DMode.CAM)\n    cam_boxes2 = CameraInstance3DBoxes(cam_boxes2_tensor)\n    cam_overlaps_3d = cam_boxes1.overlaps(cam_boxes1, cam_boxes2)\n    assert torch.allclose(expected_iou_tensor, cam_overlaps_3d, rtol=0.001, atol=0.0001)\n    assert torch.allclose(cam_overlaps_3d, overlaps_3d_iou, rtol=0.001, atol=0.0001)\n    with pytest.raises(AssertionError):\n        cam_boxes1.overlaps(cam_boxes1, boxes1)\n    with pytest.raises(AssertionError):\n        boxes1.overlaps(cam_boxes1, boxes1)"
        ]
    },
    {
        "func_name": "test_depth_boxes3d",
        "original": "def test_depth_boxes3d():\n    empty_boxes = []\n    boxes = DepthInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    np_boxes = np.array([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, --0.44255, 0.8234, 0.5325, 1.0099, 2.9971]], dtype=np.float32)\n    boxes_1 = DepthInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.1093], [2.3262, 3.3065, 0.9475]])\n    assert torch.allclose(boxes_1.gravity_center, expected_tensor)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, 0.9385, 2.1404, 3.0601], [2.3262, 3.3065, 0.8234, 0.5325, 2.9971]])\n    assert torch.allclose(boxes_1.bev, expected_tensor)\n    expected_tensor = torch.tensor([[1.0164, 1.4597, 1.9548, 3.6001], [1.9145, 3.0402, 2.7379, 3.5728]])\n    assert torch.allclose(boxes_1.nearest_bev, expected_tensor, 0.0001)\n    assert repr(boxes) == 'DepthInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    boxes_2 = DepthInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, 0.44255, 0.8234, 0.5325, 1.0099, 2.9971], [2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]])\n    boxes = DepthInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = DepthInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    expected_tensor = torch.tensor([[-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815], [-2.3262, 3.3065, 0.4426, 0.8234, 0.5325, 1.0099, 0.1445], [-2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 0.0723], [-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815]])\n    points = boxes.flip(bev_direction='horizontal', points=points)\n    expected_points = torch.tensor([[-0.6762, 1.2559, -1.4658, 2.5359], [-0.8784, 4.7814, -1.3857, 0.7167], [0.2517, 6.7053, -0.9697, 0.5599], [-0.552, 0.6533, -0.5265, 1.0032], [0.5358, 4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815], [-2.3262, -3.3065, 0.4426, 0.8234, 0.5325, 1.0099, -0.1445], [-2.4593, -2.587, -0.4321, 0.8597, 0.6193, 1.0204, -0.0723], [-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815]])\n    points = boxes.flip(bev_direction='vertical', points=points)\n    expected_points = torch.tensor([[-0.6762, -1.2559, -1.4658, 2.5359], [-0.8784, -4.7814, -1.3857, 0.7167], [0.2517, -6.7053, -0.9697, 0.5599], [-0.552, -0.6533, -0.5265, 1.0032], [0.5358, -4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    boxes_rot = boxes.clone()\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    (points, rot_mat_T) = boxes_rot.rotate(-0.022998953275003075, points)\n    expected_points = torch.tensor([[-0.7049, -1.24, -1.4658, 2.5359], [-0.9881, -4.7599, -1.3857, 0.7167], [0.0974, -6.7093, -0.9697, 0.5599], [-0.5669, -0.6404, -0.5265, 1.0032], [0.4302, -4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T = torch.tensor([[0.9997, -0.023, 0.0], [0.023, 0.9997, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(-0.022998953275003075, points)\n    rot_mat = np.array([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    expected_rot_mat_T = torch.tensor([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    expected_points_np = np.array([[0.7049, 1.24, -1.4658, 2.5359], [0.9881, 4.7599, -1.3857, 0.7167], [-0.0974, 6.7093, -0.9697, 0.5599], [0.5669, 0.6404, -0.5265, 1.0032], [-0.4302, 4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.99973554, -0.02299693, 0.0], [0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    depth_points = DepthPoints(points_np, points_dim=4)\n    (depth_points, rot_mat_T_np) = boxes.rotate(rot_mat, depth_points)\n    points_np = depth_points.tensor.numpy()\n    expected_rot_mat_T_np = expected_rot_mat_T_np.T\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]], [[-2.8519, -3.446, 0.4426], [-2.8519, -3.446, 1.4525], [-2.7632, -2.921, 1.4525], [-2.7632, -2.921, 0.4426], [-2.0401, -3.5833, 0.4426], [-2.0401, -3.5833, 1.4525], [-1.9513, -3.0582, 1.4525], [-1.9513, -3.0582, 0.4426]], [[-2.9755, -2.7971, -0.4321], [-2.9755, -2.7971, 0.5883], [-2.9166, -2.1806, 0.5883], [-2.9166, -2.1806, -0.4321], [-2.1197, -2.8789, -0.4321], [-2.1197, -2.8789, 0.5883], [-2.0608, -2.2624, 0.5883], [-2.0608, -2.2624, -0.4321]], [[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    th_boxes = torch.tensor([[0.61211395, 0.8129094, 0.10563634, 1.497534, 0.16927195, 0.27956772], [1.430009, 0.49797538, 0.9382923, 0.07694054, 0.9312509, 1.8919173]], dtype=torch.float32)\n    boxes = DepthInstance3DBoxes(th_boxes, box_dim=6, with_yaw=False)\n    expected_tensor = torch.tensor([[0.64884546, 0.78390356, 0.10563634, 1.50373348, 0.23795205, 0.27956772, 0], [1.45139421, 0.43169443, 0.93829232, 0.11967964, 0.93380373, 1.89191735, 0]])\n    boxes_3 = boxes.clone()\n    boxes_3.rotate(-0.04599790655000615)\n    assert torch.allclose(boxes_3.tensor, expected_tensor)\n    boxes.rotate(torch.tensor(-0.04599790655000615))\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([0, 1], dtype=torch.bool)\n    mask = boxes.in_range_3d([1, 0, -2, 2, 1, 5])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[[-0.103, 0.6649, 0.1056], [-0.103, 0.6649, 0.3852], [-0.103, 0.9029, 0.3852], [-0.103, 0.9029, 0.1056], [1.4007, 0.6649, 0.1056], [1.4007, 0.6649, 0.3852], [1.4007, 0.9029, 0.3852], [1.4007, 0.9029, 0.1056]], [[1.3916, -0.0352, 0.9383], [1.3916, -0.0352, 2.8302], [1.3916, 0.8986, 2.8302], [1.3916, 0.8986, 0.9383], [1.5112, -0.0352, 0.9383], [1.5112, -0.0352, 2.8302], [1.5112, 0.8986, 2.8302], [1.5112, 0.8986, 0.9383]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    if torch.cuda.is_available():\n        box_idxs_of_pts = boxes.points_in_boxes_all(points.cuda())\n        expected_idxs_of_pts = torch.tensor([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], device='cuda:0', dtype=torch.int32)\n        assert torch.all(box_idxs_of_pts == expected_idxs_of_pts)\n    boxes = torch.tensor([[0.3294, 1.0359, 0.1171, 1.0822, 1.1247, 1.3721, -0.4916], [-2.463, -2.6324, -0.1616, 0.9202, 1.7896, 0.1992, -0.3185]])\n    boxes = DepthInstance3DBoxes(boxes, box_dim=boxes.shape[-1], with_yaw=True, origin=(0.5, 0.5, 0.5))\n    (surface_center, line_center) = boxes.get_surface_line_center()\n    expected_surface_center = torch.tensor([[0.3294, 1.0359, 0.8031], [0.3294, 1.0359, -0.5689], [0.5949, 1.5317, 0.1171], [0.1533, 0.5018, 0.1171], [0.8064, 0.7805, 0.1171], [-0.1845, 1.2053, 0.1171], [-2.463, -2.6324, -0.062], [-2.463, -2.6324, -0.2612], [-2.0406, -1.8436, -0.1616], [-2.7432, -3.4822, -0.1616], [-2.0574, -2.8496, -0.1616], [-2.9, -2.4883, -0.1616]])\n    expected_line_center = torch.tensor([[0.8064, 0.7805, 0.8031], [-0.1845, 1.2053, 0.8031], [0.5949, 1.5317, 0.8031], [0.1533, 0.5018, 0.8031], [0.8064, 0.7805, -0.5689], [-0.1845, 1.2053, -0.5689], [0.5949, 1.5317, -0.5689], [0.1533, 0.5018, -0.5689], [1.0719, 1.2762, 0.1171], [0.6672, 0.3324, 0.1171], [0.1178, 1.7871, 0.1171], [-0.3606, 0.6713, 0.1171], [-2.0574, -2.8496, -0.062], [-2.9, -2.4883, -0.062], [-2.0406, -1.8436, -0.062], [-2.7432, -3.4822, -0.062], [-2.0574, -2.8496, -0.2612], [-2.9, -2.4883, -0.2612], [-2.0406, -1.8436, -0.2612], [-2.7432, -3.4822, -0.2612], [-1.635, -2.0607, -0.1616], [-2.3062, -3.6263, -0.1616], [-2.4462, -1.6264, -0.1616], [-3.1802, -3.3381, -0.1616]])\n    assert torch.allclose(surface_center, expected_surface_center, atol=0.0001)\n    assert torch.allclose(line_center, expected_line_center, atol=0.0001)",
        "mutated": [
            "def test_depth_boxes3d():\n    if False:\n        i = 10\n    empty_boxes = []\n    boxes = DepthInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    np_boxes = np.array([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, --0.44255, 0.8234, 0.5325, 1.0099, 2.9971]], dtype=np.float32)\n    boxes_1 = DepthInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.1093], [2.3262, 3.3065, 0.9475]])\n    assert torch.allclose(boxes_1.gravity_center, expected_tensor)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, 0.9385, 2.1404, 3.0601], [2.3262, 3.3065, 0.8234, 0.5325, 2.9971]])\n    assert torch.allclose(boxes_1.bev, expected_tensor)\n    expected_tensor = torch.tensor([[1.0164, 1.4597, 1.9548, 3.6001], [1.9145, 3.0402, 2.7379, 3.5728]])\n    assert torch.allclose(boxes_1.nearest_bev, expected_tensor, 0.0001)\n    assert repr(boxes) == 'DepthInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    boxes_2 = DepthInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, 0.44255, 0.8234, 0.5325, 1.0099, 2.9971], [2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]])\n    boxes = DepthInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = DepthInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    expected_tensor = torch.tensor([[-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815], [-2.3262, 3.3065, 0.4426, 0.8234, 0.5325, 1.0099, 0.1445], [-2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 0.0723], [-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815]])\n    points = boxes.flip(bev_direction='horizontal', points=points)\n    expected_points = torch.tensor([[-0.6762, 1.2559, -1.4658, 2.5359], [-0.8784, 4.7814, -1.3857, 0.7167], [0.2517, 6.7053, -0.9697, 0.5599], [-0.552, 0.6533, -0.5265, 1.0032], [0.5358, 4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815], [-2.3262, -3.3065, 0.4426, 0.8234, 0.5325, 1.0099, -0.1445], [-2.4593, -2.587, -0.4321, 0.8597, 0.6193, 1.0204, -0.0723], [-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815]])\n    points = boxes.flip(bev_direction='vertical', points=points)\n    expected_points = torch.tensor([[-0.6762, -1.2559, -1.4658, 2.5359], [-0.8784, -4.7814, -1.3857, 0.7167], [0.2517, -6.7053, -0.9697, 0.5599], [-0.552, -0.6533, -0.5265, 1.0032], [0.5358, -4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    boxes_rot = boxes.clone()\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    (points, rot_mat_T) = boxes_rot.rotate(-0.022998953275003075, points)\n    expected_points = torch.tensor([[-0.7049, -1.24, -1.4658, 2.5359], [-0.9881, -4.7599, -1.3857, 0.7167], [0.0974, -6.7093, -0.9697, 0.5599], [-0.5669, -0.6404, -0.5265, 1.0032], [0.4302, -4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T = torch.tensor([[0.9997, -0.023, 0.0], [0.023, 0.9997, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(-0.022998953275003075, points)\n    rot_mat = np.array([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    expected_rot_mat_T = torch.tensor([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    expected_points_np = np.array([[0.7049, 1.24, -1.4658, 2.5359], [0.9881, 4.7599, -1.3857, 0.7167], [-0.0974, 6.7093, -0.9697, 0.5599], [0.5669, 0.6404, -0.5265, 1.0032], [-0.4302, 4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.99973554, -0.02299693, 0.0], [0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    depth_points = DepthPoints(points_np, points_dim=4)\n    (depth_points, rot_mat_T_np) = boxes.rotate(rot_mat, depth_points)\n    points_np = depth_points.tensor.numpy()\n    expected_rot_mat_T_np = expected_rot_mat_T_np.T\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]], [[-2.8519, -3.446, 0.4426], [-2.8519, -3.446, 1.4525], [-2.7632, -2.921, 1.4525], [-2.7632, -2.921, 0.4426], [-2.0401, -3.5833, 0.4426], [-2.0401, -3.5833, 1.4525], [-1.9513, -3.0582, 1.4525], [-1.9513, -3.0582, 0.4426]], [[-2.9755, -2.7971, -0.4321], [-2.9755, -2.7971, 0.5883], [-2.9166, -2.1806, 0.5883], [-2.9166, -2.1806, -0.4321], [-2.1197, -2.8789, -0.4321], [-2.1197, -2.8789, 0.5883], [-2.0608, -2.2624, 0.5883], [-2.0608, -2.2624, -0.4321]], [[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    th_boxes = torch.tensor([[0.61211395, 0.8129094, 0.10563634, 1.497534, 0.16927195, 0.27956772], [1.430009, 0.49797538, 0.9382923, 0.07694054, 0.9312509, 1.8919173]], dtype=torch.float32)\n    boxes = DepthInstance3DBoxes(th_boxes, box_dim=6, with_yaw=False)\n    expected_tensor = torch.tensor([[0.64884546, 0.78390356, 0.10563634, 1.50373348, 0.23795205, 0.27956772, 0], [1.45139421, 0.43169443, 0.93829232, 0.11967964, 0.93380373, 1.89191735, 0]])\n    boxes_3 = boxes.clone()\n    boxes_3.rotate(-0.04599790655000615)\n    assert torch.allclose(boxes_3.tensor, expected_tensor)\n    boxes.rotate(torch.tensor(-0.04599790655000615))\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([0, 1], dtype=torch.bool)\n    mask = boxes.in_range_3d([1, 0, -2, 2, 1, 5])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[[-0.103, 0.6649, 0.1056], [-0.103, 0.6649, 0.3852], [-0.103, 0.9029, 0.3852], [-0.103, 0.9029, 0.1056], [1.4007, 0.6649, 0.1056], [1.4007, 0.6649, 0.3852], [1.4007, 0.9029, 0.3852], [1.4007, 0.9029, 0.1056]], [[1.3916, -0.0352, 0.9383], [1.3916, -0.0352, 2.8302], [1.3916, 0.8986, 2.8302], [1.3916, 0.8986, 0.9383], [1.5112, -0.0352, 0.9383], [1.5112, -0.0352, 2.8302], [1.5112, 0.8986, 2.8302], [1.5112, 0.8986, 0.9383]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    if torch.cuda.is_available():\n        box_idxs_of_pts = boxes.points_in_boxes_all(points.cuda())\n        expected_idxs_of_pts = torch.tensor([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], device='cuda:0', dtype=torch.int32)\n        assert torch.all(box_idxs_of_pts == expected_idxs_of_pts)\n    boxes = torch.tensor([[0.3294, 1.0359, 0.1171, 1.0822, 1.1247, 1.3721, -0.4916], [-2.463, -2.6324, -0.1616, 0.9202, 1.7896, 0.1992, -0.3185]])\n    boxes = DepthInstance3DBoxes(boxes, box_dim=boxes.shape[-1], with_yaw=True, origin=(0.5, 0.5, 0.5))\n    (surface_center, line_center) = boxes.get_surface_line_center()\n    expected_surface_center = torch.tensor([[0.3294, 1.0359, 0.8031], [0.3294, 1.0359, -0.5689], [0.5949, 1.5317, 0.1171], [0.1533, 0.5018, 0.1171], [0.8064, 0.7805, 0.1171], [-0.1845, 1.2053, 0.1171], [-2.463, -2.6324, -0.062], [-2.463, -2.6324, -0.2612], [-2.0406, -1.8436, -0.1616], [-2.7432, -3.4822, -0.1616], [-2.0574, -2.8496, -0.1616], [-2.9, -2.4883, -0.1616]])\n    expected_line_center = torch.tensor([[0.8064, 0.7805, 0.8031], [-0.1845, 1.2053, 0.8031], [0.5949, 1.5317, 0.8031], [0.1533, 0.5018, 0.8031], [0.8064, 0.7805, -0.5689], [-0.1845, 1.2053, -0.5689], [0.5949, 1.5317, -0.5689], [0.1533, 0.5018, -0.5689], [1.0719, 1.2762, 0.1171], [0.6672, 0.3324, 0.1171], [0.1178, 1.7871, 0.1171], [-0.3606, 0.6713, 0.1171], [-2.0574, -2.8496, -0.062], [-2.9, -2.4883, -0.062], [-2.0406, -1.8436, -0.062], [-2.7432, -3.4822, -0.062], [-2.0574, -2.8496, -0.2612], [-2.9, -2.4883, -0.2612], [-2.0406, -1.8436, -0.2612], [-2.7432, -3.4822, -0.2612], [-1.635, -2.0607, -0.1616], [-2.3062, -3.6263, -0.1616], [-2.4462, -1.6264, -0.1616], [-3.1802, -3.3381, -0.1616]])\n    assert torch.allclose(surface_center, expected_surface_center, atol=0.0001)\n    assert torch.allclose(line_center, expected_line_center, atol=0.0001)",
            "def test_depth_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    empty_boxes = []\n    boxes = DepthInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    np_boxes = np.array([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, --0.44255, 0.8234, 0.5325, 1.0099, 2.9971]], dtype=np.float32)\n    boxes_1 = DepthInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.1093], [2.3262, 3.3065, 0.9475]])\n    assert torch.allclose(boxes_1.gravity_center, expected_tensor)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, 0.9385, 2.1404, 3.0601], [2.3262, 3.3065, 0.8234, 0.5325, 2.9971]])\n    assert torch.allclose(boxes_1.bev, expected_tensor)\n    expected_tensor = torch.tensor([[1.0164, 1.4597, 1.9548, 3.6001], [1.9145, 3.0402, 2.7379, 3.5728]])\n    assert torch.allclose(boxes_1.nearest_bev, expected_tensor, 0.0001)\n    assert repr(boxes) == 'DepthInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    boxes_2 = DepthInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, 0.44255, 0.8234, 0.5325, 1.0099, 2.9971], [2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]])\n    boxes = DepthInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = DepthInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    expected_tensor = torch.tensor([[-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815], [-2.3262, 3.3065, 0.4426, 0.8234, 0.5325, 1.0099, 0.1445], [-2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 0.0723], [-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815]])\n    points = boxes.flip(bev_direction='horizontal', points=points)\n    expected_points = torch.tensor([[-0.6762, 1.2559, -1.4658, 2.5359], [-0.8784, 4.7814, -1.3857, 0.7167], [0.2517, 6.7053, -0.9697, 0.5599], [-0.552, 0.6533, -0.5265, 1.0032], [0.5358, 4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815], [-2.3262, -3.3065, 0.4426, 0.8234, 0.5325, 1.0099, -0.1445], [-2.4593, -2.587, -0.4321, 0.8597, 0.6193, 1.0204, -0.0723], [-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815]])\n    points = boxes.flip(bev_direction='vertical', points=points)\n    expected_points = torch.tensor([[-0.6762, -1.2559, -1.4658, 2.5359], [-0.8784, -4.7814, -1.3857, 0.7167], [0.2517, -6.7053, -0.9697, 0.5599], [-0.552, -0.6533, -0.5265, 1.0032], [0.5358, -4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    boxes_rot = boxes.clone()\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    (points, rot_mat_T) = boxes_rot.rotate(-0.022998953275003075, points)\n    expected_points = torch.tensor([[-0.7049, -1.24, -1.4658, 2.5359], [-0.9881, -4.7599, -1.3857, 0.7167], [0.0974, -6.7093, -0.9697, 0.5599], [-0.5669, -0.6404, -0.5265, 1.0032], [0.4302, -4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T = torch.tensor([[0.9997, -0.023, 0.0], [0.023, 0.9997, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(-0.022998953275003075, points)\n    rot_mat = np.array([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    expected_rot_mat_T = torch.tensor([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    expected_points_np = np.array([[0.7049, 1.24, -1.4658, 2.5359], [0.9881, 4.7599, -1.3857, 0.7167], [-0.0974, 6.7093, -0.9697, 0.5599], [0.5669, 0.6404, -0.5265, 1.0032], [-0.4302, 4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.99973554, -0.02299693, 0.0], [0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    depth_points = DepthPoints(points_np, points_dim=4)\n    (depth_points, rot_mat_T_np) = boxes.rotate(rot_mat, depth_points)\n    points_np = depth_points.tensor.numpy()\n    expected_rot_mat_T_np = expected_rot_mat_T_np.T\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]], [[-2.8519, -3.446, 0.4426], [-2.8519, -3.446, 1.4525], [-2.7632, -2.921, 1.4525], [-2.7632, -2.921, 0.4426], [-2.0401, -3.5833, 0.4426], [-2.0401, -3.5833, 1.4525], [-1.9513, -3.0582, 1.4525], [-1.9513, -3.0582, 0.4426]], [[-2.9755, -2.7971, -0.4321], [-2.9755, -2.7971, 0.5883], [-2.9166, -2.1806, 0.5883], [-2.9166, -2.1806, -0.4321], [-2.1197, -2.8789, -0.4321], [-2.1197, -2.8789, 0.5883], [-2.0608, -2.2624, 0.5883], [-2.0608, -2.2624, -0.4321]], [[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    th_boxes = torch.tensor([[0.61211395, 0.8129094, 0.10563634, 1.497534, 0.16927195, 0.27956772], [1.430009, 0.49797538, 0.9382923, 0.07694054, 0.9312509, 1.8919173]], dtype=torch.float32)\n    boxes = DepthInstance3DBoxes(th_boxes, box_dim=6, with_yaw=False)\n    expected_tensor = torch.tensor([[0.64884546, 0.78390356, 0.10563634, 1.50373348, 0.23795205, 0.27956772, 0], [1.45139421, 0.43169443, 0.93829232, 0.11967964, 0.93380373, 1.89191735, 0]])\n    boxes_3 = boxes.clone()\n    boxes_3.rotate(-0.04599790655000615)\n    assert torch.allclose(boxes_3.tensor, expected_tensor)\n    boxes.rotate(torch.tensor(-0.04599790655000615))\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([0, 1], dtype=torch.bool)\n    mask = boxes.in_range_3d([1, 0, -2, 2, 1, 5])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[[-0.103, 0.6649, 0.1056], [-0.103, 0.6649, 0.3852], [-0.103, 0.9029, 0.3852], [-0.103, 0.9029, 0.1056], [1.4007, 0.6649, 0.1056], [1.4007, 0.6649, 0.3852], [1.4007, 0.9029, 0.3852], [1.4007, 0.9029, 0.1056]], [[1.3916, -0.0352, 0.9383], [1.3916, -0.0352, 2.8302], [1.3916, 0.8986, 2.8302], [1.3916, 0.8986, 0.9383], [1.5112, -0.0352, 0.9383], [1.5112, -0.0352, 2.8302], [1.5112, 0.8986, 2.8302], [1.5112, 0.8986, 0.9383]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    if torch.cuda.is_available():\n        box_idxs_of_pts = boxes.points_in_boxes_all(points.cuda())\n        expected_idxs_of_pts = torch.tensor([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], device='cuda:0', dtype=torch.int32)\n        assert torch.all(box_idxs_of_pts == expected_idxs_of_pts)\n    boxes = torch.tensor([[0.3294, 1.0359, 0.1171, 1.0822, 1.1247, 1.3721, -0.4916], [-2.463, -2.6324, -0.1616, 0.9202, 1.7896, 0.1992, -0.3185]])\n    boxes = DepthInstance3DBoxes(boxes, box_dim=boxes.shape[-1], with_yaw=True, origin=(0.5, 0.5, 0.5))\n    (surface_center, line_center) = boxes.get_surface_line_center()\n    expected_surface_center = torch.tensor([[0.3294, 1.0359, 0.8031], [0.3294, 1.0359, -0.5689], [0.5949, 1.5317, 0.1171], [0.1533, 0.5018, 0.1171], [0.8064, 0.7805, 0.1171], [-0.1845, 1.2053, 0.1171], [-2.463, -2.6324, -0.062], [-2.463, -2.6324, -0.2612], [-2.0406, -1.8436, -0.1616], [-2.7432, -3.4822, -0.1616], [-2.0574, -2.8496, -0.1616], [-2.9, -2.4883, -0.1616]])\n    expected_line_center = torch.tensor([[0.8064, 0.7805, 0.8031], [-0.1845, 1.2053, 0.8031], [0.5949, 1.5317, 0.8031], [0.1533, 0.5018, 0.8031], [0.8064, 0.7805, -0.5689], [-0.1845, 1.2053, -0.5689], [0.5949, 1.5317, -0.5689], [0.1533, 0.5018, -0.5689], [1.0719, 1.2762, 0.1171], [0.6672, 0.3324, 0.1171], [0.1178, 1.7871, 0.1171], [-0.3606, 0.6713, 0.1171], [-2.0574, -2.8496, -0.062], [-2.9, -2.4883, -0.062], [-2.0406, -1.8436, -0.062], [-2.7432, -3.4822, -0.062], [-2.0574, -2.8496, -0.2612], [-2.9, -2.4883, -0.2612], [-2.0406, -1.8436, -0.2612], [-2.7432, -3.4822, -0.2612], [-1.635, -2.0607, -0.1616], [-2.3062, -3.6263, -0.1616], [-2.4462, -1.6264, -0.1616], [-3.1802, -3.3381, -0.1616]])\n    assert torch.allclose(surface_center, expected_surface_center, atol=0.0001)\n    assert torch.allclose(line_center, expected_line_center, atol=0.0001)",
            "def test_depth_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    empty_boxes = []\n    boxes = DepthInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    np_boxes = np.array([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, --0.44255, 0.8234, 0.5325, 1.0099, 2.9971]], dtype=np.float32)\n    boxes_1 = DepthInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.1093], [2.3262, 3.3065, 0.9475]])\n    assert torch.allclose(boxes_1.gravity_center, expected_tensor)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, 0.9385, 2.1404, 3.0601], [2.3262, 3.3065, 0.8234, 0.5325, 2.9971]])\n    assert torch.allclose(boxes_1.bev, expected_tensor)\n    expected_tensor = torch.tensor([[1.0164, 1.4597, 1.9548, 3.6001], [1.9145, 3.0402, 2.7379, 3.5728]])\n    assert torch.allclose(boxes_1.nearest_bev, expected_tensor, 0.0001)\n    assert repr(boxes) == 'DepthInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    boxes_2 = DepthInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, 0.44255, 0.8234, 0.5325, 1.0099, 2.9971], [2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]])\n    boxes = DepthInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = DepthInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    expected_tensor = torch.tensor([[-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815], [-2.3262, 3.3065, 0.4426, 0.8234, 0.5325, 1.0099, 0.1445], [-2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 0.0723], [-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815]])\n    points = boxes.flip(bev_direction='horizontal', points=points)\n    expected_points = torch.tensor([[-0.6762, 1.2559, -1.4658, 2.5359], [-0.8784, 4.7814, -1.3857, 0.7167], [0.2517, 6.7053, -0.9697, 0.5599], [-0.552, 0.6533, -0.5265, 1.0032], [0.5358, 4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815], [-2.3262, -3.3065, 0.4426, 0.8234, 0.5325, 1.0099, -0.1445], [-2.4593, -2.587, -0.4321, 0.8597, 0.6193, 1.0204, -0.0723], [-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815]])\n    points = boxes.flip(bev_direction='vertical', points=points)\n    expected_points = torch.tensor([[-0.6762, -1.2559, -1.4658, 2.5359], [-0.8784, -4.7814, -1.3857, 0.7167], [0.2517, -6.7053, -0.9697, 0.5599], [-0.552, -0.6533, -0.5265, 1.0032], [0.5358, -4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    boxes_rot = boxes.clone()\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    (points, rot_mat_T) = boxes_rot.rotate(-0.022998953275003075, points)\n    expected_points = torch.tensor([[-0.7049, -1.24, -1.4658, 2.5359], [-0.9881, -4.7599, -1.3857, 0.7167], [0.0974, -6.7093, -0.9697, 0.5599], [-0.5669, -0.6404, -0.5265, 1.0032], [0.4302, -4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T = torch.tensor([[0.9997, -0.023, 0.0], [0.023, 0.9997, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(-0.022998953275003075, points)\n    rot_mat = np.array([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    expected_rot_mat_T = torch.tensor([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    expected_points_np = np.array([[0.7049, 1.24, -1.4658, 2.5359], [0.9881, 4.7599, -1.3857, 0.7167], [-0.0974, 6.7093, -0.9697, 0.5599], [0.5669, 0.6404, -0.5265, 1.0032], [-0.4302, 4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.99973554, -0.02299693, 0.0], [0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    depth_points = DepthPoints(points_np, points_dim=4)\n    (depth_points, rot_mat_T_np) = boxes.rotate(rot_mat, depth_points)\n    points_np = depth_points.tensor.numpy()\n    expected_rot_mat_T_np = expected_rot_mat_T_np.T\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]], [[-2.8519, -3.446, 0.4426], [-2.8519, -3.446, 1.4525], [-2.7632, -2.921, 1.4525], [-2.7632, -2.921, 0.4426], [-2.0401, -3.5833, 0.4426], [-2.0401, -3.5833, 1.4525], [-1.9513, -3.0582, 1.4525], [-1.9513, -3.0582, 0.4426]], [[-2.9755, -2.7971, -0.4321], [-2.9755, -2.7971, 0.5883], [-2.9166, -2.1806, 0.5883], [-2.9166, -2.1806, -0.4321], [-2.1197, -2.8789, -0.4321], [-2.1197, -2.8789, 0.5883], [-2.0608, -2.2624, 0.5883], [-2.0608, -2.2624, -0.4321]], [[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    th_boxes = torch.tensor([[0.61211395, 0.8129094, 0.10563634, 1.497534, 0.16927195, 0.27956772], [1.430009, 0.49797538, 0.9382923, 0.07694054, 0.9312509, 1.8919173]], dtype=torch.float32)\n    boxes = DepthInstance3DBoxes(th_boxes, box_dim=6, with_yaw=False)\n    expected_tensor = torch.tensor([[0.64884546, 0.78390356, 0.10563634, 1.50373348, 0.23795205, 0.27956772, 0], [1.45139421, 0.43169443, 0.93829232, 0.11967964, 0.93380373, 1.89191735, 0]])\n    boxes_3 = boxes.clone()\n    boxes_3.rotate(-0.04599790655000615)\n    assert torch.allclose(boxes_3.tensor, expected_tensor)\n    boxes.rotate(torch.tensor(-0.04599790655000615))\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([0, 1], dtype=torch.bool)\n    mask = boxes.in_range_3d([1, 0, -2, 2, 1, 5])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[[-0.103, 0.6649, 0.1056], [-0.103, 0.6649, 0.3852], [-0.103, 0.9029, 0.3852], [-0.103, 0.9029, 0.1056], [1.4007, 0.6649, 0.1056], [1.4007, 0.6649, 0.3852], [1.4007, 0.9029, 0.3852], [1.4007, 0.9029, 0.1056]], [[1.3916, -0.0352, 0.9383], [1.3916, -0.0352, 2.8302], [1.3916, 0.8986, 2.8302], [1.3916, 0.8986, 0.9383], [1.5112, -0.0352, 0.9383], [1.5112, -0.0352, 2.8302], [1.5112, 0.8986, 2.8302], [1.5112, 0.8986, 0.9383]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    if torch.cuda.is_available():\n        box_idxs_of_pts = boxes.points_in_boxes_all(points.cuda())\n        expected_idxs_of_pts = torch.tensor([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], device='cuda:0', dtype=torch.int32)\n        assert torch.all(box_idxs_of_pts == expected_idxs_of_pts)\n    boxes = torch.tensor([[0.3294, 1.0359, 0.1171, 1.0822, 1.1247, 1.3721, -0.4916], [-2.463, -2.6324, -0.1616, 0.9202, 1.7896, 0.1992, -0.3185]])\n    boxes = DepthInstance3DBoxes(boxes, box_dim=boxes.shape[-1], with_yaw=True, origin=(0.5, 0.5, 0.5))\n    (surface_center, line_center) = boxes.get_surface_line_center()\n    expected_surface_center = torch.tensor([[0.3294, 1.0359, 0.8031], [0.3294, 1.0359, -0.5689], [0.5949, 1.5317, 0.1171], [0.1533, 0.5018, 0.1171], [0.8064, 0.7805, 0.1171], [-0.1845, 1.2053, 0.1171], [-2.463, -2.6324, -0.062], [-2.463, -2.6324, -0.2612], [-2.0406, -1.8436, -0.1616], [-2.7432, -3.4822, -0.1616], [-2.0574, -2.8496, -0.1616], [-2.9, -2.4883, -0.1616]])\n    expected_line_center = torch.tensor([[0.8064, 0.7805, 0.8031], [-0.1845, 1.2053, 0.8031], [0.5949, 1.5317, 0.8031], [0.1533, 0.5018, 0.8031], [0.8064, 0.7805, -0.5689], [-0.1845, 1.2053, -0.5689], [0.5949, 1.5317, -0.5689], [0.1533, 0.5018, -0.5689], [1.0719, 1.2762, 0.1171], [0.6672, 0.3324, 0.1171], [0.1178, 1.7871, 0.1171], [-0.3606, 0.6713, 0.1171], [-2.0574, -2.8496, -0.062], [-2.9, -2.4883, -0.062], [-2.0406, -1.8436, -0.062], [-2.7432, -3.4822, -0.062], [-2.0574, -2.8496, -0.2612], [-2.9, -2.4883, -0.2612], [-2.0406, -1.8436, -0.2612], [-2.7432, -3.4822, -0.2612], [-1.635, -2.0607, -0.1616], [-2.3062, -3.6263, -0.1616], [-2.4462, -1.6264, -0.1616], [-3.1802, -3.3381, -0.1616]])\n    assert torch.allclose(surface_center, expected_surface_center, atol=0.0001)\n    assert torch.allclose(line_center, expected_line_center, atol=0.0001)",
            "def test_depth_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    empty_boxes = []\n    boxes = DepthInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    np_boxes = np.array([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, --0.44255, 0.8234, 0.5325, 1.0099, 2.9971]], dtype=np.float32)\n    boxes_1 = DepthInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.1093], [2.3262, 3.3065, 0.9475]])\n    assert torch.allclose(boxes_1.gravity_center, expected_tensor)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, 0.9385, 2.1404, 3.0601], [2.3262, 3.3065, 0.8234, 0.5325, 2.9971]])\n    assert torch.allclose(boxes_1.bev, expected_tensor)\n    expected_tensor = torch.tensor([[1.0164, 1.4597, 1.9548, 3.6001], [1.9145, 3.0402, 2.7379, 3.5728]])\n    assert torch.allclose(boxes_1.nearest_bev, expected_tensor, 0.0001)\n    assert repr(boxes) == 'DepthInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    boxes_2 = DepthInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, 0.44255, 0.8234, 0.5325, 1.0099, 2.9971], [2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]])\n    boxes = DepthInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = DepthInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    expected_tensor = torch.tensor([[-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815], [-2.3262, 3.3065, 0.4426, 0.8234, 0.5325, 1.0099, 0.1445], [-2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 0.0723], [-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815]])\n    points = boxes.flip(bev_direction='horizontal', points=points)\n    expected_points = torch.tensor([[-0.6762, 1.2559, -1.4658, 2.5359], [-0.8784, 4.7814, -1.3857, 0.7167], [0.2517, 6.7053, -0.9697, 0.5599], [-0.552, 0.6533, -0.5265, 1.0032], [0.5358, 4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815], [-2.3262, -3.3065, 0.4426, 0.8234, 0.5325, 1.0099, -0.1445], [-2.4593, -2.587, -0.4321, 0.8597, 0.6193, 1.0204, -0.0723], [-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815]])\n    points = boxes.flip(bev_direction='vertical', points=points)\n    expected_points = torch.tensor([[-0.6762, -1.2559, -1.4658, 2.5359], [-0.8784, -4.7814, -1.3857, 0.7167], [0.2517, -6.7053, -0.9697, 0.5599], [-0.552, -0.6533, -0.5265, 1.0032], [0.5358, -4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    boxes_rot = boxes.clone()\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    (points, rot_mat_T) = boxes_rot.rotate(-0.022998953275003075, points)\n    expected_points = torch.tensor([[-0.7049, -1.24, -1.4658, 2.5359], [-0.9881, -4.7599, -1.3857, 0.7167], [0.0974, -6.7093, -0.9697, 0.5599], [-0.5669, -0.6404, -0.5265, 1.0032], [0.4302, -4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T = torch.tensor([[0.9997, -0.023, 0.0], [0.023, 0.9997, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(-0.022998953275003075, points)\n    rot_mat = np.array([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    expected_rot_mat_T = torch.tensor([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    expected_points_np = np.array([[0.7049, 1.24, -1.4658, 2.5359], [0.9881, 4.7599, -1.3857, 0.7167], [-0.0974, 6.7093, -0.9697, 0.5599], [0.5669, 0.6404, -0.5265, 1.0032], [-0.4302, 4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.99973554, -0.02299693, 0.0], [0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    depth_points = DepthPoints(points_np, points_dim=4)\n    (depth_points, rot_mat_T_np) = boxes.rotate(rot_mat, depth_points)\n    points_np = depth_points.tensor.numpy()\n    expected_rot_mat_T_np = expected_rot_mat_T_np.T\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]], [[-2.8519, -3.446, 0.4426], [-2.8519, -3.446, 1.4525], [-2.7632, -2.921, 1.4525], [-2.7632, -2.921, 0.4426], [-2.0401, -3.5833, 0.4426], [-2.0401, -3.5833, 1.4525], [-1.9513, -3.0582, 1.4525], [-1.9513, -3.0582, 0.4426]], [[-2.9755, -2.7971, -0.4321], [-2.9755, -2.7971, 0.5883], [-2.9166, -2.1806, 0.5883], [-2.9166, -2.1806, -0.4321], [-2.1197, -2.8789, -0.4321], [-2.1197, -2.8789, 0.5883], [-2.0608, -2.2624, 0.5883], [-2.0608, -2.2624, -0.4321]], [[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    th_boxes = torch.tensor([[0.61211395, 0.8129094, 0.10563634, 1.497534, 0.16927195, 0.27956772], [1.430009, 0.49797538, 0.9382923, 0.07694054, 0.9312509, 1.8919173]], dtype=torch.float32)\n    boxes = DepthInstance3DBoxes(th_boxes, box_dim=6, with_yaw=False)\n    expected_tensor = torch.tensor([[0.64884546, 0.78390356, 0.10563634, 1.50373348, 0.23795205, 0.27956772, 0], [1.45139421, 0.43169443, 0.93829232, 0.11967964, 0.93380373, 1.89191735, 0]])\n    boxes_3 = boxes.clone()\n    boxes_3.rotate(-0.04599790655000615)\n    assert torch.allclose(boxes_3.tensor, expected_tensor)\n    boxes.rotate(torch.tensor(-0.04599790655000615))\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([0, 1], dtype=torch.bool)\n    mask = boxes.in_range_3d([1, 0, -2, 2, 1, 5])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[[-0.103, 0.6649, 0.1056], [-0.103, 0.6649, 0.3852], [-0.103, 0.9029, 0.3852], [-0.103, 0.9029, 0.1056], [1.4007, 0.6649, 0.1056], [1.4007, 0.6649, 0.3852], [1.4007, 0.9029, 0.3852], [1.4007, 0.9029, 0.1056]], [[1.3916, -0.0352, 0.9383], [1.3916, -0.0352, 2.8302], [1.3916, 0.8986, 2.8302], [1.3916, 0.8986, 0.9383], [1.5112, -0.0352, 0.9383], [1.5112, -0.0352, 2.8302], [1.5112, 0.8986, 2.8302], [1.5112, 0.8986, 0.9383]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    if torch.cuda.is_available():\n        box_idxs_of_pts = boxes.points_in_boxes_all(points.cuda())\n        expected_idxs_of_pts = torch.tensor([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], device='cuda:0', dtype=torch.int32)\n        assert torch.all(box_idxs_of_pts == expected_idxs_of_pts)\n    boxes = torch.tensor([[0.3294, 1.0359, 0.1171, 1.0822, 1.1247, 1.3721, -0.4916], [-2.463, -2.6324, -0.1616, 0.9202, 1.7896, 0.1992, -0.3185]])\n    boxes = DepthInstance3DBoxes(boxes, box_dim=boxes.shape[-1], with_yaw=True, origin=(0.5, 0.5, 0.5))\n    (surface_center, line_center) = boxes.get_surface_line_center()\n    expected_surface_center = torch.tensor([[0.3294, 1.0359, 0.8031], [0.3294, 1.0359, -0.5689], [0.5949, 1.5317, 0.1171], [0.1533, 0.5018, 0.1171], [0.8064, 0.7805, 0.1171], [-0.1845, 1.2053, 0.1171], [-2.463, -2.6324, -0.062], [-2.463, -2.6324, -0.2612], [-2.0406, -1.8436, -0.1616], [-2.7432, -3.4822, -0.1616], [-2.0574, -2.8496, -0.1616], [-2.9, -2.4883, -0.1616]])\n    expected_line_center = torch.tensor([[0.8064, 0.7805, 0.8031], [-0.1845, 1.2053, 0.8031], [0.5949, 1.5317, 0.8031], [0.1533, 0.5018, 0.8031], [0.8064, 0.7805, -0.5689], [-0.1845, 1.2053, -0.5689], [0.5949, 1.5317, -0.5689], [0.1533, 0.5018, -0.5689], [1.0719, 1.2762, 0.1171], [0.6672, 0.3324, 0.1171], [0.1178, 1.7871, 0.1171], [-0.3606, 0.6713, 0.1171], [-2.0574, -2.8496, -0.062], [-2.9, -2.4883, -0.062], [-2.0406, -1.8436, -0.062], [-2.7432, -3.4822, -0.062], [-2.0574, -2.8496, -0.2612], [-2.9, -2.4883, -0.2612], [-2.0406, -1.8436, -0.2612], [-2.7432, -3.4822, -0.2612], [-1.635, -2.0607, -0.1616], [-2.3062, -3.6263, -0.1616], [-2.4462, -1.6264, -0.1616], [-3.1802, -3.3381, -0.1616]])\n    assert torch.allclose(surface_center, expected_surface_center, atol=0.0001)\n    assert torch.allclose(line_center, expected_line_center, atol=0.0001)",
            "def test_depth_boxes3d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    empty_boxes = []\n    boxes = DepthInstance3DBoxes(empty_boxes)\n    assert boxes.tensor.shape[0] == 0\n    assert boxes.tensor.shape[1] == 7\n    np_boxes = np.array([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, --0.44255, 0.8234, 0.5325, 1.0099, 2.9971]], dtype=np.float32)\n    boxes_1 = DepthInstance3DBoxes(np_boxes)\n    assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))\n    assert boxes_1.volume.size(0) == 2\n    assert (boxes_1.center == boxes_1.bottom_center).all()\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.1093], [2.3262, 3.3065, 0.9475]])\n    assert torch.allclose(boxes_1.gravity_center, expected_tensor)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, 0.9385, 2.1404, 3.0601], [2.3262, 3.3065, 0.8234, 0.5325, 2.9971]])\n    assert torch.allclose(boxes_1.bev, expected_tensor)\n    expected_tensor = torch.tensor([[1.0164, 1.4597, 1.9548, 3.6001], [1.9145, 3.0402, 2.7379, 3.5728]])\n    assert torch.allclose(boxes_1.nearest_bev, expected_tensor, 0.0001)\n    assert repr(boxes) == 'DepthInstance3DBoxes(\\n    tensor([], size=(0, 7)))'\n    th_boxes = torch.tensor([[2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]], dtype=torch.float32)\n    boxes_2 = DepthInstance3DBoxes(th_boxes)\n    assert torch.allclose(boxes_2.tensor, th_boxes)\n    boxes_2 = boxes_2.clone()\n    boxes_1 = boxes_1.to(boxes_2.device)\n    expected_tensor = torch.tensor([[1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601], [2.3262, 3.3065, 0.44255, 0.8234, 0.5325, 1.0099, 2.9971], [2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 3.0693], [1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 3.0601]])\n    boxes = DepthInstance3DBoxes.cat([boxes_1, boxes_2])\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    empty_boxes = DepthInstance3DBoxes.cat([])\n    assert empty_boxes.tensor.shape[0] == 0\n    assert empty_boxes.tensor.shape[-1] == 7\n    points = torch.tensor([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    expected_tensor = torch.tensor([[-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815], [-2.3262, 3.3065, 0.4426, 0.8234, 0.5325, 1.0099, 0.1445], [-2.4593, 2.587, -0.4321, 0.8597, 0.6193, 1.0204, 0.0723], [-1.4856, 2.5299, -0.557, 0.9385, 2.1404, 0.8954, 0.0815]])\n    points = boxes.flip(bev_direction='horizontal', points=points)\n    expected_points = torch.tensor([[-0.6762, 1.2559, -1.4658, 2.5359], [-0.8784, 4.7814, -1.3857, 0.7167], [0.2517, 6.7053, -0.9697, 0.5599], [-0.552, 0.6533, -0.5265, 1.0032], [0.5358, 4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    expected_tensor = torch.tensor([[-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815], [-2.3262, -3.3065, 0.4426, 0.8234, 0.5325, 1.0099, -0.1445], [-2.4593, -2.587, -0.4321, 0.8597, 0.6193, 1.0204, -0.0723], [-1.4856, -2.5299, -0.557, 0.9385, 2.1404, 0.8954, -0.0815]])\n    points = boxes.flip(bev_direction='vertical', points=points)\n    expected_points = torch.tensor([[-0.6762, -1.2559, -1.4658, 2.5359], [-0.8784, -4.7814, -1.3857, 0.7167], [0.2517, -6.7053, -0.9697, 0.5599], [-0.552, -0.6533, -0.5265, 1.0032], [0.5358, -4.587, -1.4741, 0.0556]])\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points)\n    boxes_rot = boxes.clone()\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    (points, rot_mat_T) = boxes_rot.rotate(-0.022998953275003075, points)\n    expected_points = torch.tensor([[-0.7049, -1.24, -1.4658, 2.5359], [-0.9881, -4.7599, -1.3857, 0.7167], [0.0974, -6.7093, -0.9697, 0.5599], [-0.5669, -0.6404, -0.5265, 1.0032], [0.4302, -4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T = torch.tensor([[0.9997, -0.023, 0.0], [0.023, 0.9997, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    (points, rot_mat_T) = boxes.rotate(-0.022998953275003075, points)\n    rot_mat = np.array([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    (points, rot_mat_T) = boxes.rotate(rot_mat, points)\n    expected_rot_mat_T = torch.tensor([[0.99973554, 0.02299693, 0.0], [-0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    assert torch.allclose(boxes_rot.tensor, expected_tensor, 0.001)\n    assert torch.allclose(points, expected_points, 0.001)\n    assert torch.allclose(rot_mat_T, expected_rot_mat_T, 0.001)\n    points_np = np.array([[0.6762, 1.2559, -1.4658, 2.5359], [0.8784, 4.7814, -1.3857, 0.7167], [-0.2517, 6.7053, -0.9697, 0.5599], [0.552, 0.6533, -0.5265, 1.0032], [-0.5358, 4.587, -1.4741, 0.0556]])\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    expected_points_np = np.array([[0.7049, 1.24, -1.4658, 2.5359], [0.9881, 4.7599, -1.3857, 0.7167], [-0.0974, 6.7093, -0.9697, 0.5599], [0.5669, 0.6404, -0.5265, 1.0032], [-0.4302, 4.5981, -1.4741, 0.0556]])\n    expected_rot_mat_T_np = np.array([[0.99973554, -0.02299693, 0.0], [0.02299693, 0.99973554, 0.0], [0.0, 0.0, 1.0]])\n    expected_tensor = torch.tensor([[-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585], [-2.4016, -3.2521, 0.4426, 0.8234, 0.5325, 1.0099, -0.1215], [-2.5181, -2.5298, -0.4321, 0.8597, 0.6193, 1.0204, -0.0493], [-1.5434, -2.4951, -0.557, 0.9385, 2.1404, 0.8954, -0.0585]])\n    expected_tensor[:, -1:] -= 0.022998953275003075 * 2\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    (points_np, rot_mat_T_np) = boxes.rotate(-0.022998953275003075, points_np)\n    depth_points = DepthPoints(points_np, points_dim=4)\n    (depth_points, rot_mat_T_np) = boxes.rotate(rot_mat, depth_points)\n    points_np = depth_points.tensor.numpy()\n    expected_rot_mat_T_np = expected_rot_mat_T_np.T\n    assert torch.allclose(boxes.tensor, expected_tensor, 0.001)\n    assert np.allclose(points_np, expected_points_np, 0.001)\n    assert np.allclose(rot_mat_T_np, expected_rot_mat_T_np, 0.001)\n    expected_tensor = torch.tensor([[[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]], [[-2.8519, -3.446, 0.4426], [-2.8519, -3.446, 1.4525], [-2.7632, -2.921, 1.4525], [-2.7632, -2.921, 0.4426], [-2.0401, -3.5833, 0.4426], [-2.0401, -3.5833, 1.4525], [-1.9513, -3.0582, 1.4525], [-1.9513, -3.0582, 0.4426]], [[-2.9755, -2.7971, -0.4321], [-2.9755, -2.7971, 0.5883], [-2.9166, -2.1806, 0.5883], [-2.9166, -2.1806, -0.4321], [-2.1197, -2.8789, -0.4321], [-2.1197, -2.8789, 0.5883], [-2.0608, -2.2624, 0.5883], [-2.0608, -2.2624, -0.4321]], [[-2.1217, -3.5105, -0.557], [-2.1217, -3.5105, 0.3384], [-1.8985, -1.3818, 0.3384], [-1.8985, -1.3818, -0.557], [-1.1883, -3.6084, -0.557], [-1.1883, -3.6084, 0.3384], [-0.9651, -1.4796, 0.3384], [-0.9651, -1.4796, -0.557]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    th_boxes = torch.tensor([[0.61211395, 0.8129094, 0.10563634, 1.497534, 0.16927195, 0.27956772], [1.430009, 0.49797538, 0.9382923, 0.07694054, 0.9312509, 1.8919173]], dtype=torch.float32)\n    boxes = DepthInstance3DBoxes(th_boxes, box_dim=6, with_yaw=False)\n    expected_tensor = torch.tensor([[0.64884546, 0.78390356, 0.10563634, 1.50373348, 0.23795205, 0.27956772, 0], [1.45139421, 0.43169443, 0.93829232, 0.11967964, 0.93380373, 1.89191735, 0]])\n    boxes_3 = boxes.clone()\n    boxes_3.rotate(-0.04599790655000615)\n    assert torch.allclose(boxes_3.tensor, expected_tensor)\n    boxes.rotate(torch.tensor(-0.04599790655000615))\n    assert torch.allclose(boxes.tensor, expected_tensor)\n    expected_tensor = torch.tensor([1, 1], dtype=torch.bool)\n    mask = boxes.in_range_bev([0.0, -40.0, 70.4, 40.0])\n    assert (mask == expected_tensor).all()\n    mask = boxes.nonempty()\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([0, 1], dtype=torch.bool)\n    mask = boxes.in_range_3d([1, 0, -2, 2, 1, 5])\n    assert (mask == expected_tensor).all()\n    expected_tensor = torch.tensor([[[-0.103, 0.6649, 0.1056], [-0.103, 0.6649, 0.3852], [-0.103, 0.9029, 0.3852], [-0.103, 0.9029, 0.1056], [1.4007, 0.6649, 0.1056], [1.4007, 0.6649, 0.3852], [1.4007, 0.9029, 0.3852], [1.4007, 0.9029, 0.1056]], [[1.3916, -0.0352, 0.9383], [1.3916, -0.0352, 2.8302], [1.3916, 0.8986, 2.8302], [1.3916, 0.8986, 0.9383], [1.5112, -0.0352, 0.9383], [1.5112, -0.0352, 2.8302], [1.5112, 0.8986, 2.8302], [1.5112, 0.8986, 0.9383]]])\n    assert torch.allclose(boxes.corners, expected_tensor, 0.001)\n    if torch.cuda.is_available():\n        box_idxs_of_pts = boxes.points_in_boxes_all(points.cuda())\n        expected_idxs_of_pts = torch.tensor([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], device='cuda:0', dtype=torch.int32)\n        assert torch.all(box_idxs_of_pts == expected_idxs_of_pts)\n    boxes = torch.tensor([[0.3294, 1.0359, 0.1171, 1.0822, 1.1247, 1.3721, -0.4916], [-2.463, -2.6324, -0.1616, 0.9202, 1.7896, 0.1992, -0.3185]])\n    boxes = DepthInstance3DBoxes(boxes, box_dim=boxes.shape[-1], with_yaw=True, origin=(0.5, 0.5, 0.5))\n    (surface_center, line_center) = boxes.get_surface_line_center()\n    expected_surface_center = torch.tensor([[0.3294, 1.0359, 0.8031], [0.3294, 1.0359, -0.5689], [0.5949, 1.5317, 0.1171], [0.1533, 0.5018, 0.1171], [0.8064, 0.7805, 0.1171], [-0.1845, 1.2053, 0.1171], [-2.463, -2.6324, -0.062], [-2.463, -2.6324, -0.2612], [-2.0406, -1.8436, -0.1616], [-2.7432, -3.4822, -0.1616], [-2.0574, -2.8496, -0.1616], [-2.9, -2.4883, -0.1616]])\n    expected_line_center = torch.tensor([[0.8064, 0.7805, 0.8031], [-0.1845, 1.2053, 0.8031], [0.5949, 1.5317, 0.8031], [0.1533, 0.5018, 0.8031], [0.8064, 0.7805, -0.5689], [-0.1845, 1.2053, -0.5689], [0.5949, 1.5317, -0.5689], [0.1533, 0.5018, -0.5689], [1.0719, 1.2762, 0.1171], [0.6672, 0.3324, 0.1171], [0.1178, 1.7871, 0.1171], [-0.3606, 0.6713, 0.1171], [-2.0574, -2.8496, -0.062], [-2.9, -2.4883, -0.062], [-2.0406, -1.8436, -0.062], [-2.7432, -3.4822, -0.062], [-2.0574, -2.8496, -0.2612], [-2.9, -2.4883, -0.2612], [-2.0406, -1.8436, -0.2612], [-2.7432, -3.4822, -0.2612], [-1.635, -2.0607, -0.1616], [-2.3062, -3.6263, -0.1616], [-2.4462, -1.6264, -0.1616], [-3.1802, -3.3381, -0.1616]])\n    assert torch.allclose(surface_center, expected_surface_center, atol=0.0001)\n    assert torch.allclose(line_center, expected_line_center, atol=0.0001)"
        ]
    },
    {
        "func_name": "test_rotation_3d_in_axis",
        "original": "def test_rotation_3d_in_axis():\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072], [-0.2555, 0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([-np.pi / 10, np.pi / 10]), axis=0, clockwise=True)\n    expected_rotated = torch.tensor([[[-0.4599, -0.0448, -0.0146], [-0.4599, -0.6144, 1.7385], [-0.4599, -0.5248, 1.7676]], [[-0.2555, -0.2552, 0.0829], [-0.2555, 0.0252, 0.9457], [-0.2555, 0.5355, 0.7799]]], dtype=torch.float32)\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    assert torch.allclose(rotated, expected_rotated, 0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    (rotated, mat) = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0, return_mat=True)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    expected_mat = torch.tensor([[[1, 0, 0], [0, 0, 1], [0, -1, 0]]]).float()\n    assert torch.allclose(rotated, expected_rotated, atol=1e-06)\n    assert torch.allclose(mat, expected_mat, atol=1e-06)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = np.array([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]]).astype(np.float32)\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=0).numpy()\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, 0.2683], [-0.2555, 0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=1).numpy()\n    expected_rotated = np.array([[[0.0, -0.0471, 0.4599], [1.8433, -0.0471, 0.4599]], [[0.0, -0.2683, -0.2555], [-0.9072, -0.2683, -0.2555]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=2).numpy()\n    expected_rotated = np.array([[[0.0471, -0.4599, 0.0], [-0.0471, -0.4599, 1.8433]], [[-0.2683, 0.2555, 0.0], [-0.2683, -0.2555, 0.9072]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.0471, 0.0], [-0.0471, 1.8433]], [[-0.2683, 0.0], [-0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles)\n    expected_rotated = np.array([[[0.0, -0.0471], [-1.8433, -0.0471]], [[0.0, 0.2683], [0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)",
        "mutated": [
            "def test_rotation_3d_in_axis():\n    if False:\n        i = 10\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072], [-0.2555, 0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([-np.pi / 10, np.pi / 10]), axis=0, clockwise=True)\n    expected_rotated = torch.tensor([[[-0.4599, -0.0448, -0.0146], [-0.4599, -0.6144, 1.7385], [-0.4599, -0.5248, 1.7676]], [[-0.2555, -0.2552, 0.0829], [-0.2555, 0.0252, 0.9457], [-0.2555, 0.5355, 0.7799]]], dtype=torch.float32)\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    assert torch.allclose(rotated, expected_rotated, 0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    (rotated, mat) = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0, return_mat=True)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    expected_mat = torch.tensor([[[1, 0, 0], [0, 0, 1], [0, -1, 0]]]).float()\n    assert torch.allclose(rotated, expected_rotated, atol=1e-06)\n    assert torch.allclose(mat, expected_mat, atol=1e-06)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = np.array([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]]).astype(np.float32)\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=0).numpy()\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, 0.2683], [-0.2555, 0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=1).numpy()\n    expected_rotated = np.array([[[0.0, -0.0471, 0.4599], [1.8433, -0.0471, 0.4599]], [[0.0, -0.2683, -0.2555], [-0.9072, -0.2683, -0.2555]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=2).numpy()\n    expected_rotated = np.array([[[0.0471, -0.4599, 0.0], [-0.0471, -0.4599, 1.8433]], [[-0.2683, 0.2555, 0.0], [-0.2683, -0.2555, 0.9072]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.0471, 0.0], [-0.0471, 1.8433]], [[-0.2683, 0.0], [-0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles)\n    expected_rotated = np.array([[[0.0, -0.0471], [-1.8433, -0.0471]], [[0.0, 0.2683], [0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)",
            "def test_rotation_3d_in_axis():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072], [-0.2555, 0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([-np.pi / 10, np.pi / 10]), axis=0, clockwise=True)\n    expected_rotated = torch.tensor([[[-0.4599, -0.0448, -0.0146], [-0.4599, -0.6144, 1.7385], [-0.4599, -0.5248, 1.7676]], [[-0.2555, -0.2552, 0.0829], [-0.2555, 0.0252, 0.9457], [-0.2555, 0.5355, 0.7799]]], dtype=torch.float32)\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    assert torch.allclose(rotated, expected_rotated, 0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    (rotated, mat) = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0, return_mat=True)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    expected_mat = torch.tensor([[[1, 0, 0], [0, 0, 1], [0, -1, 0]]]).float()\n    assert torch.allclose(rotated, expected_rotated, atol=1e-06)\n    assert torch.allclose(mat, expected_mat, atol=1e-06)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = np.array([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]]).astype(np.float32)\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=0).numpy()\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, 0.2683], [-0.2555, 0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=1).numpy()\n    expected_rotated = np.array([[[0.0, -0.0471, 0.4599], [1.8433, -0.0471, 0.4599]], [[0.0, -0.2683, -0.2555], [-0.9072, -0.2683, -0.2555]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=2).numpy()\n    expected_rotated = np.array([[[0.0471, -0.4599, 0.0], [-0.0471, -0.4599, 1.8433]], [[-0.2683, 0.2555, 0.0], [-0.2683, -0.2555, 0.9072]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.0471, 0.0], [-0.0471, 1.8433]], [[-0.2683, 0.0], [-0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles)\n    expected_rotated = np.array([[[0.0, -0.0471], [-1.8433, -0.0471]], [[0.0, 0.2683], [0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)",
            "def test_rotation_3d_in_axis():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072], [-0.2555, 0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([-np.pi / 10, np.pi / 10]), axis=0, clockwise=True)\n    expected_rotated = torch.tensor([[[-0.4599, -0.0448, -0.0146], [-0.4599, -0.6144, 1.7385], [-0.4599, -0.5248, 1.7676]], [[-0.2555, -0.2552, 0.0829], [-0.2555, 0.0252, 0.9457], [-0.2555, 0.5355, 0.7799]]], dtype=torch.float32)\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    assert torch.allclose(rotated, expected_rotated, 0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    (rotated, mat) = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0, return_mat=True)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    expected_mat = torch.tensor([[[1, 0, 0], [0, 0, 1], [0, -1, 0]]]).float()\n    assert torch.allclose(rotated, expected_rotated, atol=1e-06)\n    assert torch.allclose(mat, expected_mat, atol=1e-06)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = np.array([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]]).astype(np.float32)\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=0).numpy()\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, 0.2683], [-0.2555, 0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=1).numpy()\n    expected_rotated = np.array([[[0.0, -0.0471, 0.4599], [1.8433, -0.0471, 0.4599]], [[0.0, -0.2683, -0.2555], [-0.9072, -0.2683, -0.2555]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=2).numpy()\n    expected_rotated = np.array([[[0.0471, -0.4599, 0.0], [-0.0471, -0.4599, 1.8433]], [[-0.2683, 0.2555, 0.0], [-0.2683, -0.2555, 0.9072]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.0471, 0.0], [-0.0471, 1.8433]], [[-0.2683, 0.0], [-0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles)\n    expected_rotated = np.array([[[0.0, -0.0471], [-1.8433, -0.0471]], [[0.0, 0.2683], [0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)",
            "def test_rotation_3d_in_axis():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072], [-0.2555, 0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([-np.pi / 10, np.pi / 10]), axis=0, clockwise=True)\n    expected_rotated = torch.tensor([[[-0.4599, -0.0448, -0.0146], [-0.4599, -0.6144, 1.7385], [-0.4599, -0.5248, 1.7676]], [[-0.2555, -0.2552, 0.0829], [-0.2555, 0.0252, 0.9457], [-0.2555, 0.5355, 0.7799]]], dtype=torch.float32)\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    assert torch.allclose(rotated, expected_rotated, 0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    (rotated, mat) = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0, return_mat=True)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    expected_mat = torch.tensor([[[1, 0, 0], [0, 0, 1], [0, -1, 0]]]).float()\n    assert torch.allclose(rotated, expected_rotated, atol=1e-06)\n    assert torch.allclose(mat, expected_mat, atol=1e-06)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = np.array([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]]).astype(np.float32)\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=0).numpy()\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, 0.2683], [-0.2555, 0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=1).numpy()\n    expected_rotated = np.array([[[0.0, -0.0471, 0.4599], [1.8433, -0.0471, 0.4599]], [[0.0, -0.2683, -0.2555], [-0.9072, -0.2683, -0.2555]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=2).numpy()\n    expected_rotated = np.array([[[0.0471, -0.4599, 0.0], [-0.0471, -0.4599, 1.8433]], [[-0.2683, 0.2555, 0.0], [-0.2683, -0.2555, 0.9072]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.0471, 0.0], [-0.0471, 1.8433]], [[-0.2683, 0.0], [-0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles)\n    expected_rotated = np.array([[[0.0, -0.0471], [-1.8433, -0.0471]], [[0.0, 0.2683], [0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)",
            "def test_rotation_3d_in_axis():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072], [-0.2555, 0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([-np.pi / 10, np.pi / 10]), axis=0, clockwise=True)\n    expected_rotated = torch.tensor([[[-0.4599, -0.0448, -0.0146], [-0.4599, -0.6144, 1.7385], [-0.4599, -0.5248, 1.7676]], [[-0.2555, -0.2552, 0.0829], [-0.2555, 0.0252, 0.9457], [-0.2555, 0.5355, 0.7799]]], dtype=torch.float32)\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    rotated = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    assert torch.allclose(rotated, expected_rotated, 0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]]])\n    (rotated, mat) = rotation_3d_in_axis(points, torch.tensor([np.pi / 2]), axis=0, return_mat=True)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]]])\n    expected_mat = torch.tensor([[[1, 0, 0], [0, 0, 1], [0, -1, 0]]]).float()\n    assert torch.allclose(rotated, expected_rotated, atol=1e-06)\n    assert torch.allclose(mat, expected_mat, atol=1e-06)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = torch.tensor([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert torch.allclose(rotated, expected_rotated, atol=0.001)\n    points = np.array([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]]).astype(np.float32)\n    rotated = rotation_3d_in_axis(points, np.pi / 2, axis=0)\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, -0.2683], [-0.2555, -0.9072, -0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=0).numpy()\n    expected_rotated = np.array([[[-0.4599, 0.0, -0.0471], [-0.4599, -1.8433, -0.0471]], [[-0.2555, 0.0, 0.2683], [-0.2555, 0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, -0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [-0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=1).numpy()\n    expected_rotated = np.array([[[0.0, -0.0471, 0.4599], [1.8433, -0.0471, 0.4599]], [[0.0, -0.2683, -0.2555], [-0.9072, -0.2683, -0.2555]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.4599, -0.0471, 0.0], [-0.4599, 0.0471, 1.8433]], [[-0.2555, -0.2683, 0.0], [0.2555, -0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles, axis=2).numpy()\n    expected_rotated = np.array([[[0.0471, -0.4599, 0.0], [-0.0471, -0.4599, 1.8433]], [[-0.2683, 0.2555, 0.0], [-0.2683, -0.2555, 0.9072]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)\n    points = torch.tensor([[[-0.0471, 0.0], [-0.0471, 1.8433]], [[-0.2683, 0.0], [-0.2683, 0.9072]]])\n    angles = [np.pi / 2, -np.pi / 2]\n    rotated = rotation_3d_in_axis(points, angles)\n    expected_rotated = np.array([[[0.0, -0.0471], [-1.8433, -0.0471]], [[0.0, 0.2683], [0.9072, 0.2683]]])\n    assert np.allclose(rotated, expected_rotated, atol=0.001)"
        ]
    },
    {
        "func_name": "test_rotation_2d",
        "original": "def test_rotation_2d():\n    angles = np.array([3.14])\n    corners = np.array([[[-0.235, -0.49], [-0.235, 0.49], [0.235, 0.49], [0.235, -0.49]]])\n    corners_rotated = rotation_3d_in_axis(corners, angles)\n    expected_corners = np.array([[[0.2357801, 0.48962511], [0.2342193, -0.49037365], [-0.2357801, -0.48962511], [-0.2342193, 0.49037365]]])\n    assert np.allclose(corners_rotated, expected_corners)",
        "mutated": [
            "def test_rotation_2d():\n    if False:\n        i = 10\n    angles = np.array([3.14])\n    corners = np.array([[[-0.235, -0.49], [-0.235, 0.49], [0.235, 0.49], [0.235, -0.49]]])\n    corners_rotated = rotation_3d_in_axis(corners, angles)\n    expected_corners = np.array([[[0.2357801, 0.48962511], [0.2342193, -0.49037365], [-0.2357801, -0.48962511], [-0.2342193, 0.49037365]]])\n    assert np.allclose(corners_rotated, expected_corners)",
            "def test_rotation_2d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    angles = np.array([3.14])\n    corners = np.array([[[-0.235, -0.49], [-0.235, 0.49], [0.235, 0.49], [0.235, -0.49]]])\n    corners_rotated = rotation_3d_in_axis(corners, angles)\n    expected_corners = np.array([[[0.2357801, 0.48962511], [0.2342193, -0.49037365], [-0.2357801, -0.48962511], [-0.2342193, 0.49037365]]])\n    assert np.allclose(corners_rotated, expected_corners)",
            "def test_rotation_2d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    angles = np.array([3.14])\n    corners = np.array([[[-0.235, -0.49], [-0.235, 0.49], [0.235, 0.49], [0.235, -0.49]]])\n    corners_rotated = rotation_3d_in_axis(corners, angles)\n    expected_corners = np.array([[[0.2357801, 0.48962511], [0.2342193, -0.49037365], [-0.2357801, -0.48962511], [-0.2342193, 0.49037365]]])\n    assert np.allclose(corners_rotated, expected_corners)",
            "def test_rotation_2d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    angles = np.array([3.14])\n    corners = np.array([[[-0.235, -0.49], [-0.235, 0.49], [0.235, 0.49], [0.235, -0.49]]])\n    corners_rotated = rotation_3d_in_axis(corners, angles)\n    expected_corners = np.array([[[0.2357801, 0.48962511], [0.2342193, -0.49037365], [-0.2357801, -0.48962511], [-0.2342193, 0.49037365]]])\n    assert np.allclose(corners_rotated, expected_corners)",
            "def test_rotation_2d():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    angles = np.array([3.14])\n    corners = np.array([[[-0.235, -0.49], [-0.235, 0.49], [0.235, 0.49], [0.235, -0.49]]])\n    corners_rotated = rotation_3d_in_axis(corners, angles)\n    expected_corners = np.array([[[0.2357801, 0.48962511], [0.2342193, -0.49037365], [-0.2357801, -0.48962511], [-0.2342193, 0.49037365]]])\n    assert np.allclose(corners_rotated, expected_corners)"
        ]
    },
    {
        "func_name": "test_limit_period",
        "original": "def test_limit_period():\n    torch.manual_seed(0)\n    val = torch.rand([5, 1])\n    result = limit_period(val)\n    expected_result = torch.tensor([[0.4963], [0.7682], [0.0885], [0.132], [0.3074]])\n    assert torch.allclose(result, expected_result, 0.001)\n    val = val.numpy()\n    result = limit_period(val)\n    expected_result = expected_result.numpy()\n    assert np.allclose(result, expected_result, 0.001)",
        "mutated": [
            "def test_limit_period():\n    if False:\n        i = 10\n    torch.manual_seed(0)\n    val = torch.rand([5, 1])\n    result = limit_period(val)\n    expected_result = torch.tensor([[0.4963], [0.7682], [0.0885], [0.132], [0.3074]])\n    assert torch.allclose(result, expected_result, 0.001)\n    val = val.numpy()\n    result = limit_period(val)\n    expected_result = expected_result.numpy()\n    assert np.allclose(result, expected_result, 0.001)",
            "def test_limit_period():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    torch.manual_seed(0)\n    val = torch.rand([5, 1])\n    result = limit_period(val)\n    expected_result = torch.tensor([[0.4963], [0.7682], [0.0885], [0.132], [0.3074]])\n    assert torch.allclose(result, expected_result, 0.001)\n    val = val.numpy()\n    result = limit_period(val)\n    expected_result = expected_result.numpy()\n    assert np.allclose(result, expected_result, 0.001)",
            "def test_limit_period():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    torch.manual_seed(0)\n    val = torch.rand([5, 1])\n    result = limit_period(val)\n    expected_result = torch.tensor([[0.4963], [0.7682], [0.0885], [0.132], [0.3074]])\n    assert torch.allclose(result, expected_result, 0.001)\n    val = val.numpy()\n    result = limit_period(val)\n    expected_result = expected_result.numpy()\n    assert np.allclose(result, expected_result, 0.001)",
            "def test_limit_period():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    torch.manual_seed(0)\n    val = torch.rand([5, 1])\n    result = limit_period(val)\n    expected_result = torch.tensor([[0.4963], [0.7682], [0.0885], [0.132], [0.3074]])\n    assert torch.allclose(result, expected_result, 0.001)\n    val = val.numpy()\n    result = limit_period(val)\n    expected_result = expected_result.numpy()\n    assert np.allclose(result, expected_result, 0.001)",
            "def test_limit_period():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    torch.manual_seed(0)\n    val = torch.rand([5, 1])\n    result = limit_period(val)\n    expected_result = torch.tensor([[0.4963], [0.7682], [0.0885], [0.132], [0.3074]])\n    assert torch.allclose(result, expected_result, 0.001)\n    val = val.numpy()\n    result = limit_period(val)\n    expected_result = expected_result.numpy()\n    assert np.allclose(result, expected_result, 0.001)"
        ]
    },
    {
        "func_name": "test_xywhr2xyxyr",
        "original": "def test_xywhr2xyxyr():\n    torch.manual_seed(0)\n    xywhr = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0], [0.0, 1.0, 2.0, 3.0, 4.0]])\n    xyxyr = xywhr2xyxyr(xywhr)\n    expected_xyxyr = torch.tensor([[-0.5, 0.0, 2.5, 4.0, 5.0], [-1.0, -0.5, 1.0, 2.5, 4.0]])\n    assert torch.allclose(xyxyr, expected_xyxyr)",
        "mutated": [
            "def test_xywhr2xyxyr():\n    if False:\n        i = 10\n    torch.manual_seed(0)\n    xywhr = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0], [0.0, 1.0, 2.0, 3.0, 4.0]])\n    xyxyr = xywhr2xyxyr(xywhr)\n    expected_xyxyr = torch.tensor([[-0.5, 0.0, 2.5, 4.0, 5.0], [-1.0, -0.5, 1.0, 2.5, 4.0]])\n    assert torch.allclose(xyxyr, expected_xyxyr)",
            "def test_xywhr2xyxyr():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    torch.manual_seed(0)\n    xywhr = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0], [0.0, 1.0, 2.0, 3.0, 4.0]])\n    xyxyr = xywhr2xyxyr(xywhr)\n    expected_xyxyr = torch.tensor([[-0.5, 0.0, 2.5, 4.0, 5.0], [-1.0, -0.5, 1.0, 2.5, 4.0]])\n    assert torch.allclose(xyxyr, expected_xyxyr)",
            "def test_xywhr2xyxyr():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    torch.manual_seed(0)\n    xywhr = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0], [0.0, 1.0, 2.0, 3.0, 4.0]])\n    xyxyr = xywhr2xyxyr(xywhr)\n    expected_xyxyr = torch.tensor([[-0.5, 0.0, 2.5, 4.0, 5.0], [-1.0, -0.5, 1.0, 2.5, 4.0]])\n    assert torch.allclose(xyxyr, expected_xyxyr)",
            "def test_xywhr2xyxyr():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    torch.manual_seed(0)\n    xywhr = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0], [0.0, 1.0, 2.0, 3.0, 4.0]])\n    xyxyr = xywhr2xyxyr(xywhr)\n    expected_xyxyr = torch.tensor([[-0.5, 0.0, 2.5, 4.0, 5.0], [-1.0, -0.5, 1.0, 2.5, 4.0]])\n    assert torch.allclose(xyxyr, expected_xyxyr)",
            "def test_xywhr2xyxyr():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    torch.manual_seed(0)\n    xywhr = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0], [0.0, 1.0, 2.0, 3.0, 4.0]])\n    xyxyr = xywhr2xyxyr(xywhr)\n    expected_xyxyr = torch.tensor([[-0.5, 0.0, 2.5, 4.0, 5.0], [-1.0, -0.5, 1.0, 2.5, 4.0]])\n    assert torch.allclose(xyxyr, expected_xyxyr)"
        ]
    },
    {
        "func_name": "test_get_box_type",
        "original": "def test_get_box_type(self):\n    (box_type_3d, box_mode_3d) = get_box_type('camera')\n    assert box_type_3d == CameraInstance3DBoxes\n    assert box_mode_3d == Box3DMode.CAM\n    (box_type_3d, box_mode_3d) = get_box_type('depth')\n    assert box_type_3d == DepthInstance3DBoxes\n    assert box_mode_3d == Box3DMode.DEPTH\n    (box_type_3d, box_mode_3d) = get_box_type('lidar')\n    assert box_type_3d == LiDARInstance3DBoxes\n    assert box_mode_3d == Box3DMode.LIDAR",
        "mutated": [
            "def test_get_box_type(self):\n    if False:\n        i = 10\n    (box_type_3d, box_mode_3d) = get_box_type('camera')\n    assert box_type_3d == CameraInstance3DBoxes\n    assert box_mode_3d == Box3DMode.CAM\n    (box_type_3d, box_mode_3d) = get_box_type('depth')\n    assert box_type_3d == DepthInstance3DBoxes\n    assert box_mode_3d == Box3DMode.DEPTH\n    (box_type_3d, box_mode_3d) = get_box_type('lidar')\n    assert box_type_3d == LiDARInstance3DBoxes\n    assert box_mode_3d == Box3DMode.LIDAR",
            "def test_get_box_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (box_type_3d, box_mode_3d) = get_box_type('camera')\n    assert box_type_3d == CameraInstance3DBoxes\n    assert box_mode_3d == Box3DMode.CAM\n    (box_type_3d, box_mode_3d) = get_box_type('depth')\n    assert box_type_3d == DepthInstance3DBoxes\n    assert box_mode_3d == Box3DMode.DEPTH\n    (box_type_3d, box_mode_3d) = get_box_type('lidar')\n    assert box_type_3d == LiDARInstance3DBoxes\n    assert box_mode_3d == Box3DMode.LIDAR",
            "def test_get_box_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (box_type_3d, box_mode_3d) = get_box_type('camera')\n    assert box_type_3d == CameraInstance3DBoxes\n    assert box_mode_3d == Box3DMode.CAM\n    (box_type_3d, box_mode_3d) = get_box_type('depth')\n    assert box_type_3d == DepthInstance3DBoxes\n    assert box_mode_3d == Box3DMode.DEPTH\n    (box_type_3d, box_mode_3d) = get_box_type('lidar')\n    assert box_type_3d == LiDARInstance3DBoxes\n    assert box_mode_3d == Box3DMode.LIDAR",
            "def test_get_box_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (box_type_3d, box_mode_3d) = get_box_type('camera')\n    assert box_type_3d == CameraInstance3DBoxes\n    assert box_mode_3d == Box3DMode.CAM\n    (box_type_3d, box_mode_3d) = get_box_type('depth')\n    assert box_type_3d == DepthInstance3DBoxes\n    assert box_mode_3d == Box3DMode.DEPTH\n    (box_type_3d, box_mode_3d) = get_box_type('lidar')\n    assert box_type_3d == LiDARInstance3DBoxes\n    assert box_mode_3d == Box3DMode.LIDAR",
            "def test_get_box_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (box_type_3d, box_mode_3d) = get_box_type('camera')\n    assert box_type_3d == CameraInstance3DBoxes\n    assert box_mode_3d == Box3DMode.CAM\n    (box_type_3d, box_mode_3d) = get_box_type('depth')\n    assert box_type_3d == DepthInstance3DBoxes\n    assert box_mode_3d == Box3DMode.DEPTH\n    (box_type_3d, box_mode_3d) = get_box_type('lidar')\n    assert box_type_3d == LiDARInstance3DBoxes\n    assert box_mode_3d == Box3DMode.LIDAR"
        ]
    },
    {
        "func_name": "test_bad_box_type",
        "original": "def test_bad_box_type(self):\n    self.assertRaises(ValueError, get_box_type, 'test')",
        "mutated": [
            "def test_bad_box_type(self):\n    if False:\n        i = 10\n    self.assertRaises(ValueError, get_box_type, 'test')",
            "def test_bad_box_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.assertRaises(ValueError, get_box_type, 'test')",
            "def test_bad_box_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.assertRaises(ValueError, get_box_type, 'test')",
            "def test_bad_box_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.assertRaises(ValueError, get_box_type, 'test')",
            "def test_bad_box_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.assertRaises(ValueError, get_box_type, 'test')"
        ]
    },
    {
        "func_name": "test_points_cam2img",
        "original": "def test_points_cam2img():\n    torch.manual_seed(0)\n    points = torch.rand([5, 3])\n    proj_mat = torch.rand([4, 4])\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496], [0.6146, 0.791], [0.6994, 0.7782], [0.5623, 0.6303], [0.4359, 0.6532]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = points.numpy()\n    proj_mat = proj_mat.numpy()\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = expected_point_2d_res.numpy()\n    assert np.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = torch.from_numpy(points)\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.from_numpy(expected_point_2d_res)\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    point_2d_res = points_cam2img(points, proj_mat, with_depth=True)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496, 1.7577], [0.6146, 0.791, 1.5477], [0.6994, 0.7782, 2.0091], [0.5623, 0.6303, 1.8739], [0.4359, 0.6532, 1.2056]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)",
        "mutated": [
            "def test_points_cam2img():\n    if False:\n        i = 10\n    torch.manual_seed(0)\n    points = torch.rand([5, 3])\n    proj_mat = torch.rand([4, 4])\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496], [0.6146, 0.791], [0.6994, 0.7782], [0.5623, 0.6303], [0.4359, 0.6532]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = points.numpy()\n    proj_mat = proj_mat.numpy()\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = expected_point_2d_res.numpy()\n    assert np.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = torch.from_numpy(points)\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.from_numpy(expected_point_2d_res)\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    point_2d_res = points_cam2img(points, proj_mat, with_depth=True)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496, 1.7577], [0.6146, 0.791, 1.5477], [0.6994, 0.7782, 2.0091], [0.5623, 0.6303, 1.8739], [0.4359, 0.6532, 1.2056]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)",
            "def test_points_cam2img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    torch.manual_seed(0)\n    points = torch.rand([5, 3])\n    proj_mat = torch.rand([4, 4])\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496], [0.6146, 0.791], [0.6994, 0.7782], [0.5623, 0.6303], [0.4359, 0.6532]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = points.numpy()\n    proj_mat = proj_mat.numpy()\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = expected_point_2d_res.numpy()\n    assert np.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = torch.from_numpy(points)\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.from_numpy(expected_point_2d_res)\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    point_2d_res = points_cam2img(points, proj_mat, with_depth=True)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496, 1.7577], [0.6146, 0.791, 1.5477], [0.6994, 0.7782, 2.0091], [0.5623, 0.6303, 1.8739], [0.4359, 0.6532, 1.2056]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)",
            "def test_points_cam2img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    torch.manual_seed(0)\n    points = torch.rand([5, 3])\n    proj_mat = torch.rand([4, 4])\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496], [0.6146, 0.791], [0.6994, 0.7782], [0.5623, 0.6303], [0.4359, 0.6532]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = points.numpy()\n    proj_mat = proj_mat.numpy()\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = expected_point_2d_res.numpy()\n    assert np.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = torch.from_numpy(points)\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.from_numpy(expected_point_2d_res)\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    point_2d_res = points_cam2img(points, proj_mat, with_depth=True)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496, 1.7577], [0.6146, 0.791, 1.5477], [0.6994, 0.7782, 2.0091], [0.5623, 0.6303, 1.8739], [0.4359, 0.6532, 1.2056]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)",
            "def test_points_cam2img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    torch.manual_seed(0)\n    points = torch.rand([5, 3])\n    proj_mat = torch.rand([4, 4])\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496], [0.6146, 0.791], [0.6994, 0.7782], [0.5623, 0.6303], [0.4359, 0.6532]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = points.numpy()\n    proj_mat = proj_mat.numpy()\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = expected_point_2d_res.numpy()\n    assert np.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = torch.from_numpy(points)\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.from_numpy(expected_point_2d_res)\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    point_2d_res = points_cam2img(points, proj_mat, with_depth=True)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496, 1.7577], [0.6146, 0.791, 1.5477], [0.6994, 0.7782, 2.0091], [0.5623, 0.6303, 1.8739], [0.4359, 0.6532, 1.2056]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)",
            "def test_points_cam2img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    torch.manual_seed(0)\n    points = torch.rand([5, 3])\n    proj_mat = torch.rand([4, 4])\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496], [0.6146, 0.791], [0.6994, 0.7782], [0.5623, 0.6303], [0.4359, 0.6532]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = points.numpy()\n    proj_mat = proj_mat.numpy()\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = expected_point_2d_res.numpy()\n    assert np.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    points = torch.from_numpy(points)\n    point_2d_res = points_cam2img(points, proj_mat)\n    expected_point_2d_res = torch.from_numpy(expected_point_2d_res)\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)\n    point_2d_res = points_cam2img(points, proj_mat, with_depth=True)\n    expected_point_2d_res = torch.tensor([[0.5832, 0.6496, 1.7577], [0.6146, 0.791, 1.5477], [0.6994, 0.7782, 2.0091], [0.5623, 0.6303, 1.8739], [0.4359, 0.6532, 1.2056]])\n    assert torch.allclose(point_2d_res, expected_point_2d_res, 0.001)"
        ]
    },
    {
        "func_name": "test_points_in_boxes",
        "original": "def test_points_in_boxes():\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_all(lidar_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1], [0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 1], [1, 0, 1, 1], [0, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8, 4])\n    assert (point_indices == expected_point_indices).all()\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_part(lidar_pts)\n    expected_point_indices = torch.tensor([0, -1, 0, 3, 0, -1, 1, 1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5]], dtype=torch.float32).cuda()\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_pts = torch.tensor([[[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4]]], dtype=torch.float32).cuda()\n    point_indices = depth_boxes.points_in_boxes_all(depth_pts)\n    expected_point_indices = torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15, 2])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = depth_boxes.points_in_boxes_part(depth_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    cam_boxes = DepthInstance3DBoxes(depth_boxes).convert_to(Box3DMode.CAM)\n    depth_pts = torch.tensor([[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4], [1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]], dtype=torch.float32).cuda()\n    cam_pts = DepthPoints(depth_pts).convert_to(Coord3DMode.CAM).tensor\n    point_indices = cam_boxes.points_in_boxes_all(cam_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23, 6])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_batch(cam_pts)\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_part(cam_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 2, 3, 3, 2, 2, 3, 0, 0], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes(cam_pts)\n    assert (point_indices == expected_point_indices).all()",
        "mutated": [
            "def test_points_in_boxes():\n    if False:\n        i = 10\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_all(lidar_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1], [0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 1], [1, 0, 1, 1], [0, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8, 4])\n    assert (point_indices == expected_point_indices).all()\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_part(lidar_pts)\n    expected_point_indices = torch.tensor([0, -1, 0, 3, 0, -1, 1, 1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5]], dtype=torch.float32).cuda()\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_pts = torch.tensor([[[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4]]], dtype=torch.float32).cuda()\n    point_indices = depth_boxes.points_in_boxes_all(depth_pts)\n    expected_point_indices = torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15, 2])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = depth_boxes.points_in_boxes_part(depth_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    cam_boxes = DepthInstance3DBoxes(depth_boxes).convert_to(Box3DMode.CAM)\n    depth_pts = torch.tensor([[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4], [1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]], dtype=torch.float32).cuda()\n    cam_pts = DepthPoints(depth_pts).convert_to(Coord3DMode.CAM).tensor\n    point_indices = cam_boxes.points_in_boxes_all(cam_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23, 6])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_batch(cam_pts)\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_part(cam_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 2, 3, 3, 2, 2, 3, 0, 0], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes(cam_pts)\n    assert (point_indices == expected_point_indices).all()",
            "def test_points_in_boxes():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_all(lidar_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1], [0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 1], [1, 0, 1, 1], [0, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8, 4])\n    assert (point_indices == expected_point_indices).all()\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_part(lidar_pts)\n    expected_point_indices = torch.tensor([0, -1, 0, 3, 0, -1, 1, 1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5]], dtype=torch.float32).cuda()\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_pts = torch.tensor([[[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4]]], dtype=torch.float32).cuda()\n    point_indices = depth_boxes.points_in_boxes_all(depth_pts)\n    expected_point_indices = torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15, 2])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = depth_boxes.points_in_boxes_part(depth_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    cam_boxes = DepthInstance3DBoxes(depth_boxes).convert_to(Box3DMode.CAM)\n    depth_pts = torch.tensor([[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4], [1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]], dtype=torch.float32).cuda()\n    cam_pts = DepthPoints(depth_pts).convert_to(Coord3DMode.CAM).tensor\n    point_indices = cam_boxes.points_in_boxes_all(cam_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23, 6])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_batch(cam_pts)\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_part(cam_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 2, 3, 3, 2, 2, 3, 0, 0], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes(cam_pts)\n    assert (point_indices == expected_point_indices).all()",
            "def test_points_in_boxes():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_all(lidar_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1], [0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 1], [1, 0, 1, 1], [0, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8, 4])\n    assert (point_indices == expected_point_indices).all()\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_part(lidar_pts)\n    expected_point_indices = torch.tensor([0, -1, 0, 3, 0, -1, 1, 1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5]], dtype=torch.float32).cuda()\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_pts = torch.tensor([[[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4]]], dtype=torch.float32).cuda()\n    point_indices = depth_boxes.points_in_boxes_all(depth_pts)\n    expected_point_indices = torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15, 2])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = depth_boxes.points_in_boxes_part(depth_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    cam_boxes = DepthInstance3DBoxes(depth_boxes).convert_to(Box3DMode.CAM)\n    depth_pts = torch.tensor([[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4], [1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]], dtype=torch.float32).cuda()\n    cam_pts = DepthPoints(depth_pts).convert_to(Coord3DMode.CAM).tensor\n    point_indices = cam_boxes.points_in_boxes_all(cam_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23, 6])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_batch(cam_pts)\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_part(cam_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 2, 3, 3, 2, 2, 3, 0, 0], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes(cam_pts)\n    assert (point_indices == expected_point_indices).all()",
            "def test_points_in_boxes():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_all(lidar_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1], [0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 1], [1, 0, 1, 1], [0, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8, 4])\n    assert (point_indices == expected_point_indices).all()\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_part(lidar_pts)\n    expected_point_indices = torch.tensor([0, -1, 0, 3, 0, -1, 1, 1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5]], dtype=torch.float32).cuda()\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_pts = torch.tensor([[[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4]]], dtype=torch.float32).cuda()\n    point_indices = depth_boxes.points_in_boxes_all(depth_pts)\n    expected_point_indices = torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15, 2])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = depth_boxes.points_in_boxes_part(depth_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    cam_boxes = DepthInstance3DBoxes(depth_boxes).convert_to(Box3DMode.CAM)\n    depth_pts = torch.tensor([[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4], [1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]], dtype=torch.float32).cuda()\n    cam_pts = DepthPoints(depth_pts).convert_to(Coord3DMode.CAM).tensor\n    point_indices = cam_boxes.points_in_boxes_all(cam_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23, 6])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_batch(cam_pts)\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_part(cam_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 2, 3, 3, 2, 2, 3, 0, 0], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes(cam_pts)\n    assert (point_indices == expected_point_indices).all()",
            "def test_points_in_boxes():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not torch.cuda.is_available():\n        pytest.skip('test requires GPU and torch+cuda')\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_all(lidar_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1], [0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 1], [1, 0, 1, 1], [0, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8, 4])\n    assert (point_indices == expected_point_indices).all()\n    lidar_pts = torch.tensor([[1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]]).cuda()\n    lidar_boxes = torch.tensor([[1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    lidar_boxes = LiDARInstance3DBoxes(lidar_boxes)\n    point_indices = lidar_boxes.points_in_boxes_part(lidar_pts)\n    expected_point_indices = torch.tensor([0, -1, 0, 3, 0, -1, 1, 1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([8])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5]], dtype=torch.float32).cuda()\n    depth_boxes = DepthInstance3DBoxes(depth_boxes)\n    depth_pts = torch.tensor([[[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4]]], dtype=torch.float32).cuda()\n    point_indices = depth_boxes.points_in_boxes_all(depth_pts)\n    expected_point_indices = torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15, 2])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = depth_boxes.points_in_boxes_part(depth_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([15])\n    assert (point_indices == expected_point_indices).all()\n    depth_boxes = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.3], [-10.0, 23.0, 16.0, 10, 20, 20, 0.5], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, np.pi / 2], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, 7 * np.pi / 6], [1.0, 2.0, 0.0, 4.0, 4.0, 6.0, -np.pi / 6]], dtype=torch.float32).cuda()\n    cam_boxes = DepthInstance3DBoxes(depth_boxes).convert_to(Box3DMode.CAM)\n    depth_pts = torch.tensor([[1, 2, 3.3], [1.2, 2.5, 3.0], [0.8, 2.1, 3.5], [1.6, 2.6, 3.6], [0.8, 1.2, 3.9], [-9.2, 21.0, 18.2], [3.8, 7.9, 6.3], [4.7, 3.5, -12.2], [3.8, 7.6, -2], [-10.6, -12.9, -20], [-16, -18, 9], [-21.3, -52, -5], [0, 0, 0], [6, 7, 8], [-2, -3, -4], [1.0, 4.3, 0.1], [1.0, 4.4, 0.1], [1.1, 4.3, 0.1], [0.9, 4.3, 0.1], [1.0, -0.3, 0.1], [1.0, -0.4, 0.1], [2.9, 0.1, 6.0], [-0.9, 3.9, 6.0]], dtype=torch.float32).cuda()\n    cam_pts = DepthPoints(depth_pts).convert_to(Coord3DMode.CAM).tensor\n    point_indices = cam_boxes.points_in_boxes_all(cam_pts)\n    expected_point_indices = torch.tensor([[1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1], [0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23, 6])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_batch(cam_pts)\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes_part(cam_pts)\n    expected_point_indices = torch.tensor([0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 2, 3, 3, 2, 2, 3, 0, 0], dtype=torch.int32).cuda()\n    assert point_indices.shape == torch.Size([23])\n    assert (point_indices == expected_point_indices).all()\n    point_indices = cam_boxes.points_in_boxes(cam_pts)\n    assert (point_indices == expected_point_indices).all()"
        ]
    }
]