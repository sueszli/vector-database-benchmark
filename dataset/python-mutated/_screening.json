[
    {
        "func_name": "__init__",
        "original": "def __init__(self, screener, **kwds):\n    self.screener = screener\n    self.__dict__.update(**kwds)",
        "mutated": [
            "def __init__(self, screener, **kwds):\n    if False:\n        i = 10\n    self.screener = screener\n    self.__dict__.update(**kwds)",
            "def __init__(self, screener, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.screener = screener\n    self.__dict__.update(**kwds)",
            "def __init__(self, screener, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.screener = screener\n    self.__dict__.update(**kwds)",
            "def __init__(self, screener, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.screener = screener\n    self.__dict__.update(**kwds)",
            "def __init__(self, screener, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.screener = screener\n    self.__dict__.update(**kwds)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model, pen_weight=None, use_weights=True, k_add=30, k_max_add=30, threshold_trim=0.0001, k_max_included=20, ranking_attr='resid_pearson', ranking_project=True):\n    self.model = model\n    self.model_class = model.__class__\n    self.init_kwds = model._get_init_kwds()\n    self.init_kwds.pop('pen_weight', None)\n    self.init_kwds.pop('penal', None)\n    self.endog = model.endog\n    self.exog_keep = model.exog\n    self.k_keep = model.exog.shape[1]\n    self.nobs = len(self.endog)\n    self.penal = self._get_penal()\n    if pen_weight is not None:\n        self.pen_weight = pen_weight\n    else:\n        self.pen_weight = self.nobs * 10\n    self.use_weights = use_weights\n    self.k_add = k_add\n    self.k_max_add = k_max_add\n    self.threshold_trim = threshold_trim\n    self.k_max_included = k_max_included\n    self.ranking_attr = ranking_attr\n    self.ranking_project = ranking_project",
        "mutated": [
            "def __init__(self, model, pen_weight=None, use_weights=True, k_add=30, k_max_add=30, threshold_trim=0.0001, k_max_included=20, ranking_attr='resid_pearson', ranking_project=True):\n    if False:\n        i = 10\n    self.model = model\n    self.model_class = model.__class__\n    self.init_kwds = model._get_init_kwds()\n    self.init_kwds.pop('pen_weight', None)\n    self.init_kwds.pop('penal', None)\n    self.endog = model.endog\n    self.exog_keep = model.exog\n    self.k_keep = model.exog.shape[1]\n    self.nobs = len(self.endog)\n    self.penal = self._get_penal()\n    if pen_weight is not None:\n        self.pen_weight = pen_weight\n    else:\n        self.pen_weight = self.nobs * 10\n    self.use_weights = use_weights\n    self.k_add = k_add\n    self.k_max_add = k_max_add\n    self.threshold_trim = threshold_trim\n    self.k_max_included = k_max_included\n    self.ranking_attr = ranking_attr\n    self.ranking_project = ranking_project",
            "def __init__(self, model, pen_weight=None, use_weights=True, k_add=30, k_max_add=30, threshold_trim=0.0001, k_max_included=20, ranking_attr='resid_pearson', ranking_project=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.model = model\n    self.model_class = model.__class__\n    self.init_kwds = model._get_init_kwds()\n    self.init_kwds.pop('pen_weight', None)\n    self.init_kwds.pop('penal', None)\n    self.endog = model.endog\n    self.exog_keep = model.exog\n    self.k_keep = model.exog.shape[1]\n    self.nobs = len(self.endog)\n    self.penal = self._get_penal()\n    if pen_weight is not None:\n        self.pen_weight = pen_weight\n    else:\n        self.pen_weight = self.nobs * 10\n    self.use_weights = use_weights\n    self.k_add = k_add\n    self.k_max_add = k_max_add\n    self.threshold_trim = threshold_trim\n    self.k_max_included = k_max_included\n    self.ranking_attr = ranking_attr\n    self.ranking_project = ranking_project",
            "def __init__(self, model, pen_weight=None, use_weights=True, k_add=30, k_max_add=30, threshold_trim=0.0001, k_max_included=20, ranking_attr='resid_pearson', ranking_project=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.model = model\n    self.model_class = model.__class__\n    self.init_kwds = model._get_init_kwds()\n    self.init_kwds.pop('pen_weight', None)\n    self.init_kwds.pop('penal', None)\n    self.endog = model.endog\n    self.exog_keep = model.exog\n    self.k_keep = model.exog.shape[1]\n    self.nobs = len(self.endog)\n    self.penal = self._get_penal()\n    if pen_weight is not None:\n        self.pen_weight = pen_weight\n    else:\n        self.pen_weight = self.nobs * 10\n    self.use_weights = use_weights\n    self.k_add = k_add\n    self.k_max_add = k_max_add\n    self.threshold_trim = threshold_trim\n    self.k_max_included = k_max_included\n    self.ranking_attr = ranking_attr\n    self.ranking_project = ranking_project",
            "def __init__(self, model, pen_weight=None, use_weights=True, k_add=30, k_max_add=30, threshold_trim=0.0001, k_max_included=20, ranking_attr='resid_pearson', ranking_project=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.model = model\n    self.model_class = model.__class__\n    self.init_kwds = model._get_init_kwds()\n    self.init_kwds.pop('pen_weight', None)\n    self.init_kwds.pop('penal', None)\n    self.endog = model.endog\n    self.exog_keep = model.exog\n    self.k_keep = model.exog.shape[1]\n    self.nobs = len(self.endog)\n    self.penal = self._get_penal()\n    if pen_weight is not None:\n        self.pen_weight = pen_weight\n    else:\n        self.pen_weight = self.nobs * 10\n    self.use_weights = use_weights\n    self.k_add = k_add\n    self.k_max_add = k_max_add\n    self.threshold_trim = threshold_trim\n    self.k_max_included = k_max_included\n    self.ranking_attr = ranking_attr\n    self.ranking_project = ranking_project",
            "def __init__(self, model, pen_weight=None, use_weights=True, k_add=30, k_max_add=30, threshold_trim=0.0001, k_max_included=20, ranking_attr='resid_pearson', ranking_project=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.model = model\n    self.model_class = model.__class__\n    self.init_kwds = model._get_init_kwds()\n    self.init_kwds.pop('pen_weight', None)\n    self.init_kwds.pop('penal', None)\n    self.endog = model.endog\n    self.exog_keep = model.exog\n    self.k_keep = model.exog.shape[1]\n    self.nobs = len(self.endog)\n    self.penal = self._get_penal()\n    if pen_weight is not None:\n        self.pen_weight = pen_weight\n    else:\n        self.pen_weight = self.nobs * 10\n    self.use_weights = use_weights\n    self.k_add = k_add\n    self.k_max_add = k_max_add\n    self.threshold_trim = threshold_trim\n    self.k_max_included = k_max_included\n    self.ranking_attr = ranking_attr\n    self.ranking_project = ranking_project"
        ]
    },
    {
        "func_name": "_get_penal",
        "original": "def _get_penal(self, weights=None):\n    \"\"\"create new Penalty instance\n        \"\"\"\n    return SCADSmoothed(0.1, c0=0.0001, weights=weights)",
        "mutated": [
            "def _get_penal(self, weights=None):\n    if False:\n        i = 10\n    'create new Penalty instance\\n        '\n    return SCADSmoothed(0.1, c0=0.0001, weights=weights)",
            "def _get_penal(self, weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'create new Penalty instance\\n        '\n    return SCADSmoothed(0.1, c0=0.0001, weights=weights)",
            "def _get_penal(self, weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'create new Penalty instance\\n        '\n    return SCADSmoothed(0.1, c0=0.0001, weights=weights)",
            "def _get_penal(self, weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'create new Penalty instance\\n        '\n    return SCADSmoothed(0.1, c0=0.0001, weights=weights)",
            "def _get_penal(self, weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'create new Penalty instance\\n        '\n    return SCADSmoothed(0.1, c0=0.0001, weights=weights)"
        ]
    },
    {
        "func_name": "ranking_measure",
        "original": "def ranking_measure(self, res_pen, exog, keep=None):\n    \"\"\"compute measure for ranking exog candidates for inclusion\n        \"\"\"\n    endog = self.endog\n    if self.ranking_project:\n        assert res_pen.model.exog.shape[1] == len(keep)\n        ex_incl = res_pen.model.exog[:, keep]\n        exog = exog - ex_incl.dot(np.linalg.pinv(ex_incl).dot(exog))\n    if self.ranking_attr == 'predicted_poisson':\n        p = res_pen.params.copy()\n        if keep is not None:\n            p[~keep] = 0\n        predicted = res_pen.model.predict(p)\n        resid_factor = (endog - predicted) / np.sqrt(predicted)\n    elif self.ranking_attr[:6] == 'model.':\n        attr = self.ranking_attr.split('.')[1]\n        resid_factor = getattr(res_pen.model, attr)(res_pen.params)\n        if resid_factor.ndim == 2:\n            resid_factor = resid_factor[:, 0]\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    else:\n        resid_factor = getattr(res_pen, self.ranking_attr)\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    return mom_cond",
        "mutated": [
            "def ranking_measure(self, res_pen, exog, keep=None):\n    if False:\n        i = 10\n    'compute measure for ranking exog candidates for inclusion\\n        '\n    endog = self.endog\n    if self.ranking_project:\n        assert res_pen.model.exog.shape[1] == len(keep)\n        ex_incl = res_pen.model.exog[:, keep]\n        exog = exog - ex_incl.dot(np.linalg.pinv(ex_incl).dot(exog))\n    if self.ranking_attr == 'predicted_poisson':\n        p = res_pen.params.copy()\n        if keep is not None:\n            p[~keep] = 0\n        predicted = res_pen.model.predict(p)\n        resid_factor = (endog - predicted) / np.sqrt(predicted)\n    elif self.ranking_attr[:6] == 'model.':\n        attr = self.ranking_attr.split('.')[1]\n        resid_factor = getattr(res_pen.model, attr)(res_pen.params)\n        if resid_factor.ndim == 2:\n            resid_factor = resid_factor[:, 0]\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    else:\n        resid_factor = getattr(res_pen, self.ranking_attr)\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    return mom_cond",
            "def ranking_measure(self, res_pen, exog, keep=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'compute measure for ranking exog candidates for inclusion\\n        '\n    endog = self.endog\n    if self.ranking_project:\n        assert res_pen.model.exog.shape[1] == len(keep)\n        ex_incl = res_pen.model.exog[:, keep]\n        exog = exog - ex_incl.dot(np.linalg.pinv(ex_incl).dot(exog))\n    if self.ranking_attr == 'predicted_poisson':\n        p = res_pen.params.copy()\n        if keep is not None:\n            p[~keep] = 0\n        predicted = res_pen.model.predict(p)\n        resid_factor = (endog - predicted) / np.sqrt(predicted)\n    elif self.ranking_attr[:6] == 'model.':\n        attr = self.ranking_attr.split('.')[1]\n        resid_factor = getattr(res_pen.model, attr)(res_pen.params)\n        if resid_factor.ndim == 2:\n            resid_factor = resid_factor[:, 0]\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    else:\n        resid_factor = getattr(res_pen, self.ranking_attr)\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    return mom_cond",
            "def ranking_measure(self, res_pen, exog, keep=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'compute measure for ranking exog candidates for inclusion\\n        '\n    endog = self.endog\n    if self.ranking_project:\n        assert res_pen.model.exog.shape[1] == len(keep)\n        ex_incl = res_pen.model.exog[:, keep]\n        exog = exog - ex_incl.dot(np.linalg.pinv(ex_incl).dot(exog))\n    if self.ranking_attr == 'predicted_poisson':\n        p = res_pen.params.copy()\n        if keep is not None:\n            p[~keep] = 0\n        predicted = res_pen.model.predict(p)\n        resid_factor = (endog - predicted) / np.sqrt(predicted)\n    elif self.ranking_attr[:6] == 'model.':\n        attr = self.ranking_attr.split('.')[1]\n        resid_factor = getattr(res_pen.model, attr)(res_pen.params)\n        if resid_factor.ndim == 2:\n            resid_factor = resid_factor[:, 0]\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    else:\n        resid_factor = getattr(res_pen, self.ranking_attr)\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    return mom_cond",
            "def ranking_measure(self, res_pen, exog, keep=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'compute measure for ranking exog candidates for inclusion\\n        '\n    endog = self.endog\n    if self.ranking_project:\n        assert res_pen.model.exog.shape[1] == len(keep)\n        ex_incl = res_pen.model.exog[:, keep]\n        exog = exog - ex_incl.dot(np.linalg.pinv(ex_incl).dot(exog))\n    if self.ranking_attr == 'predicted_poisson':\n        p = res_pen.params.copy()\n        if keep is not None:\n            p[~keep] = 0\n        predicted = res_pen.model.predict(p)\n        resid_factor = (endog - predicted) / np.sqrt(predicted)\n    elif self.ranking_attr[:6] == 'model.':\n        attr = self.ranking_attr.split('.')[1]\n        resid_factor = getattr(res_pen.model, attr)(res_pen.params)\n        if resid_factor.ndim == 2:\n            resid_factor = resid_factor[:, 0]\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    else:\n        resid_factor = getattr(res_pen, self.ranking_attr)\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    return mom_cond",
            "def ranking_measure(self, res_pen, exog, keep=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'compute measure for ranking exog candidates for inclusion\\n        '\n    endog = self.endog\n    if self.ranking_project:\n        assert res_pen.model.exog.shape[1] == len(keep)\n        ex_incl = res_pen.model.exog[:, keep]\n        exog = exog - ex_incl.dot(np.linalg.pinv(ex_incl).dot(exog))\n    if self.ranking_attr == 'predicted_poisson':\n        p = res_pen.params.copy()\n        if keep is not None:\n            p[~keep] = 0\n        predicted = res_pen.model.predict(p)\n        resid_factor = (endog - predicted) / np.sqrt(predicted)\n    elif self.ranking_attr[:6] == 'model.':\n        attr = self.ranking_attr.split('.')[1]\n        resid_factor = getattr(res_pen.model, attr)(res_pen.params)\n        if resid_factor.ndim == 2:\n            resid_factor = resid_factor[:, 0]\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    else:\n        resid_factor = getattr(res_pen, self.ranking_attr)\n        mom_cond = np.abs(resid_factor.dot(exog)) ** 2\n    return mom_cond"
        ]
    },
    {
        "func_name": "screen_exog",
        "original": "def screen_exog(self, exog, endog=None, maxiter=100, method='bfgs', disp=False, fit_kwds=None):\n    \"\"\"screen and select variables (columns) in exog\n\n        Parameters\n        ----------\n        exog : ndarray\n            candidate explanatory variables that are screened for inclusion in\n            the model\n        endog : ndarray (optional)\n            use a new endog in the screening model.\n            This is not tested yet, and might not work correctly\n        maxiter : int\n            number of screening iterations\n        method : str\n            optimization method to use in fit, needs to be only of the gradient\n            optimizers\n        disp : bool\n            display option for fit during optimization\n\n        Returns\n        -------\n        res_screen : instance of ScreeningResults\n            The attribute `results_final` contains is the results instance\n            with the final model selection.\n            `idx_nonzero` contains the index of the selected exog in the full\n            exog, combined exog that are always kept plust exog_candidates.\n            see ScreeningResults for a full description\n        \"\"\"\n    model_class = self.model_class\n    if endog is None:\n        endog = self.endog\n    x0 = self.exog_keep\n    k_keep = self.k_keep\n    x1 = exog\n    k_current = x0.shape[1]\n    x = np.column_stack((x0, x1))\n    (nobs, k_vars) = x.shape\n    fkwds = fit_kwds if fit_kwds is not None else {}\n    fit_kwds = {'maxiter': 200, 'disp': False}\n    fit_kwds.update(fkwds)\n    history = defaultdict(list)\n    idx_nonzero = np.arange(k_keep, dtype=int)\n    keep = np.ones(k_keep, np.bool_)\n    idx_excl = np.arange(k_keep, k_vars)\n    mod_pen = model_class(endog, x0, **self.init_kwds)\n    mod_pen.pen_weight = 0\n    res_pen = mod_pen.fit(**fit_kwds)\n    start_params = res_pen.params\n    converged = False\n    idx_old = []\n    for it in range(maxiter):\n        x1 = x[:, idx_excl]\n        mom_cond = self.ranking_measure(res_pen, x1, keep=keep)\n        assert len(mom_cond) == len(idx_excl)\n        mcs = np.sort(mom_cond)[::-1]\n        idx_thr = min((self.k_max_add, k_current + self.k_add, len(mcs)))\n        threshold = mcs[idx_thr]\n        idx = np.concatenate((idx_nonzero, idx_excl[mom_cond > threshold]))\n        start_params2 = np.zeros(len(idx))\n        start_params2[:len(start_params)] = start_params\n        if self.use_weights:\n            weights = np.ones(len(idx))\n            weights[:k_keep] = 0\n            self.penal.weights = weights\n        mod_pen = model_class(endog, x[:, idx], penal=self.penal, pen_weight=self.pen_weight, **self.init_kwds)\n        res_pen = mod_pen.fit(method=method, start_params=start_params2, warn_convergence=False, skip_hessian=True, **fit_kwds)\n        keep = np.abs(res_pen.params) > self.threshold_trim\n        if keep.sum() > self.k_max_included:\n            thresh_params = np.sort(np.abs(res_pen.params))[-self.k_max_included]\n            keep2 = np.abs(res_pen.params) > thresh_params\n            keep = np.logical_and(keep, keep2)\n        keep[:k_keep] = True\n        idx_nonzero = idx[keep]\n        if disp:\n            print(keep)\n            print(idx_nonzero)\n        k_current = len(idx_nonzero)\n        start_params = res_pen.params[keep]\n        mask_excl = np.ones(k_vars, dtype=bool)\n        mask_excl[idx_nonzero] = False\n        idx_excl = np.nonzero(mask_excl)[0]\n        history['idx_nonzero'].append(idx_nonzero)\n        history['keep'].append(keep)\n        history['params_keep'].append(start_params)\n        history['idx_added'].append(idx)\n        if len(idx_nonzero) == len(idx_old) and (idx_nonzero == idx_old).all():\n            converged = True\n            break\n        idx_old = idx_nonzero\n    assert np.all(idx_nonzero[:k_keep] == np.arange(k_keep))\n    if self.use_weights:\n        weights = np.ones(len(idx_nonzero))\n        weights[:k_keep] = 0\n        penal = self._get_penal(weights=weights)\n    else:\n        penal = self.penal\n    mod_final = model_class(endog, x[:, idx_nonzero], penal=penal, pen_weight=self.pen_weight, **self.init_kwds)\n    res_final = mod_final.fit(method=method, start_params=start_params, warn_convergence=False, **fit_kwds)\n    xnames = ['var%4d' % ii for ii in idx_nonzero]\n    res_final.model.exog_names[k_keep:] = xnames[k_keep:]\n    res = ScreeningResults(self, results_pen=res_pen, results_final=res_final, idx_nonzero=idx_nonzero, idx_exog=idx_nonzero[k_keep:] - k_keep, idx_excl=idx_excl, history=history, converged=converged, iterations=it + 1)\n    return res",
        "mutated": [
            "def screen_exog(self, exog, endog=None, maxiter=100, method='bfgs', disp=False, fit_kwds=None):\n    if False:\n        i = 10\n    'screen and select variables (columns) in exog\\n\\n        Parameters\\n        ----------\\n        exog : ndarray\\n            candidate explanatory variables that are screened for inclusion in\\n            the model\\n        endog : ndarray (optional)\\n            use a new endog in the screening model.\\n            This is not tested yet, and might not work correctly\\n        maxiter : int\\n            number of screening iterations\\n        method : str\\n            optimization method to use in fit, needs to be only of the gradient\\n            optimizers\\n        disp : bool\\n            display option for fit during optimization\\n\\n        Returns\\n        -------\\n        res_screen : instance of ScreeningResults\\n            The attribute `results_final` contains is the results instance\\n            with the final model selection.\\n            `idx_nonzero` contains the index of the selected exog in the full\\n            exog, combined exog that are always kept plust exog_candidates.\\n            see ScreeningResults for a full description\\n        '\n    model_class = self.model_class\n    if endog is None:\n        endog = self.endog\n    x0 = self.exog_keep\n    k_keep = self.k_keep\n    x1 = exog\n    k_current = x0.shape[1]\n    x = np.column_stack((x0, x1))\n    (nobs, k_vars) = x.shape\n    fkwds = fit_kwds if fit_kwds is not None else {}\n    fit_kwds = {'maxiter': 200, 'disp': False}\n    fit_kwds.update(fkwds)\n    history = defaultdict(list)\n    idx_nonzero = np.arange(k_keep, dtype=int)\n    keep = np.ones(k_keep, np.bool_)\n    idx_excl = np.arange(k_keep, k_vars)\n    mod_pen = model_class(endog, x0, **self.init_kwds)\n    mod_pen.pen_weight = 0\n    res_pen = mod_pen.fit(**fit_kwds)\n    start_params = res_pen.params\n    converged = False\n    idx_old = []\n    for it in range(maxiter):\n        x1 = x[:, idx_excl]\n        mom_cond = self.ranking_measure(res_pen, x1, keep=keep)\n        assert len(mom_cond) == len(idx_excl)\n        mcs = np.sort(mom_cond)[::-1]\n        idx_thr = min((self.k_max_add, k_current + self.k_add, len(mcs)))\n        threshold = mcs[idx_thr]\n        idx = np.concatenate((idx_nonzero, idx_excl[mom_cond > threshold]))\n        start_params2 = np.zeros(len(idx))\n        start_params2[:len(start_params)] = start_params\n        if self.use_weights:\n            weights = np.ones(len(idx))\n            weights[:k_keep] = 0\n            self.penal.weights = weights\n        mod_pen = model_class(endog, x[:, idx], penal=self.penal, pen_weight=self.pen_weight, **self.init_kwds)\n        res_pen = mod_pen.fit(method=method, start_params=start_params2, warn_convergence=False, skip_hessian=True, **fit_kwds)\n        keep = np.abs(res_pen.params) > self.threshold_trim\n        if keep.sum() > self.k_max_included:\n            thresh_params = np.sort(np.abs(res_pen.params))[-self.k_max_included]\n            keep2 = np.abs(res_pen.params) > thresh_params\n            keep = np.logical_and(keep, keep2)\n        keep[:k_keep] = True\n        idx_nonzero = idx[keep]\n        if disp:\n            print(keep)\n            print(idx_nonzero)\n        k_current = len(idx_nonzero)\n        start_params = res_pen.params[keep]\n        mask_excl = np.ones(k_vars, dtype=bool)\n        mask_excl[idx_nonzero] = False\n        idx_excl = np.nonzero(mask_excl)[0]\n        history['idx_nonzero'].append(idx_nonzero)\n        history['keep'].append(keep)\n        history['params_keep'].append(start_params)\n        history['idx_added'].append(idx)\n        if len(idx_nonzero) == len(idx_old) and (idx_nonzero == idx_old).all():\n            converged = True\n            break\n        idx_old = idx_nonzero\n    assert np.all(idx_nonzero[:k_keep] == np.arange(k_keep))\n    if self.use_weights:\n        weights = np.ones(len(idx_nonzero))\n        weights[:k_keep] = 0\n        penal = self._get_penal(weights=weights)\n    else:\n        penal = self.penal\n    mod_final = model_class(endog, x[:, idx_nonzero], penal=penal, pen_weight=self.pen_weight, **self.init_kwds)\n    res_final = mod_final.fit(method=method, start_params=start_params, warn_convergence=False, **fit_kwds)\n    xnames = ['var%4d' % ii for ii in idx_nonzero]\n    res_final.model.exog_names[k_keep:] = xnames[k_keep:]\n    res = ScreeningResults(self, results_pen=res_pen, results_final=res_final, idx_nonzero=idx_nonzero, idx_exog=idx_nonzero[k_keep:] - k_keep, idx_excl=idx_excl, history=history, converged=converged, iterations=it + 1)\n    return res",
            "def screen_exog(self, exog, endog=None, maxiter=100, method='bfgs', disp=False, fit_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'screen and select variables (columns) in exog\\n\\n        Parameters\\n        ----------\\n        exog : ndarray\\n            candidate explanatory variables that are screened for inclusion in\\n            the model\\n        endog : ndarray (optional)\\n            use a new endog in the screening model.\\n            This is not tested yet, and might not work correctly\\n        maxiter : int\\n            number of screening iterations\\n        method : str\\n            optimization method to use in fit, needs to be only of the gradient\\n            optimizers\\n        disp : bool\\n            display option for fit during optimization\\n\\n        Returns\\n        -------\\n        res_screen : instance of ScreeningResults\\n            The attribute `results_final` contains is the results instance\\n            with the final model selection.\\n            `idx_nonzero` contains the index of the selected exog in the full\\n            exog, combined exog that are always kept plust exog_candidates.\\n            see ScreeningResults for a full description\\n        '\n    model_class = self.model_class\n    if endog is None:\n        endog = self.endog\n    x0 = self.exog_keep\n    k_keep = self.k_keep\n    x1 = exog\n    k_current = x0.shape[1]\n    x = np.column_stack((x0, x1))\n    (nobs, k_vars) = x.shape\n    fkwds = fit_kwds if fit_kwds is not None else {}\n    fit_kwds = {'maxiter': 200, 'disp': False}\n    fit_kwds.update(fkwds)\n    history = defaultdict(list)\n    idx_nonzero = np.arange(k_keep, dtype=int)\n    keep = np.ones(k_keep, np.bool_)\n    idx_excl = np.arange(k_keep, k_vars)\n    mod_pen = model_class(endog, x0, **self.init_kwds)\n    mod_pen.pen_weight = 0\n    res_pen = mod_pen.fit(**fit_kwds)\n    start_params = res_pen.params\n    converged = False\n    idx_old = []\n    for it in range(maxiter):\n        x1 = x[:, idx_excl]\n        mom_cond = self.ranking_measure(res_pen, x1, keep=keep)\n        assert len(mom_cond) == len(idx_excl)\n        mcs = np.sort(mom_cond)[::-1]\n        idx_thr = min((self.k_max_add, k_current + self.k_add, len(mcs)))\n        threshold = mcs[idx_thr]\n        idx = np.concatenate((idx_nonzero, idx_excl[mom_cond > threshold]))\n        start_params2 = np.zeros(len(idx))\n        start_params2[:len(start_params)] = start_params\n        if self.use_weights:\n            weights = np.ones(len(idx))\n            weights[:k_keep] = 0\n            self.penal.weights = weights\n        mod_pen = model_class(endog, x[:, idx], penal=self.penal, pen_weight=self.pen_weight, **self.init_kwds)\n        res_pen = mod_pen.fit(method=method, start_params=start_params2, warn_convergence=False, skip_hessian=True, **fit_kwds)\n        keep = np.abs(res_pen.params) > self.threshold_trim\n        if keep.sum() > self.k_max_included:\n            thresh_params = np.sort(np.abs(res_pen.params))[-self.k_max_included]\n            keep2 = np.abs(res_pen.params) > thresh_params\n            keep = np.logical_and(keep, keep2)\n        keep[:k_keep] = True\n        idx_nonzero = idx[keep]\n        if disp:\n            print(keep)\n            print(idx_nonzero)\n        k_current = len(idx_nonzero)\n        start_params = res_pen.params[keep]\n        mask_excl = np.ones(k_vars, dtype=bool)\n        mask_excl[idx_nonzero] = False\n        idx_excl = np.nonzero(mask_excl)[0]\n        history['idx_nonzero'].append(idx_nonzero)\n        history['keep'].append(keep)\n        history['params_keep'].append(start_params)\n        history['idx_added'].append(idx)\n        if len(idx_nonzero) == len(idx_old) and (idx_nonzero == idx_old).all():\n            converged = True\n            break\n        idx_old = idx_nonzero\n    assert np.all(idx_nonzero[:k_keep] == np.arange(k_keep))\n    if self.use_weights:\n        weights = np.ones(len(idx_nonzero))\n        weights[:k_keep] = 0\n        penal = self._get_penal(weights=weights)\n    else:\n        penal = self.penal\n    mod_final = model_class(endog, x[:, idx_nonzero], penal=penal, pen_weight=self.pen_weight, **self.init_kwds)\n    res_final = mod_final.fit(method=method, start_params=start_params, warn_convergence=False, **fit_kwds)\n    xnames = ['var%4d' % ii for ii in idx_nonzero]\n    res_final.model.exog_names[k_keep:] = xnames[k_keep:]\n    res = ScreeningResults(self, results_pen=res_pen, results_final=res_final, idx_nonzero=idx_nonzero, idx_exog=idx_nonzero[k_keep:] - k_keep, idx_excl=idx_excl, history=history, converged=converged, iterations=it + 1)\n    return res",
            "def screen_exog(self, exog, endog=None, maxiter=100, method='bfgs', disp=False, fit_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'screen and select variables (columns) in exog\\n\\n        Parameters\\n        ----------\\n        exog : ndarray\\n            candidate explanatory variables that are screened for inclusion in\\n            the model\\n        endog : ndarray (optional)\\n            use a new endog in the screening model.\\n            This is not tested yet, and might not work correctly\\n        maxiter : int\\n            number of screening iterations\\n        method : str\\n            optimization method to use in fit, needs to be only of the gradient\\n            optimizers\\n        disp : bool\\n            display option for fit during optimization\\n\\n        Returns\\n        -------\\n        res_screen : instance of ScreeningResults\\n            The attribute `results_final` contains is the results instance\\n            with the final model selection.\\n            `idx_nonzero` contains the index of the selected exog in the full\\n            exog, combined exog that are always kept plust exog_candidates.\\n            see ScreeningResults for a full description\\n        '\n    model_class = self.model_class\n    if endog is None:\n        endog = self.endog\n    x0 = self.exog_keep\n    k_keep = self.k_keep\n    x1 = exog\n    k_current = x0.shape[1]\n    x = np.column_stack((x0, x1))\n    (nobs, k_vars) = x.shape\n    fkwds = fit_kwds if fit_kwds is not None else {}\n    fit_kwds = {'maxiter': 200, 'disp': False}\n    fit_kwds.update(fkwds)\n    history = defaultdict(list)\n    idx_nonzero = np.arange(k_keep, dtype=int)\n    keep = np.ones(k_keep, np.bool_)\n    idx_excl = np.arange(k_keep, k_vars)\n    mod_pen = model_class(endog, x0, **self.init_kwds)\n    mod_pen.pen_weight = 0\n    res_pen = mod_pen.fit(**fit_kwds)\n    start_params = res_pen.params\n    converged = False\n    idx_old = []\n    for it in range(maxiter):\n        x1 = x[:, idx_excl]\n        mom_cond = self.ranking_measure(res_pen, x1, keep=keep)\n        assert len(mom_cond) == len(idx_excl)\n        mcs = np.sort(mom_cond)[::-1]\n        idx_thr = min((self.k_max_add, k_current + self.k_add, len(mcs)))\n        threshold = mcs[idx_thr]\n        idx = np.concatenate((idx_nonzero, idx_excl[mom_cond > threshold]))\n        start_params2 = np.zeros(len(idx))\n        start_params2[:len(start_params)] = start_params\n        if self.use_weights:\n            weights = np.ones(len(idx))\n            weights[:k_keep] = 0\n            self.penal.weights = weights\n        mod_pen = model_class(endog, x[:, idx], penal=self.penal, pen_weight=self.pen_weight, **self.init_kwds)\n        res_pen = mod_pen.fit(method=method, start_params=start_params2, warn_convergence=False, skip_hessian=True, **fit_kwds)\n        keep = np.abs(res_pen.params) > self.threshold_trim\n        if keep.sum() > self.k_max_included:\n            thresh_params = np.sort(np.abs(res_pen.params))[-self.k_max_included]\n            keep2 = np.abs(res_pen.params) > thresh_params\n            keep = np.logical_and(keep, keep2)\n        keep[:k_keep] = True\n        idx_nonzero = idx[keep]\n        if disp:\n            print(keep)\n            print(idx_nonzero)\n        k_current = len(idx_nonzero)\n        start_params = res_pen.params[keep]\n        mask_excl = np.ones(k_vars, dtype=bool)\n        mask_excl[idx_nonzero] = False\n        idx_excl = np.nonzero(mask_excl)[0]\n        history['idx_nonzero'].append(idx_nonzero)\n        history['keep'].append(keep)\n        history['params_keep'].append(start_params)\n        history['idx_added'].append(idx)\n        if len(idx_nonzero) == len(idx_old) and (idx_nonzero == idx_old).all():\n            converged = True\n            break\n        idx_old = idx_nonzero\n    assert np.all(idx_nonzero[:k_keep] == np.arange(k_keep))\n    if self.use_weights:\n        weights = np.ones(len(idx_nonzero))\n        weights[:k_keep] = 0\n        penal = self._get_penal(weights=weights)\n    else:\n        penal = self.penal\n    mod_final = model_class(endog, x[:, idx_nonzero], penal=penal, pen_weight=self.pen_weight, **self.init_kwds)\n    res_final = mod_final.fit(method=method, start_params=start_params, warn_convergence=False, **fit_kwds)\n    xnames = ['var%4d' % ii for ii in idx_nonzero]\n    res_final.model.exog_names[k_keep:] = xnames[k_keep:]\n    res = ScreeningResults(self, results_pen=res_pen, results_final=res_final, idx_nonzero=idx_nonzero, idx_exog=idx_nonzero[k_keep:] - k_keep, idx_excl=idx_excl, history=history, converged=converged, iterations=it + 1)\n    return res",
            "def screen_exog(self, exog, endog=None, maxiter=100, method='bfgs', disp=False, fit_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'screen and select variables (columns) in exog\\n\\n        Parameters\\n        ----------\\n        exog : ndarray\\n            candidate explanatory variables that are screened for inclusion in\\n            the model\\n        endog : ndarray (optional)\\n            use a new endog in the screening model.\\n            This is not tested yet, and might not work correctly\\n        maxiter : int\\n            number of screening iterations\\n        method : str\\n            optimization method to use in fit, needs to be only of the gradient\\n            optimizers\\n        disp : bool\\n            display option for fit during optimization\\n\\n        Returns\\n        -------\\n        res_screen : instance of ScreeningResults\\n            The attribute `results_final` contains is the results instance\\n            with the final model selection.\\n            `idx_nonzero` contains the index of the selected exog in the full\\n            exog, combined exog that are always kept plust exog_candidates.\\n            see ScreeningResults for a full description\\n        '\n    model_class = self.model_class\n    if endog is None:\n        endog = self.endog\n    x0 = self.exog_keep\n    k_keep = self.k_keep\n    x1 = exog\n    k_current = x0.shape[1]\n    x = np.column_stack((x0, x1))\n    (nobs, k_vars) = x.shape\n    fkwds = fit_kwds if fit_kwds is not None else {}\n    fit_kwds = {'maxiter': 200, 'disp': False}\n    fit_kwds.update(fkwds)\n    history = defaultdict(list)\n    idx_nonzero = np.arange(k_keep, dtype=int)\n    keep = np.ones(k_keep, np.bool_)\n    idx_excl = np.arange(k_keep, k_vars)\n    mod_pen = model_class(endog, x0, **self.init_kwds)\n    mod_pen.pen_weight = 0\n    res_pen = mod_pen.fit(**fit_kwds)\n    start_params = res_pen.params\n    converged = False\n    idx_old = []\n    for it in range(maxiter):\n        x1 = x[:, idx_excl]\n        mom_cond = self.ranking_measure(res_pen, x1, keep=keep)\n        assert len(mom_cond) == len(idx_excl)\n        mcs = np.sort(mom_cond)[::-1]\n        idx_thr = min((self.k_max_add, k_current + self.k_add, len(mcs)))\n        threshold = mcs[idx_thr]\n        idx = np.concatenate((idx_nonzero, idx_excl[mom_cond > threshold]))\n        start_params2 = np.zeros(len(idx))\n        start_params2[:len(start_params)] = start_params\n        if self.use_weights:\n            weights = np.ones(len(idx))\n            weights[:k_keep] = 0\n            self.penal.weights = weights\n        mod_pen = model_class(endog, x[:, idx], penal=self.penal, pen_weight=self.pen_weight, **self.init_kwds)\n        res_pen = mod_pen.fit(method=method, start_params=start_params2, warn_convergence=False, skip_hessian=True, **fit_kwds)\n        keep = np.abs(res_pen.params) > self.threshold_trim\n        if keep.sum() > self.k_max_included:\n            thresh_params = np.sort(np.abs(res_pen.params))[-self.k_max_included]\n            keep2 = np.abs(res_pen.params) > thresh_params\n            keep = np.logical_and(keep, keep2)\n        keep[:k_keep] = True\n        idx_nonzero = idx[keep]\n        if disp:\n            print(keep)\n            print(idx_nonzero)\n        k_current = len(idx_nonzero)\n        start_params = res_pen.params[keep]\n        mask_excl = np.ones(k_vars, dtype=bool)\n        mask_excl[idx_nonzero] = False\n        idx_excl = np.nonzero(mask_excl)[0]\n        history['idx_nonzero'].append(idx_nonzero)\n        history['keep'].append(keep)\n        history['params_keep'].append(start_params)\n        history['idx_added'].append(idx)\n        if len(idx_nonzero) == len(idx_old) and (idx_nonzero == idx_old).all():\n            converged = True\n            break\n        idx_old = idx_nonzero\n    assert np.all(idx_nonzero[:k_keep] == np.arange(k_keep))\n    if self.use_weights:\n        weights = np.ones(len(idx_nonzero))\n        weights[:k_keep] = 0\n        penal = self._get_penal(weights=weights)\n    else:\n        penal = self.penal\n    mod_final = model_class(endog, x[:, idx_nonzero], penal=penal, pen_weight=self.pen_weight, **self.init_kwds)\n    res_final = mod_final.fit(method=method, start_params=start_params, warn_convergence=False, **fit_kwds)\n    xnames = ['var%4d' % ii for ii in idx_nonzero]\n    res_final.model.exog_names[k_keep:] = xnames[k_keep:]\n    res = ScreeningResults(self, results_pen=res_pen, results_final=res_final, idx_nonzero=idx_nonzero, idx_exog=idx_nonzero[k_keep:] - k_keep, idx_excl=idx_excl, history=history, converged=converged, iterations=it + 1)\n    return res",
            "def screen_exog(self, exog, endog=None, maxiter=100, method='bfgs', disp=False, fit_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'screen and select variables (columns) in exog\\n\\n        Parameters\\n        ----------\\n        exog : ndarray\\n            candidate explanatory variables that are screened for inclusion in\\n            the model\\n        endog : ndarray (optional)\\n            use a new endog in the screening model.\\n            This is not tested yet, and might not work correctly\\n        maxiter : int\\n            number of screening iterations\\n        method : str\\n            optimization method to use in fit, needs to be only of the gradient\\n            optimizers\\n        disp : bool\\n            display option for fit during optimization\\n\\n        Returns\\n        -------\\n        res_screen : instance of ScreeningResults\\n            The attribute `results_final` contains is the results instance\\n            with the final model selection.\\n            `idx_nonzero` contains the index of the selected exog in the full\\n            exog, combined exog that are always kept plust exog_candidates.\\n            see ScreeningResults for a full description\\n        '\n    model_class = self.model_class\n    if endog is None:\n        endog = self.endog\n    x0 = self.exog_keep\n    k_keep = self.k_keep\n    x1 = exog\n    k_current = x0.shape[1]\n    x = np.column_stack((x0, x1))\n    (nobs, k_vars) = x.shape\n    fkwds = fit_kwds if fit_kwds is not None else {}\n    fit_kwds = {'maxiter': 200, 'disp': False}\n    fit_kwds.update(fkwds)\n    history = defaultdict(list)\n    idx_nonzero = np.arange(k_keep, dtype=int)\n    keep = np.ones(k_keep, np.bool_)\n    idx_excl = np.arange(k_keep, k_vars)\n    mod_pen = model_class(endog, x0, **self.init_kwds)\n    mod_pen.pen_weight = 0\n    res_pen = mod_pen.fit(**fit_kwds)\n    start_params = res_pen.params\n    converged = False\n    idx_old = []\n    for it in range(maxiter):\n        x1 = x[:, idx_excl]\n        mom_cond = self.ranking_measure(res_pen, x1, keep=keep)\n        assert len(mom_cond) == len(idx_excl)\n        mcs = np.sort(mom_cond)[::-1]\n        idx_thr = min((self.k_max_add, k_current + self.k_add, len(mcs)))\n        threshold = mcs[idx_thr]\n        idx = np.concatenate((idx_nonzero, idx_excl[mom_cond > threshold]))\n        start_params2 = np.zeros(len(idx))\n        start_params2[:len(start_params)] = start_params\n        if self.use_weights:\n            weights = np.ones(len(idx))\n            weights[:k_keep] = 0\n            self.penal.weights = weights\n        mod_pen = model_class(endog, x[:, idx], penal=self.penal, pen_weight=self.pen_weight, **self.init_kwds)\n        res_pen = mod_pen.fit(method=method, start_params=start_params2, warn_convergence=False, skip_hessian=True, **fit_kwds)\n        keep = np.abs(res_pen.params) > self.threshold_trim\n        if keep.sum() > self.k_max_included:\n            thresh_params = np.sort(np.abs(res_pen.params))[-self.k_max_included]\n            keep2 = np.abs(res_pen.params) > thresh_params\n            keep = np.logical_and(keep, keep2)\n        keep[:k_keep] = True\n        idx_nonzero = idx[keep]\n        if disp:\n            print(keep)\n            print(idx_nonzero)\n        k_current = len(idx_nonzero)\n        start_params = res_pen.params[keep]\n        mask_excl = np.ones(k_vars, dtype=bool)\n        mask_excl[idx_nonzero] = False\n        idx_excl = np.nonzero(mask_excl)[0]\n        history['idx_nonzero'].append(idx_nonzero)\n        history['keep'].append(keep)\n        history['params_keep'].append(start_params)\n        history['idx_added'].append(idx)\n        if len(idx_nonzero) == len(idx_old) and (idx_nonzero == idx_old).all():\n            converged = True\n            break\n        idx_old = idx_nonzero\n    assert np.all(idx_nonzero[:k_keep] == np.arange(k_keep))\n    if self.use_weights:\n        weights = np.ones(len(idx_nonzero))\n        weights[:k_keep] = 0\n        penal = self._get_penal(weights=weights)\n    else:\n        penal = self.penal\n    mod_final = model_class(endog, x[:, idx_nonzero], penal=penal, pen_weight=self.pen_weight, **self.init_kwds)\n    res_final = mod_final.fit(method=method, start_params=start_params, warn_convergence=False, **fit_kwds)\n    xnames = ['var%4d' % ii for ii in idx_nonzero]\n    res_final.model.exog_names[k_keep:] = xnames[k_keep:]\n    res = ScreeningResults(self, results_pen=res_pen, results_final=res_final, idx_nonzero=idx_nonzero, idx_exog=idx_nonzero[k_keep:] - k_keep, idx_excl=idx_excl, history=history, converged=converged, iterations=it + 1)\n    return res"
        ]
    },
    {
        "func_name": "screen_exog_iterator",
        "original": "def screen_exog_iterator(self, exog_iterator):\n    \"\"\"\n        batched version of screen exog\n\n        This screens variables in a two step process:\n\n        In the first step screen_exog is used on each element of the\n        exog_iterator, and the batch winners are collected.\n\n        In the second step all batch winners are combined into a new array\n        of exog candidates and `screen_exog` is used to select a final\n        model.\n\n        Parameters\n        ----------\n        exog_iterator : iterator over ndarrays\n\n        Returns\n        -------\n        res_screen_final : instance of ScreeningResults\n            This is the instance returned by the second round call to\n            `screen_exog`. Additional attributes are added to provide\n            more information about the batched selection process.\n            The index of final nonzero variables is\n            `idx_nonzero_batches` which is a 2-dimensional array with batch\n            index in the first column and variable index within batch in the\n            second column. They can be used jointly as index for the data\n            in the exog_iterator.\n            see ScreeningResults for a full description\n        \"\"\"\n    k_keep = self.k_keep\n    res_idx = []\n    exog_winner = []\n    exog_idx = []\n    for ex in exog_iterator:\n        res_screen = self.screen_exog(ex, maxiter=20)\n        res_idx.append(res_screen.idx_nonzero)\n        exog_winner.append(ex[:, res_screen.idx_nonzero[k_keep:] - k_keep])\n        exog_idx.append(res_screen.idx_nonzero[k_keep:] - k_keep)\n    exog_winner = np.column_stack(exog_winner)\n    res_screen_final = self.screen_exog(exog_winner, maxiter=20)\n    exog_winner_names = ['var%d_%d' % (bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    idx_full = [(bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    ex_final_idx = res_screen_final.idx_nonzero[k_keep:] - k_keep\n    final_names = np.array(exog_winner_names)[ex_final_idx]\n    res_screen_final.idx_nonzero_batches = np.array(idx_full)[ex_final_idx]\n    res_screen_final.exog_final_names = final_names\n    history = {'idx_nonzero': res_idx, 'idx_exog': exog_idx}\n    res_screen_final.history_batches = history\n    return res_screen_final",
        "mutated": [
            "def screen_exog_iterator(self, exog_iterator):\n    if False:\n        i = 10\n    '\\n        batched version of screen exog\\n\\n        This screens variables in a two step process:\\n\\n        In the first step screen_exog is used on each element of the\\n        exog_iterator, and the batch winners are collected.\\n\\n        In the second step all batch winners are combined into a new array\\n        of exog candidates and `screen_exog` is used to select a final\\n        model.\\n\\n        Parameters\\n        ----------\\n        exog_iterator : iterator over ndarrays\\n\\n        Returns\\n        -------\\n        res_screen_final : instance of ScreeningResults\\n            This is the instance returned by the second round call to\\n            `screen_exog`. Additional attributes are added to provide\\n            more information about the batched selection process.\\n            The index of final nonzero variables is\\n            `idx_nonzero_batches` which is a 2-dimensional array with batch\\n            index in the first column and variable index within batch in the\\n            second column. They can be used jointly as index for the data\\n            in the exog_iterator.\\n            see ScreeningResults for a full description\\n        '\n    k_keep = self.k_keep\n    res_idx = []\n    exog_winner = []\n    exog_idx = []\n    for ex in exog_iterator:\n        res_screen = self.screen_exog(ex, maxiter=20)\n        res_idx.append(res_screen.idx_nonzero)\n        exog_winner.append(ex[:, res_screen.idx_nonzero[k_keep:] - k_keep])\n        exog_idx.append(res_screen.idx_nonzero[k_keep:] - k_keep)\n    exog_winner = np.column_stack(exog_winner)\n    res_screen_final = self.screen_exog(exog_winner, maxiter=20)\n    exog_winner_names = ['var%d_%d' % (bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    idx_full = [(bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    ex_final_idx = res_screen_final.idx_nonzero[k_keep:] - k_keep\n    final_names = np.array(exog_winner_names)[ex_final_idx]\n    res_screen_final.idx_nonzero_batches = np.array(idx_full)[ex_final_idx]\n    res_screen_final.exog_final_names = final_names\n    history = {'idx_nonzero': res_idx, 'idx_exog': exog_idx}\n    res_screen_final.history_batches = history\n    return res_screen_final",
            "def screen_exog_iterator(self, exog_iterator):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        batched version of screen exog\\n\\n        This screens variables in a two step process:\\n\\n        In the first step screen_exog is used on each element of the\\n        exog_iterator, and the batch winners are collected.\\n\\n        In the second step all batch winners are combined into a new array\\n        of exog candidates and `screen_exog` is used to select a final\\n        model.\\n\\n        Parameters\\n        ----------\\n        exog_iterator : iterator over ndarrays\\n\\n        Returns\\n        -------\\n        res_screen_final : instance of ScreeningResults\\n            This is the instance returned by the second round call to\\n            `screen_exog`. Additional attributes are added to provide\\n            more information about the batched selection process.\\n            The index of final nonzero variables is\\n            `idx_nonzero_batches` which is a 2-dimensional array with batch\\n            index in the first column and variable index within batch in the\\n            second column. They can be used jointly as index for the data\\n            in the exog_iterator.\\n            see ScreeningResults for a full description\\n        '\n    k_keep = self.k_keep\n    res_idx = []\n    exog_winner = []\n    exog_idx = []\n    for ex in exog_iterator:\n        res_screen = self.screen_exog(ex, maxiter=20)\n        res_idx.append(res_screen.idx_nonzero)\n        exog_winner.append(ex[:, res_screen.idx_nonzero[k_keep:] - k_keep])\n        exog_idx.append(res_screen.idx_nonzero[k_keep:] - k_keep)\n    exog_winner = np.column_stack(exog_winner)\n    res_screen_final = self.screen_exog(exog_winner, maxiter=20)\n    exog_winner_names = ['var%d_%d' % (bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    idx_full = [(bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    ex_final_idx = res_screen_final.idx_nonzero[k_keep:] - k_keep\n    final_names = np.array(exog_winner_names)[ex_final_idx]\n    res_screen_final.idx_nonzero_batches = np.array(idx_full)[ex_final_idx]\n    res_screen_final.exog_final_names = final_names\n    history = {'idx_nonzero': res_idx, 'idx_exog': exog_idx}\n    res_screen_final.history_batches = history\n    return res_screen_final",
            "def screen_exog_iterator(self, exog_iterator):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        batched version of screen exog\\n\\n        This screens variables in a two step process:\\n\\n        In the first step screen_exog is used on each element of the\\n        exog_iterator, and the batch winners are collected.\\n\\n        In the second step all batch winners are combined into a new array\\n        of exog candidates and `screen_exog` is used to select a final\\n        model.\\n\\n        Parameters\\n        ----------\\n        exog_iterator : iterator over ndarrays\\n\\n        Returns\\n        -------\\n        res_screen_final : instance of ScreeningResults\\n            This is the instance returned by the second round call to\\n            `screen_exog`. Additional attributes are added to provide\\n            more information about the batched selection process.\\n            The index of final nonzero variables is\\n            `idx_nonzero_batches` which is a 2-dimensional array with batch\\n            index in the first column and variable index within batch in the\\n            second column. They can be used jointly as index for the data\\n            in the exog_iterator.\\n            see ScreeningResults for a full description\\n        '\n    k_keep = self.k_keep\n    res_idx = []\n    exog_winner = []\n    exog_idx = []\n    for ex in exog_iterator:\n        res_screen = self.screen_exog(ex, maxiter=20)\n        res_idx.append(res_screen.idx_nonzero)\n        exog_winner.append(ex[:, res_screen.idx_nonzero[k_keep:] - k_keep])\n        exog_idx.append(res_screen.idx_nonzero[k_keep:] - k_keep)\n    exog_winner = np.column_stack(exog_winner)\n    res_screen_final = self.screen_exog(exog_winner, maxiter=20)\n    exog_winner_names = ['var%d_%d' % (bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    idx_full = [(bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    ex_final_idx = res_screen_final.idx_nonzero[k_keep:] - k_keep\n    final_names = np.array(exog_winner_names)[ex_final_idx]\n    res_screen_final.idx_nonzero_batches = np.array(idx_full)[ex_final_idx]\n    res_screen_final.exog_final_names = final_names\n    history = {'idx_nonzero': res_idx, 'idx_exog': exog_idx}\n    res_screen_final.history_batches = history\n    return res_screen_final",
            "def screen_exog_iterator(self, exog_iterator):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        batched version of screen exog\\n\\n        This screens variables in a two step process:\\n\\n        In the first step screen_exog is used on each element of the\\n        exog_iterator, and the batch winners are collected.\\n\\n        In the second step all batch winners are combined into a new array\\n        of exog candidates and `screen_exog` is used to select a final\\n        model.\\n\\n        Parameters\\n        ----------\\n        exog_iterator : iterator over ndarrays\\n\\n        Returns\\n        -------\\n        res_screen_final : instance of ScreeningResults\\n            This is the instance returned by the second round call to\\n            `screen_exog`. Additional attributes are added to provide\\n            more information about the batched selection process.\\n            The index of final nonzero variables is\\n            `idx_nonzero_batches` which is a 2-dimensional array with batch\\n            index in the first column and variable index within batch in the\\n            second column. They can be used jointly as index for the data\\n            in the exog_iterator.\\n            see ScreeningResults for a full description\\n        '\n    k_keep = self.k_keep\n    res_idx = []\n    exog_winner = []\n    exog_idx = []\n    for ex in exog_iterator:\n        res_screen = self.screen_exog(ex, maxiter=20)\n        res_idx.append(res_screen.idx_nonzero)\n        exog_winner.append(ex[:, res_screen.idx_nonzero[k_keep:] - k_keep])\n        exog_idx.append(res_screen.idx_nonzero[k_keep:] - k_keep)\n    exog_winner = np.column_stack(exog_winner)\n    res_screen_final = self.screen_exog(exog_winner, maxiter=20)\n    exog_winner_names = ['var%d_%d' % (bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    idx_full = [(bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    ex_final_idx = res_screen_final.idx_nonzero[k_keep:] - k_keep\n    final_names = np.array(exog_winner_names)[ex_final_idx]\n    res_screen_final.idx_nonzero_batches = np.array(idx_full)[ex_final_idx]\n    res_screen_final.exog_final_names = final_names\n    history = {'idx_nonzero': res_idx, 'idx_exog': exog_idx}\n    res_screen_final.history_batches = history\n    return res_screen_final",
            "def screen_exog_iterator(self, exog_iterator):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        batched version of screen exog\\n\\n        This screens variables in a two step process:\\n\\n        In the first step screen_exog is used on each element of the\\n        exog_iterator, and the batch winners are collected.\\n\\n        In the second step all batch winners are combined into a new array\\n        of exog candidates and `screen_exog` is used to select a final\\n        model.\\n\\n        Parameters\\n        ----------\\n        exog_iterator : iterator over ndarrays\\n\\n        Returns\\n        -------\\n        res_screen_final : instance of ScreeningResults\\n            This is the instance returned by the second round call to\\n            `screen_exog`. Additional attributes are added to provide\\n            more information about the batched selection process.\\n            The index of final nonzero variables is\\n            `idx_nonzero_batches` which is a 2-dimensional array with batch\\n            index in the first column and variable index within batch in the\\n            second column. They can be used jointly as index for the data\\n            in the exog_iterator.\\n            see ScreeningResults for a full description\\n        '\n    k_keep = self.k_keep\n    res_idx = []\n    exog_winner = []\n    exog_idx = []\n    for ex in exog_iterator:\n        res_screen = self.screen_exog(ex, maxiter=20)\n        res_idx.append(res_screen.idx_nonzero)\n        exog_winner.append(ex[:, res_screen.idx_nonzero[k_keep:] - k_keep])\n        exog_idx.append(res_screen.idx_nonzero[k_keep:] - k_keep)\n    exog_winner = np.column_stack(exog_winner)\n    res_screen_final = self.screen_exog(exog_winner, maxiter=20)\n    exog_winner_names = ['var%d_%d' % (bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    idx_full = [(bidx, idx) for (bidx, batch) in enumerate(exog_idx) for idx in batch]\n    ex_final_idx = res_screen_final.idx_nonzero[k_keep:] - k_keep\n    final_names = np.array(exog_winner_names)[ex_final_idx]\n    res_screen_final.idx_nonzero_batches = np.array(idx_full)[ex_final_idx]\n    res_screen_final.exog_final_names = final_names\n    history = {'idx_nonzero': res_idx, 'idx_exog': exog_idx}\n    res_screen_final.history_batches = history\n    return res_screen_final"
        ]
    }
]