[
    {
        "func_name": "pytest_generate_tests",
        "original": "def pytest_generate_tests(metafunc):\n    bsz_rng = [1]\n    if 'refgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3, 4]\n            inp_rng = [3, 5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('refgruargs', fargs)\n    if 'gradgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3]\n            inp_rng = [5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('gradgruargs', fargs)",
        "mutated": [
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n    bsz_rng = [1]\n    if 'refgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3, 4]\n            inp_rng = [3, 5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('refgruargs', fargs)\n    if 'gradgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3]\n            inp_rng = [5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('gradgruargs', fargs)",
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bsz_rng = [1]\n    if 'refgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3, 4]\n            inp_rng = [3, 5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('refgruargs', fargs)\n    if 'gradgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3]\n            inp_rng = [5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('gradgruargs', fargs)",
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bsz_rng = [1]\n    if 'refgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3, 4]\n            inp_rng = [3, 5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('refgruargs', fargs)\n    if 'gradgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3]\n            inp_rng = [5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('gradgruargs', fargs)",
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bsz_rng = [1]\n    if 'refgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3, 4]\n            inp_rng = [3, 5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('refgruargs', fargs)\n    if 'gradgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3]\n            inp_rng = [5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('gradgruargs', fargs)",
            "def pytest_generate_tests(metafunc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bsz_rng = [1]\n    if 'refgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3, 4]\n            inp_rng = [3, 5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('refgruargs', fargs)\n    if 'gradgruargs' in metafunc.fixturenames:\n        fargs = []\n        if metafunc.config.option.all:\n            seq_rng = [2, 3]\n            inp_rng = [5, 10]\n            out_rng = [3, 5, 10]\n        else:\n            seq_rng = [3]\n            inp_rng = [5]\n            out_rng = [10]\n        fargs = itt.product(seq_rng, inp_rng, out_rng, bsz_rng)\n        metafunc.parametrize('gradgruargs', fargs)"
        ]
    },
    {
        "func_name": "test_ref_compare_ones",
        "original": "def test_ref_compare_ones(backend_default, refgruargs):\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Constant(val=1.0), [1.0, 0.0])",
        "mutated": [
            "def test_ref_compare_ones(backend_default, refgruargs):\n    if False:\n        i = 10\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Constant(val=1.0), [1.0, 0.0])",
            "def test_ref_compare_ones(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Constant(val=1.0), [1.0, 0.0])",
            "def test_ref_compare_ones(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Constant(val=1.0), [1.0, 0.0])",
            "def test_ref_compare_ones(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Constant(val=1.0), [1.0, 0.0])",
            "def test_ref_compare_ones(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Constant(val=1.0), [1.0, 0.0])"
        ]
    },
    {
        "func_name": "test_ref_compare_rand",
        "original": "def test_ref_compare_rand(backend_default, refgruargs):\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian())",
        "mutated": [
            "def test_ref_compare_rand(backend_default, refgruargs):\n    if False:\n        i = 10\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian())",
            "def test_ref_compare_rand(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian())",
            "def test_ref_compare_rand(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian())",
            "def test_ref_compare_rand(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian())",
            "def test_ref_compare_rand(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian())"
        ]
    },
    {
        "func_name": "test_ref_compare_rand_init_state",
        "original": "def test_ref_compare_rand_init_state(backend_default, refgruargs):\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian(), add_init_state=True)",
        "mutated": [
            "def test_ref_compare_rand_init_state(backend_default, refgruargs):\n    if False:\n        i = 10\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian(), add_init_state=True)",
            "def test_ref_compare_rand_init_state(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian(), add_init_state=True)",
            "def test_ref_compare_rand_init_state(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian(), add_init_state=True)",
            "def test_ref_compare_rand_init_state(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian(), add_init_state=True)",
            "def test_ref_compare_rand_init_state(backend_default, refgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (seq_len, input_size, hidden_size, batch_size) = refgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    check_gru(seq_len, input_size, hidden_size, batch_size, Gaussian(), add_init_state=True)"
        ]
    },
    {
        "func_name": "check_gru",
        "original": "def check_gru(seq_len, input_size, hidden_size, batch_size, init_func, inp_moms=[0.0, 1.0], add_init_state=False):\n    input_shape = (input_size, seq_len * batch_size)\n    output_shape = (hidden_size, seq_len * batch_size)\n    slice_shape = (hidden_size, batch_size)\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gru = GRU(hidden_size, init_func, activation=Tanh(), gate_activation=Logistic())\n    inp = np.random.rand(*input_shape) * inp_moms[1] + inp_moms[0]\n    inp_dev = gru.be.array(inp)\n    deltas = np.random.randn(*output_shape)\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state:\n        init_state = np.random.rand(*slice_shape) * inp_moms[1] + inp_moms[0]\n        init_state_dev = gru.be.array(init_state)\n        gru.fprop(inp_dev, init_state=init_state_dev)\n    else:\n        gru.fprop(inp_dev)\n    gru_ref = RefGRU(input_size, hidden_size)\n    WGRU = gru_ref.weights\n    r_range = list(range(hidden_size))\n    z_range = list(range(hidden_size, hidden_size * 2))\n    c_range = list(range(hidden_size * 2, hidden_size * 3))\n    WGRU[gru_ref.weights_ind_br][:] = gru.b.get()[r_range]\n    WGRU[gru_ref.weights_ind_bz][:] = gru.b.get()[z_range]\n    WGRU[gru_ref.weights_ind_bc][:] = gru.b.get()[c_range]\n    WGRU[gru_ref.weights_ind_Wxr][:] = gru.W_input.get()[r_range]\n    WGRU[gru_ref.weights_ind_Wxz][:] = gru.W_input.get()[z_range]\n    WGRU[gru_ref.weights_ind_Wxc][:] = gru.W_input.get()[c_range]\n    WGRU[gru_ref.weights_ind_Rhr][:] = gru.W_recur.get()[r_range]\n    WGRU[gru_ref.weights_ind_Rhz][:] = gru.W_recur.get()[z_range]\n    WGRU[gru_ref.weights_ind_Rhc][:] = gru.W_recur.get()[c_range]\n    inp_ref = inp.copy().T.reshape(seq_len, batch_size, input_size).swapaxes(1, 2)\n    deltas_ref = deltas.copy().T.reshape(seq_len, batch_size, hidden_size).swapaxes(1, 2)\n    if add_init_state:\n        init_state_ref = init_state.copy()\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref, init_state_ref)\n    else:\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref)\n    neon_logger.display('====Verifying hidden states====')\n    assert allclose_with_out(gru.outputs.get(), h_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('fprop is verified')\n    neon_logger.display('Making sure neon GRU matches numpy GRU in bprop')\n    gru.bprop(gru.be.array(deltas))\n    dWinput_neon = gru.dW_input.get()\n    dWrecur_neon = gru.dW_recur.get()\n    db_neon = gru.db.get()\n    dWxr_neon = dWinput_neon[r_range]\n    dWxz_neon = dWinput_neon[z_range]\n    dWxc_neon = dWinput_neon[c_range]\n    dWrr_neon = dWrecur_neon[r_range]\n    dWrz_neon = dWrecur_neon[z_range]\n    dWrc_neon = dWrecur_neon[c_range]\n    dbr_neon = db_neon[r_range]\n    dbz_neon = db_neon[z_range]\n    dbc_neon = db_neon[c_range]\n    drzc_neon = gru.rzhcan_delta_buffer.get()\n    dr_neon = drzc_neon[r_range]\n    dz_neon = drzc_neon[z_range]\n    dc_neon = drzc_neon[c_range]\n    dWxr_ref = dWGRU_ref[gru_ref.dW_ind_Wxr]\n    dWxz_ref = dWGRU_ref[gru_ref.dW_ind_Wxz]\n    dWxc_ref = dWGRU_ref[gru_ref.dW_ind_Wxc]\n    dWrr_ref = dWGRU_ref[gru_ref.dW_ind_Rhr]\n    dWrz_ref = dWGRU_ref[gru_ref.dW_ind_Rhz]\n    dWrc_ref = dWGRU_ref[gru_ref.dW_ind_Rhc]\n    dbr_ref = dWGRU_ref[gru_ref.dW_ind_br]\n    dbz_ref = dWGRU_ref[gru_ref.dW_ind_bz]\n    dbc_ref = dWGRU_ref[gru_ref.dW_ind_bc]\n    neon_logger.display('====Verifying r deltas ====')\n    assert allclose_with_out(dr_neon, dr_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying z deltas ====')\n    assert allclose_with_out(dz_neon, dz_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying hcan deltas ====')\n    assert allclose_with_out(dc_neon, dc_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_input====')\n    neon_logger.display('dWxr')\n    assert allclose_with_out(dWxr_neon, dWxr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxz')\n    assert allclose_with_out(dWxz_neon, dWxz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxc')\n    assert allclose_with_out(dWxc_neon, dWxc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_recur====')\n    neon_logger.display('dWrr')\n    assert allclose_with_out(dWrr_neon, dWrr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrz')\n    assert allclose_with_out(dWrz_neon, dWrz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrc')\n    assert allclose_with_out(dWrc_neon, dWrc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on bias====')\n    neon_logger.display('dbr')\n    assert allclose_with_out(dbr_neon, dbr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbz')\n    assert allclose_with_out(dbz_neon, dbz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbc')\n    assert allclose_with_out(dbc_neon, dbc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('bprop is verified')\n    return",
        "mutated": [
            "def check_gru(seq_len, input_size, hidden_size, batch_size, init_func, inp_moms=[0.0, 1.0], add_init_state=False):\n    if False:\n        i = 10\n    input_shape = (input_size, seq_len * batch_size)\n    output_shape = (hidden_size, seq_len * batch_size)\n    slice_shape = (hidden_size, batch_size)\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gru = GRU(hidden_size, init_func, activation=Tanh(), gate_activation=Logistic())\n    inp = np.random.rand(*input_shape) * inp_moms[1] + inp_moms[0]\n    inp_dev = gru.be.array(inp)\n    deltas = np.random.randn(*output_shape)\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state:\n        init_state = np.random.rand(*slice_shape) * inp_moms[1] + inp_moms[0]\n        init_state_dev = gru.be.array(init_state)\n        gru.fprop(inp_dev, init_state=init_state_dev)\n    else:\n        gru.fprop(inp_dev)\n    gru_ref = RefGRU(input_size, hidden_size)\n    WGRU = gru_ref.weights\n    r_range = list(range(hidden_size))\n    z_range = list(range(hidden_size, hidden_size * 2))\n    c_range = list(range(hidden_size * 2, hidden_size * 3))\n    WGRU[gru_ref.weights_ind_br][:] = gru.b.get()[r_range]\n    WGRU[gru_ref.weights_ind_bz][:] = gru.b.get()[z_range]\n    WGRU[gru_ref.weights_ind_bc][:] = gru.b.get()[c_range]\n    WGRU[gru_ref.weights_ind_Wxr][:] = gru.W_input.get()[r_range]\n    WGRU[gru_ref.weights_ind_Wxz][:] = gru.W_input.get()[z_range]\n    WGRU[gru_ref.weights_ind_Wxc][:] = gru.W_input.get()[c_range]\n    WGRU[gru_ref.weights_ind_Rhr][:] = gru.W_recur.get()[r_range]\n    WGRU[gru_ref.weights_ind_Rhz][:] = gru.W_recur.get()[z_range]\n    WGRU[gru_ref.weights_ind_Rhc][:] = gru.W_recur.get()[c_range]\n    inp_ref = inp.copy().T.reshape(seq_len, batch_size, input_size).swapaxes(1, 2)\n    deltas_ref = deltas.copy().T.reshape(seq_len, batch_size, hidden_size).swapaxes(1, 2)\n    if add_init_state:\n        init_state_ref = init_state.copy()\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref, init_state_ref)\n    else:\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref)\n    neon_logger.display('====Verifying hidden states====')\n    assert allclose_with_out(gru.outputs.get(), h_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('fprop is verified')\n    neon_logger.display('Making sure neon GRU matches numpy GRU in bprop')\n    gru.bprop(gru.be.array(deltas))\n    dWinput_neon = gru.dW_input.get()\n    dWrecur_neon = gru.dW_recur.get()\n    db_neon = gru.db.get()\n    dWxr_neon = dWinput_neon[r_range]\n    dWxz_neon = dWinput_neon[z_range]\n    dWxc_neon = dWinput_neon[c_range]\n    dWrr_neon = dWrecur_neon[r_range]\n    dWrz_neon = dWrecur_neon[z_range]\n    dWrc_neon = dWrecur_neon[c_range]\n    dbr_neon = db_neon[r_range]\n    dbz_neon = db_neon[z_range]\n    dbc_neon = db_neon[c_range]\n    drzc_neon = gru.rzhcan_delta_buffer.get()\n    dr_neon = drzc_neon[r_range]\n    dz_neon = drzc_neon[z_range]\n    dc_neon = drzc_neon[c_range]\n    dWxr_ref = dWGRU_ref[gru_ref.dW_ind_Wxr]\n    dWxz_ref = dWGRU_ref[gru_ref.dW_ind_Wxz]\n    dWxc_ref = dWGRU_ref[gru_ref.dW_ind_Wxc]\n    dWrr_ref = dWGRU_ref[gru_ref.dW_ind_Rhr]\n    dWrz_ref = dWGRU_ref[gru_ref.dW_ind_Rhz]\n    dWrc_ref = dWGRU_ref[gru_ref.dW_ind_Rhc]\n    dbr_ref = dWGRU_ref[gru_ref.dW_ind_br]\n    dbz_ref = dWGRU_ref[gru_ref.dW_ind_bz]\n    dbc_ref = dWGRU_ref[gru_ref.dW_ind_bc]\n    neon_logger.display('====Verifying r deltas ====')\n    assert allclose_with_out(dr_neon, dr_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying z deltas ====')\n    assert allclose_with_out(dz_neon, dz_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying hcan deltas ====')\n    assert allclose_with_out(dc_neon, dc_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_input====')\n    neon_logger.display('dWxr')\n    assert allclose_with_out(dWxr_neon, dWxr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxz')\n    assert allclose_with_out(dWxz_neon, dWxz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxc')\n    assert allclose_with_out(dWxc_neon, dWxc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_recur====')\n    neon_logger.display('dWrr')\n    assert allclose_with_out(dWrr_neon, dWrr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrz')\n    assert allclose_with_out(dWrz_neon, dWrz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrc')\n    assert allclose_with_out(dWrc_neon, dWrc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on bias====')\n    neon_logger.display('dbr')\n    assert allclose_with_out(dbr_neon, dbr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbz')\n    assert allclose_with_out(dbz_neon, dbz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbc')\n    assert allclose_with_out(dbc_neon, dbc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('bprop is verified')\n    return",
            "def check_gru(seq_len, input_size, hidden_size, batch_size, init_func, inp_moms=[0.0, 1.0], add_init_state=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_shape = (input_size, seq_len * batch_size)\n    output_shape = (hidden_size, seq_len * batch_size)\n    slice_shape = (hidden_size, batch_size)\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gru = GRU(hidden_size, init_func, activation=Tanh(), gate_activation=Logistic())\n    inp = np.random.rand(*input_shape) * inp_moms[1] + inp_moms[0]\n    inp_dev = gru.be.array(inp)\n    deltas = np.random.randn(*output_shape)\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state:\n        init_state = np.random.rand(*slice_shape) * inp_moms[1] + inp_moms[0]\n        init_state_dev = gru.be.array(init_state)\n        gru.fprop(inp_dev, init_state=init_state_dev)\n    else:\n        gru.fprop(inp_dev)\n    gru_ref = RefGRU(input_size, hidden_size)\n    WGRU = gru_ref.weights\n    r_range = list(range(hidden_size))\n    z_range = list(range(hidden_size, hidden_size * 2))\n    c_range = list(range(hidden_size * 2, hidden_size * 3))\n    WGRU[gru_ref.weights_ind_br][:] = gru.b.get()[r_range]\n    WGRU[gru_ref.weights_ind_bz][:] = gru.b.get()[z_range]\n    WGRU[gru_ref.weights_ind_bc][:] = gru.b.get()[c_range]\n    WGRU[gru_ref.weights_ind_Wxr][:] = gru.W_input.get()[r_range]\n    WGRU[gru_ref.weights_ind_Wxz][:] = gru.W_input.get()[z_range]\n    WGRU[gru_ref.weights_ind_Wxc][:] = gru.W_input.get()[c_range]\n    WGRU[gru_ref.weights_ind_Rhr][:] = gru.W_recur.get()[r_range]\n    WGRU[gru_ref.weights_ind_Rhz][:] = gru.W_recur.get()[z_range]\n    WGRU[gru_ref.weights_ind_Rhc][:] = gru.W_recur.get()[c_range]\n    inp_ref = inp.copy().T.reshape(seq_len, batch_size, input_size).swapaxes(1, 2)\n    deltas_ref = deltas.copy().T.reshape(seq_len, batch_size, hidden_size).swapaxes(1, 2)\n    if add_init_state:\n        init_state_ref = init_state.copy()\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref, init_state_ref)\n    else:\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref)\n    neon_logger.display('====Verifying hidden states====')\n    assert allclose_with_out(gru.outputs.get(), h_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('fprop is verified')\n    neon_logger.display('Making sure neon GRU matches numpy GRU in bprop')\n    gru.bprop(gru.be.array(deltas))\n    dWinput_neon = gru.dW_input.get()\n    dWrecur_neon = gru.dW_recur.get()\n    db_neon = gru.db.get()\n    dWxr_neon = dWinput_neon[r_range]\n    dWxz_neon = dWinput_neon[z_range]\n    dWxc_neon = dWinput_neon[c_range]\n    dWrr_neon = dWrecur_neon[r_range]\n    dWrz_neon = dWrecur_neon[z_range]\n    dWrc_neon = dWrecur_neon[c_range]\n    dbr_neon = db_neon[r_range]\n    dbz_neon = db_neon[z_range]\n    dbc_neon = db_neon[c_range]\n    drzc_neon = gru.rzhcan_delta_buffer.get()\n    dr_neon = drzc_neon[r_range]\n    dz_neon = drzc_neon[z_range]\n    dc_neon = drzc_neon[c_range]\n    dWxr_ref = dWGRU_ref[gru_ref.dW_ind_Wxr]\n    dWxz_ref = dWGRU_ref[gru_ref.dW_ind_Wxz]\n    dWxc_ref = dWGRU_ref[gru_ref.dW_ind_Wxc]\n    dWrr_ref = dWGRU_ref[gru_ref.dW_ind_Rhr]\n    dWrz_ref = dWGRU_ref[gru_ref.dW_ind_Rhz]\n    dWrc_ref = dWGRU_ref[gru_ref.dW_ind_Rhc]\n    dbr_ref = dWGRU_ref[gru_ref.dW_ind_br]\n    dbz_ref = dWGRU_ref[gru_ref.dW_ind_bz]\n    dbc_ref = dWGRU_ref[gru_ref.dW_ind_bc]\n    neon_logger.display('====Verifying r deltas ====')\n    assert allclose_with_out(dr_neon, dr_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying z deltas ====')\n    assert allclose_with_out(dz_neon, dz_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying hcan deltas ====')\n    assert allclose_with_out(dc_neon, dc_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_input====')\n    neon_logger.display('dWxr')\n    assert allclose_with_out(dWxr_neon, dWxr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxz')\n    assert allclose_with_out(dWxz_neon, dWxz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxc')\n    assert allclose_with_out(dWxc_neon, dWxc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_recur====')\n    neon_logger.display('dWrr')\n    assert allclose_with_out(dWrr_neon, dWrr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrz')\n    assert allclose_with_out(dWrz_neon, dWrz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrc')\n    assert allclose_with_out(dWrc_neon, dWrc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on bias====')\n    neon_logger.display('dbr')\n    assert allclose_with_out(dbr_neon, dbr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbz')\n    assert allclose_with_out(dbz_neon, dbz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbc')\n    assert allclose_with_out(dbc_neon, dbc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('bprop is verified')\n    return",
            "def check_gru(seq_len, input_size, hidden_size, batch_size, init_func, inp_moms=[0.0, 1.0], add_init_state=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_shape = (input_size, seq_len * batch_size)\n    output_shape = (hidden_size, seq_len * batch_size)\n    slice_shape = (hidden_size, batch_size)\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gru = GRU(hidden_size, init_func, activation=Tanh(), gate_activation=Logistic())\n    inp = np.random.rand(*input_shape) * inp_moms[1] + inp_moms[0]\n    inp_dev = gru.be.array(inp)\n    deltas = np.random.randn(*output_shape)\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state:\n        init_state = np.random.rand(*slice_shape) * inp_moms[1] + inp_moms[0]\n        init_state_dev = gru.be.array(init_state)\n        gru.fprop(inp_dev, init_state=init_state_dev)\n    else:\n        gru.fprop(inp_dev)\n    gru_ref = RefGRU(input_size, hidden_size)\n    WGRU = gru_ref.weights\n    r_range = list(range(hidden_size))\n    z_range = list(range(hidden_size, hidden_size * 2))\n    c_range = list(range(hidden_size * 2, hidden_size * 3))\n    WGRU[gru_ref.weights_ind_br][:] = gru.b.get()[r_range]\n    WGRU[gru_ref.weights_ind_bz][:] = gru.b.get()[z_range]\n    WGRU[gru_ref.weights_ind_bc][:] = gru.b.get()[c_range]\n    WGRU[gru_ref.weights_ind_Wxr][:] = gru.W_input.get()[r_range]\n    WGRU[gru_ref.weights_ind_Wxz][:] = gru.W_input.get()[z_range]\n    WGRU[gru_ref.weights_ind_Wxc][:] = gru.W_input.get()[c_range]\n    WGRU[gru_ref.weights_ind_Rhr][:] = gru.W_recur.get()[r_range]\n    WGRU[gru_ref.weights_ind_Rhz][:] = gru.W_recur.get()[z_range]\n    WGRU[gru_ref.weights_ind_Rhc][:] = gru.W_recur.get()[c_range]\n    inp_ref = inp.copy().T.reshape(seq_len, batch_size, input_size).swapaxes(1, 2)\n    deltas_ref = deltas.copy().T.reshape(seq_len, batch_size, hidden_size).swapaxes(1, 2)\n    if add_init_state:\n        init_state_ref = init_state.copy()\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref, init_state_ref)\n    else:\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref)\n    neon_logger.display('====Verifying hidden states====')\n    assert allclose_with_out(gru.outputs.get(), h_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('fprop is verified')\n    neon_logger.display('Making sure neon GRU matches numpy GRU in bprop')\n    gru.bprop(gru.be.array(deltas))\n    dWinput_neon = gru.dW_input.get()\n    dWrecur_neon = gru.dW_recur.get()\n    db_neon = gru.db.get()\n    dWxr_neon = dWinput_neon[r_range]\n    dWxz_neon = dWinput_neon[z_range]\n    dWxc_neon = dWinput_neon[c_range]\n    dWrr_neon = dWrecur_neon[r_range]\n    dWrz_neon = dWrecur_neon[z_range]\n    dWrc_neon = dWrecur_neon[c_range]\n    dbr_neon = db_neon[r_range]\n    dbz_neon = db_neon[z_range]\n    dbc_neon = db_neon[c_range]\n    drzc_neon = gru.rzhcan_delta_buffer.get()\n    dr_neon = drzc_neon[r_range]\n    dz_neon = drzc_neon[z_range]\n    dc_neon = drzc_neon[c_range]\n    dWxr_ref = dWGRU_ref[gru_ref.dW_ind_Wxr]\n    dWxz_ref = dWGRU_ref[gru_ref.dW_ind_Wxz]\n    dWxc_ref = dWGRU_ref[gru_ref.dW_ind_Wxc]\n    dWrr_ref = dWGRU_ref[gru_ref.dW_ind_Rhr]\n    dWrz_ref = dWGRU_ref[gru_ref.dW_ind_Rhz]\n    dWrc_ref = dWGRU_ref[gru_ref.dW_ind_Rhc]\n    dbr_ref = dWGRU_ref[gru_ref.dW_ind_br]\n    dbz_ref = dWGRU_ref[gru_ref.dW_ind_bz]\n    dbc_ref = dWGRU_ref[gru_ref.dW_ind_bc]\n    neon_logger.display('====Verifying r deltas ====')\n    assert allclose_with_out(dr_neon, dr_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying z deltas ====')\n    assert allclose_with_out(dz_neon, dz_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying hcan deltas ====')\n    assert allclose_with_out(dc_neon, dc_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_input====')\n    neon_logger.display('dWxr')\n    assert allclose_with_out(dWxr_neon, dWxr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxz')\n    assert allclose_with_out(dWxz_neon, dWxz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxc')\n    assert allclose_with_out(dWxc_neon, dWxc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_recur====')\n    neon_logger.display('dWrr')\n    assert allclose_with_out(dWrr_neon, dWrr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrz')\n    assert allclose_with_out(dWrz_neon, dWrz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrc')\n    assert allclose_with_out(dWrc_neon, dWrc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on bias====')\n    neon_logger.display('dbr')\n    assert allclose_with_out(dbr_neon, dbr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbz')\n    assert allclose_with_out(dbz_neon, dbz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbc')\n    assert allclose_with_out(dbc_neon, dbc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('bprop is verified')\n    return",
            "def check_gru(seq_len, input_size, hidden_size, batch_size, init_func, inp_moms=[0.0, 1.0], add_init_state=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_shape = (input_size, seq_len * batch_size)\n    output_shape = (hidden_size, seq_len * batch_size)\n    slice_shape = (hidden_size, batch_size)\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gru = GRU(hidden_size, init_func, activation=Tanh(), gate_activation=Logistic())\n    inp = np.random.rand(*input_shape) * inp_moms[1] + inp_moms[0]\n    inp_dev = gru.be.array(inp)\n    deltas = np.random.randn(*output_shape)\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state:\n        init_state = np.random.rand(*slice_shape) * inp_moms[1] + inp_moms[0]\n        init_state_dev = gru.be.array(init_state)\n        gru.fprop(inp_dev, init_state=init_state_dev)\n    else:\n        gru.fprop(inp_dev)\n    gru_ref = RefGRU(input_size, hidden_size)\n    WGRU = gru_ref.weights\n    r_range = list(range(hidden_size))\n    z_range = list(range(hidden_size, hidden_size * 2))\n    c_range = list(range(hidden_size * 2, hidden_size * 3))\n    WGRU[gru_ref.weights_ind_br][:] = gru.b.get()[r_range]\n    WGRU[gru_ref.weights_ind_bz][:] = gru.b.get()[z_range]\n    WGRU[gru_ref.weights_ind_bc][:] = gru.b.get()[c_range]\n    WGRU[gru_ref.weights_ind_Wxr][:] = gru.W_input.get()[r_range]\n    WGRU[gru_ref.weights_ind_Wxz][:] = gru.W_input.get()[z_range]\n    WGRU[gru_ref.weights_ind_Wxc][:] = gru.W_input.get()[c_range]\n    WGRU[gru_ref.weights_ind_Rhr][:] = gru.W_recur.get()[r_range]\n    WGRU[gru_ref.weights_ind_Rhz][:] = gru.W_recur.get()[z_range]\n    WGRU[gru_ref.weights_ind_Rhc][:] = gru.W_recur.get()[c_range]\n    inp_ref = inp.copy().T.reshape(seq_len, batch_size, input_size).swapaxes(1, 2)\n    deltas_ref = deltas.copy().T.reshape(seq_len, batch_size, hidden_size).swapaxes(1, 2)\n    if add_init_state:\n        init_state_ref = init_state.copy()\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref, init_state_ref)\n    else:\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref)\n    neon_logger.display('====Verifying hidden states====')\n    assert allclose_with_out(gru.outputs.get(), h_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('fprop is verified')\n    neon_logger.display('Making sure neon GRU matches numpy GRU in bprop')\n    gru.bprop(gru.be.array(deltas))\n    dWinput_neon = gru.dW_input.get()\n    dWrecur_neon = gru.dW_recur.get()\n    db_neon = gru.db.get()\n    dWxr_neon = dWinput_neon[r_range]\n    dWxz_neon = dWinput_neon[z_range]\n    dWxc_neon = dWinput_neon[c_range]\n    dWrr_neon = dWrecur_neon[r_range]\n    dWrz_neon = dWrecur_neon[z_range]\n    dWrc_neon = dWrecur_neon[c_range]\n    dbr_neon = db_neon[r_range]\n    dbz_neon = db_neon[z_range]\n    dbc_neon = db_neon[c_range]\n    drzc_neon = gru.rzhcan_delta_buffer.get()\n    dr_neon = drzc_neon[r_range]\n    dz_neon = drzc_neon[z_range]\n    dc_neon = drzc_neon[c_range]\n    dWxr_ref = dWGRU_ref[gru_ref.dW_ind_Wxr]\n    dWxz_ref = dWGRU_ref[gru_ref.dW_ind_Wxz]\n    dWxc_ref = dWGRU_ref[gru_ref.dW_ind_Wxc]\n    dWrr_ref = dWGRU_ref[gru_ref.dW_ind_Rhr]\n    dWrz_ref = dWGRU_ref[gru_ref.dW_ind_Rhz]\n    dWrc_ref = dWGRU_ref[gru_ref.dW_ind_Rhc]\n    dbr_ref = dWGRU_ref[gru_ref.dW_ind_br]\n    dbz_ref = dWGRU_ref[gru_ref.dW_ind_bz]\n    dbc_ref = dWGRU_ref[gru_ref.dW_ind_bc]\n    neon_logger.display('====Verifying r deltas ====')\n    assert allclose_with_out(dr_neon, dr_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying z deltas ====')\n    assert allclose_with_out(dz_neon, dz_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying hcan deltas ====')\n    assert allclose_with_out(dc_neon, dc_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_input====')\n    neon_logger.display('dWxr')\n    assert allclose_with_out(dWxr_neon, dWxr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxz')\n    assert allclose_with_out(dWxz_neon, dWxz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxc')\n    assert allclose_with_out(dWxc_neon, dWxc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_recur====')\n    neon_logger.display('dWrr')\n    assert allclose_with_out(dWrr_neon, dWrr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrz')\n    assert allclose_with_out(dWrz_neon, dWrz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrc')\n    assert allclose_with_out(dWrc_neon, dWrc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on bias====')\n    neon_logger.display('dbr')\n    assert allclose_with_out(dbr_neon, dbr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbz')\n    assert allclose_with_out(dbz_neon, dbz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbc')\n    assert allclose_with_out(dbc_neon, dbc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('bprop is verified')\n    return",
            "def check_gru(seq_len, input_size, hidden_size, batch_size, init_func, inp_moms=[0.0, 1.0], add_init_state=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_shape = (input_size, seq_len * batch_size)\n    output_shape = (hidden_size, seq_len * batch_size)\n    slice_shape = (hidden_size, batch_size)\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gru = GRU(hidden_size, init_func, activation=Tanh(), gate_activation=Logistic())\n    inp = np.random.rand(*input_shape) * inp_moms[1] + inp_moms[0]\n    inp_dev = gru.be.array(inp)\n    deltas = np.random.randn(*output_shape)\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state:\n        init_state = np.random.rand(*slice_shape) * inp_moms[1] + inp_moms[0]\n        init_state_dev = gru.be.array(init_state)\n        gru.fprop(inp_dev, init_state=init_state_dev)\n    else:\n        gru.fprop(inp_dev)\n    gru_ref = RefGRU(input_size, hidden_size)\n    WGRU = gru_ref.weights\n    r_range = list(range(hidden_size))\n    z_range = list(range(hidden_size, hidden_size * 2))\n    c_range = list(range(hidden_size * 2, hidden_size * 3))\n    WGRU[gru_ref.weights_ind_br][:] = gru.b.get()[r_range]\n    WGRU[gru_ref.weights_ind_bz][:] = gru.b.get()[z_range]\n    WGRU[gru_ref.weights_ind_bc][:] = gru.b.get()[c_range]\n    WGRU[gru_ref.weights_ind_Wxr][:] = gru.W_input.get()[r_range]\n    WGRU[gru_ref.weights_ind_Wxz][:] = gru.W_input.get()[z_range]\n    WGRU[gru_ref.weights_ind_Wxc][:] = gru.W_input.get()[c_range]\n    WGRU[gru_ref.weights_ind_Rhr][:] = gru.W_recur.get()[r_range]\n    WGRU[gru_ref.weights_ind_Rhz][:] = gru.W_recur.get()[z_range]\n    WGRU[gru_ref.weights_ind_Rhc][:] = gru.W_recur.get()[c_range]\n    inp_ref = inp.copy().T.reshape(seq_len, batch_size, input_size).swapaxes(1, 2)\n    deltas_ref = deltas.copy().T.reshape(seq_len, batch_size, hidden_size).swapaxes(1, 2)\n    if add_init_state:\n        init_state_ref = init_state.copy()\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref, init_state_ref)\n    else:\n        (dWGRU_ref, h_ref_list, dh_ref_list, dr_ref_list, dz_ref_list, dc_ref_list) = gru_ref.lossFun(inp_ref, deltas_ref)\n    neon_logger.display('====Verifying hidden states====')\n    assert allclose_with_out(gru.outputs.get(), h_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('fprop is verified')\n    neon_logger.display('Making sure neon GRU matches numpy GRU in bprop')\n    gru.bprop(gru.be.array(deltas))\n    dWinput_neon = gru.dW_input.get()\n    dWrecur_neon = gru.dW_recur.get()\n    db_neon = gru.db.get()\n    dWxr_neon = dWinput_neon[r_range]\n    dWxz_neon = dWinput_neon[z_range]\n    dWxc_neon = dWinput_neon[c_range]\n    dWrr_neon = dWrecur_neon[r_range]\n    dWrz_neon = dWrecur_neon[z_range]\n    dWrc_neon = dWrecur_neon[c_range]\n    dbr_neon = db_neon[r_range]\n    dbz_neon = db_neon[z_range]\n    dbc_neon = db_neon[c_range]\n    drzc_neon = gru.rzhcan_delta_buffer.get()\n    dr_neon = drzc_neon[r_range]\n    dz_neon = drzc_neon[z_range]\n    dc_neon = drzc_neon[c_range]\n    dWxr_ref = dWGRU_ref[gru_ref.dW_ind_Wxr]\n    dWxz_ref = dWGRU_ref[gru_ref.dW_ind_Wxz]\n    dWxc_ref = dWGRU_ref[gru_ref.dW_ind_Wxc]\n    dWrr_ref = dWGRU_ref[gru_ref.dW_ind_Rhr]\n    dWrz_ref = dWGRU_ref[gru_ref.dW_ind_Rhz]\n    dWrc_ref = dWGRU_ref[gru_ref.dW_ind_Rhc]\n    dbr_ref = dWGRU_ref[gru_ref.dW_ind_br]\n    dbz_ref = dWGRU_ref[gru_ref.dW_ind_bz]\n    dbc_ref = dWGRU_ref[gru_ref.dW_ind_bc]\n    neon_logger.display('====Verifying r deltas ====')\n    assert allclose_with_out(dr_neon, dr_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying z deltas ====')\n    assert allclose_with_out(dz_neon, dz_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying hcan deltas ====')\n    assert allclose_with_out(dc_neon, dc_ref_list, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_input====')\n    neon_logger.display('dWxr')\n    assert allclose_with_out(dWxr_neon, dWxr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxz')\n    assert allclose_with_out(dWxz_neon, dWxz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWxc')\n    assert allclose_with_out(dWxc_neon, dWxc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on W_recur====')\n    neon_logger.display('dWrr')\n    assert allclose_with_out(dWrr_neon, dWrr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrz')\n    assert allclose_with_out(dWrz_neon, dWrz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dWrc')\n    assert allclose_with_out(dWrc_neon, dWrc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('====Verifying update on bias====')\n    neon_logger.display('dbr')\n    assert allclose_with_out(dbr_neon, dbr_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbz')\n    assert allclose_with_out(dbz_neon, dbz_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('dbc')\n    assert allclose_with_out(dbc_neon, dbc_ref, rtol=0.0, atol=1e-05)\n    neon_logger.display('bprop is verified')\n    return"
        ]
    },
    {
        "func_name": "reset_gru",
        "original": "def reset_gru(gru):\n    gru.x = None\n    gru.xs = None\n    gru.outputs = None\n    return",
        "mutated": [
            "def reset_gru(gru):\n    if False:\n        i = 10\n    gru.x = None\n    gru.xs = None\n    gru.outputs = None\n    return",
            "def reset_gru(gru):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    gru.x = None\n    gru.xs = None\n    gru.outputs = None\n    return",
            "def reset_gru(gru):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    gru.x = None\n    gru.xs = None\n    gru.outputs = None\n    return",
            "def reset_gru(gru):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    gru.x = None\n    gru.xs = None\n    gru.outputs = None\n    return",
            "def reset_gru(gru):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    gru.x = None\n    gru.xs = None\n    gru.outputs = None\n    return"
        ]
    },
    {
        "func_name": "test_gradient_neon_gru",
        "original": "def test_gradient_neon_gru(backend_default, gradgruargs):\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size)",
        "mutated": [
            "def test_gradient_neon_gru(backend_default, gradgruargs):\n    if False:\n        i = 10\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size)",
            "def test_gradient_neon_gru(backend_default, gradgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size)",
            "def test_gradient_neon_gru(backend_default, gradgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size)",
            "def test_gradient_neon_gru(backend_default, gradgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size)",
            "def test_gradient_neon_gru(backend_default, gradgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size)"
        ]
    },
    {
        "func_name": "test_gradient_neon_gru_init_state",
        "original": "def test_gradient_neon_gru_init_state(backend_default, gradgruargs):\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size, True)",
        "mutated": [
            "def test_gradient_neon_gru_init_state(backend_default, gradgruargs):\n    if False:\n        i = 10\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size, True)",
            "def test_gradient_neon_gru_init_state(backend_default, gradgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size, True)",
            "def test_gradient_neon_gru_init_state(backend_default, gradgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size, True)",
            "def test_gradient_neon_gru_init_state(backend_default, gradgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size, True)",
            "def test_gradient_neon_gru_init_state(backend_default, gradgruargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (seq_len, input_size, hidden_size, batch_size) = gradgruargs\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    gradient_check(seq_len, input_size, hidden_size, batch_size, True)"
        ]
    },
    {
        "func_name": "gradient_check",
        "original": "def gradient_check(seq_len, input_size, hidden_size, batch_size, add_init_state=False, threshold=0.001):\n    min_max_err = -1.0\n    neon_logger.display('Perturb mag, max grad diff')\n    for pert_exp in range(-5, 0):\n        input_shape = (input_size, seq_len * batch_size)\n        output_shape = (hidden_size, seq_len * batch_size)\n        rand_scale = np.random.random(output_shape) * 2.0 - 1.0\n        inp = np.random.randn(*input_shape)\n        pert_mag = 10.0 ** pert_exp\n        (grad_est, deltas) = gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=add_init_state, epsilon=pert_mag, rand_scale=rand_scale, inp_bl=inp)\n        dd = np.max(np.abs(grad_est - deltas))\n        neon_logger.display('%e, %e' % (pert_mag, dd))\n        if min_max_err < 0.0 or dd < min_max_err:\n            min_max_err = dd\n        NervanaObject.be.rng_reset()\n    neon_logger.display('Worst case error %e with perturbation %e' % (min_max_err, pert_mag))\n    neon_logger.display('Threshold %e' % threshold)\n    assert min_max_err < threshold",
        "mutated": [
            "def gradient_check(seq_len, input_size, hidden_size, batch_size, add_init_state=False, threshold=0.001):\n    if False:\n        i = 10\n    min_max_err = -1.0\n    neon_logger.display('Perturb mag, max grad diff')\n    for pert_exp in range(-5, 0):\n        input_shape = (input_size, seq_len * batch_size)\n        output_shape = (hidden_size, seq_len * batch_size)\n        rand_scale = np.random.random(output_shape) * 2.0 - 1.0\n        inp = np.random.randn(*input_shape)\n        pert_mag = 10.0 ** pert_exp\n        (grad_est, deltas) = gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=add_init_state, epsilon=pert_mag, rand_scale=rand_scale, inp_bl=inp)\n        dd = np.max(np.abs(grad_est - deltas))\n        neon_logger.display('%e, %e' % (pert_mag, dd))\n        if min_max_err < 0.0 or dd < min_max_err:\n            min_max_err = dd\n        NervanaObject.be.rng_reset()\n    neon_logger.display('Worst case error %e with perturbation %e' % (min_max_err, pert_mag))\n    neon_logger.display('Threshold %e' % threshold)\n    assert min_max_err < threshold",
            "def gradient_check(seq_len, input_size, hidden_size, batch_size, add_init_state=False, threshold=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    min_max_err = -1.0\n    neon_logger.display('Perturb mag, max grad diff')\n    for pert_exp in range(-5, 0):\n        input_shape = (input_size, seq_len * batch_size)\n        output_shape = (hidden_size, seq_len * batch_size)\n        rand_scale = np.random.random(output_shape) * 2.0 - 1.0\n        inp = np.random.randn(*input_shape)\n        pert_mag = 10.0 ** pert_exp\n        (grad_est, deltas) = gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=add_init_state, epsilon=pert_mag, rand_scale=rand_scale, inp_bl=inp)\n        dd = np.max(np.abs(grad_est - deltas))\n        neon_logger.display('%e, %e' % (pert_mag, dd))\n        if min_max_err < 0.0 or dd < min_max_err:\n            min_max_err = dd\n        NervanaObject.be.rng_reset()\n    neon_logger.display('Worst case error %e with perturbation %e' % (min_max_err, pert_mag))\n    neon_logger.display('Threshold %e' % threshold)\n    assert min_max_err < threshold",
            "def gradient_check(seq_len, input_size, hidden_size, batch_size, add_init_state=False, threshold=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    min_max_err = -1.0\n    neon_logger.display('Perturb mag, max grad diff')\n    for pert_exp in range(-5, 0):\n        input_shape = (input_size, seq_len * batch_size)\n        output_shape = (hidden_size, seq_len * batch_size)\n        rand_scale = np.random.random(output_shape) * 2.0 - 1.0\n        inp = np.random.randn(*input_shape)\n        pert_mag = 10.0 ** pert_exp\n        (grad_est, deltas) = gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=add_init_state, epsilon=pert_mag, rand_scale=rand_scale, inp_bl=inp)\n        dd = np.max(np.abs(grad_est - deltas))\n        neon_logger.display('%e, %e' % (pert_mag, dd))\n        if min_max_err < 0.0 or dd < min_max_err:\n            min_max_err = dd\n        NervanaObject.be.rng_reset()\n    neon_logger.display('Worst case error %e with perturbation %e' % (min_max_err, pert_mag))\n    neon_logger.display('Threshold %e' % threshold)\n    assert min_max_err < threshold",
            "def gradient_check(seq_len, input_size, hidden_size, batch_size, add_init_state=False, threshold=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    min_max_err = -1.0\n    neon_logger.display('Perturb mag, max grad diff')\n    for pert_exp in range(-5, 0):\n        input_shape = (input_size, seq_len * batch_size)\n        output_shape = (hidden_size, seq_len * batch_size)\n        rand_scale = np.random.random(output_shape) * 2.0 - 1.0\n        inp = np.random.randn(*input_shape)\n        pert_mag = 10.0 ** pert_exp\n        (grad_est, deltas) = gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=add_init_state, epsilon=pert_mag, rand_scale=rand_scale, inp_bl=inp)\n        dd = np.max(np.abs(grad_est - deltas))\n        neon_logger.display('%e, %e' % (pert_mag, dd))\n        if min_max_err < 0.0 or dd < min_max_err:\n            min_max_err = dd\n        NervanaObject.be.rng_reset()\n    neon_logger.display('Worst case error %e with perturbation %e' % (min_max_err, pert_mag))\n    neon_logger.display('Threshold %e' % threshold)\n    assert min_max_err < threshold",
            "def gradient_check(seq_len, input_size, hidden_size, batch_size, add_init_state=False, threshold=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    min_max_err = -1.0\n    neon_logger.display('Perturb mag, max grad diff')\n    for pert_exp in range(-5, 0):\n        input_shape = (input_size, seq_len * batch_size)\n        output_shape = (hidden_size, seq_len * batch_size)\n        rand_scale = np.random.random(output_shape) * 2.0 - 1.0\n        inp = np.random.randn(*input_shape)\n        pert_mag = 10.0 ** pert_exp\n        (grad_est, deltas) = gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=add_init_state, epsilon=pert_mag, rand_scale=rand_scale, inp_bl=inp)\n        dd = np.max(np.abs(grad_est - deltas))\n        neon_logger.display('%e, %e' % (pert_mag, dd))\n        if min_max_err < 0.0 or dd < min_max_err:\n            min_max_err = dd\n        NervanaObject.be.rng_reset()\n    neon_logger.display('Worst case error %e with perturbation %e' % (min_max_err, pert_mag))\n    neon_logger.display('Threshold %e' % threshold)\n    assert min_max_err < threshold"
        ]
    },
    {
        "func_name": "gradient_calc",
        "original": "def gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=False, epsilon=None, rand_scale=None, inp_bl=None):\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    input_shape = (input_size, seq_len * batch_size)\n    if inp_bl is None:\n        inp_bl = np.random.randn(*input_shape)\n    gru = GRU(hidden_size, init=Gaussian(), activation=Tanh(), gate_activation=Logistic())\n    inpa = gru.be.array(np.copy(inp_bl))\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state is True:\n        slice_shape = (hidden_size, batch_size)\n        ini_s = np.random.randn(*slice_shape)\n        ini_s_dev = gru.be.array(ini_s.copy())\n        out_bl = gru.fprop(inpa, ini_s_dev).get()\n    else:\n        out_bl = gru.fprop(inpa).get()\n    if rand_scale is None:\n        rand_scale = np.random.random(out_bl.shape) * 2.0 - 1.0\n    deltas_neon = gru.bprop(gru.be.array(np.copy(rand_scale))).get()\n    grads_est = np.zeros(inpa.shape)\n    inp_pert = inp_bl.copy()\n    for pert_ind in range(inpa.size):\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_pos = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_pos = gru.fprop(gru.be.array(inp_pert)).get()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_neg = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_neg = gru.fprop(gru.be.array(inp_pert)).get()\n        loss_pos = np.sum(rand_scale * out_pos)\n        loss_neg = np.sum(rand_scale * out_neg)\n        grad = 0.5 / float(epsilon) * (loss_pos - loss_neg)\n        grads_est.flat[pert_ind] = grad\n        inp_pert.flat[pert_ind] = save_val\n    del gru\n    return (grads_est, deltas_neon)",
        "mutated": [
            "def gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=False, epsilon=None, rand_scale=None, inp_bl=None):\n    if False:\n        i = 10\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    input_shape = (input_size, seq_len * batch_size)\n    if inp_bl is None:\n        inp_bl = np.random.randn(*input_shape)\n    gru = GRU(hidden_size, init=Gaussian(), activation=Tanh(), gate_activation=Logistic())\n    inpa = gru.be.array(np.copy(inp_bl))\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state is True:\n        slice_shape = (hidden_size, batch_size)\n        ini_s = np.random.randn(*slice_shape)\n        ini_s_dev = gru.be.array(ini_s.copy())\n        out_bl = gru.fprop(inpa, ini_s_dev).get()\n    else:\n        out_bl = gru.fprop(inpa).get()\n    if rand_scale is None:\n        rand_scale = np.random.random(out_bl.shape) * 2.0 - 1.0\n    deltas_neon = gru.bprop(gru.be.array(np.copy(rand_scale))).get()\n    grads_est = np.zeros(inpa.shape)\n    inp_pert = inp_bl.copy()\n    for pert_ind in range(inpa.size):\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_pos = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_pos = gru.fprop(gru.be.array(inp_pert)).get()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_neg = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_neg = gru.fprop(gru.be.array(inp_pert)).get()\n        loss_pos = np.sum(rand_scale * out_pos)\n        loss_neg = np.sum(rand_scale * out_neg)\n        grad = 0.5 / float(epsilon) * (loss_pos - loss_neg)\n        grads_est.flat[pert_ind] = grad\n        inp_pert.flat[pert_ind] = save_val\n    del gru\n    return (grads_est, deltas_neon)",
            "def gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=False, epsilon=None, rand_scale=None, inp_bl=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    input_shape = (input_size, seq_len * batch_size)\n    if inp_bl is None:\n        inp_bl = np.random.randn(*input_shape)\n    gru = GRU(hidden_size, init=Gaussian(), activation=Tanh(), gate_activation=Logistic())\n    inpa = gru.be.array(np.copy(inp_bl))\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state is True:\n        slice_shape = (hidden_size, batch_size)\n        ini_s = np.random.randn(*slice_shape)\n        ini_s_dev = gru.be.array(ini_s.copy())\n        out_bl = gru.fprop(inpa, ini_s_dev).get()\n    else:\n        out_bl = gru.fprop(inpa).get()\n    if rand_scale is None:\n        rand_scale = np.random.random(out_bl.shape) * 2.0 - 1.0\n    deltas_neon = gru.bprop(gru.be.array(np.copy(rand_scale))).get()\n    grads_est = np.zeros(inpa.shape)\n    inp_pert = inp_bl.copy()\n    for pert_ind in range(inpa.size):\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_pos = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_pos = gru.fprop(gru.be.array(inp_pert)).get()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_neg = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_neg = gru.fprop(gru.be.array(inp_pert)).get()\n        loss_pos = np.sum(rand_scale * out_pos)\n        loss_neg = np.sum(rand_scale * out_neg)\n        grad = 0.5 / float(epsilon) * (loss_pos - loss_neg)\n        grads_est.flat[pert_ind] = grad\n        inp_pert.flat[pert_ind] = save_val\n    del gru\n    return (grads_est, deltas_neon)",
            "def gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=False, epsilon=None, rand_scale=None, inp_bl=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    input_shape = (input_size, seq_len * batch_size)\n    if inp_bl is None:\n        inp_bl = np.random.randn(*input_shape)\n    gru = GRU(hidden_size, init=Gaussian(), activation=Tanh(), gate_activation=Logistic())\n    inpa = gru.be.array(np.copy(inp_bl))\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state is True:\n        slice_shape = (hidden_size, batch_size)\n        ini_s = np.random.randn(*slice_shape)\n        ini_s_dev = gru.be.array(ini_s.copy())\n        out_bl = gru.fprop(inpa, ini_s_dev).get()\n    else:\n        out_bl = gru.fprop(inpa).get()\n    if rand_scale is None:\n        rand_scale = np.random.random(out_bl.shape) * 2.0 - 1.0\n    deltas_neon = gru.bprop(gru.be.array(np.copy(rand_scale))).get()\n    grads_est = np.zeros(inpa.shape)\n    inp_pert = inp_bl.copy()\n    for pert_ind in range(inpa.size):\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_pos = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_pos = gru.fprop(gru.be.array(inp_pert)).get()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_neg = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_neg = gru.fprop(gru.be.array(inp_pert)).get()\n        loss_pos = np.sum(rand_scale * out_pos)\n        loss_neg = np.sum(rand_scale * out_neg)\n        grad = 0.5 / float(epsilon) * (loss_pos - loss_neg)\n        grads_est.flat[pert_ind] = grad\n        inp_pert.flat[pert_ind] = save_val\n    del gru\n    return (grads_est, deltas_neon)",
            "def gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=False, epsilon=None, rand_scale=None, inp_bl=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    input_shape = (input_size, seq_len * batch_size)\n    if inp_bl is None:\n        inp_bl = np.random.randn(*input_shape)\n    gru = GRU(hidden_size, init=Gaussian(), activation=Tanh(), gate_activation=Logistic())\n    inpa = gru.be.array(np.copy(inp_bl))\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state is True:\n        slice_shape = (hidden_size, batch_size)\n        ini_s = np.random.randn(*slice_shape)\n        ini_s_dev = gru.be.array(ini_s.copy())\n        out_bl = gru.fprop(inpa, ini_s_dev).get()\n    else:\n        out_bl = gru.fprop(inpa).get()\n    if rand_scale is None:\n        rand_scale = np.random.random(out_bl.shape) * 2.0 - 1.0\n    deltas_neon = gru.bprop(gru.be.array(np.copy(rand_scale))).get()\n    grads_est = np.zeros(inpa.shape)\n    inp_pert = inp_bl.copy()\n    for pert_ind in range(inpa.size):\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_pos = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_pos = gru.fprop(gru.be.array(inp_pert)).get()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_neg = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_neg = gru.fprop(gru.be.array(inp_pert)).get()\n        loss_pos = np.sum(rand_scale * out_pos)\n        loss_neg = np.sum(rand_scale * out_neg)\n        grad = 0.5 / float(epsilon) * (loss_pos - loss_neg)\n        grads_est.flat[pert_ind] = grad\n        inp_pert.flat[pert_ind] = save_val\n    del gru\n    return (grads_est, deltas_neon)",
            "def gradient_calc(seq_len, input_size, hidden_size, batch_size, add_init_state=False, epsilon=None, rand_scale=None, inp_bl=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    NervanaObject.be.bsz = NervanaObject.be.batch_size = batch_size\n    input_shape = (input_size, seq_len * batch_size)\n    if inp_bl is None:\n        inp_bl = np.random.randn(*input_shape)\n    gru = GRU(hidden_size, init=Gaussian(), activation=Tanh(), gate_activation=Logistic())\n    inpa = gru.be.array(np.copy(inp_bl))\n    gru.configure((input_size, seq_len))\n    gru.prev_layer = True\n    gru.allocate()\n    test_buffer = DeltasTree()\n    gru.allocate_deltas(test_buffer)\n    test_buffer.allocate_buffers()\n    gru.set_deltas(test_buffer)\n    if add_init_state is True:\n        slice_shape = (hidden_size, batch_size)\n        ini_s = np.random.randn(*slice_shape)\n        ini_s_dev = gru.be.array(ini_s.copy())\n        out_bl = gru.fprop(inpa, ini_s_dev).get()\n    else:\n        out_bl = gru.fprop(inpa).get()\n    if rand_scale is None:\n        rand_scale = np.random.random(out_bl.shape) * 2.0 - 1.0\n    deltas_neon = gru.bprop(gru.be.array(np.copy(rand_scale))).get()\n    grads_est = np.zeros(inpa.shape)\n    inp_pert = inp_bl.copy()\n    for pert_ind in range(inpa.size):\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_pos = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_pos = gru.fprop(gru.be.array(inp_pert)).get()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        reset_gru(gru)\n        gru.allocate()\n        if add_init_state is True:\n            ini_s_dev = gru.be.array(ini_s.copy())\n            out_neg = gru.fprop(gru.be.array(inp_pert), ini_s_dev).get()\n        else:\n            out_neg = gru.fprop(gru.be.array(inp_pert)).get()\n        loss_pos = np.sum(rand_scale * out_pos)\n        loss_neg = np.sum(rand_scale * out_neg)\n        grad = 0.5 / float(epsilon) * (loss_pos - loss_neg)\n        grads_est.flat[pert_ind] = grad\n        inp_pert.flat[pert_ind] = save_val\n    del gru\n    return (grads_est, deltas_neon)"
        ]
    }
]