[
    {
        "func_name": "top_k_filter",
        "original": "def top_k_filter(logits, k, probs=False):\n    \"\"\"\n    Masks everything but the k top entries as -infinity (1e10).\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n    sum of the denominator.\n    \"\"\"\n    if k == 0:\n        return logits\n    else:\n        values = torch.topk(logits, k)[0]\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n        if probs:\n            return torch.where(logits < batch_mins, torch.ones_like(logits) * 0.0, logits)\n        return torch.where(logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits)",
        "mutated": [
            "def top_k_filter(logits, k, probs=False):\n    if False:\n        i = 10\n    \"\\n    Masks everything but the k top entries as -infinity (1e10).\\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\\n    sum of the denominator.\\n    \"\n    if k == 0:\n        return logits\n    else:\n        values = torch.topk(logits, k)[0]\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n        if probs:\n            return torch.where(logits < batch_mins, torch.ones_like(logits) * 0.0, logits)\n        return torch.where(logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits)",
            "def top_k_filter(logits, k, probs=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Masks everything but the k top entries as -infinity (1e10).\\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\\n    sum of the denominator.\\n    \"\n    if k == 0:\n        return logits\n    else:\n        values = torch.topk(logits, k)[0]\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n        if probs:\n            return torch.where(logits < batch_mins, torch.ones_like(logits) * 0.0, logits)\n        return torch.where(logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits)",
            "def top_k_filter(logits, k, probs=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Masks everything but the k top entries as -infinity (1e10).\\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\\n    sum of the denominator.\\n    \"\n    if k == 0:\n        return logits\n    else:\n        values = torch.topk(logits, k)[0]\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n        if probs:\n            return torch.where(logits < batch_mins, torch.ones_like(logits) * 0.0, logits)\n        return torch.where(logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits)",
            "def top_k_filter(logits, k, probs=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Masks everything but the k top entries as -infinity (1e10).\\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\\n    sum of the denominator.\\n    \"\n    if k == 0:\n        return logits\n    else:\n        values = torch.topk(logits, k)[0]\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n        if probs:\n            return torch.where(logits < batch_mins, torch.ones_like(logits) * 0.0, logits)\n        return torch.where(logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits)",
            "def top_k_filter(logits, k, probs=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Masks everything but the k top entries as -infinity (1e10).\\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\\n    sum of the denominator.\\n    \"\n    if k == 0:\n        return logits\n    else:\n        values = torch.topk(logits, k)[0]\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n        if probs:\n            return torch.where(logits < batch_mins, torch.ones_like(logits) * 0.0, logits)\n        return torch.where(logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits)"
        ]
    },
    {
        "func_name": "perturb_past",
        "original": "def perturb_past(past, model, last, unpert_past=None, unpert_logits=None, accumulated_hidden=None, grad_norms=None, stepsize=0.01, one_hot_bows_vectors=None, classifier=None, class_label=None, loss_type=0, num_iterations=3, horizon_length=1, window_length=0, decay=False, gamma=1.5, kl_scale=0.01, device='cuda'):\n    grad_accumulator = [np.zeros(p.shape).astype('float32') for p in past]\n    if accumulated_hidden is None:\n        accumulated_hidden = 0\n    if decay:\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / window_length)[1:]\n    else:\n        decay_mask = 1.0\n    (_, _, _, curr_length, _) = past[0].shape\n    if curr_length > window_length and window_length > 0:\n        ones_key_val_shape = tuple(past[0].shape[:-2]) + (window_length,) + tuple(past[0].shape[-1:])\n        zeros_key_val_shape = tuple(past[0].shape[:-2]) + (curr_length - window_length,) + tuple(past[0].shape[-1:])\n        ones_mask = torch.ones(ones_key_val_shape)\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n        window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\n    else:\n        window_mask = torch.ones_like(past[0]).to(device)\n    loss_per_iter = []\n    new_accumulated_hidden = None\n    for i in range(num_iterations):\n        print('Iteration ', i + 1)\n        curr_perturbation = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n        for p_ in curr_perturbation:\n            p_.retain_grad()\n        perturbed_past = list(map(add, past, curr_perturbation))\n        (_, _, _, curr_length, _) = curr_perturbation[0].shape\n        lm_output = model(last, past_key_values=perturbed_past)\n        (all_logits, all_hidden) = (lm_output['logits'], lm_output['hidden_states'])\n        hidden = all_hidden[-1]\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n        logits = all_logits[:, -1, :]\n        probs = nn.functional.softmax(logits, dim=-1)\n        loss = 0.0\n        loss_list = []\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n            for one_hot_bow in one_hot_bows_vectors:\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n                bow_loss = -torch.log(torch.sum(bow_logits))\n                loss += bow_loss\n                loss_list.append(bow_loss)\n            print(' pplm_bow_loss:', loss.data.cpu().numpy())\n        if loss_type == 2 or loss_type == 3:\n            ce_loss = nn.CrossEntropyLoss()\n            curr_unpert_past = unpert_past\n            curr_probs = torch.unsqueeze(probs, dim=1)\n            wte = model.resize_token_embeddings()\n            for _ in range(horizon_length):\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n                lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=inputs_embeds)\n                (curr_all_logits, curr_unpert_past, curr_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n                curr_logits = curr_all_logits[:, -1, :]\n                curr_probs = nn.functional.softmax(curr_logits, dim=-1)\n                curr_probs = torch.unsqueeze(curr_probs, dim=1)\n                curr_hidden = curr_all_hidden[-1]\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(curr_hidden, dim=1)\n            prediction = classifier(new_accumulated_hidden / (curr_length + 1 + horizon_length))\n            label = torch.tensor(prediction.shape[0] * [class_label], device=device, dtype=torch.long)\n            discrim_loss = ce_loss(prediction, label)\n            print(' pplm_discrim_loss:', discrim_loss.data.cpu().numpy())\n            loss += discrim_loss\n            loss_list.append(discrim_loss)\n        kl_loss = 0.0\n        if kl_scale > 0.0:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            unpert_probs = unpert_probs + SMALL_CONST * (unpert_probs <= SMALL_CONST).float().to(device).detach()\n            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\n            corrected_probs = probs + correction.detach()\n            kl_loss = kl_scale * (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n            print(' kl_loss', kl_loss.data.cpu().numpy())\n            loss += kl_loss\n        loss_per_iter.append(loss.data.cpu().numpy())\n        print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n        loss.backward()\n        if grad_norms is not None and loss_type == PPLM_BOW:\n            grad_norms = [torch.max(grad_norms[index], torch.norm(p_.grad * window_mask)) for (index, p_) in enumerate(curr_perturbation)]\n        else:\n            grad_norms = [torch.norm(p_.grad * window_mask) + SMALL_CONST for (index, p_) in enumerate(curr_perturbation)]\n        grad = [-stepsize * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy() for (index, p_) in enumerate(curr_perturbation)]\n        grad_accumulator = list(map(add, grad, grad_accumulator))\n        for p_ in curr_perturbation:\n            p_.grad.data.zero_()\n        new_past = []\n        for p_ in past:\n            new_past.append(p_.detach())\n        past = new_past\n    grad_accumulator = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n    pert_past = list(map(add, past, grad_accumulator))\n    return (pert_past, new_accumulated_hidden, grad_norms, loss_per_iter)",
        "mutated": [
            "def perturb_past(past, model, last, unpert_past=None, unpert_logits=None, accumulated_hidden=None, grad_norms=None, stepsize=0.01, one_hot_bows_vectors=None, classifier=None, class_label=None, loss_type=0, num_iterations=3, horizon_length=1, window_length=0, decay=False, gamma=1.5, kl_scale=0.01, device='cuda'):\n    if False:\n        i = 10\n    grad_accumulator = [np.zeros(p.shape).astype('float32') for p in past]\n    if accumulated_hidden is None:\n        accumulated_hidden = 0\n    if decay:\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / window_length)[1:]\n    else:\n        decay_mask = 1.0\n    (_, _, _, curr_length, _) = past[0].shape\n    if curr_length > window_length and window_length > 0:\n        ones_key_val_shape = tuple(past[0].shape[:-2]) + (window_length,) + tuple(past[0].shape[-1:])\n        zeros_key_val_shape = tuple(past[0].shape[:-2]) + (curr_length - window_length,) + tuple(past[0].shape[-1:])\n        ones_mask = torch.ones(ones_key_val_shape)\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n        window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\n    else:\n        window_mask = torch.ones_like(past[0]).to(device)\n    loss_per_iter = []\n    new_accumulated_hidden = None\n    for i in range(num_iterations):\n        print('Iteration ', i + 1)\n        curr_perturbation = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n        for p_ in curr_perturbation:\n            p_.retain_grad()\n        perturbed_past = list(map(add, past, curr_perturbation))\n        (_, _, _, curr_length, _) = curr_perturbation[0].shape\n        lm_output = model(last, past_key_values=perturbed_past)\n        (all_logits, all_hidden) = (lm_output['logits'], lm_output['hidden_states'])\n        hidden = all_hidden[-1]\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n        logits = all_logits[:, -1, :]\n        probs = nn.functional.softmax(logits, dim=-1)\n        loss = 0.0\n        loss_list = []\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n            for one_hot_bow in one_hot_bows_vectors:\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n                bow_loss = -torch.log(torch.sum(bow_logits))\n                loss += bow_loss\n                loss_list.append(bow_loss)\n            print(' pplm_bow_loss:', loss.data.cpu().numpy())\n        if loss_type == 2 or loss_type == 3:\n            ce_loss = nn.CrossEntropyLoss()\n            curr_unpert_past = unpert_past\n            curr_probs = torch.unsqueeze(probs, dim=1)\n            wte = model.resize_token_embeddings()\n            for _ in range(horizon_length):\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n                lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=inputs_embeds)\n                (curr_all_logits, curr_unpert_past, curr_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n                curr_logits = curr_all_logits[:, -1, :]\n                curr_probs = nn.functional.softmax(curr_logits, dim=-1)\n                curr_probs = torch.unsqueeze(curr_probs, dim=1)\n                curr_hidden = curr_all_hidden[-1]\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(curr_hidden, dim=1)\n            prediction = classifier(new_accumulated_hidden / (curr_length + 1 + horizon_length))\n            label = torch.tensor(prediction.shape[0] * [class_label], device=device, dtype=torch.long)\n            discrim_loss = ce_loss(prediction, label)\n            print(' pplm_discrim_loss:', discrim_loss.data.cpu().numpy())\n            loss += discrim_loss\n            loss_list.append(discrim_loss)\n        kl_loss = 0.0\n        if kl_scale > 0.0:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            unpert_probs = unpert_probs + SMALL_CONST * (unpert_probs <= SMALL_CONST).float().to(device).detach()\n            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\n            corrected_probs = probs + correction.detach()\n            kl_loss = kl_scale * (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n            print(' kl_loss', kl_loss.data.cpu().numpy())\n            loss += kl_loss\n        loss_per_iter.append(loss.data.cpu().numpy())\n        print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n        loss.backward()\n        if grad_norms is not None and loss_type == PPLM_BOW:\n            grad_norms = [torch.max(grad_norms[index], torch.norm(p_.grad * window_mask)) for (index, p_) in enumerate(curr_perturbation)]\n        else:\n            grad_norms = [torch.norm(p_.grad * window_mask) + SMALL_CONST for (index, p_) in enumerate(curr_perturbation)]\n        grad = [-stepsize * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy() for (index, p_) in enumerate(curr_perturbation)]\n        grad_accumulator = list(map(add, grad, grad_accumulator))\n        for p_ in curr_perturbation:\n            p_.grad.data.zero_()\n        new_past = []\n        for p_ in past:\n            new_past.append(p_.detach())\n        past = new_past\n    grad_accumulator = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n    pert_past = list(map(add, past, grad_accumulator))\n    return (pert_past, new_accumulated_hidden, grad_norms, loss_per_iter)",
            "def perturb_past(past, model, last, unpert_past=None, unpert_logits=None, accumulated_hidden=None, grad_norms=None, stepsize=0.01, one_hot_bows_vectors=None, classifier=None, class_label=None, loss_type=0, num_iterations=3, horizon_length=1, window_length=0, decay=False, gamma=1.5, kl_scale=0.01, device='cuda'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    grad_accumulator = [np.zeros(p.shape).astype('float32') for p in past]\n    if accumulated_hidden is None:\n        accumulated_hidden = 0\n    if decay:\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / window_length)[1:]\n    else:\n        decay_mask = 1.0\n    (_, _, _, curr_length, _) = past[0].shape\n    if curr_length > window_length and window_length > 0:\n        ones_key_val_shape = tuple(past[0].shape[:-2]) + (window_length,) + tuple(past[0].shape[-1:])\n        zeros_key_val_shape = tuple(past[0].shape[:-2]) + (curr_length - window_length,) + tuple(past[0].shape[-1:])\n        ones_mask = torch.ones(ones_key_val_shape)\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n        window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\n    else:\n        window_mask = torch.ones_like(past[0]).to(device)\n    loss_per_iter = []\n    new_accumulated_hidden = None\n    for i in range(num_iterations):\n        print('Iteration ', i + 1)\n        curr_perturbation = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n        for p_ in curr_perturbation:\n            p_.retain_grad()\n        perturbed_past = list(map(add, past, curr_perturbation))\n        (_, _, _, curr_length, _) = curr_perturbation[0].shape\n        lm_output = model(last, past_key_values=perturbed_past)\n        (all_logits, all_hidden) = (lm_output['logits'], lm_output['hidden_states'])\n        hidden = all_hidden[-1]\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n        logits = all_logits[:, -1, :]\n        probs = nn.functional.softmax(logits, dim=-1)\n        loss = 0.0\n        loss_list = []\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n            for one_hot_bow in one_hot_bows_vectors:\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n                bow_loss = -torch.log(torch.sum(bow_logits))\n                loss += bow_loss\n                loss_list.append(bow_loss)\n            print(' pplm_bow_loss:', loss.data.cpu().numpy())\n        if loss_type == 2 or loss_type == 3:\n            ce_loss = nn.CrossEntropyLoss()\n            curr_unpert_past = unpert_past\n            curr_probs = torch.unsqueeze(probs, dim=1)\n            wte = model.resize_token_embeddings()\n            for _ in range(horizon_length):\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n                lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=inputs_embeds)\n                (curr_all_logits, curr_unpert_past, curr_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n                curr_logits = curr_all_logits[:, -1, :]\n                curr_probs = nn.functional.softmax(curr_logits, dim=-1)\n                curr_probs = torch.unsqueeze(curr_probs, dim=1)\n                curr_hidden = curr_all_hidden[-1]\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(curr_hidden, dim=1)\n            prediction = classifier(new_accumulated_hidden / (curr_length + 1 + horizon_length))\n            label = torch.tensor(prediction.shape[0] * [class_label], device=device, dtype=torch.long)\n            discrim_loss = ce_loss(prediction, label)\n            print(' pplm_discrim_loss:', discrim_loss.data.cpu().numpy())\n            loss += discrim_loss\n            loss_list.append(discrim_loss)\n        kl_loss = 0.0\n        if kl_scale > 0.0:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            unpert_probs = unpert_probs + SMALL_CONST * (unpert_probs <= SMALL_CONST).float().to(device).detach()\n            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\n            corrected_probs = probs + correction.detach()\n            kl_loss = kl_scale * (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n            print(' kl_loss', kl_loss.data.cpu().numpy())\n            loss += kl_loss\n        loss_per_iter.append(loss.data.cpu().numpy())\n        print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n        loss.backward()\n        if grad_norms is not None and loss_type == PPLM_BOW:\n            grad_norms = [torch.max(grad_norms[index], torch.norm(p_.grad * window_mask)) for (index, p_) in enumerate(curr_perturbation)]\n        else:\n            grad_norms = [torch.norm(p_.grad * window_mask) + SMALL_CONST for (index, p_) in enumerate(curr_perturbation)]\n        grad = [-stepsize * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy() for (index, p_) in enumerate(curr_perturbation)]\n        grad_accumulator = list(map(add, grad, grad_accumulator))\n        for p_ in curr_perturbation:\n            p_.grad.data.zero_()\n        new_past = []\n        for p_ in past:\n            new_past.append(p_.detach())\n        past = new_past\n    grad_accumulator = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n    pert_past = list(map(add, past, grad_accumulator))\n    return (pert_past, new_accumulated_hidden, grad_norms, loss_per_iter)",
            "def perturb_past(past, model, last, unpert_past=None, unpert_logits=None, accumulated_hidden=None, grad_norms=None, stepsize=0.01, one_hot_bows_vectors=None, classifier=None, class_label=None, loss_type=0, num_iterations=3, horizon_length=1, window_length=0, decay=False, gamma=1.5, kl_scale=0.01, device='cuda'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    grad_accumulator = [np.zeros(p.shape).astype('float32') for p in past]\n    if accumulated_hidden is None:\n        accumulated_hidden = 0\n    if decay:\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / window_length)[1:]\n    else:\n        decay_mask = 1.0\n    (_, _, _, curr_length, _) = past[0].shape\n    if curr_length > window_length and window_length > 0:\n        ones_key_val_shape = tuple(past[0].shape[:-2]) + (window_length,) + tuple(past[0].shape[-1:])\n        zeros_key_val_shape = tuple(past[0].shape[:-2]) + (curr_length - window_length,) + tuple(past[0].shape[-1:])\n        ones_mask = torch.ones(ones_key_val_shape)\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n        window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\n    else:\n        window_mask = torch.ones_like(past[0]).to(device)\n    loss_per_iter = []\n    new_accumulated_hidden = None\n    for i in range(num_iterations):\n        print('Iteration ', i + 1)\n        curr_perturbation = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n        for p_ in curr_perturbation:\n            p_.retain_grad()\n        perturbed_past = list(map(add, past, curr_perturbation))\n        (_, _, _, curr_length, _) = curr_perturbation[0].shape\n        lm_output = model(last, past_key_values=perturbed_past)\n        (all_logits, all_hidden) = (lm_output['logits'], lm_output['hidden_states'])\n        hidden = all_hidden[-1]\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n        logits = all_logits[:, -1, :]\n        probs = nn.functional.softmax(logits, dim=-1)\n        loss = 0.0\n        loss_list = []\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n            for one_hot_bow in one_hot_bows_vectors:\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n                bow_loss = -torch.log(torch.sum(bow_logits))\n                loss += bow_loss\n                loss_list.append(bow_loss)\n            print(' pplm_bow_loss:', loss.data.cpu().numpy())\n        if loss_type == 2 or loss_type == 3:\n            ce_loss = nn.CrossEntropyLoss()\n            curr_unpert_past = unpert_past\n            curr_probs = torch.unsqueeze(probs, dim=1)\n            wte = model.resize_token_embeddings()\n            for _ in range(horizon_length):\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n                lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=inputs_embeds)\n                (curr_all_logits, curr_unpert_past, curr_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n                curr_logits = curr_all_logits[:, -1, :]\n                curr_probs = nn.functional.softmax(curr_logits, dim=-1)\n                curr_probs = torch.unsqueeze(curr_probs, dim=1)\n                curr_hidden = curr_all_hidden[-1]\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(curr_hidden, dim=1)\n            prediction = classifier(new_accumulated_hidden / (curr_length + 1 + horizon_length))\n            label = torch.tensor(prediction.shape[0] * [class_label], device=device, dtype=torch.long)\n            discrim_loss = ce_loss(prediction, label)\n            print(' pplm_discrim_loss:', discrim_loss.data.cpu().numpy())\n            loss += discrim_loss\n            loss_list.append(discrim_loss)\n        kl_loss = 0.0\n        if kl_scale > 0.0:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            unpert_probs = unpert_probs + SMALL_CONST * (unpert_probs <= SMALL_CONST).float().to(device).detach()\n            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\n            corrected_probs = probs + correction.detach()\n            kl_loss = kl_scale * (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n            print(' kl_loss', kl_loss.data.cpu().numpy())\n            loss += kl_loss\n        loss_per_iter.append(loss.data.cpu().numpy())\n        print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n        loss.backward()\n        if grad_norms is not None and loss_type == PPLM_BOW:\n            grad_norms = [torch.max(grad_norms[index], torch.norm(p_.grad * window_mask)) for (index, p_) in enumerate(curr_perturbation)]\n        else:\n            grad_norms = [torch.norm(p_.grad * window_mask) + SMALL_CONST for (index, p_) in enumerate(curr_perturbation)]\n        grad = [-stepsize * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy() for (index, p_) in enumerate(curr_perturbation)]\n        grad_accumulator = list(map(add, grad, grad_accumulator))\n        for p_ in curr_perturbation:\n            p_.grad.data.zero_()\n        new_past = []\n        for p_ in past:\n            new_past.append(p_.detach())\n        past = new_past\n    grad_accumulator = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n    pert_past = list(map(add, past, grad_accumulator))\n    return (pert_past, new_accumulated_hidden, grad_norms, loss_per_iter)",
            "def perturb_past(past, model, last, unpert_past=None, unpert_logits=None, accumulated_hidden=None, grad_norms=None, stepsize=0.01, one_hot_bows_vectors=None, classifier=None, class_label=None, loss_type=0, num_iterations=3, horizon_length=1, window_length=0, decay=False, gamma=1.5, kl_scale=0.01, device='cuda'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    grad_accumulator = [np.zeros(p.shape).astype('float32') for p in past]\n    if accumulated_hidden is None:\n        accumulated_hidden = 0\n    if decay:\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / window_length)[1:]\n    else:\n        decay_mask = 1.0\n    (_, _, _, curr_length, _) = past[0].shape\n    if curr_length > window_length and window_length > 0:\n        ones_key_val_shape = tuple(past[0].shape[:-2]) + (window_length,) + tuple(past[0].shape[-1:])\n        zeros_key_val_shape = tuple(past[0].shape[:-2]) + (curr_length - window_length,) + tuple(past[0].shape[-1:])\n        ones_mask = torch.ones(ones_key_val_shape)\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n        window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\n    else:\n        window_mask = torch.ones_like(past[0]).to(device)\n    loss_per_iter = []\n    new_accumulated_hidden = None\n    for i in range(num_iterations):\n        print('Iteration ', i + 1)\n        curr_perturbation = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n        for p_ in curr_perturbation:\n            p_.retain_grad()\n        perturbed_past = list(map(add, past, curr_perturbation))\n        (_, _, _, curr_length, _) = curr_perturbation[0].shape\n        lm_output = model(last, past_key_values=perturbed_past)\n        (all_logits, all_hidden) = (lm_output['logits'], lm_output['hidden_states'])\n        hidden = all_hidden[-1]\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n        logits = all_logits[:, -1, :]\n        probs = nn.functional.softmax(logits, dim=-1)\n        loss = 0.0\n        loss_list = []\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n            for one_hot_bow in one_hot_bows_vectors:\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n                bow_loss = -torch.log(torch.sum(bow_logits))\n                loss += bow_loss\n                loss_list.append(bow_loss)\n            print(' pplm_bow_loss:', loss.data.cpu().numpy())\n        if loss_type == 2 or loss_type == 3:\n            ce_loss = nn.CrossEntropyLoss()\n            curr_unpert_past = unpert_past\n            curr_probs = torch.unsqueeze(probs, dim=1)\n            wte = model.resize_token_embeddings()\n            for _ in range(horizon_length):\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n                lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=inputs_embeds)\n                (curr_all_logits, curr_unpert_past, curr_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n                curr_logits = curr_all_logits[:, -1, :]\n                curr_probs = nn.functional.softmax(curr_logits, dim=-1)\n                curr_probs = torch.unsqueeze(curr_probs, dim=1)\n                curr_hidden = curr_all_hidden[-1]\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(curr_hidden, dim=1)\n            prediction = classifier(new_accumulated_hidden / (curr_length + 1 + horizon_length))\n            label = torch.tensor(prediction.shape[0] * [class_label], device=device, dtype=torch.long)\n            discrim_loss = ce_loss(prediction, label)\n            print(' pplm_discrim_loss:', discrim_loss.data.cpu().numpy())\n            loss += discrim_loss\n            loss_list.append(discrim_loss)\n        kl_loss = 0.0\n        if kl_scale > 0.0:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            unpert_probs = unpert_probs + SMALL_CONST * (unpert_probs <= SMALL_CONST).float().to(device).detach()\n            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\n            corrected_probs = probs + correction.detach()\n            kl_loss = kl_scale * (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n            print(' kl_loss', kl_loss.data.cpu().numpy())\n            loss += kl_loss\n        loss_per_iter.append(loss.data.cpu().numpy())\n        print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n        loss.backward()\n        if grad_norms is not None and loss_type == PPLM_BOW:\n            grad_norms = [torch.max(grad_norms[index], torch.norm(p_.grad * window_mask)) for (index, p_) in enumerate(curr_perturbation)]\n        else:\n            grad_norms = [torch.norm(p_.grad * window_mask) + SMALL_CONST for (index, p_) in enumerate(curr_perturbation)]\n        grad = [-stepsize * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy() for (index, p_) in enumerate(curr_perturbation)]\n        grad_accumulator = list(map(add, grad, grad_accumulator))\n        for p_ in curr_perturbation:\n            p_.grad.data.zero_()\n        new_past = []\n        for p_ in past:\n            new_past.append(p_.detach())\n        past = new_past\n    grad_accumulator = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n    pert_past = list(map(add, past, grad_accumulator))\n    return (pert_past, new_accumulated_hidden, grad_norms, loss_per_iter)",
            "def perturb_past(past, model, last, unpert_past=None, unpert_logits=None, accumulated_hidden=None, grad_norms=None, stepsize=0.01, one_hot_bows_vectors=None, classifier=None, class_label=None, loss_type=0, num_iterations=3, horizon_length=1, window_length=0, decay=False, gamma=1.5, kl_scale=0.01, device='cuda'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    grad_accumulator = [np.zeros(p.shape).astype('float32') for p in past]\n    if accumulated_hidden is None:\n        accumulated_hidden = 0\n    if decay:\n        decay_mask = torch.arange(0.0, 1.0 + SMALL_CONST, 1.0 / window_length)[1:]\n    else:\n        decay_mask = 1.0\n    (_, _, _, curr_length, _) = past[0].shape\n    if curr_length > window_length and window_length > 0:\n        ones_key_val_shape = tuple(past[0].shape[:-2]) + (window_length,) + tuple(past[0].shape[-1:])\n        zeros_key_val_shape = tuple(past[0].shape[:-2]) + (curr_length - window_length,) + tuple(past[0].shape[-1:])\n        ones_mask = torch.ones(ones_key_val_shape)\n        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n        window_mask = torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2).to(device)\n    else:\n        window_mask = torch.ones_like(past[0]).to(device)\n    loss_per_iter = []\n    new_accumulated_hidden = None\n    for i in range(num_iterations):\n        print('Iteration ', i + 1)\n        curr_perturbation = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n        for p_ in curr_perturbation:\n            p_.retain_grad()\n        perturbed_past = list(map(add, past, curr_perturbation))\n        (_, _, _, curr_length, _) = curr_perturbation[0].shape\n        lm_output = model(last, past_key_values=perturbed_past)\n        (all_logits, all_hidden) = (lm_output['logits'], lm_output['hidden_states'])\n        hidden = all_hidden[-1]\n        new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()\n        logits = all_logits[:, -1, :]\n        probs = nn.functional.softmax(logits, dim=-1)\n        loss = 0.0\n        loss_list = []\n        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n            for one_hot_bow in one_hot_bows_vectors:\n                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n                bow_loss = -torch.log(torch.sum(bow_logits))\n                loss += bow_loss\n                loss_list.append(bow_loss)\n            print(' pplm_bow_loss:', loss.data.cpu().numpy())\n        if loss_type == 2 or loss_type == 3:\n            ce_loss = nn.CrossEntropyLoss()\n            curr_unpert_past = unpert_past\n            curr_probs = torch.unsqueeze(probs, dim=1)\n            wte = model.resize_token_embeddings()\n            for _ in range(horizon_length):\n                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n                lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=inputs_embeds)\n                (curr_all_logits, curr_unpert_past, curr_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n                curr_logits = curr_all_logits[:, -1, :]\n                curr_probs = nn.functional.softmax(curr_logits, dim=-1)\n                curr_probs = torch.unsqueeze(curr_probs, dim=1)\n                curr_hidden = curr_all_hidden[-1]\n                new_accumulated_hidden = new_accumulated_hidden + torch.sum(curr_hidden, dim=1)\n            prediction = classifier(new_accumulated_hidden / (curr_length + 1 + horizon_length))\n            label = torch.tensor(prediction.shape[0] * [class_label], device=device, dtype=torch.long)\n            discrim_loss = ce_loss(prediction, label)\n            print(' pplm_discrim_loss:', discrim_loss.data.cpu().numpy())\n            loss += discrim_loss\n            loss_list.append(discrim_loss)\n        kl_loss = 0.0\n        if kl_scale > 0.0:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            unpert_probs = unpert_probs + SMALL_CONST * (unpert_probs <= SMALL_CONST).float().to(device).detach()\n            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(device).detach()\n            corrected_probs = probs + correction.detach()\n            kl_loss = kl_scale * (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n            print(' kl_loss', kl_loss.data.cpu().numpy())\n            loss += kl_loss\n        loss_per_iter.append(loss.data.cpu().numpy())\n        print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n        loss.backward()\n        if grad_norms is not None and loss_type == PPLM_BOW:\n            grad_norms = [torch.max(grad_norms[index], torch.norm(p_.grad * window_mask)) for (index, p_) in enumerate(curr_perturbation)]\n        else:\n            grad_norms = [torch.norm(p_.grad * window_mask) + SMALL_CONST for (index, p_) in enumerate(curr_perturbation)]\n        grad = [-stepsize * (p_.grad * window_mask / grad_norms[index] ** gamma).data.cpu().numpy() for (index, p_) in enumerate(curr_perturbation)]\n        grad_accumulator = list(map(add, grad, grad_accumulator))\n        for p_ in curr_perturbation:\n            p_.grad.data.zero_()\n        new_past = []\n        for p_ in past:\n            new_past.append(p_.detach())\n        past = new_past\n    grad_accumulator = [torch.from_numpy(p_).requires_grad_(True).to(device=device) for p_ in grad_accumulator]\n    pert_past = list(map(add, past, grad_accumulator))\n    return (pert_past, new_accumulated_hidden, grad_norms, loss_per_iter)"
        ]
    },
    {
        "func_name": "get_classifier",
        "original": "def get_classifier(name: Optional[str], class_label: Union[str, int], device: str) -> Tuple[Optional[ClassificationHead], Optional[int]]:\n    if name is None:\n        return (None, None)\n    params = DISCRIMINATOR_MODELS_PARAMS[name]\n    classifier = ClassificationHead(class_size=params['class_size'], embed_size=params['embed_size']).to(device)\n    if 'url' in params:\n        resolved_archive_file = cached_path(params['url'])\n    elif 'path' in params:\n        resolved_archive_file = params['path']\n    else:\n        raise ValueError('Either url or path have to be specified in the discriminator model parameters')\n    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\n    classifier.eval()\n    if isinstance(class_label, str):\n        if class_label in params['class_vocab']:\n            label_id = params['class_vocab'][class_label]\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    elif isinstance(class_label, int):\n        if class_label in set(params['class_vocab'].values()):\n            label_id = class_label\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    else:\n        label_id = params['default_class']\n    return (classifier, label_id)",
        "mutated": [
            "def get_classifier(name: Optional[str], class_label: Union[str, int], device: str) -> Tuple[Optional[ClassificationHead], Optional[int]]:\n    if False:\n        i = 10\n    if name is None:\n        return (None, None)\n    params = DISCRIMINATOR_MODELS_PARAMS[name]\n    classifier = ClassificationHead(class_size=params['class_size'], embed_size=params['embed_size']).to(device)\n    if 'url' in params:\n        resolved_archive_file = cached_path(params['url'])\n    elif 'path' in params:\n        resolved_archive_file = params['path']\n    else:\n        raise ValueError('Either url or path have to be specified in the discriminator model parameters')\n    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\n    classifier.eval()\n    if isinstance(class_label, str):\n        if class_label in params['class_vocab']:\n            label_id = params['class_vocab'][class_label]\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    elif isinstance(class_label, int):\n        if class_label in set(params['class_vocab'].values()):\n            label_id = class_label\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    else:\n        label_id = params['default_class']\n    return (classifier, label_id)",
            "def get_classifier(name: Optional[str], class_label: Union[str, int], device: str) -> Tuple[Optional[ClassificationHead], Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if name is None:\n        return (None, None)\n    params = DISCRIMINATOR_MODELS_PARAMS[name]\n    classifier = ClassificationHead(class_size=params['class_size'], embed_size=params['embed_size']).to(device)\n    if 'url' in params:\n        resolved_archive_file = cached_path(params['url'])\n    elif 'path' in params:\n        resolved_archive_file = params['path']\n    else:\n        raise ValueError('Either url or path have to be specified in the discriminator model parameters')\n    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\n    classifier.eval()\n    if isinstance(class_label, str):\n        if class_label in params['class_vocab']:\n            label_id = params['class_vocab'][class_label]\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    elif isinstance(class_label, int):\n        if class_label in set(params['class_vocab'].values()):\n            label_id = class_label\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    else:\n        label_id = params['default_class']\n    return (classifier, label_id)",
            "def get_classifier(name: Optional[str], class_label: Union[str, int], device: str) -> Tuple[Optional[ClassificationHead], Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if name is None:\n        return (None, None)\n    params = DISCRIMINATOR_MODELS_PARAMS[name]\n    classifier = ClassificationHead(class_size=params['class_size'], embed_size=params['embed_size']).to(device)\n    if 'url' in params:\n        resolved_archive_file = cached_path(params['url'])\n    elif 'path' in params:\n        resolved_archive_file = params['path']\n    else:\n        raise ValueError('Either url or path have to be specified in the discriminator model parameters')\n    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\n    classifier.eval()\n    if isinstance(class_label, str):\n        if class_label in params['class_vocab']:\n            label_id = params['class_vocab'][class_label]\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    elif isinstance(class_label, int):\n        if class_label in set(params['class_vocab'].values()):\n            label_id = class_label\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    else:\n        label_id = params['default_class']\n    return (classifier, label_id)",
            "def get_classifier(name: Optional[str], class_label: Union[str, int], device: str) -> Tuple[Optional[ClassificationHead], Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if name is None:\n        return (None, None)\n    params = DISCRIMINATOR_MODELS_PARAMS[name]\n    classifier = ClassificationHead(class_size=params['class_size'], embed_size=params['embed_size']).to(device)\n    if 'url' in params:\n        resolved_archive_file = cached_path(params['url'])\n    elif 'path' in params:\n        resolved_archive_file = params['path']\n    else:\n        raise ValueError('Either url or path have to be specified in the discriminator model parameters')\n    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\n    classifier.eval()\n    if isinstance(class_label, str):\n        if class_label in params['class_vocab']:\n            label_id = params['class_vocab'][class_label]\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    elif isinstance(class_label, int):\n        if class_label in set(params['class_vocab'].values()):\n            label_id = class_label\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    else:\n        label_id = params['default_class']\n    return (classifier, label_id)",
            "def get_classifier(name: Optional[str], class_label: Union[str, int], device: str) -> Tuple[Optional[ClassificationHead], Optional[int]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if name is None:\n        return (None, None)\n    params = DISCRIMINATOR_MODELS_PARAMS[name]\n    classifier = ClassificationHead(class_size=params['class_size'], embed_size=params['embed_size']).to(device)\n    if 'url' in params:\n        resolved_archive_file = cached_path(params['url'])\n    elif 'path' in params:\n        resolved_archive_file = params['path']\n    else:\n        raise ValueError('Either url or path have to be specified in the discriminator model parameters')\n    classifier.load_state_dict(torch.load(resolved_archive_file, map_location=device))\n    classifier.eval()\n    if isinstance(class_label, str):\n        if class_label in params['class_vocab']:\n            label_id = params['class_vocab'][class_label]\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    elif isinstance(class_label, int):\n        if class_label in set(params['class_vocab'].values()):\n            label_id = class_label\n        else:\n            label_id = params['default_class']\n            print('class_label {} not in class_vocab'.format(class_label))\n            print('available values are: {}'.format(params['class_vocab']))\n            print('using default class {}'.format(label_id))\n    else:\n        label_id = params['default_class']\n    return (classifier, label_id)"
        ]
    },
    {
        "func_name": "get_bag_of_words_indices",
        "original": "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> List[List[List[int]]]:\n    bow_indices = []\n    for id_or_path in bag_of_words_ids_or_paths:\n        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n        else:\n            filepath = id_or_path\n        with open(filepath, 'r') as f:\n            words = f.read().strip().split('\\n')\n        bow_indices.append([tokenizer.encode(word.strip(), add_prefix_space=True) for word in words])\n    return bow_indices",
        "mutated": [
            "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> List[List[List[int]]]:\n    if False:\n        i = 10\n    bow_indices = []\n    for id_or_path in bag_of_words_ids_or_paths:\n        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n        else:\n            filepath = id_or_path\n        with open(filepath, 'r') as f:\n            words = f.read().strip().split('\\n')\n        bow_indices.append([tokenizer.encode(word.strip(), add_prefix_space=True) for word in words])\n    return bow_indices",
            "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> List[List[List[int]]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bow_indices = []\n    for id_or_path in bag_of_words_ids_or_paths:\n        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n        else:\n            filepath = id_or_path\n        with open(filepath, 'r') as f:\n            words = f.read().strip().split('\\n')\n        bow_indices.append([tokenizer.encode(word.strip(), add_prefix_space=True) for word in words])\n    return bow_indices",
            "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> List[List[List[int]]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bow_indices = []\n    for id_or_path in bag_of_words_ids_or_paths:\n        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n        else:\n            filepath = id_or_path\n        with open(filepath, 'r') as f:\n            words = f.read().strip().split('\\n')\n        bow_indices.append([tokenizer.encode(word.strip(), add_prefix_space=True) for word in words])\n    return bow_indices",
            "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> List[List[List[int]]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bow_indices = []\n    for id_or_path in bag_of_words_ids_or_paths:\n        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n        else:\n            filepath = id_or_path\n        with open(filepath, 'r') as f:\n            words = f.read().strip().split('\\n')\n        bow_indices.append([tokenizer.encode(word.strip(), add_prefix_space=True) for word in words])\n    return bow_indices",
            "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> List[List[List[int]]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bow_indices = []\n    for id_or_path in bag_of_words_ids_or_paths:\n        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n        else:\n            filepath = id_or_path\n        with open(filepath, 'r') as f:\n            words = f.read().strip().split('\\n')\n        bow_indices.append([tokenizer.encode(word.strip(), add_prefix_space=True) for word in words])\n    return bow_indices"
        ]
    },
    {
        "func_name": "build_bows_one_hot_vectors",
        "original": "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n    if bow_indices is None:\n        return None\n    one_hot_bows_vectors = []\n    for single_bow in bow_indices:\n        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n        single_bow = torch.tensor(single_bow).to(device)\n        num_words = single_bow.shape[0]\n        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n        one_hot_bow.scatter_(1, single_bow, 1)\n        one_hot_bows_vectors.append(one_hot_bow)\n    return one_hot_bows_vectors",
        "mutated": [
            "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n    if False:\n        i = 10\n    if bow_indices is None:\n        return None\n    one_hot_bows_vectors = []\n    for single_bow in bow_indices:\n        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n        single_bow = torch.tensor(single_bow).to(device)\n        num_words = single_bow.shape[0]\n        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n        one_hot_bow.scatter_(1, single_bow, 1)\n        one_hot_bows_vectors.append(one_hot_bow)\n    return one_hot_bows_vectors",
            "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if bow_indices is None:\n        return None\n    one_hot_bows_vectors = []\n    for single_bow in bow_indices:\n        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n        single_bow = torch.tensor(single_bow).to(device)\n        num_words = single_bow.shape[0]\n        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n        one_hot_bow.scatter_(1, single_bow, 1)\n        one_hot_bows_vectors.append(one_hot_bow)\n    return one_hot_bows_vectors",
            "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if bow_indices is None:\n        return None\n    one_hot_bows_vectors = []\n    for single_bow in bow_indices:\n        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n        single_bow = torch.tensor(single_bow).to(device)\n        num_words = single_bow.shape[0]\n        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n        one_hot_bow.scatter_(1, single_bow, 1)\n        one_hot_bows_vectors.append(one_hot_bow)\n    return one_hot_bows_vectors",
            "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if bow_indices is None:\n        return None\n    one_hot_bows_vectors = []\n    for single_bow in bow_indices:\n        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n        single_bow = torch.tensor(single_bow).to(device)\n        num_words = single_bow.shape[0]\n        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n        one_hot_bow.scatter_(1, single_bow, 1)\n        one_hot_bows_vectors.append(one_hot_bow)\n    return one_hot_bows_vectors",
            "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if bow_indices is None:\n        return None\n    one_hot_bows_vectors = []\n    for single_bow in bow_indices:\n        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n        single_bow = torch.tensor(single_bow).to(device)\n        num_words = single_bow.shape[0]\n        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n        one_hot_bow.scatter_(1, single_bow, 1)\n        one_hot_bows_vectors.append(one_hot_bow)\n    return one_hot_bows_vectors"
        ]
    },
    {
        "func_name": "full_text_generation",
        "original": "def full_text_generation(model, tokenizer, context=None, num_samples=1, device='cuda', bag_of_words=None, discrim=None, class_label=None, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0, **kwargs):\n    (classifier, class_id) = get_classifier(discrim, class_label, device)\n    bow_indices = []\n    if bag_of_words:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n    if bag_of_words and classifier:\n        print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n        loss_type = PPLM_BOW_DISCRIM\n    elif bag_of_words:\n        loss_type = PPLM_BOW\n        print('Using PPLM-BoW')\n    elif classifier is not None:\n        loss_type = PPLM_DISCRIM\n        print('Using PPLM-Discrim')\n    else:\n        raise Exception('Specify either a bag of words or a discriminator')\n    (unpert_gen_tok_text, _, _) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, length=length, sample=sample, perturb=False, repetition_penalty=repetition_penalty)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    pert_gen_tok_texts = []\n    discrim_losses = []\n    losses_in_time = []\n    for i in range(num_samples):\n        (pert_gen_tok_text, discrim_loss, loss_in_time) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, perturb=True, bow_indices=bow_indices, classifier=classifier, class_label=class_id, loss_type=loss_type, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n        pert_gen_tok_texts.append(pert_gen_tok_text)\n        if classifier is not None:\n            discrim_losses.append(discrim_loss.data.cpu().numpy())\n        losses_in_time.append(loss_in_time)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    return (unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time)",
        "mutated": [
            "def full_text_generation(model, tokenizer, context=None, num_samples=1, device='cuda', bag_of_words=None, discrim=None, class_label=None, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0, **kwargs):\n    if False:\n        i = 10\n    (classifier, class_id) = get_classifier(discrim, class_label, device)\n    bow_indices = []\n    if bag_of_words:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n    if bag_of_words and classifier:\n        print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n        loss_type = PPLM_BOW_DISCRIM\n    elif bag_of_words:\n        loss_type = PPLM_BOW\n        print('Using PPLM-BoW')\n    elif classifier is not None:\n        loss_type = PPLM_DISCRIM\n        print('Using PPLM-Discrim')\n    else:\n        raise Exception('Specify either a bag of words or a discriminator')\n    (unpert_gen_tok_text, _, _) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, length=length, sample=sample, perturb=False, repetition_penalty=repetition_penalty)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    pert_gen_tok_texts = []\n    discrim_losses = []\n    losses_in_time = []\n    for i in range(num_samples):\n        (pert_gen_tok_text, discrim_loss, loss_in_time) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, perturb=True, bow_indices=bow_indices, classifier=classifier, class_label=class_id, loss_type=loss_type, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n        pert_gen_tok_texts.append(pert_gen_tok_text)\n        if classifier is not None:\n            discrim_losses.append(discrim_loss.data.cpu().numpy())\n        losses_in_time.append(loss_in_time)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    return (unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time)",
            "def full_text_generation(model, tokenizer, context=None, num_samples=1, device='cuda', bag_of_words=None, discrim=None, class_label=None, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (classifier, class_id) = get_classifier(discrim, class_label, device)\n    bow_indices = []\n    if bag_of_words:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n    if bag_of_words and classifier:\n        print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n        loss_type = PPLM_BOW_DISCRIM\n    elif bag_of_words:\n        loss_type = PPLM_BOW\n        print('Using PPLM-BoW')\n    elif classifier is not None:\n        loss_type = PPLM_DISCRIM\n        print('Using PPLM-Discrim')\n    else:\n        raise Exception('Specify either a bag of words or a discriminator')\n    (unpert_gen_tok_text, _, _) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, length=length, sample=sample, perturb=False, repetition_penalty=repetition_penalty)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    pert_gen_tok_texts = []\n    discrim_losses = []\n    losses_in_time = []\n    for i in range(num_samples):\n        (pert_gen_tok_text, discrim_loss, loss_in_time) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, perturb=True, bow_indices=bow_indices, classifier=classifier, class_label=class_id, loss_type=loss_type, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n        pert_gen_tok_texts.append(pert_gen_tok_text)\n        if classifier is not None:\n            discrim_losses.append(discrim_loss.data.cpu().numpy())\n        losses_in_time.append(loss_in_time)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    return (unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time)",
            "def full_text_generation(model, tokenizer, context=None, num_samples=1, device='cuda', bag_of_words=None, discrim=None, class_label=None, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (classifier, class_id) = get_classifier(discrim, class_label, device)\n    bow_indices = []\n    if bag_of_words:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n    if bag_of_words and classifier:\n        print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n        loss_type = PPLM_BOW_DISCRIM\n    elif bag_of_words:\n        loss_type = PPLM_BOW\n        print('Using PPLM-BoW')\n    elif classifier is not None:\n        loss_type = PPLM_DISCRIM\n        print('Using PPLM-Discrim')\n    else:\n        raise Exception('Specify either a bag of words or a discriminator')\n    (unpert_gen_tok_text, _, _) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, length=length, sample=sample, perturb=False, repetition_penalty=repetition_penalty)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    pert_gen_tok_texts = []\n    discrim_losses = []\n    losses_in_time = []\n    for i in range(num_samples):\n        (pert_gen_tok_text, discrim_loss, loss_in_time) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, perturb=True, bow_indices=bow_indices, classifier=classifier, class_label=class_id, loss_type=loss_type, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n        pert_gen_tok_texts.append(pert_gen_tok_text)\n        if classifier is not None:\n            discrim_losses.append(discrim_loss.data.cpu().numpy())\n        losses_in_time.append(loss_in_time)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    return (unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time)",
            "def full_text_generation(model, tokenizer, context=None, num_samples=1, device='cuda', bag_of_words=None, discrim=None, class_label=None, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (classifier, class_id) = get_classifier(discrim, class_label, device)\n    bow_indices = []\n    if bag_of_words:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n    if bag_of_words and classifier:\n        print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n        loss_type = PPLM_BOW_DISCRIM\n    elif bag_of_words:\n        loss_type = PPLM_BOW\n        print('Using PPLM-BoW')\n    elif classifier is not None:\n        loss_type = PPLM_DISCRIM\n        print('Using PPLM-Discrim')\n    else:\n        raise Exception('Specify either a bag of words or a discriminator')\n    (unpert_gen_tok_text, _, _) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, length=length, sample=sample, perturb=False, repetition_penalty=repetition_penalty)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    pert_gen_tok_texts = []\n    discrim_losses = []\n    losses_in_time = []\n    for i in range(num_samples):\n        (pert_gen_tok_text, discrim_loss, loss_in_time) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, perturb=True, bow_indices=bow_indices, classifier=classifier, class_label=class_id, loss_type=loss_type, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n        pert_gen_tok_texts.append(pert_gen_tok_text)\n        if classifier is not None:\n            discrim_losses.append(discrim_loss.data.cpu().numpy())\n        losses_in_time.append(loss_in_time)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    return (unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time)",
            "def full_text_generation(model, tokenizer, context=None, num_samples=1, device='cuda', bag_of_words=None, discrim=None, class_label=None, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (classifier, class_id) = get_classifier(discrim, class_label, device)\n    bow_indices = []\n    if bag_of_words:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n    if bag_of_words and classifier:\n        print('Both PPLM-BoW and PPLM-Discrim are on. This is not optimized.')\n        loss_type = PPLM_BOW_DISCRIM\n    elif bag_of_words:\n        loss_type = PPLM_BOW\n        print('Using PPLM-BoW')\n    elif classifier is not None:\n        loss_type = PPLM_DISCRIM\n        print('Using PPLM-Discrim')\n    else:\n        raise Exception('Specify either a bag of words or a discriminator')\n    (unpert_gen_tok_text, _, _) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, length=length, sample=sample, perturb=False, repetition_penalty=repetition_penalty)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    pert_gen_tok_texts = []\n    discrim_losses = []\n    losses_in_time = []\n    for i in range(num_samples):\n        (pert_gen_tok_text, discrim_loss, loss_in_time) = generate_text_pplm(model=model, tokenizer=tokenizer, context=context, device=device, perturb=True, bow_indices=bow_indices, classifier=classifier, class_label=class_id, loss_type=loss_type, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n        pert_gen_tok_texts.append(pert_gen_tok_text)\n        if classifier is not None:\n            discrim_losses.append(discrim_loss.data.cpu().numpy())\n        losses_in_time.append(loss_in_time)\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    return (unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time)"
        ]
    },
    {
        "func_name": "generate_text_pplm",
        "original": "def generate_text_pplm(model, tokenizer, context=None, past=None, device='cuda', perturb=True, bow_indices=None, classifier=None, class_label=None, loss_type=0, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0):\n    output_so_far = None\n    if context:\n        context_t = torch.tensor(context, device=device, dtype=torch.long)\n        while len(context_t.shape) < 2:\n            context_t = context_t.unsqueeze(0)\n        output_so_far = context_t\n    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n    grad_norms = None\n    last = None\n    unpert_discrim_loss = 0\n    loss_in_time = []\n    for i in trange(length, ascii=True):\n        if past is None and output_so_far is not None:\n            last = output_so_far[:, -1:]\n            if output_so_far.shape[1] > 1:\n                past = model(output_so_far[:, :-1])['past_key_values']\n        lm_output = model(output_so_far)\n        (unpert_logits, unpert_past, unpert_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n        unpert_last_hidden = unpert_all_hidden[-1]\n        if i >= grad_length:\n            current_stepsize = stepsize * 0\n        else:\n            current_stepsize = stepsize\n        if not perturb or num_iterations == 0:\n            pert_past = past\n        else:\n            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n            if past is not None:\n                (pert_past, _, grad_norms, loss_this_iter) = perturb_past(past, model, last, unpert_past=unpert_past, unpert_logits=unpert_logits, accumulated_hidden=accumulated_hidden, grad_norms=grad_norms, stepsize=current_stepsize, one_hot_bows_vectors=one_hot_bows_vectors, classifier=classifier, class_label=class_label, loss_type=loss_type, num_iterations=num_iterations, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, kl_scale=kl_scale, device=device)\n                loss_in_time.append(loss_this_iter)\n            else:\n                pert_past = past\n        lm_output = model(last, past_key_values=pert_past)\n        (pert_logits, past) = (lm_output['logits'], lm_output['past_key_values'])\n        pert_logits = pert_logits[:, -1, :] / temperature\n        for token_idx in set(output_so_far[0].tolist()):\n            if pert_logits[0, token_idx] < 0:\n                pert_logits[0, token_idx] *= repetition_penalty\n            else:\n                pert_logits[0, token_idx] /= repetition_penalty\n        pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if classifier is not None:\n            ce_loss = nn.CrossEntropyLoss()\n            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n            label = torch.tensor([class_label], device=device, dtype=torch.long)\n            unpert_discrim_loss = ce_loss(prediction, label)\n            print('unperturbed discrim loss', unpert_discrim_loss.data.cpu().numpy())\n        else:\n            unpert_discrim_loss = 0\n        if perturb:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            pert_probs = pert_probs ** gm_scale * unpert_probs ** (1 - gm_scale)\n            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)\n            if torch.sum(pert_probs) <= 1:\n                pert_probs = pert_probs / torch.sum(pert_probs)\n        else:\n            pert_logits = top_k_filter(pert_logits, k=top_k)\n            pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if sample:\n            last = torch.multinomial(pert_probs, num_samples=1)\n        else:\n            (_, last) = torch.topk(pert_probs, k=1, dim=-1)\n        output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n        print(tokenizer.decode(output_so_far.tolist()[0]))\n    return (output_so_far, unpert_discrim_loss, loss_in_time)",
        "mutated": [
            "def generate_text_pplm(model, tokenizer, context=None, past=None, device='cuda', perturb=True, bow_indices=None, classifier=None, class_label=None, loss_type=0, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0):\n    if False:\n        i = 10\n    output_so_far = None\n    if context:\n        context_t = torch.tensor(context, device=device, dtype=torch.long)\n        while len(context_t.shape) < 2:\n            context_t = context_t.unsqueeze(0)\n        output_so_far = context_t\n    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n    grad_norms = None\n    last = None\n    unpert_discrim_loss = 0\n    loss_in_time = []\n    for i in trange(length, ascii=True):\n        if past is None and output_so_far is not None:\n            last = output_so_far[:, -1:]\n            if output_so_far.shape[1] > 1:\n                past = model(output_so_far[:, :-1])['past_key_values']\n        lm_output = model(output_so_far)\n        (unpert_logits, unpert_past, unpert_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n        unpert_last_hidden = unpert_all_hidden[-1]\n        if i >= grad_length:\n            current_stepsize = stepsize * 0\n        else:\n            current_stepsize = stepsize\n        if not perturb or num_iterations == 0:\n            pert_past = past\n        else:\n            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n            if past is not None:\n                (pert_past, _, grad_norms, loss_this_iter) = perturb_past(past, model, last, unpert_past=unpert_past, unpert_logits=unpert_logits, accumulated_hidden=accumulated_hidden, grad_norms=grad_norms, stepsize=current_stepsize, one_hot_bows_vectors=one_hot_bows_vectors, classifier=classifier, class_label=class_label, loss_type=loss_type, num_iterations=num_iterations, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, kl_scale=kl_scale, device=device)\n                loss_in_time.append(loss_this_iter)\n            else:\n                pert_past = past\n        lm_output = model(last, past_key_values=pert_past)\n        (pert_logits, past) = (lm_output['logits'], lm_output['past_key_values'])\n        pert_logits = pert_logits[:, -1, :] / temperature\n        for token_idx in set(output_so_far[0].tolist()):\n            if pert_logits[0, token_idx] < 0:\n                pert_logits[0, token_idx] *= repetition_penalty\n            else:\n                pert_logits[0, token_idx] /= repetition_penalty\n        pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if classifier is not None:\n            ce_loss = nn.CrossEntropyLoss()\n            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n            label = torch.tensor([class_label], device=device, dtype=torch.long)\n            unpert_discrim_loss = ce_loss(prediction, label)\n            print('unperturbed discrim loss', unpert_discrim_loss.data.cpu().numpy())\n        else:\n            unpert_discrim_loss = 0\n        if perturb:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            pert_probs = pert_probs ** gm_scale * unpert_probs ** (1 - gm_scale)\n            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)\n            if torch.sum(pert_probs) <= 1:\n                pert_probs = pert_probs / torch.sum(pert_probs)\n        else:\n            pert_logits = top_k_filter(pert_logits, k=top_k)\n            pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if sample:\n            last = torch.multinomial(pert_probs, num_samples=1)\n        else:\n            (_, last) = torch.topk(pert_probs, k=1, dim=-1)\n        output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n        print(tokenizer.decode(output_so_far.tolist()[0]))\n    return (output_so_far, unpert_discrim_loss, loss_in_time)",
            "def generate_text_pplm(model, tokenizer, context=None, past=None, device='cuda', perturb=True, bow_indices=None, classifier=None, class_label=None, loss_type=0, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output_so_far = None\n    if context:\n        context_t = torch.tensor(context, device=device, dtype=torch.long)\n        while len(context_t.shape) < 2:\n            context_t = context_t.unsqueeze(0)\n        output_so_far = context_t\n    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n    grad_norms = None\n    last = None\n    unpert_discrim_loss = 0\n    loss_in_time = []\n    for i in trange(length, ascii=True):\n        if past is None and output_so_far is not None:\n            last = output_so_far[:, -1:]\n            if output_so_far.shape[1] > 1:\n                past = model(output_so_far[:, :-1])['past_key_values']\n        lm_output = model(output_so_far)\n        (unpert_logits, unpert_past, unpert_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n        unpert_last_hidden = unpert_all_hidden[-1]\n        if i >= grad_length:\n            current_stepsize = stepsize * 0\n        else:\n            current_stepsize = stepsize\n        if not perturb or num_iterations == 0:\n            pert_past = past\n        else:\n            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n            if past is not None:\n                (pert_past, _, grad_norms, loss_this_iter) = perturb_past(past, model, last, unpert_past=unpert_past, unpert_logits=unpert_logits, accumulated_hidden=accumulated_hidden, grad_norms=grad_norms, stepsize=current_stepsize, one_hot_bows_vectors=one_hot_bows_vectors, classifier=classifier, class_label=class_label, loss_type=loss_type, num_iterations=num_iterations, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, kl_scale=kl_scale, device=device)\n                loss_in_time.append(loss_this_iter)\n            else:\n                pert_past = past\n        lm_output = model(last, past_key_values=pert_past)\n        (pert_logits, past) = (lm_output['logits'], lm_output['past_key_values'])\n        pert_logits = pert_logits[:, -1, :] / temperature\n        for token_idx in set(output_so_far[0].tolist()):\n            if pert_logits[0, token_idx] < 0:\n                pert_logits[0, token_idx] *= repetition_penalty\n            else:\n                pert_logits[0, token_idx] /= repetition_penalty\n        pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if classifier is not None:\n            ce_loss = nn.CrossEntropyLoss()\n            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n            label = torch.tensor([class_label], device=device, dtype=torch.long)\n            unpert_discrim_loss = ce_loss(prediction, label)\n            print('unperturbed discrim loss', unpert_discrim_loss.data.cpu().numpy())\n        else:\n            unpert_discrim_loss = 0\n        if perturb:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            pert_probs = pert_probs ** gm_scale * unpert_probs ** (1 - gm_scale)\n            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)\n            if torch.sum(pert_probs) <= 1:\n                pert_probs = pert_probs / torch.sum(pert_probs)\n        else:\n            pert_logits = top_k_filter(pert_logits, k=top_k)\n            pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if sample:\n            last = torch.multinomial(pert_probs, num_samples=1)\n        else:\n            (_, last) = torch.topk(pert_probs, k=1, dim=-1)\n        output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n        print(tokenizer.decode(output_so_far.tolist()[0]))\n    return (output_so_far, unpert_discrim_loss, loss_in_time)",
            "def generate_text_pplm(model, tokenizer, context=None, past=None, device='cuda', perturb=True, bow_indices=None, classifier=None, class_label=None, loss_type=0, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output_so_far = None\n    if context:\n        context_t = torch.tensor(context, device=device, dtype=torch.long)\n        while len(context_t.shape) < 2:\n            context_t = context_t.unsqueeze(0)\n        output_so_far = context_t\n    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n    grad_norms = None\n    last = None\n    unpert_discrim_loss = 0\n    loss_in_time = []\n    for i in trange(length, ascii=True):\n        if past is None and output_so_far is not None:\n            last = output_so_far[:, -1:]\n            if output_so_far.shape[1] > 1:\n                past = model(output_so_far[:, :-1])['past_key_values']\n        lm_output = model(output_so_far)\n        (unpert_logits, unpert_past, unpert_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n        unpert_last_hidden = unpert_all_hidden[-1]\n        if i >= grad_length:\n            current_stepsize = stepsize * 0\n        else:\n            current_stepsize = stepsize\n        if not perturb or num_iterations == 0:\n            pert_past = past\n        else:\n            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n            if past is not None:\n                (pert_past, _, grad_norms, loss_this_iter) = perturb_past(past, model, last, unpert_past=unpert_past, unpert_logits=unpert_logits, accumulated_hidden=accumulated_hidden, grad_norms=grad_norms, stepsize=current_stepsize, one_hot_bows_vectors=one_hot_bows_vectors, classifier=classifier, class_label=class_label, loss_type=loss_type, num_iterations=num_iterations, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, kl_scale=kl_scale, device=device)\n                loss_in_time.append(loss_this_iter)\n            else:\n                pert_past = past\n        lm_output = model(last, past_key_values=pert_past)\n        (pert_logits, past) = (lm_output['logits'], lm_output['past_key_values'])\n        pert_logits = pert_logits[:, -1, :] / temperature\n        for token_idx in set(output_so_far[0].tolist()):\n            if pert_logits[0, token_idx] < 0:\n                pert_logits[0, token_idx] *= repetition_penalty\n            else:\n                pert_logits[0, token_idx] /= repetition_penalty\n        pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if classifier is not None:\n            ce_loss = nn.CrossEntropyLoss()\n            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n            label = torch.tensor([class_label], device=device, dtype=torch.long)\n            unpert_discrim_loss = ce_loss(prediction, label)\n            print('unperturbed discrim loss', unpert_discrim_loss.data.cpu().numpy())\n        else:\n            unpert_discrim_loss = 0\n        if perturb:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            pert_probs = pert_probs ** gm_scale * unpert_probs ** (1 - gm_scale)\n            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)\n            if torch.sum(pert_probs) <= 1:\n                pert_probs = pert_probs / torch.sum(pert_probs)\n        else:\n            pert_logits = top_k_filter(pert_logits, k=top_k)\n            pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if sample:\n            last = torch.multinomial(pert_probs, num_samples=1)\n        else:\n            (_, last) = torch.topk(pert_probs, k=1, dim=-1)\n        output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n        print(tokenizer.decode(output_so_far.tolist()[0]))\n    return (output_so_far, unpert_discrim_loss, loss_in_time)",
            "def generate_text_pplm(model, tokenizer, context=None, past=None, device='cuda', perturb=True, bow_indices=None, classifier=None, class_label=None, loss_type=0, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output_so_far = None\n    if context:\n        context_t = torch.tensor(context, device=device, dtype=torch.long)\n        while len(context_t.shape) < 2:\n            context_t = context_t.unsqueeze(0)\n        output_so_far = context_t\n    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n    grad_norms = None\n    last = None\n    unpert_discrim_loss = 0\n    loss_in_time = []\n    for i in trange(length, ascii=True):\n        if past is None and output_so_far is not None:\n            last = output_so_far[:, -1:]\n            if output_so_far.shape[1] > 1:\n                past = model(output_so_far[:, :-1])['past_key_values']\n        lm_output = model(output_so_far)\n        (unpert_logits, unpert_past, unpert_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n        unpert_last_hidden = unpert_all_hidden[-1]\n        if i >= grad_length:\n            current_stepsize = stepsize * 0\n        else:\n            current_stepsize = stepsize\n        if not perturb or num_iterations == 0:\n            pert_past = past\n        else:\n            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n            if past is not None:\n                (pert_past, _, grad_norms, loss_this_iter) = perturb_past(past, model, last, unpert_past=unpert_past, unpert_logits=unpert_logits, accumulated_hidden=accumulated_hidden, grad_norms=grad_norms, stepsize=current_stepsize, one_hot_bows_vectors=one_hot_bows_vectors, classifier=classifier, class_label=class_label, loss_type=loss_type, num_iterations=num_iterations, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, kl_scale=kl_scale, device=device)\n                loss_in_time.append(loss_this_iter)\n            else:\n                pert_past = past\n        lm_output = model(last, past_key_values=pert_past)\n        (pert_logits, past) = (lm_output['logits'], lm_output['past_key_values'])\n        pert_logits = pert_logits[:, -1, :] / temperature\n        for token_idx in set(output_so_far[0].tolist()):\n            if pert_logits[0, token_idx] < 0:\n                pert_logits[0, token_idx] *= repetition_penalty\n            else:\n                pert_logits[0, token_idx] /= repetition_penalty\n        pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if classifier is not None:\n            ce_loss = nn.CrossEntropyLoss()\n            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n            label = torch.tensor([class_label], device=device, dtype=torch.long)\n            unpert_discrim_loss = ce_loss(prediction, label)\n            print('unperturbed discrim loss', unpert_discrim_loss.data.cpu().numpy())\n        else:\n            unpert_discrim_loss = 0\n        if perturb:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            pert_probs = pert_probs ** gm_scale * unpert_probs ** (1 - gm_scale)\n            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)\n            if torch.sum(pert_probs) <= 1:\n                pert_probs = pert_probs / torch.sum(pert_probs)\n        else:\n            pert_logits = top_k_filter(pert_logits, k=top_k)\n            pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if sample:\n            last = torch.multinomial(pert_probs, num_samples=1)\n        else:\n            (_, last) = torch.topk(pert_probs, k=1, dim=-1)\n        output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n        print(tokenizer.decode(output_so_far.tolist()[0]))\n    return (output_so_far, unpert_discrim_loss, loss_in_time)",
            "def generate_text_pplm(model, tokenizer, context=None, past=None, device='cuda', perturb=True, bow_indices=None, classifier=None, class_label=None, loss_type=0, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, repetition_penalty=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output_so_far = None\n    if context:\n        context_t = torch.tensor(context, device=device, dtype=torch.long)\n        while len(context_t.shape) < 2:\n            context_t = context_t.unsqueeze(0)\n        output_so_far = context_t\n    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n    grad_norms = None\n    last = None\n    unpert_discrim_loss = 0\n    loss_in_time = []\n    for i in trange(length, ascii=True):\n        if past is None and output_so_far is not None:\n            last = output_so_far[:, -1:]\n            if output_so_far.shape[1] > 1:\n                past = model(output_so_far[:, :-1])['past_key_values']\n        lm_output = model(output_so_far)\n        (unpert_logits, unpert_past, unpert_all_hidden) = (lm_output['logits'], lm_output['past_key_values'], lm_output['hidden_states'])\n        unpert_last_hidden = unpert_all_hidden[-1]\n        if i >= grad_length:\n            current_stepsize = stepsize * 0\n        else:\n            current_stepsize = stepsize\n        if not perturb or num_iterations == 0:\n            pert_past = past\n        else:\n            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n            if past is not None:\n                (pert_past, _, grad_norms, loss_this_iter) = perturb_past(past, model, last, unpert_past=unpert_past, unpert_logits=unpert_logits, accumulated_hidden=accumulated_hidden, grad_norms=grad_norms, stepsize=current_stepsize, one_hot_bows_vectors=one_hot_bows_vectors, classifier=classifier, class_label=class_label, loss_type=loss_type, num_iterations=num_iterations, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, kl_scale=kl_scale, device=device)\n                loss_in_time.append(loss_this_iter)\n            else:\n                pert_past = past\n        lm_output = model(last, past_key_values=pert_past)\n        (pert_logits, past) = (lm_output['logits'], lm_output['past_key_values'])\n        pert_logits = pert_logits[:, -1, :] / temperature\n        for token_idx in set(output_so_far[0].tolist()):\n            if pert_logits[0, token_idx] < 0:\n                pert_logits[0, token_idx] *= repetition_penalty\n            else:\n                pert_logits[0, token_idx] /= repetition_penalty\n        pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if classifier is not None:\n            ce_loss = nn.CrossEntropyLoss()\n            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n            label = torch.tensor([class_label], device=device, dtype=torch.long)\n            unpert_discrim_loss = ce_loss(prediction, label)\n            print('unperturbed discrim loss', unpert_discrim_loss.data.cpu().numpy())\n        else:\n            unpert_discrim_loss = 0\n        if perturb:\n            unpert_probs = nn.functional.softmax(unpert_logits[:, -1, :], dim=-1)\n            pert_probs = pert_probs ** gm_scale * unpert_probs ** (1 - gm_scale)\n            pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)\n            if torch.sum(pert_probs) <= 1:\n                pert_probs = pert_probs / torch.sum(pert_probs)\n        else:\n            pert_logits = top_k_filter(pert_logits, k=top_k)\n            pert_probs = nn.functional.softmax(pert_logits, dim=-1)\n        if sample:\n            last = torch.multinomial(pert_probs, num_samples=1)\n        else:\n            (_, last) = torch.topk(pert_probs, k=1, dim=-1)\n        output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n        print(tokenizer.decode(output_so_far.tolist()[0]))\n    return (output_so_far, unpert_discrim_loss, loss_in_time)"
        ]
    },
    {
        "func_name": "set_generic_model_params",
        "original": "def set_generic_model_params(discrim_weights, discrim_meta):\n    if discrim_weights is None:\n        raise ValueError('When using a generic discriminator, discrim_weights need to be specified')\n    if discrim_meta is None:\n        raise ValueError('When using a generic discriminator, discrim_meta need to be specified')\n    with open(discrim_meta, 'r') as discrim_meta_file:\n        meta = json.load(discrim_meta_file)\n    meta['path'] = discrim_weights\n    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta",
        "mutated": [
            "def set_generic_model_params(discrim_weights, discrim_meta):\n    if False:\n        i = 10\n    if discrim_weights is None:\n        raise ValueError('When using a generic discriminator, discrim_weights need to be specified')\n    if discrim_meta is None:\n        raise ValueError('When using a generic discriminator, discrim_meta need to be specified')\n    with open(discrim_meta, 'r') as discrim_meta_file:\n        meta = json.load(discrim_meta_file)\n    meta['path'] = discrim_weights\n    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta",
            "def set_generic_model_params(discrim_weights, discrim_meta):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if discrim_weights is None:\n        raise ValueError('When using a generic discriminator, discrim_weights need to be specified')\n    if discrim_meta is None:\n        raise ValueError('When using a generic discriminator, discrim_meta need to be specified')\n    with open(discrim_meta, 'r') as discrim_meta_file:\n        meta = json.load(discrim_meta_file)\n    meta['path'] = discrim_weights\n    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta",
            "def set_generic_model_params(discrim_weights, discrim_meta):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if discrim_weights is None:\n        raise ValueError('When using a generic discriminator, discrim_weights need to be specified')\n    if discrim_meta is None:\n        raise ValueError('When using a generic discriminator, discrim_meta need to be specified')\n    with open(discrim_meta, 'r') as discrim_meta_file:\n        meta = json.load(discrim_meta_file)\n    meta['path'] = discrim_weights\n    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta",
            "def set_generic_model_params(discrim_weights, discrim_meta):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if discrim_weights is None:\n        raise ValueError('When using a generic discriminator, discrim_weights need to be specified')\n    if discrim_meta is None:\n        raise ValueError('When using a generic discriminator, discrim_meta need to be specified')\n    with open(discrim_meta, 'r') as discrim_meta_file:\n        meta = json.load(discrim_meta_file)\n    meta['path'] = discrim_weights\n    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta",
            "def set_generic_model_params(discrim_weights, discrim_meta):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if discrim_weights is None:\n        raise ValueError('When using a generic discriminator, discrim_weights need to be specified')\n    if discrim_meta is None:\n        raise ValueError('When using a generic discriminator, discrim_meta need to be specified')\n    with open(discrim_meta, 'r') as discrim_meta_file:\n        meta = json.load(discrim_meta_file)\n    meta['path'] = discrim_weights\n    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta"
        ]
    },
    {
        "func_name": "run_pplm_example",
        "original": "def run_pplm_example(pretrained_model='gpt2-medium', cond_text='', uncond=False, num_samples=1, bag_of_words=None, discrim=None, discrim_weights=None, discrim_meta=None, class_label=-1, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, seed=0, no_cuda=False, colorama=False, repetition_penalty=1.0):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    device = 'cuda' if torch.cuda.is_available() and (not no_cuda) else 'cpu'\n    if discrim == 'generic':\n        set_generic_model_params(discrim_weights, discrim_meta)\n    if discrim is not None:\n        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim]['pretrained_model']\n        print(\"discrim = {}, pretrained_model set to discriminator's = {}\".format(discrim, pretrained_model))\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n    model.to(device)\n    model.eval()\n    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n    for param in model.parameters():\n        param.requires_grad = False\n    if uncond:\n        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\n    else:\n        raw_text = cond_text\n        while not raw_text:\n            print('Did you forget to add `--cond_text`? ')\n            raw_text = input('Model prompt >>> ')\n        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\n    print('= Prefix of sentence =')\n    print(tokenizer.decode(tokenized_cond_text))\n    print()\n    (unpert_gen_tok_text, pert_gen_tok_texts, _, _) = full_text_generation(model=model, tokenizer=tokenizer, context=tokenized_cond_text, device=device, num_samples=num_samples, bag_of_words=bag_of_words, discrim=discrim, class_label=class_label, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n    print('=' * 80)\n    print('= Unperturbed generated text =')\n    print(unpert_gen_text)\n    print()\n    generated_texts = []\n    bow_word_ids = set()\n    if bag_of_words and colorama:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n        for single_bow_list in bow_indices:\n            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n            bow_word_ids.update((w[0] for w in filtered))\n    for (i, pert_gen_tok_text) in enumerate(pert_gen_tok_texts):\n        try:\n            if colorama:\n                import colorama\n                pert_gen_text = ''\n                for word_id in pert_gen_tok_text.tolist()[0]:\n                    if word_id in bow_word_ids:\n                        pert_gen_text += '{}{}{}'.format(colorama.Fore.RED, tokenizer.decode([word_id]), colorama.Style.RESET_ALL)\n                    else:\n                        pert_gen_text += tokenizer.decode([word_id])\n            else:\n                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n            print('= Perturbed generated text {} ='.format(i + 1))\n            print(pert_gen_text)\n            print()\n        except Exception as exc:\n            print('Ignoring error while generating perturbed text:', exc)\n        generated_texts.append((tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text))\n    return",
        "mutated": [
            "def run_pplm_example(pretrained_model='gpt2-medium', cond_text='', uncond=False, num_samples=1, bag_of_words=None, discrim=None, discrim_weights=None, discrim_meta=None, class_label=-1, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, seed=0, no_cuda=False, colorama=False, repetition_penalty=1.0):\n    if False:\n        i = 10\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    device = 'cuda' if torch.cuda.is_available() and (not no_cuda) else 'cpu'\n    if discrim == 'generic':\n        set_generic_model_params(discrim_weights, discrim_meta)\n    if discrim is not None:\n        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim]['pretrained_model']\n        print(\"discrim = {}, pretrained_model set to discriminator's = {}\".format(discrim, pretrained_model))\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n    model.to(device)\n    model.eval()\n    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n    for param in model.parameters():\n        param.requires_grad = False\n    if uncond:\n        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\n    else:\n        raw_text = cond_text\n        while not raw_text:\n            print('Did you forget to add `--cond_text`? ')\n            raw_text = input('Model prompt >>> ')\n        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\n    print('= Prefix of sentence =')\n    print(tokenizer.decode(tokenized_cond_text))\n    print()\n    (unpert_gen_tok_text, pert_gen_tok_texts, _, _) = full_text_generation(model=model, tokenizer=tokenizer, context=tokenized_cond_text, device=device, num_samples=num_samples, bag_of_words=bag_of_words, discrim=discrim, class_label=class_label, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n    print('=' * 80)\n    print('= Unperturbed generated text =')\n    print(unpert_gen_text)\n    print()\n    generated_texts = []\n    bow_word_ids = set()\n    if bag_of_words and colorama:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n        for single_bow_list in bow_indices:\n            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n            bow_word_ids.update((w[0] for w in filtered))\n    for (i, pert_gen_tok_text) in enumerate(pert_gen_tok_texts):\n        try:\n            if colorama:\n                import colorama\n                pert_gen_text = ''\n                for word_id in pert_gen_tok_text.tolist()[0]:\n                    if word_id in bow_word_ids:\n                        pert_gen_text += '{}{}{}'.format(colorama.Fore.RED, tokenizer.decode([word_id]), colorama.Style.RESET_ALL)\n                    else:\n                        pert_gen_text += tokenizer.decode([word_id])\n            else:\n                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n            print('= Perturbed generated text {} ='.format(i + 1))\n            print(pert_gen_text)\n            print()\n        except Exception as exc:\n            print('Ignoring error while generating perturbed text:', exc)\n        generated_texts.append((tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text))\n    return",
            "def run_pplm_example(pretrained_model='gpt2-medium', cond_text='', uncond=False, num_samples=1, bag_of_words=None, discrim=None, discrim_weights=None, discrim_meta=None, class_label=-1, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, seed=0, no_cuda=False, colorama=False, repetition_penalty=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    device = 'cuda' if torch.cuda.is_available() and (not no_cuda) else 'cpu'\n    if discrim == 'generic':\n        set_generic_model_params(discrim_weights, discrim_meta)\n    if discrim is not None:\n        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim]['pretrained_model']\n        print(\"discrim = {}, pretrained_model set to discriminator's = {}\".format(discrim, pretrained_model))\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n    model.to(device)\n    model.eval()\n    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n    for param in model.parameters():\n        param.requires_grad = False\n    if uncond:\n        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\n    else:\n        raw_text = cond_text\n        while not raw_text:\n            print('Did you forget to add `--cond_text`? ')\n            raw_text = input('Model prompt >>> ')\n        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\n    print('= Prefix of sentence =')\n    print(tokenizer.decode(tokenized_cond_text))\n    print()\n    (unpert_gen_tok_text, pert_gen_tok_texts, _, _) = full_text_generation(model=model, tokenizer=tokenizer, context=tokenized_cond_text, device=device, num_samples=num_samples, bag_of_words=bag_of_words, discrim=discrim, class_label=class_label, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n    print('=' * 80)\n    print('= Unperturbed generated text =')\n    print(unpert_gen_text)\n    print()\n    generated_texts = []\n    bow_word_ids = set()\n    if bag_of_words and colorama:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n        for single_bow_list in bow_indices:\n            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n            bow_word_ids.update((w[0] for w in filtered))\n    for (i, pert_gen_tok_text) in enumerate(pert_gen_tok_texts):\n        try:\n            if colorama:\n                import colorama\n                pert_gen_text = ''\n                for word_id in pert_gen_tok_text.tolist()[0]:\n                    if word_id in bow_word_ids:\n                        pert_gen_text += '{}{}{}'.format(colorama.Fore.RED, tokenizer.decode([word_id]), colorama.Style.RESET_ALL)\n                    else:\n                        pert_gen_text += tokenizer.decode([word_id])\n            else:\n                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n            print('= Perturbed generated text {} ='.format(i + 1))\n            print(pert_gen_text)\n            print()\n        except Exception as exc:\n            print('Ignoring error while generating perturbed text:', exc)\n        generated_texts.append((tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text))\n    return",
            "def run_pplm_example(pretrained_model='gpt2-medium', cond_text='', uncond=False, num_samples=1, bag_of_words=None, discrim=None, discrim_weights=None, discrim_meta=None, class_label=-1, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, seed=0, no_cuda=False, colorama=False, repetition_penalty=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    device = 'cuda' if torch.cuda.is_available() and (not no_cuda) else 'cpu'\n    if discrim == 'generic':\n        set_generic_model_params(discrim_weights, discrim_meta)\n    if discrim is not None:\n        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim]['pretrained_model']\n        print(\"discrim = {}, pretrained_model set to discriminator's = {}\".format(discrim, pretrained_model))\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n    model.to(device)\n    model.eval()\n    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n    for param in model.parameters():\n        param.requires_grad = False\n    if uncond:\n        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\n    else:\n        raw_text = cond_text\n        while not raw_text:\n            print('Did you forget to add `--cond_text`? ')\n            raw_text = input('Model prompt >>> ')\n        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\n    print('= Prefix of sentence =')\n    print(tokenizer.decode(tokenized_cond_text))\n    print()\n    (unpert_gen_tok_text, pert_gen_tok_texts, _, _) = full_text_generation(model=model, tokenizer=tokenizer, context=tokenized_cond_text, device=device, num_samples=num_samples, bag_of_words=bag_of_words, discrim=discrim, class_label=class_label, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n    print('=' * 80)\n    print('= Unperturbed generated text =')\n    print(unpert_gen_text)\n    print()\n    generated_texts = []\n    bow_word_ids = set()\n    if bag_of_words and colorama:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n        for single_bow_list in bow_indices:\n            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n            bow_word_ids.update((w[0] for w in filtered))\n    for (i, pert_gen_tok_text) in enumerate(pert_gen_tok_texts):\n        try:\n            if colorama:\n                import colorama\n                pert_gen_text = ''\n                for word_id in pert_gen_tok_text.tolist()[0]:\n                    if word_id in bow_word_ids:\n                        pert_gen_text += '{}{}{}'.format(colorama.Fore.RED, tokenizer.decode([word_id]), colorama.Style.RESET_ALL)\n                    else:\n                        pert_gen_text += tokenizer.decode([word_id])\n            else:\n                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n            print('= Perturbed generated text {} ='.format(i + 1))\n            print(pert_gen_text)\n            print()\n        except Exception as exc:\n            print('Ignoring error while generating perturbed text:', exc)\n        generated_texts.append((tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text))\n    return",
            "def run_pplm_example(pretrained_model='gpt2-medium', cond_text='', uncond=False, num_samples=1, bag_of_words=None, discrim=None, discrim_weights=None, discrim_meta=None, class_label=-1, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, seed=0, no_cuda=False, colorama=False, repetition_penalty=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    device = 'cuda' if torch.cuda.is_available() and (not no_cuda) else 'cpu'\n    if discrim == 'generic':\n        set_generic_model_params(discrim_weights, discrim_meta)\n    if discrim is not None:\n        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim]['pretrained_model']\n        print(\"discrim = {}, pretrained_model set to discriminator's = {}\".format(discrim, pretrained_model))\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n    model.to(device)\n    model.eval()\n    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n    for param in model.parameters():\n        param.requires_grad = False\n    if uncond:\n        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\n    else:\n        raw_text = cond_text\n        while not raw_text:\n            print('Did you forget to add `--cond_text`? ')\n            raw_text = input('Model prompt >>> ')\n        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\n    print('= Prefix of sentence =')\n    print(tokenizer.decode(tokenized_cond_text))\n    print()\n    (unpert_gen_tok_text, pert_gen_tok_texts, _, _) = full_text_generation(model=model, tokenizer=tokenizer, context=tokenized_cond_text, device=device, num_samples=num_samples, bag_of_words=bag_of_words, discrim=discrim, class_label=class_label, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n    print('=' * 80)\n    print('= Unperturbed generated text =')\n    print(unpert_gen_text)\n    print()\n    generated_texts = []\n    bow_word_ids = set()\n    if bag_of_words and colorama:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n        for single_bow_list in bow_indices:\n            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n            bow_word_ids.update((w[0] for w in filtered))\n    for (i, pert_gen_tok_text) in enumerate(pert_gen_tok_texts):\n        try:\n            if colorama:\n                import colorama\n                pert_gen_text = ''\n                for word_id in pert_gen_tok_text.tolist()[0]:\n                    if word_id in bow_word_ids:\n                        pert_gen_text += '{}{}{}'.format(colorama.Fore.RED, tokenizer.decode([word_id]), colorama.Style.RESET_ALL)\n                    else:\n                        pert_gen_text += tokenizer.decode([word_id])\n            else:\n                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n            print('= Perturbed generated text {} ='.format(i + 1))\n            print(pert_gen_text)\n            print()\n        except Exception as exc:\n            print('Ignoring error while generating perturbed text:', exc)\n        generated_texts.append((tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text))\n    return",
            "def run_pplm_example(pretrained_model='gpt2-medium', cond_text='', uncond=False, num_samples=1, bag_of_words=None, discrim=None, discrim_weights=None, discrim_meta=None, class_label=-1, length=100, stepsize=0.02, temperature=1.0, top_k=10, sample=False, num_iterations=3, grad_length=10000, horizon_length=1, window_length=0, decay=False, gamma=1.5, gm_scale=0.9, kl_scale=0.01, seed=0, no_cuda=False, colorama=False, repetition_penalty=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    device = 'cuda' if torch.cuda.is_available() and (not no_cuda) else 'cpu'\n    if discrim == 'generic':\n        set_generic_model_params(discrim_weights, discrim_meta)\n    if discrim is not None:\n        pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim]['pretrained_model']\n        print(\"discrim = {}, pretrained_model set to discriminator's = {}\".format(discrim, pretrained_model))\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model, output_hidden_states=True)\n    model.to(device)\n    model.eval()\n    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n    for param in model.parameters():\n        param.requires_grad = False\n    if uncond:\n        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token])\n    else:\n        raw_text = cond_text\n        while not raw_text:\n            print('Did you forget to add `--cond_text`? ')\n            raw_text = input('Model prompt >>> ')\n        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text)\n    print('= Prefix of sentence =')\n    print(tokenizer.decode(tokenized_cond_text))\n    print()\n    (unpert_gen_tok_text, pert_gen_tok_texts, _, _) = full_text_generation(model=model, tokenizer=tokenizer, context=tokenized_cond_text, device=device, num_samples=num_samples, bag_of_words=bag_of_words, discrim=discrim, class_label=class_label, length=length, stepsize=stepsize, temperature=temperature, top_k=top_k, sample=sample, num_iterations=num_iterations, grad_length=grad_length, horizon_length=horizon_length, window_length=window_length, decay=decay, gamma=gamma, gm_scale=gm_scale, kl_scale=kl_scale, repetition_penalty=repetition_penalty)\n    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n    print('=' * 80)\n    print('= Unperturbed generated text =')\n    print(unpert_gen_text)\n    print()\n    generated_texts = []\n    bow_word_ids = set()\n    if bag_of_words and colorama:\n        bow_indices = get_bag_of_words_indices(bag_of_words.split(';'), tokenizer)\n        for single_bow_list in bow_indices:\n            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n            bow_word_ids.update((w[0] for w in filtered))\n    for (i, pert_gen_tok_text) in enumerate(pert_gen_tok_texts):\n        try:\n            if colorama:\n                import colorama\n                pert_gen_text = ''\n                for word_id in pert_gen_tok_text.tolist()[0]:\n                    if word_id in bow_word_ids:\n                        pert_gen_text += '{}{}{}'.format(colorama.Fore.RED, tokenizer.decode([word_id]), colorama.Style.RESET_ALL)\n                    else:\n                        pert_gen_text += tokenizer.decode([word_id])\n            else:\n                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n            print('= Perturbed generated text {} ='.format(i + 1))\n            print(pert_gen_text)\n            print()\n        except Exception as exc:\n            print('Ignoring error while generating perturbed text:', exc)\n        generated_texts.append((tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text))\n    return"
        ]
    }
]