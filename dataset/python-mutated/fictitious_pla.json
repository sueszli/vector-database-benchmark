[
    {
        "func_name": "__init__",
        "original": "def __init__(self, game, player_ids: List[int], policies: List[policy_std.Policy], distributions: List[distribution_std.Distribution], weights: List[float]):\n    \"\"\"Initializes the merged policy.\n\n    Args:\n      game: The game to analyze.\n      player_ids: list of player ids for which this policy applies; each should\n        be in the range 0..game.num_players()-1.\n      policies: A `List[policy_std.Policy]` object.\n      distributions: A `List[distribution_std.Distribution]` object.\n      weights: A `List[float]` object. The elements should sum to 1.\n    \"\"\"\n    super().__init__(game, player_ids)\n    self._policies = policies\n    self._distributions = distributions\n    self._weights = weights\n    assert len(policies) == len(distributions), f'Length mismatch {len(policies)} != {len(distributions)}'\n    assert len(policies) == len(weights), f'Length mismatch {len(policies)} != {len(weights)}'\n    assert math.isclose(sum(weights), 1.0), f'Weights should sum to 1, but instead sum to {sum(weights)}'",
        "mutated": [
            "def __init__(self, game, player_ids: List[int], policies: List[policy_std.Policy], distributions: List[distribution_std.Distribution], weights: List[float]):\n    if False:\n        i = 10\n    'Initializes the merged policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n      policies: A `List[policy_std.Policy]` object.\\n      distributions: A `List[distribution_std.Distribution]` object.\\n      weights: A `List[float]` object. The elements should sum to 1.\\n    '\n    super().__init__(game, player_ids)\n    self._policies = policies\n    self._distributions = distributions\n    self._weights = weights\n    assert len(policies) == len(distributions), f'Length mismatch {len(policies)} != {len(distributions)}'\n    assert len(policies) == len(weights), f'Length mismatch {len(policies)} != {len(weights)}'\n    assert math.isclose(sum(weights), 1.0), f'Weights should sum to 1, but instead sum to {sum(weights)}'",
            "def __init__(self, game, player_ids: List[int], policies: List[policy_std.Policy], distributions: List[distribution_std.Distribution], weights: List[float]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes the merged policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n      policies: A `List[policy_std.Policy]` object.\\n      distributions: A `List[distribution_std.Distribution]` object.\\n      weights: A `List[float]` object. The elements should sum to 1.\\n    '\n    super().__init__(game, player_ids)\n    self._policies = policies\n    self._distributions = distributions\n    self._weights = weights\n    assert len(policies) == len(distributions), f'Length mismatch {len(policies)} != {len(distributions)}'\n    assert len(policies) == len(weights), f'Length mismatch {len(policies)} != {len(weights)}'\n    assert math.isclose(sum(weights), 1.0), f'Weights should sum to 1, but instead sum to {sum(weights)}'",
            "def __init__(self, game, player_ids: List[int], policies: List[policy_std.Policy], distributions: List[distribution_std.Distribution], weights: List[float]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes the merged policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n      policies: A `List[policy_std.Policy]` object.\\n      distributions: A `List[distribution_std.Distribution]` object.\\n      weights: A `List[float]` object. The elements should sum to 1.\\n    '\n    super().__init__(game, player_ids)\n    self._policies = policies\n    self._distributions = distributions\n    self._weights = weights\n    assert len(policies) == len(distributions), f'Length mismatch {len(policies)} != {len(distributions)}'\n    assert len(policies) == len(weights), f'Length mismatch {len(policies)} != {len(weights)}'\n    assert math.isclose(sum(weights), 1.0), f'Weights should sum to 1, but instead sum to {sum(weights)}'",
            "def __init__(self, game, player_ids: List[int], policies: List[policy_std.Policy], distributions: List[distribution_std.Distribution], weights: List[float]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes the merged policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n      policies: A `List[policy_std.Policy]` object.\\n      distributions: A `List[distribution_std.Distribution]` object.\\n      weights: A `List[float]` object. The elements should sum to 1.\\n    '\n    super().__init__(game, player_ids)\n    self._policies = policies\n    self._distributions = distributions\n    self._weights = weights\n    assert len(policies) == len(distributions), f'Length mismatch {len(policies)} != {len(distributions)}'\n    assert len(policies) == len(weights), f'Length mismatch {len(policies)} != {len(weights)}'\n    assert math.isclose(sum(weights), 1.0), f'Weights should sum to 1, but instead sum to {sum(weights)}'",
            "def __init__(self, game, player_ids: List[int], policies: List[policy_std.Policy], distributions: List[distribution_std.Distribution], weights: List[float]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes the merged policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n      policies: A `List[policy_std.Policy]` object.\\n      distributions: A `List[distribution_std.Distribution]` object.\\n      weights: A `List[float]` object. The elements should sum to 1.\\n    '\n    super().__init__(game, player_ids)\n    self._policies = policies\n    self._distributions = distributions\n    self._weights = weights\n    assert len(policies) == len(distributions), f'Length mismatch {len(policies)} != {len(distributions)}'\n    assert len(policies) == len(weights), f'Length mismatch {len(policies)} != {len(weights)}'\n    assert math.isclose(sum(weights), 1.0), f'Weights should sum to 1, but instead sum to {sum(weights)}'"
        ]
    },
    {
        "func_name": "action_probabilities",
        "original": "def action_probabilities(self, state, player_id=None):\n    action_prob = []\n    legal = state.legal_actions()\n    num_legal = len(legal)\n    for a in legal:\n        merged_pi = 0.0\n        norm_merged_pi = 0.0\n        for (p, d, w) in zip(self._policies, self._distributions, self._weights):\n            try:\n                merged_pi += w * d(state) * p(state)[a]\n                norm_merged_pi += w * d(state)\n            except (KeyError, ValueError):\n                pass\n        if norm_merged_pi > 0.0:\n            action_prob.append((a, merged_pi / norm_merged_pi))\n        else:\n            action_prob.append((a, 1.0 / num_legal))\n    return dict(action_prob)",
        "mutated": [
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n    action_prob = []\n    legal = state.legal_actions()\n    num_legal = len(legal)\n    for a in legal:\n        merged_pi = 0.0\n        norm_merged_pi = 0.0\n        for (p, d, w) in zip(self._policies, self._distributions, self._weights):\n            try:\n                merged_pi += w * d(state) * p(state)[a]\n                norm_merged_pi += w * d(state)\n            except (KeyError, ValueError):\n                pass\n        if norm_merged_pi > 0.0:\n            action_prob.append((a, merged_pi / norm_merged_pi))\n        else:\n            action_prob.append((a, 1.0 / num_legal))\n    return dict(action_prob)",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    action_prob = []\n    legal = state.legal_actions()\n    num_legal = len(legal)\n    for a in legal:\n        merged_pi = 0.0\n        norm_merged_pi = 0.0\n        for (p, d, w) in zip(self._policies, self._distributions, self._weights):\n            try:\n                merged_pi += w * d(state) * p(state)[a]\n                norm_merged_pi += w * d(state)\n            except (KeyError, ValueError):\n                pass\n        if norm_merged_pi > 0.0:\n            action_prob.append((a, merged_pi / norm_merged_pi))\n        else:\n            action_prob.append((a, 1.0 / num_legal))\n    return dict(action_prob)",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    action_prob = []\n    legal = state.legal_actions()\n    num_legal = len(legal)\n    for a in legal:\n        merged_pi = 0.0\n        norm_merged_pi = 0.0\n        for (p, d, w) in zip(self._policies, self._distributions, self._weights):\n            try:\n                merged_pi += w * d(state) * p(state)[a]\n                norm_merged_pi += w * d(state)\n            except (KeyError, ValueError):\n                pass\n        if norm_merged_pi > 0.0:\n            action_prob.append((a, merged_pi / norm_merged_pi))\n        else:\n            action_prob.append((a, 1.0 / num_legal))\n    return dict(action_prob)",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    action_prob = []\n    legal = state.legal_actions()\n    num_legal = len(legal)\n    for a in legal:\n        merged_pi = 0.0\n        norm_merged_pi = 0.0\n        for (p, d, w) in zip(self._policies, self._distributions, self._weights):\n            try:\n                merged_pi += w * d(state) * p(state)[a]\n                norm_merged_pi += w * d(state)\n            except (KeyError, ValueError):\n                pass\n        if norm_merged_pi > 0.0:\n            action_prob.append((a, merged_pi / norm_merged_pi))\n        else:\n            action_prob.append((a, 1.0 / num_legal))\n    return dict(action_prob)",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    action_prob = []\n    legal = state.legal_actions()\n    num_legal = len(legal)\n    for a in legal:\n        merged_pi = 0.0\n        norm_merged_pi = 0.0\n        for (p, d, w) in zip(self._policies, self._distributions, self._weights):\n            try:\n                merged_pi += w * d(state) * p(state)[a]\n                norm_merged_pi += w * d(state)\n            except (KeyError, ValueError):\n                pass\n        if norm_merged_pi > 0.0:\n            action_prob.append((a, merged_pi / norm_merged_pi))\n        else:\n            action_prob.append((a, 1.0 / num_legal))\n    return dict(action_prob)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, game: pyspiel.Game, lr: Optional[float]=None, temperature: Optional[float]=None):\n    \"\"\"Initializes the greedy policy.\n\n    Args:\n      game: The game to analyze.\n      lr: The learning rate of mirror descent. If None, at iteration i it will\n        be set to 1/i.\n      temperature: If set, then instead of the greedy policy a softmax policy\n        with the specified temperature will be used to update the policy at each\n        iteration.\n    \"\"\"\n    self._game = game\n    self._lr = lr\n    self._temperature = temperature\n    self._policy = policy_std.UniformRandomPolicy(self._game)\n    self._fp_step = 0",
        "mutated": [
            "def __init__(self, game: pyspiel.Game, lr: Optional[float]=None, temperature: Optional[float]=None):\n    if False:\n        i = 10\n    'Initializes the greedy policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      lr: The learning rate of mirror descent. If None, at iteration i it will\\n        be set to 1/i.\\n      temperature: If set, then instead of the greedy policy a softmax policy\\n        with the specified temperature will be used to update the policy at each\\n        iteration.\\n    '\n    self._game = game\n    self._lr = lr\n    self._temperature = temperature\n    self._policy = policy_std.UniformRandomPolicy(self._game)\n    self._fp_step = 0",
            "def __init__(self, game: pyspiel.Game, lr: Optional[float]=None, temperature: Optional[float]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes the greedy policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      lr: The learning rate of mirror descent. If None, at iteration i it will\\n        be set to 1/i.\\n      temperature: If set, then instead of the greedy policy a softmax policy\\n        with the specified temperature will be used to update the policy at each\\n        iteration.\\n    '\n    self._game = game\n    self._lr = lr\n    self._temperature = temperature\n    self._policy = policy_std.UniformRandomPolicy(self._game)\n    self._fp_step = 0",
            "def __init__(self, game: pyspiel.Game, lr: Optional[float]=None, temperature: Optional[float]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes the greedy policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      lr: The learning rate of mirror descent. If None, at iteration i it will\\n        be set to 1/i.\\n      temperature: If set, then instead of the greedy policy a softmax policy\\n        with the specified temperature will be used to update the policy at each\\n        iteration.\\n    '\n    self._game = game\n    self._lr = lr\n    self._temperature = temperature\n    self._policy = policy_std.UniformRandomPolicy(self._game)\n    self._fp_step = 0",
            "def __init__(self, game: pyspiel.Game, lr: Optional[float]=None, temperature: Optional[float]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes the greedy policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      lr: The learning rate of mirror descent. If None, at iteration i it will\\n        be set to 1/i.\\n      temperature: If set, then instead of the greedy policy a softmax policy\\n        with the specified temperature will be used to update the policy at each\\n        iteration.\\n    '\n    self._game = game\n    self._lr = lr\n    self._temperature = temperature\n    self._policy = policy_std.UniformRandomPolicy(self._game)\n    self._fp_step = 0",
            "def __init__(self, game: pyspiel.Game, lr: Optional[float]=None, temperature: Optional[float]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes the greedy policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      lr: The learning rate of mirror descent. If None, at iteration i it will\\n        be set to 1/i.\\n      temperature: If set, then instead of the greedy policy a softmax policy\\n        with the specified temperature will be used to update the policy at each\\n        iteration.\\n    '\n    self._game = game\n    self._lr = lr\n    self._temperature = temperature\n    self._policy = policy_std.UniformRandomPolicy(self._game)\n    self._fp_step = 0"
        ]
    },
    {
        "func_name": "get_policy",
        "original": "def get_policy(self):\n    return self._policy",
        "mutated": [
            "def get_policy(self):\n    if False:\n        i = 10\n    return self._policy",
            "def get_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._policy",
            "def get_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._policy",
            "def get_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._policy",
            "def get_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._policy"
        ]
    },
    {
        "func_name": "iteration",
        "original": "def iteration(self, br_policy=None, learning_rate=None):\n    \"\"\"Returns a new `TabularPolicy` equivalent to this policy.\n\n    Args:\n      br_policy: Policy to compute the best response value for each iteration.\n        If none provided, the exact value is computed.\n      learning_rate: The learning rate.\n    \"\"\"\n    self._fp_step += 1\n    distrib = distribution.DistributionPolicy(self._game, self._policy)\n    if br_policy:\n        br_value = policy_value.PolicyValue(self._game, distrib, br_policy)\n    else:\n        br_value = best_response_value.BestResponse(self._game, distrib, value.TabularValueFunction(self._game))\n    player_ids = list(range(self._game.num_players()))\n    if self._temperature is None:\n        pi = greedy_policy.GreedyPolicy(self._game, player_ids, br_value)\n    else:\n        pi = softmax_policy.SoftmaxPolicy(self._game, player_ids, self._temperature, br_value)\n    pi = pi.to_tabular()\n    distrib_pi = distribution.DistributionPolicy(self._game, pi)\n    if learning_rate:\n        weight = learning_rate\n    else:\n        weight = self._lr if self._lr else 1.0 / (self._fp_step + 1)\n    if math.isclose(weight, 1.0):\n        self._policy = pi\n    else:\n        self._policy = MergedPolicy(self._game, player_ids, [self._policy, pi], [distrib, distrib_pi], [1.0 - weight, weight]).to_tabular()",
        "mutated": [
            "def iteration(self, br_policy=None, learning_rate=None):\n    if False:\n        i = 10\n    'Returns a new `TabularPolicy` equivalent to this policy.\\n\\n    Args:\\n      br_policy: Policy to compute the best response value for each iteration.\\n        If none provided, the exact value is computed.\\n      learning_rate: The learning rate.\\n    '\n    self._fp_step += 1\n    distrib = distribution.DistributionPolicy(self._game, self._policy)\n    if br_policy:\n        br_value = policy_value.PolicyValue(self._game, distrib, br_policy)\n    else:\n        br_value = best_response_value.BestResponse(self._game, distrib, value.TabularValueFunction(self._game))\n    player_ids = list(range(self._game.num_players()))\n    if self._temperature is None:\n        pi = greedy_policy.GreedyPolicy(self._game, player_ids, br_value)\n    else:\n        pi = softmax_policy.SoftmaxPolicy(self._game, player_ids, self._temperature, br_value)\n    pi = pi.to_tabular()\n    distrib_pi = distribution.DistributionPolicy(self._game, pi)\n    if learning_rate:\n        weight = learning_rate\n    else:\n        weight = self._lr if self._lr else 1.0 / (self._fp_step + 1)\n    if math.isclose(weight, 1.0):\n        self._policy = pi\n    else:\n        self._policy = MergedPolicy(self._game, player_ids, [self._policy, pi], [distrib, distrib_pi], [1.0 - weight, weight]).to_tabular()",
            "def iteration(self, br_policy=None, learning_rate=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a new `TabularPolicy` equivalent to this policy.\\n\\n    Args:\\n      br_policy: Policy to compute the best response value for each iteration.\\n        If none provided, the exact value is computed.\\n      learning_rate: The learning rate.\\n    '\n    self._fp_step += 1\n    distrib = distribution.DistributionPolicy(self._game, self._policy)\n    if br_policy:\n        br_value = policy_value.PolicyValue(self._game, distrib, br_policy)\n    else:\n        br_value = best_response_value.BestResponse(self._game, distrib, value.TabularValueFunction(self._game))\n    player_ids = list(range(self._game.num_players()))\n    if self._temperature is None:\n        pi = greedy_policy.GreedyPolicy(self._game, player_ids, br_value)\n    else:\n        pi = softmax_policy.SoftmaxPolicy(self._game, player_ids, self._temperature, br_value)\n    pi = pi.to_tabular()\n    distrib_pi = distribution.DistributionPolicy(self._game, pi)\n    if learning_rate:\n        weight = learning_rate\n    else:\n        weight = self._lr if self._lr else 1.0 / (self._fp_step + 1)\n    if math.isclose(weight, 1.0):\n        self._policy = pi\n    else:\n        self._policy = MergedPolicy(self._game, player_ids, [self._policy, pi], [distrib, distrib_pi], [1.0 - weight, weight]).to_tabular()",
            "def iteration(self, br_policy=None, learning_rate=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a new `TabularPolicy` equivalent to this policy.\\n\\n    Args:\\n      br_policy: Policy to compute the best response value for each iteration.\\n        If none provided, the exact value is computed.\\n      learning_rate: The learning rate.\\n    '\n    self._fp_step += 1\n    distrib = distribution.DistributionPolicy(self._game, self._policy)\n    if br_policy:\n        br_value = policy_value.PolicyValue(self._game, distrib, br_policy)\n    else:\n        br_value = best_response_value.BestResponse(self._game, distrib, value.TabularValueFunction(self._game))\n    player_ids = list(range(self._game.num_players()))\n    if self._temperature is None:\n        pi = greedy_policy.GreedyPolicy(self._game, player_ids, br_value)\n    else:\n        pi = softmax_policy.SoftmaxPolicy(self._game, player_ids, self._temperature, br_value)\n    pi = pi.to_tabular()\n    distrib_pi = distribution.DistributionPolicy(self._game, pi)\n    if learning_rate:\n        weight = learning_rate\n    else:\n        weight = self._lr if self._lr else 1.0 / (self._fp_step + 1)\n    if math.isclose(weight, 1.0):\n        self._policy = pi\n    else:\n        self._policy = MergedPolicy(self._game, player_ids, [self._policy, pi], [distrib, distrib_pi], [1.0 - weight, weight]).to_tabular()",
            "def iteration(self, br_policy=None, learning_rate=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a new `TabularPolicy` equivalent to this policy.\\n\\n    Args:\\n      br_policy: Policy to compute the best response value for each iteration.\\n        If none provided, the exact value is computed.\\n      learning_rate: The learning rate.\\n    '\n    self._fp_step += 1\n    distrib = distribution.DistributionPolicy(self._game, self._policy)\n    if br_policy:\n        br_value = policy_value.PolicyValue(self._game, distrib, br_policy)\n    else:\n        br_value = best_response_value.BestResponse(self._game, distrib, value.TabularValueFunction(self._game))\n    player_ids = list(range(self._game.num_players()))\n    if self._temperature is None:\n        pi = greedy_policy.GreedyPolicy(self._game, player_ids, br_value)\n    else:\n        pi = softmax_policy.SoftmaxPolicy(self._game, player_ids, self._temperature, br_value)\n    pi = pi.to_tabular()\n    distrib_pi = distribution.DistributionPolicy(self._game, pi)\n    if learning_rate:\n        weight = learning_rate\n    else:\n        weight = self._lr if self._lr else 1.0 / (self._fp_step + 1)\n    if math.isclose(weight, 1.0):\n        self._policy = pi\n    else:\n        self._policy = MergedPolicy(self._game, player_ids, [self._policy, pi], [distrib, distrib_pi], [1.0 - weight, weight]).to_tabular()",
            "def iteration(self, br_policy=None, learning_rate=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a new `TabularPolicy` equivalent to this policy.\\n\\n    Args:\\n      br_policy: Policy to compute the best response value for each iteration.\\n        If none provided, the exact value is computed.\\n      learning_rate: The learning rate.\\n    '\n    self._fp_step += 1\n    distrib = distribution.DistributionPolicy(self._game, self._policy)\n    if br_policy:\n        br_value = policy_value.PolicyValue(self._game, distrib, br_policy)\n    else:\n        br_value = best_response_value.BestResponse(self._game, distrib, value.TabularValueFunction(self._game))\n    player_ids = list(range(self._game.num_players()))\n    if self._temperature is None:\n        pi = greedy_policy.GreedyPolicy(self._game, player_ids, br_value)\n    else:\n        pi = softmax_policy.SoftmaxPolicy(self._game, player_ids, self._temperature, br_value)\n    pi = pi.to_tabular()\n    distrib_pi = distribution.DistributionPolicy(self._game, pi)\n    if learning_rate:\n        weight = learning_rate\n    else:\n        weight = self._lr if self._lr else 1.0 / (self._fp_step + 1)\n    if math.isclose(weight, 1.0):\n        self._policy = pi\n    else:\n        self._policy = MergedPolicy(self._game, player_ids, [self._policy, pi], [distrib, distrib_pi], [1.0 - weight, weight]).to_tabular()"
        ]
    }
]