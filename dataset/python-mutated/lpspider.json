[
    {
        "func_name": "parse",
        "original": "def parse(self, response):\n    text = response.text\n    company_name = response.xpath('//div[@class=\"name-and-welfare\"]//h1/text()')[0].extract()\n    comp_sum_tag = response.xpath('//div[@class=\"comp-summary-tag\"]/a/text()').extract()\n    stage = comp_sum_tag[0]\n    size = comp_sum_tag[1]\n    city = comp_sum_tag[2]\n    industry = comp_sum_tag[3]\n    comp_clearfix = str(response.xpath('//ul[@class=\"comp-tag-list clearfix\"]//span/text()').extract())\n    rate_num = response.xpath('//p[@class=\"rate-num\"]//span/text()')[0].extract()\n    rate_num = int(rate_num) / 100\n    job_count = int(re.search('<small data-selector=\"total\">. \u5171([0-9]+) \u4e2a', text).group(1))\n    if '\u6ce8\u518c\u8d44\u672c' in text and '\u4e07\u5143\u4eba\u6c11\u5e01' in text:\n        registered_capital = float(re.search('<li>\u6ce8\u518c\u8d44\u672c\uff1a(.*?)\u4e07\u5143\u4eba\u6c11\u5e01</li>', text).group(1))\n    else:\n        registered_capital = 0.0\n    origin_site = re.search('\"wapUrl\":\"(.*?)\",', text).group(1)\n    item = LiepinspdItem()\n    data = pd.read_csv('G:\\\\workspace\\\\y2019m01\\\\/first_lagou\\\\company300.csv', encoding='gbk')\n    try:\n        for i in range(len(data)):\n            n = 0\n            for j in data.loc[i, '\u80a1\u7968\u7b80\u79f0']:\n                if j in company_name:\n                    n += 1\n            if n == len(data.loc[i, '\u80a1\u7968\u7b80\u79f0']):\n                item['ticker'] = data.loc[i, '\u80a1\u7968\u4ee3\u7801']\n    except BaseException as e:\n        print('ticker\u5339\u914d\u9519\u8bef')\n    item['as_of_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    item['company_name'] = company_name\n    item['stage'] = stage\n    item['size'] = size\n    item['city'] = city\n    item['industry'] = industry\n    item['comp_clearfix'] = comp_clearfix\n    item['rate_num'] = rate_num\n    item['job_count'] = job_count\n    item['registered_capital'] = registered_capital\n    item['spider_time'] = datetime.strptime(str(datetime.now())[:10], '%Y-%m-%d').date()\n    item['origin_site'] = origin_site\n    yield item",
        "mutated": [
            "def parse(self, response):\n    if False:\n        i = 10\n    text = response.text\n    company_name = response.xpath('//div[@class=\"name-and-welfare\"]//h1/text()')[0].extract()\n    comp_sum_tag = response.xpath('//div[@class=\"comp-summary-tag\"]/a/text()').extract()\n    stage = comp_sum_tag[0]\n    size = comp_sum_tag[1]\n    city = comp_sum_tag[2]\n    industry = comp_sum_tag[3]\n    comp_clearfix = str(response.xpath('//ul[@class=\"comp-tag-list clearfix\"]//span/text()').extract())\n    rate_num = response.xpath('//p[@class=\"rate-num\"]//span/text()')[0].extract()\n    rate_num = int(rate_num) / 100\n    job_count = int(re.search('<small data-selector=\"total\">. \u5171([0-9]+) \u4e2a', text).group(1))\n    if '\u6ce8\u518c\u8d44\u672c' in text and '\u4e07\u5143\u4eba\u6c11\u5e01' in text:\n        registered_capital = float(re.search('<li>\u6ce8\u518c\u8d44\u672c\uff1a(.*?)\u4e07\u5143\u4eba\u6c11\u5e01</li>', text).group(1))\n    else:\n        registered_capital = 0.0\n    origin_site = re.search('\"wapUrl\":\"(.*?)\",', text).group(1)\n    item = LiepinspdItem()\n    data = pd.read_csv('G:\\\\workspace\\\\y2019m01\\\\/first_lagou\\\\company300.csv', encoding='gbk')\n    try:\n        for i in range(len(data)):\n            n = 0\n            for j in data.loc[i, '\u80a1\u7968\u7b80\u79f0']:\n                if j in company_name:\n                    n += 1\n            if n == len(data.loc[i, '\u80a1\u7968\u7b80\u79f0']):\n                item['ticker'] = data.loc[i, '\u80a1\u7968\u4ee3\u7801']\n    except BaseException as e:\n        print('ticker\u5339\u914d\u9519\u8bef')\n    item['as_of_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    item['company_name'] = company_name\n    item['stage'] = stage\n    item['size'] = size\n    item['city'] = city\n    item['industry'] = industry\n    item['comp_clearfix'] = comp_clearfix\n    item['rate_num'] = rate_num\n    item['job_count'] = job_count\n    item['registered_capital'] = registered_capital\n    item['spider_time'] = datetime.strptime(str(datetime.now())[:10], '%Y-%m-%d').date()\n    item['origin_site'] = origin_site\n    yield item",
            "def parse(self, response):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    text = response.text\n    company_name = response.xpath('//div[@class=\"name-and-welfare\"]//h1/text()')[0].extract()\n    comp_sum_tag = response.xpath('//div[@class=\"comp-summary-tag\"]/a/text()').extract()\n    stage = comp_sum_tag[0]\n    size = comp_sum_tag[1]\n    city = comp_sum_tag[2]\n    industry = comp_sum_tag[3]\n    comp_clearfix = str(response.xpath('//ul[@class=\"comp-tag-list clearfix\"]//span/text()').extract())\n    rate_num = response.xpath('//p[@class=\"rate-num\"]//span/text()')[0].extract()\n    rate_num = int(rate_num) / 100\n    job_count = int(re.search('<small data-selector=\"total\">. \u5171([0-9]+) \u4e2a', text).group(1))\n    if '\u6ce8\u518c\u8d44\u672c' in text and '\u4e07\u5143\u4eba\u6c11\u5e01' in text:\n        registered_capital = float(re.search('<li>\u6ce8\u518c\u8d44\u672c\uff1a(.*?)\u4e07\u5143\u4eba\u6c11\u5e01</li>', text).group(1))\n    else:\n        registered_capital = 0.0\n    origin_site = re.search('\"wapUrl\":\"(.*?)\",', text).group(1)\n    item = LiepinspdItem()\n    data = pd.read_csv('G:\\\\workspace\\\\y2019m01\\\\/first_lagou\\\\company300.csv', encoding='gbk')\n    try:\n        for i in range(len(data)):\n            n = 0\n            for j in data.loc[i, '\u80a1\u7968\u7b80\u79f0']:\n                if j in company_name:\n                    n += 1\n            if n == len(data.loc[i, '\u80a1\u7968\u7b80\u79f0']):\n                item['ticker'] = data.loc[i, '\u80a1\u7968\u4ee3\u7801']\n    except BaseException as e:\n        print('ticker\u5339\u914d\u9519\u8bef')\n    item['as_of_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    item['company_name'] = company_name\n    item['stage'] = stage\n    item['size'] = size\n    item['city'] = city\n    item['industry'] = industry\n    item['comp_clearfix'] = comp_clearfix\n    item['rate_num'] = rate_num\n    item['job_count'] = job_count\n    item['registered_capital'] = registered_capital\n    item['spider_time'] = datetime.strptime(str(datetime.now())[:10], '%Y-%m-%d').date()\n    item['origin_site'] = origin_site\n    yield item",
            "def parse(self, response):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    text = response.text\n    company_name = response.xpath('//div[@class=\"name-and-welfare\"]//h1/text()')[0].extract()\n    comp_sum_tag = response.xpath('//div[@class=\"comp-summary-tag\"]/a/text()').extract()\n    stage = comp_sum_tag[0]\n    size = comp_sum_tag[1]\n    city = comp_sum_tag[2]\n    industry = comp_sum_tag[3]\n    comp_clearfix = str(response.xpath('//ul[@class=\"comp-tag-list clearfix\"]//span/text()').extract())\n    rate_num = response.xpath('//p[@class=\"rate-num\"]//span/text()')[0].extract()\n    rate_num = int(rate_num) / 100\n    job_count = int(re.search('<small data-selector=\"total\">. \u5171([0-9]+) \u4e2a', text).group(1))\n    if '\u6ce8\u518c\u8d44\u672c' in text and '\u4e07\u5143\u4eba\u6c11\u5e01' in text:\n        registered_capital = float(re.search('<li>\u6ce8\u518c\u8d44\u672c\uff1a(.*?)\u4e07\u5143\u4eba\u6c11\u5e01</li>', text).group(1))\n    else:\n        registered_capital = 0.0\n    origin_site = re.search('\"wapUrl\":\"(.*?)\",', text).group(1)\n    item = LiepinspdItem()\n    data = pd.read_csv('G:\\\\workspace\\\\y2019m01\\\\/first_lagou\\\\company300.csv', encoding='gbk')\n    try:\n        for i in range(len(data)):\n            n = 0\n            for j in data.loc[i, '\u80a1\u7968\u7b80\u79f0']:\n                if j in company_name:\n                    n += 1\n            if n == len(data.loc[i, '\u80a1\u7968\u7b80\u79f0']):\n                item['ticker'] = data.loc[i, '\u80a1\u7968\u4ee3\u7801']\n    except BaseException as e:\n        print('ticker\u5339\u914d\u9519\u8bef')\n    item['as_of_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    item['company_name'] = company_name\n    item['stage'] = stage\n    item['size'] = size\n    item['city'] = city\n    item['industry'] = industry\n    item['comp_clearfix'] = comp_clearfix\n    item['rate_num'] = rate_num\n    item['job_count'] = job_count\n    item['registered_capital'] = registered_capital\n    item['spider_time'] = datetime.strptime(str(datetime.now())[:10], '%Y-%m-%d').date()\n    item['origin_site'] = origin_site\n    yield item",
            "def parse(self, response):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    text = response.text\n    company_name = response.xpath('//div[@class=\"name-and-welfare\"]//h1/text()')[0].extract()\n    comp_sum_tag = response.xpath('//div[@class=\"comp-summary-tag\"]/a/text()').extract()\n    stage = comp_sum_tag[0]\n    size = comp_sum_tag[1]\n    city = comp_sum_tag[2]\n    industry = comp_sum_tag[3]\n    comp_clearfix = str(response.xpath('//ul[@class=\"comp-tag-list clearfix\"]//span/text()').extract())\n    rate_num = response.xpath('//p[@class=\"rate-num\"]//span/text()')[0].extract()\n    rate_num = int(rate_num) / 100\n    job_count = int(re.search('<small data-selector=\"total\">. \u5171([0-9]+) \u4e2a', text).group(1))\n    if '\u6ce8\u518c\u8d44\u672c' in text and '\u4e07\u5143\u4eba\u6c11\u5e01' in text:\n        registered_capital = float(re.search('<li>\u6ce8\u518c\u8d44\u672c\uff1a(.*?)\u4e07\u5143\u4eba\u6c11\u5e01</li>', text).group(1))\n    else:\n        registered_capital = 0.0\n    origin_site = re.search('\"wapUrl\":\"(.*?)\",', text).group(1)\n    item = LiepinspdItem()\n    data = pd.read_csv('G:\\\\workspace\\\\y2019m01\\\\/first_lagou\\\\company300.csv', encoding='gbk')\n    try:\n        for i in range(len(data)):\n            n = 0\n            for j in data.loc[i, '\u80a1\u7968\u7b80\u79f0']:\n                if j in company_name:\n                    n += 1\n            if n == len(data.loc[i, '\u80a1\u7968\u7b80\u79f0']):\n                item['ticker'] = data.loc[i, '\u80a1\u7968\u4ee3\u7801']\n    except BaseException as e:\n        print('ticker\u5339\u914d\u9519\u8bef')\n    item['as_of_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    item['company_name'] = company_name\n    item['stage'] = stage\n    item['size'] = size\n    item['city'] = city\n    item['industry'] = industry\n    item['comp_clearfix'] = comp_clearfix\n    item['rate_num'] = rate_num\n    item['job_count'] = job_count\n    item['registered_capital'] = registered_capital\n    item['spider_time'] = datetime.strptime(str(datetime.now())[:10], '%Y-%m-%d').date()\n    item['origin_site'] = origin_site\n    yield item",
            "def parse(self, response):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    text = response.text\n    company_name = response.xpath('//div[@class=\"name-and-welfare\"]//h1/text()')[0].extract()\n    comp_sum_tag = response.xpath('//div[@class=\"comp-summary-tag\"]/a/text()').extract()\n    stage = comp_sum_tag[0]\n    size = comp_sum_tag[1]\n    city = comp_sum_tag[2]\n    industry = comp_sum_tag[3]\n    comp_clearfix = str(response.xpath('//ul[@class=\"comp-tag-list clearfix\"]//span/text()').extract())\n    rate_num = response.xpath('//p[@class=\"rate-num\"]//span/text()')[0].extract()\n    rate_num = int(rate_num) / 100\n    job_count = int(re.search('<small data-selector=\"total\">. \u5171([0-9]+) \u4e2a', text).group(1))\n    if '\u6ce8\u518c\u8d44\u672c' in text and '\u4e07\u5143\u4eba\u6c11\u5e01' in text:\n        registered_capital = float(re.search('<li>\u6ce8\u518c\u8d44\u672c\uff1a(.*?)\u4e07\u5143\u4eba\u6c11\u5e01</li>', text).group(1))\n    else:\n        registered_capital = 0.0\n    origin_site = re.search('\"wapUrl\":\"(.*?)\",', text).group(1)\n    item = LiepinspdItem()\n    data = pd.read_csv('G:\\\\workspace\\\\y2019m01\\\\/first_lagou\\\\company300.csv', encoding='gbk')\n    try:\n        for i in range(len(data)):\n            n = 0\n            for j in data.loc[i, '\u80a1\u7968\u7b80\u79f0']:\n                if j in company_name:\n                    n += 1\n            if n == len(data.loc[i, '\u80a1\u7968\u7b80\u79f0']):\n                item['ticker'] = data.loc[i, '\u80a1\u7968\u4ee3\u7801']\n    except BaseException as e:\n        print('ticker\u5339\u914d\u9519\u8bef')\n    item['as_of_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    item['company_name'] = company_name\n    item['stage'] = stage\n    item['size'] = size\n    item['city'] = city\n    item['industry'] = industry\n    item['comp_clearfix'] = comp_clearfix\n    item['rate_num'] = rate_num\n    item['job_count'] = job_count\n    item['registered_capital'] = registered_capital\n    item['spider_time'] = datetime.strptime(str(datetime.now())[:10], '%Y-%m-%d').date()\n    item['origin_site'] = origin_site\n    yield item"
        ]
    }
]