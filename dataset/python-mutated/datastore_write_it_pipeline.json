[
    {
        "func_name": "new_pipeline_with_job_name",
        "original": "def new_pipeline_with_job_name(pipeline_options, job_name, suffix):\n    \"\"\"Create a pipeline with the given job_name and a suffix.\"\"\"\n    gcp_options = pipeline_options.view_as(GoogleCloudOptions)\n    if job_name:\n        gcp_options.job_name = job_name + suffix\n    return TestPipeline(options=pipeline_options)",
        "mutated": [
            "def new_pipeline_with_job_name(pipeline_options, job_name, suffix):\n    if False:\n        i = 10\n    'Create a pipeline with the given job_name and a suffix.'\n    gcp_options = pipeline_options.view_as(GoogleCloudOptions)\n    if job_name:\n        gcp_options.job_name = job_name + suffix\n    return TestPipeline(options=pipeline_options)",
            "def new_pipeline_with_job_name(pipeline_options, job_name, suffix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a pipeline with the given job_name and a suffix.'\n    gcp_options = pipeline_options.view_as(GoogleCloudOptions)\n    if job_name:\n        gcp_options.job_name = job_name + suffix\n    return TestPipeline(options=pipeline_options)",
            "def new_pipeline_with_job_name(pipeline_options, job_name, suffix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a pipeline with the given job_name and a suffix.'\n    gcp_options = pipeline_options.view_as(GoogleCloudOptions)\n    if job_name:\n        gcp_options.job_name = job_name + suffix\n    return TestPipeline(options=pipeline_options)",
            "def new_pipeline_with_job_name(pipeline_options, job_name, suffix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a pipeline with the given job_name and a suffix.'\n    gcp_options = pipeline_options.view_as(GoogleCloudOptions)\n    if job_name:\n        gcp_options.job_name = job_name + suffix\n    return TestPipeline(options=pipeline_options)",
            "def new_pipeline_with_job_name(pipeline_options, job_name, suffix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a pipeline with the given job_name and a suffix.'\n    gcp_options = pipeline_options.view_as(GoogleCloudOptions)\n    if job_name:\n        gcp_options.job_name = job_name + suffix\n    return TestPipeline(options=pipeline_options)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, kind, parent_key):\n    self._kind = kind\n    self._parent_key = parent_key",
        "mutated": [
            "def __init__(self, kind, parent_key):\n    if False:\n        i = 10\n    self._kind = kind\n    self._parent_key = parent_key",
            "def __init__(self, kind, parent_key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self._kind = kind\n    self._parent_key = parent_key",
            "def __init__(self, kind, parent_key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self._kind = kind\n    self._parent_key = parent_key",
            "def __init__(self, kind, parent_key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self._kind = kind\n    self._parent_key = parent_key",
            "def __init__(self, kind, parent_key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self._kind = kind\n    self._parent_key = parent_key"
        ]
    },
    {
        "func_name": "make_entity",
        "original": "def make_entity(self, content):\n    \"\"\"Create entity from given string.\"\"\"\n    key = Key([self._kind, hashlib.sha1(content.encode('utf-8')).hexdigest()], parent=self._parent_key)\n    entity = Entity(key)\n    entity.set_properties({'content': str(content)})\n    return entity",
        "mutated": [
            "def make_entity(self, content):\n    if False:\n        i = 10\n    'Create entity from given string.'\n    key = Key([self._kind, hashlib.sha1(content.encode('utf-8')).hexdigest()], parent=self._parent_key)\n    entity = Entity(key)\n    entity.set_properties({'content': str(content)})\n    return entity",
            "def make_entity(self, content):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create entity from given string.'\n    key = Key([self._kind, hashlib.sha1(content.encode('utf-8')).hexdigest()], parent=self._parent_key)\n    entity = Entity(key)\n    entity.set_properties({'content': str(content)})\n    return entity",
            "def make_entity(self, content):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create entity from given string.'\n    key = Key([self._kind, hashlib.sha1(content.encode('utf-8')).hexdigest()], parent=self._parent_key)\n    entity = Entity(key)\n    entity.set_properties({'content': str(content)})\n    return entity",
            "def make_entity(self, content):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create entity from given string.'\n    key = Key([self._kind, hashlib.sha1(content.encode('utf-8')).hexdigest()], parent=self._parent_key)\n    entity = Entity(key)\n    entity.set_properties({'content': str(content)})\n    return entity",
            "def make_entity(self, content):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create entity from given string.'\n    key = Key([self._kind, hashlib.sha1(content.encode('utf-8')).hexdigest()], parent=self._parent_key)\n    entity = Entity(key)\n    entity.set_properties({'content': str(content)})\n    return entity"
        ]
    },
    {
        "func_name": "run",
        "original": "def run(argv=None):\n    \"\"\"Main entry point.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--kind', dest='kind', default='writereadtest', help='Datastore Kind')\n    parser.add_argument('--num_entities', dest='num_entities', type=int, required=True, help='Number of entities to write')\n    parser.add_argument('--limit', dest='limit', type=int, help='Limit of number of entities to write')\n    (known_args, pipeline_args) = parser.parse_known_args(argv)\n    pipeline_options = PipelineOptions(pipeline_args)\n    gcloud_options = pipeline_options.view_as(GoogleCloudOptions)\n    job_name = gcloud_options.job_name\n    kind = known_args.kind\n    num_entities = known_args.num_entities\n    project = gcloud_options.project\n    ancestor_key = Key([kind, str(uuid.uuid4())], project=project)\n    _LOGGER.info('Writing %s entities to %s', num_entities, project)\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-write')\n    _ = p | 'Input' >> beam.Create(list(range(num_entities))) | 'To String' >> beam.Map(str) | 'To Entity' >> beam.Map(EntityWrapper(kind, ancestor_key).make_entity) | 'Write to Datastore' >> WriteToDatastore(project, hint_num_workers=1)\n    p.run()\n    query = Query(kind=kind, project=project, ancestor=ancestor_key)\n    if known_args.limit is not None:\n        _LOGGER.info('Querying a limited set of %s entities and verifying count.', known_args.limit)\n        p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-limit')\n        query.limit = known_args.limit\n        entities = p | 'read from datastore' >> ReadFromDatastore(query)\n        assert_that(entities | beam.combiners.Count.Globally(), equal_to([known_args.limit]))\n        p.run()\n        query.limit = None\n    _LOGGER.info('Querying entities, asserting they match.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([num_entities]))\n    p.run()\n    _LOGGER.info('Deleting entities.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-delete')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    _ = entities | 'To Keys' >> beam.Map(lambda entity: entity.key) | 'delete entities' >> DeleteFromDatastore(project, hint_num_workers=1)\n    p.run()\n    _LOGGER.info('Querying for the entities to make sure there are none present.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-deleted')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([0]))\n    p.run()",
        "mutated": [
            "def run(argv=None):\n    if False:\n        i = 10\n    'Main entry point.'\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--kind', dest='kind', default='writereadtest', help='Datastore Kind')\n    parser.add_argument('--num_entities', dest='num_entities', type=int, required=True, help='Number of entities to write')\n    parser.add_argument('--limit', dest='limit', type=int, help='Limit of number of entities to write')\n    (known_args, pipeline_args) = parser.parse_known_args(argv)\n    pipeline_options = PipelineOptions(pipeline_args)\n    gcloud_options = pipeline_options.view_as(GoogleCloudOptions)\n    job_name = gcloud_options.job_name\n    kind = known_args.kind\n    num_entities = known_args.num_entities\n    project = gcloud_options.project\n    ancestor_key = Key([kind, str(uuid.uuid4())], project=project)\n    _LOGGER.info('Writing %s entities to %s', num_entities, project)\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-write')\n    _ = p | 'Input' >> beam.Create(list(range(num_entities))) | 'To String' >> beam.Map(str) | 'To Entity' >> beam.Map(EntityWrapper(kind, ancestor_key).make_entity) | 'Write to Datastore' >> WriteToDatastore(project, hint_num_workers=1)\n    p.run()\n    query = Query(kind=kind, project=project, ancestor=ancestor_key)\n    if known_args.limit is not None:\n        _LOGGER.info('Querying a limited set of %s entities and verifying count.', known_args.limit)\n        p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-limit')\n        query.limit = known_args.limit\n        entities = p | 'read from datastore' >> ReadFromDatastore(query)\n        assert_that(entities | beam.combiners.Count.Globally(), equal_to([known_args.limit]))\n        p.run()\n        query.limit = None\n    _LOGGER.info('Querying entities, asserting they match.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([num_entities]))\n    p.run()\n    _LOGGER.info('Deleting entities.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-delete')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    _ = entities | 'To Keys' >> beam.Map(lambda entity: entity.key) | 'delete entities' >> DeleteFromDatastore(project, hint_num_workers=1)\n    p.run()\n    _LOGGER.info('Querying for the entities to make sure there are none present.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-deleted')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([0]))\n    p.run()",
            "def run(argv=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Main entry point.'\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--kind', dest='kind', default='writereadtest', help='Datastore Kind')\n    parser.add_argument('--num_entities', dest='num_entities', type=int, required=True, help='Number of entities to write')\n    parser.add_argument('--limit', dest='limit', type=int, help='Limit of number of entities to write')\n    (known_args, pipeline_args) = parser.parse_known_args(argv)\n    pipeline_options = PipelineOptions(pipeline_args)\n    gcloud_options = pipeline_options.view_as(GoogleCloudOptions)\n    job_name = gcloud_options.job_name\n    kind = known_args.kind\n    num_entities = known_args.num_entities\n    project = gcloud_options.project\n    ancestor_key = Key([kind, str(uuid.uuid4())], project=project)\n    _LOGGER.info('Writing %s entities to %s', num_entities, project)\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-write')\n    _ = p | 'Input' >> beam.Create(list(range(num_entities))) | 'To String' >> beam.Map(str) | 'To Entity' >> beam.Map(EntityWrapper(kind, ancestor_key).make_entity) | 'Write to Datastore' >> WriteToDatastore(project, hint_num_workers=1)\n    p.run()\n    query = Query(kind=kind, project=project, ancestor=ancestor_key)\n    if known_args.limit is not None:\n        _LOGGER.info('Querying a limited set of %s entities and verifying count.', known_args.limit)\n        p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-limit')\n        query.limit = known_args.limit\n        entities = p | 'read from datastore' >> ReadFromDatastore(query)\n        assert_that(entities | beam.combiners.Count.Globally(), equal_to([known_args.limit]))\n        p.run()\n        query.limit = None\n    _LOGGER.info('Querying entities, asserting they match.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([num_entities]))\n    p.run()\n    _LOGGER.info('Deleting entities.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-delete')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    _ = entities | 'To Keys' >> beam.Map(lambda entity: entity.key) | 'delete entities' >> DeleteFromDatastore(project, hint_num_workers=1)\n    p.run()\n    _LOGGER.info('Querying for the entities to make sure there are none present.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-deleted')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([0]))\n    p.run()",
            "def run(argv=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Main entry point.'\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--kind', dest='kind', default='writereadtest', help='Datastore Kind')\n    parser.add_argument('--num_entities', dest='num_entities', type=int, required=True, help='Number of entities to write')\n    parser.add_argument('--limit', dest='limit', type=int, help='Limit of number of entities to write')\n    (known_args, pipeline_args) = parser.parse_known_args(argv)\n    pipeline_options = PipelineOptions(pipeline_args)\n    gcloud_options = pipeline_options.view_as(GoogleCloudOptions)\n    job_name = gcloud_options.job_name\n    kind = known_args.kind\n    num_entities = known_args.num_entities\n    project = gcloud_options.project\n    ancestor_key = Key([kind, str(uuid.uuid4())], project=project)\n    _LOGGER.info('Writing %s entities to %s', num_entities, project)\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-write')\n    _ = p | 'Input' >> beam.Create(list(range(num_entities))) | 'To String' >> beam.Map(str) | 'To Entity' >> beam.Map(EntityWrapper(kind, ancestor_key).make_entity) | 'Write to Datastore' >> WriteToDatastore(project, hint_num_workers=1)\n    p.run()\n    query = Query(kind=kind, project=project, ancestor=ancestor_key)\n    if known_args.limit is not None:\n        _LOGGER.info('Querying a limited set of %s entities and verifying count.', known_args.limit)\n        p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-limit')\n        query.limit = known_args.limit\n        entities = p | 'read from datastore' >> ReadFromDatastore(query)\n        assert_that(entities | beam.combiners.Count.Globally(), equal_to([known_args.limit]))\n        p.run()\n        query.limit = None\n    _LOGGER.info('Querying entities, asserting they match.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([num_entities]))\n    p.run()\n    _LOGGER.info('Deleting entities.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-delete')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    _ = entities | 'To Keys' >> beam.Map(lambda entity: entity.key) | 'delete entities' >> DeleteFromDatastore(project, hint_num_workers=1)\n    p.run()\n    _LOGGER.info('Querying for the entities to make sure there are none present.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-deleted')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([0]))\n    p.run()",
            "def run(argv=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Main entry point.'\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--kind', dest='kind', default='writereadtest', help='Datastore Kind')\n    parser.add_argument('--num_entities', dest='num_entities', type=int, required=True, help='Number of entities to write')\n    parser.add_argument('--limit', dest='limit', type=int, help='Limit of number of entities to write')\n    (known_args, pipeline_args) = parser.parse_known_args(argv)\n    pipeline_options = PipelineOptions(pipeline_args)\n    gcloud_options = pipeline_options.view_as(GoogleCloudOptions)\n    job_name = gcloud_options.job_name\n    kind = known_args.kind\n    num_entities = known_args.num_entities\n    project = gcloud_options.project\n    ancestor_key = Key([kind, str(uuid.uuid4())], project=project)\n    _LOGGER.info('Writing %s entities to %s', num_entities, project)\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-write')\n    _ = p | 'Input' >> beam.Create(list(range(num_entities))) | 'To String' >> beam.Map(str) | 'To Entity' >> beam.Map(EntityWrapper(kind, ancestor_key).make_entity) | 'Write to Datastore' >> WriteToDatastore(project, hint_num_workers=1)\n    p.run()\n    query = Query(kind=kind, project=project, ancestor=ancestor_key)\n    if known_args.limit is not None:\n        _LOGGER.info('Querying a limited set of %s entities and verifying count.', known_args.limit)\n        p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-limit')\n        query.limit = known_args.limit\n        entities = p | 'read from datastore' >> ReadFromDatastore(query)\n        assert_that(entities | beam.combiners.Count.Globally(), equal_to([known_args.limit]))\n        p.run()\n        query.limit = None\n    _LOGGER.info('Querying entities, asserting they match.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([num_entities]))\n    p.run()\n    _LOGGER.info('Deleting entities.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-delete')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    _ = entities | 'To Keys' >> beam.Map(lambda entity: entity.key) | 'delete entities' >> DeleteFromDatastore(project, hint_num_workers=1)\n    p.run()\n    _LOGGER.info('Querying for the entities to make sure there are none present.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-deleted')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([0]))\n    p.run()",
            "def run(argv=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Main entry point.'\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--kind', dest='kind', default='writereadtest', help='Datastore Kind')\n    parser.add_argument('--num_entities', dest='num_entities', type=int, required=True, help='Number of entities to write')\n    parser.add_argument('--limit', dest='limit', type=int, help='Limit of number of entities to write')\n    (known_args, pipeline_args) = parser.parse_known_args(argv)\n    pipeline_options = PipelineOptions(pipeline_args)\n    gcloud_options = pipeline_options.view_as(GoogleCloudOptions)\n    job_name = gcloud_options.job_name\n    kind = known_args.kind\n    num_entities = known_args.num_entities\n    project = gcloud_options.project\n    ancestor_key = Key([kind, str(uuid.uuid4())], project=project)\n    _LOGGER.info('Writing %s entities to %s', num_entities, project)\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-write')\n    _ = p | 'Input' >> beam.Create(list(range(num_entities))) | 'To String' >> beam.Map(str) | 'To Entity' >> beam.Map(EntityWrapper(kind, ancestor_key).make_entity) | 'Write to Datastore' >> WriteToDatastore(project, hint_num_workers=1)\n    p.run()\n    query = Query(kind=kind, project=project, ancestor=ancestor_key)\n    if known_args.limit is not None:\n        _LOGGER.info('Querying a limited set of %s entities and verifying count.', known_args.limit)\n        p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-limit')\n        query.limit = known_args.limit\n        entities = p | 'read from datastore' >> ReadFromDatastore(query)\n        assert_that(entities | beam.combiners.Count.Globally(), equal_to([known_args.limit]))\n        p.run()\n        query.limit = None\n    _LOGGER.info('Querying entities, asserting they match.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([num_entities]))\n    p.run()\n    _LOGGER.info('Deleting entities.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-delete')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    _ = entities | 'To Keys' >> beam.Map(lambda entity: entity.key) | 'delete entities' >> DeleteFromDatastore(project, hint_num_workers=1)\n    p.run()\n    _LOGGER.info('Querying for the entities to make sure there are none present.')\n    p = new_pipeline_with_job_name(pipeline_options, job_name, '-verify-deleted')\n    entities = p | 'read from datastore' >> ReadFromDatastore(query)\n    assert_that(entities | beam.combiners.Count.Globally(), equal_to([0]))\n    p.run()"
        ]
    }
]