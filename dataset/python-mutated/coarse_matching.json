[
    {
        "func_name": "mask_border",
        "original": "def mask_border(m, b: int, v):\n    \"\"\" Mask borders with value\n    Args:\n        m (torch.Tensor): [N, H0, W0, H1, W1]\n        b (int)\n        v (m.dtype)\n    \"\"\"\n    if b <= 0:\n        return\n    m[:, :b] = v\n    m[:, :, :b] = v\n    m[:, :, :, :b] = v\n    m[:, :, :, :, :b] = v\n    m[:, -b:] = v\n    m[:, :, -b:] = v\n    m[:, :, :, -b:] = v\n    m[:, :, :, :, -b:] = v",
        "mutated": [
            "def mask_border(m, b: int, v):\n    if False:\n        i = 10\n    ' Mask borders with value\\n    Args:\\n        m (torch.Tensor): [N, H0, W0, H1, W1]\\n        b (int)\\n        v (m.dtype)\\n    '\n    if b <= 0:\n        return\n    m[:, :b] = v\n    m[:, :, :b] = v\n    m[:, :, :, :b] = v\n    m[:, :, :, :, :b] = v\n    m[:, -b:] = v\n    m[:, :, -b:] = v\n    m[:, :, :, -b:] = v\n    m[:, :, :, :, -b:] = v",
            "def mask_border(m, b: int, v):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Mask borders with value\\n    Args:\\n        m (torch.Tensor): [N, H0, W0, H1, W1]\\n        b (int)\\n        v (m.dtype)\\n    '\n    if b <= 0:\n        return\n    m[:, :b] = v\n    m[:, :, :b] = v\n    m[:, :, :, :b] = v\n    m[:, :, :, :, :b] = v\n    m[:, -b:] = v\n    m[:, :, -b:] = v\n    m[:, :, :, -b:] = v\n    m[:, :, :, :, -b:] = v",
            "def mask_border(m, b: int, v):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Mask borders with value\\n    Args:\\n        m (torch.Tensor): [N, H0, W0, H1, W1]\\n        b (int)\\n        v (m.dtype)\\n    '\n    if b <= 0:\n        return\n    m[:, :b] = v\n    m[:, :, :b] = v\n    m[:, :, :, :b] = v\n    m[:, :, :, :, :b] = v\n    m[:, -b:] = v\n    m[:, :, -b:] = v\n    m[:, :, :, -b:] = v\n    m[:, :, :, :, -b:] = v",
            "def mask_border(m, b: int, v):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Mask borders with value\\n    Args:\\n        m (torch.Tensor): [N, H0, W0, H1, W1]\\n        b (int)\\n        v (m.dtype)\\n    '\n    if b <= 0:\n        return\n    m[:, :b] = v\n    m[:, :, :b] = v\n    m[:, :, :, :b] = v\n    m[:, :, :, :, :b] = v\n    m[:, -b:] = v\n    m[:, :, -b:] = v\n    m[:, :, :, -b:] = v\n    m[:, :, :, :, -b:] = v",
            "def mask_border(m, b: int, v):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Mask borders with value\\n    Args:\\n        m (torch.Tensor): [N, H0, W0, H1, W1]\\n        b (int)\\n        v (m.dtype)\\n    '\n    if b <= 0:\n        return\n    m[:, :b] = v\n    m[:, :, :b] = v\n    m[:, :, :, :b] = v\n    m[:, :, :, :, :b] = v\n    m[:, -b:] = v\n    m[:, :, -b:] = v\n    m[:, :, :, -b:] = v\n    m[:, :, :, :, -b:] = v"
        ]
    },
    {
        "func_name": "mask_border_with_padding",
        "original": "def mask_border_with_padding(m, bd, v, p_m0, p_m1):\n    if bd <= 0:\n        return\n    m[:, :bd] = v\n    m[:, :, :bd] = v\n    m[:, :, :, :bd] = v\n    m[:, :, :, :, :bd] = v\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0].int(), p_m0.sum(-1).max(-1)[0].int())\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0].int(), p_m1.sum(-1).max(-1)[0].int())\n    for (b_idx, (h0, w0, h1, w1)) in enumerate(zip(h0s, w0s, h1s, w1s)):\n        m[b_idx, h0 - bd:] = v\n        m[b_idx, :, w0 - bd:] = v\n        m[b_idx, :, :, h1 - bd:] = v\n        m[b_idx, :, :, :, w1 - bd:] = v",
        "mutated": [
            "def mask_border_with_padding(m, bd, v, p_m0, p_m1):\n    if False:\n        i = 10\n    if bd <= 0:\n        return\n    m[:, :bd] = v\n    m[:, :, :bd] = v\n    m[:, :, :, :bd] = v\n    m[:, :, :, :, :bd] = v\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0].int(), p_m0.sum(-1).max(-1)[0].int())\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0].int(), p_m1.sum(-1).max(-1)[0].int())\n    for (b_idx, (h0, w0, h1, w1)) in enumerate(zip(h0s, w0s, h1s, w1s)):\n        m[b_idx, h0 - bd:] = v\n        m[b_idx, :, w0 - bd:] = v\n        m[b_idx, :, :, h1 - bd:] = v\n        m[b_idx, :, :, :, w1 - bd:] = v",
            "def mask_border_with_padding(m, bd, v, p_m0, p_m1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if bd <= 0:\n        return\n    m[:, :bd] = v\n    m[:, :, :bd] = v\n    m[:, :, :, :bd] = v\n    m[:, :, :, :, :bd] = v\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0].int(), p_m0.sum(-1).max(-1)[0].int())\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0].int(), p_m1.sum(-1).max(-1)[0].int())\n    for (b_idx, (h0, w0, h1, w1)) in enumerate(zip(h0s, w0s, h1s, w1s)):\n        m[b_idx, h0 - bd:] = v\n        m[b_idx, :, w0 - bd:] = v\n        m[b_idx, :, :, h1 - bd:] = v\n        m[b_idx, :, :, :, w1 - bd:] = v",
            "def mask_border_with_padding(m, bd, v, p_m0, p_m1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if bd <= 0:\n        return\n    m[:, :bd] = v\n    m[:, :, :bd] = v\n    m[:, :, :, :bd] = v\n    m[:, :, :, :, :bd] = v\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0].int(), p_m0.sum(-1).max(-1)[0].int())\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0].int(), p_m1.sum(-1).max(-1)[0].int())\n    for (b_idx, (h0, w0, h1, w1)) in enumerate(zip(h0s, w0s, h1s, w1s)):\n        m[b_idx, h0 - bd:] = v\n        m[b_idx, :, w0 - bd:] = v\n        m[b_idx, :, :, h1 - bd:] = v\n        m[b_idx, :, :, :, w1 - bd:] = v",
            "def mask_border_with_padding(m, bd, v, p_m0, p_m1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if bd <= 0:\n        return\n    m[:, :bd] = v\n    m[:, :, :bd] = v\n    m[:, :, :, :bd] = v\n    m[:, :, :, :, :bd] = v\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0].int(), p_m0.sum(-1).max(-1)[0].int())\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0].int(), p_m1.sum(-1).max(-1)[0].int())\n    for (b_idx, (h0, w0, h1, w1)) in enumerate(zip(h0s, w0s, h1s, w1s)):\n        m[b_idx, h0 - bd:] = v\n        m[b_idx, :, w0 - bd:] = v\n        m[b_idx, :, :, h1 - bd:] = v\n        m[b_idx, :, :, :, w1 - bd:] = v",
            "def mask_border_with_padding(m, bd, v, p_m0, p_m1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if bd <= 0:\n        return\n    m[:, :bd] = v\n    m[:, :, :bd] = v\n    m[:, :, :, :bd] = v\n    m[:, :, :, :, :bd] = v\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0].int(), p_m0.sum(-1).max(-1)[0].int())\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0].int(), p_m1.sum(-1).max(-1)[0].int())\n    for (b_idx, (h0, w0, h1, w1)) in enumerate(zip(h0s, w0s, h1s, w1s)):\n        m[b_idx, h0 - bd:] = v\n        m[b_idx, :, w0 - bd:] = v\n        m[b_idx, :, :, h1 - bd:] = v\n        m[b_idx, :, :, :, w1 - bd:] = v"
        ]
    },
    {
        "func_name": "compute_max_candidates",
        "original": "def compute_max_candidates(p_m0, p_m1):\n    \"\"\"Compute the max candidates of all pairs within a batch\n\n    Args:\n        p_m0, p_m1 (torch.Tensor): padded masks\n    \"\"\"\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0])\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0])\n    max_cand = torch.sum(torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])\n    return max_cand",
        "mutated": [
            "def compute_max_candidates(p_m0, p_m1):\n    if False:\n        i = 10\n    'Compute the max candidates of all pairs within a batch\\n\\n    Args:\\n        p_m0, p_m1 (torch.Tensor): padded masks\\n    '\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0])\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0])\n    max_cand = torch.sum(torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])\n    return max_cand",
            "def compute_max_candidates(p_m0, p_m1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the max candidates of all pairs within a batch\\n\\n    Args:\\n        p_m0, p_m1 (torch.Tensor): padded masks\\n    '\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0])\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0])\n    max_cand = torch.sum(torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])\n    return max_cand",
            "def compute_max_candidates(p_m0, p_m1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the max candidates of all pairs within a batch\\n\\n    Args:\\n        p_m0, p_m1 (torch.Tensor): padded masks\\n    '\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0])\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0])\n    max_cand = torch.sum(torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])\n    return max_cand",
            "def compute_max_candidates(p_m0, p_m1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the max candidates of all pairs within a batch\\n\\n    Args:\\n        p_m0, p_m1 (torch.Tensor): padded masks\\n    '\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0])\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0])\n    max_cand = torch.sum(torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])\n    return max_cand",
            "def compute_max_candidates(p_m0, p_m1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the max candidates of all pairs within a batch\\n\\n    Args:\\n        p_m0, p_m1 (torch.Tensor): padded masks\\n    '\n    (h0s, w0s) = (p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0])\n    (h1s, w1s) = (p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0])\n    max_cand = torch.sum(torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])\n    return max_cand"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, config):\n    super().__init__()\n    self.config = config\n    self.thr = config['thr']\n    self.border_rm = config['border_rm']\n    self.train_coarse_percent = config['train_coarse_percent']\n    self.train_pad_num_gt_min = config['train_pad_num_gt_min']\n    self.match_type = config['match_type']\n    if self.match_type == 'dual_softmax':\n        self.temperature = config['dsmax_temperature']\n    elif self.match_type == 'sinkhorn':\n        try:\n            from .superglue import log_optimal_transport\n        except ImportError:\n            raise ImportError('download superglue.py first!')\n        self.log_optimal_transport = log_optimal_transport\n        self.bin_score = nn.Parameter(torch.tensor(config['skh_init_bin_score'], requires_grad=True))\n        self.skh_iters = config['skh_iters']\n        self.skh_prefilter = config['skh_prefilter']\n    else:\n        raise NotImplementedError()",
        "mutated": [
            "def __init__(self, config):\n    if False:\n        i = 10\n    super().__init__()\n    self.config = config\n    self.thr = config['thr']\n    self.border_rm = config['border_rm']\n    self.train_coarse_percent = config['train_coarse_percent']\n    self.train_pad_num_gt_min = config['train_pad_num_gt_min']\n    self.match_type = config['match_type']\n    if self.match_type == 'dual_softmax':\n        self.temperature = config['dsmax_temperature']\n    elif self.match_type == 'sinkhorn':\n        try:\n            from .superglue import log_optimal_transport\n        except ImportError:\n            raise ImportError('download superglue.py first!')\n        self.log_optimal_transport = log_optimal_transport\n        self.bin_score = nn.Parameter(torch.tensor(config['skh_init_bin_score'], requires_grad=True))\n        self.skh_iters = config['skh_iters']\n        self.skh_prefilter = config['skh_prefilter']\n    else:\n        raise NotImplementedError()",
            "def __init__(self, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.config = config\n    self.thr = config['thr']\n    self.border_rm = config['border_rm']\n    self.train_coarse_percent = config['train_coarse_percent']\n    self.train_pad_num_gt_min = config['train_pad_num_gt_min']\n    self.match_type = config['match_type']\n    if self.match_type == 'dual_softmax':\n        self.temperature = config['dsmax_temperature']\n    elif self.match_type == 'sinkhorn':\n        try:\n            from .superglue import log_optimal_transport\n        except ImportError:\n            raise ImportError('download superglue.py first!')\n        self.log_optimal_transport = log_optimal_transport\n        self.bin_score = nn.Parameter(torch.tensor(config['skh_init_bin_score'], requires_grad=True))\n        self.skh_iters = config['skh_iters']\n        self.skh_prefilter = config['skh_prefilter']\n    else:\n        raise NotImplementedError()",
            "def __init__(self, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.config = config\n    self.thr = config['thr']\n    self.border_rm = config['border_rm']\n    self.train_coarse_percent = config['train_coarse_percent']\n    self.train_pad_num_gt_min = config['train_pad_num_gt_min']\n    self.match_type = config['match_type']\n    if self.match_type == 'dual_softmax':\n        self.temperature = config['dsmax_temperature']\n    elif self.match_type == 'sinkhorn':\n        try:\n            from .superglue import log_optimal_transport\n        except ImportError:\n            raise ImportError('download superglue.py first!')\n        self.log_optimal_transport = log_optimal_transport\n        self.bin_score = nn.Parameter(torch.tensor(config['skh_init_bin_score'], requires_grad=True))\n        self.skh_iters = config['skh_iters']\n        self.skh_prefilter = config['skh_prefilter']\n    else:\n        raise NotImplementedError()",
            "def __init__(self, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.config = config\n    self.thr = config['thr']\n    self.border_rm = config['border_rm']\n    self.train_coarse_percent = config['train_coarse_percent']\n    self.train_pad_num_gt_min = config['train_pad_num_gt_min']\n    self.match_type = config['match_type']\n    if self.match_type == 'dual_softmax':\n        self.temperature = config['dsmax_temperature']\n    elif self.match_type == 'sinkhorn':\n        try:\n            from .superglue import log_optimal_transport\n        except ImportError:\n            raise ImportError('download superglue.py first!')\n        self.log_optimal_transport = log_optimal_transport\n        self.bin_score = nn.Parameter(torch.tensor(config['skh_init_bin_score'], requires_grad=True))\n        self.skh_iters = config['skh_iters']\n        self.skh_prefilter = config['skh_prefilter']\n    else:\n        raise NotImplementedError()",
            "def __init__(self, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.config = config\n    self.thr = config['thr']\n    self.border_rm = config['border_rm']\n    self.train_coarse_percent = config['train_coarse_percent']\n    self.train_pad_num_gt_min = config['train_pad_num_gt_min']\n    self.match_type = config['match_type']\n    if self.match_type == 'dual_softmax':\n        self.temperature = config['dsmax_temperature']\n    elif self.match_type == 'sinkhorn':\n        try:\n            from .superglue import log_optimal_transport\n        except ImportError:\n            raise ImportError('download superglue.py first!')\n        self.log_optimal_transport = log_optimal_transport\n        self.bin_score = nn.Parameter(torch.tensor(config['skh_init_bin_score'], requires_grad=True))\n        self.skh_iters = config['skh_iters']\n        self.skh_prefilter = config['skh_prefilter']\n    else:\n        raise NotImplementedError()"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, feat_c0, feat_c1, data, mask_c0=None, mask_c1=None):\n    \"\"\"\n        Args:\n            feat0 (torch.Tensor): [N, L, C]\n            feat1 (torch.Tensor): [N, S, C]\n            data (dict)\n            mask_c0 (torch.Tensor): [N, L] (optional)\n            mask_c1 (torch.Tensor): [N, S] (optional)\n        Update:\n            data (dict): {\n                'b_ids' (torch.Tensor): [M'],\n                'i_ids' (torch.Tensor): [M'],\n                'j_ids' (torch.Tensor): [M'],\n                'gt_mask' (torch.Tensor): [M'],\n                'mkpts0_c' (torch.Tensor): [M, 2],\n                'mkpts1_c' (torch.Tensor): [M, 2],\n                'mconf' (torch.Tensor): [M]}\n            NOTE: M' != M during training.\n        \"\"\"\n    (_, L, S, _) = (feat_c0.size(0), feat_c0.size(1), feat_c1.size(1), feat_c0.size(2))\n    (feat_c0, feat_c1) = map(lambda feat: feat / feat.shape[-1] ** 0.5, [feat_c0, feat_c1])\n    if self.match_type == 'dual_softmax':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1) / self.temperature\n        if mask_c0 is not None:\n            sim_matrix.masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        conf_matrix = F.softmax(sim_matrix, 1) * F.softmax(sim_matrix, 2)\n    elif self.match_type == 'sinkhorn':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1)\n        if mask_c0 is not None:\n            sim_matrix[:, :L, :S].masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        log_assign_matrix = self.log_optimal_transport(sim_matrix, self.bin_score, self.skh_iters)\n        assign_matrix = log_assign_matrix.exp()\n        conf_matrix = assign_matrix[:, :-1, :-1]\n        if not self.training and self.skh_prefilter:\n            filter0 = (assign_matrix.max(dim=2)[1] == S)[:, :-1]\n            filter1 = (assign_matrix.max(dim=1)[1] == L)[:, :-1]\n            conf_matrix[filter0[..., None].repeat(1, 1, S)] = 0\n            conf_matrix[filter1[:, None].repeat(1, L, 1)] = 0\n        if self.config['sparse_spvs']:\n            data.update({'conf_matrix_with_bin': assign_matrix.clone()})\n    data.update({'conf_matrix': conf_matrix})\n    data.update(**self.get_coarse_match(conf_matrix, data))",
        "mutated": [
            "def forward(self, feat_c0, feat_c1, data, mask_c0=None, mask_c1=None):\n    if False:\n        i = 10\n    \"\\n        Args:\\n            feat0 (torch.Tensor): [N, L, C]\\n            feat1 (torch.Tensor): [N, S, C]\\n            data (dict)\\n            mask_c0 (torch.Tensor): [N, L] (optional)\\n            mask_c1 (torch.Tensor): [N, S] (optional)\\n        Update:\\n            data (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n            NOTE: M' != M during training.\\n        \"\n    (_, L, S, _) = (feat_c0.size(0), feat_c0.size(1), feat_c1.size(1), feat_c0.size(2))\n    (feat_c0, feat_c1) = map(lambda feat: feat / feat.shape[-1] ** 0.5, [feat_c0, feat_c1])\n    if self.match_type == 'dual_softmax':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1) / self.temperature\n        if mask_c0 is not None:\n            sim_matrix.masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        conf_matrix = F.softmax(sim_matrix, 1) * F.softmax(sim_matrix, 2)\n    elif self.match_type == 'sinkhorn':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1)\n        if mask_c0 is not None:\n            sim_matrix[:, :L, :S].masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        log_assign_matrix = self.log_optimal_transport(sim_matrix, self.bin_score, self.skh_iters)\n        assign_matrix = log_assign_matrix.exp()\n        conf_matrix = assign_matrix[:, :-1, :-1]\n        if not self.training and self.skh_prefilter:\n            filter0 = (assign_matrix.max(dim=2)[1] == S)[:, :-1]\n            filter1 = (assign_matrix.max(dim=1)[1] == L)[:, :-1]\n            conf_matrix[filter0[..., None].repeat(1, 1, S)] = 0\n            conf_matrix[filter1[:, None].repeat(1, L, 1)] = 0\n        if self.config['sparse_spvs']:\n            data.update({'conf_matrix_with_bin': assign_matrix.clone()})\n    data.update({'conf_matrix': conf_matrix})\n    data.update(**self.get_coarse_match(conf_matrix, data))",
            "def forward(self, feat_c0, feat_c1, data, mask_c0=None, mask_c1=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Args:\\n            feat0 (torch.Tensor): [N, L, C]\\n            feat1 (torch.Tensor): [N, S, C]\\n            data (dict)\\n            mask_c0 (torch.Tensor): [N, L] (optional)\\n            mask_c1 (torch.Tensor): [N, S] (optional)\\n        Update:\\n            data (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n            NOTE: M' != M during training.\\n        \"\n    (_, L, S, _) = (feat_c0.size(0), feat_c0.size(1), feat_c1.size(1), feat_c0.size(2))\n    (feat_c0, feat_c1) = map(lambda feat: feat / feat.shape[-1] ** 0.5, [feat_c0, feat_c1])\n    if self.match_type == 'dual_softmax':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1) / self.temperature\n        if mask_c0 is not None:\n            sim_matrix.masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        conf_matrix = F.softmax(sim_matrix, 1) * F.softmax(sim_matrix, 2)\n    elif self.match_type == 'sinkhorn':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1)\n        if mask_c0 is not None:\n            sim_matrix[:, :L, :S].masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        log_assign_matrix = self.log_optimal_transport(sim_matrix, self.bin_score, self.skh_iters)\n        assign_matrix = log_assign_matrix.exp()\n        conf_matrix = assign_matrix[:, :-1, :-1]\n        if not self.training and self.skh_prefilter:\n            filter0 = (assign_matrix.max(dim=2)[1] == S)[:, :-1]\n            filter1 = (assign_matrix.max(dim=1)[1] == L)[:, :-1]\n            conf_matrix[filter0[..., None].repeat(1, 1, S)] = 0\n            conf_matrix[filter1[:, None].repeat(1, L, 1)] = 0\n        if self.config['sparse_spvs']:\n            data.update({'conf_matrix_with_bin': assign_matrix.clone()})\n    data.update({'conf_matrix': conf_matrix})\n    data.update(**self.get_coarse_match(conf_matrix, data))",
            "def forward(self, feat_c0, feat_c1, data, mask_c0=None, mask_c1=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Args:\\n            feat0 (torch.Tensor): [N, L, C]\\n            feat1 (torch.Tensor): [N, S, C]\\n            data (dict)\\n            mask_c0 (torch.Tensor): [N, L] (optional)\\n            mask_c1 (torch.Tensor): [N, S] (optional)\\n        Update:\\n            data (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n            NOTE: M' != M during training.\\n        \"\n    (_, L, S, _) = (feat_c0.size(0), feat_c0.size(1), feat_c1.size(1), feat_c0.size(2))\n    (feat_c0, feat_c1) = map(lambda feat: feat / feat.shape[-1] ** 0.5, [feat_c0, feat_c1])\n    if self.match_type == 'dual_softmax':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1) / self.temperature\n        if mask_c0 is not None:\n            sim_matrix.masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        conf_matrix = F.softmax(sim_matrix, 1) * F.softmax(sim_matrix, 2)\n    elif self.match_type == 'sinkhorn':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1)\n        if mask_c0 is not None:\n            sim_matrix[:, :L, :S].masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        log_assign_matrix = self.log_optimal_transport(sim_matrix, self.bin_score, self.skh_iters)\n        assign_matrix = log_assign_matrix.exp()\n        conf_matrix = assign_matrix[:, :-1, :-1]\n        if not self.training and self.skh_prefilter:\n            filter0 = (assign_matrix.max(dim=2)[1] == S)[:, :-1]\n            filter1 = (assign_matrix.max(dim=1)[1] == L)[:, :-1]\n            conf_matrix[filter0[..., None].repeat(1, 1, S)] = 0\n            conf_matrix[filter1[:, None].repeat(1, L, 1)] = 0\n        if self.config['sparse_spvs']:\n            data.update({'conf_matrix_with_bin': assign_matrix.clone()})\n    data.update({'conf_matrix': conf_matrix})\n    data.update(**self.get_coarse_match(conf_matrix, data))",
            "def forward(self, feat_c0, feat_c1, data, mask_c0=None, mask_c1=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Args:\\n            feat0 (torch.Tensor): [N, L, C]\\n            feat1 (torch.Tensor): [N, S, C]\\n            data (dict)\\n            mask_c0 (torch.Tensor): [N, L] (optional)\\n            mask_c1 (torch.Tensor): [N, S] (optional)\\n        Update:\\n            data (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n            NOTE: M' != M during training.\\n        \"\n    (_, L, S, _) = (feat_c0.size(0), feat_c0.size(1), feat_c1.size(1), feat_c0.size(2))\n    (feat_c0, feat_c1) = map(lambda feat: feat / feat.shape[-1] ** 0.5, [feat_c0, feat_c1])\n    if self.match_type == 'dual_softmax':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1) / self.temperature\n        if mask_c0 is not None:\n            sim_matrix.masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        conf_matrix = F.softmax(sim_matrix, 1) * F.softmax(sim_matrix, 2)\n    elif self.match_type == 'sinkhorn':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1)\n        if mask_c0 is not None:\n            sim_matrix[:, :L, :S].masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        log_assign_matrix = self.log_optimal_transport(sim_matrix, self.bin_score, self.skh_iters)\n        assign_matrix = log_assign_matrix.exp()\n        conf_matrix = assign_matrix[:, :-1, :-1]\n        if not self.training and self.skh_prefilter:\n            filter0 = (assign_matrix.max(dim=2)[1] == S)[:, :-1]\n            filter1 = (assign_matrix.max(dim=1)[1] == L)[:, :-1]\n            conf_matrix[filter0[..., None].repeat(1, 1, S)] = 0\n            conf_matrix[filter1[:, None].repeat(1, L, 1)] = 0\n        if self.config['sparse_spvs']:\n            data.update({'conf_matrix_with_bin': assign_matrix.clone()})\n    data.update({'conf_matrix': conf_matrix})\n    data.update(**self.get_coarse_match(conf_matrix, data))",
            "def forward(self, feat_c0, feat_c1, data, mask_c0=None, mask_c1=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Args:\\n            feat0 (torch.Tensor): [N, L, C]\\n            feat1 (torch.Tensor): [N, S, C]\\n            data (dict)\\n            mask_c0 (torch.Tensor): [N, L] (optional)\\n            mask_c1 (torch.Tensor): [N, S] (optional)\\n        Update:\\n            data (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n            NOTE: M' != M during training.\\n        \"\n    (_, L, S, _) = (feat_c0.size(0), feat_c0.size(1), feat_c1.size(1), feat_c0.size(2))\n    (feat_c0, feat_c1) = map(lambda feat: feat / feat.shape[-1] ** 0.5, [feat_c0, feat_c1])\n    if self.match_type == 'dual_softmax':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1) / self.temperature\n        if mask_c0 is not None:\n            sim_matrix.masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        conf_matrix = F.softmax(sim_matrix, 1) * F.softmax(sim_matrix, 2)\n    elif self.match_type == 'sinkhorn':\n        sim_matrix = torch.einsum('nlc,nsc->nls', feat_c0, feat_c1)\n        if mask_c0 is not None:\n            sim_matrix[:, :L, :S].masked_fill_(~(mask_c0[..., None] * mask_c1[:, None]).bool(), -INF)\n        log_assign_matrix = self.log_optimal_transport(sim_matrix, self.bin_score, self.skh_iters)\n        assign_matrix = log_assign_matrix.exp()\n        conf_matrix = assign_matrix[:, :-1, :-1]\n        if not self.training and self.skh_prefilter:\n            filter0 = (assign_matrix.max(dim=2)[1] == S)[:, :-1]\n            filter1 = (assign_matrix.max(dim=1)[1] == L)[:, :-1]\n            conf_matrix[filter0[..., None].repeat(1, 1, S)] = 0\n            conf_matrix[filter1[:, None].repeat(1, L, 1)] = 0\n        if self.config['sparse_spvs']:\n            data.update({'conf_matrix_with_bin': assign_matrix.clone()})\n    data.update({'conf_matrix': conf_matrix})\n    data.update(**self.get_coarse_match(conf_matrix, data))"
        ]
    },
    {
        "func_name": "get_coarse_match",
        "original": "@torch.no_grad()\ndef get_coarse_match(self, conf_matrix, data):\n    \"\"\"\n        Args:\n            conf_matrix (torch.Tensor): [N, L, S]\n            data (dict): with keys ['hw0_i', 'hw1_i', 'hw0_c', 'hw1_c']\n        Returns:\n            coarse_matches (dict): {\n                'b_ids' (torch.Tensor): [M'],\n                'i_ids' (torch.Tensor): [M'],\n                'j_ids' (torch.Tensor): [M'],\n                'gt_mask' (torch.Tensor): [M'],\n                'm_bids' (torch.Tensor): [M],\n                'mkpts0_c' (torch.Tensor): [M, 2],\n                'mkpts1_c' (torch.Tensor): [M, 2],\n                'mconf' (torch.Tensor): [M]}\n        \"\"\"\n    axes_lengths = {'h0c': data['hw0_c'][0], 'w0c': data['hw0_c'][1], 'h1c': data['hw1_c'][0], 'w1c': data['hw1_c'][1]}\n    _device = conf_matrix.device\n    mask = conf_matrix > self.thr\n    mask = rearrange(mask, 'b (h0c w0c) (h1c w1c) -> b h0c w0c h1c w1c', **axes_lengths)\n    if 'mask0' not in data:\n        mask_border(mask, self.border_rm, False)\n    else:\n        mask_border_with_padding(mask, self.border_rm, False, data['mask0'], data['mask1'])\n    mask = rearrange(mask, 'b h0c w0c h1c w1c -> b (h0c w0c) (h1c w1c)', **axes_lengths)\n    mask = mask * (conf_matrix == conf_matrix.max(dim=2, keepdim=True)[0]) * (conf_matrix == conf_matrix.max(dim=1, keepdim=True)[0])\n    (mask_v, all_j_ids) = mask.max(dim=2)\n    (b_ids, i_ids) = torch.where(mask_v)\n    j_ids = all_j_ids[b_ids, i_ids]\n    mconf = conf_matrix[b_ids, i_ids, j_ids]\n    if self.training:\n        if 'mask0' not in data:\n            num_candidates_max = mask.size(0) * max(mask.size(1), mask.size(2))\n        else:\n            num_candidates_max = compute_max_candidates(data['mask0'], data['mask1'])\n        num_matches_train = int(num_candidates_max * self.train_coarse_percent)\n        num_matches_pred = len(b_ids)\n        assert self.train_pad_num_gt_min < num_matches_train, 'min-num-gt-pad should be less than num-train-matches'\n        if num_matches_pred <= num_matches_train - self.train_pad_num_gt_min:\n            pred_indices = torch.arange(num_matches_pred, device=_device)\n        else:\n            pred_indices = torch.randint(num_matches_pred, (num_matches_train - self.train_pad_num_gt_min,), device=_device)\n        gt_pad_indices = torch.randint(len(data['spv_b_ids']), (max(num_matches_train - num_matches_pred, self.train_pad_num_gt_min),), device=_device)\n        mconf_gt = torch.zeros(len(data['spv_b_ids']), device=_device)\n        (b_ids, i_ids, j_ids, mconf) = map(lambda x, y: torch.cat([x[pred_indices], y[gt_pad_indices]], dim=0), *zip([b_ids, data['spv_b_ids']], [i_ids, data['spv_i_ids']], [j_ids, data['spv_j_ids']], [mconf, mconf_gt]))\n    coarse_matches = {'b_ids': b_ids, 'i_ids': i_ids, 'j_ids': j_ids}\n    scale = data['hw0_i'][0] / data['hw0_c'][0]\n    scale0 = scale * data['scale0'][b_ids] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][b_ids] if 'scale1' in data else scale\n    mkpts0_c = torch.stack([i_ids % data['hw0_c'][1], i_ids // data['hw0_c'][1]], dim=1) * scale0\n    mkpts1_c = torch.stack([j_ids % data['hw1_c'][1], j_ids // data['hw1_c'][1]], dim=1) * scale1\n    coarse_matches.update({'gt_mask': mconf == 0, 'm_bids': b_ids[mconf != 0], 'mkpts0_c': mkpts0_c[mconf != 0], 'mkpts1_c': mkpts1_c[mconf != 0], 'mconf': mconf[mconf != 0]})\n    return coarse_matches",
        "mutated": [
            "@torch.no_grad()\ndef get_coarse_match(self, conf_matrix, data):\n    if False:\n        i = 10\n    \"\\n        Args:\\n            conf_matrix (torch.Tensor): [N, L, S]\\n            data (dict): with keys ['hw0_i', 'hw1_i', 'hw0_c', 'hw1_c']\\n        Returns:\\n            coarse_matches (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'm_bids' (torch.Tensor): [M],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n        \"\n    axes_lengths = {'h0c': data['hw0_c'][0], 'w0c': data['hw0_c'][1], 'h1c': data['hw1_c'][0], 'w1c': data['hw1_c'][1]}\n    _device = conf_matrix.device\n    mask = conf_matrix > self.thr\n    mask = rearrange(mask, 'b (h0c w0c) (h1c w1c) -> b h0c w0c h1c w1c', **axes_lengths)\n    if 'mask0' not in data:\n        mask_border(mask, self.border_rm, False)\n    else:\n        mask_border_with_padding(mask, self.border_rm, False, data['mask0'], data['mask1'])\n    mask = rearrange(mask, 'b h0c w0c h1c w1c -> b (h0c w0c) (h1c w1c)', **axes_lengths)\n    mask = mask * (conf_matrix == conf_matrix.max(dim=2, keepdim=True)[0]) * (conf_matrix == conf_matrix.max(dim=1, keepdim=True)[0])\n    (mask_v, all_j_ids) = mask.max(dim=2)\n    (b_ids, i_ids) = torch.where(mask_v)\n    j_ids = all_j_ids[b_ids, i_ids]\n    mconf = conf_matrix[b_ids, i_ids, j_ids]\n    if self.training:\n        if 'mask0' not in data:\n            num_candidates_max = mask.size(0) * max(mask.size(1), mask.size(2))\n        else:\n            num_candidates_max = compute_max_candidates(data['mask0'], data['mask1'])\n        num_matches_train = int(num_candidates_max * self.train_coarse_percent)\n        num_matches_pred = len(b_ids)\n        assert self.train_pad_num_gt_min < num_matches_train, 'min-num-gt-pad should be less than num-train-matches'\n        if num_matches_pred <= num_matches_train - self.train_pad_num_gt_min:\n            pred_indices = torch.arange(num_matches_pred, device=_device)\n        else:\n            pred_indices = torch.randint(num_matches_pred, (num_matches_train - self.train_pad_num_gt_min,), device=_device)\n        gt_pad_indices = torch.randint(len(data['spv_b_ids']), (max(num_matches_train - num_matches_pred, self.train_pad_num_gt_min),), device=_device)\n        mconf_gt = torch.zeros(len(data['spv_b_ids']), device=_device)\n        (b_ids, i_ids, j_ids, mconf) = map(lambda x, y: torch.cat([x[pred_indices], y[gt_pad_indices]], dim=0), *zip([b_ids, data['spv_b_ids']], [i_ids, data['spv_i_ids']], [j_ids, data['spv_j_ids']], [mconf, mconf_gt]))\n    coarse_matches = {'b_ids': b_ids, 'i_ids': i_ids, 'j_ids': j_ids}\n    scale = data['hw0_i'][0] / data['hw0_c'][0]\n    scale0 = scale * data['scale0'][b_ids] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][b_ids] if 'scale1' in data else scale\n    mkpts0_c = torch.stack([i_ids % data['hw0_c'][1], i_ids // data['hw0_c'][1]], dim=1) * scale0\n    mkpts1_c = torch.stack([j_ids % data['hw1_c'][1], j_ids // data['hw1_c'][1]], dim=1) * scale1\n    coarse_matches.update({'gt_mask': mconf == 0, 'm_bids': b_ids[mconf != 0], 'mkpts0_c': mkpts0_c[mconf != 0], 'mkpts1_c': mkpts1_c[mconf != 0], 'mconf': mconf[mconf != 0]})\n    return coarse_matches",
            "@torch.no_grad()\ndef get_coarse_match(self, conf_matrix, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Args:\\n            conf_matrix (torch.Tensor): [N, L, S]\\n            data (dict): with keys ['hw0_i', 'hw1_i', 'hw0_c', 'hw1_c']\\n        Returns:\\n            coarse_matches (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'm_bids' (torch.Tensor): [M],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n        \"\n    axes_lengths = {'h0c': data['hw0_c'][0], 'w0c': data['hw0_c'][1], 'h1c': data['hw1_c'][0], 'w1c': data['hw1_c'][1]}\n    _device = conf_matrix.device\n    mask = conf_matrix > self.thr\n    mask = rearrange(mask, 'b (h0c w0c) (h1c w1c) -> b h0c w0c h1c w1c', **axes_lengths)\n    if 'mask0' not in data:\n        mask_border(mask, self.border_rm, False)\n    else:\n        mask_border_with_padding(mask, self.border_rm, False, data['mask0'], data['mask1'])\n    mask = rearrange(mask, 'b h0c w0c h1c w1c -> b (h0c w0c) (h1c w1c)', **axes_lengths)\n    mask = mask * (conf_matrix == conf_matrix.max(dim=2, keepdim=True)[0]) * (conf_matrix == conf_matrix.max(dim=1, keepdim=True)[0])\n    (mask_v, all_j_ids) = mask.max(dim=2)\n    (b_ids, i_ids) = torch.where(mask_v)\n    j_ids = all_j_ids[b_ids, i_ids]\n    mconf = conf_matrix[b_ids, i_ids, j_ids]\n    if self.training:\n        if 'mask0' not in data:\n            num_candidates_max = mask.size(0) * max(mask.size(1), mask.size(2))\n        else:\n            num_candidates_max = compute_max_candidates(data['mask0'], data['mask1'])\n        num_matches_train = int(num_candidates_max * self.train_coarse_percent)\n        num_matches_pred = len(b_ids)\n        assert self.train_pad_num_gt_min < num_matches_train, 'min-num-gt-pad should be less than num-train-matches'\n        if num_matches_pred <= num_matches_train - self.train_pad_num_gt_min:\n            pred_indices = torch.arange(num_matches_pred, device=_device)\n        else:\n            pred_indices = torch.randint(num_matches_pred, (num_matches_train - self.train_pad_num_gt_min,), device=_device)\n        gt_pad_indices = torch.randint(len(data['spv_b_ids']), (max(num_matches_train - num_matches_pred, self.train_pad_num_gt_min),), device=_device)\n        mconf_gt = torch.zeros(len(data['spv_b_ids']), device=_device)\n        (b_ids, i_ids, j_ids, mconf) = map(lambda x, y: torch.cat([x[pred_indices], y[gt_pad_indices]], dim=0), *zip([b_ids, data['spv_b_ids']], [i_ids, data['spv_i_ids']], [j_ids, data['spv_j_ids']], [mconf, mconf_gt]))\n    coarse_matches = {'b_ids': b_ids, 'i_ids': i_ids, 'j_ids': j_ids}\n    scale = data['hw0_i'][0] / data['hw0_c'][0]\n    scale0 = scale * data['scale0'][b_ids] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][b_ids] if 'scale1' in data else scale\n    mkpts0_c = torch.stack([i_ids % data['hw0_c'][1], i_ids // data['hw0_c'][1]], dim=1) * scale0\n    mkpts1_c = torch.stack([j_ids % data['hw1_c'][1], j_ids // data['hw1_c'][1]], dim=1) * scale1\n    coarse_matches.update({'gt_mask': mconf == 0, 'm_bids': b_ids[mconf != 0], 'mkpts0_c': mkpts0_c[mconf != 0], 'mkpts1_c': mkpts1_c[mconf != 0], 'mconf': mconf[mconf != 0]})\n    return coarse_matches",
            "@torch.no_grad()\ndef get_coarse_match(self, conf_matrix, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Args:\\n            conf_matrix (torch.Tensor): [N, L, S]\\n            data (dict): with keys ['hw0_i', 'hw1_i', 'hw0_c', 'hw1_c']\\n        Returns:\\n            coarse_matches (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'm_bids' (torch.Tensor): [M],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n        \"\n    axes_lengths = {'h0c': data['hw0_c'][0], 'w0c': data['hw0_c'][1], 'h1c': data['hw1_c'][0], 'w1c': data['hw1_c'][1]}\n    _device = conf_matrix.device\n    mask = conf_matrix > self.thr\n    mask = rearrange(mask, 'b (h0c w0c) (h1c w1c) -> b h0c w0c h1c w1c', **axes_lengths)\n    if 'mask0' not in data:\n        mask_border(mask, self.border_rm, False)\n    else:\n        mask_border_with_padding(mask, self.border_rm, False, data['mask0'], data['mask1'])\n    mask = rearrange(mask, 'b h0c w0c h1c w1c -> b (h0c w0c) (h1c w1c)', **axes_lengths)\n    mask = mask * (conf_matrix == conf_matrix.max(dim=2, keepdim=True)[0]) * (conf_matrix == conf_matrix.max(dim=1, keepdim=True)[0])\n    (mask_v, all_j_ids) = mask.max(dim=2)\n    (b_ids, i_ids) = torch.where(mask_v)\n    j_ids = all_j_ids[b_ids, i_ids]\n    mconf = conf_matrix[b_ids, i_ids, j_ids]\n    if self.training:\n        if 'mask0' not in data:\n            num_candidates_max = mask.size(0) * max(mask.size(1), mask.size(2))\n        else:\n            num_candidates_max = compute_max_candidates(data['mask0'], data['mask1'])\n        num_matches_train = int(num_candidates_max * self.train_coarse_percent)\n        num_matches_pred = len(b_ids)\n        assert self.train_pad_num_gt_min < num_matches_train, 'min-num-gt-pad should be less than num-train-matches'\n        if num_matches_pred <= num_matches_train - self.train_pad_num_gt_min:\n            pred_indices = torch.arange(num_matches_pred, device=_device)\n        else:\n            pred_indices = torch.randint(num_matches_pred, (num_matches_train - self.train_pad_num_gt_min,), device=_device)\n        gt_pad_indices = torch.randint(len(data['spv_b_ids']), (max(num_matches_train - num_matches_pred, self.train_pad_num_gt_min),), device=_device)\n        mconf_gt = torch.zeros(len(data['spv_b_ids']), device=_device)\n        (b_ids, i_ids, j_ids, mconf) = map(lambda x, y: torch.cat([x[pred_indices], y[gt_pad_indices]], dim=0), *zip([b_ids, data['spv_b_ids']], [i_ids, data['spv_i_ids']], [j_ids, data['spv_j_ids']], [mconf, mconf_gt]))\n    coarse_matches = {'b_ids': b_ids, 'i_ids': i_ids, 'j_ids': j_ids}\n    scale = data['hw0_i'][0] / data['hw0_c'][0]\n    scale0 = scale * data['scale0'][b_ids] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][b_ids] if 'scale1' in data else scale\n    mkpts0_c = torch.stack([i_ids % data['hw0_c'][1], i_ids // data['hw0_c'][1]], dim=1) * scale0\n    mkpts1_c = torch.stack([j_ids % data['hw1_c'][1], j_ids // data['hw1_c'][1]], dim=1) * scale1\n    coarse_matches.update({'gt_mask': mconf == 0, 'm_bids': b_ids[mconf != 0], 'mkpts0_c': mkpts0_c[mconf != 0], 'mkpts1_c': mkpts1_c[mconf != 0], 'mconf': mconf[mconf != 0]})\n    return coarse_matches",
            "@torch.no_grad()\ndef get_coarse_match(self, conf_matrix, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Args:\\n            conf_matrix (torch.Tensor): [N, L, S]\\n            data (dict): with keys ['hw0_i', 'hw1_i', 'hw0_c', 'hw1_c']\\n        Returns:\\n            coarse_matches (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'm_bids' (torch.Tensor): [M],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n        \"\n    axes_lengths = {'h0c': data['hw0_c'][0], 'w0c': data['hw0_c'][1], 'h1c': data['hw1_c'][0], 'w1c': data['hw1_c'][1]}\n    _device = conf_matrix.device\n    mask = conf_matrix > self.thr\n    mask = rearrange(mask, 'b (h0c w0c) (h1c w1c) -> b h0c w0c h1c w1c', **axes_lengths)\n    if 'mask0' not in data:\n        mask_border(mask, self.border_rm, False)\n    else:\n        mask_border_with_padding(mask, self.border_rm, False, data['mask0'], data['mask1'])\n    mask = rearrange(mask, 'b h0c w0c h1c w1c -> b (h0c w0c) (h1c w1c)', **axes_lengths)\n    mask = mask * (conf_matrix == conf_matrix.max(dim=2, keepdim=True)[0]) * (conf_matrix == conf_matrix.max(dim=1, keepdim=True)[0])\n    (mask_v, all_j_ids) = mask.max(dim=2)\n    (b_ids, i_ids) = torch.where(mask_v)\n    j_ids = all_j_ids[b_ids, i_ids]\n    mconf = conf_matrix[b_ids, i_ids, j_ids]\n    if self.training:\n        if 'mask0' not in data:\n            num_candidates_max = mask.size(0) * max(mask.size(1), mask.size(2))\n        else:\n            num_candidates_max = compute_max_candidates(data['mask0'], data['mask1'])\n        num_matches_train = int(num_candidates_max * self.train_coarse_percent)\n        num_matches_pred = len(b_ids)\n        assert self.train_pad_num_gt_min < num_matches_train, 'min-num-gt-pad should be less than num-train-matches'\n        if num_matches_pred <= num_matches_train - self.train_pad_num_gt_min:\n            pred_indices = torch.arange(num_matches_pred, device=_device)\n        else:\n            pred_indices = torch.randint(num_matches_pred, (num_matches_train - self.train_pad_num_gt_min,), device=_device)\n        gt_pad_indices = torch.randint(len(data['spv_b_ids']), (max(num_matches_train - num_matches_pred, self.train_pad_num_gt_min),), device=_device)\n        mconf_gt = torch.zeros(len(data['spv_b_ids']), device=_device)\n        (b_ids, i_ids, j_ids, mconf) = map(lambda x, y: torch.cat([x[pred_indices], y[gt_pad_indices]], dim=0), *zip([b_ids, data['spv_b_ids']], [i_ids, data['spv_i_ids']], [j_ids, data['spv_j_ids']], [mconf, mconf_gt]))\n    coarse_matches = {'b_ids': b_ids, 'i_ids': i_ids, 'j_ids': j_ids}\n    scale = data['hw0_i'][0] / data['hw0_c'][0]\n    scale0 = scale * data['scale0'][b_ids] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][b_ids] if 'scale1' in data else scale\n    mkpts0_c = torch.stack([i_ids % data['hw0_c'][1], i_ids // data['hw0_c'][1]], dim=1) * scale0\n    mkpts1_c = torch.stack([j_ids % data['hw1_c'][1], j_ids // data['hw1_c'][1]], dim=1) * scale1\n    coarse_matches.update({'gt_mask': mconf == 0, 'm_bids': b_ids[mconf != 0], 'mkpts0_c': mkpts0_c[mconf != 0], 'mkpts1_c': mkpts1_c[mconf != 0], 'mconf': mconf[mconf != 0]})\n    return coarse_matches",
            "@torch.no_grad()\ndef get_coarse_match(self, conf_matrix, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Args:\\n            conf_matrix (torch.Tensor): [N, L, S]\\n            data (dict): with keys ['hw0_i', 'hw1_i', 'hw0_c', 'hw1_c']\\n        Returns:\\n            coarse_matches (dict): {\\n                'b_ids' (torch.Tensor): [M'],\\n                'i_ids' (torch.Tensor): [M'],\\n                'j_ids' (torch.Tensor): [M'],\\n                'gt_mask' (torch.Tensor): [M'],\\n                'm_bids' (torch.Tensor): [M],\\n                'mkpts0_c' (torch.Tensor): [M, 2],\\n                'mkpts1_c' (torch.Tensor): [M, 2],\\n                'mconf' (torch.Tensor): [M]}\\n        \"\n    axes_lengths = {'h0c': data['hw0_c'][0], 'w0c': data['hw0_c'][1], 'h1c': data['hw1_c'][0], 'w1c': data['hw1_c'][1]}\n    _device = conf_matrix.device\n    mask = conf_matrix > self.thr\n    mask = rearrange(mask, 'b (h0c w0c) (h1c w1c) -> b h0c w0c h1c w1c', **axes_lengths)\n    if 'mask0' not in data:\n        mask_border(mask, self.border_rm, False)\n    else:\n        mask_border_with_padding(mask, self.border_rm, False, data['mask0'], data['mask1'])\n    mask = rearrange(mask, 'b h0c w0c h1c w1c -> b (h0c w0c) (h1c w1c)', **axes_lengths)\n    mask = mask * (conf_matrix == conf_matrix.max(dim=2, keepdim=True)[0]) * (conf_matrix == conf_matrix.max(dim=1, keepdim=True)[0])\n    (mask_v, all_j_ids) = mask.max(dim=2)\n    (b_ids, i_ids) = torch.where(mask_v)\n    j_ids = all_j_ids[b_ids, i_ids]\n    mconf = conf_matrix[b_ids, i_ids, j_ids]\n    if self.training:\n        if 'mask0' not in data:\n            num_candidates_max = mask.size(0) * max(mask.size(1), mask.size(2))\n        else:\n            num_candidates_max = compute_max_candidates(data['mask0'], data['mask1'])\n        num_matches_train = int(num_candidates_max * self.train_coarse_percent)\n        num_matches_pred = len(b_ids)\n        assert self.train_pad_num_gt_min < num_matches_train, 'min-num-gt-pad should be less than num-train-matches'\n        if num_matches_pred <= num_matches_train - self.train_pad_num_gt_min:\n            pred_indices = torch.arange(num_matches_pred, device=_device)\n        else:\n            pred_indices = torch.randint(num_matches_pred, (num_matches_train - self.train_pad_num_gt_min,), device=_device)\n        gt_pad_indices = torch.randint(len(data['spv_b_ids']), (max(num_matches_train - num_matches_pred, self.train_pad_num_gt_min),), device=_device)\n        mconf_gt = torch.zeros(len(data['spv_b_ids']), device=_device)\n        (b_ids, i_ids, j_ids, mconf) = map(lambda x, y: torch.cat([x[pred_indices], y[gt_pad_indices]], dim=0), *zip([b_ids, data['spv_b_ids']], [i_ids, data['spv_i_ids']], [j_ids, data['spv_j_ids']], [mconf, mconf_gt]))\n    coarse_matches = {'b_ids': b_ids, 'i_ids': i_ids, 'j_ids': j_ids}\n    scale = data['hw0_i'][0] / data['hw0_c'][0]\n    scale0 = scale * data['scale0'][b_ids] if 'scale0' in data else scale\n    scale1 = scale * data['scale1'][b_ids] if 'scale1' in data else scale\n    mkpts0_c = torch.stack([i_ids % data['hw0_c'][1], i_ids // data['hw0_c'][1]], dim=1) * scale0\n    mkpts1_c = torch.stack([j_ids % data['hw1_c'][1], j_ids // data['hw1_c'][1]], dim=1) * scale1\n    coarse_matches.update({'gt_mask': mconf == 0, 'm_bids': b_ids[mconf != 0], 'mkpts0_c': mkpts0_c[mconf != 0], 'mkpts1_c': mkpts1_c[mconf != 0], 'mconf': mconf[mconf != 0]})\n    return coarse_matches"
        ]
    }
]