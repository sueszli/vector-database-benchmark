[
    {
        "func_name": "make_time_major",
        "original": "def make_time_major(tensor: Union['torch.Tensor', List['torch.Tensor']], *, trajectory_len: int=None, recurrent_seq_len: int=None):\n    \"\"\"Swaps batch and trajectory axis.\n\n    Args:\n        tensor: A tensor or list of tensors to swap the axis of.\n            NOTE: Each tensor must have the shape [B * T] where B is the batch size and\n            T is the trajectory length.\n        trajectory_len: The length of each trajectory being transformed.\n            If None then `recurrent_seq_len` must be set.\n        recurrent_seq_len: Sequence lengths if recurrent.\n            If None then `trajectory_len` must be set.\n\n    Returns:\n        res: A tensor with swapped axes or a list of tensors with\n        swapped axes.\n    \"\"\"\n    if isinstance(tensor, (list, tuple)):\n        return [make_time_major(_tensor, trajectory_len, recurrent_seq_len) for _tensor in tensor]\n    assert trajectory_len != recurrent_seq_len and (trajectory_len is None or recurrent_seq_len is None), 'Either trajectory_len or recurrent_seq_len must be set.'\n    if recurrent_seq_len:\n        B = recurrent_seq_len.shape[0]\n        T = tensor.shape[0] // B\n    else:\n        T = trajectory_len\n        B = tensor.shape[0] // T\n    rs = torch.reshape(tensor, [B, T] + list(tensor.shape[1:]))\n    res = torch.transpose(rs, 1, 0)\n    return res",
        "mutated": [
            "def make_time_major(tensor: Union['torch.Tensor', List['torch.Tensor']], *, trajectory_len: int=None, recurrent_seq_len: int=None):\n    if False:\n        i = 10\n    'Swaps batch and trajectory axis.\\n\\n    Args:\\n        tensor: A tensor or list of tensors to swap the axis of.\\n            NOTE: Each tensor must have the shape [B * T] where B is the batch size and\\n            T is the trajectory length.\\n        trajectory_len: The length of each trajectory being transformed.\\n            If None then `recurrent_seq_len` must be set.\\n        recurrent_seq_len: Sequence lengths if recurrent.\\n            If None then `trajectory_len` must be set.\\n\\n    Returns:\\n        res: A tensor with swapped axes or a list of tensors with\\n        swapped axes.\\n    '\n    if isinstance(tensor, (list, tuple)):\n        return [make_time_major(_tensor, trajectory_len, recurrent_seq_len) for _tensor in tensor]\n    assert trajectory_len != recurrent_seq_len and (trajectory_len is None or recurrent_seq_len is None), 'Either trajectory_len or recurrent_seq_len must be set.'\n    if recurrent_seq_len:\n        B = recurrent_seq_len.shape[0]\n        T = tensor.shape[0] // B\n    else:\n        T = trajectory_len\n        B = tensor.shape[0] // T\n    rs = torch.reshape(tensor, [B, T] + list(tensor.shape[1:]))\n    res = torch.transpose(rs, 1, 0)\n    return res",
            "def make_time_major(tensor: Union['torch.Tensor', List['torch.Tensor']], *, trajectory_len: int=None, recurrent_seq_len: int=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Swaps batch and trajectory axis.\\n\\n    Args:\\n        tensor: A tensor or list of tensors to swap the axis of.\\n            NOTE: Each tensor must have the shape [B * T] where B is the batch size and\\n            T is the trajectory length.\\n        trajectory_len: The length of each trajectory being transformed.\\n            If None then `recurrent_seq_len` must be set.\\n        recurrent_seq_len: Sequence lengths if recurrent.\\n            If None then `trajectory_len` must be set.\\n\\n    Returns:\\n        res: A tensor with swapped axes or a list of tensors with\\n        swapped axes.\\n    '\n    if isinstance(tensor, (list, tuple)):\n        return [make_time_major(_tensor, trajectory_len, recurrent_seq_len) for _tensor in tensor]\n    assert trajectory_len != recurrent_seq_len and (trajectory_len is None or recurrent_seq_len is None), 'Either trajectory_len or recurrent_seq_len must be set.'\n    if recurrent_seq_len:\n        B = recurrent_seq_len.shape[0]\n        T = tensor.shape[0] // B\n    else:\n        T = trajectory_len\n        B = tensor.shape[0] // T\n    rs = torch.reshape(tensor, [B, T] + list(tensor.shape[1:]))\n    res = torch.transpose(rs, 1, 0)\n    return res",
            "def make_time_major(tensor: Union['torch.Tensor', List['torch.Tensor']], *, trajectory_len: int=None, recurrent_seq_len: int=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Swaps batch and trajectory axis.\\n\\n    Args:\\n        tensor: A tensor or list of tensors to swap the axis of.\\n            NOTE: Each tensor must have the shape [B * T] where B is the batch size and\\n            T is the trajectory length.\\n        trajectory_len: The length of each trajectory being transformed.\\n            If None then `recurrent_seq_len` must be set.\\n        recurrent_seq_len: Sequence lengths if recurrent.\\n            If None then `trajectory_len` must be set.\\n\\n    Returns:\\n        res: A tensor with swapped axes or a list of tensors with\\n        swapped axes.\\n    '\n    if isinstance(tensor, (list, tuple)):\n        return [make_time_major(_tensor, trajectory_len, recurrent_seq_len) for _tensor in tensor]\n    assert trajectory_len != recurrent_seq_len and (trajectory_len is None or recurrent_seq_len is None), 'Either trajectory_len or recurrent_seq_len must be set.'\n    if recurrent_seq_len:\n        B = recurrent_seq_len.shape[0]\n        T = tensor.shape[0] // B\n    else:\n        T = trajectory_len\n        B = tensor.shape[0] // T\n    rs = torch.reshape(tensor, [B, T] + list(tensor.shape[1:]))\n    res = torch.transpose(rs, 1, 0)\n    return res",
            "def make_time_major(tensor: Union['torch.Tensor', List['torch.Tensor']], *, trajectory_len: int=None, recurrent_seq_len: int=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Swaps batch and trajectory axis.\\n\\n    Args:\\n        tensor: A tensor or list of tensors to swap the axis of.\\n            NOTE: Each tensor must have the shape [B * T] where B is the batch size and\\n            T is the trajectory length.\\n        trajectory_len: The length of each trajectory being transformed.\\n            If None then `recurrent_seq_len` must be set.\\n        recurrent_seq_len: Sequence lengths if recurrent.\\n            If None then `trajectory_len` must be set.\\n\\n    Returns:\\n        res: A tensor with swapped axes or a list of tensors with\\n        swapped axes.\\n    '\n    if isinstance(tensor, (list, tuple)):\n        return [make_time_major(_tensor, trajectory_len, recurrent_seq_len) for _tensor in tensor]\n    assert trajectory_len != recurrent_seq_len and (trajectory_len is None or recurrent_seq_len is None), 'Either trajectory_len or recurrent_seq_len must be set.'\n    if recurrent_seq_len:\n        B = recurrent_seq_len.shape[0]\n        T = tensor.shape[0] // B\n    else:\n        T = trajectory_len\n        B = tensor.shape[0] // T\n    rs = torch.reshape(tensor, [B, T] + list(tensor.shape[1:]))\n    res = torch.transpose(rs, 1, 0)\n    return res",
            "def make_time_major(tensor: Union['torch.Tensor', List['torch.Tensor']], *, trajectory_len: int=None, recurrent_seq_len: int=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Swaps batch and trajectory axis.\\n\\n    Args:\\n        tensor: A tensor or list of tensors to swap the axis of.\\n            NOTE: Each tensor must have the shape [B * T] where B is the batch size and\\n            T is the trajectory length.\\n        trajectory_len: The length of each trajectory being transformed.\\n            If None then `recurrent_seq_len` must be set.\\n        recurrent_seq_len: Sequence lengths if recurrent.\\n            If None then `trajectory_len` must be set.\\n\\n    Returns:\\n        res: A tensor with swapped axes or a list of tensors with\\n        swapped axes.\\n    '\n    if isinstance(tensor, (list, tuple)):\n        return [make_time_major(_tensor, trajectory_len, recurrent_seq_len) for _tensor in tensor]\n    assert trajectory_len != recurrent_seq_len and (trajectory_len is None or recurrent_seq_len is None), 'Either trajectory_len or recurrent_seq_len must be set.'\n    if recurrent_seq_len:\n        B = recurrent_seq_len.shape[0]\n        T = tensor.shape[0] // B\n    else:\n        T = trajectory_len\n        B = tensor.shape[0] // T\n    rs = torch.reshape(tensor, [B, T] + list(tensor.shape[1:]))\n    res = torch.transpose(rs, 1, 0)\n    return res"
        ]
    },
    {
        "func_name": "vtrace_torch",
        "original": "def vtrace_torch(*, target_action_log_probs: 'torch.Tensor', behaviour_action_log_probs: 'torch.Tensor', discounts: 'torch.Tensor', rewards: 'torch.Tensor', values: 'torch.Tensor', bootstrap_value: 'torch.Tensor', clip_rho_threshold: Union[float, 'torch.Tensor']=1.0, clip_pg_rho_threshold: Union[float, 'torch.Tensor']=1.0):\n    \"\"\"V-trace for softmax policies implemented with torch.\n\n    Calculates V-trace actor critic targets for softmax polices as described in\n    \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner\n    Architectures\" by Espeholt, Soyer, Munos et al. (https://arxiv.org/abs/1802.01561)\n\n    The V-trace implementation used here closely resembles the one found in the\n    scalable-agent repository by Google DeepMind, available at\n    https://github.com/deepmind/scalable_agent. This version has been optimized to\n    minimize the number of floating-point operations required per V-Trace\n    calculation, achieved through the use of dynamic programming techniques. It's\n    important to note that the mathematical expressions used in this implementation\n    may appear quite different from those presented in the IMPALA paper.\n\n    The following terminology applies:\n        - `target policy` refers to the policy we are interested in improving.\n        - `behaviour policy` refers to the policy that generated the given\n            rewards and actions.\n        - `T` refers to the time dimension. This is usually either the length of the\n            trajectory or the length of the sequence if recurrent.\n        - `B` refers to the batch size.\n\n    Args:\n        target_action_log_probs: Action log probs from the target policy. A float32\n            tensor of shape [T, B].\n        behaviour_action_log_probs: Action log probs from the behaviour policy. A\n            float32 tensor of shape [T, B].\n        discounts: A float32 tensor of shape [T, B] with the discount encountered when\n            following the behaviour policy. This will be 0 for terminal timesteps\n            (done=True) and gamma (the discount factor) otherwise.\n        rewards: A float32 tensor of shape [T, B] with the rewards generated by\n            following the behaviour policy.\n        values: A float32 tensor of shape [T, B] with the value function estimates\n            wrt. the target policy.\n        bootstrap_value: A float32 of shape [B] with the value function estimate at\n            time T.\n        clip_rho_threshold: A scalar float32 tensor with the clipping threshold for\n            importance weights (rho) when calculating the baseline targets (vs).\n            rho^bar in the paper.\n        clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold\n            on rho_s in \\rho_s \\\\delta log \\\\pi(a|x) (r + \\\\gamma v_{s+1} - V(x_s)).\n    \"\"\"\n    log_rhos = target_action_log_probs - behaviour_action_log_probs\n    rhos = torch.exp(log_rhos)\n    if clip_rho_threshold is not None:\n        clipped_rhos = torch.clamp(rhos, max=clip_rho_threshold)\n    else:\n        clipped_rhos = rhos\n    cs = torch.clamp(rhos, max=1.0)\n    values_t_plus_1 = torch.cat([values[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n    discounts_cpu = discounts.to('cpu')\n    cs_cpu = cs.to('cpu')\n    deltas_cpu = deltas.to('cpu')\n    vs_minus_v_xs_cpu = [torch.zeros_like(bootstrap_value, device='cpu')]\n    for i in reversed(range(len(discounts_cpu))):\n        (discount_t, c_t, delta_t) = (discounts_cpu[i], cs_cpu[i], deltas_cpu[i])\n        vs_minus_v_xs_cpu.append(delta_t + discount_t * c_t * vs_minus_v_xs_cpu[-1])\n    vs_minus_v_xs_cpu = torch.stack(vs_minus_v_xs_cpu[1:])\n    vs_minus_v_xs = vs_minus_v_xs_cpu.to(deltas.device)\n    vs_minus_v_xs = torch.flip(vs_minus_v_xs, dims=[0])\n    vs = torch.add(vs_minus_v_xs, values)\n    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    if clip_pg_rho_threshold is not None:\n        clipped_pg_rhos = torch.clamp(rhos, max=clip_pg_rho_threshold)\n    else:\n        clipped_pg_rhos = rhos\n    pg_advantages = clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values)\n    return (torch.detach(vs), torch.detach(pg_advantages))",
        "mutated": [
            "def vtrace_torch(*, target_action_log_probs: 'torch.Tensor', behaviour_action_log_probs: 'torch.Tensor', discounts: 'torch.Tensor', rewards: 'torch.Tensor', values: 'torch.Tensor', bootstrap_value: 'torch.Tensor', clip_rho_threshold: Union[float, 'torch.Tensor']=1.0, clip_pg_rho_threshold: Union[float, 'torch.Tensor']=1.0):\n    if False:\n        i = 10\n    'V-trace for softmax policies implemented with torch.\\n\\n    Calculates V-trace actor critic targets for softmax polices as described in\\n    \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner\\n    Architectures\" by Espeholt, Soyer, Munos et al. (https://arxiv.org/abs/1802.01561)\\n\\n    The V-trace implementation used here closely resembles the one found in the\\n    scalable-agent repository by Google DeepMind, available at\\n    https://github.com/deepmind/scalable_agent. This version has been optimized to\\n    minimize the number of floating-point operations required per V-Trace\\n    calculation, achieved through the use of dynamic programming techniques. It\\'s\\n    important to note that the mathematical expressions used in this implementation\\n    may appear quite different from those presented in the IMPALA paper.\\n\\n    The following terminology applies:\\n        - `target policy` refers to the policy we are interested in improving.\\n        - `behaviour policy` refers to the policy that generated the given\\n            rewards and actions.\\n        - `T` refers to the time dimension. This is usually either the length of the\\n            trajectory or the length of the sequence if recurrent.\\n        - `B` refers to the batch size.\\n\\n    Args:\\n        target_action_log_probs: Action log probs from the target policy. A float32\\n            tensor of shape [T, B].\\n        behaviour_action_log_probs: Action log probs from the behaviour policy. A\\n            float32 tensor of shape [T, B].\\n        discounts: A float32 tensor of shape [T, B] with the discount encountered when\\n            following the behaviour policy. This will be 0 for terminal timesteps\\n            (done=True) and gamma (the discount factor) otherwise.\\n        rewards: A float32 tensor of shape [T, B] with the rewards generated by\\n            following the behaviour policy.\\n        values: A float32 tensor of shape [T, B] with the value function estimates\\n            wrt. the target policy.\\n        bootstrap_value: A float32 of shape [B] with the value function estimate at\\n            time T.\\n        clip_rho_threshold: A scalar float32 tensor with the clipping threshold for\\n            importance weights (rho) when calculating the baseline targets (vs).\\n            rho^bar in the paper.\\n        clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold\\n            on rho_s in \\rho_s \\\\delta log \\\\pi(a|x) (r + \\\\gamma v_{s+1} - V(x_s)).\\n    '\n    log_rhos = target_action_log_probs - behaviour_action_log_probs\n    rhos = torch.exp(log_rhos)\n    if clip_rho_threshold is not None:\n        clipped_rhos = torch.clamp(rhos, max=clip_rho_threshold)\n    else:\n        clipped_rhos = rhos\n    cs = torch.clamp(rhos, max=1.0)\n    values_t_plus_1 = torch.cat([values[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n    discounts_cpu = discounts.to('cpu')\n    cs_cpu = cs.to('cpu')\n    deltas_cpu = deltas.to('cpu')\n    vs_minus_v_xs_cpu = [torch.zeros_like(bootstrap_value, device='cpu')]\n    for i in reversed(range(len(discounts_cpu))):\n        (discount_t, c_t, delta_t) = (discounts_cpu[i], cs_cpu[i], deltas_cpu[i])\n        vs_minus_v_xs_cpu.append(delta_t + discount_t * c_t * vs_minus_v_xs_cpu[-1])\n    vs_minus_v_xs_cpu = torch.stack(vs_minus_v_xs_cpu[1:])\n    vs_minus_v_xs = vs_minus_v_xs_cpu.to(deltas.device)\n    vs_minus_v_xs = torch.flip(vs_minus_v_xs, dims=[0])\n    vs = torch.add(vs_minus_v_xs, values)\n    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    if clip_pg_rho_threshold is not None:\n        clipped_pg_rhos = torch.clamp(rhos, max=clip_pg_rho_threshold)\n    else:\n        clipped_pg_rhos = rhos\n    pg_advantages = clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values)\n    return (torch.detach(vs), torch.detach(pg_advantages))",
            "def vtrace_torch(*, target_action_log_probs: 'torch.Tensor', behaviour_action_log_probs: 'torch.Tensor', discounts: 'torch.Tensor', rewards: 'torch.Tensor', values: 'torch.Tensor', bootstrap_value: 'torch.Tensor', clip_rho_threshold: Union[float, 'torch.Tensor']=1.0, clip_pg_rho_threshold: Union[float, 'torch.Tensor']=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'V-trace for softmax policies implemented with torch.\\n\\n    Calculates V-trace actor critic targets for softmax polices as described in\\n    \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner\\n    Architectures\" by Espeholt, Soyer, Munos et al. (https://arxiv.org/abs/1802.01561)\\n\\n    The V-trace implementation used here closely resembles the one found in the\\n    scalable-agent repository by Google DeepMind, available at\\n    https://github.com/deepmind/scalable_agent. This version has been optimized to\\n    minimize the number of floating-point operations required per V-Trace\\n    calculation, achieved through the use of dynamic programming techniques. It\\'s\\n    important to note that the mathematical expressions used in this implementation\\n    may appear quite different from those presented in the IMPALA paper.\\n\\n    The following terminology applies:\\n        - `target policy` refers to the policy we are interested in improving.\\n        - `behaviour policy` refers to the policy that generated the given\\n            rewards and actions.\\n        - `T` refers to the time dimension. This is usually either the length of the\\n            trajectory or the length of the sequence if recurrent.\\n        - `B` refers to the batch size.\\n\\n    Args:\\n        target_action_log_probs: Action log probs from the target policy. A float32\\n            tensor of shape [T, B].\\n        behaviour_action_log_probs: Action log probs from the behaviour policy. A\\n            float32 tensor of shape [T, B].\\n        discounts: A float32 tensor of shape [T, B] with the discount encountered when\\n            following the behaviour policy. This will be 0 for terminal timesteps\\n            (done=True) and gamma (the discount factor) otherwise.\\n        rewards: A float32 tensor of shape [T, B] with the rewards generated by\\n            following the behaviour policy.\\n        values: A float32 tensor of shape [T, B] with the value function estimates\\n            wrt. the target policy.\\n        bootstrap_value: A float32 of shape [B] with the value function estimate at\\n            time T.\\n        clip_rho_threshold: A scalar float32 tensor with the clipping threshold for\\n            importance weights (rho) when calculating the baseline targets (vs).\\n            rho^bar in the paper.\\n        clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold\\n            on rho_s in \\rho_s \\\\delta log \\\\pi(a|x) (r + \\\\gamma v_{s+1} - V(x_s)).\\n    '\n    log_rhos = target_action_log_probs - behaviour_action_log_probs\n    rhos = torch.exp(log_rhos)\n    if clip_rho_threshold is not None:\n        clipped_rhos = torch.clamp(rhos, max=clip_rho_threshold)\n    else:\n        clipped_rhos = rhos\n    cs = torch.clamp(rhos, max=1.0)\n    values_t_plus_1 = torch.cat([values[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n    discounts_cpu = discounts.to('cpu')\n    cs_cpu = cs.to('cpu')\n    deltas_cpu = deltas.to('cpu')\n    vs_minus_v_xs_cpu = [torch.zeros_like(bootstrap_value, device='cpu')]\n    for i in reversed(range(len(discounts_cpu))):\n        (discount_t, c_t, delta_t) = (discounts_cpu[i], cs_cpu[i], deltas_cpu[i])\n        vs_minus_v_xs_cpu.append(delta_t + discount_t * c_t * vs_minus_v_xs_cpu[-1])\n    vs_minus_v_xs_cpu = torch.stack(vs_minus_v_xs_cpu[1:])\n    vs_minus_v_xs = vs_minus_v_xs_cpu.to(deltas.device)\n    vs_minus_v_xs = torch.flip(vs_minus_v_xs, dims=[0])\n    vs = torch.add(vs_minus_v_xs, values)\n    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    if clip_pg_rho_threshold is not None:\n        clipped_pg_rhos = torch.clamp(rhos, max=clip_pg_rho_threshold)\n    else:\n        clipped_pg_rhos = rhos\n    pg_advantages = clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values)\n    return (torch.detach(vs), torch.detach(pg_advantages))",
            "def vtrace_torch(*, target_action_log_probs: 'torch.Tensor', behaviour_action_log_probs: 'torch.Tensor', discounts: 'torch.Tensor', rewards: 'torch.Tensor', values: 'torch.Tensor', bootstrap_value: 'torch.Tensor', clip_rho_threshold: Union[float, 'torch.Tensor']=1.0, clip_pg_rho_threshold: Union[float, 'torch.Tensor']=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'V-trace for softmax policies implemented with torch.\\n\\n    Calculates V-trace actor critic targets for softmax polices as described in\\n    \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner\\n    Architectures\" by Espeholt, Soyer, Munos et al. (https://arxiv.org/abs/1802.01561)\\n\\n    The V-trace implementation used here closely resembles the one found in the\\n    scalable-agent repository by Google DeepMind, available at\\n    https://github.com/deepmind/scalable_agent. This version has been optimized to\\n    minimize the number of floating-point operations required per V-Trace\\n    calculation, achieved through the use of dynamic programming techniques. It\\'s\\n    important to note that the mathematical expressions used in this implementation\\n    may appear quite different from those presented in the IMPALA paper.\\n\\n    The following terminology applies:\\n        - `target policy` refers to the policy we are interested in improving.\\n        - `behaviour policy` refers to the policy that generated the given\\n            rewards and actions.\\n        - `T` refers to the time dimension. This is usually either the length of the\\n            trajectory or the length of the sequence if recurrent.\\n        - `B` refers to the batch size.\\n\\n    Args:\\n        target_action_log_probs: Action log probs from the target policy. A float32\\n            tensor of shape [T, B].\\n        behaviour_action_log_probs: Action log probs from the behaviour policy. A\\n            float32 tensor of shape [T, B].\\n        discounts: A float32 tensor of shape [T, B] with the discount encountered when\\n            following the behaviour policy. This will be 0 for terminal timesteps\\n            (done=True) and gamma (the discount factor) otherwise.\\n        rewards: A float32 tensor of shape [T, B] with the rewards generated by\\n            following the behaviour policy.\\n        values: A float32 tensor of shape [T, B] with the value function estimates\\n            wrt. the target policy.\\n        bootstrap_value: A float32 of shape [B] with the value function estimate at\\n            time T.\\n        clip_rho_threshold: A scalar float32 tensor with the clipping threshold for\\n            importance weights (rho) when calculating the baseline targets (vs).\\n            rho^bar in the paper.\\n        clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold\\n            on rho_s in \\rho_s \\\\delta log \\\\pi(a|x) (r + \\\\gamma v_{s+1} - V(x_s)).\\n    '\n    log_rhos = target_action_log_probs - behaviour_action_log_probs\n    rhos = torch.exp(log_rhos)\n    if clip_rho_threshold is not None:\n        clipped_rhos = torch.clamp(rhos, max=clip_rho_threshold)\n    else:\n        clipped_rhos = rhos\n    cs = torch.clamp(rhos, max=1.0)\n    values_t_plus_1 = torch.cat([values[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n    discounts_cpu = discounts.to('cpu')\n    cs_cpu = cs.to('cpu')\n    deltas_cpu = deltas.to('cpu')\n    vs_minus_v_xs_cpu = [torch.zeros_like(bootstrap_value, device='cpu')]\n    for i in reversed(range(len(discounts_cpu))):\n        (discount_t, c_t, delta_t) = (discounts_cpu[i], cs_cpu[i], deltas_cpu[i])\n        vs_minus_v_xs_cpu.append(delta_t + discount_t * c_t * vs_minus_v_xs_cpu[-1])\n    vs_minus_v_xs_cpu = torch.stack(vs_minus_v_xs_cpu[1:])\n    vs_minus_v_xs = vs_minus_v_xs_cpu.to(deltas.device)\n    vs_minus_v_xs = torch.flip(vs_minus_v_xs, dims=[0])\n    vs = torch.add(vs_minus_v_xs, values)\n    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    if clip_pg_rho_threshold is not None:\n        clipped_pg_rhos = torch.clamp(rhos, max=clip_pg_rho_threshold)\n    else:\n        clipped_pg_rhos = rhos\n    pg_advantages = clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values)\n    return (torch.detach(vs), torch.detach(pg_advantages))",
            "def vtrace_torch(*, target_action_log_probs: 'torch.Tensor', behaviour_action_log_probs: 'torch.Tensor', discounts: 'torch.Tensor', rewards: 'torch.Tensor', values: 'torch.Tensor', bootstrap_value: 'torch.Tensor', clip_rho_threshold: Union[float, 'torch.Tensor']=1.0, clip_pg_rho_threshold: Union[float, 'torch.Tensor']=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'V-trace for softmax policies implemented with torch.\\n\\n    Calculates V-trace actor critic targets for softmax polices as described in\\n    \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner\\n    Architectures\" by Espeholt, Soyer, Munos et al. (https://arxiv.org/abs/1802.01561)\\n\\n    The V-trace implementation used here closely resembles the one found in the\\n    scalable-agent repository by Google DeepMind, available at\\n    https://github.com/deepmind/scalable_agent. This version has been optimized to\\n    minimize the number of floating-point operations required per V-Trace\\n    calculation, achieved through the use of dynamic programming techniques. It\\'s\\n    important to note that the mathematical expressions used in this implementation\\n    may appear quite different from those presented in the IMPALA paper.\\n\\n    The following terminology applies:\\n        - `target policy` refers to the policy we are interested in improving.\\n        - `behaviour policy` refers to the policy that generated the given\\n            rewards and actions.\\n        - `T` refers to the time dimension. This is usually either the length of the\\n            trajectory or the length of the sequence if recurrent.\\n        - `B` refers to the batch size.\\n\\n    Args:\\n        target_action_log_probs: Action log probs from the target policy. A float32\\n            tensor of shape [T, B].\\n        behaviour_action_log_probs: Action log probs from the behaviour policy. A\\n            float32 tensor of shape [T, B].\\n        discounts: A float32 tensor of shape [T, B] with the discount encountered when\\n            following the behaviour policy. This will be 0 for terminal timesteps\\n            (done=True) and gamma (the discount factor) otherwise.\\n        rewards: A float32 tensor of shape [T, B] with the rewards generated by\\n            following the behaviour policy.\\n        values: A float32 tensor of shape [T, B] with the value function estimates\\n            wrt. the target policy.\\n        bootstrap_value: A float32 of shape [B] with the value function estimate at\\n            time T.\\n        clip_rho_threshold: A scalar float32 tensor with the clipping threshold for\\n            importance weights (rho) when calculating the baseline targets (vs).\\n            rho^bar in the paper.\\n        clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold\\n            on rho_s in \\rho_s \\\\delta log \\\\pi(a|x) (r + \\\\gamma v_{s+1} - V(x_s)).\\n    '\n    log_rhos = target_action_log_probs - behaviour_action_log_probs\n    rhos = torch.exp(log_rhos)\n    if clip_rho_threshold is not None:\n        clipped_rhos = torch.clamp(rhos, max=clip_rho_threshold)\n    else:\n        clipped_rhos = rhos\n    cs = torch.clamp(rhos, max=1.0)\n    values_t_plus_1 = torch.cat([values[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n    discounts_cpu = discounts.to('cpu')\n    cs_cpu = cs.to('cpu')\n    deltas_cpu = deltas.to('cpu')\n    vs_minus_v_xs_cpu = [torch.zeros_like(bootstrap_value, device='cpu')]\n    for i in reversed(range(len(discounts_cpu))):\n        (discount_t, c_t, delta_t) = (discounts_cpu[i], cs_cpu[i], deltas_cpu[i])\n        vs_minus_v_xs_cpu.append(delta_t + discount_t * c_t * vs_minus_v_xs_cpu[-1])\n    vs_minus_v_xs_cpu = torch.stack(vs_minus_v_xs_cpu[1:])\n    vs_minus_v_xs = vs_minus_v_xs_cpu.to(deltas.device)\n    vs_minus_v_xs = torch.flip(vs_minus_v_xs, dims=[0])\n    vs = torch.add(vs_minus_v_xs, values)\n    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    if clip_pg_rho_threshold is not None:\n        clipped_pg_rhos = torch.clamp(rhos, max=clip_pg_rho_threshold)\n    else:\n        clipped_pg_rhos = rhos\n    pg_advantages = clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values)\n    return (torch.detach(vs), torch.detach(pg_advantages))",
            "def vtrace_torch(*, target_action_log_probs: 'torch.Tensor', behaviour_action_log_probs: 'torch.Tensor', discounts: 'torch.Tensor', rewards: 'torch.Tensor', values: 'torch.Tensor', bootstrap_value: 'torch.Tensor', clip_rho_threshold: Union[float, 'torch.Tensor']=1.0, clip_pg_rho_threshold: Union[float, 'torch.Tensor']=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'V-trace for softmax policies implemented with torch.\\n\\n    Calculates V-trace actor critic targets for softmax polices as described in\\n    \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner\\n    Architectures\" by Espeholt, Soyer, Munos et al. (https://arxiv.org/abs/1802.01561)\\n\\n    The V-trace implementation used here closely resembles the one found in the\\n    scalable-agent repository by Google DeepMind, available at\\n    https://github.com/deepmind/scalable_agent. This version has been optimized to\\n    minimize the number of floating-point operations required per V-Trace\\n    calculation, achieved through the use of dynamic programming techniques. It\\'s\\n    important to note that the mathematical expressions used in this implementation\\n    may appear quite different from those presented in the IMPALA paper.\\n\\n    The following terminology applies:\\n        - `target policy` refers to the policy we are interested in improving.\\n        - `behaviour policy` refers to the policy that generated the given\\n            rewards and actions.\\n        - `T` refers to the time dimension. This is usually either the length of the\\n            trajectory or the length of the sequence if recurrent.\\n        - `B` refers to the batch size.\\n\\n    Args:\\n        target_action_log_probs: Action log probs from the target policy. A float32\\n            tensor of shape [T, B].\\n        behaviour_action_log_probs: Action log probs from the behaviour policy. A\\n            float32 tensor of shape [T, B].\\n        discounts: A float32 tensor of shape [T, B] with the discount encountered when\\n            following the behaviour policy. This will be 0 for terminal timesteps\\n            (done=True) and gamma (the discount factor) otherwise.\\n        rewards: A float32 tensor of shape [T, B] with the rewards generated by\\n            following the behaviour policy.\\n        values: A float32 tensor of shape [T, B] with the value function estimates\\n            wrt. the target policy.\\n        bootstrap_value: A float32 of shape [B] with the value function estimate at\\n            time T.\\n        clip_rho_threshold: A scalar float32 tensor with the clipping threshold for\\n            importance weights (rho) when calculating the baseline targets (vs).\\n            rho^bar in the paper.\\n        clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold\\n            on rho_s in \\rho_s \\\\delta log \\\\pi(a|x) (r + \\\\gamma v_{s+1} - V(x_s)).\\n    '\n    log_rhos = target_action_log_probs - behaviour_action_log_probs\n    rhos = torch.exp(log_rhos)\n    if clip_rho_threshold is not None:\n        clipped_rhos = torch.clamp(rhos, max=clip_rho_threshold)\n    else:\n        clipped_rhos = rhos\n    cs = torch.clamp(rhos, max=1.0)\n    values_t_plus_1 = torch.cat([values[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)\n    discounts_cpu = discounts.to('cpu')\n    cs_cpu = cs.to('cpu')\n    deltas_cpu = deltas.to('cpu')\n    vs_minus_v_xs_cpu = [torch.zeros_like(bootstrap_value, device='cpu')]\n    for i in reversed(range(len(discounts_cpu))):\n        (discount_t, c_t, delta_t) = (discounts_cpu[i], cs_cpu[i], deltas_cpu[i])\n        vs_minus_v_xs_cpu.append(delta_t + discount_t * c_t * vs_minus_v_xs_cpu[-1])\n    vs_minus_v_xs_cpu = torch.stack(vs_minus_v_xs_cpu[1:])\n    vs_minus_v_xs = vs_minus_v_xs_cpu.to(deltas.device)\n    vs_minus_v_xs = torch.flip(vs_minus_v_xs, dims=[0])\n    vs = torch.add(vs_minus_v_xs, values)\n    vs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], axis=0)\n    if clip_pg_rho_threshold is not None:\n        clipped_pg_rhos = torch.clamp(rhos, max=clip_pg_rho_threshold)\n    else:\n        clipped_pg_rhos = rhos\n    pg_advantages = clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values)\n    return (torch.detach(vs), torch.detach(pg_advantages))"
        ]
    }
]