[
    {
        "func_name": "__init__",
        "original": "def __init__(self, cfg_file: str, *args, **kwargs):\n    \"\"\" A trainer is used for Sequence Classification\n\n        Based on Config file (*.yaml or *.json), the trainer trains or evaluates on a dataset\n\n        Args:\n            cfg_file (str): the path of config file\n        Raises:\n            ValueError: _description_\n        \"\"\"\n    super().__init__(cfg_file)",
        "mutated": [
            "def __init__(self, cfg_file: str, *args, **kwargs):\n    if False:\n        i = 10\n    ' A trainer is used for Sequence Classification\\n\\n        Based on Config file (*.yaml or *.json), the trainer trains or evaluates on a dataset\\n\\n        Args:\\n            cfg_file (str): the path of config file\\n        Raises:\\n            ValueError: _description_\\n        '\n    super().__init__(cfg_file)",
            "def __init__(self, cfg_file: str, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' A trainer is used for Sequence Classification\\n\\n        Based on Config file (*.yaml or *.json), the trainer trains or evaluates on a dataset\\n\\n        Args:\\n            cfg_file (str): the path of config file\\n        Raises:\\n            ValueError: _description_\\n        '\n    super().__init__(cfg_file)",
            "def __init__(self, cfg_file: str, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' A trainer is used for Sequence Classification\\n\\n        Based on Config file (*.yaml or *.json), the trainer trains or evaluates on a dataset\\n\\n        Args:\\n            cfg_file (str): the path of config file\\n        Raises:\\n            ValueError: _description_\\n        '\n    super().__init__(cfg_file)",
            "def __init__(self, cfg_file: str, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' A trainer is used for Sequence Classification\\n\\n        Based on Config file (*.yaml or *.json), the trainer trains or evaluates on a dataset\\n\\n        Args:\\n            cfg_file (str): the path of config file\\n        Raises:\\n            ValueError: _description_\\n        '\n    super().__init__(cfg_file)",
            "def __init__(self, cfg_file: str, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' A trainer is used for Sequence Classification\\n\\n        Based on Config file (*.yaml or *.json), the trainer trains or evaluates on a dataset\\n\\n        Args:\\n            cfg_file (str): the path of config file\\n        Raises:\\n            ValueError: _description_\\n        '\n    super().__init__(cfg_file)"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(self, *args, **kwargs):\n    logger.info('Train')\n    ...",
        "mutated": [
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n    logger.info('Train')\n    ...",
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    logger.info('Train')\n    ...",
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    logger.info('Train')\n    ...",
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    logger.info('Train')\n    ...",
            "def train(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    logger.info('Train')\n    ..."
        ]
    },
    {
        "func_name": "__attr_is_exist",
        "original": "def __attr_is_exist(self, attr: str) -> Tuple[Union[str, bool]]:\n    \"\"\"get attribute from config, if the attribute does exist, return false\n\n        Example:\n\n        >>> self.__attr_is_exist(\"model path\")\n        >>> out: (model-path, \"/workspace/bert-base-sst2\")\n        >>> self.__attr_is_exist(\"model weights\")\n        >>> out: (model-weights, False)\n\n        Args:\n            attr (str): attribute str, \"model path\" -> config[\"model\"][path]\n\n        Returns:\n            Tuple[Union[str, bool]]:[target attribute name, the target attribute or False]\n        \"\"\"\n    paths = attr.split(' ')\n    attr_str: str = '-'.join(paths)\n    target = self.cfg[paths[0]] if hasattr(self.cfg, paths[0]) else None\n    for path_ in paths[1:]:\n        if not hasattr(target, path_):\n            return (attr_str, False)\n        target = target[path_]\n    if target and target != '':\n        return (attr_str, target)\n    return (attr_str, False)",
        "mutated": [
            "def __attr_is_exist(self, attr: str) -> Tuple[Union[str, bool]]:\n    if False:\n        i = 10\n    'get attribute from config, if the attribute does exist, return false\\n\\n        Example:\\n\\n        >>> self.__attr_is_exist(\"model path\")\\n        >>> out: (model-path, \"/workspace/bert-base-sst2\")\\n        >>> self.__attr_is_exist(\"model weights\")\\n        >>> out: (model-weights, False)\\n\\n        Args:\\n            attr (str): attribute str, \"model path\" -> config[\"model\"][path]\\n\\n        Returns:\\n            Tuple[Union[str, bool]]:[target attribute name, the target attribute or False]\\n        '\n    paths = attr.split(' ')\n    attr_str: str = '-'.join(paths)\n    target = self.cfg[paths[0]] if hasattr(self.cfg, paths[0]) else None\n    for path_ in paths[1:]:\n        if not hasattr(target, path_):\n            return (attr_str, False)\n        target = target[path_]\n    if target and target != '':\n        return (attr_str, target)\n    return (attr_str, False)",
            "def __attr_is_exist(self, attr: str) -> Tuple[Union[str, bool]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'get attribute from config, if the attribute does exist, return false\\n\\n        Example:\\n\\n        >>> self.__attr_is_exist(\"model path\")\\n        >>> out: (model-path, \"/workspace/bert-base-sst2\")\\n        >>> self.__attr_is_exist(\"model weights\")\\n        >>> out: (model-weights, False)\\n\\n        Args:\\n            attr (str): attribute str, \"model path\" -> config[\"model\"][path]\\n\\n        Returns:\\n            Tuple[Union[str, bool]]:[target attribute name, the target attribute or False]\\n        '\n    paths = attr.split(' ')\n    attr_str: str = '-'.join(paths)\n    target = self.cfg[paths[0]] if hasattr(self.cfg, paths[0]) else None\n    for path_ in paths[1:]:\n        if not hasattr(target, path_):\n            return (attr_str, False)\n        target = target[path_]\n    if target and target != '':\n        return (attr_str, target)\n    return (attr_str, False)",
            "def __attr_is_exist(self, attr: str) -> Tuple[Union[str, bool]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'get attribute from config, if the attribute does exist, return false\\n\\n        Example:\\n\\n        >>> self.__attr_is_exist(\"model path\")\\n        >>> out: (model-path, \"/workspace/bert-base-sst2\")\\n        >>> self.__attr_is_exist(\"model weights\")\\n        >>> out: (model-weights, False)\\n\\n        Args:\\n            attr (str): attribute str, \"model path\" -> config[\"model\"][path]\\n\\n        Returns:\\n            Tuple[Union[str, bool]]:[target attribute name, the target attribute or False]\\n        '\n    paths = attr.split(' ')\n    attr_str: str = '-'.join(paths)\n    target = self.cfg[paths[0]] if hasattr(self.cfg, paths[0]) else None\n    for path_ in paths[1:]:\n        if not hasattr(target, path_):\n            return (attr_str, False)\n        target = target[path_]\n    if target and target != '':\n        return (attr_str, target)\n    return (attr_str, False)",
            "def __attr_is_exist(self, attr: str) -> Tuple[Union[str, bool]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'get attribute from config, if the attribute does exist, return false\\n\\n        Example:\\n\\n        >>> self.__attr_is_exist(\"model path\")\\n        >>> out: (model-path, \"/workspace/bert-base-sst2\")\\n        >>> self.__attr_is_exist(\"model weights\")\\n        >>> out: (model-weights, False)\\n\\n        Args:\\n            attr (str): attribute str, \"model path\" -> config[\"model\"][path]\\n\\n        Returns:\\n            Tuple[Union[str, bool]]:[target attribute name, the target attribute or False]\\n        '\n    paths = attr.split(' ')\n    attr_str: str = '-'.join(paths)\n    target = self.cfg[paths[0]] if hasattr(self.cfg, paths[0]) else None\n    for path_ in paths[1:]:\n        if not hasattr(target, path_):\n            return (attr_str, False)\n        target = target[path_]\n    if target and target != '':\n        return (attr_str, target)\n    return (attr_str, False)",
            "def __attr_is_exist(self, attr: str) -> Tuple[Union[str, bool]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'get attribute from config, if the attribute does exist, return false\\n\\n        Example:\\n\\n        >>> self.__attr_is_exist(\"model path\")\\n        >>> out: (model-path, \"/workspace/bert-base-sst2\")\\n        >>> self.__attr_is_exist(\"model weights\")\\n        >>> out: (model-weights, False)\\n\\n        Args:\\n            attr (str): attribute str, \"model path\" -> config[\"model\"][path]\\n\\n        Returns:\\n            Tuple[Union[str, bool]]:[target attribute name, the target attribute or False]\\n        '\n    paths = attr.split(' ')\n    attr_str: str = '-'.join(paths)\n    target = self.cfg[paths[0]] if hasattr(self.cfg, paths[0]) else None\n    for path_ in paths[1:]:\n        if not hasattr(target, path_):\n            return (attr_str, False)\n        target = target[path_]\n    if target and target != '':\n        return (attr_str, target)\n    return (attr_str, False)"
        ]
    },
    {
        "func_name": "evaluate",
        "original": "def evaluate(self, checkpoint_path: Optional[str]=None, *args, **kwargs) -> Dict[str, float]:\n    \"\"\"evaluate a dataset\n\n        evaluate a dataset via a specific model from the `checkpoint_path` path, if the `checkpoint_path`\n        does not exist, read from the config file.\n\n        Args:\n            checkpoint_path (Optional[str], optional): the model path. Defaults to None.\n\n        Returns:\n            Dict[str, float]: the results about the evaluation\n            Example:\n            {\"accuracy\": 0.5091743119266054, \"f1\": 0.673780487804878}\n        \"\"\"\n    import torch\n    from easynlp.appzoo import load_dataset\n    from easynlp.appzoo.dataset import GeneralDataset\n    from easynlp.appzoo.sequence_classification.model import SequenceClassification\n    from easynlp.utils import losses\n    from sklearn.metrics import f1_score\n    from torch.utils.data import DataLoader\n    raise_str = 'Attribute {} is not given in config file!'\n    metrics = self.__attr_is_exist('evaluation metrics')\n    eval_batch_size = self.__attr_is_exist('evaluation batch_size')\n    test_dataset_path = self.__attr_is_exist('dataset valid file')\n    attrs = [metrics, eval_batch_size, test_dataset_path]\n    for attr_ in attrs:\n        if not attr_[-1]:\n            raise AttributeError(raise_str.format(attr_[0]))\n    if not checkpoint_path:\n        checkpoint_path = self.__attr_is_exist('evaluation model_path')[-1]\n        if not checkpoint_path:\n            raise ValueError('Argument checkout_path must be passed if the evaluation-model_path is not given in config file!')\n    max_sequence_length = kwargs.get('max_sequence_length', self.__attr_is_exist('evaluation max_sequence_length')[-1])\n    if not max_sequence_length:\n        raise ValueError('Argument max_sequence_length must be passed if the evaluation-max_sequence_length does not exist in config file!')\n    raw_dataset = load_dataset(*test_dataset_path[-1].split('/'))\n    valid_dataset = raw_dataset['validation']\n    pre_dataset = GeneralDataset(valid_dataset, checkpoint_path, max_sequence_length)\n    valid_dataloader = DataLoader(pre_dataset, batch_size=eval_batch_size[-1], shuffle=False, collate_fn=pre_dataset.batch_fn)\n    model = SequenceClassification.from_pretrained(checkpoint_path)\n    model.eval()\n    total_loss = 0\n    total_steps = 0\n    total_samples = 0\n    hit_num = 0\n    total_num = 0\n    logits_list = list()\n    y_trues = list()\n    total_spent_time = 0.0\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    for (_step, batch) in enumerate(valid_dataloader):\n        try:\n            batch = {key: val.to(device) if isinstance(val, torch.Tensor) else val for (key, val) in batch.items()}\n        except RuntimeError:\n            batch = {key: val for (key, val) in batch.items()}\n        infer_start_time = time.time()\n        with torch.no_grad():\n            label_ids = batch.pop('label_ids')\n            outputs = model(batch)\n        infer_end_time = time.time()\n        total_spent_time += infer_end_time - infer_start_time\n        assert 'logits' in outputs\n        logits = outputs['logits']\n        y_trues.extend(label_ids.tolist())\n        logits_list.extend(logits.tolist())\n        hit_num += torch.sum(torch.argmax(logits, dim=-1) == label_ids).item()\n        total_num += label_ids.shape[0]\n        if len(logits.shape) == 1 or logits.shape[-1] == 1:\n            tmp_loss = losses.mse_loss(logits, label_ids)\n        elif len(logits.shape) == 2:\n            tmp_loss = losses.cross_entropy(logits, label_ids)\n        else:\n            raise RuntimeError\n        total_loss += tmp_loss.mean().item()\n        total_steps += 1\n        total_samples += valid_dataloader.batch_size\n        if (_step + 1) % 100 == 0:\n            total_step = len(valid_dataloader.dataset) // valid_dataloader.batch_size\n            logger.info('Eval: {}/{} steps finished'.format(_step + 1, total_step))\n    logger.info('Inference time = {:.2f}s, [{:.4f} ms / sample] '.format(total_spent_time, total_spent_time * 1000 / total_samples))\n    eval_loss = total_loss / total_steps\n    logger.info('Eval loss: {}'.format(eval_loss))\n    logits_list = np.array(logits_list)\n    eval_outputs = list()\n    for metric in metrics[-1]:\n        if metric.endswith('accuracy'):\n            acc = hit_num / total_num\n            logger.info('Accuracy: {}'.format(acc))\n            eval_outputs.append(('accuracy', acc))\n        elif metric == 'f1':\n            if model.config.num_labels == 2:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1))\n                logger.info('F1: {}'.format(f1))\n                eval_outputs.append(('f1', f1))\n            else:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='macro')\n                logger.info('Macro F1: {}'.format(f1))\n                eval_outputs.append(('macro-f1', f1))\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='micro')\n                logger.info('Micro F1: {}'.format(f1))\n                eval_outputs.append(('micro-f1', f1))\n        else:\n            raise NotImplementedError('Metric %s not implemented' % metric)\n    return dict(eval_outputs)",
        "mutated": [
            "def evaluate(self, checkpoint_path: Optional[str]=None, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n    'evaluate a dataset\\n\\n        evaluate a dataset via a specific model from the `checkpoint_path` path, if the `checkpoint_path`\\n        does not exist, read from the config file.\\n\\n        Args:\\n            checkpoint_path (Optional[str], optional): the model path. Defaults to None.\\n\\n        Returns:\\n            Dict[str, float]: the results about the evaluation\\n            Example:\\n            {\"accuracy\": 0.5091743119266054, \"f1\": 0.673780487804878}\\n        '\n    import torch\n    from easynlp.appzoo import load_dataset\n    from easynlp.appzoo.dataset import GeneralDataset\n    from easynlp.appzoo.sequence_classification.model import SequenceClassification\n    from easynlp.utils import losses\n    from sklearn.metrics import f1_score\n    from torch.utils.data import DataLoader\n    raise_str = 'Attribute {} is not given in config file!'\n    metrics = self.__attr_is_exist('evaluation metrics')\n    eval_batch_size = self.__attr_is_exist('evaluation batch_size')\n    test_dataset_path = self.__attr_is_exist('dataset valid file')\n    attrs = [metrics, eval_batch_size, test_dataset_path]\n    for attr_ in attrs:\n        if not attr_[-1]:\n            raise AttributeError(raise_str.format(attr_[0]))\n    if not checkpoint_path:\n        checkpoint_path = self.__attr_is_exist('evaluation model_path')[-1]\n        if not checkpoint_path:\n            raise ValueError('Argument checkout_path must be passed if the evaluation-model_path is not given in config file!')\n    max_sequence_length = kwargs.get('max_sequence_length', self.__attr_is_exist('evaluation max_sequence_length')[-1])\n    if not max_sequence_length:\n        raise ValueError('Argument max_sequence_length must be passed if the evaluation-max_sequence_length does not exist in config file!')\n    raw_dataset = load_dataset(*test_dataset_path[-1].split('/'))\n    valid_dataset = raw_dataset['validation']\n    pre_dataset = GeneralDataset(valid_dataset, checkpoint_path, max_sequence_length)\n    valid_dataloader = DataLoader(pre_dataset, batch_size=eval_batch_size[-1], shuffle=False, collate_fn=pre_dataset.batch_fn)\n    model = SequenceClassification.from_pretrained(checkpoint_path)\n    model.eval()\n    total_loss = 0\n    total_steps = 0\n    total_samples = 0\n    hit_num = 0\n    total_num = 0\n    logits_list = list()\n    y_trues = list()\n    total_spent_time = 0.0\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    for (_step, batch) in enumerate(valid_dataloader):\n        try:\n            batch = {key: val.to(device) if isinstance(val, torch.Tensor) else val for (key, val) in batch.items()}\n        except RuntimeError:\n            batch = {key: val for (key, val) in batch.items()}\n        infer_start_time = time.time()\n        with torch.no_grad():\n            label_ids = batch.pop('label_ids')\n            outputs = model(batch)\n        infer_end_time = time.time()\n        total_spent_time += infer_end_time - infer_start_time\n        assert 'logits' in outputs\n        logits = outputs['logits']\n        y_trues.extend(label_ids.tolist())\n        logits_list.extend(logits.tolist())\n        hit_num += torch.sum(torch.argmax(logits, dim=-1) == label_ids).item()\n        total_num += label_ids.shape[0]\n        if len(logits.shape) == 1 or logits.shape[-1] == 1:\n            tmp_loss = losses.mse_loss(logits, label_ids)\n        elif len(logits.shape) == 2:\n            tmp_loss = losses.cross_entropy(logits, label_ids)\n        else:\n            raise RuntimeError\n        total_loss += tmp_loss.mean().item()\n        total_steps += 1\n        total_samples += valid_dataloader.batch_size\n        if (_step + 1) % 100 == 0:\n            total_step = len(valid_dataloader.dataset) // valid_dataloader.batch_size\n            logger.info('Eval: {}/{} steps finished'.format(_step + 1, total_step))\n    logger.info('Inference time = {:.2f}s, [{:.4f} ms / sample] '.format(total_spent_time, total_spent_time * 1000 / total_samples))\n    eval_loss = total_loss / total_steps\n    logger.info('Eval loss: {}'.format(eval_loss))\n    logits_list = np.array(logits_list)\n    eval_outputs = list()\n    for metric in metrics[-1]:\n        if metric.endswith('accuracy'):\n            acc = hit_num / total_num\n            logger.info('Accuracy: {}'.format(acc))\n            eval_outputs.append(('accuracy', acc))\n        elif metric == 'f1':\n            if model.config.num_labels == 2:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1))\n                logger.info('F1: {}'.format(f1))\n                eval_outputs.append(('f1', f1))\n            else:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='macro')\n                logger.info('Macro F1: {}'.format(f1))\n                eval_outputs.append(('macro-f1', f1))\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='micro')\n                logger.info('Micro F1: {}'.format(f1))\n                eval_outputs.append(('micro-f1', f1))\n        else:\n            raise NotImplementedError('Metric %s not implemented' % metric)\n    return dict(eval_outputs)",
            "def evaluate(self, checkpoint_path: Optional[str]=None, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'evaluate a dataset\\n\\n        evaluate a dataset via a specific model from the `checkpoint_path` path, if the `checkpoint_path`\\n        does not exist, read from the config file.\\n\\n        Args:\\n            checkpoint_path (Optional[str], optional): the model path. Defaults to None.\\n\\n        Returns:\\n            Dict[str, float]: the results about the evaluation\\n            Example:\\n            {\"accuracy\": 0.5091743119266054, \"f1\": 0.673780487804878}\\n        '\n    import torch\n    from easynlp.appzoo import load_dataset\n    from easynlp.appzoo.dataset import GeneralDataset\n    from easynlp.appzoo.sequence_classification.model import SequenceClassification\n    from easynlp.utils import losses\n    from sklearn.metrics import f1_score\n    from torch.utils.data import DataLoader\n    raise_str = 'Attribute {} is not given in config file!'\n    metrics = self.__attr_is_exist('evaluation metrics')\n    eval_batch_size = self.__attr_is_exist('evaluation batch_size')\n    test_dataset_path = self.__attr_is_exist('dataset valid file')\n    attrs = [metrics, eval_batch_size, test_dataset_path]\n    for attr_ in attrs:\n        if not attr_[-1]:\n            raise AttributeError(raise_str.format(attr_[0]))\n    if not checkpoint_path:\n        checkpoint_path = self.__attr_is_exist('evaluation model_path')[-1]\n        if not checkpoint_path:\n            raise ValueError('Argument checkout_path must be passed if the evaluation-model_path is not given in config file!')\n    max_sequence_length = kwargs.get('max_sequence_length', self.__attr_is_exist('evaluation max_sequence_length')[-1])\n    if not max_sequence_length:\n        raise ValueError('Argument max_sequence_length must be passed if the evaluation-max_sequence_length does not exist in config file!')\n    raw_dataset = load_dataset(*test_dataset_path[-1].split('/'))\n    valid_dataset = raw_dataset['validation']\n    pre_dataset = GeneralDataset(valid_dataset, checkpoint_path, max_sequence_length)\n    valid_dataloader = DataLoader(pre_dataset, batch_size=eval_batch_size[-1], shuffle=False, collate_fn=pre_dataset.batch_fn)\n    model = SequenceClassification.from_pretrained(checkpoint_path)\n    model.eval()\n    total_loss = 0\n    total_steps = 0\n    total_samples = 0\n    hit_num = 0\n    total_num = 0\n    logits_list = list()\n    y_trues = list()\n    total_spent_time = 0.0\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    for (_step, batch) in enumerate(valid_dataloader):\n        try:\n            batch = {key: val.to(device) if isinstance(val, torch.Tensor) else val for (key, val) in batch.items()}\n        except RuntimeError:\n            batch = {key: val for (key, val) in batch.items()}\n        infer_start_time = time.time()\n        with torch.no_grad():\n            label_ids = batch.pop('label_ids')\n            outputs = model(batch)\n        infer_end_time = time.time()\n        total_spent_time += infer_end_time - infer_start_time\n        assert 'logits' in outputs\n        logits = outputs['logits']\n        y_trues.extend(label_ids.tolist())\n        logits_list.extend(logits.tolist())\n        hit_num += torch.sum(torch.argmax(logits, dim=-1) == label_ids).item()\n        total_num += label_ids.shape[0]\n        if len(logits.shape) == 1 or logits.shape[-1] == 1:\n            tmp_loss = losses.mse_loss(logits, label_ids)\n        elif len(logits.shape) == 2:\n            tmp_loss = losses.cross_entropy(logits, label_ids)\n        else:\n            raise RuntimeError\n        total_loss += tmp_loss.mean().item()\n        total_steps += 1\n        total_samples += valid_dataloader.batch_size\n        if (_step + 1) % 100 == 0:\n            total_step = len(valid_dataloader.dataset) // valid_dataloader.batch_size\n            logger.info('Eval: {}/{} steps finished'.format(_step + 1, total_step))\n    logger.info('Inference time = {:.2f}s, [{:.4f} ms / sample] '.format(total_spent_time, total_spent_time * 1000 / total_samples))\n    eval_loss = total_loss / total_steps\n    logger.info('Eval loss: {}'.format(eval_loss))\n    logits_list = np.array(logits_list)\n    eval_outputs = list()\n    for metric in metrics[-1]:\n        if metric.endswith('accuracy'):\n            acc = hit_num / total_num\n            logger.info('Accuracy: {}'.format(acc))\n            eval_outputs.append(('accuracy', acc))\n        elif metric == 'f1':\n            if model.config.num_labels == 2:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1))\n                logger.info('F1: {}'.format(f1))\n                eval_outputs.append(('f1', f1))\n            else:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='macro')\n                logger.info('Macro F1: {}'.format(f1))\n                eval_outputs.append(('macro-f1', f1))\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='micro')\n                logger.info('Micro F1: {}'.format(f1))\n                eval_outputs.append(('micro-f1', f1))\n        else:\n            raise NotImplementedError('Metric %s not implemented' % metric)\n    return dict(eval_outputs)",
            "def evaluate(self, checkpoint_path: Optional[str]=None, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'evaluate a dataset\\n\\n        evaluate a dataset via a specific model from the `checkpoint_path` path, if the `checkpoint_path`\\n        does not exist, read from the config file.\\n\\n        Args:\\n            checkpoint_path (Optional[str], optional): the model path. Defaults to None.\\n\\n        Returns:\\n            Dict[str, float]: the results about the evaluation\\n            Example:\\n            {\"accuracy\": 0.5091743119266054, \"f1\": 0.673780487804878}\\n        '\n    import torch\n    from easynlp.appzoo import load_dataset\n    from easynlp.appzoo.dataset import GeneralDataset\n    from easynlp.appzoo.sequence_classification.model import SequenceClassification\n    from easynlp.utils import losses\n    from sklearn.metrics import f1_score\n    from torch.utils.data import DataLoader\n    raise_str = 'Attribute {} is not given in config file!'\n    metrics = self.__attr_is_exist('evaluation metrics')\n    eval_batch_size = self.__attr_is_exist('evaluation batch_size')\n    test_dataset_path = self.__attr_is_exist('dataset valid file')\n    attrs = [metrics, eval_batch_size, test_dataset_path]\n    for attr_ in attrs:\n        if not attr_[-1]:\n            raise AttributeError(raise_str.format(attr_[0]))\n    if not checkpoint_path:\n        checkpoint_path = self.__attr_is_exist('evaluation model_path')[-1]\n        if not checkpoint_path:\n            raise ValueError('Argument checkout_path must be passed if the evaluation-model_path is not given in config file!')\n    max_sequence_length = kwargs.get('max_sequence_length', self.__attr_is_exist('evaluation max_sequence_length')[-1])\n    if not max_sequence_length:\n        raise ValueError('Argument max_sequence_length must be passed if the evaluation-max_sequence_length does not exist in config file!')\n    raw_dataset = load_dataset(*test_dataset_path[-1].split('/'))\n    valid_dataset = raw_dataset['validation']\n    pre_dataset = GeneralDataset(valid_dataset, checkpoint_path, max_sequence_length)\n    valid_dataloader = DataLoader(pre_dataset, batch_size=eval_batch_size[-1], shuffle=False, collate_fn=pre_dataset.batch_fn)\n    model = SequenceClassification.from_pretrained(checkpoint_path)\n    model.eval()\n    total_loss = 0\n    total_steps = 0\n    total_samples = 0\n    hit_num = 0\n    total_num = 0\n    logits_list = list()\n    y_trues = list()\n    total_spent_time = 0.0\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    for (_step, batch) in enumerate(valid_dataloader):\n        try:\n            batch = {key: val.to(device) if isinstance(val, torch.Tensor) else val for (key, val) in batch.items()}\n        except RuntimeError:\n            batch = {key: val for (key, val) in batch.items()}\n        infer_start_time = time.time()\n        with torch.no_grad():\n            label_ids = batch.pop('label_ids')\n            outputs = model(batch)\n        infer_end_time = time.time()\n        total_spent_time += infer_end_time - infer_start_time\n        assert 'logits' in outputs\n        logits = outputs['logits']\n        y_trues.extend(label_ids.tolist())\n        logits_list.extend(logits.tolist())\n        hit_num += torch.sum(torch.argmax(logits, dim=-1) == label_ids).item()\n        total_num += label_ids.shape[0]\n        if len(logits.shape) == 1 or logits.shape[-1] == 1:\n            tmp_loss = losses.mse_loss(logits, label_ids)\n        elif len(logits.shape) == 2:\n            tmp_loss = losses.cross_entropy(logits, label_ids)\n        else:\n            raise RuntimeError\n        total_loss += tmp_loss.mean().item()\n        total_steps += 1\n        total_samples += valid_dataloader.batch_size\n        if (_step + 1) % 100 == 0:\n            total_step = len(valid_dataloader.dataset) // valid_dataloader.batch_size\n            logger.info('Eval: {}/{} steps finished'.format(_step + 1, total_step))\n    logger.info('Inference time = {:.2f}s, [{:.4f} ms / sample] '.format(total_spent_time, total_spent_time * 1000 / total_samples))\n    eval_loss = total_loss / total_steps\n    logger.info('Eval loss: {}'.format(eval_loss))\n    logits_list = np.array(logits_list)\n    eval_outputs = list()\n    for metric in metrics[-1]:\n        if metric.endswith('accuracy'):\n            acc = hit_num / total_num\n            logger.info('Accuracy: {}'.format(acc))\n            eval_outputs.append(('accuracy', acc))\n        elif metric == 'f1':\n            if model.config.num_labels == 2:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1))\n                logger.info('F1: {}'.format(f1))\n                eval_outputs.append(('f1', f1))\n            else:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='macro')\n                logger.info('Macro F1: {}'.format(f1))\n                eval_outputs.append(('macro-f1', f1))\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='micro')\n                logger.info('Micro F1: {}'.format(f1))\n                eval_outputs.append(('micro-f1', f1))\n        else:\n            raise NotImplementedError('Metric %s not implemented' % metric)\n    return dict(eval_outputs)",
            "def evaluate(self, checkpoint_path: Optional[str]=None, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'evaluate a dataset\\n\\n        evaluate a dataset via a specific model from the `checkpoint_path` path, if the `checkpoint_path`\\n        does not exist, read from the config file.\\n\\n        Args:\\n            checkpoint_path (Optional[str], optional): the model path. Defaults to None.\\n\\n        Returns:\\n            Dict[str, float]: the results about the evaluation\\n            Example:\\n            {\"accuracy\": 0.5091743119266054, \"f1\": 0.673780487804878}\\n        '\n    import torch\n    from easynlp.appzoo import load_dataset\n    from easynlp.appzoo.dataset import GeneralDataset\n    from easynlp.appzoo.sequence_classification.model import SequenceClassification\n    from easynlp.utils import losses\n    from sklearn.metrics import f1_score\n    from torch.utils.data import DataLoader\n    raise_str = 'Attribute {} is not given in config file!'\n    metrics = self.__attr_is_exist('evaluation metrics')\n    eval_batch_size = self.__attr_is_exist('evaluation batch_size')\n    test_dataset_path = self.__attr_is_exist('dataset valid file')\n    attrs = [metrics, eval_batch_size, test_dataset_path]\n    for attr_ in attrs:\n        if not attr_[-1]:\n            raise AttributeError(raise_str.format(attr_[0]))\n    if not checkpoint_path:\n        checkpoint_path = self.__attr_is_exist('evaluation model_path')[-1]\n        if not checkpoint_path:\n            raise ValueError('Argument checkout_path must be passed if the evaluation-model_path is not given in config file!')\n    max_sequence_length = kwargs.get('max_sequence_length', self.__attr_is_exist('evaluation max_sequence_length')[-1])\n    if not max_sequence_length:\n        raise ValueError('Argument max_sequence_length must be passed if the evaluation-max_sequence_length does not exist in config file!')\n    raw_dataset = load_dataset(*test_dataset_path[-1].split('/'))\n    valid_dataset = raw_dataset['validation']\n    pre_dataset = GeneralDataset(valid_dataset, checkpoint_path, max_sequence_length)\n    valid_dataloader = DataLoader(pre_dataset, batch_size=eval_batch_size[-1], shuffle=False, collate_fn=pre_dataset.batch_fn)\n    model = SequenceClassification.from_pretrained(checkpoint_path)\n    model.eval()\n    total_loss = 0\n    total_steps = 0\n    total_samples = 0\n    hit_num = 0\n    total_num = 0\n    logits_list = list()\n    y_trues = list()\n    total_spent_time = 0.0\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    for (_step, batch) in enumerate(valid_dataloader):\n        try:\n            batch = {key: val.to(device) if isinstance(val, torch.Tensor) else val for (key, val) in batch.items()}\n        except RuntimeError:\n            batch = {key: val for (key, val) in batch.items()}\n        infer_start_time = time.time()\n        with torch.no_grad():\n            label_ids = batch.pop('label_ids')\n            outputs = model(batch)\n        infer_end_time = time.time()\n        total_spent_time += infer_end_time - infer_start_time\n        assert 'logits' in outputs\n        logits = outputs['logits']\n        y_trues.extend(label_ids.tolist())\n        logits_list.extend(logits.tolist())\n        hit_num += torch.sum(torch.argmax(logits, dim=-1) == label_ids).item()\n        total_num += label_ids.shape[0]\n        if len(logits.shape) == 1 or logits.shape[-1] == 1:\n            tmp_loss = losses.mse_loss(logits, label_ids)\n        elif len(logits.shape) == 2:\n            tmp_loss = losses.cross_entropy(logits, label_ids)\n        else:\n            raise RuntimeError\n        total_loss += tmp_loss.mean().item()\n        total_steps += 1\n        total_samples += valid_dataloader.batch_size\n        if (_step + 1) % 100 == 0:\n            total_step = len(valid_dataloader.dataset) // valid_dataloader.batch_size\n            logger.info('Eval: {}/{} steps finished'.format(_step + 1, total_step))\n    logger.info('Inference time = {:.2f}s, [{:.4f} ms / sample] '.format(total_spent_time, total_spent_time * 1000 / total_samples))\n    eval_loss = total_loss / total_steps\n    logger.info('Eval loss: {}'.format(eval_loss))\n    logits_list = np.array(logits_list)\n    eval_outputs = list()\n    for metric in metrics[-1]:\n        if metric.endswith('accuracy'):\n            acc = hit_num / total_num\n            logger.info('Accuracy: {}'.format(acc))\n            eval_outputs.append(('accuracy', acc))\n        elif metric == 'f1':\n            if model.config.num_labels == 2:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1))\n                logger.info('F1: {}'.format(f1))\n                eval_outputs.append(('f1', f1))\n            else:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='macro')\n                logger.info('Macro F1: {}'.format(f1))\n                eval_outputs.append(('macro-f1', f1))\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='micro')\n                logger.info('Micro F1: {}'.format(f1))\n                eval_outputs.append(('micro-f1', f1))\n        else:\n            raise NotImplementedError('Metric %s not implemented' % metric)\n    return dict(eval_outputs)",
            "def evaluate(self, checkpoint_path: Optional[str]=None, *args, **kwargs) -> Dict[str, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'evaluate a dataset\\n\\n        evaluate a dataset via a specific model from the `checkpoint_path` path, if the `checkpoint_path`\\n        does not exist, read from the config file.\\n\\n        Args:\\n            checkpoint_path (Optional[str], optional): the model path. Defaults to None.\\n\\n        Returns:\\n            Dict[str, float]: the results about the evaluation\\n            Example:\\n            {\"accuracy\": 0.5091743119266054, \"f1\": 0.673780487804878}\\n        '\n    import torch\n    from easynlp.appzoo import load_dataset\n    from easynlp.appzoo.dataset import GeneralDataset\n    from easynlp.appzoo.sequence_classification.model import SequenceClassification\n    from easynlp.utils import losses\n    from sklearn.metrics import f1_score\n    from torch.utils.data import DataLoader\n    raise_str = 'Attribute {} is not given in config file!'\n    metrics = self.__attr_is_exist('evaluation metrics')\n    eval_batch_size = self.__attr_is_exist('evaluation batch_size')\n    test_dataset_path = self.__attr_is_exist('dataset valid file')\n    attrs = [metrics, eval_batch_size, test_dataset_path]\n    for attr_ in attrs:\n        if not attr_[-1]:\n            raise AttributeError(raise_str.format(attr_[0]))\n    if not checkpoint_path:\n        checkpoint_path = self.__attr_is_exist('evaluation model_path')[-1]\n        if not checkpoint_path:\n            raise ValueError('Argument checkout_path must be passed if the evaluation-model_path is not given in config file!')\n    max_sequence_length = kwargs.get('max_sequence_length', self.__attr_is_exist('evaluation max_sequence_length')[-1])\n    if not max_sequence_length:\n        raise ValueError('Argument max_sequence_length must be passed if the evaluation-max_sequence_length does not exist in config file!')\n    raw_dataset = load_dataset(*test_dataset_path[-1].split('/'))\n    valid_dataset = raw_dataset['validation']\n    pre_dataset = GeneralDataset(valid_dataset, checkpoint_path, max_sequence_length)\n    valid_dataloader = DataLoader(pre_dataset, batch_size=eval_batch_size[-1], shuffle=False, collate_fn=pre_dataset.batch_fn)\n    model = SequenceClassification.from_pretrained(checkpoint_path)\n    model.eval()\n    total_loss = 0\n    total_steps = 0\n    total_samples = 0\n    hit_num = 0\n    total_num = 0\n    logits_list = list()\n    y_trues = list()\n    total_spent_time = 0.0\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    for (_step, batch) in enumerate(valid_dataloader):\n        try:\n            batch = {key: val.to(device) if isinstance(val, torch.Tensor) else val for (key, val) in batch.items()}\n        except RuntimeError:\n            batch = {key: val for (key, val) in batch.items()}\n        infer_start_time = time.time()\n        with torch.no_grad():\n            label_ids = batch.pop('label_ids')\n            outputs = model(batch)\n        infer_end_time = time.time()\n        total_spent_time += infer_end_time - infer_start_time\n        assert 'logits' in outputs\n        logits = outputs['logits']\n        y_trues.extend(label_ids.tolist())\n        logits_list.extend(logits.tolist())\n        hit_num += torch.sum(torch.argmax(logits, dim=-1) == label_ids).item()\n        total_num += label_ids.shape[0]\n        if len(logits.shape) == 1 or logits.shape[-1] == 1:\n            tmp_loss = losses.mse_loss(logits, label_ids)\n        elif len(logits.shape) == 2:\n            tmp_loss = losses.cross_entropy(logits, label_ids)\n        else:\n            raise RuntimeError\n        total_loss += tmp_loss.mean().item()\n        total_steps += 1\n        total_samples += valid_dataloader.batch_size\n        if (_step + 1) % 100 == 0:\n            total_step = len(valid_dataloader.dataset) // valid_dataloader.batch_size\n            logger.info('Eval: {}/{} steps finished'.format(_step + 1, total_step))\n    logger.info('Inference time = {:.2f}s, [{:.4f} ms / sample] '.format(total_spent_time, total_spent_time * 1000 / total_samples))\n    eval_loss = total_loss / total_steps\n    logger.info('Eval loss: {}'.format(eval_loss))\n    logits_list = np.array(logits_list)\n    eval_outputs = list()\n    for metric in metrics[-1]:\n        if metric.endswith('accuracy'):\n            acc = hit_num / total_num\n            logger.info('Accuracy: {}'.format(acc))\n            eval_outputs.append(('accuracy', acc))\n        elif metric == 'f1':\n            if model.config.num_labels == 2:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1))\n                logger.info('F1: {}'.format(f1))\n                eval_outputs.append(('f1', f1))\n            else:\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='macro')\n                logger.info('Macro F1: {}'.format(f1))\n                eval_outputs.append(('macro-f1', f1))\n                f1 = f1_score(y_trues, np.argmax(logits_list, axis=-1), average='micro')\n                logger.info('Micro F1: {}'.format(f1))\n                eval_outputs.append(('micro-f1', f1))\n        else:\n            raise NotImplementedError('Metric %s not implemented' % metric)\n    return dict(eval_outputs)"
        ]
    }
]