[
    {
        "func_name": "parse_args",
        "original": "def parse_args():\n    parser = argparse.ArgumentParser(description='Finetune (m)LUKE on a token classification task (such as NER) with the accelerate library')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--text_column_name', type=str, default=None, help='The column name of text to input in the file (a csv or JSON file).')\n    parser.add_argument('--label_column_name', type=str, default=None, help='The column name of label to input in the file (a csv or JSON file).')\n    parser.add_argument('--max_length', type=int, default=128, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_entity_length', type=int, default=32, help='The maximum total input entity length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_mention_length', type=int, default=30, help='The maximum total input mention length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=True)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--label_all_tokens', action='store_true', help='Setting labels of all special tokens to -100 and thus PyTorch will ignore them.')\n    parser.add_argument('--return_entity_level_metrics', action='store_true', help='Indication whether entity level metrics are to be returner.')\n    parser.add_argument('--task_name', type=str, default='ner', choices=['ner', 'pos', 'chunk'], help='The name of the task.')\n    parser.add_argument('--debug', action='store_true', help='Activate debug mode and run training only with a subset of data.')\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    args = parser.parse_args()\n    if args.task_name is None and args.train_file is None and (args.validation_file is None):\n        raise ValueError('Need either a task name or a training/validation file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
        "mutated": [
            "def parse_args():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser(description='Finetune (m)LUKE on a token classification task (such as NER) with the accelerate library')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--text_column_name', type=str, default=None, help='The column name of text to input in the file (a csv or JSON file).')\n    parser.add_argument('--label_column_name', type=str, default=None, help='The column name of label to input in the file (a csv or JSON file).')\n    parser.add_argument('--max_length', type=int, default=128, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_entity_length', type=int, default=32, help='The maximum total input entity length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_mention_length', type=int, default=30, help='The maximum total input mention length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=True)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--label_all_tokens', action='store_true', help='Setting labels of all special tokens to -100 and thus PyTorch will ignore them.')\n    parser.add_argument('--return_entity_level_metrics', action='store_true', help='Indication whether entity level metrics are to be returner.')\n    parser.add_argument('--task_name', type=str, default='ner', choices=['ner', 'pos', 'chunk'], help='The name of the task.')\n    parser.add_argument('--debug', action='store_true', help='Activate debug mode and run training only with a subset of data.')\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    args = parser.parse_args()\n    if args.task_name is None and args.train_file is None and (args.validation_file is None):\n        raise ValueError('Need either a task name or a training/validation file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser(description='Finetune (m)LUKE on a token classification task (such as NER) with the accelerate library')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--text_column_name', type=str, default=None, help='The column name of text to input in the file (a csv or JSON file).')\n    parser.add_argument('--label_column_name', type=str, default=None, help='The column name of label to input in the file (a csv or JSON file).')\n    parser.add_argument('--max_length', type=int, default=128, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_entity_length', type=int, default=32, help='The maximum total input entity length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_mention_length', type=int, default=30, help='The maximum total input mention length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=True)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--label_all_tokens', action='store_true', help='Setting labels of all special tokens to -100 and thus PyTorch will ignore them.')\n    parser.add_argument('--return_entity_level_metrics', action='store_true', help='Indication whether entity level metrics are to be returner.')\n    parser.add_argument('--task_name', type=str, default='ner', choices=['ner', 'pos', 'chunk'], help='The name of the task.')\n    parser.add_argument('--debug', action='store_true', help='Activate debug mode and run training only with a subset of data.')\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    args = parser.parse_args()\n    if args.task_name is None and args.train_file is None and (args.validation_file is None):\n        raise ValueError('Need either a task name or a training/validation file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser(description='Finetune (m)LUKE on a token classification task (such as NER) with the accelerate library')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--text_column_name', type=str, default=None, help='The column name of text to input in the file (a csv or JSON file).')\n    parser.add_argument('--label_column_name', type=str, default=None, help='The column name of label to input in the file (a csv or JSON file).')\n    parser.add_argument('--max_length', type=int, default=128, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_entity_length', type=int, default=32, help='The maximum total input entity length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_mention_length', type=int, default=30, help='The maximum total input mention length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=True)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--label_all_tokens', action='store_true', help='Setting labels of all special tokens to -100 and thus PyTorch will ignore them.')\n    parser.add_argument('--return_entity_level_metrics', action='store_true', help='Indication whether entity level metrics are to be returner.')\n    parser.add_argument('--task_name', type=str, default='ner', choices=['ner', 'pos', 'chunk'], help='The name of the task.')\n    parser.add_argument('--debug', action='store_true', help='Activate debug mode and run training only with a subset of data.')\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    args = parser.parse_args()\n    if args.task_name is None and args.train_file is None and (args.validation_file is None):\n        raise ValueError('Need either a task name or a training/validation file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser(description='Finetune (m)LUKE on a token classification task (such as NER) with the accelerate library')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--text_column_name', type=str, default=None, help='The column name of text to input in the file (a csv or JSON file).')\n    parser.add_argument('--label_column_name', type=str, default=None, help='The column name of label to input in the file (a csv or JSON file).')\n    parser.add_argument('--max_length', type=int, default=128, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_entity_length', type=int, default=32, help='The maximum total input entity length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_mention_length', type=int, default=30, help='The maximum total input mention length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=True)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--label_all_tokens', action='store_true', help='Setting labels of all special tokens to -100 and thus PyTorch will ignore them.')\n    parser.add_argument('--return_entity_level_metrics', action='store_true', help='Indication whether entity level metrics are to be returner.')\n    parser.add_argument('--task_name', type=str, default='ner', choices=['ner', 'pos', 'chunk'], help='The name of the task.')\n    parser.add_argument('--debug', action='store_true', help='Activate debug mode and run training only with a subset of data.')\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    args = parser.parse_args()\n    if args.task_name is None and args.train_file is None and (args.validation_file is None):\n        raise ValueError('Need either a task name or a training/validation file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser(description='Finetune (m)LUKE on a token classification task (such as NER) with the accelerate library')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--text_column_name', type=str, default=None, help='The column name of text to input in the file (a csv or JSON file).')\n    parser.add_argument('--label_column_name', type=str, default=None, help='The column name of label to input in the file (a csv or JSON file).')\n    parser.add_argument('--max_length', type=int, default=128, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_entity_length', type=int, default=32, help='The maximum total input entity length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--max_mention_length', type=int, default=30, help='The maximum total input mention length after tokenization (Used only for (M)Luke models). Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_length` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=True)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--label_all_tokens', action='store_true', help='Setting labels of all special tokens to -100 and thus PyTorch will ignore them.')\n    parser.add_argument('--return_entity_level_metrics', action='store_true', help='Indication whether entity level metrics are to be returner.')\n    parser.add_argument('--task_name', type=str, default='ner', choices=['ner', 'pos', 'chunk'], help='The name of the task.')\n    parser.add_argument('--debug', action='store_true', help='Activate debug mode and run training only with a subset of data.')\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    args = parser.parse_args()\n    if args.task_name is None and args.train_file is None and (args.validation_file is None):\n        raise ValueError('Need either a task name or a training/validation file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args"
        ]
    },
    {
        "func_name": "get_label_list",
        "original": "def get_label_list(labels):\n    unique_labels = set()\n    for label in labels:\n        unique_labels = unique_labels | set(label)\n    label_list = list(unique_labels)\n    label_list.sort()\n    return label_list",
        "mutated": [
            "def get_label_list(labels):\n    if False:\n        i = 10\n    unique_labels = set()\n    for label in labels:\n        unique_labels = unique_labels | set(label)\n    label_list = list(unique_labels)\n    label_list.sort()\n    return label_list",
            "def get_label_list(labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    unique_labels = set()\n    for label in labels:\n        unique_labels = unique_labels | set(label)\n    label_list = list(unique_labels)\n    label_list.sort()\n    return label_list",
            "def get_label_list(labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    unique_labels = set()\n    for label in labels:\n        unique_labels = unique_labels | set(label)\n    label_list = list(unique_labels)\n    label_list.sort()\n    return label_list",
            "def get_label_list(labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    unique_labels = set()\n    for label in labels:\n        unique_labels = unique_labels | set(label)\n    label_list = list(unique_labels)\n    label_list.sort()\n    return label_list",
            "def get_label_list(labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    unique_labels = set()\n    for label in labels:\n        unique_labels = unique_labels | set(label)\n    label_list = list(unique_labels)\n    label_list.sort()\n    return label_list"
        ]
    },
    {
        "func_name": "compute_sentence_boundaries_for_luke",
        "original": "def compute_sentence_boundaries_for_luke(examples):\n    sentence_boundaries = []\n    for tokens in examples[text_column_name]:\n        sentence_boundaries.append([0, len(tokens)])\n    examples['sentence_boundaries'] = sentence_boundaries\n    return examples",
        "mutated": [
            "def compute_sentence_boundaries_for_luke(examples):\n    if False:\n        i = 10\n    sentence_boundaries = []\n    for tokens in examples[text_column_name]:\n        sentence_boundaries.append([0, len(tokens)])\n    examples['sentence_boundaries'] = sentence_boundaries\n    return examples",
            "def compute_sentence_boundaries_for_luke(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sentence_boundaries = []\n    for tokens in examples[text_column_name]:\n        sentence_boundaries.append([0, len(tokens)])\n    examples['sentence_boundaries'] = sentence_boundaries\n    return examples",
            "def compute_sentence_boundaries_for_luke(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sentence_boundaries = []\n    for tokens in examples[text_column_name]:\n        sentence_boundaries.append([0, len(tokens)])\n    examples['sentence_boundaries'] = sentence_boundaries\n    return examples",
            "def compute_sentence_boundaries_for_luke(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sentence_boundaries = []\n    for tokens in examples[text_column_name]:\n        sentence_boundaries.append([0, len(tokens)])\n    examples['sentence_boundaries'] = sentence_boundaries\n    return examples",
            "def compute_sentence_boundaries_for_luke(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sentence_boundaries = []\n    for tokens in examples[text_column_name]:\n        sentence_boundaries.append([0, len(tokens)])\n    examples['sentence_boundaries'] = sentence_boundaries\n    return examples"
        ]
    },
    {
        "func_name": "compute_entity_spans_for_luke",
        "original": "def compute_entity_spans_for_luke(examples):\n    all_entity_spans = []\n    texts = []\n    all_labels_entity_spans = []\n    all_original_entity_spans = []\n    for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n        subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n        total_subword_length = sum(subword_lengths)\n        (_, context_end) = sentence_boundaries\n        if total_subword_length > args.max_length - 2:\n            cur_length = sum(subword_lengths[:context_end])\n            idx = context_end - 1\n            while cur_length > args.max_length - 2:\n                cur_length -= subword_lengths[idx]\n                context_end -= 1\n                idx -= 1\n        text = ''\n        sentence_words = tokens[:context_end]\n        sentence_subword_lengths = subword_lengths[:context_end]\n        word_start_char_positions = []\n        word_end_char_positions = []\n        labels_positions = {}\n        for (word, label) in zip(sentence_words, labels):\n            if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                text = text.rstrip()\n            word_start_char_positions.append(len(text))\n            text += word\n            word_end_char_positions.append(len(text))\n            text += ' '\n            labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n        text = text.rstrip()\n        texts.append(text)\n        entity_spans = []\n        labels_entity_spans = []\n        original_entity_spans = []\n        for word_start in range(len(sentence_words)):\n            for word_end in range(word_start, len(sentence_words)):\n                if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                    entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                    original_entity_spans.append((word_start, word_end + 1))\n                    if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                        labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                    else:\n                        labels_entity_spans.append(0)\n        all_entity_spans.append(entity_spans)\n        all_labels_entity_spans.append(labels_entity_spans)\n        all_original_entity_spans.append(original_entity_spans)\n    examples['entity_spans'] = all_entity_spans\n    examples['text'] = texts\n    examples['labels_entity_spans'] = all_labels_entity_spans\n    examples['original_entity_spans'] = all_original_entity_spans\n    return examples",
        "mutated": [
            "def compute_entity_spans_for_luke(examples):\n    if False:\n        i = 10\n    all_entity_spans = []\n    texts = []\n    all_labels_entity_spans = []\n    all_original_entity_spans = []\n    for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n        subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n        total_subword_length = sum(subword_lengths)\n        (_, context_end) = sentence_boundaries\n        if total_subword_length > args.max_length - 2:\n            cur_length = sum(subword_lengths[:context_end])\n            idx = context_end - 1\n            while cur_length > args.max_length - 2:\n                cur_length -= subword_lengths[idx]\n                context_end -= 1\n                idx -= 1\n        text = ''\n        sentence_words = tokens[:context_end]\n        sentence_subword_lengths = subword_lengths[:context_end]\n        word_start_char_positions = []\n        word_end_char_positions = []\n        labels_positions = {}\n        for (word, label) in zip(sentence_words, labels):\n            if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                text = text.rstrip()\n            word_start_char_positions.append(len(text))\n            text += word\n            word_end_char_positions.append(len(text))\n            text += ' '\n            labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n        text = text.rstrip()\n        texts.append(text)\n        entity_spans = []\n        labels_entity_spans = []\n        original_entity_spans = []\n        for word_start in range(len(sentence_words)):\n            for word_end in range(word_start, len(sentence_words)):\n                if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                    entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                    original_entity_spans.append((word_start, word_end + 1))\n                    if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                        labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                    else:\n                        labels_entity_spans.append(0)\n        all_entity_spans.append(entity_spans)\n        all_labels_entity_spans.append(labels_entity_spans)\n        all_original_entity_spans.append(original_entity_spans)\n    examples['entity_spans'] = all_entity_spans\n    examples['text'] = texts\n    examples['labels_entity_spans'] = all_labels_entity_spans\n    examples['original_entity_spans'] = all_original_entity_spans\n    return examples",
            "def compute_entity_spans_for_luke(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    all_entity_spans = []\n    texts = []\n    all_labels_entity_spans = []\n    all_original_entity_spans = []\n    for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n        subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n        total_subword_length = sum(subword_lengths)\n        (_, context_end) = sentence_boundaries\n        if total_subword_length > args.max_length - 2:\n            cur_length = sum(subword_lengths[:context_end])\n            idx = context_end - 1\n            while cur_length > args.max_length - 2:\n                cur_length -= subword_lengths[idx]\n                context_end -= 1\n                idx -= 1\n        text = ''\n        sentence_words = tokens[:context_end]\n        sentence_subword_lengths = subword_lengths[:context_end]\n        word_start_char_positions = []\n        word_end_char_positions = []\n        labels_positions = {}\n        for (word, label) in zip(sentence_words, labels):\n            if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                text = text.rstrip()\n            word_start_char_positions.append(len(text))\n            text += word\n            word_end_char_positions.append(len(text))\n            text += ' '\n            labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n        text = text.rstrip()\n        texts.append(text)\n        entity_spans = []\n        labels_entity_spans = []\n        original_entity_spans = []\n        for word_start in range(len(sentence_words)):\n            for word_end in range(word_start, len(sentence_words)):\n                if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                    entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                    original_entity_spans.append((word_start, word_end + 1))\n                    if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                        labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                    else:\n                        labels_entity_spans.append(0)\n        all_entity_spans.append(entity_spans)\n        all_labels_entity_spans.append(labels_entity_spans)\n        all_original_entity_spans.append(original_entity_spans)\n    examples['entity_spans'] = all_entity_spans\n    examples['text'] = texts\n    examples['labels_entity_spans'] = all_labels_entity_spans\n    examples['original_entity_spans'] = all_original_entity_spans\n    return examples",
            "def compute_entity_spans_for_luke(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    all_entity_spans = []\n    texts = []\n    all_labels_entity_spans = []\n    all_original_entity_spans = []\n    for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n        subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n        total_subword_length = sum(subword_lengths)\n        (_, context_end) = sentence_boundaries\n        if total_subword_length > args.max_length - 2:\n            cur_length = sum(subword_lengths[:context_end])\n            idx = context_end - 1\n            while cur_length > args.max_length - 2:\n                cur_length -= subword_lengths[idx]\n                context_end -= 1\n                idx -= 1\n        text = ''\n        sentence_words = tokens[:context_end]\n        sentence_subword_lengths = subword_lengths[:context_end]\n        word_start_char_positions = []\n        word_end_char_positions = []\n        labels_positions = {}\n        for (word, label) in zip(sentence_words, labels):\n            if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                text = text.rstrip()\n            word_start_char_positions.append(len(text))\n            text += word\n            word_end_char_positions.append(len(text))\n            text += ' '\n            labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n        text = text.rstrip()\n        texts.append(text)\n        entity_spans = []\n        labels_entity_spans = []\n        original_entity_spans = []\n        for word_start in range(len(sentence_words)):\n            for word_end in range(word_start, len(sentence_words)):\n                if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                    entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                    original_entity_spans.append((word_start, word_end + 1))\n                    if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                        labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                    else:\n                        labels_entity_spans.append(0)\n        all_entity_spans.append(entity_spans)\n        all_labels_entity_spans.append(labels_entity_spans)\n        all_original_entity_spans.append(original_entity_spans)\n    examples['entity_spans'] = all_entity_spans\n    examples['text'] = texts\n    examples['labels_entity_spans'] = all_labels_entity_spans\n    examples['original_entity_spans'] = all_original_entity_spans\n    return examples",
            "def compute_entity_spans_for_luke(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    all_entity_spans = []\n    texts = []\n    all_labels_entity_spans = []\n    all_original_entity_spans = []\n    for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n        subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n        total_subword_length = sum(subword_lengths)\n        (_, context_end) = sentence_boundaries\n        if total_subword_length > args.max_length - 2:\n            cur_length = sum(subword_lengths[:context_end])\n            idx = context_end - 1\n            while cur_length > args.max_length - 2:\n                cur_length -= subword_lengths[idx]\n                context_end -= 1\n                idx -= 1\n        text = ''\n        sentence_words = tokens[:context_end]\n        sentence_subword_lengths = subword_lengths[:context_end]\n        word_start_char_positions = []\n        word_end_char_positions = []\n        labels_positions = {}\n        for (word, label) in zip(sentence_words, labels):\n            if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                text = text.rstrip()\n            word_start_char_positions.append(len(text))\n            text += word\n            word_end_char_positions.append(len(text))\n            text += ' '\n            labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n        text = text.rstrip()\n        texts.append(text)\n        entity_spans = []\n        labels_entity_spans = []\n        original_entity_spans = []\n        for word_start in range(len(sentence_words)):\n            for word_end in range(word_start, len(sentence_words)):\n                if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                    entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                    original_entity_spans.append((word_start, word_end + 1))\n                    if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                        labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                    else:\n                        labels_entity_spans.append(0)\n        all_entity_spans.append(entity_spans)\n        all_labels_entity_spans.append(labels_entity_spans)\n        all_original_entity_spans.append(original_entity_spans)\n    examples['entity_spans'] = all_entity_spans\n    examples['text'] = texts\n    examples['labels_entity_spans'] = all_labels_entity_spans\n    examples['original_entity_spans'] = all_original_entity_spans\n    return examples",
            "def compute_entity_spans_for_luke(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    all_entity_spans = []\n    texts = []\n    all_labels_entity_spans = []\n    all_original_entity_spans = []\n    for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n        subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n        total_subword_length = sum(subword_lengths)\n        (_, context_end) = sentence_boundaries\n        if total_subword_length > args.max_length - 2:\n            cur_length = sum(subword_lengths[:context_end])\n            idx = context_end - 1\n            while cur_length > args.max_length - 2:\n                cur_length -= subword_lengths[idx]\n                context_end -= 1\n                idx -= 1\n        text = ''\n        sentence_words = tokens[:context_end]\n        sentence_subword_lengths = subword_lengths[:context_end]\n        word_start_char_positions = []\n        word_end_char_positions = []\n        labels_positions = {}\n        for (word, label) in zip(sentence_words, labels):\n            if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                text = text.rstrip()\n            word_start_char_positions.append(len(text))\n            text += word\n            word_end_char_positions.append(len(text))\n            text += ' '\n            labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n        text = text.rstrip()\n        texts.append(text)\n        entity_spans = []\n        labels_entity_spans = []\n        original_entity_spans = []\n        for word_start in range(len(sentence_words)):\n            for word_end in range(word_start, len(sentence_words)):\n                if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                    entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                    original_entity_spans.append((word_start, word_end + 1))\n                    if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                        labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                    else:\n                        labels_entity_spans.append(0)\n        all_entity_spans.append(entity_spans)\n        all_labels_entity_spans.append(labels_entity_spans)\n        all_original_entity_spans.append(original_entity_spans)\n    examples['entity_spans'] = all_entity_spans\n    examples['text'] = texts\n    examples['labels_entity_spans'] = all_labels_entity_spans\n    examples['original_entity_spans'] = all_original_entity_spans\n    return examples"
        ]
    },
    {
        "func_name": "tokenize_and_align_labels",
        "original": "def tokenize_and_align_labels(examples):\n    entity_spans = []\n    for v in examples['entity_spans']:\n        entity_spans.append(list(map(tuple, v)))\n    tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n    if padding == 'max_length':\n        tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n    else:\n        tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n        tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n        tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n    return tokenized_inputs",
        "mutated": [
            "def tokenize_and_align_labels(examples):\n    if False:\n        i = 10\n    entity_spans = []\n    for v in examples['entity_spans']:\n        entity_spans.append(list(map(tuple, v)))\n    tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n    if padding == 'max_length':\n        tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n    else:\n        tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n        tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n        tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n    return tokenized_inputs",
            "def tokenize_and_align_labels(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    entity_spans = []\n    for v in examples['entity_spans']:\n        entity_spans.append(list(map(tuple, v)))\n    tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n    if padding == 'max_length':\n        tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n    else:\n        tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n        tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n        tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n    return tokenized_inputs",
            "def tokenize_and_align_labels(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    entity_spans = []\n    for v in examples['entity_spans']:\n        entity_spans.append(list(map(tuple, v)))\n    tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n    if padding == 'max_length':\n        tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n    else:\n        tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n        tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n        tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n    return tokenized_inputs",
            "def tokenize_and_align_labels(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    entity_spans = []\n    for v in examples['entity_spans']:\n        entity_spans.append(list(map(tuple, v)))\n    tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n    if padding == 'max_length':\n        tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n    else:\n        tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n        tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n        tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n    return tokenized_inputs",
            "def tokenize_and_align_labels(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    entity_spans = []\n    for v in examples['entity_spans']:\n        entity_spans.append(list(map(tuple, v)))\n    tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n    if padding == 'max_length':\n        tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n        tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n    else:\n        tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n        tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n        tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n    return tokenized_inputs"
        ]
    },
    {
        "func_name": "get_luke_labels",
        "original": "def get_luke_labels(outputs, ner_tags, original_entity_spans):\n    true_predictions = []\n    true_labels = []\n    for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n        true_tags = [val for val in tags if val != -1]\n        true_original_spans = [val for val in original_spans if val != (-1, -1)]\n        max_indices = torch.argmax(output, axis=1)\n        max_logits = torch.max(output, axis=1).values\n        predictions = []\n        for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n            if index != 0:\n                predictions.append((logit, span, label_list[index]))\n        predicted_sequence = [label_list[0]] * len(true_tags)\n        for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n            if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                predicted_sequence[span[0]] = label\n                if span[1] - span[0] > 1:\n                    predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n        true_predictions.append(predicted_sequence)\n        true_labels.append([label_list[tag_id] for tag_id in true_tags])\n    return (true_predictions, true_labels)",
        "mutated": [
            "def get_luke_labels(outputs, ner_tags, original_entity_spans):\n    if False:\n        i = 10\n    true_predictions = []\n    true_labels = []\n    for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n        true_tags = [val for val in tags if val != -1]\n        true_original_spans = [val for val in original_spans if val != (-1, -1)]\n        max_indices = torch.argmax(output, axis=1)\n        max_logits = torch.max(output, axis=1).values\n        predictions = []\n        for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n            if index != 0:\n                predictions.append((logit, span, label_list[index]))\n        predicted_sequence = [label_list[0]] * len(true_tags)\n        for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n            if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                predicted_sequence[span[0]] = label\n                if span[1] - span[0] > 1:\n                    predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n        true_predictions.append(predicted_sequence)\n        true_labels.append([label_list[tag_id] for tag_id in true_tags])\n    return (true_predictions, true_labels)",
            "def get_luke_labels(outputs, ner_tags, original_entity_spans):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    true_predictions = []\n    true_labels = []\n    for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n        true_tags = [val for val in tags if val != -1]\n        true_original_spans = [val for val in original_spans if val != (-1, -1)]\n        max_indices = torch.argmax(output, axis=1)\n        max_logits = torch.max(output, axis=1).values\n        predictions = []\n        for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n            if index != 0:\n                predictions.append((logit, span, label_list[index]))\n        predicted_sequence = [label_list[0]] * len(true_tags)\n        for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n            if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                predicted_sequence[span[0]] = label\n                if span[1] - span[0] > 1:\n                    predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n        true_predictions.append(predicted_sequence)\n        true_labels.append([label_list[tag_id] for tag_id in true_tags])\n    return (true_predictions, true_labels)",
            "def get_luke_labels(outputs, ner_tags, original_entity_spans):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    true_predictions = []\n    true_labels = []\n    for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n        true_tags = [val for val in tags if val != -1]\n        true_original_spans = [val for val in original_spans if val != (-1, -1)]\n        max_indices = torch.argmax(output, axis=1)\n        max_logits = torch.max(output, axis=1).values\n        predictions = []\n        for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n            if index != 0:\n                predictions.append((logit, span, label_list[index]))\n        predicted_sequence = [label_list[0]] * len(true_tags)\n        for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n            if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                predicted_sequence[span[0]] = label\n                if span[1] - span[0] > 1:\n                    predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n        true_predictions.append(predicted_sequence)\n        true_labels.append([label_list[tag_id] for tag_id in true_tags])\n    return (true_predictions, true_labels)",
            "def get_luke_labels(outputs, ner_tags, original_entity_spans):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    true_predictions = []\n    true_labels = []\n    for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n        true_tags = [val for val in tags if val != -1]\n        true_original_spans = [val for val in original_spans if val != (-1, -1)]\n        max_indices = torch.argmax(output, axis=1)\n        max_logits = torch.max(output, axis=1).values\n        predictions = []\n        for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n            if index != 0:\n                predictions.append((logit, span, label_list[index]))\n        predicted_sequence = [label_list[0]] * len(true_tags)\n        for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n            if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                predicted_sequence[span[0]] = label\n                if span[1] - span[0] > 1:\n                    predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n        true_predictions.append(predicted_sequence)\n        true_labels.append([label_list[tag_id] for tag_id in true_tags])\n    return (true_predictions, true_labels)",
            "def get_luke_labels(outputs, ner_tags, original_entity_spans):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    true_predictions = []\n    true_labels = []\n    for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n        true_tags = [val for val in tags if val != -1]\n        true_original_spans = [val for val in original_spans if val != (-1, -1)]\n        max_indices = torch.argmax(output, axis=1)\n        max_logits = torch.max(output, axis=1).values\n        predictions = []\n        for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n            if index != 0:\n                predictions.append((logit, span, label_list[index]))\n        predicted_sequence = [label_list[0]] * len(true_tags)\n        for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n            if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                predicted_sequence[span[0]] = label\n                if span[1] - span[0] > 1:\n                    predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n        true_predictions.append(predicted_sequence)\n        true_labels.append([label_list[tag_id] for tag_id in true_tags])\n    return (true_predictions, true_labels)"
        ]
    },
    {
        "func_name": "compute_metrics",
        "original": "def compute_metrics():\n    results = metric.compute()\n    if args.return_entity_level_metrics:\n        final_results = {}\n        for (key, value) in results.items():\n            if isinstance(value, dict):\n                for (n, v) in value.items():\n                    final_results[f'{key}_{n}'] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}",
        "mutated": [
            "def compute_metrics():\n    if False:\n        i = 10\n    results = metric.compute()\n    if args.return_entity_level_metrics:\n        final_results = {}\n        for (key, value) in results.items():\n            if isinstance(value, dict):\n                for (n, v) in value.items():\n                    final_results[f'{key}_{n}'] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}",
            "def compute_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    results = metric.compute()\n    if args.return_entity_level_metrics:\n        final_results = {}\n        for (key, value) in results.items():\n            if isinstance(value, dict):\n                for (n, v) in value.items():\n                    final_results[f'{key}_{n}'] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}",
            "def compute_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    results = metric.compute()\n    if args.return_entity_level_metrics:\n        final_results = {}\n        for (key, value) in results.items():\n            if isinstance(value, dict):\n                for (n, v) in value.items():\n                    final_results[f'{key}_{n}'] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}",
            "def compute_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    results = metric.compute()\n    if args.return_entity_level_metrics:\n        final_results = {}\n        for (key, value) in results.items():\n            if isinstance(value, dict):\n                for (n, v) in value.items():\n                    final_results[f'{key}_{n}'] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}",
            "def compute_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    results = metric.compute()\n    if args.return_entity_level_metrics:\n        final_results = {}\n        for (key, value) in results.items():\n            if isinstance(value, dict):\n                for (n, v) in value.items():\n                    final_results[f'{key}_{n}'] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    args = parse_args()\n    handler = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[handler])\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    if raw_datasets['train'] is not None:\n        column_names = raw_datasets['train'].column_names\n        features = raw_datasets['train'].features\n    else:\n        column_names = raw_datasets['validation'].column_names\n        features = raw_datasets['validation'].features\n    if args.text_column_name is not None:\n        text_column_name = args.text_column_name\n    elif 'tokens' in column_names:\n        text_column_name = 'tokens'\n    else:\n        text_column_name = column_names[0]\n    if args.label_column_name is not None:\n        label_column_name = args.label_column_name\n    elif f'{args.task_name}_tags' in column_names:\n        label_column_name = f'{args.task_name}_tags'\n    else:\n        label_column_name = column_names[1]\n\n    def get_label_list(labels):\n        unique_labels = set()\n        for label in labels:\n            unique_labels = unique_labels | set(label)\n        label_list = list(unique_labels)\n        label_list.sort()\n        return label_list\n    if isinstance(features[label_column_name].feature, ClassLabel):\n        label_list = features[label_column_name].feature.names\n    else:\n        label_list = get_label_list(raw_datasets['train'][label_column_name])\n    num_labels = len(label_list)\n    b_to_i_label = []\n    for (idx, label) in enumerate(label_list):\n        if label.startswith('B-') and label.replace('B-', 'I-') in label_list:\n            b_to_i_label.append(label_list.index(label.replace('B-', 'I-')))\n        else:\n            b_to_i_label.append(idx)\n    if args.config_name:\n        config = LukeConfig.from_pretrained(args.config_name, num_labels=num_labels)\n    elif args.model_name_or_path:\n        config = LukeConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n    else:\n        logger.warning('You are instantiating a new config instance from scratch.')\n    tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n    if not tokenizer_name_or_path:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    tokenizer = LukeTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=False, task='entity_span_classification', max_entity_length=args.max_entity_length, max_mention_length=args.max_mention_length)\n    if args.model_name_or_path:\n        model = LukeForEntitySpanClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n    else:\n        logger.info('Training new model from scratch')\n        model = LukeForEntitySpanClassification.from_config(config)\n    model.resize_token_embeddings(len(tokenizer))\n    padding = 'max_length' if args.pad_to_max_length else False\n\n    def compute_sentence_boundaries_for_luke(examples):\n        sentence_boundaries = []\n        for tokens in examples[text_column_name]:\n            sentence_boundaries.append([0, len(tokens)])\n        examples['sentence_boundaries'] = sentence_boundaries\n        return examples\n\n    def compute_entity_spans_for_luke(examples):\n        all_entity_spans = []\n        texts = []\n        all_labels_entity_spans = []\n        all_original_entity_spans = []\n        for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n            subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n            total_subword_length = sum(subword_lengths)\n            (_, context_end) = sentence_boundaries\n            if total_subword_length > args.max_length - 2:\n                cur_length = sum(subword_lengths[:context_end])\n                idx = context_end - 1\n                while cur_length > args.max_length - 2:\n                    cur_length -= subword_lengths[idx]\n                    context_end -= 1\n                    idx -= 1\n            text = ''\n            sentence_words = tokens[:context_end]\n            sentence_subword_lengths = subword_lengths[:context_end]\n            word_start_char_positions = []\n            word_end_char_positions = []\n            labels_positions = {}\n            for (word, label) in zip(sentence_words, labels):\n                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                    text = text.rstrip()\n                word_start_char_positions.append(len(text))\n                text += word\n                word_end_char_positions.append(len(text))\n                text += ' '\n                labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n            text = text.rstrip()\n            texts.append(text)\n            entity_spans = []\n            labels_entity_spans = []\n            original_entity_spans = []\n            for word_start in range(len(sentence_words)):\n                for word_end in range(word_start, len(sentence_words)):\n                    if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                        entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                        original_entity_spans.append((word_start, word_end + 1))\n                        if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                            labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                        else:\n                            labels_entity_spans.append(0)\n            all_entity_spans.append(entity_spans)\n            all_labels_entity_spans.append(labels_entity_spans)\n            all_original_entity_spans.append(original_entity_spans)\n        examples['entity_spans'] = all_entity_spans\n        examples['text'] = texts\n        examples['labels_entity_spans'] = all_labels_entity_spans\n        examples['original_entity_spans'] = all_original_entity_spans\n        return examples\n\n    def tokenize_and_align_labels(examples):\n        entity_spans = []\n        for v in examples['entity_spans']:\n            entity_spans.append(list(map(tuple, v)))\n        tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n        if padding == 'max_length':\n            tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n        else:\n            tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n            tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n            tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n        return tokenized_inputs\n    with accelerator.main_process_first():\n        raw_datasets = raw_datasets.map(compute_sentence_boundaries_for_luke, batched=True, desc='Adding sentence boundaries')\n        raw_datasets = raw_datasets.map(compute_entity_spans_for_luke, batched=True, desc='Adding sentence spans')\n        processed_raw_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names, desc='Running tokenizer on dataset')\n    train_dataset = processed_raw_datasets['train']\n    eval_dataset = processed_raw_datasets['validation']\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorForLukeTokenClassification(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    device = accelerator.device\n    model.to(device)\n    (model, optimizer, train_dataloader, eval_dataloader) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps, num_training_steps=args.max_train_steps)\n    metric = load_metric('seqeval')\n\n    def get_luke_labels(outputs, ner_tags, original_entity_spans):\n        true_predictions = []\n        true_labels = []\n        for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n            true_tags = [val for val in tags if val != -1]\n            true_original_spans = [val for val in original_spans if val != (-1, -1)]\n            max_indices = torch.argmax(output, axis=1)\n            max_logits = torch.max(output, axis=1).values\n            predictions = []\n            for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n                if index != 0:\n                    predictions.append((logit, span, label_list[index]))\n            predicted_sequence = [label_list[0]] * len(true_tags)\n            for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n                if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                    predicted_sequence[span[0]] = label\n                    if span[1] - span[0] > 1:\n                        predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n            true_predictions.append(predicted_sequence)\n            true_labels.append([label_list[tag_id] for tag_id in true_tags])\n        return (true_predictions, true_labels)\n\n    def compute_metrics():\n        results = metric.compute()\n        if args.return_entity_level_metrics:\n            final_results = {}\n            for (key, value) in results.items():\n                if isinstance(value, dict):\n                    for (n, v) in value.items():\n                        final_results[f'{key}_{n}'] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for (step, batch) in enumerate(train_dataloader):\n            _ = batch.pop('original_entity_spans')\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n            if completed_steps >= args.max_train_steps:\n                break\n        model.eval()\n        for (step, batch) in enumerate(eval_dataloader):\n            original_entity_spans = batch.pop('original_entity_spans')\n            with torch.no_grad():\n                outputs = model(**batch)\n            (preds, refs) = get_luke_labels(outputs, batch[label_column_name], original_entity_spans)\n            metric.add_batch(predictions=preds, references=refs)\n        eval_metric = compute_metrics()\n        accelerator.print(f'epoch {epoch}:', eval_metric)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    args = parse_args()\n    handler = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[handler])\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    if raw_datasets['train'] is not None:\n        column_names = raw_datasets['train'].column_names\n        features = raw_datasets['train'].features\n    else:\n        column_names = raw_datasets['validation'].column_names\n        features = raw_datasets['validation'].features\n    if args.text_column_name is not None:\n        text_column_name = args.text_column_name\n    elif 'tokens' in column_names:\n        text_column_name = 'tokens'\n    else:\n        text_column_name = column_names[0]\n    if args.label_column_name is not None:\n        label_column_name = args.label_column_name\n    elif f'{args.task_name}_tags' in column_names:\n        label_column_name = f'{args.task_name}_tags'\n    else:\n        label_column_name = column_names[1]\n\n    def get_label_list(labels):\n        unique_labels = set()\n        for label in labels:\n            unique_labels = unique_labels | set(label)\n        label_list = list(unique_labels)\n        label_list.sort()\n        return label_list\n    if isinstance(features[label_column_name].feature, ClassLabel):\n        label_list = features[label_column_name].feature.names\n    else:\n        label_list = get_label_list(raw_datasets['train'][label_column_name])\n    num_labels = len(label_list)\n    b_to_i_label = []\n    for (idx, label) in enumerate(label_list):\n        if label.startswith('B-') and label.replace('B-', 'I-') in label_list:\n            b_to_i_label.append(label_list.index(label.replace('B-', 'I-')))\n        else:\n            b_to_i_label.append(idx)\n    if args.config_name:\n        config = LukeConfig.from_pretrained(args.config_name, num_labels=num_labels)\n    elif args.model_name_or_path:\n        config = LukeConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n    else:\n        logger.warning('You are instantiating a new config instance from scratch.')\n    tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n    if not tokenizer_name_or_path:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    tokenizer = LukeTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=False, task='entity_span_classification', max_entity_length=args.max_entity_length, max_mention_length=args.max_mention_length)\n    if args.model_name_or_path:\n        model = LukeForEntitySpanClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n    else:\n        logger.info('Training new model from scratch')\n        model = LukeForEntitySpanClassification.from_config(config)\n    model.resize_token_embeddings(len(tokenizer))\n    padding = 'max_length' if args.pad_to_max_length else False\n\n    def compute_sentence_boundaries_for_luke(examples):\n        sentence_boundaries = []\n        for tokens in examples[text_column_name]:\n            sentence_boundaries.append([0, len(tokens)])\n        examples['sentence_boundaries'] = sentence_boundaries\n        return examples\n\n    def compute_entity_spans_for_luke(examples):\n        all_entity_spans = []\n        texts = []\n        all_labels_entity_spans = []\n        all_original_entity_spans = []\n        for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n            subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n            total_subword_length = sum(subword_lengths)\n            (_, context_end) = sentence_boundaries\n            if total_subword_length > args.max_length - 2:\n                cur_length = sum(subword_lengths[:context_end])\n                idx = context_end - 1\n                while cur_length > args.max_length - 2:\n                    cur_length -= subword_lengths[idx]\n                    context_end -= 1\n                    idx -= 1\n            text = ''\n            sentence_words = tokens[:context_end]\n            sentence_subword_lengths = subword_lengths[:context_end]\n            word_start_char_positions = []\n            word_end_char_positions = []\n            labels_positions = {}\n            for (word, label) in zip(sentence_words, labels):\n                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                    text = text.rstrip()\n                word_start_char_positions.append(len(text))\n                text += word\n                word_end_char_positions.append(len(text))\n                text += ' '\n                labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n            text = text.rstrip()\n            texts.append(text)\n            entity_spans = []\n            labels_entity_spans = []\n            original_entity_spans = []\n            for word_start in range(len(sentence_words)):\n                for word_end in range(word_start, len(sentence_words)):\n                    if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                        entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                        original_entity_spans.append((word_start, word_end + 1))\n                        if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                            labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                        else:\n                            labels_entity_spans.append(0)\n            all_entity_spans.append(entity_spans)\n            all_labels_entity_spans.append(labels_entity_spans)\n            all_original_entity_spans.append(original_entity_spans)\n        examples['entity_spans'] = all_entity_spans\n        examples['text'] = texts\n        examples['labels_entity_spans'] = all_labels_entity_spans\n        examples['original_entity_spans'] = all_original_entity_spans\n        return examples\n\n    def tokenize_and_align_labels(examples):\n        entity_spans = []\n        for v in examples['entity_spans']:\n            entity_spans.append(list(map(tuple, v)))\n        tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n        if padding == 'max_length':\n            tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n        else:\n            tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n            tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n            tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n        return tokenized_inputs\n    with accelerator.main_process_first():\n        raw_datasets = raw_datasets.map(compute_sentence_boundaries_for_luke, batched=True, desc='Adding sentence boundaries')\n        raw_datasets = raw_datasets.map(compute_entity_spans_for_luke, batched=True, desc='Adding sentence spans')\n        processed_raw_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names, desc='Running tokenizer on dataset')\n    train_dataset = processed_raw_datasets['train']\n    eval_dataset = processed_raw_datasets['validation']\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorForLukeTokenClassification(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    device = accelerator.device\n    model.to(device)\n    (model, optimizer, train_dataloader, eval_dataloader) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps, num_training_steps=args.max_train_steps)\n    metric = load_metric('seqeval')\n\n    def get_luke_labels(outputs, ner_tags, original_entity_spans):\n        true_predictions = []\n        true_labels = []\n        for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n            true_tags = [val for val in tags if val != -1]\n            true_original_spans = [val for val in original_spans if val != (-1, -1)]\n            max_indices = torch.argmax(output, axis=1)\n            max_logits = torch.max(output, axis=1).values\n            predictions = []\n            for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n                if index != 0:\n                    predictions.append((logit, span, label_list[index]))\n            predicted_sequence = [label_list[0]] * len(true_tags)\n            for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n                if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                    predicted_sequence[span[0]] = label\n                    if span[1] - span[0] > 1:\n                        predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n            true_predictions.append(predicted_sequence)\n            true_labels.append([label_list[tag_id] for tag_id in true_tags])\n        return (true_predictions, true_labels)\n\n    def compute_metrics():\n        results = metric.compute()\n        if args.return_entity_level_metrics:\n            final_results = {}\n            for (key, value) in results.items():\n                if isinstance(value, dict):\n                    for (n, v) in value.items():\n                        final_results[f'{key}_{n}'] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for (step, batch) in enumerate(train_dataloader):\n            _ = batch.pop('original_entity_spans')\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n            if completed_steps >= args.max_train_steps:\n                break\n        model.eval()\n        for (step, batch) in enumerate(eval_dataloader):\n            original_entity_spans = batch.pop('original_entity_spans')\n            with torch.no_grad():\n                outputs = model(**batch)\n            (preds, refs) = get_luke_labels(outputs, batch[label_column_name], original_entity_spans)\n            metric.add_batch(predictions=preds, references=refs)\n        eval_metric = compute_metrics()\n        accelerator.print(f'epoch {epoch}:', eval_metric)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    args = parse_args()\n    handler = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[handler])\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    if raw_datasets['train'] is not None:\n        column_names = raw_datasets['train'].column_names\n        features = raw_datasets['train'].features\n    else:\n        column_names = raw_datasets['validation'].column_names\n        features = raw_datasets['validation'].features\n    if args.text_column_name is not None:\n        text_column_name = args.text_column_name\n    elif 'tokens' in column_names:\n        text_column_name = 'tokens'\n    else:\n        text_column_name = column_names[0]\n    if args.label_column_name is not None:\n        label_column_name = args.label_column_name\n    elif f'{args.task_name}_tags' in column_names:\n        label_column_name = f'{args.task_name}_tags'\n    else:\n        label_column_name = column_names[1]\n\n    def get_label_list(labels):\n        unique_labels = set()\n        for label in labels:\n            unique_labels = unique_labels | set(label)\n        label_list = list(unique_labels)\n        label_list.sort()\n        return label_list\n    if isinstance(features[label_column_name].feature, ClassLabel):\n        label_list = features[label_column_name].feature.names\n    else:\n        label_list = get_label_list(raw_datasets['train'][label_column_name])\n    num_labels = len(label_list)\n    b_to_i_label = []\n    for (idx, label) in enumerate(label_list):\n        if label.startswith('B-') and label.replace('B-', 'I-') in label_list:\n            b_to_i_label.append(label_list.index(label.replace('B-', 'I-')))\n        else:\n            b_to_i_label.append(idx)\n    if args.config_name:\n        config = LukeConfig.from_pretrained(args.config_name, num_labels=num_labels)\n    elif args.model_name_or_path:\n        config = LukeConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n    else:\n        logger.warning('You are instantiating a new config instance from scratch.')\n    tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n    if not tokenizer_name_or_path:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    tokenizer = LukeTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=False, task='entity_span_classification', max_entity_length=args.max_entity_length, max_mention_length=args.max_mention_length)\n    if args.model_name_or_path:\n        model = LukeForEntitySpanClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n    else:\n        logger.info('Training new model from scratch')\n        model = LukeForEntitySpanClassification.from_config(config)\n    model.resize_token_embeddings(len(tokenizer))\n    padding = 'max_length' if args.pad_to_max_length else False\n\n    def compute_sentence_boundaries_for_luke(examples):\n        sentence_boundaries = []\n        for tokens in examples[text_column_name]:\n            sentence_boundaries.append([0, len(tokens)])\n        examples['sentence_boundaries'] = sentence_boundaries\n        return examples\n\n    def compute_entity_spans_for_luke(examples):\n        all_entity_spans = []\n        texts = []\n        all_labels_entity_spans = []\n        all_original_entity_spans = []\n        for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n            subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n            total_subword_length = sum(subword_lengths)\n            (_, context_end) = sentence_boundaries\n            if total_subword_length > args.max_length - 2:\n                cur_length = sum(subword_lengths[:context_end])\n                idx = context_end - 1\n                while cur_length > args.max_length - 2:\n                    cur_length -= subword_lengths[idx]\n                    context_end -= 1\n                    idx -= 1\n            text = ''\n            sentence_words = tokens[:context_end]\n            sentence_subword_lengths = subword_lengths[:context_end]\n            word_start_char_positions = []\n            word_end_char_positions = []\n            labels_positions = {}\n            for (word, label) in zip(sentence_words, labels):\n                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                    text = text.rstrip()\n                word_start_char_positions.append(len(text))\n                text += word\n                word_end_char_positions.append(len(text))\n                text += ' '\n                labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n            text = text.rstrip()\n            texts.append(text)\n            entity_spans = []\n            labels_entity_spans = []\n            original_entity_spans = []\n            for word_start in range(len(sentence_words)):\n                for word_end in range(word_start, len(sentence_words)):\n                    if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                        entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                        original_entity_spans.append((word_start, word_end + 1))\n                        if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                            labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                        else:\n                            labels_entity_spans.append(0)\n            all_entity_spans.append(entity_spans)\n            all_labels_entity_spans.append(labels_entity_spans)\n            all_original_entity_spans.append(original_entity_spans)\n        examples['entity_spans'] = all_entity_spans\n        examples['text'] = texts\n        examples['labels_entity_spans'] = all_labels_entity_spans\n        examples['original_entity_spans'] = all_original_entity_spans\n        return examples\n\n    def tokenize_and_align_labels(examples):\n        entity_spans = []\n        for v in examples['entity_spans']:\n            entity_spans.append(list(map(tuple, v)))\n        tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n        if padding == 'max_length':\n            tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n        else:\n            tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n            tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n            tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n        return tokenized_inputs\n    with accelerator.main_process_first():\n        raw_datasets = raw_datasets.map(compute_sentence_boundaries_for_luke, batched=True, desc='Adding sentence boundaries')\n        raw_datasets = raw_datasets.map(compute_entity_spans_for_luke, batched=True, desc='Adding sentence spans')\n        processed_raw_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names, desc='Running tokenizer on dataset')\n    train_dataset = processed_raw_datasets['train']\n    eval_dataset = processed_raw_datasets['validation']\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorForLukeTokenClassification(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    device = accelerator.device\n    model.to(device)\n    (model, optimizer, train_dataloader, eval_dataloader) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps, num_training_steps=args.max_train_steps)\n    metric = load_metric('seqeval')\n\n    def get_luke_labels(outputs, ner_tags, original_entity_spans):\n        true_predictions = []\n        true_labels = []\n        for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n            true_tags = [val for val in tags if val != -1]\n            true_original_spans = [val for val in original_spans if val != (-1, -1)]\n            max_indices = torch.argmax(output, axis=1)\n            max_logits = torch.max(output, axis=1).values\n            predictions = []\n            for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n                if index != 0:\n                    predictions.append((logit, span, label_list[index]))\n            predicted_sequence = [label_list[0]] * len(true_tags)\n            for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n                if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                    predicted_sequence[span[0]] = label\n                    if span[1] - span[0] > 1:\n                        predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n            true_predictions.append(predicted_sequence)\n            true_labels.append([label_list[tag_id] for tag_id in true_tags])\n        return (true_predictions, true_labels)\n\n    def compute_metrics():\n        results = metric.compute()\n        if args.return_entity_level_metrics:\n            final_results = {}\n            for (key, value) in results.items():\n                if isinstance(value, dict):\n                    for (n, v) in value.items():\n                        final_results[f'{key}_{n}'] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for (step, batch) in enumerate(train_dataloader):\n            _ = batch.pop('original_entity_spans')\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n            if completed_steps >= args.max_train_steps:\n                break\n        model.eval()\n        for (step, batch) in enumerate(eval_dataloader):\n            original_entity_spans = batch.pop('original_entity_spans')\n            with torch.no_grad():\n                outputs = model(**batch)\n            (preds, refs) = get_luke_labels(outputs, batch[label_column_name], original_entity_spans)\n            metric.add_batch(predictions=preds, references=refs)\n        eval_metric = compute_metrics()\n        accelerator.print(f'epoch {epoch}:', eval_metric)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    args = parse_args()\n    handler = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[handler])\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    if raw_datasets['train'] is not None:\n        column_names = raw_datasets['train'].column_names\n        features = raw_datasets['train'].features\n    else:\n        column_names = raw_datasets['validation'].column_names\n        features = raw_datasets['validation'].features\n    if args.text_column_name is not None:\n        text_column_name = args.text_column_name\n    elif 'tokens' in column_names:\n        text_column_name = 'tokens'\n    else:\n        text_column_name = column_names[0]\n    if args.label_column_name is not None:\n        label_column_name = args.label_column_name\n    elif f'{args.task_name}_tags' in column_names:\n        label_column_name = f'{args.task_name}_tags'\n    else:\n        label_column_name = column_names[1]\n\n    def get_label_list(labels):\n        unique_labels = set()\n        for label in labels:\n            unique_labels = unique_labels | set(label)\n        label_list = list(unique_labels)\n        label_list.sort()\n        return label_list\n    if isinstance(features[label_column_name].feature, ClassLabel):\n        label_list = features[label_column_name].feature.names\n    else:\n        label_list = get_label_list(raw_datasets['train'][label_column_name])\n    num_labels = len(label_list)\n    b_to_i_label = []\n    for (idx, label) in enumerate(label_list):\n        if label.startswith('B-') and label.replace('B-', 'I-') in label_list:\n            b_to_i_label.append(label_list.index(label.replace('B-', 'I-')))\n        else:\n            b_to_i_label.append(idx)\n    if args.config_name:\n        config = LukeConfig.from_pretrained(args.config_name, num_labels=num_labels)\n    elif args.model_name_or_path:\n        config = LukeConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n    else:\n        logger.warning('You are instantiating a new config instance from scratch.')\n    tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n    if not tokenizer_name_or_path:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    tokenizer = LukeTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=False, task='entity_span_classification', max_entity_length=args.max_entity_length, max_mention_length=args.max_mention_length)\n    if args.model_name_or_path:\n        model = LukeForEntitySpanClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n    else:\n        logger.info('Training new model from scratch')\n        model = LukeForEntitySpanClassification.from_config(config)\n    model.resize_token_embeddings(len(tokenizer))\n    padding = 'max_length' if args.pad_to_max_length else False\n\n    def compute_sentence_boundaries_for_luke(examples):\n        sentence_boundaries = []\n        for tokens in examples[text_column_name]:\n            sentence_boundaries.append([0, len(tokens)])\n        examples['sentence_boundaries'] = sentence_boundaries\n        return examples\n\n    def compute_entity_spans_for_luke(examples):\n        all_entity_spans = []\n        texts = []\n        all_labels_entity_spans = []\n        all_original_entity_spans = []\n        for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n            subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n            total_subword_length = sum(subword_lengths)\n            (_, context_end) = sentence_boundaries\n            if total_subword_length > args.max_length - 2:\n                cur_length = sum(subword_lengths[:context_end])\n                idx = context_end - 1\n                while cur_length > args.max_length - 2:\n                    cur_length -= subword_lengths[idx]\n                    context_end -= 1\n                    idx -= 1\n            text = ''\n            sentence_words = tokens[:context_end]\n            sentence_subword_lengths = subword_lengths[:context_end]\n            word_start_char_positions = []\n            word_end_char_positions = []\n            labels_positions = {}\n            for (word, label) in zip(sentence_words, labels):\n                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                    text = text.rstrip()\n                word_start_char_positions.append(len(text))\n                text += word\n                word_end_char_positions.append(len(text))\n                text += ' '\n                labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n            text = text.rstrip()\n            texts.append(text)\n            entity_spans = []\n            labels_entity_spans = []\n            original_entity_spans = []\n            for word_start in range(len(sentence_words)):\n                for word_end in range(word_start, len(sentence_words)):\n                    if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                        entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                        original_entity_spans.append((word_start, word_end + 1))\n                        if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                            labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                        else:\n                            labels_entity_spans.append(0)\n            all_entity_spans.append(entity_spans)\n            all_labels_entity_spans.append(labels_entity_spans)\n            all_original_entity_spans.append(original_entity_spans)\n        examples['entity_spans'] = all_entity_spans\n        examples['text'] = texts\n        examples['labels_entity_spans'] = all_labels_entity_spans\n        examples['original_entity_spans'] = all_original_entity_spans\n        return examples\n\n    def tokenize_and_align_labels(examples):\n        entity_spans = []\n        for v in examples['entity_spans']:\n            entity_spans.append(list(map(tuple, v)))\n        tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n        if padding == 'max_length':\n            tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n        else:\n            tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n            tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n            tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n        return tokenized_inputs\n    with accelerator.main_process_first():\n        raw_datasets = raw_datasets.map(compute_sentence_boundaries_for_luke, batched=True, desc='Adding sentence boundaries')\n        raw_datasets = raw_datasets.map(compute_entity_spans_for_luke, batched=True, desc='Adding sentence spans')\n        processed_raw_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names, desc='Running tokenizer on dataset')\n    train_dataset = processed_raw_datasets['train']\n    eval_dataset = processed_raw_datasets['validation']\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorForLukeTokenClassification(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    device = accelerator.device\n    model.to(device)\n    (model, optimizer, train_dataloader, eval_dataloader) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps, num_training_steps=args.max_train_steps)\n    metric = load_metric('seqeval')\n\n    def get_luke_labels(outputs, ner_tags, original_entity_spans):\n        true_predictions = []\n        true_labels = []\n        for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n            true_tags = [val for val in tags if val != -1]\n            true_original_spans = [val for val in original_spans if val != (-1, -1)]\n            max_indices = torch.argmax(output, axis=1)\n            max_logits = torch.max(output, axis=1).values\n            predictions = []\n            for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n                if index != 0:\n                    predictions.append((logit, span, label_list[index]))\n            predicted_sequence = [label_list[0]] * len(true_tags)\n            for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n                if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                    predicted_sequence[span[0]] = label\n                    if span[1] - span[0] > 1:\n                        predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n            true_predictions.append(predicted_sequence)\n            true_labels.append([label_list[tag_id] for tag_id in true_tags])\n        return (true_predictions, true_labels)\n\n    def compute_metrics():\n        results = metric.compute()\n        if args.return_entity_level_metrics:\n            final_results = {}\n            for (key, value) in results.items():\n                if isinstance(value, dict):\n                    for (n, v) in value.items():\n                        final_results[f'{key}_{n}'] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for (step, batch) in enumerate(train_dataloader):\n            _ = batch.pop('original_entity_spans')\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n            if completed_steps >= args.max_train_steps:\n                break\n        model.eval()\n        for (step, batch) in enumerate(eval_dataloader):\n            original_entity_spans = batch.pop('original_entity_spans')\n            with torch.no_grad():\n                outputs = model(**batch)\n            (preds, refs) = get_luke_labels(outputs, batch[label_column_name], original_entity_spans)\n            metric.add_batch(predictions=preds, references=refs)\n        eval_metric = compute_metrics()\n        accelerator.print(f'epoch {epoch}:', eval_metric)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    args = parse_args()\n    handler = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[handler])\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    if raw_datasets['train'] is not None:\n        column_names = raw_datasets['train'].column_names\n        features = raw_datasets['train'].features\n    else:\n        column_names = raw_datasets['validation'].column_names\n        features = raw_datasets['validation'].features\n    if args.text_column_name is not None:\n        text_column_name = args.text_column_name\n    elif 'tokens' in column_names:\n        text_column_name = 'tokens'\n    else:\n        text_column_name = column_names[0]\n    if args.label_column_name is not None:\n        label_column_name = args.label_column_name\n    elif f'{args.task_name}_tags' in column_names:\n        label_column_name = f'{args.task_name}_tags'\n    else:\n        label_column_name = column_names[1]\n\n    def get_label_list(labels):\n        unique_labels = set()\n        for label in labels:\n            unique_labels = unique_labels | set(label)\n        label_list = list(unique_labels)\n        label_list.sort()\n        return label_list\n    if isinstance(features[label_column_name].feature, ClassLabel):\n        label_list = features[label_column_name].feature.names\n    else:\n        label_list = get_label_list(raw_datasets['train'][label_column_name])\n    num_labels = len(label_list)\n    b_to_i_label = []\n    for (idx, label) in enumerate(label_list):\n        if label.startswith('B-') and label.replace('B-', 'I-') in label_list:\n            b_to_i_label.append(label_list.index(label.replace('B-', 'I-')))\n        else:\n            b_to_i_label.append(idx)\n    if args.config_name:\n        config = LukeConfig.from_pretrained(args.config_name, num_labels=num_labels)\n    elif args.model_name_or_path:\n        config = LukeConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n    else:\n        logger.warning('You are instantiating a new config instance from scratch.')\n    tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n    if not tokenizer_name_or_path:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    tokenizer = LukeTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=False, task='entity_span_classification', max_entity_length=args.max_entity_length, max_mention_length=args.max_mention_length)\n    if args.model_name_or_path:\n        model = LukeForEntitySpanClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n    else:\n        logger.info('Training new model from scratch')\n        model = LukeForEntitySpanClassification.from_config(config)\n    model.resize_token_embeddings(len(tokenizer))\n    padding = 'max_length' if args.pad_to_max_length else False\n\n    def compute_sentence_boundaries_for_luke(examples):\n        sentence_boundaries = []\n        for tokens in examples[text_column_name]:\n            sentence_boundaries.append([0, len(tokens)])\n        examples['sentence_boundaries'] = sentence_boundaries\n        return examples\n\n    def compute_entity_spans_for_luke(examples):\n        all_entity_spans = []\n        texts = []\n        all_labels_entity_spans = []\n        all_original_entity_spans = []\n        for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n            subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n            total_subword_length = sum(subword_lengths)\n            (_, context_end) = sentence_boundaries\n            if total_subword_length > args.max_length - 2:\n                cur_length = sum(subword_lengths[:context_end])\n                idx = context_end - 1\n                while cur_length > args.max_length - 2:\n                    cur_length -= subword_lengths[idx]\n                    context_end -= 1\n                    idx -= 1\n            text = ''\n            sentence_words = tokens[:context_end]\n            sentence_subword_lengths = subword_lengths[:context_end]\n            word_start_char_positions = []\n            word_end_char_positions = []\n            labels_positions = {}\n            for (word, label) in zip(sentence_words, labels):\n                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                    text = text.rstrip()\n                word_start_char_positions.append(len(text))\n                text += word\n                word_end_char_positions.append(len(text))\n                text += ' '\n                labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n            text = text.rstrip()\n            texts.append(text)\n            entity_spans = []\n            labels_entity_spans = []\n            original_entity_spans = []\n            for word_start in range(len(sentence_words)):\n                for word_end in range(word_start, len(sentence_words)):\n                    if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                        entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                        original_entity_spans.append((word_start, word_end + 1))\n                        if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                            labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                        else:\n                            labels_entity_spans.append(0)\n            all_entity_spans.append(entity_spans)\n            all_labels_entity_spans.append(labels_entity_spans)\n            all_original_entity_spans.append(original_entity_spans)\n        examples['entity_spans'] = all_entity_spans\n        examples['text'] = texts\n        examples['labels_entity_spans'] = all_labels_entity_spans\n        examples['original_entity_spans'] = all_original_entity_spans\n        return examples\n\n    def tokenize_and_align_labels(examples):\n        entity_spans = []\n        for v in examples['entity_spans']:\n            entity_spans.append(list(map(tuple, v)))\n        tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n        if padding == 'max_length':\n            tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n        else:\n            tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n            tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n            tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n        return tokenized_inputs\n    with accelerator.main_process_first():\n        raw_datasets = raw_datasets.map(compute_sentence_boundaries_for_luke, batched=True, desc='Adding sentence boundaries')\n        raw_datasets = raw_datasets.map(compute_entity_spans_for_luke, batched=True, desc='Adding sentence spans')\n        processed_raw_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names, desc='Running tokenizer on dataset')\n    train_dataset = processed_raw_datasets['train']\n    eval_dataset = processed_raw_datasets['validation']\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorForLukeTokenClassification(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    device = accelerator.device\n    model.to(device)\n    (model, optimizer, train_dataloader, eval_dataloader) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps, num_training_steps=args.max_train_steps)\n    metric = load_metric('seqeval')\n\n    def get_luke_labels(outputs, ner_tags, original_entity_spans):\n        true_predictions = []\n        true_labels = []\n        for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n            true_tags = [val for val in tags if val != -1]\n            true_original_spans = [val for val in original_spans if val != (-1, -1)]\n            max_indices = torch.argmax(output, axis=1)\n            max_logits = torch.max(output, axis=1).values\n            predictions = []\n            for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n                if index != 0:\n                    predictions.append((logit, span, label_list[index]))\n            predicted_sequence = [label_list[0]] * len(true_tags)\n            for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n                if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                    predicted_sequence[span[0]] = label\n                    if span[1] - span[0] > 1:\n                        predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n            true_predictions.append(predicted_sequence)\n            true_labels.append([label_list[tag_id] for tag_id in true_tags])\n        return (true_predictions, true_labels)\n\n    def compute_metrics():\n        results = metric.compute()\n        if args.return_entity_level_metrics:\n            final_results = {}\n            for (key, value) in results.items():\n                if isinstance(value, dict):\n                    for (n, v) in value.items():\n                        final_results[f'{key}_{n}'] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for (step, batch) in enumerate(train_dataloader):\n            _ = batch.pop('original_entity_spans')\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n            if completed_steps >= args.max_train_steps:\n                break\n        model.eval()\n        for (step, batch) in enumerate(eval_dataloader):\n            original_entity_spans = batch.pop('original_entity_spans')\n            with torch.no_grad():\n                outputs = model(**batch)\n            (preds, refs) = get_luke_labels(outputs, batch[label_column_name], original_entity_spans)\n            metric.add_batch(predictions=preds, references=refs)\n        eval_metric = compute_metrics()\n        accelerator.print(f'epoch {epoch}:', eval_metric)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    args = parse_args()\n    handler = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(kwargs_handlers=[handler])\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files)\n    if args.debug:\n        for split in raw_datasets.keys():\n            raw_datasets[split] = raw_datasets[split].select(range(100))\n    if raw_datasets['train'] is not None:\n        column_names = raw_datasets['train'].column_names\n        features = raw_datasets['train'].features\n    else:\n        column_names = raw_datasets['validation'].column_names\n        features = raw_datasets['validation'].features\n    if args.text_column_name is not None:\n        text_column_name = args.text_column_name\n    elif 'tokens' in column_names:\n        text_column_name = 'tokens'\n    else:\n        text_column_name = column_names[0]\n    if args.label_column_name is not None:\n        label_column_name = args.label_column_name\n    elif f'{args.task_name}_tags' in column_names:\n        label_column_name = f'{args.task_name}_tags'\n    else:\n        label_column_name = column_names[1]\n\n    def get_label_list(labels):\n        unique_labels = set()\n        for label in labels:\n            unique_labels = unique_labels | set(label)\n        label_list = list(unique_labels)\n        label_list.sort()\n        return label_list\n    if isinstance(features[label_column_name].feature, ClassLabel):\n        label_list = features[label_column_name].feature.names\n    else:\n        label_list = get_label_list(raw_datasets['train'][label_column_name])\n    num_labels = len(label_list)\n    b_to_i_label = []\n    for (idx, label) in enumerate(label_list):\n        if label.startswith('B-') and label.replace('B-', 'I-') in label_list:\n            b_to_i_label.append(label_list.index(label.replace('B-', 'I-')))\n        else:\n            b_to_i_label.append(idx)\n    if args.config_name:\n        config = LukeConfig.from_pretrained(args.config_name, num_labels=num_labels)\n    elif args.model_name_or_path:\n        config = LukeConfig.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n    else:\n        logger.warning('You are instantiating a new config instance from scratch.')\n    tokenizer_name_or_path = args.tokenizer_name if args.tokenizer_name else args.model_name_or_path\n    if not tokenizer_name_or_path:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    tokenizer = LukeTokenizer.from_pretrained(tokenizer_name_or_path, use_fast=False, task='entity_span_classification', max_entity_length=args.max_entity_length, max_mention_length=args.max_mention_length)\n    if args.model_name_or_path:\n        model = LukeForEntitySpanClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n    else:\n        logger.info('Training new model from scratch')\n        model = LukeForEntitySpanClassification.from_config(config)\n    model.resize_token_embeddings(len(tokenizer))\n    padding = 'max_length' if args.pad_to_max_length else False\n\n    def compute_sentence_boundaries_for_luke(examples):\n        sentence_boundaries = []\n        for tokens in examples[text_column_name]:\n            sentence_boundaries.append([0, len(tokens)])\n        examples['sentence_boundaries'] = sentence_boundaries\n        return examples\n\n    def compute_entity_spans_for_luke(examples):\n        all_entity_spans = []\n        texts = []\n        all_labels_entity_spans = []\n        all_original_entity_spans = []\n        for (labels, tokens, sentence_boundaries) in zip(examples[label_column_name], examples[text_column_name], examples['sentence_boundaries']):\n            subword_lengths = [len(tokenizer.tokenize(token)) for token in tokens]\n            total_subword_length = sum(subword_lengths)\n            (_, context_end) = sentence_boundaries\n            if total_subword_length > args.max_length - 2:\n                cur_length = sum(subword_lengths[:context_end])\n                idx = context_end - 1\n                while cur_length > args.max_length - 2:\n                    cur_length -= subword_lengths[idx]\n                    context_end -= 1\n                    idx -= 1\n            text = ''\n            sentence_words = tokens[:context_end]\n            sentence_subword_lengths = subword_lengths[:context_end]\n            word_start_char_positions = []\n            word_end_char_positions = []\n            labels_positions = {}\n            for (word, label) in zip(sentence_words, labels):\n                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n                    text = text.rstrip()\n                word_start_char_positions.append(len(text))\n                text += word\n                word_end_char_positions.append(len(text))\n                text += ' '\n                labels_positions[word_start_char_positions[-1], word_end_char_positions[-1]] = label\n            text = text.rstrip()\n            texts.append(text)\n            entity_spans = []\n            labels_entity_spans = []\n            original_entity_spans = []\n            for word_start in range(len(sentence_words)):\n                for word_end in range(word_start, len(sentence_words)):\n                    if sum(sentence_subword_lengths[word_start:word_end]) <= tokenizer.max_mention_length and len(entity_spans) < tokenizer.max_entity_length:\n                        entity_spans.append((word_start_char_positions[word_start], word_end_char_positions[word_end]))\n                        original_entity_spans.append((word_start, word_end + 1))\n                        if (word_start_char_positions[word_start], word_end_char_positions[word_end]) in labels_positions:\n                            labels_entity_spans.append(labels_positions[word_start_char_positions[word_start], word_end_char_positions[word_end]])\n                        else:\n                            labels_entity_spans.append(0)\n            all_entity_spans.append(entity_spans)\n            all_labels_entity_spans.append(labels_entity_spans)\n            all_original_entity_spans.append(original_entity_spans)\n        examples['entity_spans'] = all_entity_spans\n        examples['text'] = texts\n        examples['labels_entity_spans'] = all_labels_entity_spans\n        examples['original_entity_spans'] = all_original_entity_spans\n        return examples\n\n    def tokenize_and_align_labels(examples):\n        entity_spans = []\n        for v in examples['entity_spans']:\n            entity_spans.append(list(map(tuple, v)))\n        tokenized_inputs = tokenizer(examples['text'], entity_spans=entity_spans, max_length=args.max_length, padding=padding, truncation=True)\n        if padding == 'max_length':\n            tokenized_inputs['labels'] = padding_tensor(examples['labels_entity_spans'], -100, tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs['original_entity_spans'] = padding_tensor(examples['original_entity_spans'], (-1, -1), tokenizer.padding_side, tokenizer.max_entity_length)\n            tokenized_inputs[label_column_name] = padding_tensor(examples[label_column_name], -1, tokenizer.padding_side, tokenizer.max_entity_length)\n        else:\n            tokenized_inputs['labels'] = [ex[:tokenizer.max_entity_length] for ex in examples['labels_entity_spans']]\n            tokenized_inputs['original_entity_spans'] = [ex[:tokenizer.max_entity_length] for ex in examples['original_entity_spans']]\n            tokenized_inputs[label_column_name] = [ex[:tokenizer.max_entity_length] for ex in examples[label_column_name]]\n        return tokenized_inputs\n    with accelerator.main_process_first():\n        raw_datasets = raw_datasets.map(compute_sentence_boundaries_for_luke, batched=True, desc='Adding sentence boundaries')\n        raw_datasets = raw_datasets.map(compute_entity_spans_for_luke, batched=True, desc='Adding sentence spans')\n        processed_raw_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names, desc='Running tokenizer on dataset')\n    train_dataset = processed_raw_datasets['train']\n    eval_dataset = processed_raw_datasets['validation']\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorForLukeTokenClassification(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    device = accelerator.device\n    model.to(device)\n    (model, optimizer, train_dataloader, eval_dataloader) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    else:\n        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps, num_training_steps=args.max_train_steps)\n    metric = load_metric('seqeval')\n\n    def get_luke_labels(outputs, ner_tags, original_entity_spans):\n        true_predictions = []\n        true_labels = []\n        for (output, original_spans, tags) in zip(outputs.logits, original_entity_spans, ner_tags):\n            true_tags = [val for val in tags if val != -1]\n            true_original_spans = [val for val in original_spans if val != (-1, -1)]\n            max_indices = torch.argmax(output, axis=1)\n            max_logits = torch.max(output, axis=1).values\n            predictions = []\n            for (logit, index, span) in zip(max_logits, max_indices, true_original_spans):\n                if index != 0:\n                    predictions.append((logit, span, label_list[index]))\n            predicted_sequence = [label_list[0]] * len(true_tags)\n            for (_, span, label) in sorted(predictions, key=lambda o: o[0], reverse=True):\n                if all((o == label_list[0] for o in predicted_sequence[span[0]:span[1]])):\n                    predicted_sequence[span[0]] = label\n                    if span[1] - span[0] > 1:\n                        predicted_sequence[span[0] + 1:span[1]] = [label] * (span[1] - span[0] - 1)\n            true_predictions.append(predicted_sequence)\n            true_labels.append([label_list[tag_id] for tag_id in true_tags])\n        return (true_predictions, true_labels)\n\n    def compute_metrics():\n        results = metric.compute()\n        if args.return_entity_level_metrics:\n            final_results = {}\n            for (key, value) in results.items():\n                if isinstance(value, dict):\n                    for (n, v) in value.items():\n                        final_results[f'{key}_{n}'] = v\n                else:\n                    final_results[key] = value\n            return final_results\n        else:\n            return {'precision': results['overall_precision'], 'recall': results['overall_recall'], 'f1': results['overall_f1'], 'accuracy': results['overall_accuracy']}\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    for epoch in range(args.num_train_epochs):\n        model.train()\n        for (step, batch) in enumerate(train_dataloader):\n            _ = batch.pop('original_entity_spans')\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / args.gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n                completed_steps += 1\n            if completed_steps >= args.max_train_steps:\n                break\n        model.eval()\n        for (step, batch) in enumerate(eval_dataloader):\n            original_entity_spans = batch.pop('original_entity_spans')\n            with torch.no_grad():\n                outputs = model(**batch)\n            (preds, refs) = get_luke_labels(outputs, batch[label_column_name], original_entity_spans)\n            metric.add_batch(predictions=preds, references=refs)\n        eval_metric = compute_metrics()\n        accelerator.print(f'epoch {epoch}:', eval_metric)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)"
        ]
    }
]