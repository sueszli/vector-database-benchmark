from __future__ import annotations
from collections.abc import Callable
from typing import Any
from optuna._imports import try_import
from optuna.integration._lightgbm_tuner.optimize import _imports
from optuna.integration._lightgbm_tuner.optimize import LightGBMTuner
from optuna.study import Study
from optuna.trial import FrozenTrial
with try_import():
    import lightgbm as lgb

def train(params: dict[str, Any], train_set: 'lgb.Dataset', num_boost_round: int=1000, valid_sets: list['lgb.Dataset'] | tuple['lgb.Dataset', ...] | 'lgb.Dataset' | None=None, valid_names: Any | None=None, feval: Callable[..., Any] | None=None, feature_name: str='auto', categorical_feature: str='auto', keep_training_booster: bool=False, callbacks: list[Callable[..., Any]] | None=None, time_budget: int | None=None, sample_size: int | None=None, study: Study | None=None, optuna_callbacks: list[Callable[[Study, FrozenTrial], None]] | None=None, model_dir: str | None=None, verbosity: int | None=None, show_progress_bar: bool=True, *, optuna_seed: int | None=None) -> 'lgb.Booster':
    if False:
        return 10
    "Wrapper of LightGBM Training API to tune hyperparameters.\n\n    It optimizes the following hyperparameters in a stepwise manner:\n    ``lambda_l1``, ``lambda_l2``, ``num_leaves``, ``feature_fraction``, ``bagging_fraction``,\n    ``bagging_freq`` and ``min_child_samples``.\n    It is a drop-in replacement for `lightgbm.train()`_. See\n    `a simple example of LightGBM Tuner <https://github.com/optuna/optuna-examples/tree/main/\n    lightgbm/lightgbm_tuner_simple.py>`_ which optimizes the validation log loss of cancer\n    detection.\n\n    :func:`~optuna.integration.lightgbm.train` is a wrapper function of\n    :class:`~optuna.integration.lightgbm.LightGBMTuner`. To use feature in Optuna such as\n    suspended/resumed optimization and/or parallelization, refer to\n    :class:`~optuna.integration.lightgbm.LightGBMTuner` instead of this function.\n\n    .. note::\n        Arguments and keyword arguments for `lightgbm.train()`_ can be passed.\n        For ``params``, please check `the official documentation for LightGBM\n        <https://lightgbm.readthedocs.io/en/latest/Parameters.html>`_.\n\n    Args:\n        time_budget:\n            A time budget for parameter tuning in seconds.\n\n        study:\n            A :class:`~optuna.study.Study` instance to store optimization results. The\n            :class:`~optuna.trial.Trial` instances in it has the following user attributes:\n            ``elapsed_secs`` is the elapsed time since the optimization starts.\n            ``average_iteration_time`` is the average time of iteration to train the booster\n            model in the trial. ``lgbm_params`` is a JSON-serialized dictionary of LightGBM\n            parameters used in the trial.\n\n        optuna_callbacks:\n            List of Optuna callback functions that are invoked at the end of each trial.\n            Each function must accept two parameters with the following types in this order:\n            :class:`~optuna.study.Study` and :class:`~optuna.trial.FrozenTrial`.\n            Please note that this is not a ``callbacks`` argument of `lightgbm.train()`_ .\n\n        model_dir:\n            A directory to save boosters. By default, it is set to :obj:`None` and no boosters are\n            saved. Please set shared directory (e.g., directories on NFS) if you want to access\n            :meth:`~optuna.integration.lightgbm.LightGBMTuner.get_best_booster` in distributed\n            environments. Otherwise, it may raise :obj:`ValueError`. If the directory does not\n            exist, it will be created. The filenames of the boosters will be\n            ``{model_dir}/{trial_number}.pkl`` (e.g., ``./boosters/0.pkl``).\n\n        verbosity:\n            A verbosity level to change Optuna's logging level. The level is aligned to\n            `LightGBM's verbosity`_ .\n\n            .. warning::\n                Deprecated in v2.0.0. ``verbosity`` argument will be removed in the future.\n                The removal of this feature is currently scheduled for v4.0.0,\n                but this schedule is subject to change.\n\n                Please use :func:`~optuna.logging.set_verbosity` instead.\n\n        show_progress_bar:\n            Flag to show progress bars or not. To disable progress bar, set this :obj:`False`.\n\n            .. note::\n                Progress bars will be fragmented by logging messages of LightGBM and Optuna.\n                Please suppress such messages to show the progress bars properly.\n\n        optuna_seed:\n            ``seed`` of :class:`~optuna.samplers.TPESampler` for random number generator\n            that affects sampling for ``num_leaves``, ``bagging_fraction``, ``bagging_freq``,\n            ``lambda_l1``, and ``lambda_l2``.\n\n            .. note::\n                The `deterministic`_ parameter of LightGBM makes training reproducible.\n                Please enable it when you use this argument.\n\n    .. _lightgbm.train(): https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html\n    .. _LightGBM's verbosity: https://lightgbm.readthedocs.io/en/latest/Parameters.html#verbosity\n    .. _deterministic: https://lightgbm.readthedocs.io/en/latest/Parameters.html#deterministic\n    "
    _imports.check()
    auto_booster = LightGBMTuner(params=params, train_set=train_set, num_boost_round=num_boost_round, valid_sets=valid_sets, valid_names=valid_names, feval=feval, feature_name=feature_name, categorical_feature=categorical_feature, keep_training_booster=keep_training_booster, callbacks=callbacks, time_budget=time_budget, sample_size=sample_size, study=study, optuna_callbacks=optuna_callbacks, model_dir=model_dir, verbosity=verbosity, show_progress_bar=show_progress_bar, optuna_seed=optuna_seed)
    auto_booster.run()
    return auto_booster.get_best_booster()