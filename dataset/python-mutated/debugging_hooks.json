[
    {
        "func_name": "noop_hook",
        "original": "def noop_hook(_: Any, bucket: GradBucket) -> torch.futures.Future[torch.Tensor]:\n    \"\"\"\n    This DDP communication hook returns a future that wraps the input,\n    so it is a noop that does not incur any communication overheads.\n\n    This hook should **only** be used for headroom analysis of allreduce optimization,\n    instead of the normal gradient synchronization.\n    For example, if only less than 10% speedup of training time can be observed after this hook is registered,\n    it usually implies that allreduce is not a performance bottleneck for this case.\n    Such instrumentation can be particularly useful\n    if GPU traces cannot be easily retrieved or the trace analysis is complicated\n    some factors such as the overlap between allreduce and computation or the desynchronization across ranks.\n\n    Example::\n        >>> # xdoctest: +SKIP\n        >>> ddp_model.register_comm_hook(None, noop_hook)\n    \"\"\"\n    fut: torch.futures.Future[torch.Tensor] = torch.futures.Future()\n    fut.set_result(bucket.buffer())\n    return fut",
        "mutated": [
            "def noop_hook(_: Any, bucket: GradBucket) -> torch.futures.Future[torch.Tensor]:\n    if False:\n        i = 10\n    '\\n    This DDP communication hook returns a future that wraps the input,\\n    so it is a noop that does not incur any communication overheads.\\n\\n    This hook should **only** be used for headroom analysis of allreduce optimization,\\n    instead of the normal gradient synchronization.\\n    For example, if only less than 10% speedup of training time can be observed after this hook is registered,\\n    it usually implies that allreduce is not a performance bottleneck for this case.\\n    Such instrumentation can be particularly useful\\n    if GPU traces cannot be easily retrieved or the trace analysis is complicated\\n    some factors such as the overlap between allreduce and computation or the desynchronization across ranks.\\n\\n    Example::\\n        >>> # xdoctest: +SKIP\\n        >>> ddp_model.register_comm_hook(None, noop_hook)\\n    '\n    fut: torch.futures.Future[torch.Tensor] = torch.futures.Future()\n    fut.set_result(bucket.buffer())\n    return fut",
            "def noop_hook(_: Any, bucket: GradBucket) -> torch.futures.Future[torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This DDP communication hook returns a future that wraps the input,\\n    so it is a noop that does not incur any communication overheads.\\n\\n    This hook should **only** be used for headroom analysis of allreduce optimization,\\n    instead of the normal gradient synchronization.\\n    For example, if only less than 10% speedup of training time can be observed after this hook is registered,\\n    it usually implies that allreduce is not a performance bottleneck for this case.\\n    Such instrumentation can be particularly useful\\n    if GPU traces cannot be easily retrieved or the trace analysis is complicated\\n    some factors such as the overlap between allreduce and computation or the desynchronization across ranks.\\n\\n    Example::\\n        >>> # xdoctest: +SKIP\\n        >>> ddp_model.register_comm_hook(None, noop_hook)\\n    '\n    fut: torch.futures.Future[torch.Tensor] = torch.futures.Future()\n    fut.set_result(bucket.buffer())\n    return fut",
            "def noop_hook(_: Any, bucket: GradBucket) -> torch.futures.Future[torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This DDP communication hook returns a future that wraps the input,\\n    so it is a noop that does not incur any communication overheads.\\n\\n    This hook should **only** be used for headroom analysis of allreduce optimization,\\n    instead of the normal gradient synchronization.\\n    For example, if only less than 10% speedup of training time can be observed after this hook is registered,\\n    it usually implies that allreduce is not a performance bottleneck for this case.\\n    Such instrumentation can be particularly useful\\n    if GPU traces cannot be easily retrieved or the trace analysis is complicated\\n    some factors such as the overlap between allreduce and computation or the desynchronization across ranks.\\n\\n    Example::\\n        >>> # xdoctest: +SKIP\\n        >>> ddp_model.register_comm_hook(None, noop_hook)\\n    '\n    fut: torch.futures.Future[torch.Tensor] = torch.futures.Future()\n    fut.set_result(bucket.buffer())\n    return fut",
            "def noop_hook(_: Any, bucket: GradBucket) -> torch.futures.Future[torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This DDP communication hook returns a future that wraps the input,\\n    so it is a noop that does not incur any communication overheads.\\n\\n    This hook should **only** be used for headroom analysis of allreduce optimization,\\n    instead of the normal gradient synchronization.\\n    For example, if only less than 10% speedup of training time can be observed after this hook is registered,\\n    it usually implies that allreduce is not a performance bottleneck for this case.\\n    Such instrumentation can be particularly useful\\n    if GPU traces cannot be easily retrieved or the trace analysis is complicated\\n    some factors such as the overlap between allreduce and computation or the desynchronization across ranks.\\n\\n    Example::\\n        >>> # xdoctest: +SKIP\\n        >>> ddp_model.register_comm_hook(None, noop_hook)\\n    '\n    fut: torch.futures.Future[torch.Tensor] = torch.futures.Future()\n    fut.set_result(bucket.buffer())\n    return fut",
            "def noop_hook(_: Any, bucket: GradBucket) -> torch.futures.Future[torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This DDP communication hook returns a future that wraps the input,\\n    so it is a noop that does not incur any communication overheads.\\n\\n    This hook should **only** be used for headroom analysis of allreduce optimization,\\n    instead of the normal gradient synchronization.\\n    For example, if only less than 10% speedup of training time can be observed after this hook is registered,\\n    it usually implies that allreduce is not a performance bottleneck for this case.\\n    Such instrumentation can be particularly useful\\n    if GPU traces cannot be easily retrieved or the trace analysis is complicated\\n    some factors such as the overlap between allreduce and computation or the desynchronization across ranks.\\n\\n    Example::\\n        >>> # xdoctest: +SKIP\\n        >>> ddp_model.register_comm_hook(None, noop_hook)\\n    '\n    fut: torch.futures.Future[torch.Tensor] = torch.futures.Future()\n    fut.set_result(bucket.buffer())\n    return fut"
        ]
    }
]