[
    {
        "func_name": "vtrace_nstep_return",
        "original": "def vtrace_nstep_return(clipped_rhos, clipped_cs, reward, bootstrap_values, gamma=0.99, lambda_=0.95):\n    \"\"\"\n    Overview:\n        Computation of vtrace return.\n    Returns:\n        - vtrace_return (:obj:`torch.FloatTensor`): the vtrace loss item, all of them are differentiable 0-dim tensor\n    Shapes:\n        - clipped_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\n        - clipped_cs (:obj:`torch.FloatTensor`): :math:`(T, B)`\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\n        - vtrace_return (:obj:`torch.FloatTensor`):  :math:`(T, B)`\n    \"\"\"\n    deltas = clipped_rhos * (reward + gamma * bootstrap_values[1:] - bootstrap_values[:-1])\n    factor = gamma * lambda_\n    result = bootstrap_values[:-1].clone()\n    vtrace_item = 0.0\n    for t in reversed(range(reward.size()[0])):\n        vtrace_item = deltas[t] + factor * clipped_cs[t] * vtrace_item\n        result[t] += vtrace_item\n    return result",
        "mutated": [
            "def vtrace_nstep_return(clipped_rhos, clipped_cs, reward, bootstrap_values, gamma=0.99, lambda_=0.95):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Computation of vtrace return.\\n    Returns:\\n        - vtrace_return (:obj:`torch.FloatTensor`): the vtrace loss item, all of them are differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - clipped_cs (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - vtrace_return (:obj:`torch.FloatTensor`):  :math:`(T, B)`\\n    '\n    deltas = clipped_rhos * (reward + gamma * bootstrap_values[1:] - bootstrap_values[:-1])\n    factor = gamma * lambda_\n    result = bootstrap_values[:-1].clone()\n    vtrace_item = 0.0\n    for t in reversed(range(reward.size()[0])):\n        vtrace_item = deltas[t] + factor * clipped_cs[t] * vtrace_item\n        result[t] += vtrace_item\n    return result",
            "def vtrace_nstep_return(clipped_rhos, clipped_cs, reward, bootstrap_values, gamma=0.99, lambda_=0.95):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Computation of vtrace return.\\n    Returns:\\n        - vtrace_return (:obj:`torch.FloatTensor`): the vtrace loss item, all of them are differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - clipped_cs (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - vtrace_return (:obj:`torch.FloatTensor`):  :math:`(T, B)`\\n    '\n    deltas = clipped_rhos * (reward + gamma * bootstrap_values[1:] - bootstrap_values[:-1])\n    factor = gamma * lambda_\n    result = bootstrap_values[:-1].clone()\n    vtrace_item = 0.0\n    for t in reversed(range(reward.size()[0])):\n        vtrace_item = deltas[t] + factor * clipped_cs[t] * vtrace_item\n        result[t] += vtrace_item\n    return result",
            "def vtrace_nstep_return(clipped_rhos, clipped_cs, reward, bootstrap_values, gamma=0.99, lambda_=0.95):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Computation of vtrace return.\\n    Returns:\\n        - vtrace_return (:obj:`torch.FloatTensor`): the vtrace loss item, all of them are differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - clipped_cs (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - vtrace_return (:obj:`torch.FloatTensor`):  :math:`(T, B)`\\n    '\n    deltas = clipped_rhos * (reward + gamma * bootstrap_values[1:] - bootstrap_values[:-1])\n    factor = gamma * lambda_\n    result = bootstrap_values[:-1].clone()\n    vtrace_item = 0.0\n    for t in reversed(range(reward.size()[0])):\n        vtrace_item = deltas[t] + factor * clipped_cs[t] * vtrace_item\n        result[t] += vtrace_item\n    return result",
            "def vtrace_nstep_return(clipped_rhos, clipped_cs, reward, bootstrap_values, gamma=0.99, lambda_=0.95):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Computation of vtrace return.\\n    Returns:\\n        - vtrace_return (:obj:`torch.FloatTensor`): the vtrace loss item, all of them are differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - clipped_cs (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - vtrace_return (:obj:`torch.FloatTensor`):  :math:`(T, B)`\\n    '\n    deltas = clipped_rhos * (reward + gamma * bootstrap_values[1:] - bootstrap_values[:-1])\n    factor = gamma * lambda_\n    result = bootstrap_values[:-1].clone()\n    vtrace_item = 0.0\n    for t in reversed(range(reward.size()[0])):\n        vtrace_item = deltas[t] + factor * clipped_cs[t] * vtrace_item\n        result[t] += vtrace_item\n    return result",
            "def vtrace_nstep_return(clipped_rhos, clipped_cs, reward, bootstrap_values, gamma=0.99, lambda_=0.95):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Computation of vtrace return.\\n    Returns:\\n        - vtrace_return (:obj:`torch.FloatTensor`): the vtrace loss item, all of them are differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - clipped_cs (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - vtrace_return (:obj:`torch.FloatTensor`):  :math:`(T, B)`\\n    '\n    deltas = clipped_rhos * (reward + gamma * bootstrap_values[1:] - bootstrap_values[:-1])\n    factor = gamma * lambda_\n    result = bootstrap_values[:-1].clone()\n    vtrace_item = 0.0\n    for t in reversed(range(reward.size()[0])):\n        vtrace_item = deltas[t] + factor * clipped_cs[t] * vtrace_item\n        result[t] += vtrace_item\n    return result"
        ]
    },
    {
        "func_name": "vtrace_advantage",
        "original": "def vtrace_advantage(clipped_pg_rhos, reward, return_, bootstrap_values, gamma):\n    \"\"\"\n    Overview:\n        Computation of vtrace advantage.\n    Returns:\n        - vtrace_advantage (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\n    Shapes:\n        - clipped_pg_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\n        - return (:obj:`torch.FloatTensor`): :math:`(T, B)`\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T, B)`\n        - vtrace_advantage (:obj:`torch.FloatTensor`): :math:`(T, B)`\n    \"\"\"\n    return clipped_pg_rhos * (reward + gamma * return_ - bootstrap_values)",
        "mutated": [
            "def vtrace_advantage(clipped_pg_rhos, reward, return_, bootstrap_values, gamma):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Computation of vtrace advantage.\\n    Returns:\\n        - vtrace_advantage (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_pg_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - return (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - vtrace_advantage (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    '\n    return clipped_pg_rhos * (reward + gamma * return_ - bootstrap_values)",
            "def vtrace_advantage(clipped_pg_rhos, reward, return_, bootstrap_values, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Computation of vtrace advantage.\\n    Returns:\\n        - vtrace_advantage (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_pg_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - return (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - vtrace_advantage (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    '\n    return clipped_pg_rhos * (reward + gamma * return_ - bootstrap_values)",
            "def vtrace_advantage(clipped_pg_rhos, reward, return_, bootstrap_values, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Computation of vtrace advantage.\\n    Returns:\\n        - vtrace_advantage (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_pg_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - return (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - vtrace_advantage (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    '\n    return clipped_pg_rhos * (reward + gamma * return_ - bootstrap_values)",
            "def vtrace_advantage(clipped_pg_rhos, reward, return_, bootstrap_values, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Computation of vtrace advantage.\\n    Returns:\\n        - vtrace_advantage (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_pg_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - return (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - vtrace_advantage (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    '\n    return clipped_pg_rhos * (reward + gamma * return_ - bootstrap_values)",
            "def vtrace_advantage(clipped_pg_rhos, reward, return_, bootstrap_values, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Computation of vtrace advantage.\\n    Returns:\\n        - vtrace_advantage (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - clipped_pg_rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep, B is batch size\\n        - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - return (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - bootstrap_values (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n        - vtrace_advantage (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    '\n    return clipped_pg_rhos * (reward + gamma * return_ - bootstrap_values)"
        ]
    },
    {
        "func_name": "shape_fn_vtrace_discrete_action",
        "original": "def shape_fn_vtrace_discrete_action(args, kwargs):\n    \"\"\"\n    Overview:\n        Return shape of vtrace for hpc\n    Returns:\n        shape: [T, B, N]\n    \"\"\"\n    if len(args) <= 0:\n        tmp = kwargs['data'].target_output.shape\n    else:\n        tmp = args[0].target_output.shape\n    return tmp",
        "mutated": [
            "def shape_fn_vtrace_discrete_action(args, kwargs):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Return shape of vtrace for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].target_output.shape\n    else:\n        tmp = args[0].target_output.shape\n    return tmp",
            "def shape_fn_vtrace_discrete_action(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Return shape of vtrace for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].target_output.shape\n    else:\n        tmp = args[0].target_output.shape\n    return tmp",
            "def shape_fn_vtrace_discrete_action(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Return shape of vtrace for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].target_output.shape\n    else:\n        tmp = args[0].target_output.shape\n    return tmp",
            "def shape_fn_vtrace_discrete_action(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Return shape of vtrace for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].target_output.shape\n    else:\n        tmp = args[0].target_output.shape\n    return tmp",
            "def shape_fn_vtrace_discrete_action(args, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Return shape of vtrace for hpc\\n    Returns:\\n        shape: [T, B, N]\\n    '\n    if len(args) <= 0:\n        tmp = kwargs['data'].target_output.shape\n    else:\n        tmp = args[0].target_output.shape\n    return tmp"
        ]
    },
    {
        "func_name": "vtrace_error_discrete_action",
        "original": "@hpc_wrapper(shape_fn=shape_fn_vtrace_discrete_action, namedtuple_data=True, include_args=[0, 1, 2, 3, 4, 5], include_kwargs=['data', 'gamma', 'lambda_', 'rho_clip_ratio', 'c_clip_ratio', 'rho_pg_clip_ratio'])\ndef vtrace_error_discrete_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    \"\"\"\n    Overview:\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\n    Arguments:\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\n            - target_output (:obj:`torch.Tensor`): the output taking the action by the current policy network,                usually this output is network output logit\n            - behaviour_output (:obj:`torch.Tensor`): the output taking the action by the behaviour policy network,                usually this output is network output logit, which is used to produce the trajectory(collector)\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                i.e.: behaviour_action\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\n    Returns:\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\n    Shapes:\n        - target_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`, where T is timestep, B is batch size and            N is action dim\n        - behaviour_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\n    Examples:\n        >>> T, B, N = 4, 8, 16\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\n        >>> reward = torch.rand(T, B)\n        >>> target_output = torch.randn(T, B, N).requires_grad_(True)\n        >>> behaviour_output = torch.randn(T, B, N)\n        >>> action = torch.randint(0, N, size=(T, B))\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\n        >>> loss = vtrace_error_discrete_action(data, rho_clip_ratio=1.1)\n    \"\"\"\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'discrete')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Categorical(logits=target_output)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
        "mutated": [
            "@hpc_wrapper(shape_fn=shape_fn_vtrace_discrete_action, namedtuple_data=True, include_args=[0, 1, 2, 3, 4, 5], include_kwargs=['data', 'gamma', 'lambda_', 'rho_clip_ratio', 'c_clip_ratio', 'rho_pg_clip_ratio'])\ndef vtrace_error_discrete_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`torch.Tensor`): the output taking the action by the current policy network,                usually this output is network output logit\\n            - behaviour_output (:obj:`torch.Tensor`): the output taking the action by the behaviour policy network,                usually this output is network output logit, which is used to produce the trajectory(collector)\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`, where T is timestep, B is batch size and            N is action dim\\n        - behaviour_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = torch.randn(T, B, N).requires_grad_(True)\\n        >>> behaviour_output = torch.randn(T, B, N)\\n        >>> action = torch.randint(0, N, size=(T, B))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_discrete_action(data, rho_clip_ratio=1.1)\\n    '\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'discrete')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Categorical(logits=target_output)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
            "@hpc_wrapper(shape_fn=shape_fn_vtrace_discrete_action, namedtuple_data=True, include_args=[0, 1, 2, 3, 4, 5], include_kwargs=['data', 'gamma', 'lambda_', 'rho_clip_ratio', 'c_clip_ratio', 'rho_pg_clip_ratio'])\ndef vtrace_error_discrete_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`torch.Tensor`): the output taking the action by the current policy network,                usually this output is network output logit\\n            - behaviour_output (:obj:`torch.Tensor`): the output taking the action by the behaviour policy network,                usually this output is network output logit, which is used to produce the trajectory(collector)\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`, where T is timestep, B is batch size and            N is action dim\\n        - behaviour_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = torch.randn(T, B, N).requires_grad_(True)\\n        >>> behaviour_output = torch.randn(T, B, N)\\n        >>> action = torch.randint(0, N, size=(T, B))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_discrete_action(data, rho_clip_ratio=1.1)\\n    '\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'discrete')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Categorical(logits=target_output)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
            "@hpc_wrapper(shape_fn=shape_fn_vtrace_discrete_action, namedtuple_data=True, include_args=[0, 1, 2, 3, 4, 5], include_kwargs=['data', 'gamma', 'lambda_', 'rho_clip_ratio', 'c_clip_ratio', 'rho_pg_clip_ratio'])\ndef vtrace_error_discrete_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`torch.Tensor`): the output taking the action by the current policy network,                usually this output is network output logit\\n            - behaviour_output (:obj:`torch.Tensor`): the output taking the action by the behaviour policy network,                usually this output is network output logit, which is used to produce the trajectory(collector)\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`, where T is timestep, B is batch size and            N is action dim\\n        - behaviour_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = torch.randn(T, B, N).requires_grad_(True)\\n        >>> behaviour_output = torch.randn(T, B, N)\\n        >>> action = torch.randint(0, N, size=(T, B))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_discrete_action(data, rho_clip_ratio=1.1)\\n    '\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'discrete')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Categorical(logits=target_output)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
            "@hpc_wrapper(shape_fn=shape_fn_vtrace_discrete_action, namedtuple_data=True, include_args=[0, 1, 2, 3, 4, 5], include_kwargs=['data', 'gamma', 'lambda_', 'rho_clip_ratio', 'c_clip_ratio', 'rho_pg_clip_ratio'])\ndef vtrace_error_discrete_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`torch.Tensor`): the output taking the action by the current policy network,                usually this output is network output logit\\n            - behaviour_output (:obj:`torch.Tensor`): the output taking the action by the behaviour policy network,                usually this output is network output logit, which is used to produce the trajectory(collector)\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`, where T is timestep, B is batch size and            N is action dim\\n        - behaviour_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = torch.randn(T, B, N).requires_grad_(True)\\n        >>> behaviour_output = torch.randn(T, B, N)\\n        >>> action = torch.randint(0, N, size=(T, B))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_discrete_action(data, rho_clip_ratio=1.1)\\n    '\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'discrete')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Categorical(logits=target_output)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
            "@hpc_wrapper(shape_fn=shape_fn_vtrace_discrete_action, namedtuple_data=True, include_args=[0, 1, 2, 3, 4, 5], include_kwargs=['data', 'gamma', 'lambda_', 'rho_clip_ratio', 'c_clip_ratio', 'rho_pg_clip_ratio'])\ndef vtrace_error_discrete_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`torch.Tensor`): the output taking the action by the current policy network,                usually this output is network output logit\\n            - behaviour_output (:obj:`torch.Tensor`): the output taking the action by the behaviour policy network,                usually this output is network output logit, which is used to produce the trajectory(collector)\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`, where T is timestep, B is batch size and            N is action dim\\n        - behaviour_output (:obj:`torch.FloatTensor`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = torch.randn(T, B, N).requires_grad_(True)\\n        >>> behaviour_output = torch.randn(T, B, N)\\n        >>> action = torch.randint(0, N, size=(T, B))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_discrete_action(data, rho_clip_ratio=1.1)\\n    '\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'discrete')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Categorical(logits=target_output)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)"
        ]
    },
    {
        "func_name": "vtrace_error_continuous_action",
        "original": "def vtrace_error_continuous_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    \"\"\"\n    Overview:\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\n    Arguments:\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\n            - target_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the current policy network, usually this output is network output,                 which represents the distribution by reparameterization trick.\n            - behaviour_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the behaviour policy network, usually this output is network output logit,                 which represents the distribution by reparameterization trick.\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                 i.e.: behaviour_action\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\n    Returns:\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\n    Shapes:\n        - target_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`,             where T is timestep, B is batch size and             N is action dim. The keys are usually parameters of reparameterization trick.\n        - behaviour_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\n    Examples:\n        >>> T, B, N = 4, 8, 16\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\n        >>> reward = torch.rand(T, B)\n        >>> target_output = dict(\n        >>>     'mu': torch.randn(T, B, N).requires_grad_(True),\n        >>>     'sigma': torch.exp(torch.randn(T, B, N).requires_grad_(True)),\n        >>> )\n        >>> behaviour_output = dict(\n        >>>     'mu': torch.randn(T, B, N),\n        >>>     'sigma': torch.exp(torch.randn(T, B, N)),\n        >>> )\n        >>> action = torch.randn((T, B, N))\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\n        >>> loss = vtrace_error_continuous_action(data, rho_clip_ratio=1.1)\n    \"\"\"\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'continuous')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
        "mutated": [
            "def vtrace_error_continuous_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the current policy network, usually this output is network output,                 which represents the distribution by reparameterization trick.\\n            - behaviour_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the behaviour policy network, usually this output is network output logit,                 which represents the distribution by reparameterization trick.\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                 i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`,             where T is timestep, B is batch size and             N is action dim. The keys are usually parameters of reparameterization trick.\\n        - behaviour_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = dict(\\n        >>>     'mu': torch.randn(T, B, N).requires_grad_(True),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N).requires_grad_(True)),\\n        >>> )\\n        >>> behaviour_output = dict(\\n        >>>     'mu': torch.randn(T, B, N),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N)),\\n        >>> )\\n        >>> action = torch.randn((T, B, N))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_continuous_action(data, rho_clip_ratio=1.1)\\n    \"\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'continuous')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
            "def vtrace_error_continuous_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the current policy network, usually this output is network output,                 which represents the distribution by reparameterization trick.\\n            - behaviour_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the behaviour policy network, usually this output is network output logit,                 which represents the distribution by reparameterization trick.\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                 i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`,             where T is timestep, B is batch size and             N is action dim. The keys are usually parameters of reparameterization trick.\\n        - behaviour_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = dict(\\n        >>>     'mu': torch.randn(T, B, N).requires_grad_(True),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N).requires_grad_(True)),\\n        >>> )\\n        >>> behaviour_output = dict(\\n        >>>     'mu': torch.randn(T, B, N),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N)),\\n        >>> )\\n        >>> action = torch.randn((T, B, N))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_continuous_action(data, rho_clip_ratio=1.1)\\n    \"\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'continuous')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
            "def vtrace_error_continuous_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the current policy network, usually this output is network output,                 which represents the distribution by reparameterization trick.\\n            - behaviour_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the behaviour policy network, usually this output is network output logit,                 which represents the distribution by reparameterization trick.\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                 i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`,             where T is timestep, B is batch size and             N is action dim. The keys are usually parameters of reparameterization trick.\\n        - behaviour_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = dict(\\n        >>>     'mu': torch.randn(T, B, N).requires_grad_(True),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N).requires_grad_(True)),\\n        >>> )\\n        >>> behaviour_output = dict(\\n        >>>     'mu': torch.randn(T, B, N),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N)),\\n        >>> )\\n        >>> action = torch.randn((T, B, N))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_continuous_action(data, rho_clip_ratio=1.1)\\n    \"\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'continuous')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
            "def vtrace_error_continuous_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the current policy network, usually this output is network output,                 which represents the distribution by reparameterization trick.\\n            - behaviour_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the behaviour policy network, usually this output is network output logit,                 which represents the distribution by reparameterization trick.\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                 i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`,             where T is timestep, B is batch size and             N is action dim. The keys are usually parameters of reparameterization trick.\\n        - behaviour_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = dict(\\n        >>>     'mu': torch.randn(T, B, N).requires_grad_(True),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N).requires_grad_(True)),\\n        >>> )\\n        >>> behaviour_output = dict(\\n        >>>     'mu': torch.randn(T, B, N),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N)),\\n        >>> )\\n        >>> action = torch.randn((T, B, N))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_continuous_action(data, rho_clip_ratio=1.1)\\n    \"\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'continuous')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)",
            "def vtrace_error_continuous_action(data: namedtuple, gamma: float=0.99, lambda_: float=0.95, rho_clip_ratio: float=1.0, c_clip_ratio: float=1.0, rho_pg_clip_ratio: float=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)\\n    Arguments:\\n        - data (:obj:`namedtuple`): input data with fields shown in ``vtrace_data``\\n            - target_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the current policy network, usually this output is network output,                 which represents the distribution by reparameterization trick.\\n            - behaviour_output (:obj:`dict{key:torch.Tensor}`): the output taking the action                 by the behaviour policy network, usually this output is network output logit,                 which represents the distribution by reparameterization trick.\\n            - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,                 i.e.: behaviour_action\\n        - gamma: (:obj:`float`): the future discount factor, defaults to 0.95\\n        - lambda: (:obj:`float`): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0\\n        - rho_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)\\n        - c_clip_ratio (:obj:`float`): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)\\n        - rho_pg_clip_ratio (:obj:`float`): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage\\n    Returns:\\n        - trace_loss (:obj:`namedtuple`): the vtrace loss item, all of them are the differentiable 0-dim tensor\\n    Shapes:\\n        - target_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`,             where T is timestep, B is batch size and             N is action dim. The keys are usually parameters of reparameterization trick.\\n        - behaviour_output (:obj:`dict{key:torch.FloatTensor}`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`\\n        - reward (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - weight (:obj:`torch.LongTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> T, B, N = 4, 8, 16\\n        >>> value = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> reward = torch.rand(T, B)\\n        >>> target_output = dict(\\n        >>>     'mu': torch.randn(T, B, N).requires_grad_(True),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N).requires_grad_(True)),\\n        >>> )\\n        >>> behaviour_output = dict(\\n        >>>     'mu': torch.randn(T, B, N),\\n        >>>     'sigma': torch.exp(torch.randn(T, B, N)),\\n        >>> )\\n        >>> action = torch.randn((T, B, N))\\n        >>> data = vtrace_data(target_output, behaviour_output, action, value, reward, None)\\n        >>> loss = vtrace_error_continuous_action(data, rho_clip_ratio=1.1)\\n    \"\n    (target_output, behaviour_output, action, value, reward, weight) = data\n    with torch.no_grad():\n        IS = compute_importance_weights(target_output, behaviour_output, action, 'continuous')\n        rhos = torch.clamp(IS, max=rho_clip_ratio)\n        cs = torch.clamp(IS, max=c_clip_ratio)\n        return_ = vtrace_nstep_return(rhos, cs, reward, value, gamma, lambda_)\n        pg_rhos = torch.clamp(IS, max=rho_pg_clip_ratio)\n        return_t_plus_1 = torch.cat([return_[1:], value[-1:]], 0)\n        adv = vtrace_advantage(pg_rhos, reward, return_t_plus_1, value[:-1], gamma)\n    if weight is None:\n        weight = torch.ones_like(reward)\n    dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n    pg_loss = -(dist_target.log_prob(action) * adv * weight).mean()\n    value_loss = (F.mse_loss(value[:-1], return_, reduction='none') * weight).mean()\n    entropy_loss = (dist_target.entropy() * weight).mean()\n    return vtrace_loss(pg_loss, value_loss, entropy_loss)"
        ]
    }
]