[
    {
        "func_name": "fix_group_mask_conflict",
        "original": "def fix_group_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    \"\"\"\n    Fix the mask conflict between group and channel.\n    This function will modify the masks in-place.\n\n    Parameters\n    ----------\n    graph_module\n        The graph module.\n    masks\n        The masks of all modules.\n\n    Returns\n    -------\n    masks\n        The fixed masks.\n    \"\"\"\n    group_dependency = build_group_dependency(graph_module)\n    for (node, (max_group, min_group)) in group_dependency.items():\n        layername = node.target\n        if layername not in masks or 'weight' not in masks[layername]:\n            continue\n        w_mask = masks[layername]['weight']\n        shape = w_mask.shape\n        count = np.prod(shape[1:])\n        all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n        all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n        if len(all_ones) + len(all_zeros) < w_mask.size(0):\n            _logger.info('Layers %s using fine-grained pruning', layername)\n            continue\n        assert shape[0] % max_group == 0\n        step = shape[0] / max_group\n        group_masked = []\n        for i in range(max_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            group_masked.append(_tmp_list)\n        mini_masked = min([len(x) for x in group_masked])\n        need_unmask = set()\n        for gm in group_masked:\n            for i in range(mini_masked, len(gm)):\n                pos = gm[i]\n                need_unmask.add(pos)\n        step = shape[0] / min_group\n        for i in range(min_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            if len(_tmp_list) == step:\n                for pos in _tmp_list:\n                    if pos in need_unmask:\n                        need_unmask.remove(pos)\n        for pos in need_unmask:\n            masks[layername]['weight'][pos] = torch.ones(shape[1:])\n            if hasattr(masks[layername], 'bias'):\n                masks[layername]['bias'][pos] = 1\n    return masks",
        "mutated": [
            "def fix_group_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n    '\\n    Fix the mask conflict between group and channel.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    group_dependency = build_group_dependency(graph_module)\n    for (node, (max_group, min_group)) in group_dependency.items():\n        layername = node.target\n        if layername not in masks or 'weight' not in masks[layername]:\n            continue\n        w_mask = masks[layername]['weight']\n        shape = w_mask.shape\n        count = np.prod(shape[1:])\n        all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n        all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n        if len(all_ones) + len(all_zeros) < w_mask.size(0):\n            _logger.info('Layers %s using fine-grained pruning', layername)\n            continue\n        assert shape[0] % max_group == 0\n        step = shape[0] / max_group\n        group_masked = []\n        for i in range(max_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            group_masked.append(_tmp_list)\n        mini_masked = min([len(x) for x in group_masked])\n        need_unmask = set()\n        for gm in group_masked:\n            for i in range(mini_masked, len(gm)):\n                pos = gm[i]\n                need_unmask.add(pos)\n        step = shape[0] / min_group\n        for i in range(min_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            if len(_tmp_list) == step:\n                for pos in _tmp_list:\n                    if pos in need_unmask:\n                        need_unmask.remove(pos)\n        for pos in need_unmask:\n            masks[layername]['weight'][pos] = torch.ones(shape[1:])\n            if hasattr(masks[layername], 'bias'):\n                masks[layername]['bias'][pos] = 1\n    return masks",
            "def fix_group_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Fix the mask conflict between group and channel.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    group_dependency = build_group_dependency(graph_module)\n    for (node, (max_group, min_group)) in group_dependency.items():\n        layername = node.target\n        if layername not in masks or 'weight' not in masks[layername]:\n            continue\n        w_mask = masks[layername]['weight']\n        shape = w_mask.shape\n        count = np.prod(shape[1:])\n        all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n        all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n        if len(all_ones) + len(all_zeros) < w_mask.size(0):\n            _logger.info('Layers %s using fine-grained pruning', layername)\n            continue\n        assert shape[0] % max_group == 0\n        step = shape[0] / max_group\n        group_masked = []\n        for i in range(max_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            group_masked.append(_tmp_list)\n        mini_masked = min([len(x) for x in group_masked])\n        need_unmask = set()\n        for gm in group_masked:\n            for i in range(mini_masked, len(gm)):\n                pos = gm[i]\n                need_unmask.add(pos)\n        step = shape[0] / min_group\n        for i in range(min_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            if len(_tmp_list) == step:\n                for pos in _tmp_list:\n                    if pos in need_unmask:\n                        need_unmask.remove(pos)\n        for pos in need_unmask:\n            masks[layername]['weight'][pos] = torch.ones(shape[1:])\n            if hasattr(masks[layername], 'bias'):\n                masks[layername]['bias'][pos] = 1\n    return masks",
            "def fix_group_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Fix the mask conflict between group and channel.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    group_dependency = build_group_dependency(graph_module)\n    for (node, (max_group, min_group)) in group_dependency.items():\n        layername = node.target\n        if layername not in masks or 'weight' not in masks[layername]:\n            continue\n        w_mask = masks[layername]['weight']\n        shape = w_mask.shape\n        count = np.prod(shape[1:])\n        all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n        all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n        if len(all_ones) + len(all_zeros) < w_mask.size(0):\n            _logger.info('Layers %s using fine-grained pruning', layername)\n            continue\n        assert shape[0] % max_group == 0\n        step = shape[0] / max_group\n        group_masked = []\n        for i in range(max_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            group_masked.append(_tmp_list)\n        mini_masked = min([len(x) for x in group_masked])\n        need_unmask = set()\n        for gm in group_masked:\n            for i in range(mini_masked, len(gm)):\n                pos = gm[i]\n                need_unmask.add(pos)\n        step = shape[0] / min_group\n        for i in range(min_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            if len(_tmp_list) == step:\n                for pos in _tmp_list:\n                    if pos in need_unmask:\n                        need_unmask.remove(pos)\n        for pos in need_unmask:\n            masks[layername]['weight'][pos] = torch.ones(shape[1:])\n            if hasattr(masks[layername], 'bias'):\n                masks[layername]['bias'][pos] = 1\n    return masks",
            "def fix_group_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Fix the mask conflict between group and channel.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    group_dependency = build_group_dependency(graph_module)\n    for (node, (max_group, min_group)) in group_dependency.items():\n        layername = node.target\n        if layername not in masks or 'weight' not in masks[layername]:\n            continue\n        w_mask = masks[layername]['weight']\n        shape = w_mask.shape\n        count = np.prod(shape[1:])\n        all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n        all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n        if len(all_ones) + len(all_zeros) < w_mask.size(0):\n            _logger.info('Layers %s using fine-grained pruning', layername)\n            continue\n        assert shape[0] % max_group == 0\n        step = shape[0] / max_group\n        group_masked = []\n        for i in range(max_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            group_masked.append(_tmp_list)\n        mini_masked = min([len(x) for x in group_masked])\n        need_unmask = set()\n        for gm in group_masked:\n            for i in range(mini_masked, len(gm)):\n                pos = gm[i]\n                need_unmask.add(pos)\n        step = shape[0] / min_group\n        for i in range(min_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            if len(_tmp_list) == step:\n                for pos in _tmp_list:\n                    if pos in need_unmask:\n                        need_unmask.remove(pos)\n        for pos in need_unmask:\n            masks[layername]['weight'][pos] = torch.ones(shape[1:])\n            if hasattr(masks[layername], 'bias'):\n                masks[layername]['bias'][pos] = 1\n    return masks",
            "def fix_group_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Fix the mask conflict between group and channel.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    group_dependency = build_group_dependency(graph_module)\n    for (node, (max_group, min_group)) in group_dependency.items():\n        layername = node.target\n        if layername not in masks or 'weight' not in masks[layername]:\n            continue\n        w_mask = masks[layername]['weight']\n        shape = w_mask.shape\n        count = np.prod(shape[1:])\n        all_ones = (w_mask.flatten(1).sum(-1) == count).nonzero().squeeze(1).tolist()\n        all_zeros = (w_mask.flatten(1).sum(-1) == 0).nonzero().squeeze(1).tolist()\n        if len(all_ones) + len(all_zeros) < w_mask.size(0):\n            _logger.info('Layers %s using fine-grained pruning', layername)\n            continue\n        assert shape[0] % max_group == 0\n        step = shape[0] / max_group\n        group_masked = []\n        for i in range(max_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            group_masked.append(_tmp_list)\n        mini_masked = min([len(x) for x in group_masked])\n        need_unmask = set()\n        for gm in group_masked:\n            for i in range(mini_masked, len(gm)):\n                pos = gm[i]\n                need_unmask.add(pos)\n        step = shape[0] / min_group\n        for i in range(min_group):\n            _start = step * i\n            _end = step * (i + 1)\n            _tmp_list = list(filter(lambda x: _start <= x and x < _end, all_zeros))\n            if len(_tmp_list) == step:\n                for pos in _tmp_list:\n                    if pos in need_unmask:\n                        need_unmask.remove(pos)\n        for pos in need_unmask:\n            masks[layername]['weight'][pos] = torch.ones(shape[1:])\n            if hasattr(masks[layername], 'bias'):\n                masks[layername]['bias'][pos] = 1\n    return masks"
        ]
    },
    {
        "func_name": "fix_weight_sharing_mask_conflict",
        "original": "def fix_weight_sharing_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    \"\"\"\n    Fix the mask conflict produced by weight sharing.\n    This function will modify the masks in-place.\n\n    Parameters\n    ----------\n    graph_module\n        The graph module.\n    masks\n        The masks of all modules.\n\n    Returns\n    -------\n    masks\n        The fixed masks.\n    \"\"\"\n    dependency_sets = build_weight_sharing_dependency(graph_module)\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for name in d_set:\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grianed mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for name in d_set:\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
        "mutated": [
            "def fix_weight_sharing_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n    '\\n    Fix the mask conflict produced by weight sharing.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    dependency_sets = build_weight_sharing_dependency(graph_module)\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for name in d_set:\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grianed mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for name in d_set:\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
            "def fix_weight_sharing_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Fix the mask conflict produced by weight sharing.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    dependency_sets = build_weight_sharing_dependency(graph_module)\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for name in d_set:\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grianed mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for name in d_set:\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
            "def fix_weight_sharing_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Fix the mask conflict produced by weight sharing.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    dependency_sets = build_weight_sharing_dependency(graph_module)\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for name in d_set:\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grianed mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for name in d_set:\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
            "def fix_weight_sharing_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Fix the mask conflict produced by weight sharing.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    dependency_sets = build_weight_sharing_dependency(graph_module)\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for name in d_set:\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grianed mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for name in d_set:\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
            "def fix_weight_sharing_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Fix the mask conflict produced by weight sharing.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    dependency_sets = build_weight_sharing_dependency(graph_module)\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for name in d_set:\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grianed mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for name in d_set:\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks"
        ]
    },
    {
        "func_name": "fix_channel_mask_conflict",
        "original": "def fix_channel_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    \"\"\"\n    Fix the mask conflict between channel and group.\n    This function will modify the masks in-place.\n\n    Parameters\n    ----------\n    graph_module\n        The graph module.\n    masks\n        The masks of all modules.\n\n    Returns\n    -------\n    masks\n        The fixed masks.\n    \"\"\"\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    prune_type = detect_channel_prune_type(graph_module, masks)\n    print(prune_axis, prune_type)\n    dependency_sets = build_channel_dependency(graph_module, prune_axis=prune_axis, prune_type=prune_type)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for node in d_set:\n            name = node.target\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                    channel_masks.append(mask.int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grained mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for node in d_set:\n            name = node.target\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                new_mask = merged_channel_mask.type_as(orig_mask)\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
        "mutated": [
            "def fix_channel_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n    '\\n    Fix the mask conflict between channel and group.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    prune_type = detect_channel_prune_type(graph_module, masks)\n    print(prune_axis, prune_type)\n    dependency_sets = build_channel_dependency(graph_module, prune_axis=prune_axis, prune_type=prune_type)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for node in d_set:\n            name = node.target\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                    channel_masks.append(mask.int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grained mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for node in d_set:\n            name = node.target\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                new_mask = merged_channel_mask.type_as(orig_mask)\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
            "def fix_channel_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Fix the mask conflict between channel and group.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    prune_type = detect_channel_prune_type(graph_module, masks)\n    print(prune_axis, prune_type)\n    dependency_sets = build_channel_dependency(graph_module, prune_axis=prune_axis, prune_type=prune_type)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for node in d_set:\n            name = node.target\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                    channel_masks.append(mask.int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grained mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for node in d_set:\n            name = node.target\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                new_mask = merged_channel_mask.type_as(orig_mask)\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
            "def fix_channel_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Fix the mask conflict between channel and group.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    prune_type = detect_channel_prune_type(graph_module, masks)\n    print(prune_axis, prune_type)\n    dependency_sets = build_channel_dependency(graph_module, prune_axis=prune_axis, prune_type=prune_type)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for node in d_set:\n            name = node.target\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                    channel_masks.append(mask.int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grained mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for node in d_set:\n            name = node.target\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                new_mask = merged_channel_mask.type_as(orig_mask)\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
            "def fix_channel_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Fix the mask conflict between channel and group.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    prune_type = detect_channel_prune_type(graph_module, masks)\n    print(prune_axis, prune_type)\n    dependency_sets = build_channel_dependency(graph_module, prune_axis=prune_axis, prune_type=prune_type)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for node in d_set:\n            name = node.target\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                    channel_masks.append(mask.int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grained mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for node in d_set:\n            name = node.target\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                new_mask = merged_channel_mask.type_as(orig_mask)\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks",
            "def fix_channel_mask_conflict(graph_module: torch.fx.GraphModule, masks: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Fix the mask conflict between channel and group.\\n    This function will modify the masks in-place.\\n\\n    Parameters\\n    ----------\\n    graph_module\\n        The graph module.\\n    masks\\n        The masks of all modules.\\n\\n    Returns\\n    -------\\n    masks\\n        The fixed masks.\\n    '\n    prune_axis = detect_mask_prune_dim(graph_module, masks)\n    prune_type = detect_channel_prune_type(graph_module, masks)\n    print(prune_axis, prune_type)\n    dependency_sets = build_channel_dependency(graph_module, prune_axis=prune_axis, prune_type=prune_type)\n    sum_idx = (1, 2, 3) if prune_axis == 0 else (0, 2, 3)\n    (_, _tmp_tensor) = list(masks.items())[0]\n    device = list(_tmp_tensor.values())[0].device\n    for d_set in dependency_sets:\n        if len(d_set) <= 1:\n            continue\n        channel_masks = []\n        fine_grained = False\n        for node in d_set:\n            name = node.target\n            if name in masks and 'weight' in masks[name]:\n                sub_module = graph_module.get_submodule(name)\n                assert sub_module is not None\n                mask = masks[name]['weight']\n                if isinstance(sub_module, torch.nn.Conv2d):\n                    channel_mask = (mask.abs().sum(sum_idx) != 0).int()\n                    if prune_axis == 1:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                elif isinstance(sub_module, torch.nn.Linear):\n                    if prune_axis == 1:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                    else:\n                        channel_masks.append((mask.abs().sum(1) != 0).int())\n                elif isinstance(sub_module, torch.nn.Embedding):\n                    if prune_axis == 0:\n                        channel_masks.append((mask.abs().sum(0) != 0).int())\n                elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                    channel_masks.append(mask.int())\n                elif isinstance(sub_module, torch.nn.ConvTranspose2d):\n                    tmp_sum_idx = (0, 2, 3) if prune_axis == 0 else (1, 2, 3)\n                    channel_mask = (mask.abs().sum(tmp_sum_idx) != 0).int()\n                    if prune_axis == 0:\n                        channel_mask = channel_mask.repeat(sub_module.groups)\n                    channel_masks.append(channel_mask)\n                    if (channel_mask.sum() * (mask.numel() / mask.shape[1 - prune_axis])).item() != (mask > 0).sum().item():\n                        fine_grained = True\n                else:\n                    raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            else:\n                channel_masks.append(None)\n        if fine_grained:\n            _logger.info('Fine-grained mask detected')\n        if all((x is None for x in channel_masks)):\n            continue\n        num_channels_list = [len(x) for x in channel_masks if x is not None]\n        assert len(set(num_channels_list)) == 1\n        num_channels = num_channels_list[0]\n        for (i, dim_mask) in enumerate(channel_masks):\n            if dim_mask is None:\n                channel_masks[i] = torch.ones(num_channels).int().to(device)\n        merged_channel_mask = channel_masks[0].clone()\n        for i in range(1, len(channel_masks)):\n            merged_channel_mask = (merged_channel_mask + channel_masks[i] != 0).int()\n        merged_index = torch.nonzero(merged_channel_mask, as_tuple=True)[0]\n        for node in d_set:\n            name = node.target\n            if name not in masks or 'weight' not in masks[name]:\n                assert all(merged_channel_mask)\n                continue\n            orig_mask = masks[name]['weight']\n            sub_module = graph_module.get_submodule(name)\n            new_mask = torch.zeros_like(orig_mask)\n            if isinstance(sub_module, torch.nn.Conv2d):\n                if prune_axis == 0:\n                    new_mask[merged_index, :, :, :] = 1.0\n                else:\n                    new_mask[:, torch.nonzero(merged_channel_mask[:new_mask.shape[1]], as_tuple=True)[0], :, :] = 1.0\n            elif isinstance(sub_module, torch.nn.Linear):\n                if prune_axis == 0:\n                    new_mask[merged_index, :] = 1.0\n                elif prune_axis == 1:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.Embedding):\n                if prune_axis == 0:\n                    new_mask[:, merged_index] = 1.0\n            elif isinstance(sub_module, torch.nn.BatchNorm2d):\n                new_mask = merged_channel_mask.type_as(orig_mask)\n            else:\n                raise RuntimeError(f'unsupported module type: {type(sub_module).__name__}')\n            masks[name]['weight'] = new_mask\n            if 'bias' in masks[name] and masks[name]['bias'] is not None:\n                if prune_axis == 0:\n                    masks[name]['bias'] = merged_channel_mask.type_as(masks[name]['bias'])\n    return masks"
        ]
    },
    {
        "func_name": "detect_channel_prune_type",
        "original": "def detect_channel_prune_type(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> str:\n    \"\"\"\n    User can prune a channel through two ways: 1) prune\n    the corresponding filter of the conv layer(all the\n    filter related pruner), 2) prune the BN layers that\n    followed after a conv(Slim pruner). This function find\n    the pruning type of the masks.\n\n    Parameters\n    ----------\n    graph_module: torch.fx.GraphModule\n        GraphModule object which the mask can be applied on.\n    masks: dict\n        A dict object that stores the masks.\n\n    Returns:\n    -------\n    prune_type: str\n        Could be Filter or BatchNorm\n    \"\"\"\n    for layer_name in masks:\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.BatchNorm2d):\n            return 'Filter'\n    return 'BatchNorm'",
        "mutated": [
            "def detect_channel_prune_type(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> str:\n    if False:\n        i = 10\n    '\\n    User can prune a channel through two ways: 1) prune\\n    the corresponding filter of the conv layer(all the\\n    filter related pruner), 2) prune the BN layers that\\n    followed after a conv(Slim pruner). This function find\\n    the pruning type of the masks.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n\\n    Returns:\\n    -------\\n    prune_type: str\\n        Could be Filter or BatchNorm\\n    '\n    for layer_name in masks:\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.BatchNorm2d):\n            return 'Filter'\n    return 'BatchNorm'",
            "def detect_channel_prune_type(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    User can prune a channel through two ways: 1) prune\\n    the corresponding filter of the conv layer(all the\\n    filter related pruner), 2) prune the BN layers that\\n    followed after a conv(Slim pruner). This function find\\n    the pruning type of the masks.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n\\n    Returns:\\n    -------\\n    prune_type: str\\n        Could be Filter or BatchNorm\\n    '\n    for layer_name in masks:\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.BatchNorm2d):\n            return 'Filter'\n    return 'BatchNorm'",
            "def detect_channel_prune_type(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    User can prune a channel through two ways: 1) prune\\n    the corresponding filter of the conv layer(all the\\n    filter related pruner), 2) prune the BN layers that\\n    followed after a conv(Slim pruner). This function find\\n    the pruning type of the masks.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n\\n    Returns:\\n    -------\\n    prune_type: str\\n        Could be Filter or BatchNorm\\n    '\n    for layer_name in masks:\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.BatchNorm2d):\n            return 'Filter'\n    return 'BatchNorm'",
            "def detect_channel_prune_type(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    User can prune a channel through two ways: 1) prune\\n    the corresponding filter of the conv layer(all the\\n    filter related pruner), 2) prune the BN layers that\\n    followed after a conv(Slim pruner). This function find\\n    the pruning type of the masks.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n\\n    Returns:\\n    -------\\n    prune_type: str\\n        Could be Filter or BatchNorm\\n    '\n    for layer_name in masks:\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.BatchNorm2d):\n            return 'Filter'\n    return 'BatchNorm'",
            "def detect_channel_prune_type(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    User can prune a channel through two ways: 1) prune\\n    the corresponding filter of the conv layer(all the\\n    filter related pruner), 2) prune the BN layers that\\n    followed after a conv(Slim pruner). This function find\\n    the pruning type of the masks.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n\\n    Returns:\\n    -------\\n    prune_type: str\\n        Could be Filter or BatchNorm\\n    '\n    for layer_name in masks:\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.BatchNorm2d):\n            return 'Filter'\n    return 'BatchNorm'"
        ]
    },
    {
        "func_name": "detect_mask_prune_dim",
        "original": "def detect_mask_prune_dim(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> int:\n    \"\"\"\n    Detect how the masks of convolutional layers are pruned.\n\n    Parameters\n    ----------\n    graph_module: torch.fx.GraphModule\n        GraphModule object which the mask can be applied on.\n    masks: dict\n        A dict object that stores the masks.\n    Returns:\n    -------\n        How the masks of convolutional layers are pruned, this depends on pruning algorithms, it should\n        return 1 for masks generated by AMCPruner, and returns 0 for masks generated by the rest\n        NNI builtin pruners.\n        0: filter pruning, prune filters of weights which causes channels of output feature maps are pruned.\n        1: channel pruning, prune kernels corresponding to each input channels which causes channels of\n           input feature maps are pruned.\n    \"\"\"\n    (dim0_preserved, dim1_preserved) = (0.0, 0.0)\n    (dim0_num, dim1_num) = (0.0, 0.0)\n    for layer_name in masks:\n        if 'weight' not in masks[layer_name]:\n            continue\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.Conv2d):\n            continue\n        mask = masks[layer_name]['weight'].clone()\n        assert (mask >= 0).sum() == mask.numel(), 'mask values should be greater than or equal to 0.'\n        mask = (mask > 0).int()\n        mask = mask.view(mask.shape[0], mask.shape[1], -1)\n        dim0_mask = (mask.sum((1, 2)) > 0).int()\n        dim1_mask = (mask.sum((0, 2)) > 0).int()\n        dim0_preserved += dim0_mask.sum().item()\n        dim1_preserved += dim1_mask.sum().item()\n        dim0_num += len(dim0_mask)\n        dim1_num += len(dim1_mask)\n    if dim0_num == 0 or dim1_num == 0:\n        _logger.warning('no multi-dimension masks found.')\n        return 0\n    (dim0_sparsity, dim1_sparsity) = (1.0 - dim0_preserved / dim0_num, 1.0 - dim1_preserved / dim1_num)\n    _logger.info('dim0 sparsity: %f', dim0_sparsity)\n    _logger.info('dim1 sparsity: %f', dim1_sparsity)\n    if dim0_sparsity == dim1_sparsity == 0.0:\n        _logger.warning('nothing masked.')\n    if dim0_sparsity > 0 and dim1_sparsity > 0:\n        _logger.warning('both dim0 and dim1 masks found.')\n    return 0 if dim0_sparsity >= dim1_sparsity else 1",
        "mutated": [
            "def detect_mask_prune_dim(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> int:\n    if False:\n        i = 10\n    '\\n    Detect how the masks of convolutional layers are pruned.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n    Returns:\\n    -------\\n        How the masks of convolutional layers are pruned, this depends on pruning algorithms, it should\\n        return 1 for masks generated by AMCPruner, and returns 0 for masks generated by the rest\\n        NNI builtin pruners.\\n        0: filter pruning, prune filters of weights which causes channels of output feature maps are pruned.\\n        1: channel pruning, prune kernels corresponding to each input channels which causes channels of\\n           input feature maps are pruned.\\n    '\n    (dim0_preserved, dim1_preserved) = (0.0, 0.0)\n    (dim0_num, dim1_num) = (0.0, 0.0)\n    for layer_name in masks:\n        if 'weight' not in masks[layer_name]:\n            continue\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.Conv2d):\n            continue\n        mask = masks[layer_name]['weight'].clone()\n        assert (mask >= 0).sum() == mask.numel(), 'mask values should be greater than or equal to 0.'\n        mask = (mask > 0).int()\n        mask = mask.view(mask.shape[0], mask.shape[1], -1)\n        dim0_mask = (mask.sum((1, 2)) > 0).int()\n        dim1_mask = (mask.sum((0, 2)) > 0).int()\n        dim0_preserved += dim0_mask.sum().item()\n        dim1_preserved += dim1_mask.sum().item()\n        dim0_num += len(dim0_mask)\n        dim1_num += len(dim1_mask)\n    if dim0_num == 0 or dim1_num == 0:\n        _logger.warning('no multi-dimension masks found.')\n        return 0\n    (dim0_sparsity, dim1_sparsity) = (1.0 - dim0_preserved / dim0_num, 1.0 - dim1_preserved / dim1_num)\n    _logger.info('dim0 sparsity: %f', dim0_sparsity)\n    _logger.info('dim1 sparsity: %f', dim1_sparsity)\n    if dim0_sparsity == dim1_sparsity == 0.0:\n        _logger.warning('nothing masked.')\n    if dim0_sparsity > 0 and dim1_sparsity > 0:\n        _logger.warning('both dim0 and dim1 masks found.')\n    return 0 if dim0_sparsity >= dim1_sparsity else 1",
            "def detect_mask_prune_dim(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Detect how the masks of convolutional layers are pruned.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n    Returns:\\n    -------\\n        How the masks of convolutional layers are pruned, this depends on pruning algorithms, it should\\n        return 1 for masks generated by AMCPruner, and returns 0 for masks generated by the rest\\n        NNI builtin pruners.\\n        0: filter pruning, prune filters of weights which causes channels of output feature maps are pruned.\\n        1: channel pruning, prune kernels corresponding to each input channels which causes channels of\\n           input feature maps are pruned.\\n    '\n    (dim0_preserved, dim1_preserved) = (0.0, 0.0)\n    (dim0_num, dim1_num) = (0.0, 0.0)\n    for layer_name in masks:\n        if 'weight' not in masks[layer_name]:\n            continue\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.Conv2d):\n            continue\n        mask = masks[layer_name]['weight'].clone()\n        assert (mask >= 0).sum() == mask.numel(), 'mask values should be greater than or equal to 0.'\n        mask = (mask > 0).int()\n        mask = mask.view(mask.shape[0], mask.shape[1], -1)\n        dim0_mask = (mask.sum((1, 2)) > 0).int()\n        dim1_mask = (mask.sum((0, 2)) > 0).int()\n        dim0_preserved += dim0_mask.sum().item()\n        dim1_preserved += dim1_mask.sum().item()\n        dim0_num += len(dim0_mask)\n        dim1_num += len(dim1_mask)\n    if dim0_num == 0 or dim1_num == 0:\n        _logger.warning('no multi-dimension masks found.')\n        return 0\n    (dim0_sparsity, dim1_sparsity) = (1.0 - dim0_preserved / dim0_num, 1.0 - dim1_preserved / dim1_num)\n    _logger.info('dim0 sparsity: %f', dim0_sparsity)\n    _logger.info('dim1 sparsity: %f', dim1_sparsity)\n    if dim0_sparsity == dim1_sparsity == 0.0:\n        _logger.warning('nothing masked.')\n    if dim0_sparsity > 0 and dim1_sparsity > 0:\n        _logger.warning('both dim0 and dim1 masks found.')\n    return 0 if dim0_sparsity >= dim1_sparsity else 1",
            "def detect_mask_prune_dim(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Detect how the masks of convolutional layers are pruned.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n    Returns:\\n    -------\\n        How the masks of convolutional layers are pruned, this depends on pruning algorithms, it should\\n        return 1 for masks generated by AMCPruner, and returns 0 for masks generated by the rest\\n        NNI builtin pruners.\\n        0: filter pruning, prune filters of weights which causes channels of output feature maps are pruned.\\n        1: channel pruning, prune kernels corresponding to each input channels which causes channels of\\n           input feature maps are pruned.\\n    '\n    (dim0_preserved, dim1_preserved) = (0.0, 0.0)\n    (dim0_num, dim1_num) = (0.0, 0.0)\n    for layer_name in masks:\n        if 'weight' not in masks[layer_name]:\n            continue\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.Conv2d):\n            continue\n        mask = masks[layer_name]['weight'].clone()\n        assert (mask >= 0).sum() == mask.numel(), 'mask values should be greater than or equal to 0.'\n        mask = (mask > 0).int()\n        mask = mask.view(mask.shape[0], mask.shape[1], -1)\n        dim0_mask = (mask.sum((1, 2)) > 0).int()\n        dim1_mask = (mask.sum((0, 2)) > 0).int()\n        dim0_preserved += dim0_mask.sum().item()\n        dim1_preserved += dim1_mask.sum().item()\n        dim0_num += len(dim0_mask)\n        dim1_num += len(dim1_mask)\n    if dim0_num == 0 or dim1_num == 0:\n        _logger.warning('no multi-dimension masks found.')\n        return 0\n    (dim0_sparsity, dim1_sparsity) = (1.0 - dim0_preserved / dim0_num, 1.0 - dim1_preserved / dim1_num)\n    _logger.info('dim0 sparsity: %f', dim0_sparsity)\n    _logger.info('dim1 sparsity: %f', dim1_sparsity)\n    if dim0_sparsity == dim1_sparsity == 0.0:\n        _logger.warning('nothing masked.')\n    if dim0_sparsity > 0 and dim1_sparsity > 0:\n        _logger.warning('both dim0 and dim1 masks found.')\n    return 0 if dim0_sparsity >= dim1_sparsity else 1",
            "def detect_mask_prune_dim(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Detect how the masks of convolutional layers are pruned.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n    Returns:\\n    -------\\n        How the masks of convolutional layers are pruned, this depends on pruning algorithms, it should\\n        return 1 for masks generated by AMCPruner, and returns 0 for masks generated by the rest\\n        NNI builtin pruners.\\n        0: filter pruning, prune filters of weights which causes channels of output feature maps are pruned.\\n        1: channel pruning, prune kernels corresponding to each input channels which causes channels of\\n           input feature maps are pruned.\\n    '\n    (dim0_preserved, dim1_preserved) = (0.0, 0.0)\n    (dim0_num, dim1_num) = (0.0, 0.0)\n    for layer_name in masks:\n        if 'weight' not in masks[layer_name]:\n            continue\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.Conv2d):\n            continue\n        mask = masks[layer_name]['weight'].clone()\n        assert (mask >= 0).sum() == mask.numel(), 'mask values should be greater than or equal to 0.'\n        mask = (mask > 0).int()\n        mask = mask.view(mask.shape[0], mask.shape[1], -1)\n        dim0_mask = (mask.sum((1, 2)) > 0).int()\n        dim1_mask = (mask.sum((0, 2)) > 0).int()\n        dim0_preserved += dim0_mask.sum().item()\n        dim1_preserved += dim1_mask.sum().item()\n        dim0_num += len(dim0_mask)\n        dim1_num += len(dim1_mask)\n    if dim0_num == 0 or dim1_num == 0:\n        _logger.warning('no multi-dimension masks found.')\n        return 0\n    (dim0_sparsity, dim1_sparsity) = (1.0 - dim0_preserved / dim0_num, 1.0 - dim1_preserved / dim1_num)\n    _logger.info('dim0 sparsity: %f', dim0_sparsity)\n    _logger.info('dim1 sparsity: %f', dim1_sparsity)\n    if dim0_sparsity == dim1_sparsity == 0.0:\n        _logger.warning('nothing masked.')\n    if dim0_sparsity > 0 and dim1_sparsity > 0:\n        _logger.warning('both dim0 and dim1 masks found.')\n    return 0 if dim0_sparsity >= dim1_sparsity else 1",
            "def detect_mask_prune_dim(graph_module: torch.fx.GraphModule, masks: Dict[str, Dict[str, torch.Tensor]]) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Detect how the masks of convolutional layers are pruned.\\n\\n    Parameters\\n    ----------\\n    graph_module: torch.fx.GraphModule\\n        GraphModule object which the mask can be applied on.\\n    masks: dict\\n        A dict object that stores the masks.\\n    Returns:\\n    -------\\n        How the masks of convolutional layers are pruned, this depends on pruning algorithms, it should\\n        return 1 for masks generated by AMCPruner, and returns 0 for masks generated by the rest\\n        NNI builtin pruners.\\n        0: filter pruning, prune filters of weights which causes channels of output feature maps are pruned.\\n        1: channel pruning, prune kernels corresponding to each input channels which causes channels of\\n           input feature maps are pruned.\\n    '\n    (dim0_preserved, dim1_preserved) = (0.0, 0.0)\n    (dim0_num, dim1_num) = (0.0, 0.0)\n    for layer_name in masks:\n        if 'weight' not in masks[layer_name]:\n            continue\n        sub_module = graph_module.get_submodule(layer_name)\n        if sub_module is None or not isinstance(sub_module, torch.nn.Conv2d):\n            continue\n        mask = masks[layer_name]['weight'].clone()\n        assert (mask >= 0).sum() == mask.numel(), 'mask values should be greater than or equal to 0.'\n        mask = (mask > 0).int()\n        mask = mask.view(mask.shape[0], mask.shape[1], -1)\n        dim0_mask = (mask.sum((1, 2)) > 0).int()\n        dim1_mask = (mask.sum((0, 2)) > 0).int()\n        dim0_preserved += dim0_mask.sum().item()\n        dim1_preserved += dim1_mask.sum().item()\n        dim0_num += len(dim0_mask)\n        dim1_num += len(dim1_mask)\n    if dim0_num == 0 or dim1_num == 0:\n        _logger.warning('no multi-dimension masks found.')\n        return 0\n    (dim0_sparsity, dim1_sparsity) = (1.0 - dim0_preserved / dim0_num, 1.0 - dim1_preserved / dim1_num)\n    _logger.info('dim0 sparsity: %f', dim0_sparsity)\n    _logger.info('dim1 sparsity: %f', dim1_sparsity)\n    if dim0_sparsity == dim1_sparsity == 0.0:\n        _logger.warning('nothing masked.')\n    if dim0_sparsity > 0 and dim1_sparsity > 0:\n        _logger.warning('both dim0 and dim1 masks found.')\n    return 0 if dim0_sparsity >= dim1_sparsity else 1"
        ]
    }
]