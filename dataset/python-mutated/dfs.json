[
    {
        "func_name": "dfs",
        "original": "@entry_point('featuretools_dfs')\ndef dfs(dataframes=None, relationships=None, entityset=None, target_dataframe_name=None, cutoff_time=None, instance_ids=None, agg_primitives=None, trans_primitives=None, groupby_trans_primitives=None, allowed_paths=None, max_depth=2, ignore_dataframes=None, ignore_columns=None, primitive_options=None, seed_features=None, drop_contains=None, drop_exact=None, where_primitives=None, max_features=-1, cutoff_time_in_index=False, save_progress=None, features_only=False, training_window=None, approximate=None, chunk_size=None, n_jobs=1, dask_kwargs=None, verbose=False, return_types=None, progress_callback=None, include_cutoff_time=True):\n    \"\"\"Calculates a feature matrix and features given a dictionary of dataframes\n    and a list of relationships.\n\n\n    Args:\n        dataframes (dict[str -> tuple(DataFrame, str, str, dict[str -> str/Woodwork.LogicalType], dict[str->str/set], boolean)]):\n            Dictionary of DataFrames. Entries take the format\n            {dataframe name -> (dataframe, index column, time_index, logical_types, semantic_tags, make_index)}.\n            Note that only the dataframe is required. If a Woodwork DataFrame is supplied, any other parameters\n            will be ignored.\n\n        relationships (list[(str, str, str, str)]): List of relationships\n            between dataframes. List items are a tuple with the format\n            (parent dataframe name, parent column, child dataframe name, child column).\n\n        entityset (EntitySet): An already initialized entityset. Required if\n            dataframes and relationships are not defined.\n\n        target_dataframe_name (str): Name of dataframe on which to make predictions.\n\n        cutoff_time (pd.DataFrame or Datetime or str): Specifies times at which to calculate\n            the features for each instance. The resulting feature matrix will use data\n            up to and including the cutoff_time. Can either be a DataFrame, a single\n            value, or a string that can be parsed into a datetime. If a DataFrame is passed\n            the instance ids for which to calculate features must be in a column with the\n            same name as the target dataframe index or a column named `instance_id`.\n            The cutoff time values in the DataFrame must be in a column with the same name as\n            the target dataframe time index or a column named `time`. If the DataFrame has more\n            than two columns, any additional columns will be added to the resulting feature\n            matrix. If a single value is passed, this value will be used for all instances.\n\n        instance_ids (list): List of instances on which to calculate features. Only\n            used if cutoff_time is a single datetime.\n\n        agg_primitives (list[str or AggregationPrimitive], optional): List of Aggregation\n            Feature types to apply.\n\n                Default: [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\n\n        trans_primitives (list[str or TransformPrimitive], optional):\n            List of Transform Feature functions to apply.\n\n                Default: [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"num_words\", \"num_characters\"]\n\n        groupby_trans_primitives (list[str or TransformPrimitive], optional):\n            list of Transform primitives to make GroupByTransformFeatures with\n\n        allowed_paths (list[list[str]]): Allowed dataframe paths on which to make\n            features.\n\n        max_depth (int) : Maximum allowed depth of features.\n\n        ignore_dataframes (list[str], optional): List of dataframes to\n            blacklist when creating features.\n\n        ignore_columns (dict[str -> list[str]], optional): List of specific\n            columns within each dataframe to blacklist when creating features.\n\n        primitive_options (list[dict[str or tuple[str] -> dict] or dict[str or tuple[str] -> dict, optional]):\n            Specify options for a single primitive or a group of primitives.\n            Lists of option dicts are used to specify options per input for primitives\n            with multiple inputs. Each option ``dict`` can have the following keys:\n\n            ``\"include_dataframes\"``\n                List of dataframes to be included when creating features for\n                the primitive(s). All other dataframes will be ignored\n                (list[str]).\n            ``\"ignore_dataframes\"``\n                List of dataframes to be blacklisted when creating features\n                for the primitive(s) (list[str]).\n            ``\"include_columns\"``\n                List of specific columns within each dataframe to include when\n                creating features for the primitive(s). All other columns\n                in a given dataframe will be ignored (dict[str -> list[str]]).\n            ``\"ignore_columns\"``\n                List of specific columns within each dataframe to blacklist\n                when creating features for the primitive(s) (dict[str ->\n                list[str]]).\n            ``\"include_groupby_dataframes\"``\n                List of dataframes to be included when finding groupbys. All\n                other dataframes will be ignored (list[str]).\n            ``\"ignore_groupby_dataframes\"``\n                List of dataframes to blacklist when finding groupbys\n                (list[str]).\n            ``\"include_groupby_columns\"``\n                List of specific columns within each dataframe to include as\n                groupbys, if applicable. All other columns in each\n                dataframe will be ignored (dict[str -> list[str]]).\n            ``\"ignore_groupby_columns\"``\n                List of specific columns within each dataframe to blacklist\n                as groupbys (dict[str -> list[str]]).\n\n        seed_features (list[:class:`.FeatureBase`]): List of manually defined\n            features to use.\n\n        drop_contains (list[str], optional): Drop features\n            that contains these strings in name.\n\n        drop_exact (list[str], optional): Drop features that\n            exactly match these strings in name.\n\n        where_primitives (list[str or PrimitiveBase], optional):\n            List of Primitives names (or types) to apply with where clauses.\n\n                Default:\n\n                    [\"count\"]\n\n        max_features (int, optional) : Cap the number of generated features to\n                this number. If -1, no limit.\n\n        features_only (bool, optional): If True, returns the list of\n            features without calculating the feature matrix.\n\n        cutoff_time_in_index (bool): If True, return a DataFrame with a MultiIndex\n            where the second index is the cutoff time (first is instance id).\n            DataFrame will be sorted by (time, instance_id).\n\n        training_window (Timedelta or str, optional):\n            Window defining how much time before the cutoff time data\n            can be used when calculating features. If ``None`` , all data\n            before cutoff time is used. Defaults to ``None``. Month and year\n            units are not relative when Pandas Timedeltas are used. Relative\n            units should be passed as a Featuretools Timedelta or a string.\n\n        approximate (Timedelta): Bucket size to group instances with similar\n            cutoff times by for features with costly calculations. For example,\n            if bucket is 24 hours, all instances with cutoff times on the same\n            day will use the same calculation for expensive features.\n\n        save_progress (str, optional): Path to save intermediate computational results.\n\n        n_jobs (int, optional): number of parallel processes to use when\n            calculating feature matrix\n\n        chunk_size (int or float or None or \"cutoff time\", optional): Number\n            of rows of output feature matrix to calculate at time. If passed an\n            integer greater than 0, will try to use that many rows per chunk.\n            If passed a float value between 0 and 1 sets the chunk size to that\n            percentage of all instances. If passed the string \"cutoff time\",\n            rows are split per cutoff time.\n\n        dask_kwargs (dict, optional): Dictionary of keyword arguments to be\n            passed when creating the dask client and scheduler. Even if n_jobs\n            is not set, using `dask_kwargs` will enable multiprocessing.\n            Main parameters:\n\n            cluster (str or dask.distributed.LocalCluster):\n                cluster or address of cluster to send tasks to. If unspecified,\n                a cluster will be created.\n            diagnostics port (int):\n                port number to use for web dashboard.  If left unspecified, web\n                interface will not be enabled.\n\n            Valid keyword arguments for LocalCluster will also be accepted.\n\n        return_types (list[woodwork.ColumnSchema] or str, optional):\n            List of ColumnSchemas defining the types of\n            columns to return. If None, defaults to returning all\n            numeric, categorical and boolean types. If given as\n            the string 'all', returns all available types.\n\n        progress_callback (callable): function to be called with incremental progress updates.\n            Has the following parameters:\n\n                update: percentage change (float between 0 and 100) in progress since last call\n                progress_percent: percentage (float between 0 and 100) of total computation completed\n                time_elapsed: total time in seconds that has elapsed since start of call\n\n        include_cutoff_time (bool): Include data at cutoff times in feature calculations. Defaults to ``True``.\n\n    Returns:\n        list[:class:`.FeatureBase`], pd.DataFrame:\n            The list of generated feature defintions, and the feature matrix.\n            If ``features_only`` is ``True``, the feature matrix will not be generated.\n\n    Examples:\n        .. code-block:: python\n\n            from featuretools.primitives import Mean\n            # cutoff times per instance\n            dataframes = {\n                \"sessions\" : (session_df, \"id\"),\n                \"transactions\" : (transactions_df, \"id\", \"transaction_time\")\n            }\n            relationships = [(\"sessions\", \"id\", \"transactions\", \"session_id\")]\n            feature_matrix, features = dfs(dataframes=dataframes,\n                                           relationships=relationships,\n                                           target_dataframe_name=\"transactions\",\n                                           cutoff_time=cutoff_times)\n            feature_matrix\n\n            features = dfs(dataframes=dataframes,\n                           relationships=relationships,\n                           target_dataframe_name=\"transactions\",\n                           features_only=True)\n    \"\"\"\n    if not isinstance(entityset, EntitySet):\n        entityset = EntitySet('dfs', dataframes, relationships)\n    dfs_object = DeepFeatureSynthesis(target_dataframe_name, entityset, agg_primitives=agg_primitives, trans_primitives=trans_primitives, groupby_trans_primitives=groupby_trans_primitives, max_depth=max_depth, where_primitives=where_primitives, allowed_paths=allowed_paths, drop_exact=drop_exact, drop_contains=drop_contains, ignore_dataframes=ignore_dataframes, ignore_columns=ignore_columns, primitive_options=primitive_options, max_features=max_features, seed_features=seed_features)\n    features = dfs_object.build_features(verbose=verbose, return_types=return_types)\n    (trans, agg, groupby, where) = _categorize_features(features)\n    trans_unused = get_unused_primitives(trans_primitives, trans)\n    agg_unused = get_unused_primitives(agg_primitives, agg)\n    groupby_unused = get_unused_primitives(groupby_trans_primitives, groupby)\n    where_unused = get_unused_primitives(where_primitives, where)\n    unused_primitives = [trans_unused, agg_unused, groupby_unused, where_unused]\n    if any(unused_primitives):\n        warn_unused_primitives(unused_primitives)\n    if features_only:\n        return features\n    assert features != [], 'No features can be generated from the specified primitives. Please make sure the primitives you are using are compatible with the variable types in your data.'\n    feature_matrix = calculate_feature_matrix(features, entityset=entityset, cutoff_time=cutoff_time, instance_ids=instance_ids, training_window=training_window, approximate=approximate, cutoff_time_in_index=cutoff_time_in_index, save_progress=save_progress, chunk_size=chunk_size, n_jobs=n_jobs, dask_kwargs=dask_kwargs, verbose=verbose, progress_callback=progress_callback, include_cutoff_time=include_cutoff_time)\n    return (feature_matrix, features)",
        "mutated": [
            "@entry_point('featuretools_dfs')\ndef dfs(dataframes=None, relationships=None, entityset=None, target_dataframe_name=None, cutoff_time=None, instance_ids=None, agg_primitives=None, trans_primitives=None, groupby_trans_primitives=None, allowed_paths=None, max_depth=2, ignore_dataframes=None, ignore_columns=None, primitive_options=None, seed_features=None, drop_contains=None, drop_exact=None, where_primitives=None, max_features=-1, cutoff_time_in_index=False, save_progress=None, features_only=False, training_window=None, approximate=None, chunk_size=None, n_jobs=1, dask_kwargs=None, verbose=False, return_types=None, progress_callback=None, include_cutoff_time=True):\n    if False:\n        i = 10\n    'Calculates a feature matrix and features given a dictionary of dataframes\\n    and a list of relationships.\\n\\n\\n    Args:\\n        dataframes (dict[str -> tuple(DataFrame, str, str, dict[str -> str/Woodwork.LogicalType], dict[str->str/set], boolean)]):\\n            Dictionary of DataFrames. Entries take the format\\n            {dataframe name -> (dataframe, index column, time_index, logical_types, semantic_tags, make_index)}.\\n            Note that only the dataframe is required. If a Woodwork DataFrame is supplied, any other parameters\\n            will be ignored.\\n\\n        relationships (list[(str, str, str, str)]): List of relationships\\n            between dataframes. List items are a tuple with the format\\n            (parent dataframe name, parent column, child dataframe name, child column).\\n\\n        entityset (EntitySet): An already initialized entityset. Required if\\n            dataframes and relationships are not defined.\\n\\n        target_dataframe_name (str): Name of dataframe on which to make predictions.\\n\\n        cutoff_time (pd.DataFrame or Datetime or str): Specifies times at which to calculate\\n            the features for each instance. The resulting feature matrix will use data\\n            up to and including the cutoff_time. Can either be a DataFrame, a single\\n            value, or a string that can be parsed into a datetime. If a DataFrame is passed\\n            the instance ids for which to calculate features must be in a column with the\\n            same name as the target dataframe index or a column named `instance_id`.\\n            The cutoff time values in the DataFrame must be in a column with the same name as\\n            the target dataframe time index or a column named `time`. If the DataFrame has more\\n            than two columns, any additional columns will be added to the resulting feature\\n            matrix. If a single value is passed, this value will be used for all instances.\\n\\n        instance_ids (list): List of instances on which to calculate features. Only\\n            used if cutoff_time is a single datetime.\\n\\n        agg_primitives (list[str or AggregationPrimitive], optional): List of Aggregation\\n            Feature types to apply.\\n\\n                Default: [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\\n\\n        trans_primitives (list[str or TransformPrimitive], optional):\\n            List of Transform Feature functions to apply.\\n\\n                Default: [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"num_words\", \"num_characters\"]\\n\\n        groupby_trans_primitives (list[str or TransformPrimitive], optional):\\n            list of Transform primitives to make GroupByTransformFeatures with\\n\\n        allowed_paths (list[list[str]]): Allowed dataframe paths on which to make\\n            features.\\n\\n        max_depth (int) : Maximum allowed depth of features.\\n\\n        ignore_dataframes (list[str], optional): List of dataframes to\\n            blacklist when creating features.\\n\\n        ignore_columns (dict[str -> list[str]], optional): List of specific\\n            columns within each dataframe to blacklist when creating features.\\n\\n        primitive_options (list[dict[str or tuple[str] -> dict] or dict[str or tuple[str] -> dict, optional]):\\n            Specify options for a single primitive or a group of primitives.\\n            Lists of option dicts are used to specify options per input for primitives\\n            with multiple inputs. Each option ``dict`` can have the following keys:\\n\\n            ``\"include_dataframes\"``\\n                List of dataframes to be included when creating features for\\n                the primitive(s). All other dataframes will be ignored\\n                (list[str]).\\n            ``\"ignore_dataframes\"``\\n                List of dataframes to be blacklisted when creating features\\n                for the primitive(s) (list[str]).\\n            ``\"include_columns\"``\\n                List of specific columns within each dataframe to include when\\n                creating features for the primitive(s). All other columns\\n                in a given dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                when creating features for the primitive(s) (dict[str ->\\n                list[str]]).\\n            ``\"include_groupby_dataframes\"``\\n                List of dataframes to be included when finding groupbys. All\\n                other dataframes will be ignored (list[str]).\\n            ``\"ignore_groupby_dataframes\"``\\n                List of dataframes to blacklist when finding groupbys\\n                (list[str]).\\n            ``\"include_groupby_columns\"``\\n                List of specific columns within each dataframe to include as\\n                groupbys, if applicable. All other columns in each\\n                dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_groupby_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                as groupbys (dict[str -> list[str]]).\\n\\n        seed_features (list[:class:`.FeatureBase`]): List of manually defined\\n            features to use.\\n\\n        drop_contains (list[str], optional): Drop features\\n            that contains these strings in name.\\n\\n        drop_exact (list[str], optional): Drop features that\\n            exactly match these strings in name.\\n\\n        where_primitives (list[str or PrimitiveBase], optional):\\n            List of Primitives names (or types) to apply with where clauses.\\n\\n                Default:\\n\\n                    [\"count\"]\\n\\n        max_features (int, optional) : Cap the number of generated features to\\n                this number. If -1, no limit.\\n\\n        features_only (bool, optional): If True, returns the list of\\n            features without calculating the feature matrix.\\n\\n        cutoff_time_in_index (bool): If True, return a DataFrame with a MultiIndex\\n            where the second index is the cutoff time (first is instance id).\\n            DataFrame will be sorted by (time, instance_id).\\n\\n        training_window (Timedelta or str, optional):\\n            Window defining how much time before the cutoff time data\\n            can be used when calculating features. If ``None`` , all data\\n            before cutoff time is used. Defaults to ``None``. Month and year\\n            units are not relative when Pandas Timedeltas are used. Relative\\n            units should be passed as a Featuretools Timedelta or a string.\\n\\n        approximate (Timedelta): Bucket size to group instances with similar\\n            cutoff times by for features with costly calculations. For example,\\n            if bucket is 24 hours, all instances with cutoff times on the same\\n            day will use the same calculation for expensive features.\\n\\n        save_progress (str, optional): Path to save intermediate computational results.\\n\\n        n_jobs (int, optional): number of parallel processes to use when\\n            calculating feature matrix\\n\\n        chunk_size (int or float or None or \"cutoff time\", optional): Number\\n            of rows of output feature matrix to calculate at time. If passed an\\n            integer greater than 0, will try to use that many rows per chunk.\\n            If passed a float value between 0 and 1 sets the chunk size to that\\n            percentage of all instances. If passed the string \"cutoff time\",\\n            rows are split per cutoff time.\\n\\n        dask_kwargs (dict, optional): Dictionary of keyword arguments to be\\n            passed when creating the dask client and scheduler. Even if n_jobs\\n            is not set, using `dask_kwargs` will enable multiprocessing.\\n            Main parameters:\\n\\n            cluster (str or dask.distributed.LocalCluster):\\n                cluster or address of cluster to send tasks to. If unspecified,\\n                a cluster will be created.\\n            diagnostics port (int):\\n                port number to use for web dashboard.  If left unspecified, web\\n                interface will not be enabled.\\n\\n            Valid keyword arguments for LocalCluster will also be accepted.\\n\\n        return_types (list[woodwork.ColumnSchema] or str, optional):\\n            List of ColumnSchemas defining the types of\\n            columns to return. If None, defaults to returning all\\n            numeric, categorical and boolean types. If given as\\n            the string \\'all\\', returns all available types.\\n\\n        progress_callback (callable): function to be called with incremental progress updates.\\n            Has the following parameters:\\n\\n                update: percentage change (float between 0 and 100) in progress since last call\\n                progress_percent: percentage (float between 0 and 100) of total computation completed\\n                time_elapsed: total time in seconds that has elapsed since start of call\\n\\n        include_cutoff_time (bool): Include data at cutoff times in feature calculations. Defaults to ``True``.\\n\\n    Returns:\\n        list[:class:`.FeatureBase`], pd.DataFrame:\\n            The list of generated feature defintions, and the feature matrix.\\n            If ``features_only`` is ``True``, the feature matrix will not be generated.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            from featuretools.primitives import Mean\\n            # cutoff times per instance\\n            dataframes = {\\n                \"sessions\" : (session_df, \"id\"),\\n                \"transactions\" : (transactions_df, \"id\", \"transaction_time\")\\n            }\\n            relationships = [(\"sessions\", \"id\", \"transactions\", \"session_id\")]\\n            feature_matrix, features = dfs(dataframes=dataframes,\\n                                           relationships=relationships,\\n                                           target_dataframe_name=\"transactions\",\\n                                           cutoff_time=cutoff_times)\\n            feature_matrix\\n\\n            features = dfs(dataframes=dataframes,\\n                           relationships=relationships,\\n                           target_dataframe_name=\"transactions\",\\n                           features_only=True)\\n    '\n    if not isinstance(entityset, EntitySet):\n        entityset = EntitySet('dfs', dataframes, relationships)\n    dfs_object = DeepFeatureSynthesis(target_dataframe_name, entityset, agg_primitives=agg_primitives, trans_primitives=trans_primitives, groupby_trans_primitives=groupby_trans_primitives, max_depth=max_depth, where_primitives=where_primitives, allowed_paths=allowed_paths, drop_exact=drop_exact, drop_contains=drop_contains, ignore_dataframes=ignore_dataframes, ignore_columns=ignore_columns, primitive_options=primitive_options, max_features=max_features, seed_features=seed_features)\n    features = dfs_object.build_features(verbose=verbose, return_types=return_types)\n    (trans, agg, groupby, where) = _categorize_features(features)\n    trans_unused = get_unused_primitives(trans_primitives, trans)\n    agg_unused = get_unused_primitives(agg_primitives, agg)\n    groupby_unused = get_unused_primitives(groupby_trans_primitives, groupby)\n    where_unused = get_unused_primitives(where_primitives, where)\n    unused_primitives = [trans_unused, agg_unused, groupby_unused, where_unused]\n    if any(unused_primitives):\n        warn_unused_primitives(unused_primitives)\n    if features_only:\n        return features\n    assert features != [], 'No features can be generated from the specified primitives. Please make sure the primitives you are using are compatible with the variable types in your data.'\n    feature_matrix = calculate_feature_matrix(features, entityset=entityset, cutoff_time=cutoff_time, instance_ids=instance_ids, training_window=training_window, approximate=approximate, cutoff_time_in_index=cutoff_time_in_index, save_progress=save_progress, chunk_size=chunk_size, n_jobs=n_jobs, dask_kwargs=dask_kwargs, verbose=verbose, progress_callback=progress_callback, include_cutoff_time=include_cutoff_time)\n    return (feature_matrix, features)",
            "@entry_point('featuretools_dfs')\ndef dfs(dataframes=None, relationships=None, entityset=None, target_dataframe_name=None, cutoff_time=None, instance_ids=None, agg_primitives=None, trans_primitives=None, groupby_trans_primitives=None, allowed_paths=None, max_depth=2, ignore_dataframes=None, ignore_columns=None, primitive_options=None, seed_features=None, drop_contains=None, drop_exact=None, where_primitives=None, max_features=-1, cutoff_time_in_index=False, save_progress=None, features_only=False, training_window=None, approximate=None, chunk_size=None, n_jobs=1, dask_kwargs=None, verbose=False, return_types=None, progress_callback=None, include_cutoff_time=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculates a feature matrix and features given a dictionary of dataframes\\n    and a list of relationships.\\n\\n\\n    Args:\\n        dataframes (dict[str -> tuple(DataFrame, str, str, dict[str -> str/Woodwork.LogicalType], dict[str->str/set], boolean)]):\\n            Dictionary of DataFrames. Entries take the format\\n            {dataframe name -> (dataframe, index column, time_index, logical_types, semantic_tags, make_index)}.\\n            Note that only the dataframe is required. If a Woodwork DataFrame is supplied, any other parameters\\n            will be ignored.\\n\\n        relationships (list[(str, str, str, str)]): List of relationships\\n            between dataframes. List items are a tuple with the format\\n            (parent dataframe name, parent column, child dataframe name, child column).\\n\\n        entityset (EntitySet): An already initialized entityset. Required if\\n            dataframes and relationships are not defined.\\n\\n        target_dataframe_name (str): Name of dataframe on which to make predictions.\\n\\n        cutoff_time (pd.DataFrame or Datetime or str): Specifies times at which to calculate\\n            the features for each instance. The resulting feature matrix will use data\\n            up to and including the cutoff_time. Can either be a DataFrame, a single\\n            value, or a string that can be parsed into a datetime. If a DataFrame is passed\\n            the instance ids for which to calculate features must be in a column with the\\n            same name as the target dataframe index or a column named `instance_id`.\\n            The cutoff time values in the DataFrame must be in a column with the same name as\\n            the target dataframe time index or a column named `time`. If the DataFrame has more\\n            than two columns, any additional columns will be added to the resulting feature\\n            matrix. If a single value is passed, this value will be used for all instances.\\n\\n        instance_ids (list): List of instances on which to calculate features. Only\\n            used if cutoff_time is a single datetime.\\n\\n        agg_primitives (list[str or AggregationPrimitive], optional): List of Aggregation\\n            Feature types to apply.\\n\\n                Default: [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\\n\\n        trans_primitives (list[str or TransformPrimitive], optional):\\n            List of Transform Feature functions to apply.\\n\\n                Default: [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"num_words\", \"num_characters\"]\\n\\n        groupby_trans_primitives (list[str or TransformPrimitive], optional):\\n            list of Transform primitives to make GroupByTransformFeatures with\\n\\n        allowed_paths (list[list[str]]): Allowed dataframe paths on which to make\\n            features.\\n\\n        max_depth (int) : Maximum allowed depth of features.\\n\\n        ignore_dataframes (list[str], optional): List of dataframes to\\n            blacklist when creating features.\\n\\n        ignore_columns (dict[str -> list[str]], optional): List of specific\\n            columns within each dataframe to blacklist when creating features.\\n\\n        primitive_options (list[dict[str or tuple[str] -> dict] or dict[str or tuple[str] -> dict, optional]):\\n            Specify options for a single primitive or a group of primitives.\\n            Lists of option dicts are used to specify options per input for primitives\\n            with multiple inputs. Each option ``dict`` can have the following keys:\\n\\n            ``\"include_dataframes\"``\\n                List of dataframes to be included when creating features for\\n                the primitive(s). All other dataframes will be ignored\\n                (list[str]).\\n            ``\"ignore_dataframes\"``\\n                List of dataframes to be blacklisted when creating features\\n                for the primitive(s) (list[str]).\\n            ``\"include_columns\"``\\n                List of specific columns within each dataframe to include when\\n                creating features for the primitive(s). All other columns\\n                in a given dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                when creating features for the primitive(s) (dict[str ->\\n                list[str]]).\\n            ``\"include_groupby_dataframes\"``\\n                List of dataframes to be included when finding groupbys. All\\n                other dataframes will be ignored (list[str]).\\n            ``\"ignore_groupby_dataframes\"``\\n                List of dataframes to blacklist when finding groupbys\\n                (list[str]).\\n            ``\"include_groupby_columns\"``\\n                List of specific columns within each dataframe to include as\\n                groupbys, if applicable. All other columns in each\\n                dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_groupby_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                as groupbys (dict[str -> list[str]]).\\n\\n        seed_features (list[:class:`.FeatureBase`]): List of manually defined\\n            features to use.\\n\\n        drop_contains (list[str], optional): Drop features\\n            that contains these strings in name.\\n\\n        drop_exact (list[str], optional): Drop features that\\n            exactly match these strings in name.\\n\\n        where_primitives (list[str or PrimitiveBase], optional):\\n            List of Primitives names (or types) to apply with where clauses.\\n\\n                Default:\\n\\n                    [\"count\"]\\n\\n        max_features (int, optional) : Cap the number of generated features to\\n                this number. If -1, no limit.\\n\\n        features_only (bool, optional): If True, returns the list of\\n            features without calculating the feature matrix.\\n\\n        cutoff_time_in_index (bool): If True, return a DataFrame with a MultiIndex\\n            where the second index is the cutoff time (first is instance id).\\n            DataFrame will be sorted by (time, instance_id).\\n\\n        training_window (Timedelta or str, optional):\\n            Window defining how much time before the cutoff time data\\n            can be used when calculating features. If ``None`` , all data\\n            before cutoff time is used. Defaults to ``None``. Month and year\\n            units are not relative when Pandas Timedeltas are used. Relative\\n            units should be passed as a Featuretools Timedelta or a string.\\n\\n        approximate (Timedelta): Bucket size to group instances with similar\\n            cutoff times by for features with costly calculations. For example,\\n            if bucket is 24 hours, all instances with cutoff times on the same\\n            day will use the same calculation for expensive features.\\n\\n        save_progress (str, optional): Path to save intermediate computational results.\\n\\n        n_jobs (int, optional): number of parallel processes to use when\\n            calculating feature matrix\\n\\n        chunk_size (int or float or None or \"cutoff time\", optional): Number\\n            of rows of output feature matrix to calculate at time. If passed an\\n            integer greater than 0, will try to use that many rows per chunk.\\n            If passed a float value between 0 and 1 sets the chunk size to that\\n            percentage of all instances. If passed the string \"cutoff time\",\\n            rows are split per cutoff time.\\n\\n        dask_kwargs (dict, optional): Dictionary of keyword arguments to be\\n            passed when creating the dask client and scheduler. Even if n_jobs\\n            is not set, using `dask_kwargs` will enable multiprocessing.\\n            Main parameters:\\n\\n            cluster (str or dask.distributed.LocalCluster):\\n                cluster or address of cluster to send tasks to. If unspecified,\\n                a cluster will be created.\\n            diagnostics port (int):\\n                port number to use for web dashboard.  If left unspecified, web\\n                interface will not be enabled.\\n\\n            Valid keyword arguments for LocalCluster will also be accepted.\\n\\n        return_types (list[woodwork.ColumnSchema] or str, optional):\\n            List of ColumnSchemas defining the types of\\n            columns to return. If None, defaults to returning all\\n            numeric, categorical and boolean types. If given as\\n            the string \\'all\\', returns all available types.\\n\\n        progress_callback (callable): function to be called with incremental progress updates.\\n            Has the following parameters:\\n\\n                update: percentage change (float between 0 and 100) in progress since last call\\n                progress_percent: percentage (float between 0 and 100) of total computation completed\\n                time_elapsed: total time in seconds that has elapsed since start of call\\n\\n        include_cutoff_time (bool): Include data at cutoff times in feature calculations. Defaults to ``True``.\\n\\n    Returns:\\n        list[:class:`.FeatureBase`], pd.DataFrame:\\n            The list of generated feature defintions, and the feature matrix.\\n            If ``features_only`` is ``True``, the feature matrix will not be generated.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            from featuretools.primitives import Mean\\n            # cutoff times per instance\\n            dataframes = {\\n                \"sessions\" : (session_df, \"id\"),\\n                \"transactions\" : (transactions_df, \"id\", \"transaction_time\")\\n            }\\n            relationships = [(\"sessions\", \"id\", \"transactions\", \"session_id\")]\\n            feature_matrix, features = dfs(dataframes=dataframes,\\n                                           relationships=relationships,\\n                                           target_dataframe_name=\"transactions\",\\n                                           cutoff_time=cutoff_times)\\n            feature_matrix\\n\\n            features = dfs(dataframes=dataframes,\\n                           relationships=relationships,\\n                           target_dataframe_name=\"transactions\",\\n                           features_only=True)\\n    '\n    if not isinstance(entityset, EntitySet):\n        entityset = EntitySet('dfs', dataframes, relationships)\n    dfs_object = DeepFeatureSynthesis(target_dataframe_name, entityset, agg_primitives=agg_primitives, trans_primitives=trans_primitives, groupby_trans_primitives=groupby_trans_primitives, max_depth=max_depth, where_primitives=where_primitives, allowed_paths=allowed_paths, drop_exact=drop_exact, drop_contains=drop_contains, ignore_dataframes=ignore_dataframes, ignore_columns=ignore_columns, primitive_options=primitive_options, max_features=max_features, seed_features=seed_features)\n    features = dfs_object.build_features(verbose=verbose, return_types=return_types)\n    (trans, agg, groupby, where) = _categorize_features(features)\n    trans_unused = get_unused_primitives(trans_primitives, trans)\n    agg_unused = get_unused_primitives(agg_primitives, agg)\n    groupby_unused = get_unused_primitives(groupby_trans_primitives, groupby)\n    where_unused = get_unused_primitives(where_primitives, where)\n    unused_primitives = [trans_unused, agg_unused, groupby_unused, where_unused]\n    if any(unused_primitives):\n        warn_unused_primitives(unused_primitives)\n    if features_only:\n        return features\n    assert features != [], 'No features can be generated from the specified primitives. Please make sure the primitives you are using are compatible with the variable types in your data.'\n    feature_matrix = calculate_feature_matrix(features, entityset=entityset, cutoff_time=cutoff_time, instance_ids=instance_ids, training_window=training_window, approximate=approximate, cutoff_time_in_index=cutoff_time_in_index, save_progress=save_progress, chunk_size=chunk_size, n_jobs=n_jobs, dask_kwargs=dask_kwargs, verbose=verbose, progress_callback=progress_callback, include_cutoff_time=include_cutoff_time)\n    return (feature_matrix, features)",
            "@entry_point('featuretools_dfs')\ndef dfs(dataframes=None, relationships=None, entityset=None, target_dataframe_name=None, cutoff_time=None, instance_ids=None, agg_primitives=None, trans_primitives=None, groupby_trans_primitives=None, allowed_paths=None, max_depth=2, ignore_dataframes=None, ignore_columns=None, primitive_options=None, seed_features=None, drop_contains=None, drop_exact=None, where_primitives=None, max_features=-1, cutoff_time_in_index=False, save_progress=None, features_only=False, training_window=None, approximate=None, chunk_size=None, n_jobs=1, dask_kwargs=None, verbose=False, return_types=None, progress_callback=None, include_cutoff_time=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculates a feature matrix and features given a dictionary of dataframes\\n    and a list of relationships.\\n\\n\\n    Args:\\n        dataframes (dict[str -> tuple(DataFrame, str, str, dict[str -> str/Woodwork.LogicalType], dict[str->str/set], boolean)]):\\n            Dictionary of DataFrames. Entries take the format\\n            {dataframe name -> (dataframe, index column, time_index, logical_types, semantic_tags, make_index)}.\\n            Note that only the dataframe is required. If a Woodwork DataFrame is supplied, any other parameters\\n            will be ignored.\\n\\n        relationships (list[(str, str, str, str)]): List of relationships\\n            between dataframes. List items are a tuple with the format\\n            (parent dataframe name, parent column, child dataframe name, child column).\\n\\n        entityset (EntitySet): An already initialized entityset. Required if\\n            dataframes and relationships are not defined.\\n\\n        target_dataframe_name (str): Name of dataframe on which to make predictions.\\n\\n        cutoff_time (pd.DataFrame or Datetime or str): Specifies times at which to calculate\\n            the features for each instance. The resulting feature matrix will use data\\n            up to and including the cutoff_time. Can either be a DataFrame, a single\\n            value, or a string that can be parsed into a datetime. If a DataFrame is passed\\n            the instance ids for which to calculate features must be in a column with the\\n            same name as the target dataframe index or a column named `instance_id`.\\n            The cutoff time values in the DataFrame must be in a column with the same name as\\n            the target dataframe time index or a column named `time`. If the DataFrame has more\\n            than two columns, any additional columns will be added to the resulting feature\\n            matrix. If a single value is passed, this value will be used for all instances.\\n\\n        instance_ids (list): List of instances on which to calculate features. Only\\n            used if cutoff_time is a single datetime.\\n\\n        agg_primitives (list[str or AggregationPrimitive], optional): List of Aggregation\\n            Feature types to apply.\\n\\n                Default: [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\\n\\n        trans_primitives (list[str or TransformPrimitive], optional):\\n            List of Transform Feature functions to apply.\\n\\n                Default: [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"num_words\", \"num_characters\"]\\n\\n        groupby_trans_primitives (list[str or TransformPrimitive], optional):\\n            list of Transform primitives to make GroupByTransformFeatures with\\n\\n        allowed_paths (list[list[str]]): Allowed dataframe paths on which to make\\n            features.\\n\\n        max_depth (int) : Maximum allowed depth of features.\\n\\n        ignore_dataframes (list[str], optional): List of dataframes to\\n            blacklist when creating features.\\n\\n        ignore_columns (dict[str -> list[str]], optional): List of specific\\n            columns within each dataframe to blacklist when creating features.\\n\\n        primitive_options (list[dict[str or tuple[str] -> dict] or dict[str or tuple[str] -> dict, optional]):\\n            Specify options for a single primitive or a group of primitives.\\n            Lists of option dicts are used to specify options per input for primitives\\n            with multiple inputs. Each option ``dict`` can have the following keys:\\n\\n            ``\"include_dataframes\"``\\n                List of dataframes to be included when creating features for\\n                the primitive(s). All other dataframes will be ignored\\n                (list[str]).\\n            ``\"ignore_dataframes\"``\\n                List of dataframes to be blacklisted when creating features\\n                for the primitive(s) (list[str]).\\n            ``\"include_columns\"``\\n                List of specific columns within each dataframe to include when\\n                creating features for the primitive(s). All other columns\\n                in a given dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                when creating features for the primitive(s) (dict[str ->\\n                list[str]]).\\n            ``\"include_groupby_dataframes\"``\\n                List of dataframes to be included when finding groupbys. All\\n                other dataframes will be ignored (list[str]).\\n            ``\"ignore_groupby_dataframes\"``\\n                List of dataframes to blacklist when finding groupbys\\n                (list[str]).\\n            ``\"include_groupby_columns\"``\\n                List of specific columns within each dataframe to include as\\n                groupbys, if applicable. All other columns in each\\n                dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_groupby_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                as groupbys (dict[str -> list[str]]).\\n\\n        seed_features (list[:class:`.FeatureBase`]): List of manually defined\\n            features to use.\\n\\n        drop_contains (list[str], optional): Drop features\\n            that contains these strings in name.\\n\\n        drop_exact (list[str], optional): Drop features that\\n            exactly match these strings in name.\\n\\n        where_primitives (list[str or PrimitiveBase], optional):\\n            List of Primitives names (or types) to apply with where clauses.\\n\\n                Default:\\n\\n                    [\"count\"]\\n\\n        max_features (int, optional) : Cap the number of generated features to\\n                this number. If -1, no limit.\\n\\n        features_only (bool, optional): If True, returns the list of\\n            features without calculating the feature matrix.\\n\\n        cutoff_time_in_index (bool): If True, return a DataFrame with a MultiIndex\\n            where the second index is the cutoff time (first is instance id).\\n            DataFrame will be sorted by (time, instance_id).\\n\\n        training_window (Timedelta or str, optional):\\n            Window defining how much time before the cutoff time data\\n            can be used when calculating features. If ``None`` , all data\\n            before cutoff time is used. Defaults to ``None``. Month and year\\n            units are not relative when Pandas Timedeltas are used. Relative\\n            units should be passed as a Featuretools Timedelta or a string.\\n\\n        approximate (Timedelta): Bucket size to group instances with similar\\n            cutoff times by for features with costly calculations. For example,\\n            if bucket is 24 hours, all instances with cutoff times on the same\\n            day will use the same calculation for expensive features.\\n\\n        save_progress (str, optional): Path to save intermediate computational results.\\n\\n        n_jobs (int, optional): number of parallel processes to use when\\n            calculating feature matrix\\n\\n        chunk_size (int or float or None or \"cutoff time\", optional): Number\\n            of rows of output feature matrix to calculate at time. If passed an\\n            integer greater than 0, will try to use that many rows per chunk.\\n            If passed a float value between 0 and 1 sets the chunk size to that\\n            percentage of all instances. If passed the string \"cutoff time\",\\n            rows are split per cutoff time.\\n\\n        dask_kwargs (dict, optional): Dictionary of keyword arguments to be\\n            passed when creating the dask client and scheduler. Even if n_jobs\\n            is not set, using `dask_kwargs` will enable multiprocessing.\\n            Main parameters:\\n\\n            cluster (str or dask.distributed.LocalCluster):\\n                cluster or address of cluster to send tasks to. If unspecified,\\n                a cluster will be created.\\n            diagnostics port (int):\\n                port number to use for web dashboard.  If left unspecified, web\\n                interface will not be enabled.\\n\\n            Valid keyword arguments for LocalCluster will also be accepted.\\n\\n        return_types (list[woodwork.ColumnSchema] or str, optional):\\n            List of ColumnSchemas defining the types of\\n            columns to return. If None, defaults to returning all\\n            numeric, categorical and boolean types. If given as\\n            the string \\'all\\', returns all available types.\\n\\n        progress_callback (callable): function to be called with incremental progress updates.\\n            Has the following parameters:\\n\\n                update: percentage change (float between 0 and 100) in progress since last call\\n                progress_percent: percentage (float between 0 and 100) of total computation completed\\n                time_elapsed: total time in seconds that has elapsed since start of call\\n\\n        include_cutoff_time (bool): Include data at cutoff times in feature calculations. Defaults to ``True``.\\n\\n    Returns:\\n        list[:class:`.FeatureBase`], pd.DataFrame:\\n            The list of generated feature defintions, and the feature matrix.\\n            If ``features_only`` is ``True``, the feature matrix will not be generated.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            from featuretools.primitives import Mean\\n            # cutoff times per instance\\n            dataframes = {\\n                \"sessions\" : (session_df, \"id\"),\\n                \"transactions\" : (transactions_df, \"id\", \"transaction_time\")\\n            }\\n            relationships = [(\"sessions\", \"id\", \"transactions\", \"session_id\")]\\n            feature_matrix, features = dfs(dataframes=dataframes,\\n                                           relationships=relationships,\\n                                           target_dataframe_name=\"transactions\",\\n                                           cutoff_time=cutoff_times)\\n            feature_matrix\\n\\n            features = dfs(dataframes=dataframes,\\n                           relationships=relationships,\\n                           target_dataframe_name=\"transactions\",\\n                           features_only=True)\\n    '\n    if not isinstance(entityset, EntitySet):\n        entityset = EntitySet('dfs', dataframes, relationships)\n    dfs_object = DeepFeatureSynthesis(target_dataframe_name, entityset, agg_primitives=agg_primitives, trans_primitives=trans_primitives, groupby_trans_primitives=groupby_trans_primitives, max_depth=max_depth, where_primitives=where_primitives, allowed_paths=allowed_paths, drop_exact=drop_exact, drop_contains=drop_contains, ignore_dataframes=ignore_dataframes, ignore_columns=ignore_columns, primitive_options=primitive_options, max_features=max_features, seed_features=seed_features)\n    features = dfs_object.build_features(verbose=verbose, return_types=return_types)\n    (trans, agg, groupby, where) = _categorize_features(features)\n    trans_unused = get_unused_primitives(trans_primitives, trans)\n    agg_unused = get_unused_primitives(agg_primitives, agg)\n    groupby_unused = get_unused_primitives(groupby_trans_primitives, groupby)\n    where_unused = get_unused_primitives(where_primitives, where)\n    unused_primitives = [trans_unused, agg_unused, groupby_unused, where_unused]\n    if any(unused_primitives):\n        warn_unused_primitives(unused_primitives)\n    if features_only:\n        return features\n    assert features != [], 'No features can be generated from the specified primitives. Please make sure the primitives you are using are compatible with the variable types in your data.'\n    feature_matrix = calculate_feature_matrix(features, entityset=entityset, cutoff_time=cutoff_time, instance_ids=instance_ids, training_window=training_window, approximate=approximate, cutoff_time_in_index=cutoff_time_in_index, save_progress=save_progress, chunk_size=chunk_size, n_jobs=n_jobs, dask_kwargs=dask_kwargs, verbose=verbose, progress_callback=progress_callback, include_cutoff_time=include_cutoff_time)\n    return (feature_matrix, features)",
            "@entry_point('featuretools_dfs')\ndef dfs(dataframes=None, relationships=None, entityset=None, target_dataframe_name=None, cutoff_time=None, instance_ids=None, agg_primitives=None, trans_primitives=None, groupby_trans_primitives=None, allowed_paths=None, max_depth=2, ignore_dataframes=None, ignore_columns=None, primitive_options=None, seed_features=None, drop_contains=None, drop_exact=None, where_primitives=None, max_features=-1, cutoff_time_in_index=False, save_progress=None, features_only=False, training_window=None, approximate=None, chunk_size=None, n_jobs=1, dask_kwargs=None, verbose=False, return_types=None, progress_callback=None, include_cutoff_time=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculates a feature matrix and features given a dictionary of dataframes\\n    and a list of relationships.\\n\\n\\n    Args:\\n        dataframes (dict[str -> tuple(DataFrame, str, str, dict[str -> str/Woodwork.LogicalType], dict[str->str/set], boolean)]):\\n            Dictionary of DataFrames. Entries take the format\\n            {dataframe name -> (dataframe, index column, time_index, logical_types, semantic_tags, make_index)}.\\n            Note that only the dataframe is required. If a Woodwork DataFrame is supplied, any other parameters\\n            will be ignored.\\n\\n        relationships (list[(str, str, str, str)]): List of relationships\\n            between dataframes. List items are a tuple with the format\\n            (parent dataframe name, parent column, child dataframe name, child column).\\n\\n        entityset (EntitySet): An already initialized entityset. Required if\\n            dataframes and relationships are not defined.\\n\\n        target_dataframe_name (str): Name of dataframe on which to make predictions.\\n\\n        cutoff_time (pd.DataFrame or Datetime or str): Specifies times at which to calculate\\n            the features for each instance. The resulting feature matrix will use data\\n            up to and including the cutoff_time. Can either be a DataFrame, a single\\n            value, or a string that can be parsed into a datetime. If a DataFrame is passed\\n            the instance ids for which to calculate features must be in a column with the\\n            same name as the target dataframe index or a column named `instance_id`.\\n            The cutoff time values in the DataFrame must be in a column with the same name as\\n            the target dataframe time index or a column named `time`. If the DataFrame has more\\n            than two columns, any additional columns will be added to the resulting feature\\n            matrix. If a single value is passed, this value will be used for all instances.\\n\\n        instance_ids (list): List of instances on which to calculate features. Only\\n            used if cutoff_time is a single datetime.\\n\\n        agg_primitives (list[str or AggregationPrimitive], optional): List of Aggregation\\n            Feature types to apply.\\n\\n                Default: [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\\n\\n        trans_primitives (list[str or TransformPrimitive], optional):\\n            List of Transform Feature functions to apply.\\n\\n                Default: [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"num_words\", \"num_characters\"]\\n\\n        groupby_trans_primitives (list[str or TransformPrimitive], optional):\\n            list of Transform primitives to make GroupByTransformFeatures with\\n\\n        allowed_paths (list[list[str]]): Allowed dataframe paths on which to make\\n            features.\\n\\n        max_depth (int) : Maximum allowed depth of features.\\n\\n        ignore_dataframes (list[str], optional): List of dataframes to\\n            blacklist when creating features.\\n\\n        ignore_columns (dict[str -> list[str]], optional): List of specific\\n            columns within each dataframe to blacklist when creating features.\\n\\n        primitive_options (list[dict[str or tuple[str] -> dict] or dict[str or tuple[str] -> dict, optional]):\\n            Specify options for a single primitive or a group of primitives.\\n            Lists of option dicts are used to specify options per input for primitives\\n            with multiple inputs. Each option ``dict`` can have the following keys:\\n\\n            ``\"include_dataframes\"``\\n                List of dataframes to be included when creating features for\\n                the primitive(s). All other dataframes will be ignored\\n                (list[str]).\\n            ``\"ignore_dataframes\"``\\n                List of dataframes to be blacklisted when creating features\\n                for the primitive(s) (list[str]).\\n            ``\"include_columns\"``\\n                List of specific columns within each dataframe to include when\\n                creating features for the primitive(s). All other columns\\n                in a given dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                when creating features for the primitive(s) (dict[str ->\\n                list[str]]).\\n            ``\"include_groupby_dataframes\"``\\n                List of dataframes to be included when finding groupbys. All\\n                other dataframes will be ignored (list[str]).\\n            ``\"ignore_groupby_dataframes\"``\\n                List of dataframes to blacklist when finding groupbys\\n                (list[str]).\\n            ``\"include_groupby_columns\"``\\n                List of specific columns within each dataframe to include as\\n                groupbys, if applicable. All other columns in each\\n                dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_groupby_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                as groupbys (dict[str -> list[str]]).\\n\\n        seed_features (list[:class:`.FeatureBase`]): List of manually defined\\n            features to use.\\n\\n        drop_contains (list[str], optional): Drop features\\n            that contains these strings in name.\\n\\n        drop_exact (list[str], optional): Drop features that\\n            exactly match these strings in name.\\n\\n        where_primitives (list[str or PrimitiveBase], optional):\\n            List of Primitives names (or types) to apply with where clauses.\\n\\n                Default:\\n\\n                    [\"count\"]\\n\\n        max_features (int, optional) : Cap the number of generated features to\\n                this number. If -1, no limit.\\n\\n        features_only (bool, optional): If True, returns the list of\\n            features without calculating the feature matrix.\\n\\n        cutoff_time_in_index (bool): If True, return a DataFrame with a MultiIndex\\n            where the second index is the cutoff time (first is instance id).\\n            DataFrame will be sorted by (time, instance_id).\\n\\n        training_window (Timedelta or str, optional):\\n            Window defining how much time before the cutoff time data\\n            can be used when calculating features. If ``None`` , all data\\n            before cutoff time is used. Defaults to ``None``. Month and year\\n            units are not relative when Pandas Timedeltas are used. Relative\\n            units should be passed as a Featuretools Timedelta or a string.\\n\\n        approximate (Timedelta): Bucket size to group instances with similar\\n            cutoff times by for features with costly calculations. For example,\\n            if bucket is 24 hours, all instances with cutoff times on the same\\n            day will use the same calculation for expensive features.\\n\\n        save_progress (str, optional): Path to save intermediate computational results.\\n\\n        n_jobs (int, optional): number of parallel processes to use when\\n            calculating feature matrix\\n\\n        chunk_size (int or float or None or \"cutoff time\", optional): Number\\n            of rows of output feature matrix to calculate at time. If passed an\\n            integer greater than 0, will try to use that many rows per chunk.\\n            If passed a float value between 0 and 1 sets the chunk size to that\\n            percentage of all instances. If passed the string \"cutoff time\",\\n            rows are split per cutoff time.\\n\\n        dask_kwargs (dict, optional): Dictionary of keyword arguments to be\\n            passed when creating the dask client and scheduler. Even if n_jobs\\n            is not set, using `dask_kwargs` will enable multiprocessing.\\n            Main parameters:\\n\\n            cluster (str or dask.distributed.LocalCluster):\\n                cluster or address of cluster to send tasks to. If unspecified,\\n                a cluster will be created.\\n            diagnostics port (int):\\n                port number to use for web dashboard.  If left unspecified, web\\n                interface will not be enabled.\\n\\n            Valid keyword arguments for LocalCluster will also be accepted.\\n\\n        return_types (list[woodwork.ColumnSchema] or str, optional):\\n            List of ColumnSchemas defining the types of\\n            columns to return. If None, defaults to returning all\\n            numeric, categorical and boolean types. If given as\\n            the string \\'all\\', returns all available types.\\n\\n        progress_callback (callable): function to be called with incremental progress updates.\\n            Has the following parameters:\\n\\n                update: percentage change (float between 0 and 100) in progress since last call\\n                progress_percent: percentage (float between 0 and 100) of total computation completed\\n                time_elapsed: total time in seconds that has elapsed since start of call\\n\\n        include_cutoff_time (bool): Include data at cutoff times in feature calculations. Defaults to ``True``.\\n\\n    Returns:\\n        list[:class:`.FeatureBase`], pd.DataFrame:\\n            The list of generated feature defintions, and the feature matrix.\\n            If ``features_only`` is ``True``, the feature matrix will not be generated.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            from featuretools.primitives import Mean\\n            # cutoff times per instance\\n            dataframes = {\\n                \"sessions\" : (session_df, \"id\"),\\n                \"transactions\" : (transactions_df, \"id\", \"transaction_time\")\\n            }\\n            relationships = [(\"sessions\", \"id\", \"transactions\", \"session_id\")]\\n            feature_matrix, features = dfs(dataframes=dataframes,\\n                                           relationships=relationships,\\n                                           target_dataframe_name=\"transactions\",\\n                                           cutoff_time=cutoff_times)\\n            feature_matrix\\n\\n            features = dfs(dataframes=dataframes,\\n                           relationships=relationships,\\n                           target_dataframe_name=\"transactions\",\\n                           features_only=True)\\n    '\n    if not isinstance(entityset, EntitySet):\n        entityset = EntitySet('dfs', dataframes, relationships)\n    dfs_object = DeepFeatureSynthesis(target_dataframe_name, entityset, agg_primitives=agg_primitives, trans_primitives=trans_primitives, groupby_trans_primitives=groupby_trans_primitives, max_depth=max_depth, where_primitives=where_primitives, allowed_paths=allowed_paths, drop_exact=drop_exact, drop_contains=drop_contains, ignore_dataframes=ignore_dataframes, ignore_columns=ignore_columns, primitive_options=primitive_options, max_features=max_features, seed_features=seed_features)\n    features = dfs_object.build_features(verbose=verbose, return_types=return_types)\n    (trans, agg, groupby, where) = _categorize_features(features)\n    trans_unused = get_unused_primitives(trans_primitives, trans)\n    agg_unused = get_unused_primitives(agg_primitives, agg)\n    groupby_unused = get_unused_primitives(groupby_trans_primitives, groupby)\n    where_unused = get_unused_primitives(where_primitives, where)\n    unused_primitives = [trans_unused, agg_unused, groupby_unused, where_unused]\n    if any(unused_primitives):\n        warn_unused_primitives(unused_primitives)\n    if features_only:\n        return features\n    assert features != [], 'No features can be generated from the specified primitives. Please make sure the primitives you are using are compatible with the variable types in your data.'\n    feature_matrix = calculate_feature_matrix(features, entityset=entityset, cutoff_time=cutoff_time, instance_ids=instance_ids, training_window=training_window, approximate=approximate, cutoff_time_in_index=cutoff_time_in_index, save_progress=save_progress, chunk_size=chunk_size, n_jobs=n_jobs, dask_kwargs=dask_kwargs, verbose=verbose, progress_callback=progress_callback, include_cutoff_time=include_cutoff_time)\n    return (feature_matrix, features)",
            "@entry_point('featuretools_dfs')\ndef dfs(dataframes=None, relationships=None, entityset=None, target_dataframe_name=None, cutoff_time=None, instance_ids=None, agg_primitives=None, trans_primitives=None, groupby_trans_primitives=None, allowed_paths=None, max_depth=2, ignore_dataframes=None, ignore_columns=None, primitive_options=None, seed_features=None, drop_contains=None, drop_exact=None, where_primitives=None, max_features=-1, cutoff_time_in_index=False, save_progress=None, features_only=False, training_window=None, approximate=None, chunk_size=None, n_jobs=1, dask_kwargs=None, verbose=False, return_types=None, progress_callback=None, include_cutoff_time=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculates a feature matrix and features given a dictionary of dataframes\\n    and a list of relationships.\\n\\n\\n    Args:\\n        dataframes (dict[str -> tuple(DataFrame, str, str, dict[str -> str/Woodwork.LogicalType], dict[str->str/set], boolean)]):\\n            Dictionary of DataFrames. Entries take the format\\n            {dataframe name -> (dataframe, index column, time_index, logical_types, semantic_tags, make_index)}.\\n            Note that only the dataframe is required. If a Woodwork DataFrame is supplied, any other parameters\\n            will be ignored.\\n\\n        relationships (list[(str, str, str, str)]): List of relationships\\n            between dataframes. List items are a tuple with the format\\n            (parent dataframe name, parent column, child dataframe name, child column).\\n\\n        entityset (EntitySet): An already initialized entityset. Required if\\n            dataframes and relationships are not defined.\\n\\n        target_dataframe_name (str): Name of dataframe on which to make predictions.\\n\\n        cutoff_time (pd.DataFrame or Datetime or str): Specifies times at which to calculate\\n            the features for each instance. The resulting feature matrix will use data\\n            up to and including the cutoff_time. Can either be a DataFrame, a single\\n            value, or a string that can be parsed into a datetime. If a DataFrame is passed\\n            the instance ids for which to calculate features must be in a column with the\\n            same name as the target dataframe index or a column named `instance_id`.\\n            The cutoff time values in the DataFrame must be in a column with the same name as\\n            the target dataframe time index or a column named `time`. If the DataFrame has more\\n            than two columns, any additional columns will be added to the resulting feature\\n            matrix. If a single value is passed, this value will be used for all instances.\\n\\n        instance_ids (list): List of instances on which to calculate features. Only\\n            used if cutoff_time is a single datetime.\\n\\n        agg_primitives (list[str or AggregationPrimitive], optional): List of Aggregation\\n            Feature types to apply.\\n\\n                Default: [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\\n\\n        trans_primitives (list[str or TransformPrimitive], optional):\\n            List of Transform Feature functions to apply.\\n\\n                Default: [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"num_words\", \"num_characters\"]\\n\\n        groupby_trans_primitives (list[str or TransformPrimitive], optional):\\n            list of Transform primitives to make GroupByTransformFeatures with\\n\\n        allowed_paths (list[list[str]]): Allowed dataframe paths on which to make\\n            features.\\n\\n        max_depth (int) : Maximum allowed depth of features.\\n\\n        ignore_dataframes (list[str], optional): List of dataframes to\\n            blacklist when creating features.\\n\\n        ignore_columns (dict[str -> list[str]], optional): List of specific\\n            columns within each dataframe to blacklist when creating features.\\n\\n        primitive_options (list[dict[str or tuple[str] -> dict] or dict[str or tuple[str] -> dict, optional]):\\n            Specify options for a single primitive or a group of primitives.\\n            Lists of option dicts are used to specify options per input for primitives\\n            with multiple inputs. Each option ``dict`` can have the following keys:\\n\\n            ``\"include_dataframes\"``\\n                List of dataframes to be included when creating features for\\n                the primitive(s). All other dataframes will be ignored\\n                (list[str]).\\n            ``\"ignore_dataframes\"``\\n                List of dataframes to be blacklisted when creating features\\n                for the primitive(s) (list[str]).\\n            ``\"include_columns\"``\\n                List of specific columns within each dataframe to include when\\n                creating features for the primitive(s). All other columns\\n                in a given dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                when creating features for the primitive(s) (dict[str ->\\n                list[str]]).\\n            ``\"include_groupby_dataframes\"``\\n                List of dataframes to be included when finding groupbys. All\\n                other dataframes will be ignored (list[str]).\\n            ``\"ignore_groupby_dataframes\"``\\n                List of dataframes to blacklist when finding groupbys\\n                (list[str]).\\n            ``\"include_groupby_columns\"``\\n                List of specific columns within each dataframe to include as\\n                groupbys, if applicable. All other columns in each\\n                dataframe will be ignored (dict[str -> list[str]]).\\n            ``\"ignore_groupby_columns\"``\\n                List of specific columns within each dataframe to blacklist\\n                as groupbys (dict[str -> list[str]]).\\n\\n        seed_features (list[:class:`.FeatureBase`]): List of manually defined\\n            features to use.\\n\\n        drop_contains (list[str], optional): Drop features\\n            that contains these strings in name.\\n\\n        drop_exact (list[str], optional): Drop features that\\n            exactly match these strings in name.\\n\\n        where_primitives (list[str or PrimitiveBase], optional):\\n            List of Primitives names (or types) to apply with where clauses.\\n\\n                Default:\\n\\n                    [\"count\"]\\n\\n        max_features (int, optional) : Cap the number of generated features to\\n                this number. If -1, no limit.\\n\\n        features_only (bool, optional): If True, returns the list of\\n            features without calculating the feature matrix.\\n\\n        cutoff_time_in_index (bool): If True, return a DataFrame with a MultiIndex\\n            where the second index is the cutoff time (first is instance id).\\n            DataFrame will be sorted by (time, instance_id).\\n\\n        training_window (Timedelta or str, optional):\\n            Window defining how much time before the cutoff time data\\n            can be used when calculating features. If ``None`` , all data\\n            before cutoff time is used. Defaults to ``None``. Month and year\\n            units are not relative when Pandas Timedeltas are used. Relative\\n            units should be passed as a Featuretools Timedelta or a string.\\n\\n        approximate (Timedelta): Bucket size to group instances with similar\\n            cutoff times by for features with costly calculations. For example,\\n            if bucket is 24 hours, all instances with cutoff times on the same\\n            day will use the same calculation for expensive features.\\n\\n        save_progress (str, optional): Path to save intermediate computational results.\\n\\n        n_jobs (int, optional): number of parallel processes to use when\\n            calculating feature matrix\\n\\n        chunk_size (int or float or None or \"cutoff time\", optional): Number\\n            of rows of output feature matrix to calculate at time. If passed an\\n            integer greater than 0, will try to use that many rows per chunk.\\n            If passed a float value between 0 and 1 sets the chunk size to that\\n            percentage of all instances. If passed the string \"cutoff time\",\\n            rows are split per cutoff time.\\n\\n        dask_kwargs (dict, optional): Dictionary of keyword arguments to be\\n            passed when creating the dask client and scheduler. Even if n_jobs\\n            is not set, using `dask_kwargs` will enable multiprocessing.\\n            Main parameters:\\n\\n            cluster (str or dask.distributed.LocalCluster):\\n                cluster or address of cluster to send tasks to. If unspecified,\\n                a cluster will be created.\\n            diagnostics port (int):\\n                port number to use for web dashboard.  If left unspecified, web\\n                interface will not be enabled.\\n\\n            Valid keyword arguments for LocalCluster will also be accepted.\\n\\n        return_types (list[woodwork.ColumnSchema] or str, optional):\\n            List of ColumnSchemas defining the types of\\n            columns to return. If None, defaults to returning all\\n            numeric, categorical and boolean types. If given as\\n            the string \\'all\\', returns all available types.\\n\\n        progress_callback (callable): function to be called with incremental progress updates.\\n            Has the following parameters:\\n\\n                update: percentage change (float between 0 and 100) in progress since last call\\n                progress_percent: percentage (float between 0 and 100) of total computation completed\\n                time_elapsed: total time in seconds that has elapsed since start of call\\n\\n        include_cutoff_time (bool): Include data at cutoff times in feature calculations. Defaults to ``True``.\\n\\n    Returns:\\n        list[:class:`.FeatureBase`], pd.DataFrame:\\n            The list of generated feature defintions, and the feature matrix.\\n            If ``features_only`` is ``True``, the feature matrix will not be generated.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            from featuretools.primitives import Mean\\n            # cutoff times per instance\\n            dataframes = {\\n                \"sessions\" : (session_df, \"id\"),\\n                \"transactions\" : (transactions_df, \"id\", \"transaction_time\")\\n            }\\n            relationships = [(\"sessions\", \"id\", \"transactions\", \"session_id\")]\\n            feature_matrix, features = dfs(dataframes=dataframes,\\n                                           relationships=relationships,\\n                                           target_dataframe_name=\"transactions\",\\n                                           cutoff_time=cutoff_times)\\n            feature_matrix\\n\\n            features = dfs(dataframes=dataframes,\\n                           relationships=relationships,\\n                           target_dataframe_name=\"transactions\",\\n                           features_only=True)\\n    '\n    if not isinstance(entityset, EntitySet):\n        entityset = EntitySet('dfs', dataframes, relationships)\n    dfs_object = DeepFeatureSynthesis(target_dataframe_name, entityset, agg_primitives=agg_primitives, trans_primitives=trans_primitives, groupby_trans_primitives=groupby_trans_primitives, max_depth=max_depth, where_primitives=where_primitives, allowed_paths=allowed_paths, drop_exact=drop_exact, drop_contains=drop_contains, ignore_dataframes=ignore_dataframes, ignore_columns=ignore_columns, primitive_options=primitive_options, max_features=max_features, seed_features=seed_features)\n    features = dfs_object.build_features(verbose=verbose, return_types=return_types)\n    (trans, agg, groupby, where) = _categorize_features(features)\n    trans_unused = get_unused_primitives(trans_primitives, trans)\n    agg_unused = get_unused_primitives(agg_primitives, agg)\n    groupby_unused = get_unused_primitives(groupby_trans_primitives, groupby)\n    where_unused = get_unused_primitives(where_primitives, where)\n    unused_primitives = [trans_unused, agg_unused, groupby_unused, where_unused]\n    if any(unused_primitives):\n        warn_unused_primitives(unused_primitives)\n    if features_only:\n        return features\n    assert features != [], 'No features can be generated from the specified primitives. Please make sure the primitives you are using are compatible with the variable types in your data.'\n    feature_matrix = calculate_feature_matrix(features, entityset=entityset, cutoff_time=cutoff_time, instance_ids=instance_ids, training_window=training_window, approximate=approximate, cutoff_time_in_index=cutoff_time_in_index, save_progress=save_progress, chunk_size=chunk_size, n_jobs=n_jobs, dask_kwargs=dask_kwargs, verbose=verbose, progress_callback=progress_callback, include_cutoff_time=include_cutoff_time)\n    return (feature_matrix, features)"
        ]
    },
    {
        "func_name": "warn_unused_primitives",
        "original": "def warn_unused_primitives(unused_primitives):\n    messages = ['  trans_primitives: {}\\n', '  agg_primitives: {}\\n', '  groupby_trans_primitives: {}\\n', '  where_primitives: {}\\n']\n    unused_string = ''\n    for (primitives, message) in zip(unused_primitives, messages):\n        if primitives:\n            unused_string += message.format(primitives)\n    warning_msg = 'Some specified primitives were not used during DFS:\\n{}'.format(unused_string) + 'This may be caused by a using a value of max_depth that is too small, not setting interesting values, ' + 'or it may indicate no compatible columns for the primitive were found in the data. If the DFS call ' + 'contained multiple instances of a primitive in the list above, none of them were used.'\n    warnings.warn(warning_msg, UnusedPrimitiveWarning)",
        "mutated": [
            "def warn_unused_primitives(unused_primitives):\n    if False:\n        i = 10\n    messages = ['  trans_primitives: {}\\n', '  agg_primitives: {}\\n', '  groupby_trans_primitives: {}\\n', '  where_primitives: {}\\n']\n    unused_string = ''\n    for (primitives, message) in zip(unused_primitives, messages):\n        if primitives:\n            unused_string += message.format(primitives)\n    warning_msg = 'Some specified primitives were not used during DFS:\\n{}'.format(unused_string) + 'This may be caused by a using a value of max_depth that is too small, not setting interesting values, ' + 'or it may indicate no compatible columns for the primitive were found in the data. If the DFS call ' + 'contained multiple instances of a primitive in the list above, none of them were used.'\n    warnings.warn(warning_msg, UnusedPrimitiveWarning)",
            "def warn_unused_primitives(unused_primitives):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    messages = ['  trans_primitives: {}\\n', '  agg_primitives: {}\\n', '  groupby_trans_primitives: {}\\n', '  where_primitives: {}\\n']\n    unused_string = ''\n    for (primitives, message) in zip(unused_primitives, messages):\n        if primitives:\n            unused_string += message.format(primitives)\n    warning_msg = 'Some specified primitives were not used during DFS:\\n{}'.format(unused_string) + 'This may be caused by a using a value of max_depth that is too small, not setting interesting values, ' + 'or it may indicate no compatible columns for the primitive were found in the data. If the DFS call ' + 'contained multiple instances of a primitive in the list above, none of them were used.'\n    warnings.warn(warning_msg, UnusedPrimitiveWarning)",
            "def warn_unused_primitives(unused_primitives):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    messages = ['  trans_primitives: {}\\n', '  agg_primitives: {}\\n', '  groupby_trans_primitives: {}\\n', '  where_primitives: {}\\n']\n    unused_string = ''\n    for (primitives, message) in zip(unused_primitives, messages):\n        if primitives:\n            unused_string += message.format(primitives)\n    warning_msg = 'Some specified primitives were not used during DFS:\\n{}'.format(unused_string) + 'This may be caused by a using a value of max_depth that is too small, not setting interesting values, ' + 'or it may indicate no compatible columns for the primitive were found in the data. If the DFS call ' + 'contained multiple instances of a primitive in the list above, none of them were used.'\n    warnings.warn(warning_msg, UnusedPrimitiveWarning)",
            "def warn_unused_primitives(unused_primitives):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    messages = ['  trans_primitives: {}\\n', '  agg_primitives: {}\\n', '  groupby_trans_primitives: {}\\n', '  where_primitives: {}\\n']\n    unused_string = ''\n    for (primitives, message) in zip(unused_primitives, messages):\n        if primitives:\n            unused_string += message.format(primitives)\n    warning_msg = 'Some specified primitives were not used during DFS:\\n{}'.format(unused_string) + 'This may be caused by a using a value of max_depth that is too small, not setting interesting values, ' + 'or it may indicate no compatible columns for the primitive were found in the data. If the DFS call ' + 'contained multiple instances of a primitive in the list above, none of them were used.'\n    warnings.warn(warning_msg, UnusedPrimitiveWarning)",
            "def warn_unused_primitives(unused_primitives):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    messages = ['  trans_primitives: {}\\n', '  agg_primitives: {}\\n', '  groupby_trans_primitives: {}\\n', '  where_primitives: {}\\n']\n    unused_string = ''\n    for (primitives, message) in zip(unused_primitives, messages):\n        if primitives:\n            unused_string += message.format(primitives)\n    warning_msg = 'Some specified primitives were not used during DFS:\\n{}'.format(unused_string) + 'This may be caused by a using a value of max_depth that is too small, not setting interesting values, ' + 'or it may indicate no compatible columns for the primitive were found in the data. If the DFS call ' + 'contained multiple instances of a primitive in the list above, none of them were used.'\n    warnings.warn(warning_msg, UnusedPrimitiveWarning)"
        ]
    }
]