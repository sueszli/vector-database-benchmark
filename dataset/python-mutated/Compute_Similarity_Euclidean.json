[
    {
        "func_name": "__init__",
        "original": "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=False, normalize_avg_row=False, similarity_from_distance_mode='lin', row_weights=None, **args):\n    \"\"\"\n        Computes the euclidean similarity on the columns of dataMatrix\n        If it is computed on URM=|users|x|items|, pass the URM as is.\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n        :param dataMatrix:\n        :param topK:\n        :param normalize\n        :param row_weights:         Multiply the values in each row by a specified value. Array\n        :param similarity_from_distance_mode:       \"exp\"        euclidean_similarity = 1/(e ^ euclidean_distance)\n                                                    \"lin\"        euclidean_similarity = 1/(1 + euclidean_distance)\n                                                    \"log\"        euclidean_similarity = 1/log(1 + euclidean_distance)\n        :param args:                accepts other arguments not needed by the current object\n\n        \"\"\"\n    super(Compute_Similarity_Euclidean, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    self.normalize_avg_row = normalize_avg_row\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.dataMatrix = dataMatrix.copy()\n    self.similarity_is_exp = False\n    self.similarity_is_lin = False\n    self.similarity_is_log = False\n    if similarity_from_distance_mode == 'exp':\n        self.similarity_is_exp = True\n    elif similarity_from_distance_mode == 'lin':\n        self.similarity_is_lin = True\n    elif similarity_from_distance_mode == 'log':\n        self.similarity_is_log = True\n    else:\n        raise ValueError(\"Compute_Similarity_Euclidean: value for argument 'mode' not recognized. Allowed values are: 'exp', 'lin', 'log'. Passed value was '{}'\".format(similarity_from_distance_mode))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Compute_Similarity_Euclidean: provided row_weights and dataMatrix have different number of rows.row_weights has {} rows, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
        "mutated": [
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=False, normalize_avg_row=False, similarity_from_distance_mode='lin', row_weights=None, **args):\n    if False:\n        i = 10\n    '\\n        Computes the euclidean similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param normalize\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param similarity_from_distance_mode:       \"exp\"        euclidean_similarity = 1/(e ^ euclidean_distance)\\n                                                    \"lin\"        euclidean_similarity = 1/(1 + euclidean_distance)\\n                                                    \"log\"        euclidean_similarity = 1/log(1 + euclidean_distance)\\n        :param args:                accepts other arguments not needed by the current object\\n\\n        '\n    super(Compute_Similarity_Euclidean, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    self.normalize_avg_row = normalize_avg_row\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.dataMatrix = dataMatrix.copy()\n    self.similarity_is_exp = False\n    self.similarity_is_lin = False\n    self.similarity_is_log = False\n    if similarity_from_distance_mode == 'exp':\n        self.similarity_is_exp = True\n    elif similarity_from_distance_mode == 'lin':\n        self.similarity_is_lin = True\n    elif similarity_from_distance_mode == 'log':\n        self.similarity_is_log = True\n    else:\n        raise ValueError(\"Compute_Similarity_Euclidean: value for argument 'mode' not recognized. Allowed values are: 'exp', 'lin', 'log'. Passed value was '{}'\".format(similarity_from_distance_mode))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Compute_Similarity_Euclidean: provided row_weights and dataMatrix have different number of rows.row_weights has {} rows, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=False, normalize_avg_row=False, similarity_from_distance_mode='lin', row_weights=None, **args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Computes the euclidean similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param normalize\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param similarity_from_distance_mode:       \"exp\"        euclidean_similarity = 1/(e ^ euclidean_distance)\\n                                                    \"lin\"        euclidean_similarity = 1/(1 + euclidean_distance)\\n                                                    \"log\"        euclidean_similarity = 1/log(1 + euclidean_distance)\\n        :param args:                accepts other arguments not needed by the current object\\n\\n        '\n    super(Compute_Similarity_Euclidean, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    self.normalize_avg_row = normalize_avg_row\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.dataMatrix = dataMatrix.copy()\n    self.similarity_is_exp = False\n    self.similarity_is_lin = False\n    self.similarity_is_log = False\n    if similarity_from_distance_mode == 'exp':\n        self.similarity_is_exp = True\n    elif similarity_from_distance_mode == 'lin':\n        self.similarity_is_lin = True\n    elif similarity_from_distance_mode == 'log':\n        self.similarity_is_log = True\n    else:\n        raise ValueError(\"Compute_Similarity_Euclidean: value for argument 'mode' not recognized. Allowed values are: 'exp', 'lin', 'log'. Passed value was '{}'\".format(similarity_from_distance_mode))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Compute_Similarity_Euclidean: provided row_weights and dataMatrix have different number of rows.row_weights has {} rows, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=False, normalize_avg_row=False, similarity_from_distance_mode='lin', row_weights=None, **args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Computes the euclidean similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param normalize\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param similarity_from_distance_mode:       \"exp\"        euclidean_similarity = 1/(e ^ euclidean_distance)\\n                                                    \"lin\"        euclidean_similarity = 1/(1 + euclidean_distance)\\n                                                    \"log\"        euclidean_similarity = 1/log(1 + euclidean_distance)\\n        :param args:                accepts other arguments not needed by the current object\\n\\n        '\n    super(Compute_Similarity_Euclidean, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    self.normalize_avg_row = normalize_avg_row\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.dataMatrix = dataMatrix.copy()\n    self.similarity_is_exp = False\n    self.similarity_is_lin = False\n    self.similarity_is_log = False\n    if similarity_from_distance_mode == 'exp':\n        self.similarity_is_exp = True\n    elif similarity_from_distance_mode == 'lin':\n        self.similarity_is_lin = True\n    elif similarity_from_distance_mode == 'log':\n        self.similarity_is_log = True\n    else:\n        raise ValueError(\"Compute_Similarity_Euclidean: value for argument 'mode' not recognized. Allowed values are: 'exp', 'lin', 'log'. Passed value was '{}'\".format(similarity_from_distance_mode))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Compute_Similarity_Euclidean: provided row_weights and dataMatrix have different number of rows.row_weights has {} rows, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=False, normalize_avg_row=False, similarity_from_distance_mode='lin', row_weights=None, **args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Computes the euclidean similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param normalize\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param similarity_from_distance_mode:       \"exp\"        euclidean_similarity = 1/(e ^ euclidean_distance)\\n                                                    \"lin\"        euclidean_similarity = 1/(1 + euclidean_distance)\\n                                                    \"log\"        euclidean_similarity = 1/log(1 + euclidean_distance)\\n        :param args:                accepts other arguments not needed by the current object\\n\\n        '\n    super(Compute_Similarity_Euclidean, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    self.normalize_avg_row = normalize_avg_row\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.dataMatrix = dataMatrix.copy()\n    self.similarity_is_exp = False\n    self.similarity_is_lin = False\n    self.similarity_is_log = False\n    if similarity_from_distance_mode == 'exp':\n        self.similarity_is_exp = True\n    elif similarity_from_distance_mode == 'lin':\n        self.similarity_is_lin = True\n    elif similarity_from_distance_mode == 'log':\n        self.similarity_is_log = True\n    else:\n        raise ValueError(\"Compute_Similarity_Euclidean: value for argument 'mode' not recognized. Allowed values are: 'exp', 'lin', 'log'. Passed value was '{}'\".format(similarity_from_distance_mode))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Compute_Similarity_Euclidean: provided row_weights and dataMatrix have different number of rows.row_weights has {} rows, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=False, normalize_avg_row=False, similarity_from_distance_mode='lin', row_weights=None, **args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Computes the euclidean similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param normalize\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param similarity_from_distance_mode:       \"exp\"        euclidean_similarity = 1/(e ^ euclidean_distance)\\n                                                    \"lin\"        euclidean_similarity = 1/(1 + euclidean_distance)\\n                                                    \"log\"        euclidean_similarity = 1/log(1 + euclidean_distance)\\n        :param args:                accepts other arguments not needed by the current object\\n\\n        '\n    super(Compute_Similarity_Euclidean, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    self.normalize_avg_row = normalize_avg_row\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.dataMatrix = dataMatrix.copy()\n    self.similarity_is_exp = False\n    self.similarity_is_lin = False\n    self.similarity_is_log = False\n    if similarity_from_distance_mode == 'exp':\n        self.similarity_is_exp = True\n    elif similarity_from_distance_mode == 'lin':\n        self.similarity_is_lin = True\n    elif similarity_from_distance_mode == 'log':\n        self.similarity_is_log = True\n    else:\n        raise ValueError(\"Compute_Similarity_Euclidean: value for argument 'mode' not recognized. Allowed values are: 'exp', 'lin', 'log'. Passed value was '{}'\".format(similarity_from_distance_mode))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Compute_Similarity_Euclidean: provided row_weights and dataMatrix have different number of rows.row_weights has {} rows, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T"
        ]
    },
    {
        "func_name": "compute_similarity",
        "original": "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    \"\"\"\n        Compute the similarity for the given dataset\n        :param self:\n        :param start_col: column to begin with\n        :param end_col: column to stop before, end_col is excluded\n        :return:\n        \"\"\"\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    item_distance_initial = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    sumOfSquared = np.sqrt(item_distance_initial)\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            item_distance = item_distance_initial.copy()\n            item_distance += item_distance_initial[columnIndex]\n            item_distance -= 2 * this_column_weights\n            item_distance[columnIndex] = 0.0\n            if self.use_row_weights:\n                item_distance = np.multiply(item_distance, self.row_weights)\n            if self.normalize:\n                denominator = sumOfSquared[columnIndex] * sumOfSquared\n                item_distance[denominator != 0.0] /= denominator[denominator != 0.0]\n            if self.normalize_avg_row:\n                item_distance /= self.n_rows\n            nonzero_distance_mask = item_distance > 0.0\n            item_distance[nonzero_distance_mask] = np.sqrt(item_distance[nonzero_distance_mask])\n            if self.similarity_is_exp:\n                item_similarity = 1 / (np.exp(item_distance) + self.shrink + 1e-09)\n            elif self.similarity_is_lin:\n                item_similarity = 1 / (item_distance + self.shrink + 1e-09)\n            elif self.similarity_is_log:\n                item_similarity = 1 / (np.log(item_distance + 1) + self.shrink + 1e-09)\n            else:\n                assert False\n            item_similarity[columnIndex] = 0.0\n            this_column_weights = item_similarity\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
        "mutated": [
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    item_distance_initial = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    sumOfSquared = np.sqrt(item_distance_initial)\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            item_distance = item_distance_initial.copy()\n            item_distance += item_distance_initial[columnIndex]\n            item_distance -= 2 * this_column_weights\n            item_distance[columnIndex] = 0.0\n            if self.use_row_weights:\n                item_distance = np.multiply(item_distance, self.row_weights)\n            if self.normalize:\n                denominator = sumOfSquared[columnIndex] * sumOfSquared\n                item_distance[denominator != 0.0] /= denominator[denominator != 0.0]\n            if self.normalize_avg_row:\n                item_distance /= self.n_rows\n            nonzero_distance_mask = item_distance > 0.0\n            item_distance[nonzero_distance_mask] = np.sqrt(item_distance[nonzero_distance_mask])\n            if self.similarity_is_exp:\n                item_similarity = 1 / (np.exp(item_distance) + self.shrink + 1e-09)\n            elif self.similarity_is_lin:\n                item_similarity = 1 / (item_distance + self.shrink + 1e-09)\n            elif self.similarity_is_log:\n                item_similarity = 1 / (np.log(item_distance + 1) + self.shrink + 1e-09)\n            else:\n                assert False\n            item_similarity[columnIndex] = 0.0\n            this_column_weights = item_similarity\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    item_distance_initial = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    sumOfSquared = np.sqrt(item_distance_initial)\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            item_distance = item_distance_initial.copy()\n            item_distance += item_distance_initial[columnIndex]\n            item_distance -= 2 * this_column_weights\n            item_distance[columnIndex] = 0.0\n            if self.use_row_weights:\n                item_distance = np.multiply(item_distance, self.row_weights)\n            if self.normalize:\n                denominator = sumOfSquared[columnIndex] * sumOfSquared\n                item_distance[denominator != 0.0] /= denominator[denominator != 0.0]\n            if self.normalize_avg_row:\n                item_distance /= self.n_rows\n            nonzero_distance_mask = item_distance > 0.0\n            item_distance[nonzero_distance_mask] = np.sqrt(item_distance[nonzero_distance_mask])\n            if self.similarity_is_exp:\n                item_similarity = 1 / (np.exp(item_distance) + self.shrink + 1e-09)\n            elif self.similarity_is_lin:\n                item_similarity = 1 / (item_distance + self.shrink + 1e-09)\n            elif self.similarity_is_log:\n                item_similarity = 1 / (np.log(item_distance + 1) + self.shrink + 1e-09)\n            else:\n                assert False\n            item_similarity[columnIndex] = 0.0\n            this_column_weights = item_similarity\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    item_distance_initial = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    sumOfSquared = np.sqrt(item_distance_initial)\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            item_distance = item_distance_initial.copy()\n            item_distance += item_distance_initial[columnIndex]\n            item_distance -= 2 * this_column_weights\n            item_distance[columnIndex] = 0.0\n            if self.use_row_weights:\n                item_distance = np.multiply(item_distance, self.row_weights)\n            if self.normalize:\n                denominator = sumOfSquared[columnIndex] * sumOfSquared\n                item_distance[denominator != 0.0] /= denominator[denominator != 0.0]\n            if self.normalize_avg_row:\n                item_distance /= self.n_rows\n            nonzero_distance_mask = item_distance > 0.0\n            item_distance[nonzero_distance_mask] = np.sqrt(item_distance[nonzero_distance_mask])\n            if self.similarity_is_exp:\n                item_similarity = 1 / (np.exp(item_distance) + self.shrink + 1e-09)\n            elif self.similarity_is_lin:\n                item_similarity = 1 / (item_distance + self.shrink + 1e-09)\n            elif self.similarity_is_log:\n                item_similarity = 1 / (np.log(item_distance + 1) + self.shrink + 1e-09)\n            else:\n                assert False\n            item_similarity[columnIndex] = 0.0\n            this_column_weights = item_similarity\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    item_distance_initial = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    sumOfSquared = np.sqrt(item_distance_initial)\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            item_distance = item_distance_initial.copy()\n            item_distance += item_distance_initial[columnIndex]\n            item_distance -= 2 * this_column_weights\n            item_distance[columnIndex] = 0.0\n            if self.use_row_weights:\n                item_distance = np.multiply(item_distance, self.row_weights)\n            if self.normalize:\n                denominator = sumOfSquared[columnIndex] * sumOfSquared\n                item_distance[denominator != 0.0] /= denominator[denominator != 0.0]\n            if self.normalize_avg_row:\n                item_distance /= self.n_rows\n            nonzero_distance_mask = item_distance > 0.0\n            item_distance[nonzero_distance_mask] = np.sqrt(item_distance[nonzero_distance_mask])\n            if self.similarity_is_exp:\n                item_similarity = 1 / (np.exp(item_distance) + self.shrink + 1e-09)\n            elif self.similarity_is_lin:\n                item_similarity = 1 / (item_distance + self.shrink + 1e-09)\n            elif self.similarity_is_log:\n                item_similarity = 1 / (np.log(item_distance + 1) + self.shrink + 1e-09)\n            else:\n                assert False\n            item_similarity[columnIndex] = 0.0\n            this_column_weights = item_similarity\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    item_distance_initial = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    sumOfSquared = np.sqrt(item_distance_initial)\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            item_distance = item_distance_initial.copy()\n            item_distance += item_distance_initial[columnIndex]\n            item_distance -= 2 * this_column_weights\n            item_distance[columnIndex] = 0.0\n            if self.use_row_weights:\n                item_distance = np.multiply(item_distance, self.row_weights)\n            if self.normalize:\n                denominator = sumOfSquared[columnIndex] * sumOfSquared\n                item_distance[denominator != 0.0] /= denominator[denominator != 0.0]\n            if self.normalize_avg_row:\n                item_distance /= self.n_rows\n            nonzero_distance_mask = item_distance > 0.0\n            item_distance[nonzero_distance_mask] = np.sqrt(item_distance[nonzero_distance_mask])\n            if self.similarity_is_exp:\n                item_similarity = 1 / (np.exp(item_distance) + self.shrink + 1e-09)\n            elif self.similarity_is_lin:\n                item_similarity = 1 / (item_distance + self.shrink + 1e-09)\n            elif self.similarity_is_log:\n                item_similarity = 1 / (np.log(item_distance + 1) + self.shrink + 1e-09)\n            else:\n                assert False\n            item_similarity[columnIndex] = 0.0\n            this_column_weights = item_similarity\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse"
        ]
    }
]