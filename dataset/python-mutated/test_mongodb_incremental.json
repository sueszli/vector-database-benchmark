[
    {
        "func_name": "z_string_generator",
        "original": "def z_string_generator(size=6):\n    return 'z' * size",
        "mutated": [
            "def z_string_generator(size=6):\n    if False:\n        i = 10\n    return 'z' * size",
            "def z_string_generator(size=6):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 'z' * size",
            "def z_string_generator(size=6):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 'z' * size",
            "def z_string_generator(size=6):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 'z' * size",
            "def z_string_generator(size=6):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 'z' * size"
        ]
    },
    {
        "func_name": "generate_simple_coll_docs",
        "original": "def generate_simple_coll_docs(num_docs):\n    docs = []\n    start_datetime = datetime(2018, 1, 1, 19, 29, 14, 578000)\n    for int_value in range(num_docs):\n        start_datetime = start_datetime + timedelta(days=5)\n        docs.append({'int_field': int_value, 'string_field': z_string_generator(int_value), 'date_field': start_datetime, 'double_field': int_value + 1.00001, 'timestamp_field': bson.timestamp.Timestamp(int_value + 1565897157, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282{:03d}'.format(int_value)), '64_bit_int_field': 34359738368 + int_value})\n    return docs",
        "mutated": [
            "def generate_simple_coll_docs(num_docs):\n    if False:\n        i = 10\n    docs = []\n    start_datetime = datetime(2018, 1, 1, 19, 29, 14, 578000)\n    for int_value in range(num_docs):\n        start_datetime = start_datetime + timedelta(days=5)\n        docs.append({'int_field': int_value, 'string_field': z_string_generator(int_value), 'date_field': start_datetime, 'double_field': int_value + 1.00001, 'timestamp_field': bson.timestamp.Timestamp(int_value + 1565897157, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282{:03d}'.format(int_value)), '64_bit_int_field': 34359738368 + int_value})\n    return docs",
            "def generate_simple_coll_docs(num_docs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    docs = []\n    start_datetime = datetime(2018, 1, 1, 19, 29, 14, 578000)\n    for int_value in range(num_docs):\n        start_datetime = start_datetime + timedelta(days=5)\n        docs.append({'int_field': int_value, 'string_field': z_string_generator(int_value), 'date_field': start_datetime, 'double_field': int_value + 1.00001, 'timestamp_field': bson.timestamp.Timestamp(int_value + 1565897157, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282{:03d}'.format(int_value)), '64_bit_int_field': 34359738368 + int_value})\n    return docs",
            "def generate_simple_coll_docs(num_docs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    docs = []\n    start_datetime = datetime(2018, 1, 1, 19, 29, 14, 578000)\n    for int_value in range(num_docs):\n        start_datetime = start_datetime + timedelta(days=5)\n        docs.append({'int_field': int_value, 'string_field': z_string_generator(int_value), 'date_field': start_datetime, 'double_field': int_value + 1.00001, 'timestamp_field': bson.timestamp.Timestamp(int_value + 1565897157, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282{:03d}'.format(int_value)), '64_bit_int_field': 34359738368 + int_value})\n    return docs",
            "def generate_simple_coll_docs(num_docs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    docs = []\n    start_datetime = datetime(2018, 1, 1, 19, 29, 14, 578000)\n    for int_value in range(num_docs):\n        start_datetime = start_datetime + timedelta(days=5)\n        docs.append({'int_field': int_value, 'string_field': z_string_generator(int_value), 'date_field': start_datetime, 'double_field': int_value + 1.00001, 'timestamp_field': bson.timestamp.Timestamp(int_value + 1565897157, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282{:03d}'.format(int_value)), '64_bit_int_field': 34359738368 + int_value})\n    return docs",
            "def generate_simple_coll_docs(num_docs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    docs = []\n    start_datetime = datetime(2018, 1, 1, 19, 29, 14, 578000)\n    for int_value in range(num_docs):\n        start_datetime = start_datetime + timedelta(days=5)\n        docs.append({'int_field': int_value, 'string_field': z_string_generator(int_value), 'date_field': start_datetime, 'double_field': int_value + 1.00001, 'timestamp_field': bson.timestamp.Timestamp(int_value + 1565897157, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282{:03d}'.format(int_value)), '64_bit_int_field': 34359738368 + int_value})\n    return docs"
        ]
    },
    {
        "func_name": "key_names",
        "original": "def key_names(self):\n    return ['int_field', 'string_field', 'date_field', 'timestamp_field', 'uuid_field', '64_bit_int_field', 'double_field']",
        "mutated": [
            "def key_names(self):\n    if False:\n        i = 10\n    return ['int_field', 'string_field', 'date_field', 'timestamp_field', 'uuid_field', '64_bit_int_field', 'double_field']",
            "def key_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return ['int_field', 'string_field', 'date_field', 'timestamp_field', 'uuid_field', '64_bit_int_field', 'double_field']",
            "def key_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return ['int_field', 'string_field', 'date_field', 'timestamp_field', 'uuid_field', '64_bit_int_field', 'double_field']",
            "def key_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return ['int_field', 'string_field', 'date_field', 'timestamp_field', 'uuid_field', '64_bit_int_field', 'double_field']",
            "def key_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return ['int_field', 'string_field', 'date_field', 'timestamp_field', 'uuid_field', '64_bit_int_field', 'double_field']"
        ]
    },
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    ensure_environment_variables_set()\n    with get_test_connection() as client:\n        drop_all_collections(client)\n        client['simple_db']['simple_coll_1'].insert_many(generate_simple_coll_docs(50))\n        client['simple_db']['simple_coll_2'].insert_many(generate_simple_coll_docs(100))\n        client['simple_db']['simple_coll_1'].create_index([('date_field', pymongo.ASCENDING)])\n        client['simple_db']['simple_coll_2'].create_index([('date_field', pymongo.ASCENDING)])\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_many(generate_simple_coll_docs(50))\n            client['simple_db']['simple_coll_{}'.format(key_name)].create_index([(key_name, pymongo.ASCENDING)])",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    ensure_environment_variables_set()\n    with get_test_connection() as client:\n        drop_all_collections(client)\n        client['simple_db']['simple_coll_1'].insert_many(generate_simple_coll_docs(50))\n        client['simple_db']['simple_coll_2'].insert_many(generate_simple_coll_docs(100))\n        client['simple_db']['simple_coll_1'].create_index([('date_field', pymongo.ASCENDING)])\n        client['simple_db']['simple_coll_2'].create_index([('date_field', pymongo.ASCENDING)])\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_many(generate_simple_coll_docs(50))\n            client['simple_db']['simple_coll_{}'.format(key_name)].create_index([(key_name, pymongo.ASCENDING)])",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ensure_environment_variables_set()\n    with get_test_connection() as client:\n        drop_all_collections(client)\n        client['simple_db']['simple_coll_1'].insert_many(generate_simple_coll_docs(50))\n        client['simple_db']['simple_coll_2'].insert_many(generate_simple_coll_docs(100))\n        client['simple_db']['simple_coll_1'].create_index([('date_field', pymongo.ASCENDING)])\n        client['simple_db']['simple_coll_2'].create_index([('date_field', pymongo.ASCENDING)])\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_many(generate_simple_coll_docs(50))\n            client['simple_db']['simple_coll_{}'.format(key_name)].create_index([(key_name, pymongo.ASCENDING)])",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ensure_environment_variables_set()\n    with get_test_connection() as client:\n        drop_all_collections(client)\n        client['simple_db']['simple_coll_1'].insert_many(generate_simple_coll_docs(50))\n        client['simple_db']['simple_coll_2'].insert_many(generate_simple_coll_docs(100))\n        client['simple_db']['simple_coll_1'].create_index([('date_field', pymongo.ASCENDING)])\n        client['simple_db']['simple_coll_2'].create_index([('date_field', pymongo.ASCENDING)])\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_many(generate_simple_coll_docs(50))\n            client['simple_db']['simple_coll_{}'.format(key_name)].create_index([(key_name, pymongo.ASCENDING)])",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ensure_environment_variables_set()\n    with get_test_connection() as client:\n        drop_all_collections(client)\n        client['simple_db']['simple_coll_1'].insert_many(generate_simple_coll_docs(50))\n        client['simple_db']['simple_coll_2'].insert_many(generate_simple_coll_docs(100))\n        client['simple_db']['simple_coll_1'].create_index([('date_field', pymongo.ASCENDING)])\n        client['simple_db']['simple_coll_2'].create_index([('date_field', pymongo.ASCENDING)])\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_many(generate_simple_coll_docs(50))\n            client['simple_db']['simple_coll_{}'.format(key_name)].create_index([(key_name, pymongo.ASCENDING)])",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ensure_environment_variables_set()\n    with get_test_connection() as client:\n        drop_all_collections(client)\n        client['simple_db']['simple_coll_1'].insert_many(generate_simple_coll_docs(50))\n        client['simple_db']['simple_coll_2'].insert_many(generate_simple_coll_docs(100))\n        client['simple_db']['simple_coll_1'].create_index([('date_field', pymongo.ASCENDING)])\n        client['simple_db']['simple_coll_2'].create_index([('date_field', pymongo.ASCENDING)])\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_many(generate_simple_coll_docs(50))\n            client['simple_db']['simple_coll_{}'.format(key_name)].create_index([(key_name, pymongo.ASCENDING)])"
        ]
    },
    {
        "func_name": "expected_check_streams",
        "original": "def expected_check_streams(self):\n    return {'simple_db-simple_coll_1', 'simple_db-simple_coll_2', *['simple_db-simple_coll_{}'.format(k) for k in self.key_names()]}",
        "mutated": [
            "def expected_check_streams(self):\n    if False:\n        i = 10\n    return {'simple_db-simple_coll_1', 'simple_db-simple_coll_2', *['simple_db-simple_coll_{}'.format(k) for k in self.key_names()]}",
            "def expected_check_streams(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'simple_db-simple_coll_1', 'simple_db-simple_coll_2', *['simple_db-simple_coll_{}'.format(k) for k in self.key_names()]}",
            "def expected_check_streams(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'simple_db-simple_coll_1', 'simple_db-simple_coll_2', *['simple_db-simple_coll_{}'.format(k) for k in self.key_names()]}",
            "def expected_check_streams(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'simple_db-simple_coll_1', 'simple_db-simple_coll_2', *['simple_db-simple_coll_{}'.format(k) for k in self.key_names()]}",
            "def expected_check_streams(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'simple_db-simple_coll_1', 'simple_db-simple_coll_2', *['simple_db-simple_coll_{}'.format(k) for k in self.key_names()]}"
        ]
    },
    {
        "func_name": "expected_pks",
        "original": "def expected_pks(self):\n    return {'simple_coll_1': {'_id'}, 'simple_coll_2': {'_id'}, **{'simple_coll_{}'.format(k): {'_id'} for k in self.key_names()}}",
        "mutated": [
            "def expected_pks(self):\n    if False:\n        i = 10\n    return {'simple_coll_1': {'_id'}, 'simple_coll_2': {'_id'}, **{'simple_coll_{}'.format(k): {'_id'} for k in self.key_names()}}",
            "def expected_pks(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'simple_coll_1': {'_id'}, 'simple_coll_2': {'_id'}, **{'simple_coll_{}'.format(k): {'_id'} for k in self.key_names()}}",
            "def expected_pks(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'simple_coll_1': {'_id'}, 'simple_coll_2': {'_id'}, **{'simple_coll_{}'.format(k): {'_id'} for k in self.key_names()}}",
            "def expected_pks(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'simple_coll_1': {'_id'}, 'simple_coll_2': {'_id'}, **{'simple_coll_{}'.format(k): {'_id'} for k in self.key_names()}}",
            "def expected_pks(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'simple_coll_1': {'_id'}, 'simple_coll_2': {'_id'}, **{'simple_coll_{}'.format(k): {'_id'} for k in self.key_names()}}"
        ]
    },
    {
        "func_name": "expected_valid_replication_keys",
        "original": "def expected_valid_replication_keys(self):\n    return {'simple_coll_1': {'_id', 'date_field'}, 'simple_coll_2': {'_id', 'date_field'}, **{'simple_coll_{}'.format(k): {'_id', k} for k in self.key_names()}}",
        "mutated": [
            "def expected_valid_replication_keys(self):\n    if False:\n        i = 10\n    return {'simple_coll_1': {'_id', 'date_field'}, 'simple_coll_2': {'_id', 'date_field'}, **{'simple_coll_{}'.format(k): {'_id', k} for k in self.key_names()}}",
            "def expected_valid_replication_keys(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'simple_coll_1': {'_id', 'date_field'}, 'simple_coll_2': {'_id', 'date_field'}, **{'simple_coll_{}'.format(k): {'_id', k} for k in self.key_names()}}",
            "def expected_valid_replication_keys(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'simple_coll_1': {'_id', 'date_field'}, 'simple_coll_2': {'_id', 'date_field'}, **{'simple_coll_{}'.format(k): {'_id', k} for k in self.key_names()}}",
            "def expected_valid_replication_keys(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'simple_coll_1': {'_id', 'date_field'}, 'simple_coll_2': {'_id', 'date_field'}, **{'simple_coll_{}'.format(k): {'_id', k} for k in self.key_names()}}",
            "def expected_valid_replication_keys(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'simple_coll_1': {'_id', 'date_field'}, 'simple_coll_2': {'_id', 'date_field'}, **{'simple_coll_{}'.format(k): {'_id', k} for k in self.key_names()}}"
        ]
    },
    {
        "func_name": "expected_row_counts",
        "original": "def expected_row_counts(self):\n    return {'simple_coll_1': 50, 'simple_coll_2': 100, **{'simple_coll_{}'.format(k): 50 for k in self.key_names()}}",
        "mutated": [
            "def expected_row_counts(self):\n    if False:\n        i = 10\n    return {'simple_coll_1': 50, 'simple_coll_2': 100, **{'simple_coll_{}'.format(k): 50 for k in self.key_names()}}",
            "def expected_row_counts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'simple_coll_1': 50, 'simple_coll_2': 100, **{'simple_coll_{}'.format(k): 50 for k in self.key_names()}}",
            "def expected_row_counts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'simple_coll_1': 50, 'simple_coll_2': 100, **{'simple_coll_{}'.format(k): 50 for k in self.key_names()}}",
            "def expected_row_counts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'simple_coll_1': 50, 'simple_coll_2': 100, **{'simple_coll_{}'.format(k): 50 for k in self.key_names()}}",
            "def expected_row_counts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'simple_coll_1': 50, 'simple_coll_2': 100, **{'simple_coll_{}'.format(k): 50 for k in self.key_names()}}"
        ]
    },
    {
        "func_name": "expected_last_sync_row_counts",
        "original": "def expected_last_sync_row_counts(self):\n    return {'simple_coll_1': 53, 'simple_coll_2': 102, **{'simple_coll_{}'.format(k): 1 for k in self.key_names()}}",
        "mutated": [
            "def expected_last_sync_row_counts(self):\n    if False:\n        i = 10\n    return {'simple_coll_1': 53, 'simple_coll_2': 102, **{'simple_coll_{}'.format(k): 1 for k in self.key_names()}}",
            "def expected_last_sync_row_counts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'simple_coll_1': 53, 'simple_coll_2': 102, **{'simple_coll_{}'.format(k): 1 for k in self.key_names()}}",
            "def expected_last_sync_row_counts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'simple_coll_1': 53, 'simple_coll_2': 102, **{'simple_coll_{}'.format(k): 1 for k in self.key_names()}}",
            "def expected_last_sync_row_counts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'simple_coll_1': 53, 'simple_coll_2': 102, **{'simple_coll_{}'.format(k): 1 for k in self.key_names()}}",
            "def expected_last_sync_row_counts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'simple_coll_1': 53, 'simple_coll_2': 102, **{'simple_coll_{}'.format(k): 1 for k in self.key_names()}}"
        ]
    },
    {
        "func_name": "expected_incremental_int_fields",
        "original": "def expected_incremental_int_fields(self):\n    return {'simple_coll_1': {49, 50, 51, 0}, 'simple_coll_2': {99, 100, 101, 0}, **{'simple_coll_{}'.format(k): {49, 50, 51, 0} for k in self.key_names() if k not in 'simple_coll_int_field'}, 'simple_coll_int_field': {49, 50, 51, 52}}",
        "mutated": [
            "def expected_incremental_int_fields(self):\n    if False:\n        i = 10\n    return {'simple_coll_1': {49, 50, 51, 0}, 'simple_coll_2': {99, 100, 101, 0}, **{'simple_coll_{}'.format(k): {49, 50, 51, 0} for k in self.key_names() if k not in 'simple_coll_int_field'}, 'simple_coll_int_field': {49, 50, 51, 52}}",
            "def expected_incremental_int_fields(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'simple_coll_1': {49, 50, 51, 0}, 'simple_coll_2': {99, 100, 101, 0}, **{'simple_coll_{}'.format(k): {49, 50, 51, 0} for k in self.key_names() if k not in 'simple_coll_int_field'}, 'simple_coll_int_field': {49, 50, 51, 52}}",
            "def expected_incremental_int_fields(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'simple_coll_1': {49, 50, 51, 0}, 'simple_coll_2': {99, 100, 101, 0}, **{'simple_coll_{}'.format(k): {49, 50, 51, 0} for k in self.key_names() if k not in 'simple_coll_int_field'}, 'simple_coll_int_field': {49, 50, 51, 52}}",
            "def expected_incremental_int_fields(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'simple_coll_1': {49, 50, 51, 0}, 'simple_coll_2': {99, 100, 101, 0}, **{'simple_coll_{}'.format(k): {49, 50, 51, 0} for k in self.key_names() if k not in 'simple_coll_int_field'}, 'simple_coll_int_field': {49, 50, 51, 52}}",
            "def expected_incremental_int_fields(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'simple_coll_1': {49, 50, 51, 0}, 'simple_coll_2': {99, 100, 101, 0}, **{'simple_coll_{}'.format(k): {49, 50, 51, 0} for k in self.key_names() if k not in 'simple_coll_int_field'}, 'simple_coll_int_field': {49, 50, 51, 52}}"
        ]
    },
    {
        "func_name": "expected_sync_streams",
        "original": "def expected_sync_streams(self):\n    return {'simple_coll_1', 'simple_coll_2', *['simple_coll_{}'.format(k) for k in self.key_names()]}",
        "mutated": [
            "def expected_sync_streams(self):\n    if False:\n        i = 10\n    return {'simple_coll_1', 'simple_coll_2', *['simple_coll_{}'.format(k) for k in self.key_names()]}",
            "def expected_sync_streams(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'simple_coll_1', 'simple_coll_2', *['simple_coll_{}'.format(k) for k in self.key_names()]}",
            "def expected_sync_streams(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'simple_coll_1', 'simple_coll_2', *['simple_coll_{}'.format(k) for k in self.key_names()]}",
            "def expected_sync_streams(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'simple_coll_1', 'simple_coll_2', *['simple_coll_{}'.format(k) for k in self.key_names()]}",
            "def expected_sync_streams(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'simple_coll_1', 'simple_coll_2', *['simple_coll_{}'.format(k) for k in self.key_names()]}"
        ]
    },
    {
        "func_name": "name",
        "original": "def name(self):\n    return 'tap_tester_mongodb_incremental'",
        "mutated": [
            "def name(self):\n    if False:\n        i = 10\n    return 'tap_tester_mongodb_incremental'",
            "def name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 'tap_tester_mongodb_incremental'",
            "def name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 'tap_tester_mongodb_incremental'",
            "def name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 'tap_tester_mongodb_incremental'",
            "def name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 'tap_tester_mongodb_incremental'"
        ]
    },
    {
        "func_name": "tap_name",
        "original": "def tap_name(self):\n    return 'tap-mongodb'",
        "mutated": [
            "def tap_name(self):\n    if False:\n        i = 10\n    return 'tap-mongodb'",
            "def tap_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 'tap-mongodb'",
            "def tap_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 'tap-mongodb'",
            "def tap_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 'tap-mongodb'",
            "def tap_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 'tap-mongodb'"
        ]
    },
    {
        "func_name": "get_type",
        "original": "def get_type(self):\n    return 'platform.mongodb'",
        "mutated": [
            "def get_type(self):\n    if False:\n        i = 10\n    return 'platform.mongodb'",
            "def get_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 'platform.mongodb'",
            "def get_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 'platform.mongodb'",
            "def get_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 'platform.mongodb'",
            "def get_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 'platform.mongodb'"
        ]
    },
    {
        "func_name": "get_credentials",
        "original": "def get_credentials(self):\n    return {'password': os.getenv('TAP_MONGODB_PASSWORD')}",
        "mutated": [
            "def get_credentials(self):\n    if False:\n        i = 10\n    return {'password': os.getenv('TAP_MONGODB_PASSWORD')}",
            "def get_credentials(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'password': os.getenv('TAP_MONGODB_PASSWORD')}",
            "def get_credentials(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'password': os.getenv('TAP_MONGODB_PASSWORD')}",
            "def get_credentials(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'password': os.getenv('TAP_MONGODB_PASSWORD')}",
            "def get_credentials(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'password': os.getenv('TAP_MONGODB_PASSWORD')}"
        ]
    },
    {
        "func_name": "get_properties",
        "original": "def get_properties(self):\n    return {'host': os.getenv('TAP_MONGODB_HOST'), 'port': os.getenv('TAP_MONGODB_PORT'), 'user': os.getenv('TAP_MONGODB_USER'), 'database': os.getenv('TAP_MONGODB_DBNAME')}",
        "mutated": [
            "def get_properties(self):\n    if False:\n        i = 10\n    return {'host': os.getenv('TAP_MONGODB_HOST'), 'port': os.getenv('TAP_MONGODB_PORT'), 'user': os.getenv('TAP_MONGODB_USER'), 'database': os.getenv('TAP_MONGODB_DBNAME')}",
            "def get_properties(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'host': os.getenv('TAP_MONGODB_HOST'), 'port': os.getenv('TAP_MONGODB_PORT'), 'user': os.getenv('TAP_MONGODB_USER'), 'database': os.getenv('TAP_MONGODB_DBNAME')}",
            "def get_properties(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'host': os.getenv('TAP_MONGODB_HOST'), 'port': os.getenv('TAP_MONGODB_PORT'), 'user': os.getenv('TAP_MONGODB_USER'), 'database': os.getenv('TAP_MONGODB_DBNAME')}",
            "def get_properties(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'host': os.getenv('TAP_MONGODB_HOST'), 'port': os.getenv('TAP_MONGODB_PORT'), 'user': os.getenv('TAP_MONGODB_USER'), 'database': os.getenv('TAP_MONGODB_DBNAME')}",
            "def get_properties(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'host': os.getenv('TAP_MONGODB_HOST'), 'port': os.getenv('TAP_MONGODB_PORT'), 'user': os.getenv('TAP_MONGODB_USER'), 'database': os.getenv('TAP_MONGODB_DBNAME')}"
        ]
    },
    {
        "func_name": "test_run",
        "original": "def test_run(self):\n    conn_id = connections.ensure_connection(self)\n    check_job_name = runner.run_check_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, check_job_name)\n    menagerie.verify_check_exit_status(self, exit_status, check_job_name)\n    catalog = menagerie.get_catalog(conn_id)\n    found_catalogs = menagerie.get_catalogs(conn_id)\n    found_streams = {entry['tap_stream_id'] for entry in catalog['streams']}\n    self.assertSetEqual(self.expected_check_streams(), found_streams)\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            stream = tap_stream_id.split('-')[1]\n            expected_primary_key = self.expected_pks()[stream]\n            expected_row_count = self.expected_row_counts()[stream]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            found_stream = [entry for entry in catalog['streams'] if entry['tap_stream_id'] == tap_stream_id][0]\n            stream_metadata = [entry['metadata'] for entry in found_stream['metadata'] if entry['breadcrumb'] == []][0]\n            primary_key = set(stream_metadata.get('table-key-properties'))\n            row_count = stream_metadata.get('row-count')\n            replication_key = set(stream_metadata.get('valid-replication-keys'))\n            self.assertSetEqual(expected_primary_key, primary_key)\n            self.assertEqual(expected_row_count, row_count)\n            self.assertSetEqual(replication_key, expected_replication_keys)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        rep_key = 'date_field'\n        for key in self.key_names():\n            if key in stream_catalog['stream_name']:\n                rep_key = key\n        additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': rep_key}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    expected_schema = {'type': 'object'}\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            persisted_schema = messages_by_stream[tap_stream_id]['schema']\n            self.assertDictEqual(expected_schema, persisted_schema)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            expected_row_count = self.expected_row_counts()[tap_stream_id]\n            row_count = record_count_by_stream[tap_stream_id]\n            self.assertEqual(expected_row_count, row_count)\n    state = menagerie.get_state(conn_id)\n    expected_state_keys = {'last_replication_method', 'replication_key_name', 'replication_key_type', 'replication_key_value', 'version'}\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            bookmark = state['bookmarks'][tap_stream_id]\n            stream = tap_stream_id.split('-')[1]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            replication_key = bookmark['replication_key_name']\n            replication_key_type = bookmark['replication_key_type']\n            self.assertSetEqual(expected_state_keys, set(bookmark.keys()))\n            for key in expected_state_keys:\n                self.assertIsNotNone(bookmark[key])\n            self.assertEqual('INCREMENTAL', bookmark['last_replication_method'])\n            self.assertIn(replication_key, expected_replication_keys)\n            self.assertIn(replication_key_type, VALID_REPLICATION_TYPES)\n            self.assertIsNone(state['currently_syncing'])\n    with get_test_connection() as client:\n        update_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one()\n        client['simple_db']['simple_coll_1'].find_one_and_update({'_id': update_doc_coll_1['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        update_doc_coll_2 = client['simple_db']['simple_coll_2'].find_one()\n        client['simple_db']['simple_coll_2'].find_one_and_update({'_id': update_doc_coll_2['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        for key_name in self.key_names():\n            if key_name == 'int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'double_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                double_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': double_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == '64_bit_int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                bit64_int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': bit64_int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'date_field':\n                date_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': date_based_coll['_id']}, {'$set': {'{}'.format(key_name): datetime(2021, 1, 1, 15, 30, 14, 222000)}})\n            elif key_name == 'timestamp_field':\n                timestamp_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': timestamp_based_coll['_id']}, {'$set': {'{}'.format(key_name): bson.timestamp.Timestamp(1565897157 + 99, 1)}})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 50, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 13, 19, 29, 14, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 51, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 18, 19, 29, 14, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 100, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 21, 19, 29, 14, 578000), 'double_field': 101.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 100, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282100'), '64_bit_int_field': 34359738368 + 100})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 101, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 26, 19, 29, 14, 578000), 'double_field': 102.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 101, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282101'), '64_bit_int_field': 34359738368 + 101})\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 50, 'string_field': z_string_generator(50), 'date_field': datetime(2018, 9, 13, 19, 29, 15, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 51, 'string_field': z_string_generator(51), 'date_field': datetime(2018, 9, 18, 19, 29, 16, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(4, v)\n    for stream_name in self.expected_sync_streams():\n        if stream_name not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            actual = set([x['data']['int_field'] for x in records_by_stream[stream_name]])\n            self.assertEqual(self.expected_incremental_int_fields()[stream_name], actual)\n    no_rep_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one({'int_field': 20})\n    client['simple_db']['simple_coll_1'].find_one_and_update({'_id': no_rep_doc_coll_1['_id']}, {'$set': {'string_field': 'No_replication'}})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    messages_by_stream = runner.get_records_from_target_output()\n    second_state = menagerie.get_state(conn_id)\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    doc_from_simple_coll_1 = records_by_stream['simple_coll_1']\n    self.assertNotEqual(doc_from_simple_coll_1[0]['data']['_id'], no_rep_doc_coll_1['_id'])\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(1, v)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        additional_md = []\n        if stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_1':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'LOG_BASED'}}]\n        elif stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_2':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': 'timestamp_field'}}]\n        else:\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': stream_catalog['stream_name'].replace('simple_coll_', '')}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    self.assertDictEqual(record_count_by_stream, self.expected_last_sync_row_counts())",
        "mutated": [
            "def test_run(self):\n    if False:\n        i = 10\n    conn_id = connections.ensure_connection(self)\n    check_job_name = runner.run_check_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, check_job_name)\n    menagerie.verify_check_exit_status(self, exit_status, check_job_name)\n    catalog = menagerie.get_catalog(conn_id)\n    found_catalogs = menagerie.get_catalogs(conn_id)\n    found_streams = {entry['tap_stream_id'] for entry in catalog['streams']}\n    self.assertSetEqual(self.expected_check_streams(), found_streams)\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            stream = tap_stream_id.split('-')[1]\n            expected_primary_key = self.expected_pks()[stream]\n            expected_row_count = self.expected_row_counts()[stream]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            found_stream = [entry for entry in catalog['streams'] if entry['tap_stream_id'] == tap_stream_id][0]\n            stream_metadata = [entry['metadata'] for entry in found_stream['metadata'] if entry['breadcrumb'] == []][0]\n            primary_key = set(stream_metadata.get('table-key-properties'))\n            row_count = stream_metadata.get('row-count')\n            replication_key = set(stream_metadata.get('valid-replication-keys'))\n            self.assertSetEqual(expected_primary_key, primary_key)\n            self.assertEqual(expected_row_count, row_count)\n            self.assertSetEqual(replication_key, expected_replication_keys)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        rep_key = 'date_field'\n        for key in self.key_names():\n            if key in stream_catalog['stream_name']:\n                rep_key = key\n        additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': rep_key}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    expected_schema = {'type': 'object'}\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            persisted_schema = messages_by_stream[tap_stream_id]['schema']\n            self.assertDictEqual(expected_schema, persisted_schema)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            expected_row_count = self.expected_row_counts()[tap_stream_id]\n            row_count = record_count_by_stream[tap_stream_id]\n            self.assertEqual(expected_row_count, row_count)\n    state = menagerie.get_state(conn_id)\n    expected_state_keys = {'last_replication_method', 'replication_key_name', 'replication_key_type', 'replication_key_value', 'version'}\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            bookmark = state['bookmarks'][tap_stream_id]\n            stream = tap_stream_id.split('-')[1]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            replication_key = bookmark['replication_key_name']\n            replication_key_type = bookmark['replication_key_type']\n            self.assertSetEqual(expected_state_keys, set(bookmark.keys()))\n            for key in expected_state_keys:\n                self.assertIsNotNone(bookmark[key])\n            self.assertEqual('INCREMENTAL', bookmark['last_replication_method'])\n            self.assertIn(replication_key, expected_replication_keys)\n            self.assertIn(replication_key_type, VALID_REPLICATION_TYPES)\n            self.assertIsNone(state['currently_syncing'])\n    with get_test_connection() as client:\n        update_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one()\n        client['simple_db']['simple_coll_1'].find_one_and_update({'_id': update_doc_coll_1['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        update_doc_coll_2 = client['simple_db']['simple_coll_2'].find_one()\n        client['simple_db']['simple_coll_2'].find_one_and_update({'_id': update_doc_coll_2['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        for key_name in self.key_names():\n            if key_name == 'int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'double_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                double_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': double_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == '64_bit_int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                bit64_int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': bit64_int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'date_field':\n                date_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': date_based_coll['_id']}, {'$set': {'{}'.format(key_name): datetime(2021, 1, 1, 15, 30, 14, 222000)}})\n            elif key_name == 'timestamp_field':\n                timestamp_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': timestamp_based_coll['_id']}, {'$set': {'{}'.format(key_name): bson.timestamp.Timestamp(1565897157 + 99, 1)}})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 50, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 13, 19, 29, 14, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 51, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 18, 19, 29, 14, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 100, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 21, 19, 29, 14, 578000), 'double_field': 101.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 100, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282100'), '64_bit_int_field': 34359738368 + 100})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 101, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 26, 19, 29, 14, 578000), 'double_field': 102.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 101, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282101'), '64_bit_int_field': 34359738368 + 101})\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 50, 'string_field': z_string_generator(50), 'date_field': datetime(2018, 9, 13, 19, 29, 15, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 51, 'string_field': z_string_generator(51), 'date_field': datetime(2018, 9, 18, 19, 29, 16, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(4, v)\n    for stream_name in self.expected_sync_streams():\n        if stream_name not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            actual = set([x['data']['int_field'] for x in records_by_stream[stream_name]])\n            self.assertEqual(self.expected_incremental_int_fields()[stream_name], actual)\n    no_rep_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one({'int_field': 20})\n    client['simple_db']['simple_coll_1'].find_one_and_update({'_id': no_rep_doc_coll_1['_id']}, {'$set': {'string_field': 'No_replication'}})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    messages_by_stream = runner.get_records_from_target_output()\n    second_state = menagerie.get_state(conn_id)\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    doc_from_simple_coll_1 = records_by_stream['simple_coll_1']\n    self.assertNotEqual(doc_from_simple_coll_1[0]['data']['_id'], no_rep_doc_coll_1['_id'])\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(1, v)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        additional_md = []\n        if stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_1':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'LOG_BASED'}}]\n        elif stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_2':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': 'timestamp_field'}}]\n        else:\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': stream_catalog['stream_name'].replace('simple_coll_', '')}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    self.assertDictEqual(record_count_by_stream, self.expected_last_sync_row_counts())",
            "def test_run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    conn_id = connections.ensure_connection(self)\n    check_job_name = runner.run_check_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, check_job_name)\n    menagerie.verify_check_exit_status(self, exit_status, check_job_name)\n    catalog = menagerie.get_catalog(conn_id)\n    found_catalogs = menagerie.get_catalogs(conn_id)\n    found_streams = {entry['tap_stream_id'] for entry in catalog['streams']}\n    self.assertSetEqual(self.expected_check_streams(), found_streams)\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            stream = tap_stream_id.split('-')[1]\n            expected_primary_key = self.expected_pks()[stream]\n            expected_row_count = self.expected_row_counts()[stream]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            found_stream = [entry for entry in catalog['streams'] if entry['tap_stream_id'] == tap_stream_id][0]\n            stream_metadata = [entry['metadata'] for entry in found_stream['metadata'] if entry['breadcrumb'] == []][0]\n            primary_key = set(stream_metadata.get('table-key-properties'))\n            row_count = stream_metadata.get('row-count')\n            replication_key = set(stream_metadata.get('valid-replication-keys'))\n            self.assertSetEqual(expected_primary_key, primary_key)\n            self.assertEqual(expected_row_count, row_count)\n            self.assertSetEqual(replication_key, expected_replication_keys)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        rep_key = 'date_field'\n        for key in self.key_names():\n            if key in stream_catalog['stream_name']:\n                rep_key = key\n        additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': rep_key}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    expected_schema = {'type': 'object'}\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            persisted_schema = messages_by_stream[tap_stream_id]['schema']\n            self.assertDictEqual(expected_schema, persisted_schema)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            expected_row_count = self.expected_row_counts()[tap_stream_id]\n            row_count = record_count_by_stream[tap_stream_id]\n            self.assertEqual(expected_row_count, row_count)\n    state = menagerie.get_state(conn_id)\n    expected_state_keys = {'last_replication_method', 'replication_key_name', 'replication_key_type', 'replication_key_value', 'version'}\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            bookmark = state['bookmarks'][tap_stream_id]\n            stream = tap_stream_id.split('-')[1]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            replication_key = bookmark['replication_key_name']\n            replication_key_type = bookmark['replication_key_type']\n            self.assertSetEqual(expected_state_keys, set(bookmark.keys()))\n            for key in expected_state_keys:\n                self.assertIsNotNone(bookmark[key])\n            self.assertEqual('INCREMENTAL', bookmark['last_replication_method'])\n            self.assertIn(replication_key, expected_replication_keys)\n            self.assertIn(replication_key_type, VALID_REPLICATION_TYPES)\n            self.assertIsNone(state['currently_syncing'])\n    with get_test_connection() as client:\n        update_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one()\n        client['simple_db']['simple_coll_1'].find_one_and_update({'_id': update_doc_coll_1['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        update_doc_coll_2 = client['simple_db']['simple_coll_2'].find_one()\n        client['simple_db']['simple_coll_2'].find_one_and_update({'_id': update_doc_coll_2['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        for key_name in self.key_names():\n            if key_name == 'int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'double_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                double_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': double_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == '64_bit_int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                bit64_int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': bit64_int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'date_field':\n                date_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': date_based_coll['_id']}, {'$set': {'{}'.format(key_name): datetime(2021, 1, 1, 15, 30, 14, 222000)}})\n            elif key_name == 'timestamp_field':\n                timestamp_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': timestamp_based_coll['_id']}, {'$set': {'{}'.format(key_name): bson.timestamp.Timestamp(1565897157 + 99, 1)}})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 50, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 13, 19, 29, 14, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 51, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 18, 19, 29, 14, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 100, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 21, 19, 29, 14, 578000), 'double_field': 101.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 100, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282100'), '64_bit_int_field': 34359738368 + 100})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 101, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 26, 19, 29, 14, 578000), 'double_field': 102.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 101, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282101'), '64_bit_int_field': 34359738368 + 101})\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 50, 'string_field': z_string_generator(50), 'date_field': datetime(2018, 9, 13, 19, 29, 15, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 51, 'string_field': z_string_generator(51), 'date_field': datetime(2018, 9, 18, 19, 29, 16, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(4, v)\n    for stream_name in self.expected_sync_streams():\n        if stream_name not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            actual = set([x['data']['int_field'] for x in records_by_stream[stream_name]])\n            self.assertEqual(self.expected_incremental_int_fields()[stream_name], actual)\n    no_rep_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one({'int_field': 20})\n    client['simple_db']['simple_coll_1'].find_one_and_update({'_id': no_rep_doc_coll_1['_id']}, {'$set': {'string_field': 'No_replication'}})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    messages_by_stream = runner.get_records_from_target_output()\n    second_state = menagerie.get_state(conn_id)\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    doc_from_simple_coll_1 = records_by_stream['simple_coll_1']\n    self.assertNotEqual(doc_from_simple_coll_1[0]['data']['_id'], no_rep_doc_coll_1['_id'])\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(1, v)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        additional_md = []\n        if stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_1':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'LOG_BASED'}}]\n        elif stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_2':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': 'timestamp_field'}}]\n        else:\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': stream_catalog['stream_name'].replace('simple_coll_', '')}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    self.assertDictEqual(record_count_by_stream, self.expected_last_sync_row_counts())",
            "def test_run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    conn_id = connections.ensure_connection(self)\n    check_job_name = runner.run_check_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, check_job_name)\n    menagerie.verify_check_exit_status(self, exit_status, check_job_name)\n    catalog = menagerie.get_catalog(conn_id)\n    found_catalogs = menagerie.get_catalogs(conn_id)\n    found_streams = {entry['tap_stream_id'] for entry in catalog['streams']}\n    self.assertSetEqual(self.expected_check_streams(), found_streams)\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            stream = tap_stream_id.split('-')[1]\n            expected_primary_key = self.expected_pks()[stream]\n            expected_row_count = self.expected_row_counts()[stream]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            found_stream = [entry for entry in catalog['streams'] if entry['tap_stream_id'] == tap_stream_id][0]\n            stream_metadata = [entry['metadata'] for entry in found_stream['metadata'] if entry['breadcrumb'] == []][0]\n            primary_key = set(stream_metadata.get('table-key-properties'))\n            row_count = stream_metadata.get('row-count')\n            replication_key = set(stream_metadata.get('valid-replication-keys'))\n            self.assertSetEqual(expected_primary_key, primary_key)\n            self.assertEqual(expected_row_count, row_count)\n            self.assertSetEqual(replication_key, expected_replication_keys)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        rep_key = 'date_field'\n        for key in self.key_names():\n            if key in stream_catalog['stream_name']:\n                rep_key = key\n        additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': rep_key}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    expected_schema = {'type': 'object'}\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            persisted_schema = messages_by_stream[tap_stream_id]['schema']\n            self.assertDictEqual(expected_schema, persisted_schema)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            expected_row_count = self.expected_row_counts()[tap_stream_id]\n            row_count = record_count_by_stream[tap_stream_id]\n            self.assertEqual(expected_row_count, row_count)\n    state = menagerie.get_state(conn_id)\n    expected_state_keys = {'last_replication_method', 'replication_key_name', 'replication_key_type', 'replication_key_value', 'version'}\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            bookmark = state['bookmarks'][tap_stream_id]\n            stream = tap_stream_id.split('-')[1]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            replication_key = bookmark['replication_key_name']\n            replication_key_type = bookmark['replication_key_type']\n            self.assertSetEqual(expected_state_keys, set(bookmark.keys()))\n            for key in expected_state_keys:\n                self.assertIsNotNone(bookmark[key])\n            self.assertEqual('INCREMENTAL', bookmark['last_replication_method'])\n            self.assertIn(replication_key, expected_replication_keys)\n            self.assertIn(replication_key_type, VALID_REPLICATION_TYPES)\n            self.assertIsNone(state['currently_syncing'])\n    with get_test_connection() as client:\n        update_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one()\n        client['simple_db']['simple_coll_1'].find_one_and_update({'_id': update_doc_coll_1['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        update_doc_coll_2 = client['simple_db']['simple_coll_2'].find_one()\n        client['simple_db']['simple_coll_2'].find_one_and_update({'_id': update_doc_coll_2['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        for key_name in self.key_names():\n            if key_name == 'int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'double_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                double_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': double_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == '64_bit_int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                bit64_int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': bit64_int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'date_field':\n                date_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': date_based_coll['_id']}, {'$set': {'{}'.format(key_name): datetime(2021, 1, 1, 15, 30, 14, 222000)}})\n            elif key_name == 'timestamp_field':\n                timestamp_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': timestamp_based_coll['_id']}, {'$set': {'{}'.format(key_name): bson.timestamp.Timestamp(1565897157 + 99, 1)}})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 50, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 13, 19, 29, 14, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 51, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 18, 19, 29, 14, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 100, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 21, 19, 29, 14, 578000), 'double_field': 101.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 100, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282100'), '64_bit_int_field': 34359738368 + 100})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 101, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 26, 19, 29, 14, 578000), 'double_field': 102.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 101, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282101'), '64_bit_int_field': 34359738368 + 101})\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 50, 'string_field': z_string_generator(50), 'date_field': datetime(2018, 9, 13, 19, 29, 15, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 51, 'string_field': z_string_generator(51), 'date_field': datetime(2018, 9, 18, 19, 29, 16, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(4, v)\n    for stream_name in self.expected_sync_streams():\n        if stream_name not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            actual = set([x['data']['int_field'] for x in records_by_stream[stream_name]])\n            self.assertEqual(self.expected_incremental_int_fields()[stream_name], actual)\n    no_rep_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one({'int_field': 20})\n    client['simple_db']['simple_coll_1'].find_one_and_update({'_id': no_rep_doc_coll_1['_id']}, {'$set': {'string_field': 'No_replication'}})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    messages_by_stream = runner.get_records_from_target_output()\n    second_state = menagerie.get_state(conn_id)\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    doc_from_simple_coll_1 = records_by_stream['simple_coll_1']\n    self.assertNotEqual(doc_from_simple_coll_1[0]['data']['_id'], no_rep_doc_coll_1['_id'])\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(1, v)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        additional_md = []\n        if stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_1':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'LOG_BASED'}}]\n        elif stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_2':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': 'timestamp_field'}}]\n        else:\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': stream_catalog['stream_name'].replace('simple_coll_', '')}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    self.assertDictEqual(record_count_by_stream, self.expected_last_sync_row_counts())",
            "def test_run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    conn_id = connections.ensure_connection(self)\n    check_job_name = runner.run_check_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, check_job_name)\n    menagerie.verify_check_exit_status(self, exit_status, check_job_name)\n    catalog = menagerie.get_catalog(conn_id)\n    found_catalogs = menagerie.get_catalogs(conn_id)\n    found_streams = {entry['tap_stream_id'] for entry in catalog['streams']}\n    self.assertSetEqual(self.expected_check_streams(), found_streams)\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            stream = tap_stream_id.split('-')[1]\n            expected_primary_key = self.expected_pks()[stream]\n            expected_row_count = self.expected_row_counts()[stream]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            found_stream = [entry for entry in catalog['streams'] if entry['tap_stream_id'] == tap_stream_id][0]\n            stream_metadata = [entry['metadata'] for entry in found_stream['metadata'] if entry['breadcrumb'] == []][0]\n            primary_key = set(stream_metadata.get('table-key-properties'))\n            row_count = stream_metadata.get('row-count')\n            replication_key = set(stream_metadata.get('valid-replication-keys'))\n            self.assertSetEqual(expected_primary_key, primary_key)\n            self.assertEqual(expected_row_count, row_count)\n            self.assertSetEqual(replication_key, expected_replication_keys)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        rep_key = 'date_field'\n        for key in self.key_names():\n            if key in stream_catalog['stream_name']:\n                rep_key = key\n        additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': rep_key}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    expected_schema = {'type': 'object'}\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            persisted_schema = messages_by_stream[tap_stream_id]['schema']\n            self.assertDictEqual(expected_schema, persisted_schema)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            expected_row_count = self.expected_row_counts()[tap_stream_id]\n            row_count = record_count_by_stream[tap_stream_id]\n            self.assertEqual(expected_row_count, row_count)\n    state = menagerie.get_state(conn_id)\n    expected_state_keys = {'last_replication_method', 'replication_key_name', 'replication_key_type', 'replication_key_value', 'version'}\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            bookmark = state['bookmarks'][tap_stream_id]\n            stream = tap_stream_id.split('-')[1]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            replication_key = bookmark['replication_key_name']\n            replication_key_type = bookmark['replication_key_type']\n            self.assertSetEqual(expected_state_keys, set(bookmark.keys()))\n            for key in expected_state_keys:\n                self.assertIsNotNone(bookmark[key])\n            self.assertEqual('INCREMENTAL', bookmark['last_replication_method'])\n            self.assertIn(replication_key, expected_replication_keys)\n            self.assertIn(replication_key_type, VALID_REPLICATION_TYPES)\n            self.assertIsNone(state['currently_syncing'])\n    with get_test_connection() as client:\n        update_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one()\n        client['simple_db']['simple_coll_1'].find_one_and_update({'_id': update_doc_coll_1['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        update_doc_coll_2 = client['simple_db']['simple_coll_2'].find_one()\n        client['simple_db']['simple_coll_2'].find_one_and_update({'_id': update_doc_coll_2['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        for key_name in self.key_names():\n            if key_name == 'int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'double_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                double_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': double_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == '64_bit_int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                bit64_int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': bit64_int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'date_field':\n                date_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': date_based_coll['_id']}, {'$set': {'{}'.format(key_name): datetime(2021, 1, 1, 15, 30, 14, 222000)}})\n            elif key_name == 'timestamp_field':\n                timestamp_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': timestamp_based_coll['_id']}, {'$set': {'{}'.format(key_name): bson.timestamp.Timestamp(1565897157 + 99, 1)}})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 50, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 13, 19, 29, 14, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 51, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 18, 19, 29, 14, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 100, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 21, 19, 29, 14, 578000), 'double_field': 101.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 100, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282100'), '64_bit_int_field': 34359738368 + 100})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 101, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 26, 19, 29, 14, 578000), 'double_field': 102.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 101, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282101'), '64_bit_int_field': 34359738368 + 101})\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 50, 'string_field': z_string_generator(50), 'date_field': datetime(2018, 9, 13, 19, 29, 15, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 51, 'string_field': z_string_generator(51), 'date_field': datetime(2018, 9, 18, 19, 29, 16, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(4, v)\n    for stream_name in self.expected_sync_streams():\n        if stream_name not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            actual = set([x['data']['int_field'] for x in records_by_stream[stream_name]])\n            self.assertEqual(self.expected_incremental_int_fields()[stream_name], actual)\n    no_rep_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one({'int_field': 20})\n    client['simple_db']['simple_coll_1'].find_one_and_update({'_id': no_rep_doc_coll_1['_id']}, {'$set': {'string_field': 'No_replication'}})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    messages_by_stream = runner.get_records_from_target_output()\n    second_state = menagerie.get_state(conn_id)\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    doc_from_simple_coll_1 = records_by_stream['simple_coll_1']\n    self.assertNotEqual(doc_from_simple_coll_1[0]['data']['_id'], no_rep_doc_coll_1['_id'])\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(1, v)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        additional_md = []\n        if stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_1':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'LOG_BASED'}}]\n        elif stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_2':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': 'timestamp_field'}}]\n        else:\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': stream_catalog['stream_name'].replace('simple_coll_', '')}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    self.assertDictEqual(record_count_by_stream, self.expected_last_sync_row_counts())",
            "def test_run(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    conn_id = connections.ensure_connection(self)\n    check_job_name = runner.run_check_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, check_job_name)\n    menagerie.verify_check_exit_status(self, exit_status, check_job_name)\n    catalog = menagerie.get_catalog(conn_id)\n    found_catalogs = menagerie.get_catalogs(conn_id)\n    found_streams = {entry['tap_stream_id'] for entry in catalog['streams']}\n    self.assertSetEqual(self.expected_check_streams(), found_streams)\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            stream = tap_stream_id.split('-')[1]\n            expected_primary_key = self.expected_pks()[stream]\n            expected_row_count = self.expected_row_counts()[stream]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            found_stream = [entry for entry in catalog['streams'] if entry['tap_stream_id'] == tap_stream_id][0]\n            stream_metadata = [entry['metadata'] for entry in found_stream['metadata'] if entry['breadcrumb'] == []][0]\n            primary_key = set(stream_metadata.get('table-key-properties'))\n            row_count = stream_metadata.get('row-count')\n            replication_key = set(stream_metadata.get('valid-replication-keys'))\n            self.assertSetEqual(expected_primary_key, primary_key)\n            self.assertEqual(expected_row_count, row_count)\n            self.assertSetEqual(replication_key, expected_replication_keys)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        rep_key = 'date_field'\n        for key in self.key_names():\n            if key in stream_catalog['stream_name']:\n                rep_key = key\n        additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': rep_key}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    expected_schema = {'type': 'object'}\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            persisted_schema = messages_by_stream[tap_stream_id]['schema']\n            self.assertDictEqual(expected_schema, persisted_schema)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for tap_stream_id in self.expected_sync_streams():\n        with self.subTest(stream=tap_stream_id):\n            expected_row_count = self.expected_row_counts()[tap_stream_id]\n            row_count = record_count_by_stream[tap_stream_id]\n            self.assertEqual(expected_row_count, row_count)\n    state = menagerie.get_state(conn_id)\n    expected_state_keys = {'last_replication_method', 'replication_key_name', 'replication_key_type', 'replication_key_value', 'version'}\n    for tap_stream_id in self.expected_check_streams():\n        with self.subTest(stream=tap_stream_id):\n            bookmark = state['bookmarks'][tap_stream_id]\n            stream = tap_stream_id.split('-')[1]\n            expected_replication_keys = self.expected_valid_replication_keys()[stream]\n            replication_key = bookmark['replication_key_name']\n            replication_key_type = bookmark['replication_key_type']\n            self.assertSetEqual(expected_state_keys, set(bookmark.keys()))\n            for key in expected_state_keys:\n                self.assertIsNotNone(bookmark[key])\n            self.assertEqual('INCREMENTAL', bookmark['last_replication_method'])\n            self.assertIn(replication_key, expected_replication_keys)\n            self.assertIn(replication_key_type, VALID_REPLICATION_TYPES)\n            self.assertIsNone(state['currently_syncing'])\n    with get_test_connection() as client:\n        update_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one()\n        client['simple_db']['simple_coll_1'].find_one_and_update({'_id': update_doc_coll_1['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        update_doc_coll_2 = client['simple_db']['simple_coll_2'].find_one()\n        client['simple_db']['simple_coll_2'].find_one_and_update({'_id': update_doc_coll_2['_id']}, {'$set': {'date_field': datetime(2020, 1, 1, 19, 29, 14, 578000)}})\n        for key_name in self.key_names():\n            if key_name == 'int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'double_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                double_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': double_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == '64_bit_int_field':\n                doc_to_update = client['simple_db']['simple_coll_{}'.format(key_name)].find_one(sort=[('{}'.format(key_name), -1)])\n                value = doc_to_update['{}'.format(key_name)]\n                bit64_int_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': bit64_int_based_coll['_id']}, {'$set': {'{}'.format(key_name): value + 3}})\n            elif key_name == 'date_field':\n                date_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': date_based_coll['_id']}, {'$set': {'{}'.format(key_name): datetime(2021, 1, 1, 15, 30, 14, 222000)}})\n            elif key_name == 'timestamp_field':\n                timestamp_based_coll = client['simple_db']['simple_coll_{}'.format(key_name)].find_one()\n                client['simple_db']['simple_coll_{}'.format(key_name)].find_one_and_update({'_id': timestamp_based_coll['_id']}, {'$set': {'{}'.format(key_name): bson.timestamp.Timestamp(1565897157 + 99, 1)}})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 50, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 13, 19, 29, 14, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n        client['simple_db']['simple_coll_1'].insert_one({'int_field': 51, 'string_field': z_string_generator(), 'date_field': datetime(2018, 9, 18, 19, 29, 14, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 100, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 21, 19, 29, 14, 578000), 'double_field': 101.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 100, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282100'), '64_bit_int_field': 34359738368 + 100})\n        client['simple_db']['simple_coll_2'].insert_one({'int_field': 101, 'string_field': z_string_generator(), 'date_field': datetime(2019, 5, 26, 19, 29, 14, 578000), 'double_field': 102.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 101, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282101'), '64_bit_int_field': 34359738368 + 101})\n        for key_name in self.key_names():\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 50, 'string_field': z_string_generator(50), 'date_field': datetime(2018, 9, 13, 19, 29, 15, 578000), 'double_field': 51.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 50, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282050'), '64_bit_int_field': 34359738368 + 50})\n            client['simple_db']['simple_coll_{}'.format(key_name)].insert_one({'int_field': 51, 'string_field': z_string_generator(51), 'date_field': datetime(2018, 9, 18, 19, 29, 16, 578000), 'double_field': 52.001, 'timestamp_field': bson.timestamp.Timestamp(1565897157 + 51, 1), 'uuid_field': uuid.UUID('3e139ff5-d622-45c6-bf9e-1dfec7282051'), '64_bit_int_field': 34359738368 + 51})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    messages_by_stream = runner.get_records_from_target_output()\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(4, v)\n    for stream_name in self.expected_sync_streams():\n        if stream_name not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            actual = set([x['data']['int_field'] for x in records_by_stream[stream_name]])\n            self.assertEqual(self.expected_incremental_int_fields()[stream_name], actual)\n    no_rep_doc_coll_1 = client['simple_db']['simple_coll_1'].find_one({'int_field': 20})\n    client['simple_db']['simple_coll_1'].find_one_and_update({'_id': no_rep_doc_coll_1['_id']}, {'$set': {'string_field': 'No_replication'}})\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    messages_by_stream = runner.get_records_from_target_output()\n    second_state = menagerie.get_state(conn_id)\n    records_by_stream = {}\n    for stream_name in self.expected_sync_streams():\n        records_by_stream[stream_name] = [x for x in messages_by_stream[stream_name]['messages'] if x.get('action') == 'upsert']\n    doc_from_simple_coll_1 = records_by_stream['simple_coll_1']\n    self.assertNotEqual(doc_from_simple_coll_1[0]['data']['_id'], no_rep_doc_coll_1['_id'])\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    for (k, v) in record_count_by_stream.items():\n        if k not in ('simple_coll_uuid_field', 'simple_coll_string_field'):\n            self.assertEqual(1, v)\n    for stream_catalog in found_catalogs:\n        annotated_schema = menagerie.get_annotated_schema(conn_id, stream_catalog['stream_id'])\n        additional_md = []\n        if stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_1':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'LOG_BASED'}}]\n        elif stream_catalog['tap_stream_id'] == 'simple_db-simple_coll_2':\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': 'timestamp_field'}}]\n        else:\n            additional_md = [{'breadcrumb': [], 'metadata': {'replication-method': 'INCREMENTAL', 'replication-key': stream_catalog['stream_name'].replace('simple_coll_', '')}}]\n        selected_metadata = connections.select_catalog_and_fields_via_metadata(conn_id, stream_catalog, annotated_schema, additional_md)\n    sync_job_name = runner.run_sync_mode(self, conn_id)\n    exit_status = menagerie.get_exit_status(conn_id, sync_job_name)\n    menagerie.verify_sync_exit_status(self, exit_status, sync_job_name)\n    record_count_by_stream = runner.examine_target_output_file(self, conn_id, self.expected_sync_streams(), self.expected_pks())\n    self.assertDictEqual(record_count_by_stream, self.expected_last_sync_row_counts())"
        ]
    }
]