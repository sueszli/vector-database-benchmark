[
    {
        "func_name": "f",
        "original": "def f(n):\n    poly = np.array([1, np.log(n), np.log(n) ** 2])\n    return np.exp(poly.dot(params.T))",
        "mutated": [
            "def f(n):\n    if False:\n        i = 10\n    poly = np.array([1, np.log(n), np.log(n) ** 2])\n    return np.exp(poly.dot(params.T))",
            "def f(n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    poly = np.array([1, np.log(n), np.log(n) ** 2])\n    return np.exp(poly.dot(params.T))",
            "def f(n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    poly = np.array([1, np.log(n), np.log(n) ** 2])\n    return np.exp(poly.dot(params.T))",
            "def f(n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    poly = np.array([1, np.log(n), np.log(n) ** 2])\n    return np.exp(poly.dot(params.T))",
            "def f(n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    poly = np.array([1, np.log(n), np.log(n) ** 2])\n    return np.exp(poly.dot(params.T))"
        ]
    },
    {
        "func_name": "_make_asymptotic_function",
        "original": "def _make_asymptotic_function(params):\n    \"\"\"\n    Generates an asymptotic distribution callable from a param matrix\n\n    Polynomial is a[0] * x**(-1/2) + a[1] * x**(-1) + a[2] * x**(-3/2)\n\n    Parameters\n    ----------\n    params : ndarray\n        Array with shape (nalpha, 3) where nalpha is the number of\n        significance levels\n    \"\"\"\n\n    def f(n):\n        poly = np.array([1, np.log(n), np.log(n) ** 2])\n        return np.exp(poly.dot(params.T))\n    return f",
        "mutated": [
            "def _make_asymptotic_function(params):\n    if False:\n        i = 10\n    '\\n    Generates an asymptotic distribution callable from a param matrix\\n\\n    Polynomial is a[0] * x**(-1/2) + a[1] * x**(-1) + a[2] * x**(-3/2)\\n\\n    Parameters\\n    ----------\\n    params : ndarray\\n        Array with shape (nalpha, 3) where nalpha is the number of\\n        significance levels\\n    '\n\n    def f(n):\n        poly = np.array([1, np.log(n), np.log(n) ** 2])\n        return np.exp(poly.dot(params.T))\n    return f",
            "def _make_asymptotic_function(params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Generates an asymptotic distribution callable from a param matrix\\n\\n    Polynomial is a[0] * x**(-1/2) + a[1] * x**(-1) + a[2] * x**(-3/2)\\n\\n    Parameters\\n    ----------\\n    params : ndarray\\n        Array with shape (nalpha, 3) where nalpha is the number of\\n        significance levels\\n    '\n\n    def f(n):\n        poly = np.array([1, np.log(n), np.log(n) ** 2])\n        return np.exp(poly.dot(params.T))\n    return f",
            "def _make_asymptotic_function(params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Generates an asymptotic distribution callable from a param matrix\\n\\n    Polynomial is a[0] * x**(-1/2) + a[1] * x**(-1) + a[2] * x**(-3/2)\\n\\n    Parameters\\n    ----------\\n    params : ndarray\\n        Array with shape (nalpha, 3) where nalpha is the number of\\n        significance levels\\n    '\n\n    def f(n):\n        poly = np.array([1, np.log(n), np.log(n) ** 2])\n        return np.exp(poly.dot(params.T))\n    return f",
            "def _make_asymptotic_function(params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Generates an asymptotic distribution callable from a param matrix\\n\\n    Polynomial is a[0] * x**(-1/2) + a[1] * x**(-1) + a[2] * x**(-3/2)\\n\\n    Parameters\\n    ----------\\n    params : ndarray\\n        Array with shape (nalpha, 3) where nalpha is the number of\\n        significance levels\\n    '\n\n    def f(n):\n        poly = np.array([1, np.log(n), np.log(n) ** 2])\n        return np.exp(poly.dot(params.T))\n    return f",
            "def _make_asymptotic_function(params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Generates an asymptotic distribution callable from a param matrix\\n\\n    Polynomial is a[0] * x**(-1/2) + a[1] * x**(-1) + a[2] * x**(-3/2)\\n\\n    Parameters\\n    ----------\\n    params : ndarray\\n        Array with shape (nalpha, 3) where nalpha is the number of\\n        significance levels\\n    '\n\n    def f(n):\n        poly = np.array([1, np.log(n), np.log(n) ** 2])\n        return np.exp(poly.dot(params.T))\n    return f"
        ]
    },
    {
        "func_name": "ksstat",
        "original": "def ksstat(x, cdf, alternative='two_sided', args=()):\n    \"\"\"\n    Calculate statistic for the Kolmogorov-Smirnov test for goodness of fit\n\n    This calculates the test statistic for a test of the distribution G(x) of\n    an observed variable against a given distribution F(x). Under the null\n    hypothesis the two distributions are identical, G(x)=F(x). The\n    alternative hypothesis can be either 'two_sided' (default), 'less'\n    or 'greater'. The KS test is only valid for continuous distributions.\n\n    Parameters\n    ----------\n    x : array_like, 1d\n        array of observations\n    cdf : str or callable\n        string: name of a distribution in scipy.stats\n        callable: function to evaluate cdf\n    alternative : 'two_sided' (default), 'less' or 'greater'\n        defines the alternative hypothesis (see explanation)\n    args : tuple, sequence\n        distribution parameters for call to cdf\n\n\n    Returns\n    -------\n    D : float\n        KS test statistic, either D, D+ or D-\n\n    See Also\n    --------\n    scipy.stats.kstest\n\n    Notes\n    -----\n\n    In the one-sided test, the alternative is that the empirical\n    cumulative distribution function of the random variable is \"less\"\n    or \"greater\" than the cumulative distribution function F(x) of the\n    hypothesis, G(x)<=F(x), resp. G(x)>=F(x).\n\n    In contrast to scipy.stats.kstest, this function only calculates the\n    statistic which can be used either as distance measure or to implement\n    case specific p-values.\n    \"\"\"\n    nobs = float(len(x))\n    if isinstance(cdf, str):\n        cdf = getattr(stats.distributions, cdf).cdf\n    elif hasattr(cdf, 'cdf'):\n        cdf = getattr(cdf, 'cdf')\n    x = np.sort(x)\n    cdfvals = cdf(x, *args)\n    d_plus = (np.arange(1.0, nobs + 1) / nobs - cdfvals).max()\n    d_min = (cdfvals - np.arange(0.0, nobs) / nobs).max()\n    if alternative == 'greater':\n        return d_plus\n    elif alternative == 'less':\n        return d_min\n    return np.max([d_plus, d_min])",
        "mutated": [
            "def ksstat(x, cdf, alternative='two_sided', args=()):\n    if False:\n        i = 10\n    '\\n    Calculate statistic for the Kolmogorov-Smirnov test for goodness of fit\\n\\n    This calculates the test statistic for a test of the distribution G(x) of\\n    an observed variable against a given distribution F(x). Under the null\\n    hypothesis the two distributions are identical, G(x)=F(x). The\\n    alternative hypothesis can be either \\'two_sided\\' (default), \\'less\\'\\n    or \\'greater\\'. The KS test is only valid for continuous distributions.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        array of observations\\n    cdf : str or callable\\n        string: name of a distribution in scipy.stats\\n        callable: function to evaluate cdf\\n    alternative : \\'two_sided\\' (default), \\'less\\' or \\'greater\\'\\n        defines the alternative hypothesis (see explanation)\\n    args : tuple, sequence\\n        distribution parameters for call to cdf\\n\\n\\n    Returns\\n    -------\\n    D : float\\n        KS test statistic, either D, D+ or D-\\n\\n    See Also\\n    --------\\n    scipy.stats.kstest\\n\\n    Notes\\n    -----\\n\\n    In the one-sided test, the alternative is that the empirical\\n    cumulative distribution function of the random variable is \"less\"\\n    or \"greater\" than the cumulative distribution function F(x) of the\\n    hypothesis, G(x)<=F(x), resp. G(x)>=F(x).\\n\\n    In contrast to scipy.stats.kstest, this function only calculates the\\n    statistic which can be used either as distance measure or to implement\\n    case specific p-values.\\n    '\n    nobs = float(len(x))\n    if isinstance(cdf, str):\n        cdf = getattr(stats.distributions, cdf).cdf\n    elif hasattr(cdf, 'cdf'):\n        cdf = getattr(cdf, 'cdf')\n    x = np.sort(x)\n    cdfvals = cdf(x, *args)\n    d_plus = (np.arange(1.0, nobs + 1) / nobs - cdfvals).max()\n    d_min = (cdfvals - np.arange(0.0, nobs) / nobs).max()\n    if alternative == 'greater':\n        return d_plus\n    elif alternative == 'less':\n        return d_min\n    return np.max([d_plus, d_min])",
            "def ksstat(x, cdf, alternative='two_sided', args=()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate statistic for the Kolmogorov-Smirnov test for goodness of fit\\n\\n    This calculates the test statistic for a test of the distribution G(x) of\\n    an observed variable against a given distribution F(x). Under the null\\n    hypothesis the two distributions are identical, G(x)=F(x). The\\n    alternative hypothesis can be either \\'two_sided\\' (default), \\'less\\'\\n    or \\'greater\\'. The KS test is only valid for continuous distributions.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        array of observations\\n    cdf : str or callable\\n        string: name of a distribution in scipy.stats\\n        callable: function to evaluate cdf\\n    alternative : \\'two_sided\\' (default), \\'less\\' or \\'greater\\'\\n        defines the alternative hypothesis (see explanation)\\n    args : tuple, sequence\\n        distribution parameters for call to cdf\\n\\n\\n    Returns\\n    -------\\n    D : float\\n        KS test statistic, either D, D+ or D-\\n\\n    See Also\\n    --------\\n    scipy.stats.kstest\\n\\n    Notes\\n    -----\\n\\n    In the one-sided test, the alternative is that the empirical\\n    cumulative distribution function of the random variable is \"less\"\\n    or \"greater\" than the cumulative distribution function F(x) of the\\n    hypothesis, G(x)<=F(x), resp. G(x)>=F(x).\\n\\n    In contrast to scipy.stats.kstest, this function only calculates the\\n    statistic which can be used either as distance measure or to implement\\n    case specific p-values.\\n    '\n    nobs = float(len(x))\n    if isinstance(cdf, str):\n        cdf = getattr(stats.distributions, cdf).cdf\n    elif hasattr(cdf, 'cdf'):\n        cdf = getattr(cdf, 'cdf')\n    x = np.sort(x)\n    cdfvals = cdf(x, *args)\n    d_plus = (np.arange(1.0, nobs + 1) / nobs - cdfvals).max()\n    d_min = (cdfvals - np.arange(0.0, nobs) / nobs).max()\n    if alternative == 'greater':\n        return d_plus\n    elif alternative == 'less':\n        return d_min\n    return np.max([d_plus, d_min])",
            "def ksstat(x, cdf, alternative='two_sided', args=()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate statistic for the Kolmogorov-Smirnov test for goodness of fit\\n\\n    This calculates the test statistic for a test of the distribution G(x) of\\n    an observed variable against a given distribution F(x). Under the null\\n    hypothesis the two distributions are identical, G(x)=F(x). The\\n    alternative hypothesis can be either \\'two_sided\\' (default), \\'less\\'\\n    or \\'greater\\'. The KS test is only valid for continuous distributions.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        array of observations\\n    cdf : str or callable\\n        string: name of a distribution in scipy.stats\\n        callable: function to evaluate cdf\\n    alternative : \\'two_sided\\' (default), \\'less\\' or \\'greater\\'\\n        defines the alternative hypothesis (see explanation)\\n    args : tuple, sequence\\n        distribution parameters for call to cdf\\n\\n\\n    Returns\\n    -------\\n    D : float\\n        KS test statistic, either D, D+ or D-\\n\\n    See Also\\n    --------\\n    scipy.stats.kstest\\n\\n    Notes\\n    -----\\n\\n    In the one-sided test, the alternative is that the empirical\\n    cumulative distribution function of the random variable is \"less\"\\n    or \"greater\" than the cumulative distribution function F(x) of the\\n    hypothesis, G(x)<=F(x), resp. G(x)>=F(x).\\n\\n    In contrast to scipy.stats.kstest, this function only calculates the\\n    statistic which can be used either as distance measure or to implement\\n    case specific p-values.\\n    '\n    nobs = float(len(x))\n    if isinstance(cdf, str):\n        cdf = getattr(stats.distributions, cdf).cdf\n    elif hasattr(cdf, 'cdf'):\n        cdf = getattr(cdf, 'cdf')\n    x = np.sort(x)\n    cdfvals = cdf(x, *args)\n    d_plus = (np.arange(1.0, nobs + 1) / nobs - cdfvals).max()\n    d_min = (cdfvals - np.arange(0.0, nobs) / nobs).max()\n    if alternative == 'greater':\n        return d_plus\n    elif alternative == 'less':\n        return d_min\n    return np.max([d_plus, d_min])",
            "def ksstat(x, cdf, alternative='two_sided', args=()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate statistic for the Kolmogorov-Smirnov test for goodness of fit\\n\\n    This calculates the test statistic for a test of the distribution G(x) of\\n    an observed variable against a given distribution F(x). Under the null\\n    hypothesis the two distributions are identical, G(x)=F(x). The\\n    alternative hypothesis can be either \\'two_sided\\' (default), \\'less\\'\\n    or \\'greater\\'. The KS test is only valid for continuous distributions.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        array of observations\\n    cdf : str or callable\\n        string: name of a distribution in scipy.stats\\n        callable: function to evaluate cdf\\n    alternative : \\'two_sided\\' (default), \\'less\\' or \\'greater\\'\\n        defines the alternative hypothesis (see explanation)\\n    args : tuple, sequence\\n        distribution parameters for call to cdf\\n\\n\\n    Returns\\n    -------\\n    D : float\\n        KS test statistic, either D, D+ or D-\\n\\n    See Also\\n    --------\\n    scipy.stats.kstest\\n\\n    Notes\\n    -----\\n\\n    In the one-sided test, the alternative is that the empirical\\n    cumulative distribution function of the random variable is \"less\"\\n    or \"greater\" than the cumulative distribution function F(x) of the\\n    hypothesis, G(x)<=F(x), resp. G(x)>=F(x).\\n\\n    In contrast to scipy.stats.kstest, this function only calculates the\\n    statistic which can be used either as distance measure or to implement\\n    case specific p-values.\\n    '\n    nobs = float(len(x))\n    if isinstance(cdf, str):\n        cdf = getattr(stats.distributions, cdf).cdf\n    elif hasattr(cdf, 'cdf'):\n        cdf = getattr(cdf, 'cdf')\n    x = np.sort(x)\n    cdfvals = cdf(x, *args)\n    d_plus = (np.arange(1.0, nobs + 1) / nobs - cdfvals).max()\n    d_min = (cdfvals - np.arange(0.0, nobs) / nobs).max()\n    if alternative == 'greater':\n        return d_plus\n    elif alternative == 'less':\n        return d_min\n    return np.max([d_plus, d_min])",
            "def ksstat(x, cdf, alternative='two_sided', args=()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate statistic for the Kolmogorov-Smirnov test for goodness of fit\\n\\n    This calculates the test statistic for a test of the distribution G(x) of\\n    an observed variable against a given distribution F(x). Under the null\\n    hypothesis the two distributions are identical, G(x)=F(x). The\\n    alternative hypothesis can be either \\'two_sided\\' (default), \\'less\\'\\n    or \\'greater\\'. The KS test is only valid for continuous distributions.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        array of observations\\n    cdf : str or callable\\n        string: name of a distribution in scipy.stats\\n        callable: function to evaluate cdf\\n    alternative : \\'two_sided\\' (default), \\'less\\' or \\'greater\\'\\n        defines the alternative hypothesis (see explanation)\\n    args : tuple, sequence\\n        distribution parameters for call to cdf\\n\\n\\n    Returns\\n    -------\\n    D : float\\n        KS test statistic, either D, D+ or D-\\n\\n    See Also\\n    --------\\n    scipy.stats.kstest\\n\\n    Notes\\n    -----\\n\\n    In the one-sided test, the alternative is that the empirical\\n    cumulative distribution function of the random variable is \"less\"\\n    or \"greater\" than the cumulative distribution function F(x) of the\\n    hypothesis, G(x)<=F(x), resp. G(x)>=F(x).\\n\\n    In contrast to scipy.stats.kstest, this function only calculates the\\n    statistic which can be used either as distance measure or to implement\\n    case specific p-values.\\n    '\n    nobs = float(len(x))\n    if isinstance(cdf, str):\n        cdf = getattr(stats.distributions, cdf).cdf\n    elif hasattr(cdf, 'cdf'):\n        cdf = getattr(cdf, 'cdf')\n    x = np.sort(x)\n    cdfvals = cdf(x, *args)\n    d_plus = (np.arange(1.0, nobs + 1) / nobs - cdfvals).max()\n    d_min = (cdfvals - np.arange(0.0, nobs) / nobs).max()\n    if alternative == 'greater':\n        return d_plus\n    elif alternative == 'less':\n        return d_min\n    return np.max([d_plus, d_min])"
        ]
    },
    {
        "func_name": "get_lilliefors_table",
        "original": "def get_lilliefors_table(dist='norm'):\n    \"\"\"\n    Generates tables for significance levels of Lilliefors test statistics\n\n    Tables for available normal and exponential distribution testing,\n    as specified in Lilliefors references above\n\n    Parameters\n    ----------\n    dist : str\n        distribution being tested in set {'norm', 'exp'}.\n\n    Returns\n    -------\n    lf : TableDist object.\n        table of critical values\n    \"\"\"\n    alpha = 1 - np.array(PERCENTILES) / 100.0\n    alpha = alpha[::-1]\n    dist = 'normal' if dist == 'norm' else dist\n    if dist not in critical_values:\n        raise ValueError(\"Invalid dist parameter. Must be 'norm' or 'exp'\")\n    cv_data = critical_values[dist]\n    acv_data = asymp_critical_values[dist]\n    size = np.array(sorted(cv_data), dtype=float)\n    crit_lf = np.array([cv_data[key] for key in sorted(cv_data)])\n    crit_lf = crit_lf[:, ::-1]\n    asym_params = np.array([acv_data[key] for key in sorted(acv_data)])\n    asymp_fn = _make_asymptotic_function(asym_params[::-1])\n    lf = TableDist(alpha, size, crit_lf, asymptotic=asymp_fn)\n    return lf",
        "mutated": [
            "def get_lilliefors_table(dist='norm'):\n    if False:\n        i = 10\n    \"\\n    Generates tables for significance levels of Lilliefors test statistics\\n\\n    Tables for available normal and exponential distribution testing,\\n    as specified in Lilliefors references above\\n\\n    Parameters\\n    ----------\\n    dist : str\\n        distribution being tested in set {'norm', 'exp'}.\\n\\n    Returns\\n    -------\\n    lf : TableDist object.\\n        table of critical values\\n    \"\n    alpha = 1 - np.array(PERCENTILES) / 100.0\n    alpha = alpha[::-1]\n    dist = 'normal' if dist == 'norm' else dist\n    if dist not in critical_values:\n        raise ValueError(\"Invalid dist parameter. Must be 'norm' or 'exp'\")\n    cv_data = critical_values[dist]\n    acv_data = asymp_critical_values[dist]\n    size = np.array(sorted(cv_data), dtype=float)\n    crit_lf = np.array([cv_data[key] for key in sorted(cv_data)])\n    crit_lf = crit_lf[:, ::-1]\n    asym_params = np.array([acv_data[key] for key in sorted(acv_data)])\n    asymp_fn = _make_asymptotic_function(asym_params[::-1])\n    lf = TableDist(alpha, size, crit_lf, asymptotic=asymp_fn)\n    return lf",
            "def get_lilliefors_table(dist='norm'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Generates tables for significance levels of Lilliefors test statistics\\n\\n    Tables for available normal and exponential distribution testing,\\n    as specified in Lilliefors references above\\n\\n    Parameters\\n    ----------\\n    dist : str\\n        distribution being tested in set {'norm', 'exp'}.\\n\\n    Returns\\n    -------\\n    lf : TableDist object.\\n        table of critical values\\n    \"\n    alpha = 1 - np.array(PERCENTILES) / 100.0\n    alpha = alpha[::-1]\n    dist = 'normal' if dist == 'norm' else dist\n    if dist not in critical_values:\n        raise ValueError(\"Invalid dist parameter. Must be 'norm' or 'exp'\")\n    cv_data = critical_values[dist]\n    acv_data = asymp_critical_values[dist]\n    size = np.array(sorted(cv_data), dtype=float)\n    crit_lf = np.array([cv_data[key] for key in sorted(cv_data)])\n    crit_lf = crit_lf[:, ::-1]\n    asym_params = np.array([acv_data[key] for key in sorted(acv_data)])\n    asymp_fn = _make_asymptotic_function(asym_params[::-1])\n    lf = TableDist(alpha, size, crit_lf, asymptotic=asymp_fn)\n    return lf",
            "def get_lilliefors_table(dist='norm'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Generates tables for significance levels of Lilliefors test statistics\\n\\n    Tables for available normal and exponential distribution testing,\\n    as specified in Lilliefors references above\\n\\n    Parameters\\n    ----------\\n    dist : str\\n        distribution being tested in set {'norm', 'exp'}.\\n\\n    Returns\\n    -------\\n    lf : TableDist object.\\n        table of critical values\\n    \"\n    alpha = 1 - np.array(PERCENTILES) / 100.0\n    alpha = alpha[::-1]\n    dist = 'normal' if dist == 'norm' else dist\n    if dist not in critical_values:\n        raise ValueError(\"Invalid dist parameter. Must be 'norm' or 'exp'\")\n    cv_data = critical_values[dist]\n    acv_data = asymp_critical_values[dist]\n    size = np.array(sorted(cv_data), dtype=float)\n    crit_lf = np.array([cv_data[key] for key in sorted(cv_data)])\n    crit_lf = crit_lf[:, ::-1]\n    asym_params = np.array([acv_data[key] for key in sorted(acv_data)])\n    asymp_fn = _make_asymptotic_function(asym_params[::-1])\n    lf = TableDist(alpha, size, crit_lf, asymptotic=asymp_fn)\n    return lf",
            "def get_lilliefors_table(dist='norm'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Generates tables for significance levels of Lilliefors test statistics\\n\\n    Tables for available normal and exponential distribution testing,\\n    as specified in Lilliefors references above\\n\\n    Parameters\\n    ----------\\n    dist : str\\n        distribution being tested in set {'norm', 'exp'}.\\n\\n    Returns\\n    -------\\n    lf : TableDist object.\\n        table of critical values\\n    \"\n    alpha = 1 - np.array(PERCENTILES) / 100.0\n    alpha = alpha[::-1]\n    dist = 'normal' if dist == 'norm' else dist\n    if dist not in critical_values:\n        raise ValueError(\"Invalid dist parameter. Must be 'norm' or 'exp'\")\n    cv_data = critical_values[dist]\n    acv_data = asymp_critical_values[dist]\n    size = np.array(sorted(cv_data), dtype=float)\n    crit_lf = np.array([cv_data[key] for key in sorted(cv_data)])\n    crit_lf = crit_lf[:, ::-1]\n    asym_params = np.array([acv_data[key] for key in sorted(acv_data)])\n    asymp_fn = _make_asymptotic_function(asym_params[::-1])\n    lf = TableDist(alpha, size, crit_lf, asymptotic=asymp_fn)\n    return lf",
            "def get_lilliefors_table(dist='norm'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Generates tables for significance levels of Lilliefors test statistics\\n\\n    Tables for available normal and exponential distribution testing,\\n    as specified in Lilliefors references above\\n\\n    Parameters\\n    ----------\\n    dist : str\\n        distribution being tested in set {'norm', 'exp'}.\\n\\n    Returns\\n    -------\\n    lf : TableDist object.\\n        table of critical values\\n    \"\n    alpha = 1 - np.array(PERCENTILES) / 100.0\n    alpha = alpha[::-1]\n    dist = 'normal' if dist == 'norm' else dist\n    if dist not in critical_values:\n        raise ValueError(\"Invalid dist parameter. Must be 'norm' or 'exp'\")\n    cv_data = critical_values[dist]\n    acv_data = asymp_critical_values[dist]\n    size = np.array(sorted(cv_data), dtype=float)\n    crit_lf = np.array([cv_data[key] for key in sorted(cv_data)])\n    crit_lf = crit_lf[:, ::-1]\n    asym_params = np.array([acv_data[key] for key in sorted(acv_data)])\n    asymp_fn = _make_asymptotic_function(asym_params[::-1])\n    lf = TableDist(alpha, size, crit_lf, asymptotic=asymp_fn)\n    return lf"
        ]
    },
    {
        "func_name": "pval_lf",
        "original": "def pval_lf(d_max, n):\n    \"\"\"\n    Approximate pvalues for Lilliefors test\n\n    This is only valid for pvalues smaller than 0.1 which is not checked in\n    this function.\n\n    Parameters\n    ----------\n    d_max : array_like\n        two-sided Kolmogorov-Smirnov test statistic\n    n : int or float\n        sample size\n\n    Returns\n    -------\n    p-value : float or ndarray\n        pvalue according to approximation formula of Dallal and Wilkinson.\n\n    Notes\n    -----\n    This is mainly a helper function where the calling code should dispatch\n    on bound violations. Therefore it does not check whether the pvalue is in\n    the valid range.\n\n    Precision for the pvalues is around 2 to 3 decimals. This approximation is\n    also used by other statistical packages (e.g. R:fBasics) but might not be\n    the most precise available.\n\n    References\n    ----------\n    DallalWilkinson1986\n    \"\"\"\n    if n > 100:\n        d_max *= (n / 100.0) ** 0.49\n        n = 100\n    pval = np.exp(-7.01256 * d_max ** 2 * (n + 2.78019) + 2.99587 * d_max * np.sqrt(n + 2.78019) - 0.122119 + 0.974598 / np.sqrt(n) + 1.67997 / n)\n    return pval",
        "mutated": [
            "def pval_lf(d_max, n):\n    if False:\n        i = 10\n    '\\n    Approximate pvalues for Lilliefors test\\n\\n    This is only valid for pvalues smaller than 0.1 which is not checked in\\n    this function.\\n\\n    Parameters\\n    ----------\\n    d_max : array_like\\n        two-sided Kolmogorov-Smirnov test statistic\\n    n : int or float\\n        sample size\\n\\n    Returns\\n    -------\\n    p-value : float or ndarray\\n        pvalue according to approximation formula of Dallal and Wilkinson.\\n\\n    Notes\\n    -----\\n    This is mainly a helper function where the calling code should dispatch\\n    on bound violations. Therefore it does not check whether the pvalue is in\\n    the valid range.\\n\\n    Precision for the pvalues is around 2 to 3 decimals. This approximation is\\n    also used by other statistical packages (e.g. R:fBasics) but might not be\\n    the most precise available.\\n\\n    References\\n    ----------\\n    DallalWilkinson1986\\n    '\n    if n > 100:\n        d_max *= (n / 100.0) ** 0.49\n        n = 100\n    pval = np.exp(-7.01256 * d_max ** 2 * (n + 2.78019) + 2.99587 * d_max * np.sqrt(n + 2.78019) - 0.122119 + 0.974598 / np.sqrt(n) + 1.67997 / n)\n    return pval",
            "def pval_lf(d_max, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Approximate pvalues for Lilliefors test\\n\\n    This is only valid for pvalues smaller than 0.1 which is not checked in\\n    this function.\\n\\n    Parameters\\n    ----------\\n    d_max : array_like\\n        two-sided Kolmogorov-Smirnov test statistic\\n    n : int or float\\n        sample size\\n\\n    Returns\\n    -------\\n    p-value : float or ndarray\\n        pvalue according to approximation formula of Dallal and Wilkinson.\\n\\n    Notes\\n    -----\\n    This is mainly a helper function where the calling code should dispatch\\n    on bound violations. Therefore it does not check whether the pvalue is in\\n    the valid range.\\n\\n    Precision for the pvalues is around 2 to 3 decimals. This approximation is\\n    also used by other statistical packages (e.g. R:fBasics) but might not be\\n    the most precise available.\\n\\n    References\\n    ----------\\n    DallalWilkinson1986\\n    '\n    if n > 100:\n        d_max *= (n / 100.0) ** 0.49\n        n = 100\n    pval = np.exp(-7.01256 * d_max ** 2 * (n + 2.78019) + 2.99587 * d_max * np.sqrt(n + 2.78019) - 0.122119 + 0.974598 / np.sqrt(n) + 1.67997 / n)\n    return pval",
            "def pval_lf(d_max, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Approximate pvalues for Lilliefors test\\n\\n    This is only valid for pvalues smaller than 0.1 which is not checked in\\n    this function.\\n\\n    Parameters\\n    ----------\\n    d_max : array_like\\n        two-sided Kolmogorov-Smirnov test statistic\\n    n : int or float\\n        sample size\\n\\n    Returns\\n    -------\\n    p-value : float or ndarray\\n        pvalue according to approximation formula of Dallal and Wilkinson.\\n\\n    Notes\\n    -----\\n    This is mainly a helper function where the calling code should dispatch\\n    on bound violations. Therefore it does not check whether the pvalue is in\\n    the valid range.\\n\\n    Precision for the pvalues is around 2 to 3 decimals. This approximation is\\n    also used by other statistical packages (e.g. R:fBasics) but might not be\\n    the most precise available.\\n\\n    References\\n    ----------\\n    DallalWilkinson1986\\n    '\n    if n > 100:\n        d_max *= (n / 100.0) ** 0.49\n        n = 100\n    pval = np.exp(-7.01256 * d_max ** 2 * (n + 2.78019) + 2.99587 * d_max * np.sqrt(n + 2.78019) - 0.122119 + 0.974598 / np.sqrt(n) + 1.67997 / n)\n    return pval",
            "def pval_lf(d_max, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Approximate pvalues for Lilliefors test\\n\\n    This is only valid for pvalues smaller than 0.1 which is not checked in\\n    this function.\\n\\n    Parameters\\n    ----------\\n    d_max : array_like\\n        two-sided Kolmogorov-Smirnov test statistic\\n    n : int or float\\n        sample size\\n\\n    Returns\\n    -------\\n    p-value : float or ndarray\\n        pvalue according to approximation formula of Dallal and Wilkinson.\\n\\n    Notes\\n    -----\\n    This is mainly a helper function where the calling code should dispatch\\n    on bound violations. Therefore it does not check whether the pvalue is in\\n    the valid range.\\n\\n    Precision for the pvalues is around 2 to 3 decimals. This approximation is\\n    also used by other statistical packages (e.g. R:fBasics) but might not be\\n    the most precise available.\\n\\n    References\\n    ----------\\n    DallalWilkinson1986\\n    '\n    if n > 100:\n        d_max *= (n / 100.0) ** 0.49\n        n = 100\n    pval = np.exp(-7.01256 * d_max ** 2 * (n + 2.78019) + 2.99587 * d_max * np.sqrt(n + 2.78019) - 0.122119 + 0.974598 / np.sqrt(n) + 1.67997 / n)\n    return pval",
            "def pval_lf(d_max, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Approximate pvalues for Lilliefors test\\n\\n    This is only valid for pvalues smaller than 0.1 which is not checked in\\n    this function.\\n\\n    Parameters\\n    ----------\\n    d_max : array_like\\n        two-sided Kolmogorov-Smirnov test statistic\\n    n : int or float\\n        sample size\\n\\n    Returns\\n    -------\\n    p-value : float or ndarray\\n        pvalue according to approximation formula of Dallal and Wilkinson.\\n\\n    Notes\\n    -----\\n    This is mainly a helper function where the calling code should dispatch\\n    on bound violations. Therefore it does not check whether the pvalue is in\\n    the valid range.\\n\\n    Precision for the pvalues is around 2 to 3 decimals. This approximation is\\n    also used by other statistical packages (e.g. R:fBasics) but might not be\\n    the most precise available.\\n\\n    References\\n    ----------\\n    DallalWilkinson1986\\n    '\n    if n > 100:\n        d_max *= (n / 100.0) ** 0.49\n        n = 100\n    pval = np.exp(-7.01256 * d_max ** 2 * (n + 2.78019) + 2.99587 * d_max * np.sqrt(n + 2.78019) - 0.122119 + 0.974598 / np.sqrt(n) + 1.67997 / n)\n    return pval"
        ]
    },
    {
        "func_name": "kstest_fit",
        "original": "def kstest_fit(x, dist='norm', pvalmethod='table'):\n    \"\"\"\n    Test assumed normal or exponential distribution using Lilliefors' test.\n\n    Lilliefors' test is a Kolmogorov-Smirnov test with estimated parameters.\n\n    Parameters\n    ----------\n    x : array_like, 1d\n        Data to test.\n    dist : {'norm', 'exp'}, optional\n        The assumed distribution.\n    pvalmethod : {'approx', 'table'}, optional\n        The method used to compute the p-value of the test statistic. In\n        general, 'table' is preferred and makes use of a very large simulation.\n        'approx' is only valid for normality. if `dist = 'exp'` `table` is\n        always used. 'approx' uses the approximation formula of Dalal and\n        Wilkinson, valid for pvalues < 0.1. If the pvalue is larger than 0.1,\n        then the result of `table` is returned.\n\n    Returns\n    -------\n    ksstat : float\n        Kolmogorov-Smirnov test statistic with estimated mean and variance.\n    pvalue : float\n        If the pvalue is lower than some threshold, e.g. 0.05, then we can\n        reject the Null hypothesis that the sample comes from a normal\n        distribution.\n\n    Notes\n    -----\n    'table' uses an improved table based on 10,000,000 simulations. The\n    critical values are approximated using\n    log(cv_alpha) = b_alpha + c[0] log(n) + c[1] log(n)**2\n    where cv_alpha is the critical value for a test with size alpha,\n    b_alpha is an alpha-specific intercept term and c[1] and c[2] are\n    coefficients that are shared all alphas.\n    Values in the table are linearly interpolated. Values outside the\n    range are be returned as bounds, 0.990 for large and 0.001 for small\n    pvalues.\n\n    For implementation details, see  lilliefors_critical_value_simulation.py in\n    the test directory.\n    \"\"\"\n    pvalmethod = string_like(pvalmethod, 'pvalmethod', options=('approx', 'table'))\n    x = np.asarray(x)\n    if x.ndim == 2 and x.shape[1] == 1:\n        x = x[:, 0]\n    elif x.ndim != 1:\n        raise ValueError('Invalid parameter `x`: must be a one-dimensional array-like or a single-column DataFrame')\n    nobs = len(x)\n    if dist == 'norm':\n        z = (x - x.mean()) / x.std(ddof=1)\n        test_d = stats.norm.cdf\n        lilliefors_table = lilliefors_table_norm\n    elif dist == 'exp':\n        z = x / x.mean()\n        test_d = stats.expon.cdf\n        lilliefors_table = lilliefors_table_expon\n        pvalmethod = 'table'\n    else:\n        raise ValueError(\"Invalid dist parameter, must be 'norm' or 'exp'\")\n    min_nobs = 4 if dist == 'norm' else 3\n    if nobs < min_nobs:\n        raise ValueError('Test for distribution {0} requires at least {1} observations'.format(dist, min_nobs))\n    d_ks = ksstat(z, test_d, alternative='two_sided')\n    if pvalmethod == 'approx':\n        pval = pval_lf(d_ks, nobs)\n        if pval > 0.1:\n            pval = lilliefors_table.prob(d_ks, nobs)\n    else:\n        pval = lilliefors_table.prob(d_ks, nobs)\n    return (d_ks, pval)",
        "mutated": [
            "def kstest_fit(x, dist='norm', pvalmethod='table'):\n    if False:\n        i = 10\n    \"\\n    Test assumed normal or exponential distribution using Lilliefors' test.\\n\\n    Lilliefors' test is a Kolmogorov-Smirnov test with estimated parameters.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        Data to test.\\n    dist : {'norm', 'exp'}, optional\\n        The assumed distribution.\\n    pvalmethod : {'approx', 'table'}, optional\\n        The method used to compute the p-value of the test statistic. In\\n        general, 'table' is preferred and makes use of a very large simulation.\\n        'approx' is only valid for normality. if `dist = 'exp'` `table` is\\n        always used. 'approx' uses the approximation formula of Dalal and\\n        Wilkinson, valid for pvalues < 0.1. If the pvalue is larger than 0.1,\\n        then the result of `table` is returned.\\n\\n    Returns\\n    -------\\n    ksstat : float\\n        Kolmogorov-Smirnov test statistic with estimated mean and variance.\\n    pvalue : float\\n        If the pvalue is lower than some threshold, e.g. 0.05, then we can\\n        reject the Null hypothesis that the sample comes from a normal\\n        distribution.\\n\\n    Notes\\n    -----\\n    'table' uses an improved table based on 10,000,000 simulations. The\\n    critical values are approximated using\\n    log(cv_alpha) = b_alpha + c[0] log(n) + c[1] log(n)**2\\n    where cv_alpha is the critical value for a test with size alpha,\\n    b_alpha is an alpha-specific intercept term and c[1] and c[2] are\\n    coefficients that are shared all alphas.\\n    Values in the table are linearly interpolated. Values outside the\\n    range are be returned as bounds, 0.990 for large and 0.001 for small\\n    pvalues.\\n\\n    For implementation details, see  lilliefors_critical_value_simulation.py in\\n    the test directory.\\n    \"\n    pvalmethod = string_like(pvalmethod, 'pvalmethod', options=('approx', 'table'))\n    x = np.asarray(x)\n    if x.ndim == 2 and x.shape[1] == 1:\n        x = x[:, 0]\n    elif x.ndim != 1:\n        raise ValueError('Invalid parameter `x`: must be a one-dimensional array-like or a single-column DataFrame')\n    nobs = len(x)\n    if dist == 'norm':\n        z = (x - x.mean()) / x.std(ddof=1)\n        test_d = stats.norm.cdf\n        lilliefors_table = lilliefors_table_norm\n    elif dist == 'exp':\n        z = x / x.mean()\n        test_d = stats.expon.cdf\n        lilliefors_table = lilliefors_table_expon\n        pvalmethod = 'table'\n    else:\n        raise ValueError(\"Invalid dist parameter, must be 'norm' or 'exp'\")\n    min_nobs = 4 if dist == 'norm' else 3\n    if nobs < min_nobs:\n        raise ValueError('Test for distribution {0} requires at least {1} observations'.format(dist, min_nobs))\n    d_ks = ksstat(z, test_d, alternative='two_sided')\n    if pvalmethod == 'approx':\n        pval = pval_lf(d_ks, nobs)\n        if pval > 0.1:\n            pval = lilliefors_table.prob(d_ks, nobs)\n    else:\n        pval = lilliefors_table.prob(d_ks, nobs)\n    return (d_ks, pval)",
            "def kstest_fit(x, dist='norm', pvalmethod='table'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Test assumed normal or exponential distribution using Lilliefors' test.\\n\\n    Lilliefors' test is a Kolmogorov-Smirnov test with estimated parameters.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        Data to test.\\n    dist : {'norm', 'exp'}, optional\\n        The assumed distribution.\\n    pvalmethod : {'approx', 'table'}, optional\\n        The method used to compute the p-value of the test statistic. In\\n        general, 'table' is preferred and makes use of a very large simulation.\\n        'approx' is only valid for normality. if `dist = 'exp'` `table` is\\n        always used. 'approx' uses the approximation formula of Dalal and\\n        Wilkinson, valid for pvalues < 0.1. If the pvalue is larger than 0.1,\\n        then the result of `table` is returned.\\n\\n    Returns\\n    -------\\n    ksstat : float\\n        Kolmogorov-Smirnov test statistic with estimated mean and variance.\\n    pvalue : float\\n        If the pvalue is lower than some threshold, e.g. 0.05, then we can\\n        reject the Null hypothesis that the sample comes from a normal\\n        distribution.\\n\\n    Notes\\n    -----\\n    'table' uses an improved table based on 10,000,000 simulations. The\\n    critical values are approximated using\\n    log(cv_alpha) = b_alpha + c[0] log(n) + c[1] log(n)**2\\n    where cv_alpha is the critical value for a test with size alpha,\\n    b_alpha is an alpha-specific intercept term and c[1] and c[2] are\\n    coefficients that are shared all alphas.\\n    Values in the table are linearly interpolated. Values outside the\\n    range are be returned as bounds, 0.990 for large and 0.001 for small\\n    pvalues.\\n\\n    For implementation details, see  lilliefors_critical_value_simulation.py in\\n    the test directory.\\n    \"\n    pvalmethod = string_like(pvalmethod, 'pvalmethod', options=('approx', 'table'))\n    x = np.asarray(x)\n    if x.ndim == 2 and x.shape[1] == 1:\n        x = x[:, 0]\n    elif x.ndim != 1:\n        raise ValueError('Invalid parameter `x`: must be a one-dimensional array-like or a single-column DataFrame')\n    nobs = len(x)\n    if dist == 'norm':\n        z = (x - x.mean()) / x.std(ddof=1)\n        test_d = stats.norm.cdf\n        lilliefors_table = lilliefors_table_norm\n    elif dist == 'exp':\n        z = x / x.mean()\n        test_d = stats.expon.cdf\n        lilliefors_table = lilliefors_table_expon\n        pvalmethod = 'table'\n    else:\n        raise ValueError(\"Invalid dist parameter, must be 'norm' or 'exp'\")\n    min_nobs = 4 if dist == 'norm' else 3\n    if nobs < min_nobs:\n        raise ValueError('Test for distribution {0} requires at least {1} observations'.format(dist, min_nobs))\n    d_ks = ksstat(z, test_d, alternative='two_sided')\n    if pvalmethod == 'approx':\n        pval = pval_lf(d_ks, nobs)\n        if pval > 0.1:\n            pval = lilliefors_table.prob(d_ks, nobs)\n    else:\n        pval = lilliefors_table.prob(d_ks, nobs)\n    return (d_ks, pval)",
            "def kstest_fit(x, dist='norm', pvalmethod='table'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Test assumed normal or exponential distribution using Lilliefors' test.\\n\\n    Lilliefors' test is a Kolmogorov-Smirnov test with estimated parameters.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        Data to test.\\n    dist : {'norm', 'exp'}, optional\\n        The assumed distribution.\\n    pvalmethod : {'approx', 'table'}, optional\\n        The method used to compute the p-value of the test statistic. In\\n        general, 'table' is preferred and makes use of a very large simulation.\\n        'approx' is only valid for normality. if `dist = 'exp'` `table` is\\n        always used. 'approx' uses the approximation formula of Dalal and\\n        Wilkinson, valid for pvalues < 0.1. If the pvalue is larger than 0.1,\\n        then the result of `table` is returned.\\n\\n    Returns\\n    -------\\n    ksstat : float\\n        Kolmogorov-Smirnov test statistic with estimated mean and variance.\\n    pvalue : float\\n        If the pvalue is lower than some threshold, e.g. 0.05, then we can\\n        reject the Null hypothesis that the sample comes from a normal\\n        distribution.\\n\\n    Notes\\n    -----\\n    'table' uses an improved table based on 10,000,000 simulations. The\\n    critical values are approximated using\\n    log(cv_alpha) = b_alpha + c[0] log(n) + c[1] log(n)**2\\n    where cv_alpha is the critical value for a test with size alpha,\\n    b_alpha is an alpha-specific intercept term and c[1] and c[2] are\\n    coefficients that are shared all alphas.\\n    Values in the table are linearly interpolated. Values outside the\\n    range are be returned as bounds, 0.990 for large and 0.001 for small\\n    pvalues.\\n\\n    For implementation details, see  lilliefors_critical_value_simulation.py in\\n    the test directory.\\n    \"\n    pvalmethod = string_like(pvalmethod, 'pvalmethod', options=('approx', 'table'))\n    x = np.asarray(x)\n    if x.ndim == 2 and x.shape[1] == 1:\n        x = x[:, 0]\n    elif x.ndim != 1:\n        raise ValueError('Invalid parameter `x`: must be a one-dimensional array-like or a single-column DataFrame')\n    nobs = len(x)\n    if dist == 'norm':\n        z = (x - x.mean()) / x.std(ddof=1)\n        test_d = stats.norm.cdf\n        lilliefors_table = lilliefors_table_norm\n    elif dist == 'exp':\n        z = x / x.mean()\n        test_d = stats.expon.cdf\n        lilliefors_table = lilliefors_table_expon\n        pvalmethod = 'table'\n    else:\n        raise ValueError(\"Invalid dist parameter, must be 'norm' or 'exp'\")\n    min_nobs = 4 if dist == 'norm' else 3\n    if nobs < min_nobs:\n        raise ValueError('Test for distribution {0} requires at least {1} observations'.format(dist, min_nobs))\n    d_ks = ksstat(z, test_d, alternative='two_sided')\n    if pvalmethod == 'approx':\n        pval = pval_lf(d_ks, nobs)\n        if pval > 0.1:\n            pval = lilliefors_table.prob(d_ks, nobs)\n    else:\n        pval = lilliefors_table.prob(d_ks, nobs)\n    return (d_ks, pval)",
            "def kstest_fit(x, dist='norm', pvalmethod='table'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Test assumed normal or exponential distribution using Lilliefors' test.\\n\\n    Lilliefors' test is a Kolmogorov-Smirnov test with estimated parameters.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        Data to test.\\n    dist : {'norm', 'exp'}, optional\\n        The assumed distribution.\\n    pvalmethod : {'approx', 'table'}, optional\\n        The method used to compute the p-value of the test statistic. In\\n        general, 'table' is preferred and makes use of a very large simulation.\\n        'approx' is only valid for normality. if `dist = 'exp'` `table` is\\n        always used. 'approx' uses the approximation formula of Dalal and\\n        Wilkinson, valid for pvalues < 0.1. If the pvalue is larger than 0.1,\\n        then the result of `table` is returned.\\n\\n    Returns\\n    -------\\n    ksstat : float\\n        Kolmogorov-Smirnov test statistic with estimated mean and variance.\\n    pvalue : float\\n        If the pvalue is lower than some threshold, e.g. 0.05, then we can\\n        reject the Null hypothesis that the sample comes from a normal\\n        distribution.\\n\\n    Notes\\n    -----\\n    'table' uses an improved table based on 10,000,000 simulations. The\\n    critical values are approximated using\\n    log(cv_alpha) = b_alpha + c[0] log(n) + c[1] log(n)**2\\n    where cv_alpha is the critical value for a test with size alpha,\\n    b_alpha is an alpha-specific intercept term and c[1] and c[2] are\\n    coefficients that are shared all alphas.\\n    Values in the table are linearly interpolated. Values outside the\\n    range are be returned as bounds, 0.990 for large and 0.001 for small\\n    pvalues.\\n\\n    For implementation details, see  lilliefors_critical_value_simulation.py in\\n    the test directory.\\n    \"\n    pvalmethod = string_like(pvalmethod, 'pvalmethod', options=('approx', 'table'))\n    x = np.asarray(x)\n    if x.ndim == 2 and x.shape[1] == 1:\n        x = x[:, 0]\n    elif x.ndim != 1:\n        raise ValueError('Invalid parameter `x`: must be a one-dimensional array-like or a single-column DataFrame')\n    nobs = len(x)\n    if dist == 'norm':\n        z = (x - x.mean()) / x.std(ddof=1)\n        test_d = stats.norm.cdf\n        lilliefors_table = lilliefors_table_norm\n    elif dist == 'exp':\n        z = x / x.mean()\n        test_d = stats.expon.cdf\n        lilliefors_table = lilliefors_table_expon\n        pvalmethod = 'table'\n    else:\n        raise ValueError(\"Invalid dist parameter, must be 'norm' or 'exp'\")\n    min_nobs = 4 if dist == 'norm' else 3\n    if nobs < min_nobs:\n        raise ValueError('Test for distribution {0} requires at least {1} observations'.format(dist, min_nobs))\n    d_ks = ksstat(z, test_d, alternative='two_sided')\n    if pvalmethod == 'approx':\n        pval = pval_lf(d_ks, nobs)\n        if pval > 0.1:\n            pval = lilliefors_table.prob(d_ks, nobs)\n    else:\n        pval = lilliefors_table.prob(d_ks, nobs)\n    return (d_ks, pval)",
            "def kstest_fit(x, dist='norm', pvalmethod='table'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Test assumed normal or exponential distribution using Lilliefors' test.\\n\\n    Lilliefors' test is a Kolmogorov-Smirnov test with estimated parameters.\\n\\n    Parameters\\n    ----------\\n    x : array_like, 1d\\n        Data to test.\\n    dist : {'norm', 'exp'}, optional\\n        The assumed distribution.\\n    pvalmethod : {'approx', 'table'}, optional\\n        The method used to compute the p-value of the test statistic. In\\n        general, 'table' is preferred and makes use of a very large simulation.\\n        'approx' is only valid for normality. if `dist = 'exp'` `table` is\\n        always used. 'approx' uses the approximation formula of Dalal and\\n        Wilkinson, valid for pvalues < 0.1. If the pvalue is larger than 0.1,\\n        then the result of `table` is returned.\\n\\n    Returns\\n    -------\\n    ksstat : float\\n        Kolmogorov-Smirnov test statistic with estimated mean and variance.\\n    pvalue : float\\n        If the pvalue is lower than some threshold, e.g. 0.05, then we can\\n        reject the Null hypothesis that the sample comes from a normal\\n        distribution.\\n\\n    Notes\\n    -----\\n    'table' uses an improved table based on 10,000,000 simulations. The\\n    critical values are approximated using\\n    log(cv_alpha) = b_alpha + c[0] log(n) + c[1] log(n)**2\\n    where cv_alpha is the critical value for a test with size alpha,\\n    b_alpha is an alpha-specific intercept term and c[1] and c[2] are\\n    coefficients that are shared all alphas.\\n    Values in the table are linearly interpolated. Values outside the\\n    range are be returned as bounds, 0.990 for large and 0.001 for small\\n    pvalues.\\n\\n    For implementation details, see  lilliefors_critical_value_simulation.py in\\n    the test directory.\\n    \"\n    pvalmethod = string_like(pvalmethod, 'pvalmethod', options=('approx', 'table'))\n    x = np.asarray(x)\n    if x.ndim == 2 and x.shape[1] == 1:\n        x = x[:, 0]\n    elif x.ndim != 1:\n        raise ValueError('Invalid parameter `x`: must be a one-dimensional array-like or a single-column DataFrame')\n    nobs = len(x)\n    if dist == 'norm':\n        z = (x - x.mean()) / x.std(ddof=1)\n        test_d = stats.norm.cdf\n        lilliefors_table = lilliefors_table_norm\n    elif dist == 'exp':\n        z = x / x.mean()\n        test_d = stats.expon.cdf\n        lilliefors_table = lilliefors_table_expon\n        pvalmethod = 'table'\n    else:\n        raise ValueError(\"Invalid dist parameter, must be 'norm' or 'exp'\")\n    min_nobs = 4 if dist == 'norm' else 3\n    if nobs < min_nobs:\n        raise ValueError('Test for distribution {0} requires at least {1} observations'.format(dist, min_nobs))\n    d_ks = ksstat(z, test_d, alternative='two_sided')\n    if pvalmethod == 'approx':\n        pval = pval_lf(d_ks, nobs)\n        if pval > 0.1:\n            pval = lilliefors_table.prob(d_ks, nobs)\n    else:\n        pval = lilliefors_table.prob(d_ks, nobs)\n    return (d_ks, pval)"
        ]
    }
]