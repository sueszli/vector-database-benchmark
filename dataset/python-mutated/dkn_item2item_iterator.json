[
    {
        "func_name": "__init__",
        "original": "def __init__(self, hparams, graph):\n    \"\"\"This new iterator is for DKN's item-to-item recommendations version.\n        The tutorial can be found `on this notebook <https://github.com/microsoft/recommenders/blob/main/examples/07_tutorials/KDD2020-tutorial/step4_run_dkn_item2item.ipynb>`_.\n\n        Compared with user-to-item recommendations, we don't need the user behavior module.\n        So the placeholder can be simplified from the original DKNTextIterator.\n\n        Args:\n            hparams (object): Global hyper-parameters.\n            graph (object): The running graph.\n        \"\"\"\n    self.hparams = hparams\n    self.graph = graph\n    self.neg_num = hparams.neg_num\n    self.batch_size = hparams.batch_size * (self.neg_num + 2)\n    self.doc_size = hparams.doc_size\n    with self.graph.as_default():\n        self.candidate_news_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_index')\n        self.candidate_news_entity_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_entity_index')\n    self._loading_nessary_files()",
        "mutated": [
            "def __init__(self, hparams, graph):\n    if False:\n        i = 10\n    \"This new iterator is for DKN's item-to-item recommendations version.\\n        The tutorial can be found `on this notebook <https://github.com/microsoft/recommenders/blob/main/examples/07_tutorials/KDD2020-tutorial/step4_run_dkn_item2item.ipynb>`_.\\n\\n        Compared with user-to-item recommendations, we don't need the user behavior module.\\n        So the placeholder can be simplified from the original DKNTextIterator.\\n\\n        Args:\\n            hparams (object): Global hyper-parameters.\\n            graph (object): The running graph.\\n        \"\n    self.hparams = hparams\n    self.graph = graph\n    self.neg_num = hparams.neg_num\n    self.batch_size = hparams.batch_size * (self.neg_num + 2)\n    self.doc_size = hparams.doc_size\n    with self.graph.as_default():\n        self.candidate_news_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_index')\n        self.candidate_news_entity_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_entity_index')\n    self._loading_nessary_files()",
            "def __init__(self, hparams, graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"This new iterator is for DKN's item-to-item recommendations version.\\n        The tutorial can be found `on this notebook <https://github.com/microsoft/recommenders/blob/main/examples/07_tutorials/KDD2020-tutorial/step4_run_dkn_item2item.ipynb>`_.\\n\\n        Compared with user-to-item recommendations, we don't need the user behavior module.\\n        So the placeholder can be simplified from the original DKNTextIterator.\\n\\n        Args:\\n            hparams (object): Global hyper-parameters.\\n            graph (object): The running graph.\\n        \"\n    self.hparams = hparams\n    self.graph = graph\n    self.neg_num = hparams.neg_num\n    self.batch_size = hparams.batch_size * (self.neg_num + 2)\n    self.doc_size = hparams.doc_size\n    with self.graph.as_default():\n        self.candidate_news_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_index')\n        self.candidate_news_entity_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_entity_index')\n    self._loading_nessary_files()",
            "def __init__(self, hparams, graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"This new iterator is for DKN's item-to-item recommendations version.\\n        The tutorial can be found `on this notebook <https://github.com/microsoft/recommenders/blob/main/examples/07_tutorials/KDD2020-tutorial/step4_run_dkn_item2item.ipynb>`_.\\n\\n        Compared with user-to-item recommendations, we don't need the user behavior module.\\n        So the placeholder can be simplified from the original DKNTextIterator.\\n\\n        Args:\\n            hparams (object): Global hyper-parameters.\\n            graph (object): The running graph.\\n        \"\n    self.hparams = hparams\n    self.graph = graph\n    self.neg_num = hparams.neg_num\n    self.batch_size = hparams.batch_size * (self.neg_num + 2)\n    self.doc_size = hparams.doc_size\n    with self.graph.as_default():\n        self.candidate_news_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_index')\n        self.candidate_news_entity_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_entity_index')\n    self._loading_nessary_files()",
            "def __init__(self, hparams, graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"This new iterator is for DKN's item-to-item recommendations version.\\n        The tutorial can be found `on this notebook <https://github.com/microsoft/recommenders/blob/main/examples/07_tutorials/KDD2020-tutorial/step4_run_dkn_item2item.ipynb>`_.\\n\\n        Compared with user-to-item recommendations, we don't need the user behavior module.\\n        So the placeholder can be simplified from the original DKNTextIterator.\\n\\n        Args:\\n            hparams (object): Global hyper-parameters.\\n            graph (object): The running graph.\\n        \"\n    self.hparams = hparams\n    self.graph = graph\n    self.neg_num = hparams.neg_num\n    self.batch_size = hparams.batch_size * (self.neg_num + 2)\n    self.doc_size = hparams.doc_size\n    with self.graph.as_default():\n        self.candidate_news_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_index')\n        self.candidate_news_entity_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_entity_index')\n    self._loading_nessary_files()",
            "def __init__(self, hparams, graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"This new iterator is for DKN's item-to-item recommendations version.\\n        The tutorial can be found `on this notebook <https://github.com/microsoft/recommenders/blob/main/examples/07_tutorials/KDD2020-tutorial/step4_run_dkn_item2item.ipynb>`_.\\n\\n        Compared with user-to-item recommendations, we don't need the user behavior module.\\n        So the placeholder can be simplified from the original DKNTextIterator.\\n\\n        Args:\\n            hparams (object): Global hyper-parameters.\\n            graph (object): The running graph.\\n        \"\n    self.hparams = hparams\n    self.graph = graph\n    self.neg_num = hparams.neg_num\n    self.batch_size = hparams.batch_size * (self.neg_num + 2)\n    self.doc_size = hparams.doc_size\n    with self.graph.as_default():\n        self.candidate_news_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_index')\n        self.candidate_news_entity_index_batch = tf.compat.v1.placeholder(tf.int64, [self.batch_size, self.doc_size], name='candidate_news_entity_index')\n    self._loading_nessary_files()"
        ]
    },
    {
        "func_name": "_loading_nessary_files",
        "original": "def _loading_nessary_files(self):\n    \"\"\"Only one feature file is needed:  `news_feature_file`.\n        This function loads the news article's features into two dictionaries: `self.news_word_index` and `self.news_entity_index`.\n        \"\"\"\n    hparams = self.hparams\n    self.news_word_index = {}\n    self.news_entity_index = {}\n    with open(hparams.news_feature_file, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            (newsid, word_index, entity_index) = line.strip().split(' ')\n            self.news_word_index[newsid] = [int(item) for item in word_index.split(',')]\n            self.news_entity_index[newsid] = [int(item) for item in entity_index.split(',')]",
        "mutated": [
            "def _loading_nessary_files(self):\n    if False:\n        i = 10\n    \"Only one feature file is needed:  `news_feature_file`.\\n        This function loads the news article's features into two dictionaries: `self.news_word_index` and `self.news_entity_index`.\\n        \"\n    hparams = self.hparams\n    self.news_word_index = {}\n    self.news_entity_index = {}\n    with open(hparams.news_feature_file, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            (newsid, word_index, entity_index) = line.strip().split(' ')\n            self.news_word_index[newsid] = [int(item) for item in word_index.split(',')]\n            self.news_entity_index[newsid] = [int(item) for item in entity_index.split(',')]",
            "def _loading_nessary_files(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Only one feature file is needed:  `news_feature_file`.\\n        This function loads the news article's features into two dictionaries: `self.news_word_index` and `self.news_entity_index`.\\n        \"\n    hparams = self.hparams\n    self.news_word_index = {}\n    self.news_entity_index = {}\n    with open(hparams.news_feature_file, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            (newsid, word_index, entity_index) = line.strip().split(' ')\n            self.news_word_index[newsid] = [int(item) for item in word_index.split(',')]\n            self.news_entity_index[newsid] = [int(item) for item in entity_index.split(',')]",
            "def _loading_nessary_files(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Only one feature file is needed:  `news_feature_file`.\\n        This function loads the news article's features into two dictionaries: `self.news_word_index` and `self.news_entity_index`.\\n        \"\n    hparams = self.hparams\n    self.news_word_index = {}\n    self.news_entity_index = {}\n    with open(hparams.news_feature_file, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            (newsid, word_index, entity_index) = line.strip().split(' ')\n            self.news_word_index[newsid] = [int(item) for item in word_index.split(',')]\n            self.news_entity_index[newsid] = [int(item) for item in entity_index.split(',')]",
            "def _loading_nessary_files(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Only one feature file is needed:  `news_feature_file`.\\n        This function loads the news article's features into two dictionaries: `self.news_word_index` and `self.news_entity_index`.\\n        \"\n    hparams = self.hparams\n    self.news_word_index = {}\n    self.news_entity_index = {}\n    with open(hparams.news_feature_file, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            (newsid, word_index, entity_index) = line.strip().split(' ')\n            self.news_word_index[newsid] = [int(item) for item in word_index.split(',')]\n            self.news_entity_index[newsid] = [int(item) for item in entity_index.split(',')]",
            "def _loading_nessary_files(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Only one feature file is needed:  `news_feature_file`.\\n        This function loads the news article's features into two dictionaries: `self.news_word_index` and `self.news_entity_index`.\\n        \"\n    hparams = self.hparams\n    self.news_word_index = {}\n    self.news_entity_index = {}\n    with open(hparams.news_feature_file, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            (newsid, word_index, entity_index) = line.strip().split(' ')\n            self.news_word_index[newsid] = [int(item) for item in word_index.split(',')]\n            self.news_entity_index[newsid] = [int(item) for item in entity_index.split(',')]"
        ]
    },
    {
        "func_name": "load_data_from_file",
        "original": "def load_data_from_file(self, infile):\n    \"\"\"This function will return a mini-batch of data with features,\n        by looking up `news_word_index` dictionary and `news_entity_index` dictionary according to the news article's ID.\n\n        Args:\n            infile (str): File path. Each line of `infile` is a news article's ID.\n\n        Yields:\n            dict, list, int:\n            - A dictionary that maps graph elements to numpy arrays.\n            - A list with news article's ID.\n            - Size of the data in a batch.\n        \"\"\"\n    newsid_list = []\n    candidate_news_index_batch = []\n    candidate_news_entity_index_batch = []\n    cnt = 0\n    with open(infile, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            newsid = line.strip()\n            (word_index, entity_index) = (self.news_word_index[newsid], self.news_entity_index[newsid])\n            newsid_list.append(newsid)\n            candidate_news_index_batch.append(word_index)\n            candidate_news_entity_index_batch.append(entity_index)\n            cnt += 1\n            if cnt >= self.batch_size:\n                res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n                data_size = self.batch_size\n                yield (self.gen_infer_feed_dict(res), newsid_list, data_size)\n                candidate_news_index_batch = []\n                candidate_news_entity_index_batch = []\n                newsid_list = []\n                cnt = 0\n        if cnt > 0:\n            data_size = cnt\n            while cnt < self.batch_size:\n                candidate_news_index_batch.append(candidate_news_index_batch[cnt % data_size])\n                candidate_news_entity_index_batch.append(candidate_news_entity_index_batch[cnt % data_size])\n                cnt += 1\n            res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n            yield (self.gen_infer_feed_dict(res), newsid_list, data_size)",
        "mutated": [
            "def load_data_from_file(self, infile):\n    if False:\n        i = 10\n    \"This function will return a mini-batch of data with features,\\n        by looking up `news_word_index` dictionary and `news_entity_index` dictionary according to the news article's ID.\\n\\n        Args:\\n            infile (str): File path. Each line of `infile` is a news article's ID.\\n\\n        Yields:\\n            dict, list, int:\\n            - A dictionary that maps graph elements to numpy arrays.\\n            - A list with news article's ID.\\n            - Size of the data in a batch.\\n        \"\n    newsid_list = []\n    candidate_news_index_batch = []\n    candidate_news_entity_index_batch = []\n    cnt = 0\n    with open(infile, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            newsid = line.strip()\n            (word_index, entity_index) = (self.news_word_index[newsid], self.news_entity_index[newsid])\n            newsid_list.append(newsid)\n            candidate_news_index_batch.append(word_index)\n            candidate_news_entity_index_batch.append(entity_index)\n            cnt += 1\n            if cnt >= self.batch_size:\n                res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n                data_size = self.batch_size\n                yield (self.gen_infer_feed_dict(res), newsid_list, data_size)\n                candidate_news_index_batch = []\n                candidate_news_entity_index_batch = []\n                newsid_list = []\n                cnt = 0\n        if cnt > 0:\n            data_size = cnt\n            while cnt < self.batch_size:\n                candidate_news_index_batch.append(candidate_news_index_batch[cnt % data_size])\n                candidate_news_entity_index_batch.append(candidate_news_entity_index_batch[cnt % data_size])\n                cnt += 1\n            res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n            yield (self.gen_infer_feed_dict(res), newsid_list, data_size)",
            "def load_data_from_file(self, infile):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"This function will return a mini-batch of data with features,\\n        by looking up `news_word_index` dictionary and `news_entity_index` dictionary according to the news article's ID.\\n\\n        Args:\\n            infile (str): File path. Each line of `infile` is a news article's ID.\\n\\n        Yields:\\n            dict, list, int:\\n            - A dictionary that maps graph elements to numpy arrays.\\n            - A list with news article's ID.\\n            - Size of the data in a batch.\\n        \"\n    newsid_list = []\n    candidate_news_index_batch = []\n    candidate_news_entity_index_batch = []\n    cnt = 0\n    with open(infile, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            newsid = line.strip()\n            (word_index, entity_index) = (self.news_word_index[newsid], self.news_entity_index[newsid])\n            newsid_list.append(newsid)\n            candidate_news_index_batch.append(word_index)\n            candidate_news_entity_index_batch.append(entity_index)\n            cnt += 1\n            if cnt >= self.batch_size:\n                res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n                data_size = self.batch_size\n                yield (self.gen_infer_feed_dict(res), newsid_list, data_size)\n                candidate_news_index_batch = []\n                candidate_news_entity_index_batch = []\n                newsid_list = []\n                cnt = 0\n        if cnt > 0:\n            data_size = cnt\n            while cnt < self.batch_size:\n                candidate_news_index_batch.append(candidate_news_index_batch[cnt % data_size])\n                candidate_news_entity_index_batch.append(candidate_news_entity_index_batch[cnt % data_size])\n                cnt += 1\n            res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n            yield (self.gen_infer_feed_dict(res), newsid_list, data_size)",
            "def load_data_from_file(self, infile):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"This function will return a mini-batch of data with features,\\n        by looking up `news_word_index` dictionary and `news_entity_index` dictionary according to the news article's ID.\\n\\n        Args:\\n            infile (str): File path. Each line of `infile` is a news article's ID.\\n\\n        Yields:\\n            dict, list, int:\\n            - A dictionary that maps graph elements to numpy arrays.\\n            - A list with news article's ID.\\n            - Size of the data in a batch.\\n        \"\n    newsid_list = []\n    candidate_news_index_batch = []\n    candidate_news_entity_index_batch = []\n    cnt = 0\n    with open(infile, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            newsid = line.strip()\n            (word_index, entity_index) = (self.news_word_index[newsid], self.news_entity_index[newsid])\n            newsid_list.append(newsid)\n            candidate_news_index_batch.append(word_index)\n            candidate_news_entity_index_batch.append(entity_index)\n            cnt += 1\n            if cnt >= self.batch_size:\n                res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n                data_size = self.batch_size\n                yield (self.gen_infer_feed_dict(res), newsid_list, data_size)\n                candidate_news_index_batch = []\n                candidate_news_entity_index_batch = []\n                newsid_list = []\n                cnt = 0\n        if cnt > 0:\n            data_size = cnt\n            while cnt < self.batch_size:\n                candidate_news_index_batch.append(candidate_news_index_batch[cnt % data_size])\n                candidate_news_entity_index_batch.append(candidate_news_entity_index_batch[cnt % data_size])\n                cnt += 1\n            res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n            yield (self.gen_infer_feed_dict(res), newsid_list, data_size)",
            "def load_data_from_file(self, infile):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"This function will return a mini-batch of data with features,\\n        by looking up `news_word_index` dictionary and `news_entity_index` dictionary according to the news article's ID.\\n\\n        Args:\\n            infile (str): File path. Each line of `infile` is a news article's ID.\\n\\n        Yields:\\n            dict, list, int:\\n            - A dictionary that maps graph elements to numpy arrays.\\n            - A list with news article's ID.\\n            - Size of the data in a batch.\\n        \"\n    newsid_list = []\n    candidate_news_index_batch = []\n    candidate_news_entity_index_batch = []\n    cnt = 0\n    with open(infile, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            newsid = line.strip()\n            (word_index, entity_index) = (self.news_word_index[newsid], self.news_entity_index[newsid])\n            newsid_list.append(newsid)\n            candidate_news_index_batch.append(word_index)\n            candidate_news_entity_index_batch.append(entity_index)\n            cnt += 1\n            if cnt >= self.batch_size:\n                res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n                data_size = self.batch_size\n                yield (self.gen_infer_feed_dict(res), newsid_list, data_size)\n                candidate_news_index_batch = []\n                candidate_news_entity_index_batch = []\n                newsid_list = []\n                cnt = 0\n        if cnt > 0:\n            data_size = cnt\n            while cnt < self.batch_size:\n                candidate_news_index_batch.append(candidate_news_index_batch[cnt % data_size])\n                candidate_news_entity_index_batch.append(candidate_news_entity_index_batch[cnt % data_size])\n                cnt += 1\n            res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n            yield (self.gen_infer_feed_dict(res), newsid_list, data_size)",
            "def load_data_from_file(self, infile):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"This function will return a mini-batch of data with features,\\n        by looking up `news_word_index` dictionary and `news_entity_index` dictionary according to the news article's ID.\\n\\n        Args:\\n            infile (str): File path. Each line of `infile` is a news article's ID.\\n\\n        Yields:\\n            dict, list, int:\\n            - A dictionary that maps graph elements to numpy arrays.\\n            - A list with news article's ID.\\n            - Size of the data in a batch.\\n        \"\n    newsid_list = []\n    candidate_news_index_batch = []\n    candidate_news_entity_index_batch = []\n    cnt = 0\n    with open(infile, 'r') as rd:\n        while True:\n            line = rd.readline()\n            if not line:\n                break\n            newsid = line.strip()\n            (word_index, entity_index) = (self.news_word_index[newsid], self.news_entity_index[newsid])\n            newsid_list.append(newsid)\n            candidate_news_index_batch.append(word_index)\n            candidate_news_entity_index_batch.append(entity_index)\n            cnt += 1\n            if cnt >= self.batch_size:\n                res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n                data_size = self.batch_size\n                yield (self.gen_infer_feed_dict(res), newsid_list, data_size)\n                candidate_news_index_batch = []\n                candidate_news_entity_index_batch = []\n                newsid_list = []\n                cnt = 0\n        if cnt > 0:\n            data_size = cnt\n            while cnt < self.batch_size:\n                candidate_news_index_batch.append(candidate_news_index_batch[cnt % data_size])\n                candidate_news_entity_index_batch.append(candidate_news_entity_index_batch[cnt % data_size])\n                cnt += 1\n            res = self._convert_infer_data(candidate_news_index_batch, candidate_news_entity_index_batch)\n            yield (self.gen_infer_feed_dict(res), newsid_list, data_size)"
        ]
    }
]