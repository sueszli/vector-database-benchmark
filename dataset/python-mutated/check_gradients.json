[
    {
        "func_name": "compute_jacobian_finite_differences",
        "original": "def compute_jacobian_finite_differences(x0, fn, epsilon):\n    \"\"\"Computes the Jacobian using finite differences\n\n    x0:      The positions at which to compute J.\n\n    fn:      A function of the form fn(x) which returns a single numpy array.\n\n    epsilon: A scalar or an array that can be broadcasted to the same\n             shape as x0.\n    \"\"\"\n    dtype = x0.dtype\n    y0 = fn(x0)\n    h = np.zeros_like(x0)\n    J = np.zeros((x0.size, y0.size), dtype=dtype)\n    epsilon_arr = np.broadcast_to(epsilon, x0.shape)\n    for i in range(x0.size):\n        eps = epsilon_arr.flat[i]\n        h.flat[i] = eps\n        J[i, :] = ((fn(x0 + h) - y0) / eps).flat\n        h.flat[i] = 0\n    return J",
        "mutated": [
            "def compute_jacobian_finite_differences(x0, fn, epsilon):\n    if False:\n        i = 10\n    'Computes the Jacobian using finite differences\\n\\n    x0:      The positions at which to compute J.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0.\\n    '\n    dtype = x0.dtype\n    y0 = fn(x0)\n    h = np.zeros_like(x0)\n    J = np.zeros((x0.size, y0.size), dtype=dtype)\n    epsilon_arr = np.broadcast_to(epsilon, x0.shape)\n    for i in range(x0.size):\n        eps = epsilon_arr.flat[i]\n        h.flat[i] = eps\n        J[i, :] = ((fn(x0 + h) - y0) / eps).flat\n        h.flat[i] = 0\n    return J",
            "def compute_jacobian_finite_differences(x0, fn, epsilon):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes the Jacobian using finite differences\\n\\n    x0:      The positions at which to compute J.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0.\\n    '\n    dtype = x0.dtype\n    y0 = fn(x0)\n    h = np.zeros_like(x0)\n    J = np.zeros((x0.size, y0.size), dtype=dtype)\n    epsilon_arr = np.broadcast_to(epsilon, x0.shape)\n    for i in range(x0.size):\n        eps = epsilon_arr.flat[i]\n        h.flat[i] = eps\n        J[i, :] = ((fn(x0 + h) - y0) / eps).flat\n        h.flat[i] = 0\n    return J",
            "def compute_jacobian_finite_differences(x0, fn, epsilon):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes the Jacobian using finite differences\\n\\n    x0:      The positions at which to compute J.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0.\\n    '\n    dtype = x0.dtype\n    y0 = fn(x0)\n    h = np.zeros_like(x0)\n    J = np.zeros((x0.size, y0.size), dtype=dtype)\n    epsilon_arr = np.broadcast_to(epsilon, x0.shape)\n    for i in range(x0.size):\n        eps = epsilon_arr.flat[i]\n        h.flat[i] = eps\n        J[i, :] = ((fn(x0 + h) - y0) / eps).flat\n        h.flat[i] = 0\n    return J",
            "def compute_jacobian_finite_differences(x0, fn, epsilon):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes the Jacobian using finite differences\\n\\n    x0:      The positions at which to compute J.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0.\\n    '\n    dtype = x0.dtype\n    y0 = fn(x0)\n    h = np.zeros_like(x0)\n    J = np.zeros((x0.size, y0.size), dtype=dtype)\n    epsilon_arr = np.broadcast_to(epsilon, x0.shape)\n    for i in range(x0.size):\n        eps = epsilon_arr.flat[i]\n        h.flat[i] = eps\n        J[i, :] = ((fn(x0 + h) - y0) / eps).flat\n        h.flat[i] = 0\n    return J",
            "def compute_jacobian_finite_differences(x0, fn, epsilon):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes the Jacobian using finite differences\\n\\n    x0:      The positions at which to compute J.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0.\\n    '\n    dtype = x0.dtype\n    y0 = fn(x0)\n    h = np.zeros_like(x0)\n    J = np.zeros((x0.size, y0.size), dtype=dtype)\n    epsilon_arr = np.broadcast_to(epsilon, x0.shape)\n    for i in range(x0.size):\n        eps = epsilon_arr.flat[i]\n        h.flat[i] = eps\n        J[i, :] = ((fn(x0 + h) - y0) / eps).flat\n        h.flat[i] = 0\n    return J"
        ]
    },
    {
        "func_name": "compute_jacobian_analytical",
        "original": "def compute_jacobian_analytical(x0, y_shape, fn_grad, y_bp=None):\n    \"\"\"Computes the analytical Jacobian\n\n    x0:      The position at which to compute J.\n\n    y_shape: The shape of the backpropagated value, i.e. the shape of\n             the output of the corresponding function 'fn'.\n\n    fn_grad: The gradient of the original function with the form\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\n             value and 'x0' is the original input to 'fn'. The output of\n             the function is the gradient of x wrt to y.\n\n    y_bp:    Optional array with custom values for individually scaling\n             the gradients.\n\n    \"\"\"\n    dtype = x0.dtype\n    y_size = 1\n    for k in y_shape:\n        y_size *= k\n    J = np.zeros((x0.size, y_size), dtype=dtype)\n    y = np.zeros(y_shape, dtype=dtype)\n    y_bp_arr = np.broadcast_to(y_bp, y_shape) if not y_bp is None else np.ones(y_shape, dtype=dtype)\n    for j in range(y_size):\n        y.flat[j] = y_bp_arr.flat[j]\n        J[:, j] = fn_grad(y, x0).flat\n        y.flat[j] = 0\n    return J",
        "mutated": [
            "def compute_jacobian_analytical(x0, y_shape, fn_grad, y_bp=None):\n    if False:\n        i = 10\n    \"Computes the analytical Jacobian\\n\\n    x0:      The position at which to compute J.\\n\\n    y_shape: The shape of the backpropagated value, i.e. the shape of\\n             the output of the corresponding function 'fn'.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    y_bp:    Optional array with custom values for individually scaling\\n             the gradients.\\n\\n    \"\n    dtype = x0.dtype\n    y_size = 1\n    for k in y_shape:\n        y_size *= k\n    J = np.zeros((x0.size, y_size), dtype=dtype)\n    y = np.zeros(y_shape, dtype=dtype)\n    y_bp_arr = np.broadcast_to(y_bp, y_shape) if not y_bp is None else np.ones(y_shape, dtype=dtype)\n    for j in range(y_size):\n        y.flat[j] = y_bp_arr.flat[j]\n        J[:, j] = fn_grad(y, x0).flat\n        y.flat[j] = 0\n    return J",
            "def compute_jacobian_analytical(x0, y_shape, fn_grad, y_bp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes the analytical Jacobian\\n\\n    x0:      The position at which to compute J.\\n\\n    y_shape: The shape of the backpropagated value, i.e. the shape of\\n             the output of the corresponding function 'fn'.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    y_bp:    Optional array with custom values for individually scaling\\n             the gradients.\\n\\n    \"\n    dtype = x0.dtype\n    y_size = 1\n    for k in y_shape:\n        y_size *= k\n    J = np.zeros((x0.size, y_size), dtype=dtype)\n    y = np.zeros(y_shape, dtype=dtype)\n    y_bp_arr = np.broadcast_to(y_bp, y_shape) if not y_bp is None else np.ones(y_shape, dtype=dtype)\n    for j in range(y_size):\n        y.flat[j] = y_bp_arr.flat[j]\n        J[:, j] = fn_grad(y, x0).flat\n        y.flat[j] = 0\n    return J",
            "def compute_jacobian_analytical(x0, y_shape, fn_grad, y_bp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes the analytical Jacobian\\n\\n    x0:      The position at which to compute J.\\n\\n    y_shape: The shape of the backpropagated value, i.e. the shape of\\n             the output of the corresponding function 'fn'.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    y_bp:    Optional array with custom values for individually scaling\\n             the gradients.\\n\\n    \"\n    dtype = x0.dtype\n    y_size = 1\n    for k in y_shape:\n        y_size *= k\n    J = np.zeros((x0.size, y_size), dtype=dtype)\n    y = np.zeros(y_shape, dtype=dtype)\n    y_bp_arr = np.broadcast_to(y_bp, y_shape) if not y_bp is None else np.ones(y_shape, dtype=dtype)\n    for j in range(y_size):\n        y.flat[j] = y_bp_arr.flat[j]\n        J[:, j] = fn_grad(y, x0).flat\n        y.flat[j] = 0\n    return J",
            "def compute_jacobian_analytical(x0, y_shape, fn_grad, y_bp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes the analytical Jacobian\\n\\n    x0:      The position at which to compute J.\\n\\n    y_shape: The shape of the backpropagated value, i.e. the shape of\\n             the output of the corresponding function 'fn'.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    y_bp:    Optional array with custom values for individually scaling\\n             the gradients.\\n\\n    \"\n    dtype = x0.dtype\n    y_size = 1\n    for k in y_shape:\n        y_size *= k\n    J = np.zeros((x0.size, y_size), dtype=dtype)\n    y = np.zeros(y_shape, dtype=dtype)\n    y_bp_arr = np.broadcast_to(y_bp, y_shape) if not y_bp is None else np.ones(y_shape, dtype=dtype)\n    for j in range(y_size):\n        y.flat[j] = y_bp_arr.flat[j]\n        J[:, j] = fn_grad(y, x0).flat\n        y.flat[j] = 0\n    return J",
            "def compute_jacobian_analytical(x0, y_shape, fn_grad, y_bp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes the analytical Jacobian\\n\\n    x0:      The position at which to compute J.\\n\\n    y_shape: The shape of the backpropagated value, i.e. the shape of\\n             the output of the corresponding function 'fn'.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    y_bp:    Optional array with custom values for individually scaling\\n             the gradients.\\n\\n    \"\n    dtype = x0.dtype\n    y_size = 1\n    for k in y_shape:\n        y_size *= k\n    J = np.zeros((x0.size, y_size), dtype=dtype)\n    y = np.zeros(y_shape, dtype=dtype)\n    y_bp_arr = np.broadcast_to(y_bp, y_shape) if not y_bp is None else np.ones(y_shape, dtype=dtype)\n    for j in range(y_size):\n        y.flat[j] = y_bp_arr.flat[j]\n        J[:, j] = fn_grad(y, x0).flat\n        y.flat[j] = 0\n    return J"
        ]
    },
    {
        "func_name": "check_gradients",
        "original": "def check_gradients(x0, fn, fn_grad, epsilon=1e-06, rtol=0.001, atol=1e-05, debug_outputs=OrderedDict()):\n    \"\"\"Checks if the numerical and analytical gradients are compatible for a function 'fn'\n\n    x0:      The position at which to compute the gradients.\n\n    fn:      A function of the form fn(x) which returns a single numpy array.\n\n    fn_grad: The gradient of the original function with the form\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\n             value and 'x0' is the original input to 'fn'. The output of\n             the function is the gradient of x wrt to y.\n\n    epsilon: A scalar or an array that can be broadcasted to the same\n             shape as x0. This is used for computing the numerical Jacobian\n\n    rtol:    The relative tolerance parameter used in numpy.allclose()\n\n    atol:    The absolute tolerance parameter used in numpy.allclose()\n\n    debug_outputs: Output variable which stores additional outputs useful for\n                   debugging in a dictionary.\n    \"\"\"\n    dtype = x0.dtype\n    y = fn(x0)\n    grad = fn_grad(np.zeros(y.shape, dtype=dtype), x0)\n    grad_shape_correct = x0.shape == grad.shape\n    if not grad_shape_correct:\n        print('The shape of the gradient [{0}] does not match the shape of \"x0\" [{1}].'.format(grad.shape, x0.shape))\n    zero_grad = np.count_nonzero(grad) == 0\n    if not zero_grad:\n        print('The gradient is not zero for a zero backprop vector.')\n    ana_J = compute_jacobian_analytical(x0, y.shape, fn_grad)\n    ana_J2 = compute_jacobian_analytical(x0, y.shape, fn_grad, 2 * np.ones(y.shape, dtype=x0.dtype))\n    num_J = compute_jacobian_finite_differences(x0, fn, epsilon)\n    does_scale = np.allclose(0.5 * ana_J2, ana_J, rtol, atol)\n    isclose = np.allclose(ana_J, num_J, rtol, atol)\n    ana_J_iszero = np.all(ana_J == 0)\n    if ana_J_iszero and (not np.allclose(num_J, np.zeros_like(num_J), rtol, atol)):\n        print('The values of the analytical Jacobian are all zero but the values of the numerical Jacobian are not.')\n    elif not does_scale:\n        print('The gradients do not scale with respect to the backpropagated values.')\n    if not isclose:\n        print('The gradients are not close to the numerical Jacobian.')\n    debug_outputs.update(OrderedDict([('isclose', isclose), ('does_scale', does_scale), ('ana_J_iszero', ana_J_iszero), ('grad_shape_correct', grad_shape_correct), ('zero_grad', zero_grad), ('ana_J', ana_J), ('num_J', num_J), ('absdiff', np.abs(ana_J - num_J))]))\n    result = isclose and does_scale\n    return result",
        "mutated": [
            "def check_gradients(x0, fn, fn_grad, epsilon=1e-06, rtol=0.001, atol=1e-05, debug_outputs=OrderedDict()):\n    if False:\n        i = 10\n    \"Checks if the numerical and analytical gradients are compatible for a function 'fn'\\n\\n    x0:      The position at which to compute the gradients.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0. This is used for computing the numerical Jacobian\\n\\n    rtol:    The relative tolerance parameter used in numpy.allclose()\\n\\n    atol:    The absolute tolerance parameter used in numpy.allclose()\\n\\n    debug_outputs: Output variable which stores additional outputs useful for\\n                   debugging in a dictionary.\\n    \"\n    dtype = x0.dtype\n    y = fn(x0)\n    grad = fn_grad(np.zeros(y.shape, dtype=dtype), x0)\n    grad_shape_correct = x0.shape == grad.shape\n    if not grad_shape_correct:\n        print('The shape of the gradient [{0}] does not match the shape of \"x0\" [{1}].'.format(grad.shape, x0.shape))\n    zero_grad = np.count_nonzero(grad) == 0\n    if not zero_grad:\n        print('The gradient is not zero for a zero backprop vector.')\n    ana_J = compute_jacobian_analytical(x0, y.shape, fn_grad)\n    ana_J2 = compute_jacobian_analytical(x0, y.shape, fn_grad, 2 * np.ones(y.shape, dtype=x0.dtype))\n    num_J = compute_jacobian_finite_differences(x0, fn, epsilon)\n    does_scale = np.allclose(0.5 * ana_J2, ana_J, rtol, atol)\n    isclose = np.allclose(ana_J, num_J, rtol, atol)\n    ana_J_iszero = np.all(ana_J == 0)\n    if ana_J_iszero and (not np.allclose(num_J, np.zeros_like(num_J), rtol, atol)):\n        print('The values of the analytical Jacobian are all zero but the values of the numerical Jacobian are not.')\n    elif not does_scale:\n        print('The gradients do not scale with respect to the backpropagated values.')\n    if not isclose:\n        print('The gradients are not close to the numerical Jacobian.')\n    debug_outputs.update(OrderedDict([('isclose', isclose), ('does_scale', does_scale), ('ana_J_iszero', ana_J_iszero), ('grad_shape_correct', grad_shape_correct), ('zero_grad', zero_grad), ('ana_J', ana_J), ('num_J', num_J), ('absdiff', np.abs(ana_J - num_J))]))\n    result = isclose and does_scale\n    return result",
            "def check_gradients(x0, fn, fn_grad, epsilon=1e-06, rtol=0.001, atol=1e-05, debug_outputs=OrderedDict()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Checks if the numerical and analytical gradients are compatible for a function 'fn'\\n\\n    x0:      The position at which to compute the gradients.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0. This is used for computing the numerical Jacobian\\n\\n    rtol:    The relative tolerance parameter used in numpy.allclose()\\n\\n    atol:    The absolute tolerance parameter used in numpy.allclose()\\n\\n    debug_outputs: Output variable which stores additional outputs useful for\\n                   debugging in a dictionary.\\n    \"\n    dtype = x0.dtype\n    y = fn(x0)\n    grad = fn_grad(np.zeros(y.shape, dtype=dtype), x0)\n    grad_shape_correct = x0.shape == grad.shape\n    if not grad_shape_correct:\n        print('The shape of the gradient [{0}] does not match the shape of \"x0\" [{1}].'.format(grad.shape, x0.shape))\n    zero_grad = np.count_nonzero(grad) == 0\n    if not zero_grad:\n        print('The gradient is not zero for a zero backprop vector.')\n    ana_J = compute_jacobian_analytical(x0, y.shape, fn_grad)\n    ana_J2 = compute_jacobian_analytical(x0, y.shape, fn_grad, 2 * np.ones(y.shape, dtype=x0.dtype))\n    num_J = compute_jacobian_finite_differences(x0, fn, epsilon)\n    does_scale = np.allclose(0.5 * ana_J2, ana_J, rtol, atol)\n    isclose = np.allclose(ana_J, num_J, rtol, atol)\n    ana_J_iszero = np.all(ana_J == 0)\n    if ana_J_iszero and (not np.allclose(num_J, np.zeros_like(num_J), rtol, atol)):\n        print('The values of the analytical Jacobian are all zero but the values of the numerical Jacobian are not.')\n    elif not does_scale:\n        print('The gradients do not scale with respect to the backpropagated values.')\n    if not isclose:\n        print('The gradients are not close to the numerical Jacobian.')\n    debug_outputs.update(OrderedDict([('isclose', isclose), ('does_scale', does_scale), ('ana_J_iszero', ana_J_iszero), ('grad_shape_correct', grad_shape_correct), ('zero_grad', zero_grad), ('ana_J', ana_J), ('num_J', num_J), ('absdiff', np.abs(ana_J - num_J))]))\n    result = isclose and does_scale\n    return result",
            "def check_gradients(x0, fn, fn_grad, epsilon=1e-06, rtol=0.001, atol=1e-05, debug_outputs=OrderedDict()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Checks if the numerical and analytical gradients are compatible for a function 'fn'\\n\\n    x0:      The position at which to compute the gradients.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0. This is used for computing the numerical Jacobian\\n\\n    rtol:    The relative tolerance parameter used in numpy.allclose()\\n\\n    atol:    The absolute tolerance parameter used in numpy.allclose()\\n\\n    debug_outputs: Output variable which stores additional outputs useful for\\n                   debugging in a dictionary.\\n    \"\n    dtype = x0.dtype\n    y = fn(x0)\n    grad = fn_grad(np.zeros(y.shape, dtype=dtype), x0)\n    grad_shape_correct = x0.shape == grad.shape\n    if not grad_shape_correct:\n        print('The shape of the gradient [{0}] does not match the shape of \"x0\" [{1}].'.format(grad.shape, x0.shape))\n    zero_grad = np.count_nonzero(grad) == 0\n    if not zero_grad:\n        print('The gradient is not zero for a zero backprop vector.')\n    ana_J = compute_jacobian_analytical(x0, y.shape, fn_grad)\n    ana_J2 = compute_jacobian_analytical(x0, y.shape, fn_grad, 2 * np.ones(y.shape, dtype=x0.dtype))\n    num_J = compute_jacobian_finite_differences(x0, fn, epsilon)\n    does_scale = np.allclose(0.5 * ana_J2, ana_J, rtol, atol)\n    isclose = np.allclose(ana_J, num_J, rtol, atol)\n    ana_J_iszero = np.all(ana_J == 0)\n    if ana_J_iszero and (not np.allclose(num_J, np.zeros_like(num_J), rtol, atol)):\n        print('The values of the analytical Jacobian are all zero but the values of the numerical Jacobian are not.')\n    elif not does_scale:\n        print('The gradients do not scale with respect to the backpropagated values.')\n    if not isclose:\n        print('The gradients are not close to the numerical Jacobian.')\n    debug_outputs.update(OrderedDict([('isclose', isclose), ('does_scale', does_scale), ('ana_J_iszero', ana_J_iszero), ('grad_shape_correct', grad_shape_correct), ('zero_grad', zero_grad), ('ana_J', ana_J), ('num_J', num_J), ('absdiff', np.abs(ana_J - num_J))]))\n    result = isclose and does_scale\n    return result",
            "def check_gradients(x0, fn, fn_grad, epsilon=1e-06, rtol=0.001, atol=1e-05, debug_outputs=OrderedDict()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Checks if the numerical and analytical gradients are compatible for a function 'fn'\\n\\n    x0:      The position at which to compute the gradients.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0. This is used for computing the numerical Jacobian\\n\\n    rtol:    The relative tolerance parameter used in numpy.allclose()\\n\\n    atol:    The absolute tolerance parameter used in numpy.allclose()\\n\\n    debug_outputs: Output variable which stores additional outputs useful for\\n                   debugging in a dictionary.\\n    \"\n    dtype = x0.dtype\n    y = fn(x0)\n    grad = fn_grad(np.zeros(y.shape, dtype=dtype), x0)\n    grad_shape_correct = x0.shape == grad.shape\n    if not grad_shape_correct:\n        print('The shape of the gradient [{0}] does not match the shape of \"x0\" [{1}].'.format(grad.shape, x0.shape))\n    zero_grad = np.count_nonzero(grad) == 0\n    if not zero_grad:\n        print('The gradient is not zero for a zero backprop vector.')\n    ana_J = compute_jacobian_analytical(x0, y.shape, fn_grad)\n    ana_J2 = compute_jacobian_analytical(x0, y.shape, fn_grad, 2 * np.ones(y.shape, dtype=x0.dtype))\n    num_J = compute_jacobian_finite_differences(x0, fn, epsilon)\n    does_scale = np.allclose(0.5 * ana_J2, ana_J, rtol, atol)\n    isclose = np.allclose(ana_J, num_J, rtol, atol)\n    ana_J_iszero = np.all(ana_J == 0)\n    if ana_J_iszero and (not np.allclose(num_J, np.zeros_like(num_J), rtol, atol)):\n        print('The values of the analytical Jacobian are all zero but the values of the numerical Jacobian are not.')\n    elif not does_scale:\n        print('The gradients do not scale with respect to the backpropagated values.')\n    if not isclose:\n        print('The gradients are not close to the numerical Jacobian.')\n    debug_outputs.update(OrderedDict([('isclose', isclose), ('does_scale', does_scale), ('ana_J_iszero', ana_J_iszero), ('grad_shape_correct', grad_shape_correct), ('zero_grad', zero_grad), ('ana_J', ana_J), ('num_J', num_J), ('absdiff', np.abs(ana_J - num_J))]))\n    result = isclose and does_scale\n    return result",
            "def check_gradients(x0, fn, fn_grad, epsilon=1e-06, rtol=0.001, atol=1e-05, debug_outputs=OrderedDict()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Checks if the numerical and analytical gradients are compatible for a function 'fn'\\n\\n    x0:      The position at which to compute the gradients.\\n\\n    fn:      A function of the form fn(x) which returns a single numpy array.\\n\\n    fn_grad: The gradient of the original function with the form\\n             x_grad = fn_grad(y_bp, x0) where 'y_bp' is the backpropagated\\n             value and 'x0' is the original input to 'fn'. The output of\\n             the function is the gradient of x wrt to y.\\n\\n    epsilon: A scalar or an array that can be broadcasted to the same\\n             shape as x0. This is used for computing the numerical Jacobian\\n\\n    rtol:    The relative tolerance parameter used in numpy.allclose()\\n\\n    atol:    The absolute tolerance parameter used in numpy.allclose()\\n\\n    debug_outputs: Output variable which stores additional outputs useful for\\n                   debugging in a dictionary.\\n    \"\n    dtype = x0.dtype\n    y = fn(x0)\n    grad = fn_grad(np.zeros(y.shape, dtype=dtype), x0)\n    grad_shape_correct = x0.shape == grad.shape\n    if not grad_shape_correct:\n        print('The shape of the gradient [{0}] does not match the shape of \"x0\" [{1}].'.format(grad.shape, x0.shape))\n    zero_grad = np.count_nonzero(grad) == 0\n    if not zero_grad:\n        print('The gradient is not zero for a zero backprop vector.')\n    ana_J = compute_jacobian_analytical(x0, y.shape, fn_grad)\n    ana_J2 = compute_jacobian_analytical(x0, y.shape, fn_grad, 2 * np.ones(y.shape, dtype=x0.dtype))\n    num_J = compute_jacobian_finite_differences(x0, fn, epsilon)\n    does_scale = np.allclose(0.5 * ana_J2, ana_J, rtol, atol)\n    isclose = np.allclose(ana_J, num_J, rtol, atol)\n    ana_J_iszero = np.all(ana_J == 0)\n    if ana_J_iszero and (not np.allclose(num_J, np.zeros_like(num_J), rtol, atol)):\n        print('The values of the analytical Jacobian are all zero but the values of the numerical Jacobian are not.')\n    elif not does_scale:\n        print('The gradients do not scale with respect to the backpropagated values.')\n    if not isclose:\n        print('The gradients are not close to the numerical Jacobian.')\n    debug_outputs.update(OrderedDict([('isclose', isclose), ('does_scale', does_scale), ('ana_J_iszero', ana_J_iszero), ('grad_shape_correct', grad_shape_correct), ('zero_grad', zero_grad), ('ana_J', ana_J), ('num_J', num_J), ('absdiff', np.abs(ana_J - num_J))]))\n    result = isclose and does_scale\n    return result"
        ]
    }
]