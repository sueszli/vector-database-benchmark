[
    {
        "func_name": "convert_luke_checkpoint",
        "original": "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')['module']\n    entity_vocab = load_original_entity_vocab(entity_vocab_path)\n    entity_vocab['[MASK2]'] = max(entity_vocab.values()) + 1\n    config.entity_vocab_size += 1\n    tokenizer = XLMRobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'r') as f:\n        tokenizer_config = json.load(f)\n    tokenizer_config['tokenizer_class'] = 'MLukeTokenizer'\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'w') as f:\n        json.dump(tokenizer_config, f)\n    with open(os.path.join(pytorch_dump_folder_path, MLukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    ent_init_index = tokenizer.convert_tokens_to_ids(['@'])[0]\n    ent2_init_index = tokenizer.convert_tokens_to_ids(['#'])[0]\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[ent_init_index].unsqueeze(0)\n    ent2_emb = word_emb[ent2_init_index].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for bias_name in ['lm_head.decoder.bias', 'lm_head.bias']:\n        decoder_bias = state_dict[bias_name]\n        ent_decoder_bias = decoder_bias[ent_init_index].unsqueeze(0)\n        ent2_decoder_bias = decoder_bias[ent2_init_index].unsqueeze(0)\n        state_dict[bias_name] = torch.cat([decoder_bias, ent_decoder_bias, ent2_decoder_bias])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_mask_emb = entity_emb[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_embeddings.entity_embeddings.weight'] = torch.cat([entity_emb, entity_mask_emb])\n    entity_prediction_bias = state_dict['entity_predictions.bias']\n    entity_mask_bias = entity_prediction_bias[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_predictions.bias'] = torch.cat([entity_prediction_bias, entity_mask_bias])\n    model = LukeForMaskedLM(config=config).eval()\n    state_dict.pop('entity_predictions.decoder.weight')\n    state_dict.pop('lm_head.decoder.weight')\n    state_dict.pop('lm_head.decoder.bias')\n    state_dict_for_hugging_face = OrderedDict()\n    for (key, value) in state_dict.items():\n        if not (key.startswith('lm_head') or key.startswith('entity_predictions')):\n            state_dict_for_hugging_face[f'luke.{key}'] = state_dict[key]\n        else:\n            state_dict_for_hugging_face[key] = state_dict[key]\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict_for_hugging_face, strict=False)\n    if set(unexpected_keys) != {'luke.embeddings.position_ids'}:\n        raise ValueError(f'Unexpected unexpected_keys: {unexpected_keys}')\n    if set(missing_keys) != {'lm_head.decoder.weight', 'lm_head.decoder.bias', 'entity_predictions.decoder.weight'}:\n        raise ValueError(f'Unexpected missing_keys: {missing_keys}')\n    model.tie_weights()\n    assert (model.luke.embeddings.word_embeddings.weight == model.lm_head.decoder.weight).all()\n    assert (model.luke.entity_embeddings.entity_embeddings.weight == model.entity_predictions.decoder.weight).all()\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'ISO 639-3 uses the code fas for the dialects spoken across Iran and \u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3 (Afghanistan).'\n    span = (0, 9)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 33, 768))\n        expected_slice = torch.tensor([[0.0892, 0.0596, -0.2819], [0.0134, 0.1199, 0.0573], [-0.0169, 0.0927, 0.0644]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[-0.1482, 0.0609, 0.0322]])\n    if not outputs.entity_last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    text = 'Tokyo is the capital of <mask>.'\n    span = (24, 30)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    input_ids = encoding['input_ids'][0].tolist()\n    mask_position_id = input_ids.index(tokenizer.convert_tokens_to_ids('<mask>'))\n    predicted_id = outputs.logits[0][mask_position_id].argmax(dim=-1)\n    assert 'Japan' == tokenizer.decode(predicted_id)\n    predicted_entity_id = outputs.entity_logits[0][0].argmax().item()\n    multilingual_predicted_entities = [entity for (entity, entity_id) in tokenizer.entity_vocab.items() if entity_id == predicted_entity_id]\n    assert [e for e in multilingual_predicted_entities if e.startswith('en:')][0] == 'en:Japan'\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')['module']\n    entity_vocab = load_original_entity_vocab(entity_vocab_path)\n    entity_vocab['[MASK2]'] = max(entity_vocab.values()) + 1\n    config.entity_vocab_size += 1\n    tokenizer = XLMRobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'r') as f:\n        tokenizer_config = json.load(f)\n    tokenizer_config['tokenizer_class'] = 'MLukeTokenizer'\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'w') as f:\n        json.dump(tokenizer_config, f)\n    with open(os.path.join(pytorch_dump_folder_path, MLukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    ent_init_index = tokenizer.convert_tokens_to_ids(['@'])[0]\n    ent2_init_index = tokenizer.convert_tokens_to_ids(['#'])[0]\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[ent_init_index].unsqueeze(0)\n    ent2_emb = word_emb[ent2_init_index].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for bias_name in ['lm_head.decoder.bias', 'lm_head.bias']:\n        decoder_bias = state_dict[bias_name]\n        ent_decoder_bias = decoder_bias[ent_init_index].unsqueeze(0)\n        ent2_decoder_bias = decoder_bias[ent2_init_index].unsqueeze(0)\n        state_dict[bias_name] = torch.cat([decoder_bias, ent_decoder_bias, ent2_decoder_bias])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_mask_emb = entity_emb[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_embeddings.entity_embeddings.weight'] = torch.cat([entity_emb, entity_mask_emb])\n    entity_prediction_bias = state_dict['entity_predictions.bias']\n    entity_mask_bias = entity_prediction_bias[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_predictions.bias'] = torch.cat([entity_prediction_bias, entity_mask_bias])\n    model = LukeForMaskedLM(config=config).eval()\n    state_dict.pop('entity_predictions.decoder.weight')\n    state_dict.pop('lm_head.decoder.weight')\n    state_dict.pop('lm_head.decoder.bias')\n    state_dict_for_hugging_face = OrderedDict()\n    for (key, value) in state_dict.items():\n        if not (key.startswith('lm_head') or key.startswith('entity_predictions')):\n            state_dict_for_hugging_face[f'luke.{key}'] = state_dict[key]\n        else:\n            state_dict_for_hugging_face[key] = state_dict[key]\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict_for_hugging_face, strict=False)\n    if set(unexpected_keys) != {'luke.embeddings.position_ids'}:\n        raise ValueError(f'Unexpected unexpected_keys: {unexpected_keys}')\n    if set(missing_keys) != {'lm_head.decoder.weight', 'lm_head.decoder.bias', 'entity_predictions.decoder.weight'}:\n        raise ValueError(f'Unexpected missing_keys: {missing_keys}')\n    model.tie_weights()\n    assert (model.luke.embeddings.word_embeddings.weight == model.lm_head.decoder.weight).all()\n    assert (model.luke.entity_embeddings.entity_embeddings.weight == model.entity_predictions.decoder.weight).all()\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'ISO 639-3 uses the code fas for the dialects spoken across Iran and \u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3 (Afghanistan).'\n    span = (0, 9)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 33, 768))\n        expected_slice = torch.tensor([[0.0892, 0.0596, -0.2819], [0.0134, 0.1199, 0.0573], [-0.0169, 0.0927, 0.0644]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[-0.1482, 0.0609, 0.0322]])\n    if not outputs.entity_last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    text = 'Tokyo is the capital of <mask>.'\n    span = (24, 30)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    input_ids = encoding['input_ids'][0].tolist()\n    mask_position_id = input_ids.index(tokenizer.convert_tokens_to_ids('<mask>'))\n    predicted_id = outputs.logits[0][mask_position_id].argmax(dim=-1)\n    assert 'Japan' == tokenizer.decode(predicted_id)\n    predicted_entity_id = outputs.entity_logits[0][0].argmax().item()\n    multilingual_predicted_entities = [entity for (entity, entity_id) in tokenizer.entity_vocab.items() if entity_id == predicted_entity_id]\n    assert [e for e in multilingual_predicted_entities if e.startswith('en:')][0] == 'en:Japan'\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')['module']\n    entity_vocab = load_original_entity_vocab(entity_vocab_path)\n    entity_vocab['[MASK2]'] = max(entity_vocab.values()) + 1\n    config.entity_vocab_size += 1\n    tokenizer = XLMRobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'r') as f:\n        tokenizer_config = json.load(f)\n    tokenizer_config['tokenizer_class'] = 'MLukeTokenizer'\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'w') as f:\n        json.dump(tokenizer_config, f)\n    with open(os.path.join(pytorch_dump_folder_path, MLukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    ent_init_index = tokenizer.convert_tokens_to_ids(['@'])[0]\n    ent2_init_index = tokenizer.convert_tokens_to_ids(['#'])[0]\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[ent_init_index].unsqueeze(0)\n    ent2_emb = word_emb[ent2_init_index].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for bias_name in ['lm_head.decoder.bias', 'lm_head.bias']:\n        decoder_bias = state_dict[bias_name]\n        ent_decoder_bias = decoder_bias[ent_init_index].unsqueeze(0)\n        ent2_decoder_bias = decoder_bias[ent2_init_index].unsqueeze(0)\n        state_dict[bias_name] = torch.cat([decoder_bias, ent_decoder_bias, ent2_decoder_bias])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_mask_emb = entity_emb[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_embeddings.entity_embeddings.weight'] = torch.cat([entity_emb, entity_mask_emb])\n    entity_prediction_bias = state_dict['entity_predictions.bias']\n    entity_mask_bias = entity_prediction_bias[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_predictions.bias'] = torch.cat([entity_prediction_bias, entity_mask_bias])\n    model = LukeForMaskedLM(config=config).eval()\n    state_dict.pop('entity_predictions.decoder.weight')\n    state_dict.pop('lm_head.decoder.weight')\n    state_dict.pop('lm_head.decoder.bias')\n    state_dict_for_hugging_face = OrderedDict()\n    for (key, value) in state_dict.items():\n        if not (key.startswith('lm_head') or key.startswith('entity_predictions')):\n            state_dict_for_hugging_face[f'luke.{key}'] = state_dict[key]\n        else:\n            state_dict_for_hugging_face[key] = state_dict[key]\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict_for_hugging_face, strict=False)\n    if set(unexpected_keys) != {'luke.embeddings.position_ids'}:\n        raise ValueError(f'Unexpected unexpected_keys: {unexpected_keys}')\n    if set(missing_keys) != {'lm_head.decoder.weight', 'lm_head.decoder.bias', 'entity_predictions.decoder.weight'}:\n        raise ValueError(f'Unexpected missing_keys: {missing_keys}')\n    model.tie_weights()\n    assert (model.luke.embeddings.word_embeddings.weight == model.lm_head.decoder.weight).all()\n    assert (model.luke.entity_embeddings.entity_embeddings.weight == model.entity_predictions.decoder.weight).all()\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'ISO 639-3 uses the code fas for the dialects spoken across Iran and \u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3 (Afghanistan).'\n    span = (0, 9)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 33, 768))\n        expected_slice = torch.tensor([[0.0892, 0.0596, -0.2819], [0.0134, 0.1199, 0.0573], [-0.0169, 0.0927, 0.0644]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[-0.1482, 0.0609, 0.0322]])\n    if not outputs.entity_last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    text = 'Tokyo is the capital of <mask>.'\n    span = (24, 30)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    input_ids = encoding['input_ids'][0].tolist()\n    mask_position_id = input_ids.index(tokenizer.convert_tokens_to_ids('<mask>'))\n    predicted_id = outputs.logits[0][mask_position_id].argmax(dim=-1)\n    assert 'Japan' == tokenizer.decode(predicted_id)\n    predicted_entity_id = outputs.entity_logits[0][0].argmax().item()\n    multilingual_predicted_entities = [entity for (entity, entity_id) in tokenizer.entity_vocab.items() if entity_id == predicted_entity_id]\n    assert [e for e in multilingual_predicted_entities if e.startswith('en:')][0] == 'en:Japan'\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')['module']\n    entity_vocab = load_original_entity_vocab(entity_vocab_path)\n    entity_vocab['[MASK2]'] = max(entity_vocab.values()) + 1\n    config.entity_vocab_size += 1\n    tokenizer = XLMRobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'r') as f:\n        tokenizer_config = json.load(f)\n    tokenizer_config['tokenizer_class'] = 'MLukeTokenizer'\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'w') as f:\n        json.dump(tokenizer_config, f)\n    with open(os.path.join(pytorch_dump_folder_path, MLukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    ent_init_index = tokenizer.convert_tokens_to_ids(['@'])[0]\n    ent2_init_index = tokenizer.convert_tokens_to_ids(['#'])[0]\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[ent_init_index].unsqueeze(0)\n    ent2_emb = word_emb[ent2_init_index].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for bias_name in ['lm_head.decoder.bias', 'lm_head.bias']:\n        decoder_bias = state_dict[bias_name]\n        ent_decoder_bias = decoder_bias[ent_init_index].unsqueeze(0)\n        ent2_decoder_bias = decoder_bias[ent2_init_index].unsqueeze(0)\n        state_dict[bias_name] = torch.cat([decoder_bias, ent_decoder_bias, ent2_decoder_bias])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_mask_emb = entity_emb[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_embeddings.entity_embeddings.weight'] = torch.cat([entity_emb, entity_mask_emb])\n    entity_prediction_bias = state_dict['entity_predictions.bias']\n    entity_mask_bias = entity_prediction_bias[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_predictions.bias'] = torch.cat([entity_prediction_bias, entity_mask_bias])\n    model = LukeForMaskedLM(config=config).eval()\n    state_dict.pop('entity_predictions.decoder.weight')\n    state_dict.pop('lm_head.decoder.weight')\n    state_dict.pop('lm_head.decoder.bias')\n    state_dict_for_hugging_face = OrderedDict()\n    for (key, value) in state_dict.items():\n        if not (key.startswith('lm_head') or key.startswith('entity_predictions')):\n            state_dict_for_hugging_face[f'luke.{key}'] = state_dict[key]\n        else:\n            state_dict_for_hugging_face[key] = state_dict[key]\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict_for_hugging_face, strict=False)\n    if set(unexpected_keys) != {'luke.embeddings.position_ids'}:\n        raise ValueError(f'Unexpected unexpected_keys: {unexpected_keys}')\n    if set(missing_keys) != {'lm_head.decoder.weight', 'lm_head.decoder.bias', 'entity_predictions.decoder.weight'}:\n        raise ValueError(f'Unexpected missing_keys: {missing_keys}')\n    model.tie_weights()\n    assert (model.luke.embeddings.word_embeddings.weight == model.lm_head.decoder.weight).all()\n    assert (model.luke.entity_embeddings.entity_embeddings.weight == model.entity_predictions.decoder.weight).all()\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'ISO 639-3 uses the code fas for the dialects spoken across Iran and \u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3 (Afghanistan).'\n    span = (0, 9)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 33, 768))\n        expected_slice = torch.tensor([[0.0892, 0.0596, -0.2819], [0.0134, 0.1199, 0.0573], [-0.0169, 0.0927, 0.0644]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[-0.1482, 0.0609, 0.0322]])\n    if not outputs.entity_last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    text = 'Tokyo is the capital of <mask>.'\n    span = (24, 30)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    input_ids = encoding['input_ids'][0].tolist()\n    mask_position_id = input_ids.index(tokenizer.convert_tokens_to_ids('<mask>'))\n    predicted_id = outputs.logits[0][mask_position_id].argmax(dim=-1)\n    assert 'Japan' == tokenizer.decode(predicted_id)\n    predicted_entity_id = outputs.entity_logits[0][0].argmax().item()\n    multilingual_predicted_entities = [entity for (entity, entity_id) in tokenizer.entity_vocab.items() if entity_id == predicted_entity_id]\n    assert [e for e in multilingual_predicted_entities if e.startswith('en:')][0] == 'en:Japan'\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')['module']\n    entity_vocab = load_original_entity_vocab(entity_vocab_path)\n    entity_vocab['[MASK2]'] = max(entity_vocab.values()) + 1\n    config.entity_vocab_size += 1\n    tokenizer = XLMRobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'r') as f:\n        tokenizer_config = json.load(f)\n    tokenizer_config['tokenizer_class'] = 'MLukeTokenizer'\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'w') as f:\n        json.dump(tokenizer_config, f)\n    with open(os.path.join(pytorch_dump_folder_path, MLukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    ent_init_index = tokenizer.convert_tokens_to_ids(['@'])[0]\n    ent2_init_index = tokenizer.convert_tokens_to_ids(['#'])[0]\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[ent_init_index].unsqueeze(0)\n    ent2_emb = word_emb[ent2_init_index].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for bias_name in ['lm_head.decoder.bias', 'lm_head.bias']:\n        decoder_bias = state_dict[bias_name]\n        ent_decoder_bias = decoder_bias[ent_init_index].unsqueeze(0)\n        ent2_decoder_bias = decoder_bias[ent2_init_index].unsqueeze(0)\n        state_dict[bias_name] = torch.cat([decoder_bias, ent_decoder_bias, ent2_decoder_bias])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_mask_emb = entity_emb[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_embeddings.entity_embeddings.weight'] = torch.cat([entity_emb, entity_mask_emb])\n    entity_prediction_bias = state_dict['entity_predictions.bias']\n    entity_mask_bias = entity_prediction_bias[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_predictions.bias'] = torch.cat([entity_prediction_bias, entity_mask_bias])\n    model = LukeForMaskedLM(config=config).eval()\n    state_dict.pop('entity_predictions.decoder.weight')\n    state_dict.pop('lm_head.decoder.weight')\n    state_dict.pop('lm_head.decoder.bias')\n    state_dict_for_hugging_face = OrderedDict()\n    for (key, value) in state_dict.items():\n        if not (key.startswith('lm_head') or key.startswith('entity_predictions')):\n            state_dict_for_hugging_face[f'luke.{key}'] = state_dict[key]\n        else:\n            state_dict_for_hugging_face[key] = state_dict[key]\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict_for_hugging_face, strict=False)\n    if set(unexpected_keys) != {'luke.embeddings.position_ids'}:\n        raise ValueError(f'Unexpected unexpected_keys: {unexpected_keys}')\n    if set(missing_keys) != {'lm_head.decoder.weight', 'lm_head.decoder.bias', 'entity_predictions.decoder.weight'}:\n        raise ValueError(f'Unexpected missing_keys: {missing_keys}')\n    model.tie_weights()\n    assert (model.luke.embeddings.word_embeddings.weight == model.lm_head.decoder.weight).all()\n    assert (model.luke.entity_embeddings.entity_embeddings.weight == model.entity_predictions.decoder.weight).all()\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'ISO 639-3 uses the code fas for the dialects spoken across Iran and \u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3 (Afghanistan).'\n    span = (0, 9)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 33, 768))\n        expected_slice = torch.tensor([[0.0892, 0.0596, -0.2819], [0.0134, 0.1199, 0.0573], [-0.0169, 0.0927, 0.0644]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[-0.1482, 0.0609, 0.0322]])\n    if not outputs.entity_last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    text = 'Tokyo is the capital of <mask>.'\n    span = (24, 30)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    input_ids = encoding['input_ids'][0].tolist()\n    mask_position_id = input_ids.index(tokenizer.convert_tokens_to_ids('<mask>'))\n    predicted_id = outputs.logits[0][mask_position_id].argmax(dim=-1)\n    assert 'Japan' == tokenizer.decode(predicted_id)\n    predicted_entity_id = outputs.entity_logits[0][0].argmax().item()\n    multilingual_predicted_entities = [entity for (entity, entity_id) in tokenizer.entity_vocab.items() if entity_id == predicted_entity_id]\n    assert [e for e in multilingual_predicted_entities if e.startswith('en:')][0] == 'en:Japan'\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with open(metadata_path) as metadata_file:\n        metadata = json.load(metadata_file)\n    config = LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])\n    state_dict = torch.load(checkpoint_path, map_location='cpu')['module']\n    entity_vocab = load_original_entity_vocab(entity_vocab_path)\n    entity_vocab['[MASK2]'] = max(entity_vocab.values()) + 1\n    config.entity_vocab_size += 1\n    tokenizer = XLMRobertaTokenizer.from_pretrained(metadata['model_config']['bert_model_name'])\n    entity_token_1 = AddedToken('<ent>', lstrip=False, rstrip=False)\n    entity_token_2 = AddedToken('<ent2>', lstrip=False, rstrip=False)\n    tokenizer.add_special_tokens({'additional_special_tokens': [entity_token_1, entity_token_2]})\n    config.vocab_size += 2\n    print(f'Saving tokenizer to {pytorch_dump_folder_path}')\n    tokenizer.save_pretrained(pytorch_dump_folder_path)\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'r') as f:\n        tokenizer_config = json.load(f)\n    tokenizer_config['tokenizer_class'] = 'MLukeTokenizer'\n    with open(os.path.join(pytorch_dump_folder_path, 'tokenizer_config.json'), 'w') as f:\n        json.dump(tokenizer_config, f)\n    with open(os.path.join(pytorch_dump_folder_path, MLukeTokenizer.vocab_files_names['entity_vocab_file']), 'w') as f:\n        json.dump(entity_vocab, f)\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    ent_init_index = tokenizer.convert_tokens_to_ids(['@'])[0]\n    ent2_init_index = tokenizer.convert_tokens_to_ids(['#'])[0]\n    word_emb = state_dict['embeddings.word_embeddings.weight']\n    ent_emb = word_emb[ent_init_index].unsqueeze(0)\n    ent2_emb = word_emb[ent2_init_index].unsqueeze(0)\n    state_dict['embeddings.word_embeddings.weight'] = torch.cat([word_emb, ent_emb, ent2_emb])\n    for bias_name in ['lm_head.decoder.bias', 'lm_head.bias']:\n        decoder_bias = state_dict[bias_name]\n        ent_decoder_bias = decoder_bias[ent_init_index].unsqueeze(0)\n        ent2_decoder_bias = decoder_bias[ent2_init_index].unsqueeze(0)\n        state_dict[bias_name] = torch.cat([decoder_bias, ent_decoder_bias, ent2_decoder_bias])\n    for layer_index in range(config.num_hidden_layers):\n        for matrix_name in ['query.weight', 'query.bias']:\n            prefix = f'encoder.layer.{layer_index}.attention.self.'\n            state_dict[prefix + 'w2e_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2w_' + matrix_name] = state_dict[prefix + matrix_name]\n            state_dict[prefix + 'e2e_' + matrix_name] = state_dict[prefix + matrix_name]\n    entity_emb = state_dict['entity_embeddings.entity_embeddings.weight']\n    entity_mask_emb = entity_emb[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_embeddings.entity_embeddings.weight'] = torch.cat([entity_emb, entity_mask_emb])\n    entity_prediction_bias = state_dict['entity_predictions.bias']\n    entity_mask_bias = entity_prediction_bias[entity_vocab['[MASK]']].unsqueeze(0)\n    state_dict['entity_predictions.bias'] = torch.cat([entity_prediction_bias, entity_mask_bias])\n    model = LukeForMaskedLM(config=config).eval()\n    state_dict.pop('entity_predictions.decoder.weight')\n    state_dict.pop('lm_head.decoder.weight')\n    state_dict.pop('lm_head.decoder.bias')\n    state_dict_for_hugging_face = OrderedDict()\n    for (key, value) in state_dict.items():\n        if not (key.startswith('lm_head') or key.startswith('entity_predictions')):\n            state_dict_for_hugging_face[f'luke.{key}'] = state_dict[key]\n        else:\n            state_dict_for_hugging_face[key] = state_dict[key]\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict_for_hugging_face, strict=False)\n    if set(unexpected_keys) != {'luke.embeddings.position_ids'}:\n        raise ValueError(f'Unexpected unexpected_keys: {unexpected_keys}')\n    if set(missing_keys) != {'lm_head.decoder.weight', 'lm_head.decoder.bias', 'entity_predictions.decoder.weight'}:\n        raise ValueError(f'Unexpected missing_keys: {missing_keys}')\n    model.tie_weights()\n    assert (model.luke.embeddings.word_embeddings.weight == model.lm_head.decoder.weight).all()\n    assert (model.luke.entity_embeddings.entity_embeddings.weight == model.entity_predictions.decoder.weight).all()\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')\n    text = 'ISO 639-3 uses the code fas for the dialects spoken across Iran and \u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3 (Afghanistan).'\n    span = (0, 9)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 33, 768))\n        expected_slice = torch.tensor([[0.0892, 0.0596, -0.2819], [0.0134, 0.1199, 0.0573], [-0.0169, 0.0927, 0.0644]])\n    if not outputs.last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.last_hidden_state.shape is {outputs.last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    if model_size == 'large':\n        raise NotImplementedError\n    else:\n        expected_shape = torch.Size((1, 1, 768))\n        expected_slice = torch.tensor([[-0.1482, 0.0609, 0.0322]])\n    if not outputs.entity_last_hidden_state.shape == expected_shape:\n        raise ValueError(f'Outputs.entity_last_hidden_state.shape is {outputs.entity_last_hidden_state.shape}, Expected shape is {expected_shape}')\n    if not torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=0.0001):\n        raise ValueError\n    tokenizer = MLukeTokenizer.from_pretrained(pytorch_dump_folder_path)\n    text = 'Tokyo is the capital of <mask>.'\n    span = (24, 30)\n    encoding = tokenizer(text, entity_spans=[span], return_tensors='pt')\n    outputs = model(**encoding)\n    input_ids = encoding['input_ids'][0].tolist()\n    mask_position_id = input_ids.index(tokenizer.convert_tokens_to_ids('<mask>'))\n    predicted_id = outputs.logits[0][mask_position_id].argmax(dim=-1)\n    assert 'Japan' == tokenizer.decode(predicted_id)\n    predicted_entity_id = outputs.entity_logits[0][0].argmax().item()\n    multilingual_predicted_entities = [entity for (entity, entity_id) in tokenizer.entity_vocab.items() if entity_id == predicted_entity_id]\n    assert [e for e in multilingual_predicted_entities if e.startswith('en:')][0] == 'en:Japan'\n    print('Saving PyTorch model to {}'.format(pytorch_dump_folder_path))\n    model.save_pretrained(pytorch_dump_folder_path)"
        ]
    },
    {
        "func_name": "load_original_entity_vocab",
        "original": "def load_original_entity_vocab(entity_vocab_path):\n    SPECIAL_TOKENS = ['[MASK]', '[PAD]', '[UNK]']\n    data = [json.loads(line) for line in open(entity_vocab_path)]\n    new_mapping = {}\n    for entry in data:\n        entity_id = entry['id']\n        for (entity_name, language) in entry['entities']:\n            if entity_name in SPECIAL_TOKENS:\n                new_mapping[entity_name] = entity_id\n                break\n            new_entity_name = f'{language}:{entity_name}'\n            new_mapping[new_entity_name] = entity_id\n    return new_mapping",
        "mutated": [
            "def load_original_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n    SPECIAL_TOKENS = ['[MASK]', '[PAD]', '[UNK]']\n    data = [json.loads(line) for line in open(entity_vocab_path)]\n    new_mapping = {}\n    for entry in data:\n        entity_id = entry['id']\n        for (entity_name, language) in entry['entities']:\n            if entity_name in SPECIAL_TOKENS:\n                new_mapping[entity_name] = entity_id\n                break\n            new_entity_name = f'{language}:{entity_name}'\n            new_mapping[new_entity_name] = entity_id\n    return new_mapping",
            "def load_original_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    SPECIAL_TOKENS = ['[MASK]', '[PAD]', '[UNK]']\n    data = [json.loads(line) for line in open(entity_vocab_path)]\n    new_mapping = {}\n    for entry in data:\n        entity_id = entry['id']\n        for (entity_name, language) in entry['entities']:\n            if entity_name in SPECIAL_TOKENS:\n                new_mapping[entity_name] = entity_id\n                break\n            new_entity_name = f'{language}:{entity_name}'\n            new_mapping[new_entity_name] = entity_id\n    return new_mapping",
            "def load_original_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    SPECIAL_TOKENS = ['[MASK]', '[PAD]', '[UNK]']\n    data = [json.loads(line) for line in open(entity_vocab_path)]\n    new_mapping = {}\n    for entry in data:\n        entity_id = entry['id']\n        for (entity_name, language) in entry['entities']:\n            if entity_name in SPECIAL_TOKENS:\n                new_mapping[entity_name] = entity_id\n                break\n            new_entity_name = f'{language}:{entity_name}'\n            new_mapping[new_entity_name] = entity_id\n    return new_mapping",
            "def load_original_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    SPECIAL_TOKENS = ['[MASK]', '[PAD]', '[UNK]']\n    data = [json.loads(line) for line in open(entity_vocab_path)]\n    new_mapping = {}\n    for entry in data:\n        entity_id = entry['id']\n        for (entity_name, language) in entry['entities']:\n            if entity_name in SPECIAL_TOKENS:\n                new_mapping[entity_name] = entity_id\n                break\n            new_entity_name = f'{language}:{entity_name}'\n            new_mapping[new_entity_name] = entity_id\n    return new_mapping",
            "def load_original_entity_vocab(entity_vocab_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    SPECIAL_TOKENS = ['[MASK]', '[PAD]', '[UNK]']\n    data = [json.loads(line) for line in open(entity_vocab_path)]\n    new_mapping = {}\n    for entry in data:\n        entity_id = entry['id']\n        for (entity_name, language) in entry['entities']:\n            if entity_name in SPECIAL_TOKENS:\n                new_mapping[entity_name] = entity_id\n                break\n            new_entity_name = f'{language}:{entity_name}'\n            new_mapping[new_entity_name] = entity_id\n    return new_mapping"
        ]
    }
]