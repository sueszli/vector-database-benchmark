[
    {
        "func_name": "get_l1_distribution_dist",
        "original": "def get_l1_distribution_dist(mu1, mu2):\n    mu1d = mu1.distribution\n    mu2d = mu2.distribution\n    states = set(list(mu1d.keys()) + list(mu2d.keys()))\n    return sum([abs(mu1d.get(a, 0.0) - mu2d.get(a, 0.0)) for a in states]) * FLAGS.dt / FLAGS.horizon",
        "mutated": [
            "def get_l1_distribution_dist(mu1, mu2):\n    if False:\n        i = 10\n    mu1d = mu1.distribution\n    mu2d = mu2.distribution\n    states = set(list(mu1d.keys()) + list(mu2d.keys()))\n    return sum([abs(mu1d.get(a, 0.0) - mu2d.get(a, 0.0)) for a in states]) * FLAGS.dt / FLAGS.horizon",
            "def get_l1_distribution_dist(mu1, mu2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mu1d = mu1.distribution\n    mu2d = mu2.distribution\n    states = set(list(mu1d.keys()) + list(mu2d.keys()))\n    return sum([abs(mu1d.get(a, 0.0) - mu2d.get(a, 0.0)) for a in states]) * FLAGS.dt / FLAGS.horizon",
            "def get_l1_distribution_dist(mu1, mu2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mu1d = mu1.distribution\n    mu2d = mu2.distribution\n    states = set(list(mu1d.keys()) + list(mu2d.keys()))\n    return sum([abs(mu1d.get(a, 0.0) - mu2d.get(a, 0.0)) for a in states]) * FLAGS.dt / FLAGS.horizon",
            "def get_l1_distribution_dist(mu1, mu2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mu1d = mu1.distribution\n    mu2d = mu2.distribution\n    states = set(list(mu1d.keys()) + list(mu2d.keys()))\n    return sum([abs(mu1d.get(a, 0.0) - mu2d.get(a, 0.0)) for a in states]) * FLAGS.dt / FLAGS.horizon",
            "def get_l1_distribution_dist(mu1, mu2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mu1d = mu1.distribution\n    mu2d = mu2.distribution\n    states = set(list(mu1d.keys()) + list(mu2d.keys()))\n    return sum([abs(mu1d.get(a, 0.0) - mu2d.get(a, 0.0)) for a in states]) * FLAGS.dt / FLAGS.horizon"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, game, player_ids):\n    \"\"\"Initializes the projected policy.\n\n    Args:\n      game: The game to analyze.\n      player_ids: list of player ids for which this policy applies; each should\n        be in the range 0..game.num_players()-1.\n    \"\"\"\n    super(LinearPolicy, self).__init__(game, player_ids)",
        "mutated": [
            "def __init__(self, game, player_ids):\n    if False:\n        i = 10\n    'Initializes the projected policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n    '\n    super(LinearPolicy, self).__init__(game, player_ids)",
            "def __init__(self, game, player_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes the projected policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n    '\n    super(LinearPolicy, self).__init__(game, player_ids)",
            "def __init__(self, game, player_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes the projected policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n    '\n    super(LinearPolicy, self).__init__(game, player_ids)",
            "def __init__(self, game, player_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes the projected policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n    '\n    super(LinearPolicy, self).__init__(game, player_ids)",
            "def __init__(self, game, player_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes the projected policy.\\n\\n    Args:\\n      game: The game to analyze.\\n      player_ids: list of player ids for which this policy applies; each should\\n        be in the range 0..game.num_players()-1.\\n    '\n    super(LinearPolicy, self).__init__(game, player_ids)"
        ]
    },
    {
        "func_name": "action_probabilities",
        "original": "def action_probabilities(self, state, player_id=None):\n    mu_bar_t = state.distribution_average()\n    x_t = state.x\n    q = state.cross_q\n    n_actions_per_side = state.n_actions_per_side\n    lin_action = (q + state.eta_t()) * (mu_bar_t - x_t)\n    action = n_actions_per_side + min(n_actions_per_side, max(round(lin_action), -n_actions_per_side))\n    action_prob = [(a, 0.0) for a in state.legal_actions()]\n    action_prob[action] = (action, 1.0)\n    return dict(action_prob)",
        "mutated": [
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n    mu_bar_t = state.distribution_average()\n    x_t = state.x\n    q = state.cross_q\n    n_actions_per_side = state.n_actions_per_side\n    lin_action = (q + state.eta_t()) * (mu_bar_t - x_t)\n    action = n_actions_per_side + min(n_actions_per_side, max(round(lin_action), -n_actions_per_side))\n    action_prob = [(a, 0.0) for a in state.legal_actions()]\n    action_prob[action] = (action, 1.0)\n    return dict(action_prob)",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mu_bar_t = state.distribution_average()\n    x_t = state.x\n    q = state.cross_q\n    n_actions_per_side = state.n_actions_per_side\n    lin_action = (q + state.eta_t()) * (mu_bar_t - x_t)\n    action = n_actions_per_side + min(n_actions_per_side, max(round(lin_action), -n_actions_per_side))\n    action_prob = [(a, 0.0) for a in state.legal_actions()]\n    action_prob[action] = (action, 1.0)\n    return dict(action_prob)",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mu_bar_t = state.distribution_average()\n    x_t = state.x\n    q = state.cross_q\n    n_actions_per_side = state.n_actions_per_side\n    lin_action = (q + state.eta_t()) * (mu_bar_t - x_t)\n    action = n_actions_per_side + min(n_actions_per_side, max(round(lin_action), -n_actions_per_side))\n    action_prob = [(a, 0.0) for a in state.legal_actions()]\n    action_prob[action] = (action, 1.0)\n    return dict(action_prob)",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mu_bar_t = state.distribution_average()\n    x_t = state.x\n    q = state.cross_q\n    n_actions_per_side = state.n_actions_per_side\n    lin_action = (q + state.eta_t()) * (mu_bar_t - x_t)\n    action = n_actions_per_side + min(n_actions_per_side, max(round(lin_action), -n_actions_per_side))\n    action_prob = [(a, 0.0) for a in state.legal_actions()]\n    action_prob[action] = (action, 1.0)\n    return dict(action_prob)",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mu_bar_t = state.distribution_average()\n    x_t = state.x\n    q = state.cross_q\n    n_actions_per_side = state.n_actions_per_side\n    lin_action = (q + state.eta_t()) * (mu_bar_t - x_t)\n    action = n_actions_per_side + min(n_actions_per_side, max(round(lin_action), -n_actions_per_side))\n    action_prob = [(a, 0.0) for a in state.legal_actions()]\n    action_prob[action] = (action, 1.0)\n    return dict(action_prob)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(argv: Sequence[str]) -> None:\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    mfg_game = pyspiel.load_game(FLAGS.game, {'dt': FLAGS.dt, 'size': FLAGS.size, 'horizon': FLAGS.horizon, 'n_actions_per_side': FLAGS.n_actions_per_side, 'volatility': FLAGS.volatility})\n    uniform_policy = policy.UniformRandomPolicy(mfg_game)\n    nash_conv_fp = nash_conv.NashConv(mfg_game, uniform_policy)\n    print('Uniform Policy Nashconv:', nash_conv_fp.nash_conv())\n    theoretical_control = LinearPolicy(mfg_game, list(range(mfg_game.num_players())))\n    theoretical_distribution = distribution.DistributionPolicy(mfg_game, theoretical_control)\n    discretized_optimal_value = policy_value.PolicyValue(mfg_game, theoretical_distribution, theoretical_control).eval_state(mfg_game.new_initial_state())\n    th_expl = nash_conv.NashConv(mfg_game, theoretical_control).nash_conv()\n    print('Theoretical policy NashConv : {}'.format(th_expl))\n    print('Theoretical policy Value : {}'.format(discretized_optimal_value))\n    fp = fictitious_play.FictitiousPlay(mfg_game)\n    md = mirror_descent.MirrorDescent(mfg_game)\n    for j in range(1000):\n        print('\\n\\nIteration', j, '\\n')\n        fp.iteration()\n        fp_policy = fp.get_policy()\n        nash_conv_fp = nash_conv.NashConv(mfg_game, fp_policy)\n        print('Nashconv of the current FP policy', nash_conv_fp.nash_conv())\n        fp_current_distribution = distribution.DistributionPolicy(mfg_game, fp.get_policy())\n        fp_l1_dist = get_l1_distribution_dist(fp_current_distribution, theoretical_distribution)\n        print('L1 distance between FP and theoretical policy : {}'.format(fp_l1_dist))\n        md.iteration()\n        md_policy = md.get_policy()\n        nash_conv_md = nash_conv.NashConv(mfg_game, md_policy)\n        print('')\n        print('Nashconv of the current MD policy', nash_conv_md.nash_conv())\n        md_current_distribution = md._distribution\n        md_l1_dist = get_l1_distribution_dist(md_current_distribution, theoretical_distribution)\n        print('L1 distance between OMD and theoretical policy : {}'.format(md_l1_dist))",
        "mutated": [
            "def main(argv: Sequence[str]) -> None:\n    if False:\n        i = 10\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    mfg_game = pyspiel.load_game(FLAGS.game, {'dt': FLAGS.dt, 'size': FLAGS.size, 'horizon': FLAGS.horizon, 'n_actions_per_side': FLAGS.n_actions_per_side, 'volatility': FLAGS.volatility})\n    uniform_policy = policy.UniformRandomPolicy(mfg_game)\n    nash_conv_fp = nash_conv.NashConv(mfg_game, uniform_policy)\n    print('Uniform Policy Nashconv:', nash_conv_fp.nash_conv())\n    theoretical_control = LinearPolicy(mfg_game, list(range(mfg_game.num_players())))\n    theoretical_distribution = distribution.DistributionPolicy(mfg_game, theoretical_control)\n    discretized_optimal_value = policy_value.PolicyValue(mfg_game, theoretical_distribution, theoretical_control).eval_state(mfg_game.new_initial_state())\n    th_expl = nash_conv.NashConv(mfg_game, theoretical_control).nash_conv()\n    print('Theoretical policy NashConv : {}'.format(th_expl))\n    print('Theoretical policy Value : {}'.format(discretized_optimal_value))\n    fp = fictitious_play.FictitiousPlay(mfg_game)\n    md = mirror_descent.MirrorDescent(mfg_game)\n    for j in range(1000):\n        print('\\n\\nIteration', j, '\\n')\n        fp.iteration()\n        fp_policy = fp.get_policy()\n        nash_conv_fp = nash_conv.NashConv(mfg_game, fp_policy)\n        print('Nashconv of the current FP policy', nash_conv_fp.nash_conv())\n        fp_current_distribution = distribution.DistributionPolicy(mfg_game, fp.get_policy())\n        fp_l1_dist = get_l1_distribution_dist(fp_current_distribution, theoretical_distribution)\n        print('L1 distance between FP and theoretical policy : {}'.format(fp_l1_dist))\n        md.iteration()\n        md_policy = md.get_policy()\n        nash_conv_md = nash_conv.NashConv(mfg_game, md_policy)\n        print('')\n        print('Nashconv of the current MD policy', nash_conv_md.nash_conv())\n        md_current_distribution = md._distribution\n        md_l1_dist = get_l1_distribution_dist(md_current_distribution, theoretical_distribution)\n        print('L1 distance between OMD and theoretical policy : {}'.format(md_l1_dist))",
            "def main(argv: Sequence[str]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    mfg_game = pyspiel.load_game(FLAGS.game, {'dt': FLAGS.dt, 'size': FLAGS.size, 'horizon': FLAGS.horizon, 'n_actions_per_side': FLAGS.n_actions_per_side, 'volatility': FLAGS.volatility})\n    uniform_policy = policy.UniformRandomPolicy(mfg_game)\n    nash_conv_fp = nash_conv.NashConv(mfg_game, uniform_policy)\n    print('Uniform Policy Nashconv:', nash_conv_fp.nash_conv())\n    theoretical_control = LinearPolicy(mfg_game, list(range(mfg_game.num_players())))\n    theoretical_distribution = distribution.DistributionPolicy(mfg_game, theoretical_control)\n    discretized_optimal_value = policy_value.PolicyValue(mfg_game, theoretical_distribution, theoretical_control).eval_state(mfg_game.new_initial_state())\n    th_expl = nash_conv.NashConv(mfg_game, theoretical_control).nash_conv()\n    print('Theoretical policy NashConv : {}'.format(th_expl))\n    print('Theoretical policy Value : {}'.format(discretized_optimal_value))\n    fp = fictitious_play.FictitiousPlay(mfg_game)\n    md = mirror_descent.MirrorDescent(mfg_game)\n    for j in range(1000):\n        print('\\n\\nIteration', j, '\\n')\n        fp.iteration()\n        fp_policy = fp.get_policy()\n        nash_conv_fp = nash_conv.NashConv(mfg_game, fp_policy)\n        print('Nashconv of the current FP policy', nash_conv_fp.nash_conv())\n        fp_current_distribution = distribution.DistributionPolicy(mfg_game, fp.get_policy())\n        fp_l1_dist = get_l1_distribution_dist(fp_current_distribution, theoretical_distribution)\n        print('L1 distance between FP and theoretical policy : {}'.format(fp_l1_dist))\n        md.iteration()\n        md_policy = md.get_policy()\n        nash_conv_md = nash_conv.NashConv(mfg_game, md_policy)\n        print('')\n        print('Nashconv of the current MD policy', nash_conv_md.nash_conv())\n        md_current_distribution = md._distribution\n        md_l1_dist = get_l1_distribution_dist(md_current_distribution, theoretical_distribution)\n        print('L1 distance between OMD and theoretical policy : {}'.format(md_l1_dist))",
            "def main(argv: Sequence[str]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    mfg_game = pyspiel.load_game(FLAGS.game, {'dt': FLAGS.dt, 'size': FLAGS.size, 'horizon': FLAGS.horizon, 'n_actions_per_side': FLAGS.n_actions_per_side, 'volatility': FLAGS.volatility})\n    uniform_policy = policy.UniformRandomPolicy(mfg_game)\n    nash_conv_fp = nash_conv.NashConv(mfg_game, uniform_policy)\n    print('Uniform Policy Nashconv:', nash_conv_fp.nash_conv())\n    theoretical_control = LinearPolicy(mfg_game, list(range(mfg_game.num_players())))\n    theoretical_distribution = distribution.DistributionPolicy(mfg_game, theoretical_control)\n    discretized_optimal_value = policy_value.PolicyValue(mfg_game, theoretical_distribution, theoretical_control).eval_state(mfg_game.new_initial_state())\n    th_expl = nash_conv.NashConv(mfg_game, theoretical_control).nash_conv()\n    print('Theoretical policy NashConv : {}'.format(th_expl))\n    print('Theoretical policy Value : {}'.format(discretized_optimal_value))\n    fp = fictitious_play.FictitiousPlay(mfg_game)\n    md = mirror_descent.MirrorDescent(mfg_game)\n    for j in range(1000):\n        print('\\n\\nIteration', j, '\\n')\n        fp.iteration()\n        fp_policy = fp.get_policy()\n        nash_conv_fp = nash_conv.NashConv(mfg_game, fp_policy)\n        print('Nashconv of the current FP policy', nash_conv_fp.nash_conv())\n        fp_current_distribution = distribution.DistributionPolicy(mfg_game, fp.get_policy())\n        fp_l1_dist = get_l1_distribution_dist(fp_current_distribution, theoretical_distribution)\n        print('L1 distance between FP and theoretical policy : {}'.format(fp_l1_dist))\n        md.iteration()\n        md_policy = md.get_policy()\n        nash_conv_md = nash_conv.NashConv(mfg_game, md_policy)\n        print('')\n        print('Nashconv of the current MD policy', nash_conv_md.nash_conv())\n        md_current_distribution = md._distribution\n        md_l1_dist = get_l1_distribution_dist(md_current_distribution, theoretical_distribution)\n        print('L1 distance between OMD and theoretical policy : {}'.format(md_l1_dist))",
            "def main(argv: Sequence[str]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    mfg_game = pyspiel.load_game(FLAGS.game, {'dt': FLAGS.dt, 'size': FLAGS.size, 'horizon': FLAGS.horizon, 'n_actions_per_side': FLAGS.n_actions_per_side, 'volatility': FLAGS.volatility})\n    uniform_policy = policy.UniformRandomPolicy(mfg_game)\n    nash_conv_fp = nash_conv.NashConv(mfg_game, uniform_policy)\n    print('Uniform Policy Nashconv:', nash_conv_fp.nash_conv())\n    theoretical_control = LinearPolicy(mfg_game, list(range(mfg_game.num_players())))\n    theoretical_distribution = distribution.DistributionPolicy(mfg_game, theoretical_control)\n    discretized_optimal_value = policy_value.PolicyValue(mfg_game, theoretical_distribution, theoretical_control).eval_state(mfg_game.new_initial_state())\n    th_expl = nash_conv.NashConv(mfg_game, theoretical_control).nash_conv()\n    print('Theoretical policy NashConv : {}'.format(th_expl))\n    print('Theoretical policy Value : {}'.format(discretized_optimal_value))\n    fp = fictitious_play.FictitiousPlay(mfg_game)\n    md = mirror_descent.MirrorDescent(mfg_game)\n    for j in range(1000):\n        print('\\n\\nIteration', j, '\\n')\n        fp.iteration()\n        fp_policy = fp.get_policy()\n        nash_conv_fp = nash_conv.NashConv(mfg_game, fp_policy)\n        print('Nashconv of the current FP policy', nash_conv_fp.nash_conv())\n        fp_current_distribution = distribution.DistributionPolicy(mfg_game, fp.get_policy())\n        fp_l1_dist = get_l1_distribution_dist(fp_current_distribution, theoretical_distribution)\n        print('L1 distance between FP and theoretical policy : {}'.format(fp_l1_dist))\n        md.iteration()\n        md_policy = md.get_policy()\n        nash_conv_md = nash_conv.NashConv(mfg_game, md_policy)\n        print('')\n        print('Nashconv of the current MD policy', nash_conv_md.nash_conv())\n        md_current_distribution = md._distribution\n        md_l1_dist = get_l1_distribution_dist(md_current_distribution, theoretical_distribution)\n        print('L1 distance between OMD and theoretical policy : {}'.format(md_l1_dist))",
            "def main(argv: Sequence[str]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    mfg_game = pyspiel.load_game(FLAGS.game, {'dt': FLAGS.dt, 'size': FLAGS.size, 'horizon': FLAGS.horizon, 'n_actions_per_side': FLAGS.n_actions_per_side, 'volatility': FLAGS.volatility})\n    uniform_policy = policy.UniformRandomPolicy(mfg_game)\n    nash_conv_fp = nash_conv.NashConv(mfg_game, uniform_policy)\n    print('Uniform Policy Nashconv:', nash_conv_fp.nash_conv())\n    theoretical_control = LinearPolicy(mfg_game, list(range(mfg_game.num_players())))\n    theoretical_distribution = distribution.DistributionPolicy(mfg_game, theoretical_control)\n    discretized_optimal_value = policy_value.PolicyValue(mfg_game, theoretical_distribution, theoretical_control).eval_state(mfg_game.new_initial_state())\n    th_expl = nash_conv.NashConv(mfg_game, theoretical_control).nash_conv()\n    print('Theoretical policy NashConv : {}'.format(th_expl))\n    print('Theoretical policy Value : {}'.format(discretized_optimal_value))\n    fp = fictitious_play.FictitiousPlay(mfg_game)\n    md = mirror_descent.MirrorDescent(mfg_game)\n    for j in range(1000):\n        print('\\n\\nIteration', j, '\\n')\n        fp.iteration()\n        fp_policy = fp.get_policy()\n        nash_conv_fp = nash_conv.NashConv(mfg_game, fp_policy)\n        print('Nashconv of the current FP policy', nash_conv_fp.nash_conv())\n        fp_current_distribution = distribution.DistributionPolicy(mfg_game, fp.get_policy())\n        fp_l1_dist = get_l1_distribution_dist(fp_current_distribution, theoretical_distribution)\n        print('L1 distance between FP and theoretical policy : {}'.format(fp_l1_dist))\n        md.iteration()\n        md_policy = md.get_policy()\n        nash_conv_md = nash_conv.NashConv(mfg_game, md_policy)\n        print('')\n        print('Nashconv of the current MD policy', nash_conv_md.nash_conv())\n        md_current_distribution = md._distribution\n        md_l1_dist = get_l1_distribution_dist(md_current_distribution, theoretical_distribution)\n        print('L1 distance between OMD and theoretical policy : {}'.format(md_l1_dist))"
        ]
    }
]