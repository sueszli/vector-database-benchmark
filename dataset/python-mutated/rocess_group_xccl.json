[
    {
        "func_name": "init_process_group",
        "original": "def init_process_group(strategy=None):\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6173, is_master, nranks)\n    pg_group = core.ProcessGroupCustom.create(store, paddle.distributed.ParallelEnv().device_type, rank, nranks)\n    return pg_group",
        "mutated": [
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6173, is_master, nranks)\n    pg_group = core.ProcessGroupCustom.create(store, paddle.distributed.ParallelEnv().device_type, rank, nranks)\n    return pg_group",
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6173, is_master, nranks)\n    pg_group = core.ProcessGroupCustom.create(store, paddle.distributed.ParallelEnv().device_type, rank, nranks)\n    return pg_group",
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6173, is_master, nranks)\n    pg_group = core.ProcessGroupCustom.create(store, paddle.distributed.ParallelEnv().device_type, rank, nranks)\n    return pg_group",
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6173, is_master, nranks)\n    pg_group = core.ProcessGroupCustom.create(store, paddle.distributed.ParallelEnv().device_type, rank, nranks)\n    return pg_group",
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = paddle.distributed.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    store = paddle.base.core.TCPStore('127.0.0.1', 6173, is_master, nranks)\n    pg_group = core.ProcessGroupCustom.create(store, paddle.distributed.ParallelEnv().device_type, rank, nranks)\n    return pg_group"
        ]
    },
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()"
        ]
    },
    {
        "func_name": "config",
        "original": "def config(self):\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
        "mutated": [
            "def config(self):\n    if False:\n        i = 10\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)"
        ]
    },
    {
        "func_name": "test_create_process_group_xccl",
        "original": "def test_create_process_group_xccl(self):\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('custom_cpu:%d' % device_id)\n    pg = init_process_group()\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = pg.broadcast(tensor_x, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    else:\n        task = pg.broadcast(tensor_y, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    print('test broadcast api ok')\n    if pg.rank() == 0:\n        task = pg.barrier(device_id)\n        task.wait()\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    return\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_out, tensor_x, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_gather(tensor_out, tensor_y, sync_op=True)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        task = pg.alltoall(tensor_y, tensor_out2)\n        task.wait()\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    print('test scatter api ok\\n')",
        "mutated": [
            "def test_create_process_group_xccl(self):\n    if False:\n        i = 10\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('custom_cpu:%d' % device_id)\n    pg = init_process_group()\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = pg.broadcast(tensor_x, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    else:\n        task = pg.broadcast(tensor_y, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    print('test broadcast api ok')\n    if pg.rank() == 0:\n        task = pg.barrier(device_id)\n        task.wait()\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    return\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_out, tensor_x, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_gather(tensor_out, tensor_y, sync_op=True)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        task = pg.alltoall(tensor_y, tensor_out2)\n        task.wait()\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    print('test scatter api ok\\n')",
            "def test_create_process_group_xccl(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('custom_cpu:%d' % device_id)\n    pg = init_process_group()\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = pg.broadcast(tensor_x, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    else:\n        task = pg.broadcast(tensor_y, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    print('test broadcast api ok')\n    if pg.rank() == 0:\n        task = pg.barrier(device_id)\n        task.wait()\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    return\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_out, tensor_x, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_gather(tensor_out, tensor_y, sync_op=True)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        task = pg.alltoall(tensor_y, tensor_out2)\n        task.wait()\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    print('test scatter api ok\\n')",
            "def test_create_process_group_xccl(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('custom_cpu:%d' % device_id)\n    pg = init_process_group()\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = pg.broadcast(tensor_x, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    else:\n        task = pg.broadcast(tensor_y, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    print('test broadcast api ok')\n    if pg.rank() == 0:\n        task = pg.barrier(device_id)\n        task.wait()\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    return\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_out, tensor_x, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_gather(tensor_out, tensor_y, sync_op=True)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        task = pg.alltoall(tensor_y, tensor_out2)\n        task.wait()\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    print('test scatter api ok\\n')",
            "def test_create_process_group_xccl(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('custom_cpu:%d' % device_id)\n    pg = init_process_group()\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = pg.broadcast(tensor_x, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    else:\n        task = pg.broadcast(tensor_y, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    print('test broadcast api ok')\n    if pg.rank() == 0:\n        task = pg.barrier(device_id)\n        task.wait()\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    return\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_out, tensor_x, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_gather(tensor_out, tensor_y, sync_op=True)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        task = pg.alltoall(tensor_y, tensor_out2)\n        task.wait()\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    print('test scatter api ok\\n')",
            "def test_create_process_group_xccl(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('custom_cpu:%d' % device_id)\n    pg = init_process_group()\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.SUM, sync_op=True)\n        task.wait()\n    print('test allreduce sum api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = pg.all_reduce(tensor_x, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_reduce(tensor_y, core.ReduceOp.MAX, sync_op=True)\n        task.wait()\n    print('test allreduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = pg.broadcast(tensor_x, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    else:\n        task = pg.broadcast(tensor_y, 0, sync_op=True)\n        task.wait()\n        assert task.is_completed()\n    print('test broadcast api ok')\n    if pg.rank() == 0:\n        task = pg.barrier(device_id)\n        task.wait()\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    return\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_out, tensor_x, sync_op=True)\n        task.wait()\n    else:\n        task = pg.all_gather(tensor_out, tensor_y, sync_op=True)\n        task.wait()\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    print('test allgather api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        task = pg.alltoall(tensor_y, tensor_out2)\n        task.wait()\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = pg.reduce(tensor_x, 0)\n        task.wait()\n    else:\n        task = pg.reduce(tensor_y, 0)\n        task.wait()\n    print('test reduce sum api ok\\n')\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    else:\n        task = pg.scatter(tensor_x, tensor_y, 0)\n        task.wait()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    print('test scatter api ok\\n')"
        ]
    }
]