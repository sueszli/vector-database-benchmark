[
    {
        "func_name": "p_boolean_document",
        "original": "def p_boolean_document(corpus, segmented_topics):\n    \"\"\"Perform the boolean document probability estimation. Boolean document estimates the probability of a single word\n    as the number of documents in which the word occurs divided by the total number of documents.\n\n    Parameters\n    ----------\n    corpus : iterable of list of (int, int)\n        The corpus of documents.\n    segmented_topics: list of (int, int).\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\n\n    Returns\n    -------\n    :class:`~gensim.topic_coherence.text_analysis.CorpusAccumulator`\n        Word occurrence accumulator instance that can be used to lookup token frequencies and co-occurrence frequencies.\n\n    Examples\n    ---------\n    .. sourcecode:: pycon\n\n        >>> from gensim.topic_coherence import probability_estimation\n        >>> from gensim.corpora.hashdictionary import HashDictionary\n        >>>\n        >>>\n        >>> texts = [\n        ...     ['human', 'interface', 'computer'],\n        ...     ['eps', 'user', 'interface', 'system'],\n        ...     ['system', 'human', 'system', 'eps'],\n        ...     ['user', 'response', 'time'],\n        ...     ['trees'],\n        ...     ['graph', 'trees']\n        ... ]\n        >>> dictionary = HashDictionary(texts)\n        >>> w2id = dictionary.token2id\n        >>>\n        >>> # create segmented_topics\n        >>> segmented_topics = [\n        ...     [\n        ...         (w2id['system'], w2id['graph']),\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['computer'], w2id['system'])\n        ...     ],\n        ...     [\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['user'], w2id['graph']),\n        ...         (w2id['user'], w2id['computer'])]\n        ... ]\n        >>> # create corpus\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\n        >>>\n        >>> result = probability_estimation.p_boolean_document(corpus, segmented_topics)\n        >>> result.index_to_dict()\n        {10608: set([0]), 12736: set([1, 3]), 18451: set([5]), 5798: set([1, 2])}\n\n    \"\"\"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    return CorpusAccumulator(top_ids).accumulate(corpus)",
        "mutated": [
            "def p_boolean_document(corpus, segmented_topics):\n    if False:\n        i = 10\n    \"Perform the boolean document probability estimation. Boolean document estimates the probability of a single word\\n    as the number of documents in which the word occurs divided by the total number of documents.\\n\\n    Parameters\\n    ----------\\n    corpus : iterable of list of (int, int)\\n        The corpus of documents.\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.CorpusAccumulator`\\n        Word occurrence accumulator instance that can be used to lookup token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>>\\n        >>> result = probability_estimation.p_boolean_document(corpus, segmented_topics)\\n        >>> result.index_to_dict()\\n        {10608: set([0]), 12736: set([1, 3]), 18451: set([5]), 5798: set([1, 2])}\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    return CorpusAccumulator(top_ids).accumulate(corpus)",
            "def p_boolean_document(corpus, segmented_topics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Perform the boolean document probability estimation. Boolean document estimates the probability of a single word\\n    as the number of documents in which the word occurs divided by the total number of documents.\\n\\n    Parameters\\n    ----------\\n    corpus : iterable of list of (int, int)\\n        The corpus of documents.\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.CorpusAccumulator`\\n        Word occurrence accumulator instance that can be used to lookup token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>>\\n        >>> result = probability_estimation.p_boolean_document(corpus, segmented_topics)\\n        >>> result.index_to_dict()\\n        {10608: set([0]), 12736: set([1, 3]), 18451: set([5]), 5798: set([1, 2])}\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    return CorpusAccumulator(top_ids).accumulate(corpus)",
            "def p_boolean_document(corpus, segmented_topics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Perform the boolean document probability estimation. Boolean document estimates the probability of a single word\\n    as the number of documents in which the word occurs divided by the total number of documents.\\n\\n    Parameters\\n    ----------\\n    corpus : iterable of list of (int, int)\\n        The corpus of documents.\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.CorpusAccumulator`\\n        Word occurrence accumulator instance that can be used to lookup token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>>\\n        >>> result = probability_estimation.p_boolean_document(corpus, segmented_topics)\\n        >>> result.index_to_dict()\\n        {10608: set([0]), 12736: set([1, 3]), 18451: set([5]), 5798: set([1, 2])}\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    return CorpusAccumulator(top_ids).accumulate(corpus)",
            "def p_boolean_document(corpus, segmented_topics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Perform the boolean document probability estimation. Boolean document estimates the probability of a single word\\n    as the number of documents in which the word occurs divided by the total number of documents.\\n\\n    Parameters\\n    ----------\\n    corpus : iterable of list of (int, int)\\n        The corpus of documents.\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.CorpusAccumulator`\\n        Word occurrence accumulator instance that can be used to lookup token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>>\\n        >>> result = probability_estimation.p_boolean_document(corpus, segmented_topics)\\n        >>> result.index_to_dict()\\n        {10608: set([0]), 12736: set([1, 3]), 18451: set([5]), 5798: set([1, 2])}\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    return CorpusAccumulator(top_ids).accumulate(corpus)",
            "def p_boolean_document(corpus, segmented_topics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Perform the boolean document probability estimation. Boolean document estimates the probability of a single word\\n    as the number of documents in which the word occurs divided by the total number of documents.\\n\\n    Parameters\\n    ----------\\n    corpus : iterable of list of (int, int)\\n        The corpus of documents.\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.CorpusAccumulator`\\n        Word occurrence accumulator instance that can be used to lookup token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>>\\n        >>> result = probability_estimation.p_boolean_document(corpus, segmented_topics)\\n        >>> result.index_to_dict()\\n        {10608: set([0]), 12736: set([1, 3]), 18451: set([5]), 5798: set([1, 2])}\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    return CorpusAccumulator(top_ids).accumulate(corpus)"
        ]
    },
    {
        "func_name": "p_boolean_sliding_window",
        "original": "def p_boolean_sliding_window(texts, segmented_topics, dictionary, window_size, processes=1):\n    \"\"\"Perform the boolean sliding window probability estimation.\n\n    Parameters\n    ----------\n    texts : iterable of iterable of str\n        Input text\n    segmented_topics: list of (int, int)\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\n    dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n        Gensim dictionary mapping of the tokens and ids.\n    window_size : int\n        Size of the sliding window, 110 found out to be the ideal size for large corpora.\n    processes : int, optional\n        Number of process that will be used for\n        :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\n\n    Notes\n    -----\n    Boolean sliding window determines word counts using a sliding window. The window\n    moves over  the documents one word token per step. Each step defines a new virtual\n    document  by copying the window content. Boolean document is applied to these virtual\n    documents to compute word probabilities.\n\n    Returns\n    -------\n    :class:`~gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator`\n        if `processes` = 1 OR\n    :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\n        otherwise. This is word occurrence accumulator instance that can be used to lookup\n        token frequencies and co-occurrence frequencies.\n\n    Examples\n    ---------\n    .. sourcecode:: pycon\n\n        >>> from gensim.topic_coherence import probability_estimation\n        >>> from gensim.corpora.hashdictionary import HashDictionary\n        >>>\n        >>>\n        >>> texts = [\n        ...     ['human', 'interface', 'computer'],\n        ...     ['eps', 'user', 'interface', 'system'],\n        ...     ['system', 'human', 'system', 'eps'],\n        ...     ['user', 'response', 'time'],\n        ...     ['trees'],\n        ...     ['graph', 'trees']\n        ... ]\n        >>> dictionary = HashDictionary(texts)\n        >>> w2id = dictionary.token2id\n\n        >>>\n        >>> # create segmented_topics\n        >>> segmented_topics = [\n        ...     [\n        ...         (w2id['system'], w2id['graph']),\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['computer'], w2id['system'])\n        ...     ],\n        ...     [\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['user'], w2id['graph']),\n        ...         (w2id['user'], w2id['computer'])]\n        ... ]\n        >>> # create corpus\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\n        >>> accumulator = probability_estimation.p_boolean_sliding_window(texts, segmented_topics, dictionary, 2)\n        >>>\n        >>> (accumulator[w2id['computer']], accumulator[w2id['user']], accumulator[w2id['system']])\n        (1, 3, 4)\n\n    \"\"\"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    if processes <= 1:\n        accumulator = WordOccurrenceAccumulator(top_ids, dictionary)\n    else:\n        accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n    logger.info('using %s to estimate probabilities from sliding windows', accumulator)\n    return accumulator.accumulate(texts, window_size)",
        "mutated": [
            "def p_boolean_sliding_window(texts, segmented_topics, dictionary, window_size, processes=1):\n    if False:\n        i = 10\n    \"Perform the boolean sliding window probability estimation.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics: list of (int, int)\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n    dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int\\n        Size of the sliding window, 110 found out to be the ideal size for large corpora.\\n    processes : int, optional\\n        Number of process that will be used for\\n        :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n\\n    Notes\\n    -----\\n    Boolean sliding window determines word counts using a sliding window. The window\\n    moves over  the documents one word token per step. Each step defines a new virtual\\n    document  by copying the window content. Boolean document is applied to these virtual\\n    documents to compute word probabilities.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator`\\n        if `processes` = 1 OR\\n    :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n        otherwise. This is word occurrence accumulator instance that can be used to lookup\\n        token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> accumulator = probability_estimation.p_boolean_sliding_window(texts, segmented_topics, dictionary, 2)\\n        >>>\\n        >>> (accumulator[w2id['computer']], accumulator[w2id['user']], accumulator[w2id['system']])\\n        (1, 3, 4)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    if processes <= 1:\n        accumulator = WordOccurrenceAccumulator(top_ids, dictionary)\n    else:\n        accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n    logger.info('using %s to estimate probabilities from sliding windows', accumulator)\n    return accumulator.accumulate(texts, window_size)",
            "def p_boolean_sliding_window(texts, segmented_topics, dictionary, window_size, processes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Perform the boolean sliding window probability estimation.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics: list of (int, int)\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n    dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int\\n        Size of the sliding window, 110 found out to be the ideal size for large corpora.\\n    processes : int, optional\\n        Number of process that will be used for\\n        :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n\\n    Notes\\n    -----\\n    Boolean sliding window determines word counts using a sliding window. The window\\n    moves over  the documents one word token per step. Each step defines a new virtual\\n    document  by copying the window content. Boolean document is applied to these virtual\\n    documents to compute word probabilities.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator`\\n        if `processes` = 1 OR\\n    :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n        otherwise. This is word occurrence accumulator instance that can be used to lookup\\n        token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> accumulator = probability_estimation.p_boolean_sliding_window(texts, segmented_topics, dictionary, 2)\\n        >>>\\n        >>> (accumulator[w2id['computer']], accumulator[w2id['user']], accumulator[w2id['system']])\\n        (1, 3, 4)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    if processes <= 1:\n        accumulator = WordOccurrenceAccumulator(top_ids, dictionary)\n    else:\n        accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n    logger.info('using %s to estimate probabilities from sliding windows', accumulator)\n    return accumulator.accumulate(texts, window_size)",
            "def p_boolean_sliding_window(texts, segmented_topics, dictionary, window_size, processes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Perform the boolean sliding window probability estimation.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics: list of (int, int)\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n    dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int\\n        Size of the sliding window, 110 found out to be the ideal size for large corpora.\\n    processes : int, optional\\n        Number of process that will be used for\\n        :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n\\n    Notes\\n    -----\\n    Boolean sliding window determines word counts using a sliding window. The window\\n    moves over  the documents one word token per step. Each step defines a new virtual\\n    document  by copying the window content. Boolean document is applied to these virtual\\n    documents to compute word probabilities.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator`\\n        if `processes` = 1 OR\\n    :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n        otherwise. This is word occurrence accumulator instance that can be used to lookup\\n        token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> accumulator = probability_estimation.p_boolean_sliding_window(texts, segmented_topics, dictionary, 2)\\n        >>>\\n        >>> (accumulator[w2id['computer']], accumulator[w2id['user']], accumulator[w2id['system']])\\n        (1, 3, 4)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    if processes <= 1:\n        accumulator = WordOccurrenceAccumulator(top_ids, dictionary)\n    else:\n        accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n    logger.info('using %s to estimate probabilities from sliding windows', accumulator)\n    return accumulator.accumulate(texts, window_size)",
            "def p_boolean_sliding_window(texts, segmented_topics, dictionary, window_size, processes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Perform the boolean sliding window probability estimation.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics: list of (int, int)\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n    dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int\\n        Size of the sliding window, 110 found out to be the ideal size for large corpora.\\n    processes : int, optional\\n        Number of process that will be used for\\n        :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n\\n    Notes\\n    -----\\n    Boolean sliding window determines word counts using a sliding window. The window\\n    moves over  the documents one word token per step. Each step defines a new virtual\\n    document  by copying the window content. Boolean document is applied to these virtual\\n    documents to compute word probabilities.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator`\\n        if `processes` = 1 OR\\n    :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n        otherwise. This is word occurrence accumulator instance that can be used to lookup\\n        token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> accumulator = probability_estimation.p_boolean_sliding_window(texts, segmented_topics, dictionary, 2)\\n        >>>\\n        >>> (accumulator[w2id['computer']], accumulator[w2id['user']], accumulator[w2id['system']])\\n        (1, 3, 4)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    if processes <= 1:\n        accumulator = WordOccurrenceAccumulator(top_ids, dictionary)\n    else:\n        accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n    logger.info('using %s to estimate probabilities from sliding windows', accumulator)\n    return accumulator.accumulate(texts, window_size)",
            "def p_boolean_sliding_window(texts, segmented_topics, dictionary, window_size, processes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Perform the boolean sliding window probability estimation.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics: list of (int, int)\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n    dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int\\n        Size of the sliding window, 110 found out to be the ideal size for large corpora.\\n    processes : int, optional\\n        Number of process that will be used for\\n        :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n\\n    Notes\\n    -----\\n    Boolean sliding window determines word counts using a sliding window. The window\\n    moves over  the documents one word token per step. Each step defines a new virtual\\n    document  by copying the window content. Boolean document is applied to these virtual\\n    documents to compute word probabilities.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator`\\n        if `processes` = 1 OR\\n    :class:`~gensim.topic_coherence.text_analysis.ParallelWordOccurrenceAccumulator`\\n        otherwise. This is word occurrence accumulator instance that can be used to lookup\\n        token frequencies and co-occurrence frequencies.\\n\\n    Examples\\n    ---------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>>\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> accumulator = probability_estimation.p_boolean_sliding_window(texts, segmented_topics, dictionary, 2)\\n        >>>\\n        >>> (accumulator[w2id['computer']], accumulator[w2id['user']], accumulator[w2id['system']])\\n        (1, 3, 4)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    if processes <= 1:\n        accumulator = WordOccurrenceAccumulator(top_ids, dictionary)\n    else:\n        accumulator = ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n    logger.info('using %s to estimate probabilities from sliding windows', accumulator)\n    return accumulator.accumulate(texts, window_size)"
        ]
    },
    {
        "func_name": "p_word2vec",
        "original": "def p_word2vec(texts, segmented_topics, dictionary, window_size=None, processes=1, model=None):\n    \"\"\"Train word2vec model on `texts` if `model` is not None.\n\n    Parameters\n    ----------\n    texts : iterable of iterable of str\n        Input text\n    segmented_topics : iterable of iterable of str\n        Output from the segmentation of topics. Could be simply topics too.\n    dictionary : :class:`~gensim.corpora.dictionary`\n        Gensim dictionary mapping of the tokens and ids.\n    window_size : int, optional\n        Size of the sliding window.\n    processes : int, optional\n        Number of processes to use.\n    model : :class:`~gensim.models.word2vec.Word2Vec` or :class:`~gensim.models.keyedvectors.KeyedVectors`, optional\n        If None, a new Word2Vec model is trained on the given text corpus. Otherwise,\n        it should be a pre-trained Word2Vec context vectors.\n\n    Returns\n    -------\n    :class:`~gensim.topic_coherence.text_analysis.WordVectorsAccumulator`\n        Text accumulator with trained context vectors.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.topic_coherence import probability_estimation\n        >>> from gensim.corpora.hashdictionary import HashDictionary\n        >>> from gensim.models import word2vec\n        >>>\n        >>> texts = [\n        ...     ['human', 'interface', 'computer'],\n        ...     ['eps', 'user', 'interface', 'system'],\n        ...     ['system', 'human', 'system', 'eps'],\n        ...     ['user', 'response', 'time'],\n        ...     ['trees'],\n        ...     ['graph', 'trees']\n        ... ]\n        >>> dictionary = HashDictionary(texts)\n        >>> w2id = dictionary.token2id\n\n        >>>\n        >>> # create segmented_topics\n        >>> segmented_topics = [\n        ...     [\n        ...         (w2id['system'], w2id['graph']),\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['computer'], w2id['system'])\n        ...     ],\n        ...     [\n        ...         (w2id['computer'], w2id['graph']),\n        ...         (w2id['user'], w2id['graph']),\n        ...         (w2id['user'], w2id['computer'])]\n        ... ]\n        >>> # create corpus\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\n        >>> sentences = [\n        ...     ['human', 'interface', 'computer'],\n        ...     ['survey', 'user', 'computer', 'system', 'response', 'time']\n        ... ]\n        >>> model = word2vec.Word2Vec(sentences, vector_size=100, min_count=1)\n        >>> accumulator = probability_estimation.p_word2vec(texts, segmented_topics, dictionary, 2, 1, model)\n\n    \"\"\"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    accumulator = WordVectorsAccumulator(top_ids, dictionary, model, window=window_size, workers=processes)\n    return accumulator.accumulate(texts, window_size)",
        "mutated": [
            "def p_word2vec(texts, segmented_topics, dictionary, window_size=None, processes=1, model=None):\n    if False:\n        i = 10\n    \"Train word2vec model on `texts` if `model` is not None.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics : iterable of iterable of str\\n        Output from the segmentation of topics. Could be simply topics too.\\n    dictionary : :class:`~gensim.corpora.dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int, optional\\n        Size of the sliding window.\\n    processes : int, optional\\n        Number of processes to use.\\n    model : :class:`~gensim.models.word2vec.Word2Vec` or :class:`~gensim.models.keyedvectors.KeyedVectors`, optional\\n        If None, a new Word2Vec model is trained on the given text corpus. Otherwise,\\n        it should be a pre-trained Word2Vec context vectors.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordVectorsAccumulator`\\n        Text accumulator with trained context vectors.\\n\\n    Examples\\n    --------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>> from gensim.models import word2vec\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> sentences = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['survey', 'user', 'computer', 'system', 'response', 'time']\\n        ... ]\\n        >>> model = word2vec.Word2Vec(sentences, vector_size=100, min_count=1)\\n        >>> accumulator = probability_estimation.p_word2vec(texts, segmented_topics, dictionary, 2, 1, model)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    accumulator = WordVectorsAccumulator(top_ids, dictionary, model, window=window_size, workers=processes)\n    return accumulator.accumulate(texts, window_size)",
            "def p_word2vec(texts, segmented_topics, dictionary, window_size=None, processes=1, model=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Train word2vec model on `texts` if `model` is not None.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics : iterable of iterable of str\\n        Output from the segmentation of topics. Could be simply topics too.\\n    dictionary : :class:`~gensim.corpora.dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int, optional\\n        Size of the sliding window.\\n    processes : int, optional\\n        Number of processes to use.\\n    model : :class:`~gensim.models.word2vec.Word2Vec` or :class:`~gensim.models.keyedvectors.KeyedVectors`, optional\\n        If None, a new Word2Vec model is trained on the given text corpus. Otherwise,\\n        it should be a pre-trained Word2Vec context vectors.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordVectorsAccumulator`\\n        Text accumulator with trained context vectors.\\n\\n    Examples\\n    --------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>> from gensim.models import word2vec\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> sentences = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['survey', 'user', 'computer', 'system', 'response', 'time']\\n        ... ]\\n        >>> model = word2vec.Word2Vec(sentences, vector_size=100, min_count=1)\\n        >>> accumulator = probability_estimation.p_word2vec(texts, segmented_topics, dictionary, 2, 1, model)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    accumulator = WordVectorsAccumulator(top_ids, dictionary, model, window=window_size, workers=processes)\n    return accumulator.accumulate(texts, window_size)",
            "def p_word2vec(texts, segmented_topics, dictionary, window_size=None, processes=1, model=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Train word2vec model on `texts` if `model` is not None.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics : iterable of iterable of str\\n        Output from the segmentation of topics. Could be simply topics too.\\n    dictionary : :class:`~gensim.corpora.dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int, optional\\n        Size of the sliding window.\\n    processes : int, optional\\n        Number of processes to use.\\n    model : :class:`~gensim.models.word2vec.Word2Vec` or :class:`~gensim.models.keyedvectors.KeyedVectors`, optional\\n        If None, a new Word2Vec model is trained on the given text corpus. Otherwise,\\n        it should be a pre-trained Word2Vec context vectors.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordVectorsAccumulator`\\n        Text accumulator with trained context vectors.\\n\\n    Examples\\n    --------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>> from gensim.models import word2vec\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> sentences = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['survey', 'user', 'computer', 'system', 'response', 'time']\\n        ... ]\\n        >>> model = word2vec.Word2Vec(sentences, vector_size=100, min_count=1)\\n        >>> accumulator = probability_estimation.p_word2vec(texts, segmented_topics, dictionary, 2, 1, model)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    accumulator = WordVectorsAccumulator(top_ids, dictionary, model, window=window_size, workers=processes)\n    return accumulator.accumulate(texts, window_size)",
            "def p_word2vec(texts, segmented_topics, dictionary, window_size=None, processes=1, model=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Train word2vec model on `texts` if `model` is not None.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics : iterable of iterable of str\\n        Output from the segmentation of topics. Could be simply topics too.\\n    dictionary : :class:`~gensim.corpora.dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int, optional\\n        Size of the sliding window.\\n    processes : int, optional\\n        Number of processes to use.\\n    model : :class:`~gensim.models.word2vec.Word2Vec` or :class:`~gensim.models.keyedvectors.KeyedVectors`, optional\\n        If None, a new Word2Vec model is trained on the given text corpus. Otherwise,\\n        it should be a pre-trained Word2Vec context vectors.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordVectorsAccumulator`\\n        Text accumulator with trained context vectors.\\n\\n    Examples\\n    --------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>> from gensim.models import word2vec\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> sentences = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['survey', 'user', 'computer', 'system', 'response', 'time']\\n        ... ]\\n        >>> model = word2vec.Word2Vec(sentences, vector_size=100, min_count=1)\\n        >>> accumulator = probability_estimation.p_word2vec(texts, segmented_topics, dictionary, 2, 1, model)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    accumulator = WordVectorsAccumulator(top_ids, dictionary, model, window=window_size, workers=processes)\n    return accumulator.accumulate(texts, window_size)",
            "def p_word2vec(texts, segmented_topics, dictionary, window_size=None, processes=1, model=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Train word2vec model on `texts` if `model` is not None.\\n\\n    Parameters\\n    ----------\\n    texts : iterable of iterable of str\\n        Input text\\n    segmented_topics : iterable of iterable of str\\n        Output from the segmentation of topics. Could be simply topics too.\\n    dictionary : :class:`~gensim.corpora.dictionary`\\n        Gensim dictionary mapping of the tokens and ids.\\n    window_size : int, optional\\n        Size of the sliding window.\\n    processes : int, optional\\n        Number of processes to use.\\n    model : :class:`~gensim.models.word2vec.Word2Vec` or :class:`~gensim.models.keyedvectors.KeyedVectors`, optional\\n        If None, a new Word2Vec model is trained on the given text corpus. Otherwise,\\n        it should be a pre-trained Word2Vec context vectors.\\n\\n    Returns\\n    -------\\n    :class:`~gensim.topic_coherence.text_analysis.WordVectorsAccumulator`\\n        Text accumulator with trained context vectors.\\n\\n    Examples\\n    --------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>> from gensim.corpora.hashdictionary import HashDictionary\\n        >>> from gensim.models import word2vec\\n        >>>\\n        >>> texts = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['eps', 'user', 'interface', 'system'],\\n        ...     ['system', 'human', 'system', 'eps'],\\n        ...     ['user', 'response', 'time'],\\n        ...     ['trees'],\\n        ...     ['graph', 'trees']\\n        ... ]\\n        >>> dictionary = HashDictionary(texts)\\n        >>> w2id = dictionary.token2id\\n\\n        >>>\\n        >>> # create segmented_topics\\n        >>> segmented_topics = [\\n        ...     [\\n        ...         (w2id['system'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['computer'], w2id['system'])\\n        ...     ],\\n        ...     [\\n        ...         (w2id['computer'], w2id['graph']),\\n        ...         (w2id['user'], w2id['graph']),\\n        ...         (w2id['user'], w2id['computer'])]\\n        ... ]\\n        >>> # create corpus\\n        >>> corpus = [dictionary.doc2bow(text) for text in texts]\\n        >>> sentences = [\\n        ...     ['human', 'interface', 'computer'],\\n        ...     ['survey', 'user', 'computer', 'system', 'response', 'time']\\n        ... ]\\n        >>> model = word2vec.Word2Vec(sentences, vector_size=100, min_count=1)\\n        >>> accumulator = probability_estimation.p_word2vec(texts, segmented_topics, dictionary, 2, 1, model)\\n\\n    \"\n    top_ids = unique_ids_from_segments(segmented_topics)\n    accumulator = WordVectorsAccumulator(top_ids, dictionary, model, window=window_size, workers=processes)\n    return accumulator.accumulate(texts, window_size)"
        ]
    },
    {
        "func_name": "unique_ids_from_segments",
        "original": "def unique_ids_from_segments(segmented_topics):\n    \"\"\"Return the set of all unique ids in a list of segmented topics.\n\n    Parameters\n    ----------\n    segmented_topics: list of (int, int).\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\n\n    Returns\n    -------\n    set\n        Set of unique ids across all topic segments.\n\n    Example\n    -------\n    .. sourcecode:: pycon\n\n        >>> from gensim.topic_coherence import probability_estimation\n        >>>\n        >>> segmentation = [[(1, 2)]]\n        >>> probability_estimation.unique_ids_from_segments(segmentation)\n        set([1, 2])\n\n    \"\"\"\n    unique_ids = set()\n    for s_i in segmented_topics:\n        for word_id in itertools.chain.from_iterable(s_i):\n            if hasattr(word_id, '__iter__'):\n                unique_ids.update(word_id)\n            else:\n                unique_ids.add(word_id)\n    return unique_ids",
        "mutated": [
            "def unique_ids_from_segments(segmented_topics):\n    if False:\n        i = 10\n    'Return the set of all unique ids in a list of segmented topics.\\n\\n    Parameters\\n    ----------\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    set\\n        Set of unique ids across all topic segments.\\n\\n    Example\\n    -------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>>\\n        >>> segmentation = [[(1, 2)]]\\n        >>> probability_estimation.unique_ids_from_segments(segmentation)\\n        set([1, 2])\\n\\n    '\n    unique_ids = set()\n    for s_i in segmented_topics:\n        for word_id in itertools.chain.from_iterable(s_i):\n            if hasattr(word_id, '__iter__'):\n                unique_ids.update(word_id)\n            else:\n                unique_ids.add(word_id)\n    return unique_ids",
            "def unique_ids_from_segments(segmented_topics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return the set of all unique ids in a list of segmented topics.\\n\\n    Parameters\\n    ----------\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    set\\n        Set of unique ids across all topic segments.\\n\\n    Example\\n    -------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>>\\n        >>> segmentation = [[(1, 2)]]\\n        >>> probability_estimation.unique_ids_from_segments(segmentation)\\n        set([1, 2])\\n\\n    '\n    unique_ids = set()\n    for s_i in segmented_topics:\n        for word_id in itertools.chain.from_iterable(s_i):\n            if hasattr(word_id, '__iter__'):\n                unique_ids.update(word_id)\n            else:\n                unique_ids.add(word_id)\n    return unique_ids",
            "def unique_ids_from_segments(segmented_topics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return the set of all unique ids in a list of segmented topics.\\n\\n    Parameters\\n    ----------\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    set\\n        Set of unique ids across all topic segments.\\n\\n    Example\\n    -------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>>\\n        >>> segmentation = [[(1, 2)]]\\n        >>> probability_estimation.unique_ids_from_segments(segmentation)\\n        set([1, 2])\\n\\n    '\n    unique_ids = set()\n    for s_i in segmented_topics:\n        for word_id in itertools.chain.from_iterable(s_i):\n            if hasattr(word_id, '__iter__'):\n                unique_ids.update(word_id)\n            else:\n                unique_ids.add(word_id)\n    return unique_ids",
            "def unique_ids_from_segments(segmented_topics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return the set of all unique ids in a list of segmented topics.\\n\\n    Parameters\\n    ----------\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    set\\n        Set of unique ids across all topic segments.\\n\\n    Example\\n    -------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>>\\n        >>> segmentation = [[(1, 2)]]\\n        >>> probability_estimation.unique_ids_from_segments(segmentation)\\n        set([1, 2])\\n\\n    '\n    unique_ids = set()\n    for s_i in segmented_topics:\n        for word_id in itertools.chain.from_iterable(s_i):\n            if hasattr(word_id, '__iter__'):\n                unique_ids.update(word_id)\n            else:\n                unique_ids.add(word_id)\n    return unique_ids",
            "def unique_ids_from_segments(segmented_topics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return the set of all unique ids in a list of segmented topics.\\n\\n    Parameters\\n    ----------\\n    segmented_topics: list of (int, int).\\n        Each tuple (word_id_set1, word_id_set2) is either a single integer, or a `numpy.ndarray` of integers.\\n\\n    Returns\\n    -------\\n    set\\n        Set of unique ids across all topic segments.\\n\\n    Example\\n    -------\\n    .. sourcecode:: pycon\\n\\n        >>> from gensim.topic_coherence import probability_estimation\\n        >>>\\n        >>> segmentation = [[(1, 2)]]\\n        >>> probability_estimation.unique_ids_from_segments(segmentation)\\n        set([1, 2])\\n\\n    '\n    unique_ids = set()\n    for s_i in segmented_topics:\n        for word_id in itertools.chain.from_iterable(s_i):\n            if hasattr(word_id, '__iter__'):\n                unique_ids.update(word_id)\n            else:\n                unique_ids.add(word_id)\n    return unique_ids"
        ]
    }
]