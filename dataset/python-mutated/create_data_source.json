[
    {
        "func_name": "main",
        "original": "def main(infile: IO, outfile: IO) -> None:\n    \"\"\"\n    Main method for creating a Python data source instance.\n\n    This process is invoked from the `UserDefinedPythonDataSourceRunner.runInPython` method\n    in JVM. This process is responsible for creating a `DataSource` object and send the\n    information needed back to the JVM.\n\n    The JVM sends the following information to this process:\n    - a `DataSource` class representing the data source to be created.\n    - a provider name in string.\n    - a list of paths in string.\n    - an optional user-specified schema in json string.\n    - a dictionary of options in string.\n\n    This process then creates a `DataSource` instance using the above information and\n    sends the pickled instance as well as the schema back to the JVM.\n    \"\"\"\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source_cls = read_command(pickleSer, infile)\n        if not (isinstance(data_source_cls, type) and issubclass(data_source_cls, DataSource)):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': 'a subclass of DataSource', 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        if not inspect.ismethod(data_source_cls.name):\n            raise PySparkTypeError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"'name()' method to be a classmethod\", 'actual': f\"'{type(data_source_cls.name).__name__}'\"})\n        provider = utf8_deserializer.loads(infile)\n        if provider.lower() != data_source_cls.name().lower():\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': f'provider with name {data_source_cls.name()}', 'actual': f\"'{provider}'\"})\n        num_paths = read_int(infile)\n        paths: List[str] = []\n        for _ in range(num_paths):\n            paths.append(utf8_deserializer.loads(infile))\n        user_specified_schema = None\n        if read_bool(infile):\n            user_specified_schema = _parse_datatype_json_string(utf8_deserializer.loads(infile))\n            if not isinstance(user_specified_schema, StructType):\n                raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"the user-defined schema to be a 'StructType'\", 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        options = dict()\n        num_options = read_int(infile)\n        for _ in range(num_options):\n            key = utf8_deserializer.loads(infile)\n            value = utf8_deserializer.loads(infile)\n            options[key] = value\n        try:\n            data_source = data_source_cls(paths=paths, userSpecifiedSchema=user_specified_schema, options=options)\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'instance', 'error': str(e)})\n        is_ddl_string = False\n        if user_specified_schema is None:\n            try:\n                schema = data_source.schema()\n                if isinstance(schema, str):\n                    is_ddl_string = True\n            except NotImplementedError:\n                raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'instance', 'method': 'schema'})\n        else:\n            schema = user_specified_schema\n        assert schema is not None\n        pickleSer._write_with_length(data_source, outfile)\n        write_int(int(is_ddl_string), outfile)\n        if is_ddl_string:\n            write_with_length(schema.encode('utf-8'), outfile)\n        else:\n            write_with_length(schema.json().encode('utf-8'), outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
        "mutated": [
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n    '\\n    Main method for creating a Python data source instance.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceRunner.runInPython` method\\n    in JVM. This process is responsible for creating a `DataSource` object and send the\\n    information needed back to the JVM.\\n\\n    The JVM sends the following information to this process:\\n    - a `DataSource` class representing the data source to be created.\\n    - a provider name in string.\\n    - a list of paths in string.\\n    - an optional user-specified schema in json string.\\n    - a dictionary of options in string.\\n\\n    This process then creates a `DataSource` instance using the above information and\\n    sends the pickled instance as well as the schema back to the JVM.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source_cls = read_command(pickleSer, infile)\n        if not (isinstance(data_source_cls, type) and issubclass(data_source_cls, DataSource)):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': 'a subclass of DataSource', 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        if not inspect.ismethod(data_source_cls.name):\n            raise PySparkTypeError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"'name()' method to be a classmethod\", 'actual': f\"'{type(data_source_cls.name).__name__}'\"})\n        provider = utf8_deserializer.loads(infile)\n        if provider.lower() != data_source_cls.name().lower():\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': f'provider with name {data_source_cls.name()}', 'actual': f\"'{provider}'\"})\n        num_paths = read_int(infile)\n        paths: List[str] = []\n        for _ in range(num_paths):\n            paths.append(utf8_deserializer.loads(infile))\n        user_specified_schema = None\n        if read_bool(infile):\n            user_specified_schema = _parse_datatype_json_string(utf8_deserializer.loads(infile))\n            if not isinstance(user_specified_schema, StructType):\n                raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"the user-defined schema to be a 'StructType'\", 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        options = dict()\n        num_options = read_int(infile)\n        for _ in range(num_options):\n            key = utf8_deserializer.loads(infile)\n            value = utf8_deserializer.loads(infile)\n            options[key] = value\n        try:\n            data_source = data_source_cls(paths=paths, userSpecifiedSchema=user_specified_schema, options=options)\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'instance', 'error': str(e)})\n        is_ddl_string = False\n        if user_specified_schema is None:\n            try:\n                schema = data_source.schema()\n                if isinstance(schema, str):\n                    is_ddl_string = True\n            except NotImplementedError:\n                raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'instance', 'method': 'schema'})\n        else:\n            schema = user_specified_schema\n        assert schema is not None\n        pickleSer._write_with_length(data_source, outfile)\n        write_int(int(is_ddl_string), outfile)\n        if is_ddl_string:\n            write_with_length(schema.encode('utf-8'), outfile)\n        else:\n            write_with_length(schema.json().encode('utf-8'), outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Main method for creating a Python data source instance.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceRunner.runInPython` method\\n    in JVM. This process is responsible for creating a `DataSource` object and send the\\n    information needed back to the JVM.\\n\\n    The JVM sends the following information to this process:\\n    - a `DataSource` class representing the data source to be created.\\n    - a provider name in string.\\n    - a list of paths in string.\\n    - an optional user-specified schema in json string.\\n    - a dictionary of options in string.\\n\\n    This process then creates a `DataSource` instance using the above information and\\n    sends the pickled instance as well as the schema back to the JVM.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source_cls = read_command(pickleSer, infile)\n        if not (isinstance(data_source_cls, type) and issubclass(data_source_cls, DataSource)):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': 'a subclass of DataSource', 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        if not inspect.ismethod(data_source_cls.name):\n            raise PySparkTypeError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"'name()' method to be a classmethod\", 'actual': f\"'{type(data_source_cls.name).__name__}'\"})\n        provider = utf8_deserializer.loads(infile)\n        if provider.lower() != data_source_cls.name().lower():\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': f'provider with name {data_source_cls.name()}', 'actual': f\"'{provider}'\"})\n        num_paths = read_int(infile)\n        paths: List[str] = []\n        for _ in range(num_paths):\n            paths.append(utf8_deserializer.loads(infile))\n        user_specified_schema = None\n        if read_bool(infile):\n            user_specified_schema = _parse_datatype_json_string(utf8_deserializer.loads(infile))\n            if not isinstance(user_specified_schema, StructType):\n                raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"the user-defined schema to be a 'StructType'\", 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        options = dict()\n        num_options = read_int(infile)\n        for _ in range(num_options):\n            key = utf8_deserializer.loads(infile)\n            value = utf8_deserializer.loads(infile)\n            options[key] = value\n        try:\n            data_source = data_source_cls(paths=paths, userSpecifiedSchema=user_specified_schema, options=options)\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'instance', 'error': str(e)})\n        is_ddl_string = False\n        if user_specified_schema is None:\n            try:\n                schema = data_source.schema()\n                if isinstance(schema, str):\n                    is_ddl_string = True\n            except NotImplementedError:\n                raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'instance', 'method': 'schema'})\n        else:\n            schema = user_specified_schema\n        assert schema is not None\n        pickleSer._write_with_length(data_source, outfile)\n        write_int(int(is_ddl_string), outfile)\n        if is_ddl_string:\n            write_with_length(schema.encode('utf-8'), outfile)\n        else:\n            write_with_length(schema.json().encode('utf-8'), outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Main method for creating a Python data source instance.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceRunner.runInPython` method\\n    in JVM. This process is responsible for creating a `DataSource` object and send the\\n    information needed back to the JVM.\\n\\n    The JVM sends the following information to this process:\\n    - a `DataSource` class representing the data source to be created.\\n    - a provider name in string.\\n    - a list of paths in string.\\n    - an optional user-specified schema in json string.\\n    - a dictionary of options in string.\\n\\n    This process then creates a `DataSource` instance using the above information and\\n    sends the pickled instance as well as the schema back to the JVM.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source_cls = read_command(pickleSer, infile)\n        if not (isinstance(data_source_cls, type) and issubclass(data_source_cls, DataSource)):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': 'a subclass of DataSource', 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        if not inspect.ismethod(data_source_cls.name):\n            raise PySparkTypeError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"'name()' method to be a classmethod\", 'actual': f\"'{type(data_source_cls.name).__name__}'\"})\n        provider = utf8_deserializer.loads(infile)\n        if provider.lower() != data_source_cls.name().lower():\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': f'provider with name {data_source_cls.name()}', 'actual': f\"'{provider}'\"})\n        num_paths = read_int(infile)\n        paths: List[str] = []\n        for _ in range(num_paths):\n            paths.append(utf8_deserializer.loads(infile))\n        user_specified_schema = None\n        if read_bool(infile):\n            user_specified_schema = _parse_datatype_json_string(utf8_deserializer.loads(infile))\n            if not isinstance(user_specified_schema, StructType):\n                raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"the user-defined schema to be a 'StructType'\", 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        options = dict()\n        num_options = read_int(infile)\n        for _ in range(num_options):\n            key = utf8_deserializer.loads(infile)\n            value = utf8_deserializer.loads(infile)\n            options[key] = value\n        try:\n            data_source = data_source_cls(paths=paths, userSpecifiedSchema=user_specified_schema, options=options)\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'instance', 'error': str(e)})\n        is_ddl_string = False\n        if user_specified_schema is None:\n            try:\n                schema = data_source.schema()\n                if isinstance(schema, str):\n                    is_ddl_string = True\n            except NotImplementedError:\n                raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'instance', 'method': 'schema'})\n        else:\n            schema = user_specified_schema\n        assert schema is not None\n        pickleSer._write_with_length(data_source, outfile)\n        write_int(int(is_ddl_string), outfile)\n        if is_ddl_string:\n            write_with_length(schema.encode('utf-8'), outfile)\n        else:\n            write_with_length(schema.json().encode('utf-8'), outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Main method for creating a Python data source instance.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceRunner.runInPython` method\\n    in JVM. This process is responsible for creating a `DataSource` object and send the\\n    information needed back to the JVM.\\n\\n    The JVM sends the following information to this process:\\n    - a `DataSource` class representing the data source to be created.\\n    - a provider name in string.\\n    - a list of paths in string.\\n    - an optional user-specified schema in json string.\\n    - a dictionary of options in string.\\n\\n    This process then creates a `DataSource` instance using the above information and\\n    sends the pickled instance as well as the schema back to the JVM.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source_cls = read_command(pickleSer, infile)\n        if not (isinstance(data_source_cls, type) and issubclass(data_source_cls, DataSource)):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': 'a subclass of DataSource', 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        if not inspect.ismethod(data_source_cls.name):\n            raise PySparkTypeError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"'name()' method to be a classmethod\", 'actual': f\"'{type(data_source_cls.name).__name__}'\"})\n        provider = utf8_deserializer.loads(infile)\n        if provider.lower() != data_source_cls.name().lower():\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': f'provider with name {data_source_cls.name()}', 'actual': f\"'{provider}'\"})\n        num_paths = read_int(infile)\n        paths: List[str] = []\n        for _ in range(num_paths):\n            paths.append(utf8_deserializer.loads(infile))\n        user_specified_schema = None\n        if read_bool(infile):\n            user_specified_schema = _parse_datatype_json_string(utf8_deserializer.loads(infile))\n            if not isinstance(user_specified_schema, StructType):\n                raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"the user-defined schema to be a 'StructType'\", 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        options = dict()\n        num_options = read_int(infile)\n        for _ in range(num_options):\n            key = utf8_deserializer.loads(infile)\n            value = utf8_deserializer.loads(infile)\n            options[key] = value\n        try:\n            data_source = data_source_cls(paths=paths, userSpecifiedSchema=user_specified_schema, options=options)\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'instance', 'error': str(e)})\n        is_ddl_string = False\n        if user_specified_schema is None:\n            try:\n                schema = data_source.schema()\n                if isinstance(schema, str):\n                    is_ddl_string = True\n            except NotImplementedError:\n                raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'instance', 'method': 'schema'})\n        else:\n            schema = user_specified_schema\n        assert schema is not None\n        pickleSer._write_with_length(data_source, outfile)\n        write_int(int(is_ddl_string), outfile)\n        if is_ddl_string:\n            write_with_length(schema.encode('utf-8'), outfile)\n        else:\n            write_with_length(schema.json().encode('utf-8'), outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Main method for creating a Python data source instance.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceRunner.runInPython` method\\n    in JVM. This process is responsible for creating a `DataSource` object and send the\\n    information needed back to the JVM.\\n\\n    The JVM sends the following information to this process:\\n    - a `DataSource` class representing the data source to be created.\\n    - a provider name in string.\\n    - a list of paths in string.\\n    - an optional user-specified schema in json string.\\n    - a dictionary of options in string.\\n\\n    This process then creates a `DataSource` instance using the above information and\\n    sends the pickled instance as well as the schema back to the JVM.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source_cls = read_command(pickleSer, infile)\n        if not (isinstance(data_source_cls, type) and issubclass(data_source_cls, DataSource)):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': 'a subclass of DataSource', 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        if not inspect.ismethod(data_source_cls.name):\n            raise PySparkTypeError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"'name()' method to be a classmethod\", 'actual': f\"'{type(data_source_cls.name).__name__}'\"})\n        provider = utf8_deserializer.loads(infile)\n        if provider.lower() != data_source_cls.name().lower():\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': f'provider with name {data_source_cls.name()}', 'actual': f\"'{provider}'\"})\n        num_paths = read_int(infile)\n        paths: List[str] = []\n        for _ in range(num_paths):\n            paths.append(utf8_deserializer.loads(infile))\n        user_specified_schema = None\n        if read_bool(infile):\n            user_specified_schema = _parse_datatype_json_string(utf8_deserializer.loads(infile))\n            if not isinstance(user_specified_schema, StructType):\n                raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"the user-defined schema to be a 'StructType'\", 'actual': f\"'{type(data_source_cls).__name__}'\"})\n        options = dict()\n        num_options = read_int(infile)\n        for _ in range(num_options):\n            key = utf8_deserializer.loads(infile)\n            value = utf8_deserializer.loads(infile)\n            options[key] = value\n        try:\n            data_source = data_source_cls(paths=paths, userSpecifiedSchema=user_specified_schema, options=options)\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'instance', 'error': str(e)})\n        is_ddl_string = False\n        if user_specified_schema is None:\n            try:\n                schema = data_source.schema()\n                if isinstance(schema, str):\n                    is_ddl_string = True\n            except NotImplementedError:\n                raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'instance', 'method': 'schema'})\n        else:\n            schema = user_specified_schema\n        assert schema is not None\n        pickleSer._write_with_length(data_source, outfile)\n        write_int(int(is_ddl_string), outfile)\n        if is_ddl_string:\n            write_with_length(schema.encode('utf-8'), outfile)\n        else:\n            write_with_length(schema.json().encode('utf-8'), outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)"
        ]
    }
]