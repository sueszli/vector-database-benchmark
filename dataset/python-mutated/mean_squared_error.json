[
    {
        "func_name": "check_type_forward",
        "original": "def check_type_forward(self, in_types):\n    type_check._argname(in_types, ('x0', 'x1'))\n    type_check.expect(in_types[0].dtype.kind == 'f', in_types[0].dtype == in_types[1].dtype, in_types[0].shape == in_types[1].shape)",
        "mutated": [
            "def check_type_forward(self, in_types):\n    if False:\n        i = 10\n    type_check._argname(in_types, ('x0', 'x1'))\n    type_check.expect(in_types[0].dtype.kind == 'f', in_types[0].dtype == in_types[1].dtype, in_types[0].shape == in_types[1].shape)",
            "def check_type_forward(self, in_types):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    type_check._argname(in_types, ('x0', 'x1'))\n    type_check.expect(in_types[0].dtype.kind == 'f', in_types[0].dtype == in_types[1].dtype, in_types[0].shape == in_types[1].shape)",
            "def check_type_forward(self, in_types):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    type_check._argname(in_types, ('x0', 'x1'))\n    type_check.expect(in_types[0].dtype.kind == 'f', in_types[0].dtype == in_types[1].dtype, in_types[0].shape == in_types[1].shape)",
            "def check_type_forward(self, in_types):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    type_check._argname(in_types, ('x0', 'x1'))\n    type_check.expect(in_types[0].dtype.kind == 'f', in_types[0].dtype == in_types[1].dtype, in_types[0].shape == in_types[1].shape)",
            "def check_type_forward(self, in_types):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    type_check._argname(in_types, ('x0', 'x1'))\n    type_check.expect(in_types[0].dtype.kind == 'f', in_types[0].dtype == in_types[1].dtype, in_types[0].shape == in_types[1].shape)"
        ]
    },
    {
        "func_name": "forward_cpu",
        "original": "def forward_cpu(self, inputs):\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (numpy.array(diff.dot(diff) / diff.size, dtype=diff.dtype),)",
        "mutated": [
            "def forward_cpu(self, inputs):\n    if False:\n        i = 10\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (numpy.array(diff.dot(diff) / diff.size, dtype=diff.dtype),)",
            "def forward_cpu(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (numpy.array(diff.dot(diff) / diff.size, dtype=diff.dtype),)",
            "def forward_cpu(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (numpy.array(diff.dot(diff) / diff.size, dtype=diff.dtype),)",
            "def forward_cpu(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (numpy.array(diff.dot(diff) / diff.size, dtype=diff.dtype),)",
            "def forward_cpu(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (numpy.array(diff.dot(diff) / diff.size, dtype=diff.dtype),)"
        ]
    },
    {
        "func_name": "forward_gpu",
        "original": "def forward_gpu(self, inputs):\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (diff.dot(diff) / diff.dtype.type(diff.size),)",
        "mutated": [
            "def forward_gpu(self, inputs):\n    if False:\n        i = 10\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (diff.dot(diff) / diff.dtype.type(diff.size),)",
            "def forward_gpu(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (diff.dot(diff) / diff.dtype.type(diff.size),)",
            "def forward_gpu(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (diff.dot(diff) / diff.dtype.type(diff.size),)",
            "def forward_gpu(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (diff.dot(diff) / diff.dtype.type(diff.size),)",
            "def forward_gpu(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.retain_inputs((0, 1))\n    diff = (inputs[0] - inputs[1]).ravel()\n    return (diff.dot(diff) / diff.dtype.type(diff.size),)"
        ]
    },
    {
        "func_name": "backward",
        "original": "def backward(self, indexes, gy):\n    (x0, x1) = self.get_retained_inputs()\n    ret = []\n    diff = x0 - x1\n    gy0 = chainer.functions.broadcast_to(gy[0], diff.shape)\n    gx0 = gy0 * diff * (2.0 / diff.size)\n    if 0 in indexes:\n        ret.append(gx0)\n    if 1 in indexes:\n        ret.append(-gx0)\n    return ret",
        "mutated": [
            "def backward(self, indexes, gy):\n    if False:\n        i = 10\n    (x0, x1) = self.get_retained_inputs()\n    ret = []\n    diff = x0 - x1\n    gy0 = chainer.functions.broadcast_to(gy[0], diff.shape)\n    gx0 = gy0 * diff * (2.0 / diff.size)\n    if 0 in indexes:\n        ret.append(gx0)\n    if 1 in indexes:\n        ret.append(-gx0)\n    return ret",
            "def backward(self, indexes, gy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (x0, x1) = self.get_retained_inputs()\n    ret = []\n    diff = x0 - x1\n    gy0 = chainer.functions.broadcast_to(gy[0], diff.shape)\n    gx0 = gy0 * diff * (2.0 / diff.size)\n    if 0 in indexes:\n        ret.append(gx0)\n    if 1 in indexes:\n        ret.append(-gx0)\n    return ret",
            "def backward(self, indexes, gy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (x0, x1) = self.get_retained_inputs()\n    ret = []\n    diff = x0 - x1\n    gy0 = chainer.functions.broadcast_to(gy[0], diff.shape)\n    gx0 = gy0 * diff * (2.0 / diff.size)\n    if 0 in indexes:\n        ret.append(gx0)\n    if 1 in indexes:\n        ret.append(-gx0)\n    return ret",
            "def backward(self, indexes, gy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (x0, x1) = self.get_retained_inputs()\n    ret = []\n    diff = x0 - x1\n    gy0 = chainer.functions.broadcast_to(gy[0], diff.shape)\n    gx0 = gy0 * diff * (2.0 / diff.size)\n    if 0 in indexes:\n        ret.append(gx0)\n    if 1 in indexes:\n        ret.append(-gx0)\n    return ret",
            "def backward(self, indexes, gy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (x0, x1) = self.get_retained_inputs()\n    ret = []\n    diff = x0 - x1\n    gy0 = chainer.functions.broadcast_to(gy[0], diff.shape)\n    gx0 = gy0 * diff * (2.0 / diff.size)\n    if 0 in indexes:\n        ret.append(gx0)\n    if 1 in indexes:\n        ret.append(-gx0)\n    return ret"
        ]
    },
    {
        "func_name": "mean_squared_error",
        "original": "def mean_squared_error(x0, x1):\n    \"\"\"Mean squared error function.\n\n    The function computes the mean squared error between two variables. The\n    mean is taken over the minibatch. Args ``x0`` and ``x1`` must have the\n    same dimensions. Note that the error is not scaled by 1/2.\n\n    Args:\n        x0 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\n        x1 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\n\n    Returns:\n        ~chainer.Variable:\n            A variable holding an array representing the mean squared\n            error of two inputs.\n\n     .. admonition:: Example\n\n        1D array examples:\n\n        >>> x = np.array([1, 2, 3, 4]).astype(np.float32)\n        >>> y = np.array([0, 0, 0, 0]).astype(np.float32)\n        >>> F.mean_squared_error(x, y)\n        variable(7.5)\n        >>> x = np.array([1, 2, 3, 4, 5, 6]).astype(np.float32)\n        >>> y = np.array([7, 8, 9, 10, 11, 12]).astype(np.float32)\n        >>> F.mean_squared_error(x, y)\n        variable(36.)\n\n        2D array example:\n\n        In this example, there are 4 elements, and thus 4 errors\n        >>> x = np.array([[1, 2], [3, 4]]).astype(np.float32)\n        >>> y = np.array([[8, 8], [8, 8]]).astype(np.float32)\n        >>> F.mean_squared_error(x, y)\n        variable(31.5)\n\n        3D array example:\n\n        In this example, there are 8 elements, and thus 8 errors\n        >>> x = np.reshape(np.array([1, 2, 3, 4, 5, 6, 7, 8]), (2, 2, 2))\n        >>> y = np.reshape(np.array([8, 8, 8, 8, 8, 8, 8, 8]), (2, 2, 2))\n        >>> x = x.astype(np.float32)\n        >>> y = y.astype(np.float32)\n        >>> F.mean_squared_error(x, y)\n        variable(17.5)\n\n    \"\"\"\n    return MeanSquaredError().apply((x0, x1))[0]",
        "mutated": [
            "def mean_squared_error(x0, x1):\n    if False:\n        i = 10\n    'Mean squared error function.\\n\\n    The function computes the mean squared error between two variables. The\\n    mean is taken over the minibatch. Args ``x0`` and ``x1`` must have the\\n    same dimensions. Note that the error is not scaled by 1/2.\\n\\n    Args:\\n        x0 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n        x1 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable holding an array representing the mean squared\\n            error of two inputs.\\n\\n     .. admonition:: Example\\n\\n        1D array examples:\\n\\n        >>> x = np.array([1, 2, 3, 4]).astype(np.float32)\\n        >>> y = np.array([0, 0, 0, 0]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(7.5)\\n        >>> x = np.array([1, 2, 3, 4, 5, 6]).astype(np.float32)\\n        >>> y = np.array([7, 8, 9, 10, 11, 12]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(36.)\\n\\n        2D array example:\\n\\n        In this example, there are 4 elements, and thus 4 errors\\n        >>> x = np.array([[1, 2], [3, 4]]).astype(np.float32)\\n        >>> y = np.array([[8, 8], [8, 8]]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(31.5)\\n\\n        3D array example:\\n\\n        In this example, there are 8 elements, and thus 8 errors\\n        >>> x = np.reshape(np.array([1, 2, 3, 4, 5, 6, 7, 8]), (2, 2, 2))\\n        >>> y = np.reshape(np.array([8, 8, 8, 8, 8, 8, 8, 8]), (2, 2, 2))\\n        >>> x = x.astype(np.float32)\\n        >>> y = y.astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(17.5)\\n\\n    '\n    return MeanSquaredError().apply((x0, x1))[0]",
            "def mean_squared_error(x0, x1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Mean squared error function.\\n\\n    The function computes the mean squared error between two variables. The\\n    mean is taken over the minibatch. Args ``x0`` and ``x1`` must have the\\n    same dimensions. Note that the error is not scaled by 1/2.\\n\\n    Args:\\n        x0 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n        x1 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable holding an array representing the mean squared\\n            error of two inputs.\\n\\n     .. admonition:: Example\\n\\n        1D array examples:\\n\\n        >>> x = np.array([1, 2, 3, 4]).astype(np.float32)\\n        >>> y = np.array([0, 0, 0, 0]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(7.5)\\n        >>> x = np.array([1, 2, 3, 4, 5, 6]).astype(np.float32)\\n        >>> y = np.array([7, 8, 9, 10, 11, 12]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(36.)\\n\\n        2D array example:\\n\\n        In this example, there are 4 elements, and thus 4 errors\\n        >>> x = np.array([[1, 2], [3, 4]]).astype(np.float32)\\n        >>> y = np.array([[8, 8], [8, 8]]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(31.5)\\n\\n        3D array example:\\n\\n        In this example, there are 8 elements, and thus 8 errors\\n        >>> x = np.reshape(np.array([1, 2, 3, 4, 5, 6, 7, 8]), (2, 2, 2))\\n        >>> y = np.reshape(np.array([8, 8, 8, 8, 8, 8, 8, 8]), (2, 2, 2))\\n        >>> x = x.astype(np.float32)\\n        >>> y = y.astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(17.5)\\n\\n    '\n    return MeanSquaredError().apply((x0, x1))[0]",
            "def mean_squared_error(x0, x1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Mean squared error function.\\n\\n    The function computes the mean squared error between two variables. The\\n    mean is taken over the minibatch. Args ``x0`` and ``x1`` must have the\\n    same dimensions. Note that the error is not scaled by 1/2.\\n\\n    Args:\\n        x0 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n        x1 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable holding an array representing the mean squared\\n            error of two inputs.\\n\\n     .. admonition:: Example\\n\\n        1D array examples:\\n\\n        >>> x = np.array([1, 2, 3, 4]).astype(np.float32)\\n        >>> y = np.array([0, 0, 0, 0]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(7.5)\\n        >>> x = np.array([1, 2, 3, 4, 5, 6]).astype(np.float32)\\n        >>> y = np.array([7, 8, 9, 10, 11, 12]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(36.)\\n\\n        2D array example:\\n\\n        In this example, there are 4 elements, and thus 4 errors\\n        >>> x = np.array([[1, 2], [3, 4]]).astype(np.float32)\\n        >>> y = np.array([[8, 8], [8, 8]]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(31.5)\\n\\n        3D array example:\\n\\n        In this example, there are 8 elements, and thus 8 errors\\n        >>> x = np.reshape(np.array([1, 2, 3, 4, 5, 6, 7, 8]), (2, 2, 2))\\n        >>> y = np.reshape(np.array([8, 8, 8, 8, 8, 8, 8, 8]), (2, 2, 2))\\n        >>> x = x.astype(np.float32)\\n        >>> y = y.astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(17.5)\\n\\n    '\n    return MeanSquaredError().apply((x0, x1))[0]",
            "def mean_squared_error(x0, x1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Mean squared error function.\\n\\n    The function computes the mean squared error between two variables. The\\n    mean is taken over the minibatch. Args ``x0`` and ``x1`` must have the\\n    same dimensions. Note that the error is not scaled by 1/2.\\n\\n    Args:\\n        x0 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n        x1 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable holding an array representing the mean squared\\n            error of two inputs.\\n\\n     .. admonition:: Example\\n\\n        1D array examples:\\n\\n        >>> x = np.array([1, 2, 3, 4]).astype(np.float32)\\n        >>> y = np.array([0, 0, 0, 0]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(7.5)\\n        >>> x = np.array([1, 2, 3, 4, 5, 6]).astype(np.float32)\\n        >>> y = np.array([7, 8, 9, 10, 11, 12]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(36.)\\n\\n        2D array example:\\n\\n        In this example, there are 4 elements, and thus 4 errors\\n        >>> x = np.array([[1, 2], [3, 4]]).astype(np.float32)\\n        >>> y = np.array([[8, 8], [8, 8]]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(31.5)\\n\\n        3D array example:\\n\\n        In this example, there are 8 elements, and thus 8 errors\\n        >>> x = np.reshape(np.array([1, 2, 3, 4, 5, 6, 7, 8]), (2, 2, 2))\\n        >>> y = np.reshape(np.array([8, 8, 8, 8, 8, 8, 8, 8]), (2, 2, 2))\\n        >>> x = x.astype(np.float32)\\n        >>> y = y.astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(17.5)\\n\\n    '\n    return MeanSquaredError().apply((x0, x1))[0]",
            "def mean_squared_error(x0, x1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Mean squared error function.\\n\\n    The function computes the mean squared error between two variables. The\\n    mean is taken over the minibatch. Args ``x0`` and ``x1`` must have the\\n    same dimensions. Note that the error is not scaled by 1/2.\\n\\n    Args:\\n        x0 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n        x1 (:class:`~chainer.Variable` or :ref:`ndarray`): Input variable.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable holding an array representing the mean squared\\n            error of two inputs.\\n\\n     .. admonition:: Example\\n\\n        1D array examples:\\n\\n        >>> x = np.array([1, 2, 3, 4]).astype(np.float32)\\n        >>> y = np.array([0, 0, 0, 0]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(7.5)\\n        >>> x = np.array([1, 2, 3, 4, 5, 6]).astype(np.float32)\\n        >>> y = np.array([7, 8, 9, 10, 11, 12]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(36.)\\n\\n        2D array example:\\n\\n        In this example, there are 4 elements, and thus 4 errors\\n        >>> x = np.array([[1, 2], [3, 4]]).astype(np.float32)\\n        >>> y = np.array([[8, 8], [8, 8]]).astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(31.5)\\n\\n        3D array example:\\n\\n        In this example, there are 8 elements, and thus 8 errors\\n        >>> x = np.reshape(np.array([1, 2, 3, 4, 5, 6, 7, 8]), (2, 2, 2))\\n        >>> y = np.reshape(np.array([8, 8, 8, 8, 8, 8, 8, 8]), (2, 2, 2))\\n        >>> x = x.astype(np.float32)\\n        >>> y = y.astype(np.float32)\\n        >>> F.mean_squared_error(x, y)\\n        variable(17.5)\\n\\n    '\n    return MeanSquaredError().apply((x0, x1))[0]"
        ]
    }
]