[
    {
        "func_name": "check_tensors",
        "original": "def check_tensors(in_out_list, name):\n    assert in_out_list is not None, f'{name} should not be None'\n    if isinstance(in_out_list, (list, tuple)):\n        assert len(in_out_list) > 0, f'{name} connot be empty'\n        for each_var in in_out_list:\n            assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n        return in_out_list\n    else:\n        assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n        return [in_out_list]",
        "mutated": [
            "def check_tensors(in_out_list, name):\n    if False:\n        i = 10\n    assert in_out_list is not None, f'{name} should not be None'\n    if isinstance(in_out_list, (list, tuple)):\n        assert len(in_out_list) > 0, f'{name} connot be empty'\n        for each_var in in_out_list:\n            assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n        return in_out_list\n    else:\n        assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n        return [in_out_list]",
            "def check_tensors(in_out_list, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert in_out_list is not None, f'{name} should not be None'\n    if isinstance(in_out_list, (list, tuple)):\n        assert len(in_out_list) > 0, f'{name} connot be empty'\n        for each_var in in_out_list:\n            assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n        return in_out_list\n    else:\n        assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n        return [in_out_list]",
            "def check_tensors(in_out_list, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert in_out_list is not None, f'{name} should not be None'\n    if isinstance(in_out_list, (list, tuple)):\n        assert len(in_out_list) > 0, f'{name} connot be empty'\n        for each_var in in_out_list:\n            assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n        return in_out_list\n    else:\n        assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n        return [in_out_list]",
            "def check_tensors(in_out_list, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert in_out_list is not None, f'{name} should not be None'\n    if isinstance(in_out_list, (list, tuple)):\n        assert len(in_out_list) > 0, f'{name} connot be empty'\n        for each_var in in_out_list:\n            assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n        return in_out_list\n    else:\n        assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n        return [in_out_list]",
            "def check_tensors(in_out_list, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert in_out_list is not None, f'{name} should not be None'\n    if isinstance(in_out_list, (list, tuple)):\n        assert len(in_out_list) > 0, f'{name} connot be empty'\n        for each_var in in_out_list:\n            assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n        return in_out_list\n    else:\n        assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n        return [in_out_list]"
        ]
    },
    {
        "func_name": "backward",
        "original": "@framework.dygraph_only\ndef backward(tensors, grad_tensors=None, retain_graph=False):\n    \"\"\"\n    Compute the backward gradients of given tensors.\n\n    Args:\n        tensors(list of Tensors): the tensors which the gradient to be computed. The tensors can not contain the same tensor.\n\n        grad_tensors(list of Tensors of None, optional): the init gradients of the `tensors`` .If not None, it must have the same length with ``tensors`` ,\n            and if any of the elements is None, then the init gradient is the default value which is filled with 1.0.\n            If None, all the gradients of the ``tensors`` is the default value which is filled with 1.0.\n            Defaults to None.\n\n        retain_graph(bool, optional): If False, the graph used to compute grads will be freed. If you would\n            like to add more ops to the built graph after calling this method( :code:`backward` ), set the parameter\n            :code:`retain_graph` to True, then the grads will be retained. Thus, setting it to False is much more memory-efficient.\n            Defaults to False.\n\n    Returns:\n        NoneType: None\n\n\n    Examples:\n        .. code-block:: python\n\n            >>> import paddle\n            >>> x = paddle.to_tensor([[1, 2], [3, 4]], dtype='float32', stop_gradient=False)\n            >>> y = paddle.to_tensor([[3, 2], [3, 4]], dtype='float32')\n\n            >>> grad_tensor1 = paddle.to_tensor([[1,2], [2, 3]], dtype='float32')\n            >>> grad_tensor2 = paddle.to_tensor([[1,1], [1, 1]], dtype='float32')\n\n            >>> z1 = paddle.matmul(x, y)\n            >>> z2 = paddle.matmul(x, y)\n\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, grad_tensor2], True)\n            >>> print(x.grad)\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\n            [[12., 18.],\n             [17., 25.]])\n\n\n            >>> x.clear_grad()\n\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, None], True)\n            >>> print(x.grad)\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\n            [[12., 18.],\n             [17., 25.]])\n\n            >>> x.clear_grad()\n\n            >>> paddle.autograd.backward([z1, z2])\n            >>> print(x.grad)\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\n            [[10., 14.],\n             [10., 14.]])\n\n\n    \"\"\"\n\n    def check_tensors(in_out_list, name):\n        assert in_out_list is not None, f'{name} should not be None'\n        if isinstance(in_out_list, (list, tuple)):\n            assert len(in_out_list) > 0, f'{name} connot be empty'\n            for each_var in in_out_list:\n                assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n            return in_out_list\n        else:\n            assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n            return [in_out_list]\n    tensors = check_tensors(tensors, 'tensors')\n    assert len(tensors) == len(set(tensors)), \"The argument 'tensors' of paddle.autograd.backward contains duplicate paddle.Tensor object.\"\n    if grad_tensors is not None:\n        if not isinstance(grad_tensors, (list, tuple)):\n            grad_tensors = [grad_tensors]\n        for each_tensor in grad_tensors:\n            if each_tensor is not None:\n                assert isinstance(each_tensor, (paddle.Tensor, core.eager.Tensor)), \"The argument 'grad_tensors' of paddle.autograd.backward is invalid, it can be 'None', 'paddle.Tensor' or 'list[None/paddle.Tensor]'.\"\n    else:\n        grad_tensors = []\n    if len(grad_tensors) > 0:\n        assert len(tensors) == len(grad_tensors), 'The length of grad_tensors must be equal to tensors'\n    assert isinstance(retain_graph, bool), 'retain_graph must be True or False'\n    core.eager.run_backward(tensors, grad_tensors, retain_graph)",
        "mutated": [
            "@framework.dygraph_only\ndef backward(tensors, grad_tensors=None, retain_graph=False):\n    if False:\n        i = 10\n    \"\\n    Compute the backward gradients of given tensors.\\n\\n    Args:\\n        tensors(list of Tensors): the tensors which the gradient to be computed. The tensors can not contain the same tensor.\\n\\n        grad_tensors(list of Tensors of None, optional): the init gradients of the `tensors`` .If not None, it must have the same length with ``tensors`` ,\\n            and if any of the elements is None, then the init gradient is the default value which is filled with 1.0.\\n            If None, all the gradients of the ``tensors`` is the default value which is filled with 1.0.\\n            Defaults to None.\\n\\n        retain_graph(bool, optional): If False, the graph used to compute grads will be freed. If you would\\n            like to add more ops to the built graph after calling this method( :code:`backward` ), set the parameter\\n            :code:`retain_graph` to True, then the grads will be retained. Thus, setting it to False is much more memory-efficient.\\n            Defaults to False.\\n\\n    Returns:\\n        NoneType: None\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> x = paddle.to_tensor([[1, 2], [3, 4]], dtype='float32', stop_gradient=False)\\n            >>> y = paddle.to_tensor([[3, 2], [3, 4]], dtype='float32')\\n\\n            >>> grad_tensor1 = paddle.to_tensor([[1,2], [2, 3]], dtype='float32')\\n            >>> grad_tensor2 = paddle.to_tensor([[1,1], [1, 1]], dtype='float32')\\n\\n            >>> z1 = paddle.matmul(x, y)\\n            >>> z2 = paddle.matmul(x, y)\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, grad_tensor2], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, None], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2])\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[10., 14.],\\n             [10., 14.]])\\n\\n\\n    \"\n\n    def check_tensors(in_out_list, name):\n        assert in_out_list is not None, f'{name} should not be None'\n        if isinstance(in_out_list, (list, tuple)):\n            assert len(in_out_list) > 0, f'{name} connot be empty'\n            for each_var in in_out_list:\n                assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n            return in_out_list\n        else:\n            assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n            return [in_out_list]\n    tensors = check_tensors(tensors, 'tensors')\n    assert len(tensors) == len(set(tensors)), \"The argument 'tensors' of paddle.autograd.backward contains duplicate paddle.Tensor object.\"\n    if grad_tensors is not None:\n        if not isinstance(grad_tensors, (list, tuple)):\n            grad_tensors = [grad_tensors]\n        for each_tensor in grad_tensors:\n            if each_tensor is not None:\n                assert isinstance(each_tensor, (paddle.Tensor, core.eager.Tensor)), \"The argument 'grad_tensors' of paddle.autograd.backward is invalid, it can be 'None', 'paddle.Tensor' or 'list[None/paddle.Tensor]'.\"\n    else:\n        grad_tensors = []\n    if len(grad_tensors) > 0:\n        assert len(tensors) == len(grad_tensors), 'The length of grad_tensors must be equal to tensors'\n    assert isinstance(retain_graph, bool), 'retain_graph must be True or False'\n    core.eager.run_backward(tensors, grad_tensors, retain_graph)",
            "@framework.dygraph_only\ndef backward(tensors, grad_tensors=None, retain_graph=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Compute the backward gradients of given tensors.\\n\\n    Args:\\n        tensors(list of Tensors): the tensors which the gradient to be computed. The tensors can not contain the same tensor.\\n\\n        grad_tensors(list of Tensors of None, optional): the init gradients of the `tensors`` .If not None, it must have the same length with ``tensors`` ,\\n            and if any of the elements is None, then the init gradient is the default value which is filled with 1.0.\\n            If None, all the gradients of the ``tensors`` is the default value which is filled with 1.0.\\n            Defaults to None.\\n\\n        retain_graph(bool, optional): If False, the graph used to compute grads will be freed. If you would\\n            like to add more ops to the built graph after calling this method( :code:`backward` ), set the parameter\\n            :code:`retain_graph` to True, then the grads will be retained. Thus, setting it to False is much more memory-efficient.\\n            Defaults to False.\\n\\n    Returns:\\n        NoneType: None\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> x = paddle.to_tensor([[1, 2], [3, 4]], dtype='float32', stop_gradient=False)\\n            >>> y = paddle.to_tensor([[3, 2], [3, 4]], dtype='float32')\\n\\n            >>> grad_tensor1 = paddle.to_tensor([[1,2], [2, 3]], dtype='float32')\\n            >>> grad_tensor2 = paddle.to_tensor([[1,1], [1, 1]], dtype='float32')\\n\\n            >>> z1 = paddle.matmul(x, y)\\n            >>> z2 = paddle.matmul(x, y)\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, grad_tensor2], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, None], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2])\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[10., 14.],\\n             [10., 14.]])\\n\\n\\n    \"\n\n    def check_tensors(in_out_list, name):\n        assert in_out_list is not None, f'{name} should not be None'\n        if isinstance(in_out_list, (list, tuple)):\n            assert len(in_out_list) > 0, f'{name} connot be empty'\n            for each_var in in_out_list:\n                assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n            return in_out_list\n        else:\n            assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n            return [in_out_list]\n    tensors = check_tensors(tensors, 'tensors')\n    assert len(tensors) == len(set(tensors)), \"The argument 'tensors' of paddle.autograd.backward contains duplicate paddle.Tensor object.\"\n    if grad_tensors is not None:\n        if not isinstance(grad_tensors, (list, tuple)):\n            grad_tensors = [grad_tensors]\n        for each_tensor in grad_tensors:\n            if each_tensor is not None:\n                assert isinstance(each_tensor, (paddle.Tensor, core.eager.Tensor)), \"The argument 'grad_tensors' of paddle.autograd.backward is invalid, it can be 'None', 'paddle.Tensor' or 'list[None/paddle.Tensor]'.\"\n    else:\n        grad_tensors = []\n    if len(grad_tensors) > 0:\n        assert len(tensors) == len(grad_tensors), 'The length of grad_tensors must be equal to tensors'\n    assert isinstance(retain_graph, bool), 'retain_graph must be True or False'\n    core.eager.run_backward(tensors, grad_tensors, retain_graph)",
            "@framework.dygraph_only\ndef backward(tensors, grad_tensors=None, retain_graph=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Compute the backward gradients of given tensors.\\n\\n    Args:\\n        tensors(list of Tensors): the tensors which the gradient to be computed. The tensors can not contain the same tensor.\\n\\n        grad_tensors(list of Tensors of None, optional): the init gradients of the `tensors`` .If not None, it must have the same length with ``tensors`` ,\\n            and if any of the elements is None, then the init gradient is the default value which is filled with 1.0.\\n            If None, all the gradients of the ``tensors`` is the default value which is filled with 1.0.\\n            Defaults to None.\\n\\n        retain_graph(bool, optional): If False, the graph used to compute grads will be freed. If you would\\n            like to add more ops to the built graph after calling this method( :code:`backward` ), set the parameter\\n            :code:`retain_graph` to True, then the grads will be retained. Thus, setting it to False is much more memory-efficient.\\n            Defaults to False.\\n\\n    Returns:\\n        NoneType: None\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> x = paddle.to_tensor([[1, 2], [3, 4]], dtype='float32', stop_gradient=False)\\n            >>> y = paddle.to_tensor([[3, 2], [3, 4]], dtype='float32')\\n\\n            >>> grad_tensor1 = paddle.to_tensor([[1,2], [2, 3]], dtype='float32')\\n            >>> grad_tensor2 = paddle.to_tensor([[1,1], [1, 1]], dtype='float32')\\n\\n            >>> z1 = paddle.matmul(x, y)\\n            >>> z2 = paddle.matmul(x, y)\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, grad_tensor2], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, None], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2])\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[10., 14.],\\n             [10., 14.]])\\n\\n\\n    \"\n\n    def check_tensors(in_out_list, name):\n        assert in_out_list is not None, f'{name} should not be None'\n        if isinstance(in_out_list, (list, tuple)):\n            assert len(in_out_list) > 0, f'{name} connot be empty'\n            for each_var in in_out_list:\n                assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n            return in_out_list\n        else:\n            assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n            return [in_out_list]\n    tensors = check_tensors(tensors, 'tensors')\n    assert len(tensors) == len(set(tensors)), \"The argument 'tensors' of paddle.autograd.backward contains duplicate paddle.Tensor object.\"\n    if grad_tensors is not None:\n        if not isinstance(grad_tensors, (list, tuple)):\n            grad_tensors = [grad_tensors]\n        for each_tensor in grad_tensors:\n            if each_tensor is not None:\n                assert isinstance(each_tensor, (paddle.Tensor, core.eager.Tensor)), \"The argument 'grad_tensors' of paddle.autograd.backward is invalid, it can be 'None', 'paddle.Tensor' or 'list[None/paddle.Tensor]'.\"\n    else:\n        grad_tensors = []\n    if len(grad_tensors) > 0:\n        assert len(tensors) == len(grad_tensors), 'The length of grad_tensors must be equal to tensors'\n    assert isinstance(retain_graph, bool), 'retain_graph must be True or False'\n    core.eager.run_backward(tensors, grad_tensors, retain_graph)",
            "@framework.dygraph_only\ndef backward(tensors, grad_tensors=None, retain_graph=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Compute the backward gradients of given tensors.\\n\\n    Args:\\n        tensors(list of Tensors): the tensors which the gradient to be computed. The tensors can not contain the same tensor.\\n\\n        grad_tensors(list of Tensors of None, optional): the init gradients of the `tensors`` .If not None, it must have the same length with ``tensors`` ,\\n            and if any of the elements is None, then the init gradient is the default value which is filled with 1.0.\\n            If None, all the gradients of the ``tensors`` is the default value which is filled with 1.0.\\n            Defaults to None.\\n\\n        retain_graph(bool, optional): If False, the graph used to compute grads will be freed. If you would\\n            like to add more ops to the built graph after calling this method( :code:`backward` ), set the parameter\\n            :code:`retain_graph` to True, then the grads will be retained. Thus, setting it to False is much more memory-efficient.\\n            Defaults to False.\\n\\n    Returns:\\n        NoneType: None\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> x = paddle.to_tensor([[1, 2], [3, 4]], dtype='float32', stop_gradient=False)\\n            >>> y = paddle.to_tensor([[3, 2], [3, 4]], dtype='float32')\\n\\n            >>> grad_tensor1 = paddle.to_tensor([[1,2], [2, 3]], dtype='float32')\\n            >>> grad_tensor2 = paddle.to_tensor([[1,1], [1, 1]], dtype='float32')\\n\\n            >>> z1 = paddle.matmul(x, y)\\n            >>> z2 = paddle.matmul(x, y)\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, grad_tensor2], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, None], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2])\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[10., 14.],\\n             [10., 14.]])\\n\\n\\n    \"\n\n    def check_tensors(in_out_list, name):\n        assert in_out_list is not None, f'{name} should not be None'\n        if isinstance(in_out_list, (list, tuple)):\n            assert len(in_out_list) > 0, f'{name} connot be empty'\n            for each_var in in_out_list:\n                assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n            return in_out_list\n        else:\n            assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n            return [in_out_list]\n    tensors = check_tensors(tensors, 'tensors')\n    assert len(tensors) == len(set(tensors)), \"The argument 'tensors' of paddle.autograd.backward contains duplicate paddle.Tensor object.\"\n    if grad_tensors is not None:\n        if not isinstance(grad_tensors, (list, tuple)):\n            grad_tensors = [grad_tensors]\n        for each_tensor in grad_tensors:\n            if each_tensor is not None:\n                assert isinstance(each_tensor, (paddle.Tensor, core.eager.Tensor)), \"The argument 'grad_tensors' of paddle.autograd.backward is invalid, it can be 'None', 'paddle.Tensor' or 'list[None/paddle.Tensor]'.\"\n    else:\n        grad_tensors = []\n    if len(grad_tensors) > 0:\n        assert len(tensors) == len(grad_tensors), 'The length of grad_tensors must be equal to tensors'\n    assert isinstance(retain_graph, bool), 'retain_graph must be True or False'\n    core.eager.run_backward(tensors, grad_tensors, retain_graph)",
            "@framework.dygraph_only\ndef backward(tensors, grad_tensors=None, retain_graph=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Compute the backward gradients of given tensors.\\n\\n    Args:\\n        tensors(list of Tensors): the tensors which the gradient to be computed. The tensors can not contain the same tensor.\\n\\n        grad_tensors(list of Tensors of None, optional): the init gradients of the `tensors`` .If not None, it must have the same length with ``tensors`` ,\\n            and if any of the elements is None, then the init gradient is the default value which is filled with 1.0.\\n            If None, all the gradients of the ``tensors`` is the default value which is filled with 1.0.\\n            Defaults to None.\\n\\n        retain_graph(bool, optional): If False, the graph used to compute grads will be freed. If you would\\n            like to add more ops to the built graph after calling this method( :code:`backward` ), set the parameter\\n            :code:`retain_graph` to True, then the grads will be retained. Thus, setting it to False is much more memory-efficient.\\n            Defaults to False.\\n\\n    Returns:\\n        NoneType: None\\n\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n            >>> x = paddle.to_tensor([[1, 2], [3, 4]], dtype='float32', stop_gradient=False)\\n            >>> y = paddle.to_tensor([[3, 2], [3, 4]], dtype='float32')\\n\\n            >>> grad_tensor1 = paddle.to_tensor([[1,2], [2, 3]], dtype='float32')\\n            >>> grad_tensor2 = paddle.to_tensor([[1,1], [1, 1]], dtype='float32')\\n\\n            >>> z1 = paddle.matmul(x, y)\\n            >>> z2 = paddle.matmul(x, y)\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, grad_tensor2], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2], [grad_tensor1, None], True)\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[12., 18.],\\n             [17., 25.]])\\n\\n            >>> x.clear_grad()\\n\\n            >>> paddle.autograd.backward([z1, z2])\\n            >>> print(x.grad)\\n            Tensor(shape=[2, 2], dtype=float32, place=Place(cpu), stop_gradient=False,\\n            [[10., 14.],\\n             [10., 14.]])\\n\\n\\n    \"\n\n    def check_tensors(in_out_list, name):\n        assert in_out_list is not None, f'{name} should not be None'\n        if isinstance(in_out_list, (list, tuple)):\n            assert len(in_out_list) > 0, f'{name} connot be empty'\n            for each_var in in_out_list:\n                assert isinstance(each_var, (paddle.Tensor, core.eager.Tensor)), f'Elements of {name} must be paddle.Tensor'\n            return in_out_list\n        else:\n            assert isinstance(in_out_list, (paddle.Tensor, core.eager.Tensor)), f'{name} must be Tensor or list of Tensor'\n            return [in_out_list]\n    tensors = check_tensors(tensors, 'tensors')\n    assert len(tensors) == len(set(tensors)), \"The argument 'tensors' of paddle.autograd.backward contains duplicate paddle.Tensor object.\"\n    if grad_tensors is not None:\n        if not isinstance(grad_tensors, (list, tuple)):\n            grad_tensors = [grad_tensors]\n        for each_tensor in grad_tensors:\n            if each_tensor is not None:\n                assert isinstance(each_tensor, (paddle.Tensor, core.eager.Tensor)), \"The argument 'grad_tensors' of paddle.autograd.backward is invalid, it can be 'None', 'paddle.Tensor' or 'list[None/paddle.Tensor]'.\"\n    else:\n        grad_tensors = []\n    if len(grad_tensors) > 0:\n        assert len(tensors) == len(grad_tensors), 'The length of grad_tensors must be equal to tensors'\n    assert isinstance(retain_graph, bool), 'retain_graph must be True or False'\n    core.eager.run_backward(tensors, grad_tensors, retain_graph)"
        ]
    }
]