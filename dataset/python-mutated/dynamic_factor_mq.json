[
    {
        "func_name": "__init__",
        "original": "def __init__(self, factor_names, factor_order, endog_factor_map, state_offset, k_endog_Q):\n    self.factor_names = factor_names\n    self.k_factors = len(self.factor_names)\n    self.factor_order = factor_order\n    self.endog_factor_map = endog_factor_map.loc[:, factor_names]\n    self.state_offset = state_offset\n    self.k_endog_Q = k_endog_Q\n    if self.k_endog_Q > 0:\n        self._factor_order = max(5, self.factor_order)\n    else:\n        self._factor_order = self.factor_order\n    self.k_states = self.k_factors * self._factor_order\n    self['factors'] = self.factors\n    self['factors_ar'] = self.factors_ar\n    self['factors_ix'] = self.factors_ix\n    self['factors_L1'] = self.factors_L1\n    self['factors_L1_5'] = self.factors_L1_5",
        "mutated": [
            "def __init__(self, factor_names, factor_order, endog_factor_map, state_offset, k_endog_Q):\n    if False:\n        i = 10\n    self.factor_names = factor_names\n    self.k_factors = len(self.factor_names)\n    self.factor_order = factor_order\n    self.endog_factor_map = endog_factor_map.loc[:, factor_names]\n    self.state_offset = state_offset\n    self.k_endog_Q = k_endog_Q\n    if self.k_endog_Q > 0:\n        self._factor_order = max(5, self.factor_order)\n    else:\n        self._factor_order = self.factor_order\n    self.k_states = self.k_factors * self._factor_order\n    self['factors'] = self.factors\n    self['factors_ar'] = self.factors_ar\n    self['factors_ix'] = self.factors_ix\n    self['factors_L1'] = self.factors_L1\n    self['factors_L1_5'] = self.factors_L1_5",
            "def __init__(self, factor_names, factor_order, endog_factor_map, state_offset, k_endog_Q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.factor_names = factor_names\n    self.k_factors = len(self.factor_names)\n    self.factor_order = factor_order\n    self.endog_factor_map = endog_factor_map.loc[:, factor_names]\n    self.state_offset = state_offset\n    self.k_endog_Q = k_endog_Q\n    if self.k_endog_Q > 0:\n        self._factor_order = max(5, self.factor_order)\n    else:\n        self._factor_order = self.factor_order\n    self.k_states = self.k_factors * self._factor_order\n    self['factors'] = self.factors\n    self['factors_ar'] = self.factors_ar\n    self['factors_ix'] = self.factors_ix\n    self['factors_L1'] = self.factors_L1\n    self['factors_L1_5'] = self.factors_L1_5",
            "def __init__(self, factor_names, factor_order, endog_factor_map, state_offset, k_endog_Q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.factor_names = factor_names\n    self.k_factors = len(self.factor_names)\n    self.factor_order = factor_order\n    self.endog_factor_map = endog_factor_map.loc[:, factor_names]\n    self.state_offset = state_offset\n    self.k_endog_Q = k_endog_Q\n    if self.k_endog_Q > 0:\n        self._factor_order = max(5, self.factor_order)\n    else:\n        self._factor_order = self.factor_order\n    self.k_states = self.k_factors * self._factor_order\n    self['factors'] = self.factors\n    self['factors_ar'] = self.factors_ar\n    self['factors_ix'] = self.factors_ix\n    self['factors_L1'] = self.factors_L1\n    self['factors_L1_5'] = self.factors_L1_5",
            "def __init__(self, factor_names, factor_order, endog_factor_map, state_offset, k_endog_Q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.factor_names = factor_names\n    self.k_factors = len(self.factor_names)\n    self.factor_order = factor_order\n    self.endog_factor_map = endog_factor_map.loc[:, factor_names]\n    self.state_offset = state_offset\n    self.k_endog_Q = k_endog_Q\n    if self.k_endog_Q > 0:\n        self._factor_order = max(5, self.factor_order)\n    else:\n        self._factor_order = self.factor_order\n    self.k_states = self.k_factors * self._factor_order\n    self['factors'] = self.factors\n    self['factors_ar'] = self.factors_ar\n    self['factors_ix'] = self.factors_ix\n    self['factors_L1'] = self.factors_L1\n    self['factors_L1_5'] = self.factors_L1_5",
            "def __init__(self, factor_names, factor_order, endog_factor_map, state_offset, k_endog_Q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.factor_names = factor_names\n    self.k_factors = len(self.factor_names)\n    self.factor_order = factor_order\n    self.endog_factor_map = endog_factor_map.loc[:, factor_names]\n    self.state_offset = state_offset\n    self.k_endog_Q = k_endog_Q\n    if self.k_endog_Q > 0:\n        self._factor_order = max(5, self.factor_order)\n    else:\n        self._factor_order = self.factor_order\n    self.k_states = self.k_factors * self._factor_order\n    self['factors'] = self.factors\n    self['factors_ar'] = self.factors_ar\n    self['factors_ix'] = self.factors_ix\n    self['factors_L1'] = self.factors_L1\n    self['factors_L1_5'] = self.factors_L1_5"
        ]
    },
    {
        "func_name": "factors_ix",
        "original": "@property\ndef factors_ix(self):\n    \"\"\"Factor state index array, shaped (k_factors, lags).\"\"\"\n    o = self.state_offset\n    return np.reshape(o + np.arange(self.k_factors * self._factor_order), (self._factor_order, self.k_factors)).T",
        "mutated": [
            "@property\ndef factors_ix(self):\n    if False:\n        i = 10\n    'Factor state index array, shaped (k_factors, lags).'\n    o = self.state_offset\n    return np.reshape(o + np.arange(self.k_factors * self._factor_order), (self._factor_order, self.k_factors)).T",
            "@property\ndef factors_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Factor state index array, shaped (k_factors, lags).'\n    o = self.state_offset\n    return np.reshape(o + np.arange(self.k_factors * self._factor_order), (self._factor_order, self.k_factors)).T",
            "@property\ndef factors_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Factor state index array, shaped (k_factors, lags).'\n    o = self.state_offset\n    return np.reshape(o + np.arange(self.k_factors * self._factor_order), (self._factor_order, self.k_factors)).T",
            "@property\ndef factors_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Factor state index array, shaped (k_factors, lags).'\n    o = self.state_offset\n    return np.reshape(o + np.arange(self.k_factors * self._factor_order), (self._factor_order, self.k_factors)).T",
            "@property\ndef factors_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Factor state index array, shaped (k_factors, lags).'\n    o = self.state_offset\n    return np.reshape(o + np.arange(self.k_factors * self._factor_order), (self._factor_order, self.k_factors)).T"
        ]
    },
    {
        "func_name": "factors",
        "original": "@property\ndef factors(self):\n    \"\"\"Factors and all lags in the state vector (max(5, p)).\"\"\"\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self._factor_order]",
        "mutated": [
            "@property\ndef factors(self):\n    if False:\n        i = 10\n    'Factors and all lags in the state vector (max(5, p)).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self._factor_order]",
            "@property\ndef factors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Factors and all lags in the state vector (max(5, p)).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self._factor_order]",
            "@property\ndef factors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Factors and all lags in the state vector (max(5, p)).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self._factor_order]",
            "@property\ndef factors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Factors and all lags in the state vector (max(5, p)).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self._factor_order]",
            "@property\ndef factors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Factors and all lags in the state vector (max(5, p)).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self._factor_order]"
        ]
    },
    {
        "func_name": "factors_ar",
        "original": "@property\ndef factors_ar(self):\n    \"\"\"Factors and all lags used in the factor autoregression (p).\"\"\"\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self.factor_order]",
        "mutated": [
            "@property\ndef factors_ar(self):\n    if False:\n        i = 10\n    'Factors and all lags used in the factor autoregression (p).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self.factor_order]",
            "@property\ndef factors_ar(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Factors and all lags used in the factor autoregression (p).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self.factor_order]",
            "@property\ndef factors_ar(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Factors and all lags used in the factor autoregression (p).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self.factor_order]",
            "@property\ndef factors_ar(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Factors and all lags used in the factor autoregression (p).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self.factor_order]",
            "@property\ndef factors_ar(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Factors and all lags used in the factor autoregression (p).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * self.factor_order]"
        ]
    },
    {
        "func_name": "factors_L1",
        "original": "@property\ndef factors_L1(self):\n    \"\"\"Factors (first block / lag only).\"\"\"\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors]",
        "mutated": [
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n    'Factors (first block / lag only).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors]",
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Factors (first block / lag only).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors]",
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Factors (first block / lag only).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors]",
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Factors (first block / lag only).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors]",
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Factors (first block / lag only).'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors]"
        ]
    },
    {
        "func_name": "factors_L1_5",
        "original": "@property\ndef factors_L1_5(self):\n    \"\"\"Factors plus four lags.\"\"\"\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * 5]",
        "mutated": [
            "@property\ndef factors_L1_5(self):\n    if False:\n        i = 10\n    'Factors plus four lags.'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * 5]",
            "@property\ndef factors_L1_5(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Factors plus four lags.'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * 5]",
            "@property\ndef factors_L1_5(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Factors plus four lags.'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * 5]",
            "@property\ndef factors_L1_5(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Factors plus four lags.'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * 5]",
            "@property\ndef factors_L1_5(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Factors plus four lags.'\n    o = self.state_offset\n    return np.s_[o:o + self.k_factors * 5]"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, k_endog_M, k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1):\n    self.k_endog_M = k_endog_M\n    self.k_endog_Q = k_endog_Q\n    self.k_endog = self.k_endog_M + self.k_endog_Q\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    factors_is_int = np.issubdtype(type(factors), np.integer)\n    factors_is_list = isinstance(factors, (list, tuple))\n    orders_is_int = np.issubdtype(type(factor_orders), np.integer)\n    if factor_multiplicities is None:\n        factor_multiplicities = 1\n    mult_is_int = np.issubdtype(type(factor_multiplicities), np.integer)\n    if not (factors_is_int or factors_is_list or isinstance(factors, dict)):\n        raise ValueError('`factors` argument must an integer number of factors, a list of global factor names, or a dictionary, mapping observed variables to factors.')\n    if not (orders_is_int or isinstance(factor_orders, dict)):\n        raise ValueError('`factor_orders` argument must either be an integer or a dictionary.')\n    if not (mult_is_int or isinstance(factor_multiplicities, dict)):\n        raise ValueError('`factor_multiplicities` argument must either be an integer or a dictionary.')\n    if factors_is_int or factors_is_list:\n        if factors_is_int and factors == 0 or (factors_is_list and len(factors) == 0):\n            raise ValueError('The model must contain at least one factor.')\n        if factors_is_list:\n            factor_names = list(factors)\n        else:\n            factor_names = [f'{i}' for i in range(factors)]\n        factors = {name: factor_names[:] for name in endog_names}\n    _factor_names = []\n    for val in factors.values():\n        _factor_names.extend(val)\n    factor_names = set(_factor_names)\n    if orders_is_int:\n        factor_orders = {factor_name: factor_orders for factor_name in factor_names}\n    if mult_is_int:\n        factor_multiplicities = {factor_name: factor_multiplicities for factor_name in factor_names}\n    (factors, factor_orders) = self._apply_factor_multiplicities(factors, factor_orders, factor_multiplicities)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._construct_endog_factor_map(factors, endog_names)\n    self.k_factors = self.endog_factor_map.shape[1]\n    if self.k_factors > self.k_endog_M:\n        raise ValueError(f'Number of factors ({self.k_factors}) cannot be greater than the number of monthly endogenous variables ({self.k_endog_M}).')\n    self.loading_counts = self.endog_factor_map.sum(axis=0).rename('count').reset_index().sort_values(['count', 'factor'], ascending=[False, True]).set_index('factor')\n    block_loading_counts = {block: np.atleast_1d(self.loading_counts.loc[list(block), 'count']).mean(axis=0) for block in factor_orders.keys()}\n    ix = pd.Index(block_loading_counts.keys(), tupleize_cols=False, name='block')\n    self.block_loading_counts = pd.Series(list(block_loading_counts.values()), index=ix, name='count').to_frame().sort_values(['count', 'block'], ascending=[False, True])['count']\n    ix = pd.Index(factor_orders.keys(), tupleize_cols=False, name='block')\n    self.factor_block_orders = pd.Series(list(factor_orders.values()), index=ix, name='order')\n    if orders_is_int:\n        keys = self.block_loading_counts.keys()\n        self.factor_block_orders = self.factor_block_orders.loc[keys]\n        self.factor_block_orders.index.name = 'block'\n    factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    missing = [name for name in self.endog_factor_map.columns if name not in factor_names.tolist()]\n    if len(missing):\n        ix = pd.Index([(factor_name,) for factor_name in missing], tupleize_cols=False, name='block')\n        default_block_orders = pd.Series(np.ones(len(ix), dtype=int), index=ix, name='order')\n        self.factor_block_orders = self.factor_block_orders.append(default_block_orders)\n        factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    duplicates = factor_names.duplicated()\n    if duplicates.any():\n        duplicate_names = set(factor_names[duplicates])\n        raise ValueError(f'Each factor can be assigned to at most one block of factors in `factor_orders`. Duplicate entries for {duplicate_names}')\n    self.factor_names = factor_names.tolist()\n    self.max_factor_order = np.max(self.factor_block_orders)\n    self.endog_factor_map = self.endog_factor_map.loc[endog_names, factor_names]\n    self.k_states_factors = 0\n    self.k_posdef_factors = 0\n    state_offset = 0\n    self.factor_blocks = []\n    for (factor_names, factor_order) in self.factor_block_orders.items():\n        block = FactorBlock(factor_names, factor_order, self.endog_factor_map, state_offset, self.k_endog_Q)\n        self.k_states_factors += block.k_states\n        self.k_posdef_factors += block.k_factors\n        state_offset += block.k_states\n        self.factor_blocks.append(block)\n    self.k_states_idio_M = self.k_endog_M if idiosyncratic_ar1 else 0\n    self.k_states_idio_Q = self.k_endog_Q * 5\n    self.k_states_idio = self.k_states_idio_M + self.k_states_idio_Q\n    self.k_posdef_idio_M = self.k_endog_M if self.idiosyncratic_ar1 else 0\n    self.k_posdef_idio_Q = self.k_endog_Q\n    self.k_posdef_idio = self.k_posdef_idio_M + self.k_posdef_idio_Q\n    self.k_states = self.k_states_factors + self.k_states_idio\n    self.k_posdef = self.k_posdef_factors + self.k_posdef_idio\n    self._endog_factor_iloc = None",
        "mutated": [
            "def __init__(self, k_endog_M, k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1):\n    if False:\n        i = 10\n    self.k_endog_M = k_endog_M\n    self.k_endog_Q = k_endog_Q\n    self.k_endog = self.k_endog_M + self.k_endog_Q\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    factors_is_int = np.issubdtype(type(factors), np.integer)\n    factors_is_list = isinstance(factors, (list, tuple))\n    orders_is_int = np.issubdtype(type(factor_orders), np.integer)\n    if factor_multiplicities is None:\n        factor_multiplicities = 1\n    mult_is_int = np.issubdtype(type(factor_multiplicities), np.integer)\n    if not (factors_is_int or factors_is_list or isinstance(factors, dict)):\n        raise ValueError('`factors` argument must an integer number of factors, a list of global factor names, or a dictionary, mapping observed variables to factors.')\n    if not (orders_is_int or isinstance(factor_orders, dict)):\n        raise ValueError('`factor_orders` argument must either be an integer or a dictionary.')\n    if not (mult_is_int or isinstance(factor_multiplicities, dict)):\n        raise ValueError('`factor_multiplicities` argument must either be an integer or a dictionary.')\n    if factors_is_int or factors_is_list:\n        if factors_is_int and factors == 0 or (factors_is_list and len(factors) == 0):\n            raise ValueError('The model must contain at least one factor.')\n        if factors_is_list:\n            factor_names = list(factors)\n        else:\n            factor_names = [f'{i}' for i in range(factors)]\n        factors = {name: factor_names[:] for name in endog_names}\n    _factor_names = []\n    for val in factors.values():\n        _factor_names.extend(val)\n    factor_names = set(_factor_names)\n    if orders_is_int:\n        factor_orders = {factor_name: factor_orders for factor_name in factor_names}\n    if mult_is_int:\n        factor_multiplicities = {factor_name: factor_multiplicities for factor_name in factor_names}\n    (factors, factor_orders) = self._apply_factor_multiplicities(factors, factor_orders, factor_multiplicities)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._construct_endog_factor_map(factors, endog_names)\n    self.k_factors = self.endog_factor_map.shape[1]\n    if self.k_factors > self.k_endog_M:\n        raise ValueError(f'Number of factors ({self.k_factors}) cannot be greater than the number of monthly endogenous variables ({self.k_endog_M}).')\n    self.loading_counts = self.endog_factor_map.sum(axis=0).rename('count').reset_index().sort_values(['count', 'factor'], ascending=[False, True]).set_index('factor')\n    block_loading_counts = {block: np.atleast_1d(self.loading_counts.loc[list(block), 'count']).mean(axis=0) for block in factor_orders.keys()}\n    ix = pd.Index(block_loading_counts.keys(), tupleize_cols=False, name='block')\n    self.block_loading_counts = pd.Series(list(block_loading_counts.values()), index=ix, name='count').to_frame().sort_values(['count', 'block'], ascending=[False, True])['count']\n    ix = pd.Index(factor_orders.keys(), tupleize_cols=False, name='block')\n    self.factor_block_orders = pd.Series(list(factor_orders.values()), index=ix, name='order')\n    if orders_is_int:\n        keys = self.block_loading_counts.keys()\n        self.factor_block_orders = self.factor_block_orders.loc[keys]\n        self.factor_block_orders.index.name = 'block'\n    factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    missing = [name for name in self.endog_factor_map.columns if name not in factor_names.tolist()]\n    if len(missing):\n        ix = pd.Index([(factor_name,) for factor_name in missing], tupleize_cols=False, name='block')\n        default_block_orders = pd.Series(np.ones(len(ix), dtype=int), index=ix, name='order')\n        self.factor_block_orders = self.factor_block_orders.append(default_block_orders)\n        factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    duplicates = factor_names.duplicated()\n    if duplicates.any():\n        duplicate_names = set(factor_names[duplicates])\n        raise ValueError(f'Each factor can be assigned to at most one block of factors in `factor_orders`. Duplicate entries for {duplicate_names}')\n    self.factor_names = factor_names.tolist()\n    self.max_factor_order = np.max(self.factor_block_orders)\n    self.endog_factor_map = self.endog_factor_map.loc[endog_names, factor_names]\n    self.k_states_factors = 0\n    self.k_posdef_factors = 0\n    state_offset = 0\n    self.factor_blocks = []\n    for (factor_names, factor_order) in self.factor_block_orders.items():\n        block = FactorBlock(factor_names, factor_order, self.endog_factor_map, state_offset, self.k_endog_Q)\n        self.k_states_factors += block.k_states\n        self.k_posdef_factors += block.k_factors\n        state_offset += block.k_states\n        self.factor_blocks.append(block)\n    self.k_states_idio_M = self.k_endog_M if idiosyncratic_ar1 else 0\n    self.k_states_idio_Q = self.k_endog_Q * 5\n    self.k_states_idio = self.k_states_idio_M + self.k_states_idio_Q\n    self.k_posdef_idio_M = self.k_endog_M if self.idiosyncratic_ar1 else 0\n    self.k_posdef_idio_Q = self.k_endog_Q\n    self.k_posdef_idio = self.k_posdef_idio_M + self.k_posdef_idio_Q\n    self.k_states = self.k_states_factors + self.k_states_idio\n    self.k_posdef = self.k_posdef_factors + self.k_posdef_idio\n    self._endog_factor_iloc = None",
            "def __init__(self, k_endog_M, k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.k_endog_M = k_endog_M\n    self.k_endog_Q = k_endog_Q\n    self.k_endog = self.k_endog_M + self.k_endog_Q\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    factors_is_int = np.issubdtype(type(factors), np.integer)\n    factors_is_list = isinstance(factors, (list, tuple))\n    orders_is_int = np.issubdtype(type(factor_orders), np.integer)\n    if factor_multiplicities is None:\n        factor_multiplicities = 1\n    mult_is_int = np.issubdtype(type(factor_multiplicities), np.integer)\n    if not (factors_is_int or factors_is_list or isinstance(factors, dict)):\n        raise ValueError('`factors` argument must an integer number of factors, a list of global factor names, or a dictionary, mapping observed variables to factors.')\n    if not (orders_is_int or isinstance(factor_orders, dict)):\n        raise ValueError('`factor_orders` argument must either be an integer or a dictionary.')\n    if not (mult_is_int or isinstance(factor_multiplicities, dict)):\n        raise ValueError('`factor_multiplicities` argument must either be an integer or a dictionary.')\n    if factors_is_int or factors_is_list:\n        if factors_is_int and factors == 0 or (factors_is_list and len(factors) == 0):\n            raise ValueError('The model must contain at least one factor.')\n        if factors_is_list:\n            factor_names = list(factors)\n        else:\n            factor_names = [f'{i}' for i in range(factors)]\n        factors = {name: factor_names[:] for name in endog_names}\n    _factor_names = []\n    for val in factors.values():\n        _factor_names.extend(val)\n    factor_names = set(_factor_names)\n    if orders_is_int:\n        factor_orders = {factor_name: factor_orders for factor_name in factor_names}\n    if mult_is_int:\n        factor_multiplicities = {factor_name: factor_multiplicities for factor_name in factor_names}\n    (factors, factor_orders) = self._apply_factor_multiplicities(factors, factor_orders, factor_multiplicities)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._construct_endog_factor_map(factors, endog_names)\n    self.k_factors = self.endog_factor_map.shape[1]\n    if self.k_factors > self.k_endog_M:\n        raise ValueError(f'Number of factors ({self.k_factors}) cannot be greater than the number of monthly endogenous variables ({self.k_endog_M}).')\n    self.loading_counts = self.endog_factor_map.sum(axis=0).rename('count').reset_index().sort_values(['count', 'factor'], ascending=[False, True]).set_index('factor')\n    block_loading_counts = {block: np.atleast_1d(self.loading_counts.loc[list(block), 'count']).mean(axis=0) for block in factor_orders.keys()}\n    ix = pd.Index(block_loading_counts.keys(), tupleize_cols=False, name='block')\n    self.block_loading_counts = pd.Series(list(block_loading_counts.values()), index=ix, name='count').to_frame().sort_values(['count', 'block'], ascending=[False, True])['count']\n    ix = pd.Index(factor_orders.keys(), tupleize_cols=False, name='block')\n    self.factor_block_orders = pd.Series(list(factor_orders.values()), index=ix, name='order')\n    if orders_is_int:\n        keys = self.block_loading_counts.keys()\n        self.factor_block_orders = self.factor_block_orders.loc[keys]\n        self.factor_block_orders.index.name = 'block'\n    factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    missing = [name for name in self.endog_factor_map.columns if name not in factor_names.tolist()]\n    if len(missing):\n        ix = pd.Index([(factor_name,) for factor_name in missing], tupleize_cols=False, name='block')\n        default_block_orders = pd.Series(np.ones(len(ix), dtype=int), index=ix, name='order')\n        self.factor_block_orders = self.factor_block_orders.append(default_block_orders)\n        factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    duplicates = factor_names.duplicated()\n    if duplicates.any():\n        duplicate_names = set(factor_names[duplicates])\n        raise ValueError(f'Each factor can be assigned to at most one block of factors in `factor_orders`. Duplicate entries for {duplicate_names}')\n    self.factor_names = factor_names.tolist()\n    self.max_factor_order = np.max(self.factor_block_orders)\n    self.endog_factor_map = self.endog_factor_map.loc[endog_names, factor_names]\n    self.k_states_factors = 0\n    self.k_posdef_factors = 0\n    state_offset = 0\n    self.factor_blocks = []\n    for (factor_names, factor_order) in self.factor_block_orders.items():\n        block = FactorBlock(factor_names, factor_order, self.endog_factor_map, state_offset, self.k_endog_Q)\n        self.k_states_factors += block.k_states\n        self.k_posdef_factors += block.k_factors\n        state_offset += block.k_states\n        self.factor_blocks.append(block)\n    self.k_states_idio_M = self.k_endog_M if idiosyncratic_ar1 else 0\n    self.k_states_idio_Q = self.k_endog_Q * 5\n    self.k_states_idio = self.k_states_idio_M + self.k_states_idio_Q\n    self.k_posdef_idio_M = self.k_endog_M if self.idiosyncratic_ar1 else 0\n    self.k_posdef_idio_Q = self.k_endog_Q\n    self.k_posdef_idio = self.k_posdef_idio_M + self.k_posdef_idio_Q\n    self.k_states = self.k_states_factors + self.k_states_idio\n    self.k_posdef = self.k_posdef_factors + self.k_posdef_idio\n    self._endog_factor_iloc = None",
            "def __init__(self, k_endog_M, k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.k_endog_M = k_endog_M\n    self.k_endog_Q = k_endog_Q\n    self.k_endog = self.k_endog_M + self.k_endog_Q\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    factors_is_int = np.issubdtype(type(factors), np.integer)\n    factors_is_list = isinstance(factors, (list, tuple))\n    orders_is_int = np.issubdtype(type(factor_orders), np.integer)\n    if factor_multiplicities is None:\n        factor_multiplicities = 1\n    mult_is_int = np.issubdtype(type(factor_multiplicities), np.integer)\n    if not (factors_is_int or factors_is_list or isinstance(factors, dict)):\n        raise ValueError('`factors` argument must an integer number of factors, a list of global factor names, or a dictionary, mapping observed variables to factors.')\n    if not (orders_is_int or isinstance(factor_orders, dict)):\n        raise ValueError('`factor_orders` argument must either be an integer or a dictionary.')\n    if not (mult_is_int or isinstance(factor_multiplicities, dict)):\n        raise ValueError('`factor_multiplicities` argument must either be an integer or a dictionary.')\n    if factors_is_int or factors_is_list:\n        if factors_is_int and factors == 0 or (factors_is_list and len(factors) == 0):\n            raise ValueError('The model must contain at least one factor.')\n        if factors_is_list:\n            factor_names = list(factors)\n        else:\n            factor_names = [f'{i}' for i in range(factors)]\n        factors = {name: factor_names[:] for name in endog_names}\n    _factor_names = []\n    for val in factors.values():\n        _factor_names.extend(val)\n    factor_names = set(_factor_names)\n    if orders_is_int:\n        factor_orders = {factor_name: factor_orders for factor_name in factor_names}\n    if mult_is_int:\n        factor_multiplicities = {factor_name: factor_multiplicities for factor_name in factor_names}\n    (factors, factor_orders) = self._apply_factor_multiplicities(factors, factor_orders, factor_multiplicities)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._construct_endog_factor_map(factors, endog_names)\n    self.k_factors = self.endog_factor_map.shape[1]\n    if self.k_factors > self.k_endog_M:\n        raise ValueError(f'Number of factors ({self.k_factors}) cannot be greater than the number of monthly endogenous variables ({self.k_endog_M}).')\n    self.loading_counts = self.endog_factor_map.sum(axis=0).rename('count').reset_index().sort_values(['count', 'factor'], ascending=[False, True]).set_index('factor')\n    block_loading_counts = {block: np.atleast_1d(self.loading_counts.loc[list(block), 'count']).mean(axis=0) for block in factor_orders.keys()}\n    ix = pd.Index(block_loading_counts.keys(), tupleize_cols=False, name='block')\n    self.block_loading_counts = pd.Series(list(block_loading_counts.values()), index=ix, name='count').to_frame().sort_values(['count', 'block'], ascending=[False, True])['count']\n    ix = pd.Index(factor_orders.keys(), tupleize_cols=False, name='block')\n    self.factor_block_orders = pd.Series(list(factor_orders.values()), index=ix, name='order')\n    if orders_is_int:\n        keys = self.block_loading_counts.keys()\n        self.factor_block_orders = self.factor_block_orders.loc[keys]\n        self.factor_block_orders.index.name = 'block'\n    factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    missing = [name for name in self.endog_factor_map.columns if name not in factor_names.tolist()]\n    if len(missing):\n        ix = pd.Index([(factor_name,) for factor_name in missing], tupleize_cols=False, name='block')\n        default_block_orders = pd.Series(np.ones(len(ix), dtype=int), index=ix, name='order')\n        self.factor_block_orders = self.factor_block_orders.append(default_block_orders)\n        factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    duplicates = factor_names.duplicated()\n    if duplicates.any():\n        duplicate_names = set(factor_names[duplicates])\n        raise ValueError(f'Each factor can be assigned to at most one block of factors in `factor_orders`. Duplicate entries for {duplicate_names}')\n    self.factor_names = factor_names.tolist()\n    self.max_factor_order = np.max(self.factor_block_orders)\n    self.endog_factor_map = self.endog_factor_map.loc[endog_names, factor_names]\n    self.k_states_factors = 0\n    self.k_posdef_factors = 0\n    state_offset = 0\n    self.factor_blocks = []\n    for (factor_names, factor_order) in self.factor_block_orders.items():\n        block = FactorBlock(factor_names, factor_order, self.endog_factor_map, state_offset, self.k_endog_Q)\n        self.k_states_factors += block.k_states\n        self.k_posdef_factors += block.k_factors\n        state_offset += block.k_states\n        self.factor_blocks.append(block)\n    self.k_states_idio_M = self.k_endog_M if idiosyncratic_ar1 else 0\n    self.k_states_idio_Q = self.k_endog_Q * 5\n    self.k_states_idio = self.k_states_idio_M + self.k_states_idio_Q\n    self.k_posdef_idio_M = self.k_endog_M if self.idiosyncratic_ar1 else 0\n    self.k_posdef_idio_Q = self.k_endog_Q\n    self.k_posdef_idio = self.k_posdef_idio_M + self.k_posdef_idio_Q\n    self.k_states = self.k_states_factors + self.k_states_idio\n    self.k_posdef = self.k_posdef_factors + self.k_posdef_idio\n    self._endog_factor_iloc = None",
            "def __init__(self, k_endog_M, k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.k_endog_M = k_endog_M\n    self.k_endog_Q = k_endog_Q\n    self.k_endog = self.k_endog_M + self.k_endog_Q\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    factors_is_int = np.issubdtype(type(factors), np.integer)\n    factors_is_list = isinstance(factors, (list, tuple))\n    orders_is_int = np.issubdtype(type(factor_orders), np.integer)\n    if factor_multiplicities is None:\n        factor_multiplicities = 1\n    mult_is_int = np.issubdtype(type(factor_multiplicities), np.integer)\n    if not (factors_is_int or factors_is_list or isinstance(factors, dict)):\n        raise ValueError('`factors` argument must an integer number of factors, a list of global factor names, or a dictionary, mapping observed variables to factors.')\n    if not (orders_is_int or isinstance(factor_orders, dict)):\n        raise ValueError('`factor_orders` argument must either be an integer or a dictionary.')\n    if not (mult_is_int or isinstance(factor_multiplicities, dict)):\n        raise ValueError('`factor_multiplicities` argument must either be an integer or a dictionary.')\n    if factors_is_int or factors_is_list:\n        if factors_is_int and factors == 0 or (factors_is_list and len(factors) == 0):\n            raise ValueError('The model must contain at least one factor.')\n        if factors_is_list:\n            factor_names = list(factors)\n        else:\n            factor_names = [f'{i}' for i in range(factors)]\n        factors = {name: factor_names[:] for name in endog_names}\n    _factor_names = []\n    for val in factors.values():\n        _factor_names.extend(val)\n    factor_names = set(_factor_names)\n    if orders_is_int:\n        factor_orders = {factor_name: factor_orders for factor_name in factor_names}\n    if mult_is_int:\n        factor_multiplicities = {factor_name: factor_multiplicities for factor_name in factor_names}\n    (factors, factor_orders) = self._apply_factor_multiplicities(factors, factor_orders, factor_multiplicities)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._construct_endog_factor_map(factors, endog_names)\n    self.k_factors = self.endog_factor_map.shape[1]\n    if self.k_factors > self.k_endog_M:\n        raise ValueError(f'Number of factors ({self.k_factors}) cannot be greater than the number of monthly endogenous variables ({self.k_endog_M}).')\n    self.loading_counts = self.endog_factor_map.sum(axis=0).rename('count').reset_index().sort_values(['count', 'factor'], ascending=[False, True]).set_index('factor')\n    block_loading_counts = {block: np.atleast_1d(self.loading_counts.loc[list(block), 'count']).mean(axis=0) for block in factor_orders.keys()}\n    ix = pd.Index(block_loading_counts.keys(), tupleize_cols=False, name='block')\n    self.block_loading_counts = pd.Series(list(block_loading_counts.values()), index=ix, name='count').to_frame().sort_values(['count', 'block'], ascending=[False, True])['count']\n    ix = pd.Index(factor_orders.keys(), tupleize_cols=False, name='block')\n    self.factor_block_orders = pd.Series(list(factor_orders.values()), index=ix, name='order')\n    if orders_is_int:\n        keys = self.block_loading_counts.keys()\n        self.factor_block_orders = self.factor_block_orders.loc[keys]\n        self.factor_block_orders.index.name = 'block'\n    factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    missing = [name for name in self.endog_factor_map.columns if name not in factor_names.tolist()]\n    if len(missing):\n        ix = pd.Index([(factor_name,) for factor_name in missing], tupleize_cols=False, name='block')\n        default_block_orders = pd.Series(np.ones(len(ix), dtype=int), index=ix, name='order')\n        self.factor_block_orders = self.factor_block_orders.append(default_block_orders)\n        factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    duplicates = factor_names.duplicated()\n    if duplicates.any():\n        duplicate_names = set(factor_names[duplicates])\n        raise ValueError(f'Each factor can be assigned to at most one block of factors in `factor_orders`. Duplicate entries for {duplicate_names}')\n    self.factor_names = factor_names.tolist()\n    self.max_factor_order = np.max(self.factor_block_orders)\n    self.endog_factor_map = self.endog_factor_map.loc[endog_names, factor_names]\n    self.k_states_factors = 0\n    self.k_posdef_factors = 0\n    state_offset = 0\n    self.factor_blocks = []\n    for (factor_names, factor_order) in self.factor_block_orders.items():\n        block = FactorBlock(factor_names, factor_order, self.endog_factor_map, state_offset, self.k_endog_Q)\n        self.k_states_factors += block.k_states\n        self.k_posdef_factors += block.k_factors\n        state_offset += block.k_states\n        self.factor_blocks.append(block)\n    self.k_states_idio_M = self.k_endog_M if idiosyncratic_ar1 else 0\n    self.k_states_idio_Q = self.k_endog_Q * 5\n    self.k_states_idio = self.k_states_idio_M + self.k_states_idio_Q\n    self.k_posdef_idio_M = self.k_endog_M if self.idiosyncratic_ar1 else 0\n    self.k_posdef_idio_Q = self.k_endog_Q\n    self.k_posdef_idio = self.k_posdef_idio_M + self.k_posdef_idio_Q\n    self.k_states = self.k_states_factors + self.k_states_idio\n    self.k_posdef = self.k_posdef_factors + self.k_posdef_idio\n    self._endog_factor_iloc = None",
            "def __init__(self, k_endog_M, k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.k_endog_M = k_endog_M\n    self.k_endog_Q = k_endog_Q\n    self.k_endog = self.k_endog_M + self.k_endog_Q\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    factors_is_int = np.issubdtype(type(factors), np.integer)\n    factors_is_list = isinstance(factors, (list, tuple))\n    orders_is_int = np.issubdtype(type(factor_orders), np.integer)\n    if factor_multiplicities is None:\n        factor_multiplicities = 1\n    mult_is_int = np.issubdtype(type(factor_multiplicities), np.integer)\n    if not (factors_is_int or factors_is_list or isinstance(factors, dict)):\n        raise ValueError('`factors` argument must an integer number of factors, a list of global factor names, or a dictionary, mapping observed variables to factors.')\n    if not (orders_is_int or isinstance(factor_orders, dict)):\n        raise ValueError('`factor_orders` argument must either be an integer or a dictionary.')\n    if not (mult_is_int or isinstance(factor_multiplicities, dict)):\n        raise ValueError('`factor_multiplicities` argument must either be an integer or a dictionary.')\n    if factors_is_int or factors_is_list:\n        if factors_is_int and factors == 0 or (factors_is_list and len(factors) == 0):\n            raise ValueError('The model must contain at least one factor.')\n        if factors_is_list:\n            factor_names = list(factors)\n        else:\n            factor_names = [f'{i}' for i in range(factors)]\n        factors = {name: factor_names[:] for name in endog_names}\n    _factor_names = []\n    for val in factors.values():\n        _factor_names.extend(val)\n    factor_names = set(_factor_names)\n    if orders_is_int:\n        factor_orders = {factor_name: factor_orders for factor_name in factor_names}\n    if mult_is_int:\n        factor_multiplicities = {factor_name: factor_multiplicities for factor_name in factor_names}\n    (factors, factor_orders) = self._apply_factor_multiplicities(factors, factor_orders, factor_multiplicities)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._construct_endog_factor_map(factors, endog_names)\n    self.k_factors = self.endog_factor_map.shape[1]\n    if self.k_factors > self.k_endog_M:\n        raise ValueError(f'Number of factors ({self.k_factors}) cannot be greater than the number of monthly endogenous variables ({self.k_endog_M}).')\n    self.loading_counts = self.endog_factor_map.sum(axis=0).rename('count').reset_index().sort_values(['count', 'factor'], ascending=[False, True]).set_index('factor')\n    block_loading_counts = {block: np.atleast_1d(self.loading_counts.loc[list(block), 'count']).mean(axis=0) for block in factor_orders.keys()}\n    ix = pd.Index(block_loading_counts.keys(), tupleize_cols=False, name='block')\n    self.block_loading_counts = pd.Series(list(block_loading_counts.values()), index=ix, name='count').to_frame().sort_values(['count', 'block'], ascending=[False, True])['count']\n    ix = pd.Index(factor_orders.keys(), tupleize_cols=False, name='block')\n    self.factor_block_orders = pd.Series(list(factor_orders.values()), index=ix, name='order')\n    if orders_is_int:\n        keys = self.block_loading_counts.keys()\n        self.factor_block_orders = self.factor_block_orders.loc[keys]\n        self.factor_block_orders.index.name = 'block'\n    factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    missing = [name for name in self.endog_factor_map.columns if name not in factor_names.tolist()]\n    if len(missing):\n        ix = pd.Index([(factor_name,) for factor_name in missing], tupleize_cols=False, name='block')\n        default_block_orders = pd.Series(np.ones(len(ix), dtype=int), index=ix, name='order')\n        self.factor_block_orders = self.factor_block_orders.append(default_block_orders)\n        factor_names = pd.Series(np.concatenate(list(self.factor_block_orders.index)))\n    duplicates = factor_names.duplicated()\n    if duplicates.any():\n        duplicate_names = set(factor_names[duplicates])\n        raise ValueError(f'Each factor can be assigned to at most one block of factors in `factor_orders`. Duplicate entries for {duplicate_names}')\n    self.factor_names = factor_names.tolist()\n    self.max_factor_order = np.max(self.factor_block_orders)\n    self.endog_factor_map = self.endog_factor_map.loc[endog_names, factor_names]\n    self.k_states_factors = 0\n    self.k_posdef_factors = 0\n    state_offset = 0\n    self.factor_blocks = []\n    for (factor_names, factor_order) in self.factor_block_orders.items():\n        block = FactorBlock(factor_names, factor_order, self.endog_factor_map, state_offset, self.k_endog_Q)\n        self.k_states_factors += block.k_states\n        self.k_posdef_factors += block.k_factors\n        state_offset += block.k_states\n        self.factor_blocks.append(block)\n    self.k_states_idio_M = self.k_endog_M if idiosyncratic_ar1 else 0\n    self.k_states_idio_Q = self.k_endog_Q * 5\n    self.k_states_idio = self.k_states_idio_M + self.k_states_idio_Q\n    self.k_posdef_idio_M = self.k_endog_M if self.idiosyncratic_ar1 else 0\n    self.k_posdef_idio_Q = self.k_endog_Q\n    self.k_posdef_idio = self.k_posdef_idio_M + self.k_posdef_idio_Q\n    self.k_states = self.k_states_factors + self.k_states_idio\n    self.k_posdef = self.k_posdef_factors + self.k_posdef_idio\n    self._endog_factor_iloc = None"
        ]
    },
    {
        "func_name": "_apply_factor_multiplicities",
        "original": "def _apply_factor_multiplicities(self, factors, factor_orders, factor_multiplicities):\n    \"\"\"\n        Expand `factors` and `factor_orders` to account for factor multiplity.\n\n        For example, if there is a `global` factor with multiplicity 2, then\n        this method expands that into `global.1` and `global.2` in both the\n        `factors` and `factor_orders` dictionaries.\n\n        Parameters\n        ----------\n        factors : dict\n            Dictionary of {endog_name: list of factor names}\n        factor_orders : dict\n            Dictionary of {tuple of factor names: factor order}\n        factor_multiplicities : dict\n            Dictionary of {factor name: factor multiplicity}\n\n        Returns\n        -------\n        new_factors : dict\n            Dictionary of {endog_name: list of factor names}, with factor names\n            expanded to incorporate multiplicities.\n        new_factors : dict\n            Dictionary of {tuple of factor names: factor order}, with factor\n            names in each tuple expanded to incorporate multiplicities.\n        \"\"\"\n    new_factors = {}\n    for (endog_name, factors_list) in factors.items():\n        new_factor_list = []\n        for factor_name in factors_list:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_factor_list += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_factor_list.append(factor_name)\n        new_factors[endog_name] = new_factor_list\n    new_factor_orders = {}\n    for (block, factor_order) in factor_orders.items():\n        if not isinstance(block, tuple):\n            block = (block,)\n        new_block = []\n        for factor_name in block:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_block += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_block += [factor_name]\n        new_factor_orders[tuple(new_block)] = factor_order\n    return (new_factors, new_factor_orders)",
        "mutated": [
            "def _apply_factor_multiplicities(self, factors, factor_orders, factor_multiplicities):\n    if False:\n        i = 10\n    '\\n        Expand `factors` and `factor_orders` to account for factor multiplity.\\n\\n        For example, if there is a `global` factor with multiplicity 2, then\\n        this method expands that into `global.1` and `global.2` in both the\\n        `factors` and `factor_orders` dictionaries.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        factor_orders : dict\\n            Dictionary of {tuple of factor names: factor order}\\n        factor_multiplicities : dict\\n            Dictionary of {factor name: factor multiplicity}\\n\\n        Returns\\n        -------\\n        new_factors : dict\\n            Dictionary of {endog_name: list of factor names}, with factor names\\n            expanded to incorporate multiplicities.\\n        new_factors : dict\\n            Dictionary of {tuple of factor names: factor order}, with factor\\n            names in each tuple expanded to incorporate multiplicities.\\n        '\n    new_factors = {}\n    for (endog_name, factors_list) in factors.items():\n        new_factor_list = []\n        for factor_name in factors_list:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_factor_list += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_factor_list.append(factor_name)\n        new_factors[endog_name] = new_factor_list\n    new_factor_orders = {}\n    for (block, factor_order) in factor_orders.items():\n        if not isinstance(block, tuple):\n            block = (block,)\n        new_block = []\n        for factor_name in block:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_block += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_block += [factor_name]\n        new_factor_orders[tuple(new_block)] = factor_order\n    return (new_factors, new_factor_orders)",
            "def _apply_factor_multiplicities(self, factors, factor_orders, factor_multiplicities):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Expand `factors` and `factor_orders` to account for factor multiplity.\\n\\n        For example, if there is a `global` factor with multiplicity 2, then\\n        this method expands that into `global.1` and `global.2` in both the\\n        `factors` and `factor_orders` dictionaries.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        factor_orders : dict\\n            Dictionary of {tuple of factor names: factor order}\\n        factor_multiplicities : dict\\n            Dictionary of {factor name: factor multiplicity}\\n\\n        Returns\\n        -------\\n        new_factors : dict\\n            Dictionary of {endog_name: list of factor names}, with factor names\\n            expanded to incorporate multiplicities.\\n        new_factors : dict\\n            Dictionary of {tuple of factor names: factor order}, with factor\\n            names in each tuple expanded to incorporate multiplicities.\\n        '\n    new_factors = {}\n    for (endog_name, factors_list) in factors.items():\n        new_factor_list = []\n        for factor_name in factors_list:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_factor_list += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_factor_list.append(factor_name)\n        new_factors[endog_name] = new_factor_list\n    new_factor_orders = {}\n    for (block, factor_order) in factor_orders.items():\n        if not isinstance(block, tuple):\n            block = (block,)\n        new_block = []\n        for factor_name in block:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_block += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_block += [factor_name]\n        new_factor_orders[tuple(new_block)] = factor_order\n    return (new_factors, new_factor_orders)",
            "def _apply_factor_multiplicities(self, factors, factor_orders, factor_multiplicities):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Expand `factors` and `factor_orders` to account for factor multiplity.\\n\\n        For example, if there is a `global` factor with multiplicity 2, then\\n        this method expands that into `global.1` and `global.2` in both the\\n        `factors` and `factor_orders` dictionaries.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        factor_orders : dict\\n            Dictionary of {tuple of factor names: factor order}\\n        factor_multiplicities : dict\\n            Dictionary of {factor name: factor multiplicity}\\n\\n        Returns\\n        -------\\n        new_factors : dict\\n            Dictionary of {endog_name: list of factor names}, with factor names\\n            expanded to incorporate multiplicities.\\n        new_factors : dict\\n            Dictionary of {tuple of factor names: factor order}, with factor\\n            names in each tuple expanded to incorporate multiplicities.\\n        '\n    new_factors = {}\n    for (endog_name, factors_list) in factors.items():\n        new_factor_list = []\n        for factor_name in factors_list:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_factor_list += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_factor_list.append(factor_name)\n        new_factors[endog_name] = new_factor_list\n    new_factor_orders = {}\n    for (block, factor_order) in factor_orders.items():\n        if not isinstance(block, tuple):\n            block = (block,)\n        new_block = []\n        for factor_name in block:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_block += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_block += [factor_name]\n        new_factor_orders[tuple(new_block)] = factor_order\n    return (new_factors, new_factor_orders)",
            "def _apply_factor_multiplicities(self, factors, factor_orders, factor_multiplicities):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Expand `factors` and `factor_orders` to account for factor multiplity.\\n\\n        For example, if there is a `global` factor with multiplicity 2, then\\n        this method expands that into `global.1` and `global.2` in both the\\n        `factors` and `factor_orders` dictionaries.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        factor_orders : dict\\n            Dictionary of {tuple of factor names: factor order}\\n        factor_multiplicities : dict\\n            Dictionary of {factor name: factor multiplicity}\\n\\n        Returns\\n        -------\\n        new_factors : dict\\n            Dictionary of {endog_name: list of factor names}, with factor names\\n            expanded to incorporate multiplicities.\\n        new_factors : dict\\n            Dictionary of {tuple of factor names: factor order}, with factor\\n            names in each tuple expanded to incorporate multiplicities.\\n        '\n    new_factors = {}\n    for (endog_name, factors_list) in factors.items():\n        new_factor_list = []\n        for factor_name in factors_list:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_factor_list += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_factor_list.append(factor_name)\n        new_factors[endog_name] = new_factor_list\n    new_factor_orders = {}\n    for (block, factor_order) in factor_orders.items():\n        if not isinstance(block, tuple):\n            block = (block,)\n        new_block = []\n        for factor_name in block:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_block += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_block += [factor_name]\n        new_factor_orders[tuple(new_block)] = factor_order\n    return (new_factors, new_factor_orders)",
            "def _apply_factor_multiplicities(self, factors, factor_orders, factor_multiplicities):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Expand `factors` and `factor_orders` to account for factor multiplity.\\n\\n        For example, if there is a `global` factor with multiplicity 2, then\\n        this method expands that into `global.1` and `global.2` in both the\\n        `factors` and `factor_orders` dictionaries.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        factor_orders : dict\\n            Dictionary of {tuple of factor names: factor order}\\n        factor_multiplicities : dict\\n            Dictionary of {factor name: factor multiplicity}\\n\\n        Returns\\n        -------\\n        new_factors : dict\\n            Dictionary of {endog_name: list of factor names}, with factor names\\n            expanded to incorporate multiplicities.\\n        new_factors : dict\\n            Dictionary of {tuple of factor names: factor order}, with factor\\n            names in each tuple expanded to incorporate multiplicities.\\n        '\n    new_factors = {}\n    for (endog_name, factors_list) in factors.items():\n        new_factor_list = []\n        for factor_name in factors_list:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_factor_list += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_factor_list.append(factor_name)\n        new_factors[endog_name] = new_factor_list\n    new_factor_orders = {}\n    for (block, factor_order) in factor_orders.items():\n        if not isinstance(block, tuple):\n            block = (block,)\n        new_block = []\n        for factor_name in block:\n            n = factor_multiplicities.get(factor_name, 1)\n            if n > 1:\n                new_block += [f'{factor_name}.{i + 1}' for i in range(n)]\n            else:\n                new_block += [factor_name]\n        new_factor_orders[tuple(new_block)] = factor_order\n    return (new_factors, new_factor_orders)"
        ]
    },
    {
        "func_name": "_construct_endog_factor_map",
        "original": "def _construct_endog_factor_map(self, factors, endog_names):\n    \"\"\"\n        Construct mapping of observed variables to factors.\n\n        Parameters\n        ----------\n        factors : dict\n            Dictionary of {endog_name: list of factor names}\n        endog_names : list of str\n            List of the names of the observed variables.\n\n        Returns\n        -------\n        endog_factor_map : pd.DataFrame\n            Boolean dataframe with `endog_names` as the index and the factor\n            names (computed from the `factors` input) as the columns. Each cell\n            is True if the associated factor is allowed to load on the\n            associated observed variable.\n\n        \"\"\"\n    missing = []\n    for (key, value) in factors.items():\n        if not isinstance(value, (list, tuple)) or len(value) == 0:\n            missing.append(key)\n    if len(missing):\n        raise ValueError(f'Each observed variable must be mapped to at least one factor in the `factors` dictionary. Variables missing factors are: {missing}.')\n    missing = set(endog_names).difference(set(factors.keys()))\n    if len(missing):\n        raise ValueError(f'If a `factors` dictionary is provided, then it must include entries for each observed variable. Missing variables are: {missing}.')\n    factor_names = {}\n    for (key, value) in factors.items():\n        if isinstance(value, str):\n            factor_names[value] = 0\n        else:\n            factor_names.update({v: 0 for v in value})\n    factor_names = list(factor_names.keys())\n    k_factors = len(factor_names)\n    endog_factor_map = pd.DataFrame(np.zeros((self.k_endog, k_factors), dtype=bool), index=pd.Index(endog_names, name='endog'), columns=pd.Index(factor_names, name='factor'))\n    for (key, value) in factors.items():\n        endog_factor_map.loc[key, value] = True\n    return endog_factor_map",
        "mutated": [
            "def _construct_endog_factor_map(self, factors, endog_names):\n    if False:\n        i = 10\n    '\\n        Construct mapping of observed variables to factors.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        endog_names : list of str\\n            List of the names of the observed variables.\\n\\n        Returns\\n        -------\\n        endog_factor_map : pd.DataFrame\\n            Boolean dataframe with `endog_names` as the index and the factor\\n            names (computed from the `factors` input) as the columns. Each cell\\n            is True if the associated factor is allowed to load on the\\n            associated observed variable.\\n\\n        '\n    missing = []\n    for (key, value) in factors.items():\n        if not isinstance(value, (list, tuple)) or len(value) == 0:\n            missing.append(key)\n    if len(missing):\n        raise ValueError(f'Each observed variable must be mapped to at least one factor in the `factors` dictionary. Variables missing factors are: {missing}.')\n    missing = set(endog_names).difference(set(factors.keys()))\n    if len(missing):\n        raise ValueError(f'If a `factors` dictionary is provided, then it must include entries for each observed variable. Missing variables are: {missing}.')\n    factor_names = {}\n    for (key, value) in factors.items():\n        if isinstance(value, str):\n            factor_names[value] = 0\n        else:\n            factor_names.update({v: 0 for v in value})\n    factor_names = list(factor_names.keys())\n    k_factors = len(factor_names)\n    endog_factor_map = pd.DataFrame(np.zeros((self.k_endog, k_factors), dtype=bool), index=pd.Index(endog_names, name='endog'), columns=pd.Index(factor_names, name='factor'))\n    for (key, value) in factors.items():\n        endog_factor_map.loc[key, value] = True\n    return endog_factor_map",
            "def _construct_endog_factor_map(self, factors, endog_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Construct mapping of observed variables to factors.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        endog_names : list of str\\n            List of the names of the observed variables.\\n\\n        Returns\\n        -------\\n        endog_factor_map : pd.DataFrame\\n            Boolean dataframe with `endog_names` as the index and the factor\\n            names (computed from the `factors` input) as the columns. Each cell\\n            is True if the associated factor is allowed to load on the\\n            associated observed variable.\\n\\n        '\n    missing = []\n    for (key, value) in factors.items():\n        if not isinstance(value, (list, tuple)) or len(value) == 0:\n            missing.append(key)\n    if len(missing):\n        raise ValueError(f'Each observed variable must be mapped to at least one factor in the `factors` dictionary. Variables missing factors are: {missing}.')\n    missing = set(endog_names).difference(set(factors.keys()))\n    if len(missing):\n        raise ValueError(f'If a `factors` dictionary is provided, then it must include entries for each observed variable. Missing variables are: {missing}.')\n    factor_names = {}\n    for (key, value) in factors.items():\n        if isinstance(value, str):\n            factor_names[value] = 0\n        else:\n            factor_names.update({v: 0 for v in value})\n    factor_names = list(factor_names.keys())\n    k_factors = len(factor_names)\n    endog_factor_map = pd.DataFrame(np.zeros((self.k_endog, k_factors), dtype=bool), index=pd.Index(endog_names, name='endog'), columns=pd.Index(factor_names, name='factor'))\n    for (key, value) in factors.items():\n        endog_factor_map.loc[key, value] = True\n    return endog_factor_map",
            "def _construct_endog_factor_map(self, factors, endog_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Construct mapping of observed variables to factors.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        endog_names : list of str\\n            List of the names of the observed variables.\\n\\n        Returns\\n        -------\\n        endog_factor_map : pd.DataFrame\\n            Boolean dataframe with `endog_names` as the index and the factor\\n            names (computed from the `factors` input) as the columns. Each cell\\n            is True if the associated factor is allowed to load on the\\n            associated observed variable.\\n\\n        '\n    missing = []\n    for (key, value) in factors.items():\n        if not isinstance(value, (list, tuple)) or len(value) == 0:\n            missing.append(key)\n    if len(missing):\n        raise ValueError(f'Each observed variable must be mapped to at least one factor in the `factors` dictionary. Variables missing factors are: {missing}.')\n    missing = set(endog_names).difference(set(factors.keys()))\n    if len(missing):\n        raise ValueError(f'If a `factors` dictionary is provided, then it must include entries for each observed variable. Missing variables are: {missing}.')\n    factor_names = {}\n    for (key, value) in factors.items():\n        if isinstance(value, str):\n            factor_names[value] = 0\n        else:\n            factor_names.update({v: 0 for v in value})\n    factor_names = list(factor_names.keys())\n    k_factors = len(factor_names)\n    endog_factor_map = pd.DataFrame(np.zeros((self.k_endog, k_factors), dtype=bool), index=pd.Index(endog_names, name='endog'), columns=pd.Index(factor_names, name='factor'))\n    for (key, value) in factors.items():\n        endog_factor_map.loc[key, value] = True\n    return endog_factor_map",
            "def _construct_endog_factor_map(self, factors, endog_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Construct mapping of observed variables to factors.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        endog_names : list of str\\n            List of the names of the observed variables.\\n\\n        Returns\\n        -------\\n        endog_factor_map : pd.DataFrame\\n            Boolean dataframe with `endog_names` as the index and the factor\\n            names (computed from the `factors` input) as the columns. Each cell\\n            is True if the associated factor is allowed to load on the\\n            associated observed variable.\\n\\n        '\n    missing = []\n    for (key, value) in factors.items():\n        if not isinstance(value, (list, tuple)) or len(value) == 0:\n            missing.append(key)\n    if len(missing):\n        raise ValueError(f'Each observed variable must be mapped to at least one factor in the `factors` dictionary. Variables missing factors are: {missing}.')\n    missing = set(endog_names).difference(set(factors.keys()))\n    if len(missing):\n        raise ValueError(f'If a `factors` dictionary is provided, then it must include entries for each observed variable. Missing variables are: {missing}.')\n    factor_names = {}\n    for (key, value) in factors.items():\n        if isinstance(value, str):\n            factor_names[value] = 0\n        else:\n            factor_names.update({v: 0 for v in value})\n    factor_names = list(factor_names.keys())\n    k_factors = len(factor_names)\n    endog_factor_map = pd.DataFrame(np.zeros((self.k_endog, k_factors), dtype=bool), index=pd.Index(endog_names, name='endog'), columns=pd.Index(factor_names, name='factor'))\n    for (key, value) in factors.items():\n        endog_factor_map.loc[key, value] = True\n    return endog_factor_map",
            "def _construct_endog_factor_map(self, factors, endog_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Construct mapping of observed variables to factors.\\n\\n        Parameters\\n        ----------\\n        factors : dict\\n            Dictionary of {endog_name: list of factor names}\\n        endog_names : list of str\\n            List of the names of the observed variables.\\n\\n        Returns\\n        -------\\n        endog_factor_map : pd.DataFrame\\n            Boolean dataframe with `endog_names` as the index and the factor\\n            names (computed from the `factors` input) as the columns. Each cell\\n            is True if the associated factor is allowed to load on the\\n            associated observed variable.\\n\\n        '\n    missing = []\n    for (key, value) in factors.items():\n        if not isinstance(value, (list, tuple)) or len(value) == 0:\n            missing.append(key)\n    if len(missing):\n        raise ValueError(f'Each observed variable must be mapped to at least one factor in the `factors` dictionary. Variables missing factors are: {missing}.')\n    missing = set(endog_names).difference(set(factors.keys()))\n    if len(missing):\n        raise ValueError(f'If a `factors` dictionary is provided, then it must include entries for each observed variable. Missing variables are: {missing}.')\n    factor_names = {}\n    for (key, value) in factors.items():\n        if isinstance(value, str):\n            factor_names[value] = 0\n        else:\n            factor_names.update({v: 0 for v in value})\n    factor_names = list(factor_names.keys())\n    k_factors = len(factor_names)\n    endog_factor_map = pd.DataFrame(np.zeros((self.k_endog, k_factors), dtype=bool), index=pd.Index(endog_names, name='endog'), columns=pd.Index(factor_names, name='factor'))\n    for (key, value) in factors.items():\n        endog_factor_map.loc[key, value] = True\n    return endog_factor_map"
        ]
    },
    {
        "func_name": "factors_L1",
        "original": "@property\ndef factors_L1(self):\n    \"\"\"Factors.\"\"\"\n    ix = np.arange(self.k_states_factors)\n    iloc = tuple((ix[block.factors_L1] for block in self.factor_blocks))\n    return np.concatenate(iloc)",
        "mutated": [
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n    'Factors.'\n    ix = np.arange(self.k_states_factors)\n    iloc = tuple((ix[block.factors_L1] for block in self.factor_blocks))\n    return np.concatenate(iloc)",
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Factors.'\n    ix = np.arange(self.k_states_factors)\n    iloc = tuple((ix[block.factors_L1] for block in self.factor_blocks))\n    return np.concatenate(iloc)",
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Factors.'\n    ix = np.arange(self.k_states_factors)\n    iloc = tuple((ix[block.factors_L1] for block in self.factor_blocks))\n    return np.concatenate(iloc)",
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Factors.'\n    ix = np.arange(self.k_states_factors)\n    iloc = tuple((ix[block.factors_L1] for block in self.factor_blocks))\n    return np.concatenate(iloc)",
            "@property\ndef factors_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Factors.'\n    ix = np.arange(self.k_states_factors)\n    iloc = tuple((ix[block.factors_L1] for block in self.factor_blocks))\n    return np.concatenate(iloc)"
        ]
    },
    {
        "func_name": "factors_L1_5_ix",
        "original": "@property\ndef factors_L1_5_ix(self):\n    \"\"\"Factors plus any lags, index shaped (5, k_factors).\"\"\"\n    ix = np.arange(self.k_states_factors)\n    iloc = []\n    for block in self.factor_blocks:\n        iloc.append(ix[block.factors_L1_5].reshape(5, block.k_factors))\n    return np.concatenate(iloc, axis=1)",
        "mutated": [
            "@property\ndef factors_L1_5_ix(self):\n    if False:\n        i = 10\n    'Factors plus any lags, index shaped (5, k_factors).'\n    ix = np.arange(self.k_states_factors)\n    iloc = []\n    for block in self.factor_blocks:\n        iloc.append(ix[block.factors_L1_5].reshape(5, block.k_factors))\n    return np.concatenate(iloc, axis=1)",
            "@property\ndef factors_L1_5_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Factors plus any lags, index shaped (5, k_factors).'\n    ix = np.arange(self.k_states_factors)\n    iloc = []\n    for block in self.factor_blocks:\n        iloc.append(ix[block.factors_L1_5].reshape(5, block.k_factors))\n    return np.concatenate(iloc, axis=1)",
            "@property\ndef factors_L1_5_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Factors plus any lags, index shaped (5, k_factors).'\n    ix = np.arange(self.k_states_factors)\n    iloc = []\n    for block in self.factor_blocks:\n        iloc.append(ix[block.factors_L1_5].reshape(5, block.k_factors))\n    return np.concatenate(iloc, axis=1)",
            "@property\ndef factors_L1_5_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Factors plus any lags, index shaped (5, k_factors).'\n    ix = np.arange(self.k_states_factors)\n    iloc = []\n    for block in self.factor_blocks:\n        iloc.append(ix[block.factors_L1_5].reshape(5, block.k_factors))\n    return np.concatenate(iloc, axis=1)",
            "@property\ndef factors_L1_5_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Factors plus any lags, index shaped (5, k_factors).'\n    ix = np.arange(self.k_states_factors)\n    iloc = []\n    for block in self.factor_blocks:\n        iloc.append(ix[block.factors_L1_5].reshape(5, block.k_factors))\n    return np.concatenate(iloc, axis=1)"
        ]
    },
    {
        "func_name": "idio_ar_L1",
        "original": "@property\ndef idio_ar_L1(self):\n    \"\"\"Idiosyncratic AR states, (first block / lag only).\"\"\"\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog\n    else:\n        ix2 = ix1 + self.k_endog_Q\n    return np.s_[ix1:ix2]",
        "mutated": [
            "@property\ndef idio_ar_L1(self):\n    if False:\n        i = 10\n    'Idiosyncratic AR states, (first block / lag only).'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog\n    else:\n        ix2 = ix1 + self.k_endog_Q\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Idiosyncratic AR states, (first block / lag only).'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog\n    else:\n        ix2 = ix1 + self.k_endog_Q\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Idiosyncratic AR states, (first block / lag only).'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog\n    else:\n        ix2 = ix1 + self.k_endog_Q\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Idiosyncratic AR states, (first block / lag only).'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog\n    else:\n        ix2 = ix1 + self.k_endog_Q\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_L1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Idiosyncratic AR states, (first block / lag only).'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog\n    else:\n        ix2 = ix1 + self.k_endog_Q\n    return np.s_[ix1:ix2]"
        ]
    },
    {
        "func_name": "idio_ar_M",
        "original": "@property\ndef idio_ar_M(self):\n    \"\"\"Idiosyncratic AR states for monthly variables.\"\"\"\n    ix1 = self.k_states_factors\n    ix2 = ix1\n    if self.idiosyncratic_ar1:\n        ix2 += self.k_endog_M\n    return np.s_[ix1:ix2]",
        "mutated": [
            "@property\ndef idio_ar_M(self):\n    if False:\n        i = 10\n    'Idiosyncratic AR states for monthly variables.'\n    ix1 = self.k_states_factors\n    ix2 = ix1\n    if self.idiosyncratic_ar1:\n        ix2 += self.k_endog_M\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_M(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Idiosyncratic AR states for monthly variables.'\n    ix1 = self.k_states_factors\n    ix2 = ix1\n    if self.idiosyncratic_ar1:\n        ix2 += self.k_endog_M\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_M(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Idiosyncratic AR states for monthly variables.'\n    ix1 = self.k_states_factors\n    ix2 = ix1\n    if self.idiosyncratic_ar1:\n        ix2 += self.k_endog_M\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_M(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Idiosyncratic AR states for monthly variables.'\n    ix1 = self.k_states_factors\n    ix2 = ix1\n    if self.idiosyncratic_ar1:\n        ix2 += self.k_endog_M\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_M(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Idiosyncratic AR states for monthly variables.'\n    ix1 = self.k_states_factors\n    ix2 = ix1\n    if self.idiosyncratic_ar1:\n        ix2 += self.k_endog_M\n    return np.s_[ix1:ix2]"
        ]
    },
    {
        "func_name": "idio_ar_Q",
        "original": "@property\ndef idio_ar_Q(self):\n    \"\"\"Idiosyncratic AR states and all lags for quarterly variables.\"\"\"\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix1 += self.k_endog_M\n    ix2 = ix1 + self.k_endog_Q * 5\n    return np.s_[ix1:ix2]",
        "mutated": [
            "@property\ndef idio_ar_Q(self):\n    if False:\n        i = 10\n    'Idiosyncratic AR states and all lags for quarterly variables.'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix1 += self.k_endog_M\n    ix2 = ix1 + self.k_endog_Q * 5\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_Q(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Idiosyncratic AR states and all lags for quarterly variables.'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix1 += self.k_endog_M\n    ix2 = ix1 + self.k_endog_Q * 5\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_Q(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Idiosyncratic AR states and all lags for quarterly variables.'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix1 += self.k_endog_M\n    ix2 = ix1 + self.k_endog_Q * 5\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_Q(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Idiosyncratic AR states and all lags for quarterly variables.'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix1 += self.k_endog_M\n    ix2 = ix1 + self.k_endog_Q * 5\n    return np.s_[ix1:ix2]",
            "@property\ndef idio_ar_Q(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Idiosyncratic AR states and all lags for quarterly variables.'\n    ix1 = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        ix1 += self.k_endog_M\n    ix2 = ix1 + self.k_endog_Q * 5\n    return np.s_[ix1:ix2]"
        ]
    },
    {
        "func_name": "idio_ar_Q_ix",
        "original": "@property\ndef idio_ar_Q_ix(self):\n    \"\"\"Idiosyncratic AR (quarterly) state index, (k_endog_Q, lags).\"\"\"\n    start = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        start += self.k_endog_M\n    return start + np.reshape(np.arange(5 * self.k_endog_Q), (5, self.k_endog_Q)).T",
        "mutated": [
            "@property\ndef idio_ar_Q_ix(self):\n    if False:\n        i = 10\n    'Idiosyncratic AR (quarterly) state index, (k_endog_Q, lags).'\n    start = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        start += self.k_endog_M\n    return start + np.reshape(np.arange(5 * self.k_endog_Q), (5, self.k_endog_Q)).T",
            "@property\ndef idio_ar_Q_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Idiosyncratic AR (quarterly) state index, (k_endog_Q, lags).'\n    start = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        start += self.k_endog_M\n    return start + np.reshape(np.arange(5 * self.k_endog_Q), (5, self.k_endog_Q)).T",
            "@property\ndef idio_ar_Q_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Idiosyncratic AR (quarterly) state index, (k_endog_Q, lags).'\n    start = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        start += self.k_endog_M\n    return start + np.reshape(np.arange(5 * self.k_endog_Q), (5, self.k_endog_Q)).T",
            "@property\ndef idio_ar_Q_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Idiosyncratic AR (quarterly) state index, (k_endog_Q, lags).'\n    start = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        start += self.k_endog_M\n    return start + np.reshape(np.arange(5 * self.k_endog_Q), (5, self.k_endog_Q)).T",
            "@property\ndef idio_ar_Q_ix(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Idiosyncratic AR (quarterly) state index, (k_endog_Q, lags).'\n    start = self.k_states_factors\n    if self.idiosyncratic_ar1:\n        start += self.k_endog_M\n    return start + np.reshape(np.arange(5 * self.k_endog_Q), (5, self.k_endog_Q)).T"
        ]
    },
    {
        "func_name": "endog_factor_iloc",
        "original": "@property\ndef endog_factor_iloc(self):\n    \"\"\"List of list of int, factor indexes for each observed variable.\"\"\"\n    if self._endog_factor_iloc is None:\n        ilocs = []\n        for i in range(self.k_endog):\n            ilocs.append(np.where(self.endog_factor_map.iloc[i])[0])\n        self._endog_factor_iloc = ilocs\n    return self._endog_factor_iloc",
        "mutated": [
            "@property\ndef endog_factor_iloc(self):\n    if False:\n        i = 10\n    'List of list of int, factor indexes for each observed variable.'\n    if self._endog_factor_iloc is None:\n        ilocs = []\n        for i in range(self.k_endog):\n            ilocs.append(np.where(self.endog_factor_map.iloc[i])[0])\n        self._endog_factor_iloc = ilocs\n    return self._endog_factor_iloc",
            "@property\ndef endog_factor_iloc(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'List of list of int, factor indexes for each observed variable.'\n    if self._endog_factor_iloc is None:\n        ilocs = []\n        for i in range(self.k_endog):\n            ilocs.append(np.where(self.endog_factor_map.iloc[i])[0])\n        self._endog_factor_iloc = ilocs\n    return self._endog_factor_iloc",
            "@property\ndef endog_factor_iloc(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'List of list of int, factor indexes for each observed variable.'\n    if self._endog_factor_iloc is None:\n        ilocs = []\n        for i in range(self.k_endog):\n            ilocs.append(np.where(self.endog_factor_map.iloc[i])[0])\n        self._endog_factor_iloc = ilocs\n    return self._endog_factor_iloc",
            "@property\ndef endog_factor_iloc(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'List of list of int, factor indexes for each observed variable.'\n    if self._endog_factor_iloc is None:\n        ilocs = []\n        for i in range(self.k_endog):\n            ilocs.append(np.where(self.endog_factor_map.iloc[i])[0])\n        self._endog_factor_iloc = ilocs\n    return self._endog_factor_iloc",
            "@property\ndef endog_factor_iloc(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'List of list of int, factor indexes for each observed variable.'\n    if self._endog_factor_iloc is None:\n        ilocs = []\n        for i in range(self.k_endog):\n            ilocs.append(np.where(self.endog_factor_map.iloc[i])[0])\n        self._endog_factor_iloc = ilocs\n    return self._endog_factor_iloc"
        ]
    },
    {
        "func_name": "__getitem__",
        "original": "def __getitem__(self, key):\n    \"\"\"\n        Use square brackets to access index / slice elements.\n\n        This is convenient in highlighting the indexing / slice quality of\n        these attributes in the code below.\n        \"\"\"\n    if key in ['factors_L1', 'factors_L1_5_ix', 'idio_ar_L1', 'idio_ar_M', 'idio_ar_Q', 'idio_ar_Q_ix']:\n        return getattr(self, key)\n    else:\n        raise KeyError(key)",
        "mutated": [
            "def __getitem__(self, key):\n    if False:\n        i = 10\n    '\\n        Use square brackets to access index / slice elements.\\n\\n        This is convenient in highlighting the indexing / slice quality of\\n        these attributes in the code below.\\n        '\n    if key in ['factors_L1', 'factors_L1_5_ix', 'idio_ar_L1', 'idio_ar_M', 'idio_ar_Q', 'idio_ar_Q_ix']:\n        return getattr(self, key)\n    else:\n        raise KeyError(key)",
            "def __getitem__(self, key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Use square brackets to access index / slice elements.\\n\\n        This is convenient in highlighting the indexing / slice quality of\\n        these attributes in the code below.\\n        '\n    if key in ['factors_L1', 'factors_L1_5_ix', 'idio_ar_L1', 'idio_ar_M', 'idio_ar_Q', 'idio_ar_Q_ix']:\n        return getattr(self, key)\n    else:\n        raise KeyError(key)",
            "def __getitem__(self, key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Use square brackets to access index / slice elements.\\n\\n        This is convenient in highlighting the indexing / slice quality of\\n        these attributes in the code below.\\n        '\n    if key in ['factors_L1', 'factors_L1_5_ix', 'idio_ar_L1', 'idio_ar_M', 'idio_ar_Q', 'idio_ar_Q_ix']:\n        return getattr(self, key)\n    else:\n        raise KeyError(key)",
            "def __getitem__(self, key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Use square brackets to access index / slice elements.\\n\\n        This is convenient in highlighting the indexing / slice quality of\\n        these attributes in the code below.\\n        '\n    if key in ['factors_L1', 'factors_L1_5_ix', 'idio_ar_L1', 'idio_ar_M', 'idio_ar_Q', 'idio_ar_Q_ix']:\n        return getattr(self, key)\n    else:\n        raise KeyError(key)",
            "def __getitem__(self, key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Use square brackets to access index / slice elements.\\n\\n        This is convenient in highlighting the indexing / slice quality of\\n        these attributes in the code below.\\n        '\n    if key in ['factors_L1', 'factors_L1_5_ix', 'idio_ar_L1', 'idio_ar_M', 'idio_ar_Q', 'idio_ar_Q_ix']:\n        return getattr(self, key)\n    else:\n        raise KeyError(key)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, endog, k_endog_monthly=None, factors=1, factor_orders=1, factor_multiplicities=None, idiosyncratic_ar1=True, standardize=True, endog_quarterly=None, init_t0=False, obs_cov_diag=False, **kwargs):\n    if endog_quarterly is not None:\n        if k_endog_monthly is not None:\n            raise ValueError('If `endog_quarterly` is specified, then `endog` must contain only monthly variables, and so `k_endog_monthly` cannot be specified since it will be inferred from the shape of `endog`.')\n        (endog, k_endog_monthly) = self.construct_endog(endog, endog_quarterly)\n    endog_is_pandas = _is_using_pandas(endog, None)\n    if endog_is_pandas:\n        if isinstance(endog, pd.Series):\n            endog = endog.to_frame()\n    elif np.ndim(endog) < 2:\n        endog = np.atleast_2d(endog).T\n    if k_endog_monthly is None:\n        k_endog_monthly = endog.shape[1]\n    if endog_is_pandas:\n        endog_names = endog.columns.tolist()\n    elif endog.shape[1] == 1:\n        endog_names = ['y']\n    else:\n        endog_names = [f'y{i + 1}' for i in range(endog.shape[1])]\n    self.k_endog_M = int_like(k_endog_monthly, 'k_endog_monthly')\n    self.k_endog_Q = endog.shape[1] - self.k_endog_M\n    s = self._s = DynamicFactorMQStates(self.k_endog_M, self.k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._s.endog_factor_map\n    self.factor_block_orders = self._s.factor_block_orders\n    self.factor_names = self._s.factor_names\n    self.k_factors = self._s.k_factors\n    self.k_factor_blocks = len(self.factor_block_orders)\n    self.max_factor_order = self._s.max_factor_order\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    self.init_t0 = init_t0\n    self.obs_cov_diag = obs_cov_diag\n    if self.init_t0:\n        if endog_is_pandas:\n            ix = pd.period_range(endog.index[0] - 1, endog.index[-1], freq=endog.index.freq)\n            endog = endog.reindex(ix)\n        else:\n            endog = np.c_[[np.nan] * endog.shape[1], endog.T].T\n    if isinstance(standardize, tuple) and len(standardize) == 2:\n        (endog_mean, endog_std) = standardize\n        n = endog.shape[1]\n        if isinstance(endog_mean, pd.Series) and (not endog_mean.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_mean.index}.')\n        else:\n            endog_mean = np.atleast_1d(endog_mean)\n        if isinstance(endog_std, pd.Series) and (not endog_std.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_std.index}.')\n        else:\n            endog_std = np.atleast_1d(endog_std)\n        if np.shape(endog_mean) != (n,) or np.shape(endog_std) != (n,):\n            raise ValueError(f'Invalid value passed for `standardize`: each element must be shaped ({n},).')\n        standardize = True\n        if endog_is_pandas:\n            endog_mean = pd.Series(endog_mean, index=endog_names)\n            endog_std = pd.Series(endog_std, index=endog_names)\n    elif standardize in [1, True]:\n        endog_mean = endog.mean(axis=0)\n        endog_std = endog.std(axis=0)\n    elif standardize in [0, False]:\n        endog_mean = np.zeros(endog.shape[1])\n        endog_std = np.ones(endog.shape[1])\n    else:\n        raise ValueError('Invalid value passed for `standardize`.')\n    self._endog_mean = endog_mean\n    self._endog_std = endog_std\n    self.standardize = standardize\n    if np.any(self._endog_std < 1e-10):\n        ix = np.where(self._endog_std < 1e-10)\n        names = np.array(endog_names)[ix[0]].tolist()\n        raise ValueError(f'Constant variable(s) found in observed variables, but constants cannot be included in this model. These variables are: {names}.')\n    if self.standardize:\n        endog = (endog - self._endog_mean) / self._endog_std\n    o = self._o = {'M': np.s_[:self.k_endog_M], 'Q': np.s_[self.k_endog_M:]}\n    super().__init__(endog, k_states=s.k_states, k_posdef=s.k_posdef, **kwargs)\n    if self.standardize:\n        self.data.orig_endog = self.data.orig_endog * self._endog_std + self._endog_mean\n    if 'initialization' not in kwargs:\n        self.ssm.initialize(self._default_initialization())\n    if self.idiosyncratic_ar1:\n        self['design', o['M'], s['idio_ar_M']] = np.eye(self.k_endog_M)\n    multipliers = [1, 2, 3, 2, 1]\n    for i in range(len(multipliers)):\n        m = multipliers[i]\n        self['design', o['Q'], s['idio_ar_Q_ix'][:, i]] = m * np.eye(self.k_endog_Q)\n    if self.obs_cov_diag:\n        self['obs_cov'] = np.eye(self.k_endog) * 0.0001\n    for block in s.factor_blocks:\n        if block.k_factors == 1:\n            tmp = 0\n        else:\n            tmp = np.zeros((block.k_factors, block.k_factors))\n        self['transition', block['factors'], block['factors']] = companion_matrix([1] + [tmp] * block._factor_order).T\n    if self.k_endog_Q == 1:\n        tmp = 0\n    else:\n        tmp = np.zeros((self.k_endog_Q, self.k_endog_Q))\n    self['transition', s['idio_ar_Q'], s['idio_ar_Q']] = companion_matrix([1] + [tmp] * 5).T\n    ix1 = ix2 = 0\n    for block in s.factor_blocks:\n        ix2 += block.k_factors\n        self['selection', block['factors_ix'][:, 0], ix1:ix2] = np.eye(block.k_factors)\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog_M\n        self['selection', s['idio_ar_M'], ix1:ix2] = np.eye(self.k_endog_M)\n        ix1 = ix2\n    ix2 = ix1 + self.k_endog_Q\n    self['selection', s['idio_ar_Q_ix'][:, 0], ix1:ix2] = np.eye(self.k_endog_Q)\n    self.params = OrderedDict([('loadings', np.sum(self.endog_factor_map.values)), ('factor_ar', np.sum([block.k_factors ** 2 * block.factor_order for block in s.factor_blocks])), ('factor_cov', np.sum([block.k_factors * (block.k_factors + 1) // 2 for block in s.factor_blocks])), ('idiosyncratic_ar1', self.k_endog if self.idiosyncratic_ar1 else 0), ('idiosyncratic_var', self.k_endog)])\n    self.k_params = np.sum(list(self.params.values()))\n    ix = np.split(np.arange(self.k_params), np.cumsum(list(self.params.values()))[:-1])\n    self._p = dict(zip(self.params.keys(), ix))\n    self._loading_constraints = {}\n    self._init_keys += ['factors', 'factor_orders', 'factor_multiplicities', 'idiosyncratic_ar1', 'standardize', 'init_t0', 'obs_cov_diag'] + list(kwargs.keys())",
        "mutated": [
            "def __init__(self, endog, k_endog_monthly=None, factors=1, factor_orders=1, factor_multiplicities=None, idiosyncratic_ar1=True, standardize=True, endog_quarterly=None, init_t0=False, obs_cov_diag=False, **kwargs):\n    if False:\n        i = 10\n    if endog_quarterly is not None:\n        if k_endog_monthly is not None:\n            raise ValueError('If `endog_quarterly` is specified, then `endog` must contain only monthly variables, and so `k_endog_monthly` cannot be specified since it will be inferred from the shape of `endog`.')\n        (endog, k_endog_monthly) = self.construct_endog(endog, endog_quarterly)\n    endog_is_pandas = _is_using_pandas(endog, None)\n    if endog_is_pandas:\n        if isinstance(endog, pd.Series):\n            endog = endog.to_frame()\n    elif np.ndim(endog) < 2:\n        endog = np.atleast_2d(endog).T\n    if k_endog_monthly is None:\n        k_endog_monthly = endog.shape[1]\n    if endog_is_pandas:\n        endog_names = endog.columns.tolist()\n    elif endog.shape[1] == 1:\n        endog_names = ['y']\n    else:\n        endog_names = [f'y{i + 1}' for i in range(endog.shape[1])]\n    self.k_endog_M = int_like(k_endog_monthly, 'k_endog_monthly')\n    self.k_endog_Q = endog.shape[1] - self.k_endog_M\n    s = self._s = DynamicFactorMQStates(self.k_endog_M, self.k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._s.endog_factor_map\n    self.factor_block_orders = self._s.factor_block_orders\n    self.factor_names = self._s.factor_names\n    self.k_factors = self._s.k_factors\n    self.k_factor_blocks = len(self.factor_block_orders)\n    self.max_factor_order = self._s.max_factor_order\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    self.init_t0 = init_t0\n    self.obs_cov_diag = obs_cov_diag\n    if self.init_t0:\n        if endog_is_pandas:\n            ix = pd.period_range(endog.index[0] - 1, endog.index[-1], freq=endog.index.freq)\n            endog = endog.reindex(ix)\n        else:\n            endog = np.c_[[np.nan] * endog.shape[1], endog.T].T\n    if isinstance(standardize, tuple) and len(standardize) == 2:\n        (endog_mean, endog_std) = standardize\n        n = endog.shape[1]\n        if isinstance(endog_mean, pd.Series) and (not endog_mean.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_mean.index}.')\n        else:\n            endog_mean = np.atleast_1d(endog_mean)\n        if isinstance(endog_std, pd.Series) and (not endog_std.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_std.index}.')\n        else:\n            endog_std = np.atleast_1d(endog_std)\n        if np.shape(endog_mean) != (n,) or np.shape(endog_std) != (n,):\n            raise ValueError(f'Invalid value passed for `standardize`: each element must be shaped ({n},).')\n        standardize = True\n        if endog_is_pandas:\n            endog_mean = pd.Series(endog_mean, index=endog_names)\n            endog_std = pd.Series(endog_std, index=endog_names)\n    elif standardize in [1, True]:\n        endog_mean = endog.mean(axis=0)\n        endog_std = endog.std(axis=0)\n    elif standardize in [0, False]:\n        endog_mean = np.zeros(endog.shape[1])\n        endog_std = np.ones(endog.shape[1])\n    else:\n        raise ValueError('Invalid value passed for `standardize`.')\n    self._endog_mean = endog_mean\n    self._endog_std = endog_std\n    self.standardize = standardize\n    if np.any(self._endog_std < 1e-10):\n        ix = np.where(self._endog_std < 1e-10)\n        names = np.array(endog_names)[ix[0]].tolist()\n        raise ValueError(f'Constant variable(s) found in observed variables, but constants cannot be included in this model. These variables are: {names}.')\n    if self.standardize:\n        endog = (endog - self._endog_mean) / self._endog_std\n    o = self._o = {'M': np.s_[:self.k_endog_M], 'Q': np.s_[self.k_endog_M:]}\n    super().__init__(endog, k_states=s.k_states, k_posdef=s.k_posdef, **kwargs)\n    if self.standardize:\n        self.data.orig_endog = self.data.orig_endog * self._endog_std + self._endog_mean\n    if 'initialization' not in kwargs:\n        self.ssm.initialize(self._default_initialization())\n    if self.idiosyncratic_ar1:\n        self['design', o['M'], s['idio_ar_M']] = np.eye(self.k_endog_M)\n    multipliers = [1, 2, 3, 2, 1]\n    for i in range(len(multipliers)):\n        m = multipliers[i]\n        self['design', o['Q'], s['idio_ar_Q_ix'][:, i]] = m * np.eye(self.k_endog_Q)\n    if self.obs_cov_diag:\n        self['obs_cov'] = np.eye(self.k_endog) * 0.0001\n    for block in s.factor_blocks:\n        if block.k_factors == 1:\n            tmp = 0\n        else:\n            tmp = np.zeros((block.k_factors, block.k_factors))\n        self['transition', block['factors'], block['factors']] = companion_matrix([1] + [tmp] * block._factor_order).T\n    if self.k_endog_Q == 1:\n        tmp = 0\n    else:\n        tmp = np.zeros((self.k_endog_Q, self.k_endog_Q))\n    self['transition', s['idio_ar_Q'], s['idio_ar_Q']] = companion_matrix([1] + [tmp] * 5).T\n    ix1 = ix2 = 0\n    for block in s.factor_blocks:\n        ix2 += block.k_factors\n        self['selection', block['factors_ix'][:, 0], ix1:ix2] = np.eye(block.k_factors)\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog_M\n        self['selection', s['idio_ar_M'], ix1:ix2] = np.eye(self.k_endog_M)\n        ix1 = ix2\n    ix2 = ix1 + self.k_endog_Q\n    self['selection', s['idio_ar_Q_ix'][:, 0], ix1:ix2] = np.eye(self.k_endog_Q)\n    self.params = OrderedDict([('loadings', np.sum(self.endog_factor_map.values)), ('factor_ar', np.sum([block.k_factors ** 2 * block.factor_order for block in s.factor_blocks])), ('factor_cov', np.sum([block.k_factors * (block.k_factors + 1) // 2 for block in s.factor_blocks])), ('idiosyncratic_ar1', self.k_endog if self.idiosyncratic_ar1 else 0), ('idiosyncratic_var', self.k_endog)])\n    self.k_params = np.sum(list(self.params.values()))\n    ix = np.split(np.arange(self.k_params), np.cumsum(list(self.params.values()))[:-1])\n    self._p = dict(zip(self.params.keys(), ix))\n    self._loading_constraints = {}\n    self._init_keys += ['factors', 'factor_orders', 'factor_multiplicities', 'idiosyncratic_ar1', 'standardize', 'init_t0', 'obs_cov_diag'] + list(kwargs.keys())",
            "def __init__(self, endog, k_endog_monthly=None, factors=1, factor_orders=1, factor_multiplicities=None, idiosyncratic_ar1=True, standardize=True, endog_quarterly=None, init_t0=False, obs_cov_diag=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if endog_quarterly is not None:\n        if k_endog_monthly is not None:\n            raise ValueError('If `endog_quarterly` is specified, then `endog` must contain only monthly variables, and so `k_endog_monthly` cannot be specified since it will be inferred from the shape of `endog`.')\n        (endog, k_endog_monthly) = self.construct_endog(endog, endog_quarterly)\n    endog_is_pandas = _is_using_pandas(endog, None)\n    if endog_is_pandas:\n        if isinstance(endog, pd.Series):\n            endog = endog.to_frame()\n    elif np.ndim(endog) < 2:\n        endog = np.atleast_2d(endog).T\n    if k_endog_monthly is None:\n        k_endog_monthly = endog.shape[1]\n    if endog_is_pandas:\n        endog_names = endog.columns.tolist()\n    elif endog.shape[1] == 1:\n        endog_names = ['y']\n    else:\n        endog_names = [f'y{i + 1}' for i in range(endog.shape[1])]\n    self.k_endog_M = int_like(k_endog_monthly, 'k_endog_monthly')\n    self.k_endog_Q = endog.shape[1] - self.k_endog_M\n    s = self._s = DynamicFactorMQStates(self.k_endog_M, self.k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._s.endog_factor_map\n    self.factor_block_orders = self._s.factor_block_orders\n    self.factor_names = self._s.factor_names\n    self.k_factors = self._s.k_factors\n    self.k_factor_blocks = len(self.factor_block_orders)\n    self.max_factor_order = self._s.max_factor_order\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    self.init_t0 = init_t0\n    self.obs_cov_diag = obs_cov_diag\n    if self.init_t0:\n        if endog_is_pandas:\n            ix = pd.period_range(endog.index[0] - 1, endog.index[-1], freq=endog.index.freq)\n            endog = endog.reindex(ix)\n        else:\n            endog = np.c_[[np.nan] * endog.shape[1], endog.T].T\n    if isinstance(standardize, tuple) and len(standardize) == 2:\n        (endog_mean, endog_std) = standardize\n        n = endog.shape[1]\n        if isinstance(endog_mean, pd.Series) and (not endog_mean.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_mean.index}.')\n        else:\n            endog_mean = np.atleast_1d(endog_mean)\n        if isinstance(endog_std, pd.Series) and (not endog_std.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_std.index}.')\n        else:\n            endog_std = np.atleast_1d(endog_std)\n        if np.shape(endog_mean) != (n,) or np.shape(endog_std) != (n,):\n            raise ValueError(f'Invalid value passed for `standardize`: each element must be shaped ({n},).')\n        standardize = True\n        if endog_is_pandas:\n            endog_mean = pd.Series(endog_mean, index=endog_names)\n            endog_std = pd.Series(endog_std, index=endog_names)\n    elif standardize in [1, True]:\n        endog_mean = endog.mean(axis=0)\n        endog_std = endog.std(axis=0)\n    elif standardize in [0, False]:\n        endog_mean = np.zeros(endog.shape[1])\n        endog_std = np.ones(endog.shape[1])\n    else:\n        raise ValueError('Invalid value passed for `standardize`.')\n    self._endog_mean = endog_mean\n    self._endog_std = endog_std\n    self.standardize = standardize\n    if np.any(self._endog_std < 1e-10):\n        ix = np.where(self._endog_std < 1e-10)\n        names = np.array(endog_names)[ix[0]].tolist()\n        raise ValueError(f'Constant variable(s) found in observed variables, but constants cannot be included in this model. These variables are: {names}.')\n    if self.standardize:\n        endog = (endog - self._endog_mean) / self._endog_std\n    o = self._o = {'M': np.s_[:self.k_endog_M], 'Q': np.s_[self.k_endog_M:]}\n    super().__init__(endog, k_states=s.k_states, k_posdef=s.k_posdef, **kwargs)\n    if self.standardize:\n        self.data.orig_endog = self.data.orig_endog * self._endog_std + self._endog_mean\n    if 'initialization' not in kwargs:\n        self.ssm.initialize(self._default_initialization())\n    if self.idiosyncratic_ar1:\n        self['design', o['M'], s['idio_ar_M']] = np.eye(self.k_endog_M)\n    multipliers = [1, 2, 3, 2, 1]\n    for i in range(len(multipliers)):\n        m = multipliers[i]\n        self['design', o['Q'], s['idio_ar_Q_ix'][:, i]] = m * np.eye(self.k_endog_Q)\n    if self.obs_cov_diag:\n        self['obs_cov'] = np.eye(self.k_endog) * 0.0001\n    for block in s.factor_blocks:\n        if block.k_factors == 1:\n            tmp = 0\n        else:\n            tmp = np.zeros((block.k_factors, block.k_factors))\n        self['transition', block['factors'], block['factors']] = companion_matrix([1] + [tmp] * block._factor_order).T\n    if self.k_endog_Q == 1:\n        tmp = 0\n    else:\n        tmp = np.zeros((self.k_endog_Q, self.k_endog_Q))\n    self['transition', s['idio_ar_Q'], s['idio_ar_Q']] = companion_matrix([1] + [tmp] * 5).T\n    ix1 = ix2 = 0\n    for block in s.factor_blocks:\n        ix2 += block.k_factors\n        self['selection', block['factors_ix'][:, 0], ix1:ix2] = np.eye(block.k_factors)\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog_M\n        self['selection', s['idio_ar_M'], ix1:ix2] = np.eye(self.k_endog_M)\n        ix1 = ix2\n    ix2 = ix1 + self.k_endog_Q\n    self['selection', s['idio_ar_Q_ix'][:, 0], ix1:ix2] = np.eye(self.k_endog_Q)\n    self.params = OrderedDict([('loadings', np.sum(self.endog_factor_map.values)), ('factor_ar', np.sum([block.k_factors ** 2 * block.factor_order for block in s.factor_blocks])), ('factor_cov', np.sum([block.k_factors * (block.k_factors + 1) // 2 for block in s.factor_blocks])), ('idiosyncratic_ar1', self.k_endog if self.idiosyncratic_ar1 else 0), ('idiosyncratic_var', self.k_endog)])\n    self.k_params = np.sum(list(self.params.values()))\n    ix = np.split(np.arange(self.k_params), np.cumsum(list(self.params.values()))[:-1])\n    self._p = dict(zip(self.params.keys(), ix))\n    self._loading_constraints = {}\n    self._init_keys += ['factors', 'factor_orders', 'factor_multiplicities', 'idiosyncratic_ar1', 'standardize', 'init_t0', 'obs_cov_diag'] + list(kwargs.keys())",
            "def __init__(self, endog, k_endog_monthly=None, factors=1, factor_orders=1, factor_multiplicities=None, idiosyncratic_ar1=True, standardize=True, endog_quarterly=None, init_t0=False, obs_cov_diag=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if endog_quarterly is not None:\n        if k_endog_monthly is not None:\n            raise ValueError('If `endog_quarterly` is specified, then `endog` must contain only monthly variables, and so `k_endog_monthly` cannot be specified since it will be inferred from the shape of `endog`.')\n        (endog, k_endog_monthly) = self.construct_endog(endog, endog_quarterly)\n    endog_is_pandas = _is_using_pandas(endog, None)\n    if endog_is_pandas:\n        if isinstance(endog, pd.Series):\n            endog = endog.to_frame()\n    elif np.ndim(endog) < 2:\n        endog = np.atleast_2d(endog).T\n    if k_endog_monthly is None:\n        k_endog_monthly = endog.shape[1]\n    if endog_is_pandas:\n        endog_names = endog.columns.tolist()\n    elif endog.shape[1] == 1:\n        endog_names = ['y']\n    else:\n        endog_names = [f'y{i + 1}' for i in range(endog.shape[1])]\n    self.k_endog_M = int_like(k_endog_monthly, 'k_endog_monthly')\n    self.k_endog_Q = endog.shape[1] - self.k_endog_M\n    s = self._s = DynamicFactorMQStates(self.k_endog_M, self.k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._s.endog_factor_map\n    self.factor_block_orders = self._s.factor_block_orders\n    self.factor_names = self._s.factor_names\n    self.k_factors = self._s.k_factors\n    self.k_factor_blocks = len(self.factor_block_orders)\n    self.max_factor_order = self._s.max_factor_order\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    self.init_t0 = init_t0\n    self.obs_cov_diag = obs_cov_diag\n    if self.init_t0:\n        if endog_is_pandas:\n            ix = pd.period_range(endog.index[0] - 1, endog.index[-1], freq=endog.index.freq)\n            endog = endog.reindex(ix)\n        else:\n            endog = np.c_[[np.nan] * endog.shape[1], endog.T].T\n    if isinstance(standardize, tuple) and len(standardize) == 2:\n        (endog_mean, endog_std) = standardize\n        n = endog.shape[1]\n        if isinstance(endog_mean, pd.Series) and (not endog_mean.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_mean.index}.')\n        else:\n            endog_mean = np.atleast_1d(endog_mean)\n        if isinstance(endog_std, pd.Series) and (not endog_std.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_std.index}.')\n        else:\n            endog_std = np.atleast_1d(endog_std)\n        if np.shape(endog_mean) != (n,) or np.shape(endog_std) != (n,):\n            raise ValueError(f'Invalid value passed for `standardize`: each element must be shaped ({n},).')\n        standardize = True\n        if endog_is_pandas:\n            endog_mean = pd.Series(endog_mean, index=endog_names)\n            endog_std = pd.Series(endog_std, index=endog_names)\n    elif standardize in [1, True]:\n        endog_mean = endog.mean(axis=0)\n        endog_std = endog.std(axis=0)\n    elif standardize in [0, False]:\n        endog_mean = np.zeros(endog.shape[1])\n        endog_std = np.ones(endog.shape[1])\n    else:\n        raise ValueError('Invalid value passed for `standardize`.')\n    self._endog_mean = endog_mean\n    self._endog_std = endog_std\n    self.standardize = standardize\n    if np.any(self._endog_std < 1e-10):\n        ix = np.where(self._endog_std < 1e-10)\n        names = np.array(endog_names)[ix[0]].tolist()\n        raise ValueError(f'Constant variable(s) found in observed variables, but constants cannot be included in this model. These variables are: {names}.')\n    if self.standardize:\n        endog = (endog - self._endog_mean) / self._endog_std\n    o = self._o = {'M': np.s_[:self.k_endog_M], 'Q': np.s_[self.k_endog_M:]}\n    super().__init__(endog, k_states=s.k_states, k_posdef=s.k_posdef, **kwargs)\n    if self.standardize:\n        self.data.orig_endog = self.data.orig_endog * self._endog_std + self._endog_mean\n    if 'initialization' not in kwargs:\n        self.ssm.initialize(self._default_initialization())\n    if self.idiosyncratic_ar1:\n        self['design', o['M'], s['idio_ar_M']] = np.eye(self.k_endog_M)\n    multipliers = [1, 2, 3, 2, 1]\n    for i in range(len(multipliers)):\n        m = multipliers[i]\n        self['design', o['Q'], s['idio_ar_Q_ix'][:, i]] = m * np.eye(self.k_endog_Q)\n    if self.obs_cov_diag:\n        self['obs_cov'] = np.eye(self.k_endog) * 0.0001\n    for block in s.factor_blocks:\n        if block.k_factors == 1:\n            tmp = 0\n        else:\n            tmp = np.zeros((block.k_factors, block.k_factors))\n        self['transition', block['factors'], block['factors']] = companion_matrix([1] + [tmp] * block._factor_order).T\n    if self.k_endog_Q == 1:\n        tmp = 0\n    else:\n        tmp = np.zeros((self.k_endog_Q, self.k_endog_Q))\n    self['transition', s['idio_ar_Q'], s['idio_ar_Q']] = companion_matrix([1] + [tmp] * 5).T\n    ix1 = ix2 = 0\n    for block in s.factor_blocks:\n        ix2 += block.k_factors\n        self['selection', block['factors_ix'][:, 0], ix1:ix2] = np.eye(block.k_factors)\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog_M\n        self['selection', s['idio_ar_M'], ix1:ix2] = np.eye(self.k_endog_M)\n        ix1 = ix2\n    ix2 = ix1 + self.k_endog_Q\n    self['selection', s['idio_ar_Q_ix'][:, 0], ix1:ix2] = np.eye(self.k_endog_Q)\n    self.params = OrderedDict([('loadings', np.sum(self.endog_factor_map.values)), ('factor_ar', np.sum([block.k_factors ** 2 * block.factor_order for block in s.factor_blocks])), ('factor_cov', np.sum([block.k_factors * (block.k_factors + 1) // 2 for block in s.factor_blocks])), ('idiosyncratic_ar1', self.k_endog if self.idiosyncratic_ar1 else 0), ('idiosyncratic_var', self.k_endog)])\n    self.k_params = np.sum(list(self.params.values()))\n    ix = np.split(np.arange(self.k_params), np.cumsum(list(self.params.values()))[:-1])\n    self._p = dict(zip(self.params.keys(), ix))\n    self._loading_constraints = {}\n    self._init_keys += ['factors', 'factor_orders', 'factor_multiplicities', 'idiosyncratic_ar1', 'standardize', 'init_t0', 'obs_cov_diag'] + list(kwargs.keys())",
            "def __init__(self, endog, k_endog_monthly=None, factors=1, factor_orders=1, factor_multiplicities=None, idiosyncratic_ar1=True, standardize=True, endog_quarterly=None, init_t0=False, obs_cov_diag=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if endog_quarterly is not None:\n        if k_endog_monthly is not None:\n            raise ValueError('If `endog_quarterly` is specified, then `endog` must contain only monthly variables, and so `k_endog_monthly` cannot be specified since it will be inferred from the shape of `endog`.')\n        (endog, k_endog_monthly) = self.construct_endog(endog, endog_quarterly)\n    endog_is_pandas = _is_using_pandas(endog, None)\n    if endog_is_pandas:\n        if isinstance(endog, pd.Series):\n            endog = endog.to_frame()\n    elif np.ndim(endog) < 2:\n        endog = np.atleast_2d(endog).T\n    if k_endog_monthly is None:\n        k_endog_monthly = endog.shape[1]\n    if endog_is_pandas:\n        endog_names = endog.columns.tolist()\n    elif endog.shape[1] == 1:\n        endog_names = ['y']\n    else:\n        endog_names = [f'y{i + 1}' for i in range(endog.shape[1])]\n    self.k_endog_M = int_like(k_endog_monthly, 'k_endog_monthly')\n    self.k_endog_Q = endog.shape[1] - self.k_endog_M\n    s = self._s = DynamicFactorMQStates(self.k_endog_M, self.k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._s.endog_factor_map\n    self.factor_block_orders = self._s.factor_block_orders\n    self.factor_names = self._s.factor_names\n    self.k_factors = self._s.k_factors\n    self.k_factor_blocks = len(self.factor_block_orders)\n    self.max_factor_order = self._s.max_factor_order\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    self.init_t0 = init_t0\n    self.obs_cov_diag = obs_cov_diag\n    if self.init_t0:\n        if endog_is_pandas:\n            ix = pd.period_range(endog.index[0] - 1, endog.index[-1], freq=endog.index.freq)\n            endog = endog.reindex(ix)\n        else:\n            endog = np.c_[[np.nan] * endog.shape[1], endog.T].T\n    if isinstance(standardize, tuple) and len(standardize) == 2:\n        (endog_mean, endog_std) = standardize\n        n = endog.shape[1]\n        if isinstance(endog_mean, pd.Series) and (not endog_mean.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_mean.index}.')\n        else:\n            endog_mean = np.atleast_1d(endog_mean)\n        if isinstance(endog_std, pd.Series) and (not endog_std.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_std.index}.')\n        else:\n            endog_std = np.atleast_1d(endog_std)\n        if np.shape(endog_mean) != (n,) or np.shape(endog_std) != (n,):\n            raise ValueError(f'Invalid value passed for `standardize`: each element must be shaped ({n},).')\n        standardize = True\n        if endog_is_pandas:\n            endog_mean = pd.Series(endog_mean, index=endog_names)\n            endog_std = pd.Series(endog_std, index=endog_names)\n    elif standardize in [1, True]:\n        endog_mean = endog.mean(axis=0)\n        endog_std = endog.std(axis=0)\n    elif standardize in [0, False]:\n        endog_mean = np.zeros(endog.shape[1])\n        endog_std = np.ones(endog.shape[1])\n    else:\n        raise ValueError('Invalid value passed for `standardize`.')\n    self._endog_mean = endog_mean\n    self._endog_std = endog_std\n    self.standardize = standardize\n    if np.any(self._endog_std < 1e-10):\n        ix = np.where(self._endog_std < 1e-10)\n        names = np.array(endog_names)[ix[0]].tolist()\n        raise ValueError(f'Constant variable(s) found in observed variables, but constants cannot be included in this model. These variables are: {names}.')\n    if self.standardize:\n        endog = (endog - self._endog_mean) / self._endog_std\n    o = self._o = {'M': np.s_[:self.k_endog_M], 'Q': np.s_[self.k_endog_M:]}\n    super().__init__(endog, k_states=s.k_states, k_posdef=s.k_posdef, **kwargs)\n    if self.standardize:\n        self.data.orig_endog = self.data.orig_endog * self._endog_std + self._endog_mean\n    if 'initialization' not in kwargs:\n        self.ssm.initialize(self._default_initialization())\n    if self.idiosyncratic_ar1:\n        self['design', o['M'], s['idio_ar_M']] = np.eye(self.k_endog_M)\n    multipliers = [1, 2, 3, 2, 1]\n    for i in range(len(multipliers)):\n        m = multipliers[i]\n        self['design', o['Q'], s['idio_ar_Q_ix'][:, i]] = m * np.eye(self.k_endog_Q)\n    if self.obs_cov_diag:\n        self['obs_cov'] = np.eye(self.k_endog) * 0.0001\n    for block in s.factor_blocks:\n        if block.k_factors == 1:\n            tmp = 0\n        else:\n            tmp = np.zeros((block.k_factors, block.k_factors))\n        self['transition', block['factors'], block['factors']] = companion_matrix([1] + [tmp] * block._factor_order).T\n    if self.k_endog_Q == 1:\n        tmp = 0\n    else:\n        tmp = np.zeros((self.k_endog_Q, self.k_endog_Q))\n    self['transition', s['idio_ar_Q'], s['idio_ar_Q']] = companion_matrix([1] + [tmp] * 5).T\n    ix1 = ix2 = 0\n    for block in s.factor_blocks:\n        ix2 += block.k_factors\n        self['selection', block['factors_ix'][:, 0], ix1:ix2] = np.eye(block.k_factors)\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog_M\n        self['selection', s['idio_ar_M'], ix1:ix2] = np.eye(self.k_endog_M)\n        ix1 = ix2\n    ix2 = ix1 + self.k_endog_Q\n    self['selection', s['idio_ar_Q_ix'][:, 0], ix1:ix2] = np.eye(self.k_endog_Q)\n    self.params = OrderedDict([('loadings', np.sum(self.endog_factor_map.values)), ('factor_ar', np.sum([block.k_factors ** 2 * block.factor_order for block in s.factor_blocks])), ('factor_cov', np.sum([block.k_factors * (block.k_factors + 1) // 2 for block in s.factor_blocks])), ('idiosyncratic_ar1', self.k_endog if self.idiosyncratic_ar1 else 0), ('idiosyncratic_var', self.k_endog)])\n    self.k_params = np.sum(list(self.params.values()))\n    ix = np.split(np.arange(self.k_params), np.cumsum(list(self.params.values()))[:-1])\n    self._p = dict(zip(self.params.keys(), ix))\n    self._loading_constraints = {}\n    self._init_keys += ['factors', 'factor_orders', 'factor_multiplicities', 'idiosyncratic_ar1', 'standardize', 'init_t0', 'obs_cov_diag'] + list(kwargs.keys())",
            "def __init__(self, endog, k_endog_monthly=None, factors=1, factor_orders=1, factor_multiplicities=None, idiosyncratic_ar1=True, standardize=True, endog_quarterly=None, init_t0=False, obs_cov_diag=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if endog_quarterly is not None:\n        if k_endog_monthly is not None:\n            raise ValueError('If `endog_quarterly` is specified, then `endog` must contain only monthly variables, and so `k_endog_monthly` cannot be specified since it will be inferred from the shape of `endog`.')\n        (endog, k_endog_monthly) = self.construct_endog(endog, endog_quarterly)\n    endog_is_pandas = _is_using_pandas(endog, None)\n    if endog_is_pandas:\n        if isinstance(endog, pd.Series):\n            endog = endog.to_frame()\n    elif np.ndim(endog) < 2:\n        endog = np.atleast_2d(endog).T\n    if k_endog_monthly is None:\n        k_endog_monthly = endog.shape[1]\n    if endog_is_pandas:\n        endog_names = endog.columns.tolist()\n    elif endog.shape[1] == 1:\n        endog_names = ['y']\n    else:\n        endog_names = [f'y{i + 1}' for i in range(endog.shape[1])]\n    self.k_endog_M = int_like(k_endog_monthly, 'k_endog_monthly')\n    self.k_endog_Q = endog.shape[1] - self.k_endog_M\n    s = self._s = DynamicFactorMQStates(self.k_endog_M, self.k_endog_Q, endog_names, factors, factor_orders, factor_multiplicities, idiosyncratic_ar1)\n    self.factors = factors\n    self.factor_orders = factor_orders\n    self.factor_multiplicities = factor_multiplicities\n    self.endog_factor_map = self._s.endog_factor_map\n    self.factor_block_orders = self._s.factor_block_orders\n    self.factor_names = self._s.factor_names\n    self.k_factors = self._s.k_factors\n    self.k_factor_blocks = len(self.factor_block_orders)\n    self.max_factor_order = self._s.max_factor_order\n    self.idiosyncratic_ar1 = idiosyncratic_ar1\n    self.init_t0 = init_t0\n    self.obs_cov_diag = obs_cov_diag\n    if self.init_t0:\n        if endog_is_pandas:\n            ix = pd.period_range(endog.index[0] - 1, endog.index[-1], freq=endog.index.freq)\n            endog = endog.reindex(ix)\n        else:\n            endog = np.c_[[np.nan] * endog.shape[1], endog.T].T\n    if isinstance(standardize, tuple) and len(standardize) == 2:\n        (endog_mean, endog_std) = standardize\n        n = endog.shape[1]\n        if isinstance(endog_mean, pd.Series) and (not endog_mean.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_mean.index}.')\n        else:\n            endog_mean = np.atleast_1d(endog_mean)\n        if isinstance(endog_std, pd.Series) and (not endog_std.index.equals(pd.Index(endog_names))):\n            raise ValueError(f'Invalid value passed for `standardize`: if a Pandas Series, must have index {endog_names}. Got {endog_std.index}.')\n        else:\n            endog_std = np.atleast_1d(endog_std)\n        if np.shape(endog_mean) != (n,) or np.shape(endog_std) != (n,):\n            raise ValueError(f'Invalid value passed for `standardize`: each element must be shaped ({n},).')\n        standardize = True\n        if endog_is_pandas:\n            endog_mean = pd.Series(endog_mean, index=endog_names)\n            endog_std = pd.Series(endog_std, index=endog_names)\n    elif standardize in [1, True]:\n        endog_mean = endog.mean(axis=0)\n        endog_std = endog.std(axis=0)\n    elif standardize in [0, False]:\n        endog_mean = np.zeros(endog.shape[1])\n        endog_std = np.ones(endog.shape[1])\n    else:\n        raise ValueError('Invalid value passed for `standardize`.')\n    self._endog_mean = endog_mean\n    self._endog_std = endog_std\n    self.standardize = standardize\n    if np.any(self._endog_std < 1e-10):\n        ix = np.where(self._endog_std < 1e-10)\n        names = np.array(endog_names)[ix[0]].tolist()\n        raise ValueError(f'Constant variable(s) found in observed variables, but constants cannot be included in this model. These variables are: {names}.')\n    if self.standardize:\n        endog = (endog - self._endog_mean) / self._endog_std\n    o = self._o = {'M': np.s_[:self.k_endog_M], 'Q': np.s_[self.k_endog_M:]}\n    super().__init__(endog, k_states=s.k_states, k_posdef=s.k_posdef, **kwargs)\n    if self.standardize:\n        self.data.orig_endog = self.data.orig_endog * self._endog_std + self._endog_mean\n    if 'initialization' not in kwargs:\n        self.ssm.initialize(self._default_initialization())\n    if self.idiosyncratic_ar1:\n        self['design', o['M'], s['idio_ar_M']] = np.eye(self.k_endog_M)\n    multipliers = [1, 2, 3, 2, 1]\n    for i in range(len(multipliers)):\n        m = multipliers[i]\n        self['design', o['Q'], s['idio_ar_Q_ix'][:, i]] = m * np.eye(self.k_endog_Q)\n    if self.obs_cov_diag:\n        self['obs_cov'] = np.eye(self.k_endog) * 0.0001\n    for block in s.factor_blocks:\n        if block.k_factors == 1:\n            tmp = 0\n        else:\n            tmp = np.zeros((block.k_factors, block.k_factors))\n        self['transition', block['factors'], block['factors']] = companion_matrix([1] + [tmp] * block._factor_order).T\n    if self.k_endog_Q == 1:\n        tmp = 0\n    else:\n        tmp = np.zeros((self.k_endog_Q, self.k_endog_Q))\n    self['transition', s['idio_ar_Q'], s['idio_ar_Q']] = companion_matrix([1] + [tmp] * 5).T\n    ix1 = ix2 = 0\n    for block in s.factor_blocks:\n        ix2 += block.k_factors\n        self['selection', block['factors_ix'][:, 0], ix1:ix2] = np.eye(block.k_factors)\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        ix2 = ix1 + self.k_endog_M\n        self['selection', s['idio_ar_M'], ix1:ix2] = np.eye(self.k_endog_M)\n        ix1 = ix2\n    ix2 = ix1 + self.k_endog_Q\n    self['selection', s['idio_ar_Q_ix'][:, 0], ix1:ix2] = np.eye(self.k_endog_Q)\n    self.params = OrderedDict([('loadings', np.sum(self.endog_factor_map.values)), ('factor_ar', np.sum([block.k_factors ** 2 * block.factor_order for block in s.factor_blocks])), ('factor_cov', np.sum([block.k_factors * (block.k_factors + 1) // 2 for block in s.factor_blocks])), ('idiosyncratic_ar1', self.k_endog if self.idiosyncratic_ar1 else 0), ('idiosyncratic_var', self.k_endog)])\n    self.k_params = np.sum(list(self.params.values()))\n    ix = np.split(np.arange(self.k_params), np.cumsum(list(self.params.values()))[:-1])\n    self._p = dict(zip(self.params.keys(), ix))\n    self._loading_constraints = {}\n    self._init_keys += ['factors', 'factor_orders', 'factor_multiplicities', 'idiosyncratic_ar1', 'standardize', 'init_t0', 'obs_cov_diag'] + list(kwargs.keys())"
        ]
    },
    {
        "func_name": "construct_endog",
        "original": "@classmethod\ndef construct_endog(cls, endog_monthly, endog_quarterly):\n    \"\"\"\n        Construct a combined dataset from separate monthly and quarterly data.\n\n        Parameters\n        ----------\n        endog_monthly : array_like\n            Monthly dataset. If a quarterly dataset is given, then this must\n            be a Pandas object with a PeriodIndex or DatetimeIndex at a monthly\n            frequency.\n        endog_quarterly : array_like or None\n            Quarterly dataset. If not None, then this must be a Pandas object\n            with a PeriodIndex or DatetimeIndex at a quarterly frequency.\n\n        Returns\n        -------\n        endog : array_like\n            If both endog_monthly and endog_quarterly were given, this is a\n            Pandas DataFrame with a PeriodIndex at the monthly frequency, with\n            all of the columns from `endog_monthly` ordered first and the\n            columns from `endog_quarterly` ordered afterwards. Otherwise it is\n            simply the input `endog_monthly` dataset.\n        k_endog_monthly : int\n            The number of monthly variables (which are ordered first) in the\n            returned `endog` dataset.\n        \"\"\"\n    if endog_quarterly is not None:\n        base_msg = 'If given both monthly and quarterly data then the monthly dataset must be a Pandas object with a date index at a monthly frequency.'\n        if not isinstance(endog_monthly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given monthly dataset is not a Pandas object. ' + base_msg)\n        elif endog_monthly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given monthly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_monthly.index, 'freqstr', 'N')[0] == 'M':\n            freqstr = getattr(endog_monthly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given monthly dataset has a non-monthly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with M if it is monthly). Got {freqstr}. ' + base_msg)\n        base_msg = 'If a quarterly dataset is given, then it must be a Pandas object with a date index at a quarterly frequency.'\n        if not isinstance(endog_quarterly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given quarterly dataset is not a Pandas object. ' + base_msg)\n        elif endog_quarterly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given quarterly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_quarterly.index, 'freqstr', 'N')[0] == 'Q':\n            freqstr = getattr(endog_quarterly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given quarterly dataset has a non-quarterly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with Q if it is quarterly). Got {freqstr}. ' + base_msg)\n        if hasattr(endog_monthly.index, 'to_period'):\n            endog_monthly = endog_monthly.to_period('M')\n        if hasattr(endog_quarterly.index, 'to_period'):\n            endog_quarterly = endog_quarterly.to_period('Q')\n        endog = pd.concat([endog_monthly, endog_quarterly.resample('M', convention='end').first()], axis=1)\n        column_counts = endog.columns.value_counts()\n        if column_counts.max() > 1:\n            columns = endog.columns.values.astype(object)\n            for name in column_counts.index:\n                count = column_counts.loc[name]\n                if count == 1:\n                    continue\n                mask = columns == name\n                columns[mask] = [f'{name}{i + 1}' for i in range(count)]\n            endog.columns = columns\n    else:\n        endog = endog_monthly.copy()\n    shape = endog_monthly.shape\n    k_endog_monthly = shape[1] if len(shape) == 2 else 1\n    return (endog, k_endog_monthly)",
        "mutated": [
            "@classmethod\ndef construct_endog(cls, endog_monthly, endog_quarterly):\n    if False:\n        i = 10\n    '\\n        Construct a combined dataset from separate monthly and quarterly data.\\n\\n        Parameters\\n        ----------\\n        endog_monthly : array_like\\n            Monthly dataset. If a quarterly dataset is given, then this must\\n            be a Pandas object with a PeriodIndex or DatetimeIndex at a monthly\\n            frequency.\\n        endog_quarterly : array_like or None\\n            Quarterly dataset. If not None, then this must be a Pandas object\\n            with a PeriodIndex or DatetimeIndex at a quarterly frequency.\\n\\n        Returns\\n        -------\\n        endog : array_like\\n            If both endog_monthly and endog_quarterly were given, this is a\\n            Pandas DataFrame with a PeriodIndex at the monthly frequency, with\\n            all of the columns from `endog_monthly` ordered first and the\\n            columns from `endog_quarterly` ordered afterwards. Otherwise it is\\n            simply the input `endog_monthly` dataset.\\n        k_endog_monthly : int\\n            The number of monthly variables (which are ordered first) in the\\n            returned `endog` dataset.\\n        '\n    if endog_quarterly is not None:\n        base_msg = 'If given both monthly and quarterly data then the monthly dataset must be a Pandas object with a date index at a monthly frequency.'\n        if not isinstance(endog_monthly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given monthly dataset is not a Pandas object. ' + base_msg)\n        elif endog_monthly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given monthly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_monthly.index, 'freqstr', 'N')[0] == 'M':\n            freqstr = getattr(endog_monthly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given monthly dataset has a non-monthly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with M if it is monthly). Got {freqstr}. ' + base_msg)\n        base_msg = 'If a quarterly dataset is given, then it must be a Pandas object with a date index at a quarterly frequency.'\n        if not isinstance(endog_quarterly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given quarterly dataset is not a Pandas object. ' + base_msg)\n        elif endog_quarterly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given quarterly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_quarterly.index, 'freqstr', 'N')[0] == 'Q':\n            freqstr = getattr(endog_quarterly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given quarterly dataset has a non-quarterly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with Q if it is quarterly). Got {freqstr}. ' + base_msg)\n        if hasattr(endog_monthly.index, 'to_period'):\n            endog_monthly = endog_monthly.to_period('M')\n        if hasattr(endog_quarterly.index, 'to_period'):\n            endog_quarterly = endog_quarterly.to_period('Q')\n        endog = pd.concat([endog_monthly, endog_quarterly.resample('M', convention='end').first()], axis=1)\n        column_counts = endog.columns.value_counts()\n        if column_counts.max() > 1:\n            columns = endog.columns.values.astype(object)\n            for name in column_counts.index:\n                count = column_counts.loc[name]\n                if count == 1:\n                    continue\n                mask = columns == name\n                columns[mask] = [f'{name}{i + 1}' for i in range(count)]\n            endog.columns = columns\n    else:\n        endog = endog_monthly.copy()\n    shape = endog_monthly.shape\n    k_endog_monthly = shape[1] if len(shape) == 2 else 1\n    return (endog, k_endog_monthly)",
            "@classmethod\ndef construct_endog(cls, endog_monthly, endog_quarterly):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Construct a combined dataset from separate monthly and quarterly data.\\n\\n        Parameters\\n        ----------\\n        endog_monthly : array_like\\n            Monthly dataset. If a quarterly dataset is given, then this must\\n            be a Pandas object with a PeriodIndex or DatetimeIndex at a monthly\\n            frequency.\\n        endog_quarterly : array_like or None\\n            Quarterly dataset. If not None, then this must be a Pandas object\\n            with a PeriodIndex or DatetimeIndex at a quarterly frequency.\\n\\n        Returns\\n        -------\\n        endog : array_like\\n            If both endog_monthly and endog_quarterly were given, this is a\\n            Pandas DataFrame with a PeriodIndex at the monthly frequency, with\\n            all of the columns from `endog_monthly` ordered first and the\\n            columns from `endog_quarterly` ordered afterwards. Otherwise it is\\n            simply the input `endog_monthly` dataset.\\n        k_endog_monthly : int\\n            The number of monthly variables (which are ordered first) in the\\n            returned `endog` dataset.\\n        '\n    if endog_quarterly is not None:\n        base_msg = 'If given both monthly and quarterly data then the monthly dataset must be a Pandas object with a date index at a monthly frequency.'\n        if not isinstance(endog_monthly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given monthly dataset is not a Pandas object. ' + base_msg)\n        elif endog_monthly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given monthly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_monthly.index, 'freqstr', 'N')[0] == 'M':\n            freqstr = getattr(endog_monthly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given monthly dataset has a non-monthly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with M if it is monthly). Got {freqstr}. ' + base_msg)\n        base_msg = 'If a quarterly dataset is given, then it must be a Pandas object with a date index at a quarterly frequency.'\n        if not isinstance(endog_quarterly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given quarterly dataset is not a Pandas object. ' + base_msg)\n        elif endog_quarterly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given quarterly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_quarterly.index, 'freqstr', 'N')[0] == 'Q':\n            freqstr = getattr(endog_quarterly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given quarterly dataset has a non-quarterly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with Q if it is quarterly). Got {freqstr}. ' + base_msg)\n        if hasattr(endog_monthly.index, 'to_period'):\n            endog_monthly = endog_monthly.to_period('M')\n        if hasattr(endog_quarterly.index, 'to_period'):\n            endog_quarterly = endog_quarterly.to_period('Q')\n        endog = pd.concat([endog_monthly, endog_quarterly.resample('M', convention='end').first()], axis=1)\n        column_counts = endog.columns.value_counts()\n        if column_counts.max() > 1:\n            columns = endog.columns.values.astype(object)\n            for name in column_counts.index:\n                count = column_counts.loc[name]\n                if count == 1:\n                    continue\n                mask = columns == name\n                columns[mask] = [f'{name}{i + 1}' for i in range(count)]\n            endog.columns = columns\n    else:\n        endog = endog_monthly.copy()\n    shape = endog_monthly.shape\n    k_endog_monthly = shape[1] if len(shape) == 2 else 1\n    return (endog, k_endog_monthly)",
            "@classmethod\ndef construct_endog(cls, endog_monthly, endog_quarterly):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Construct a combined dataset from separate monthly and quarterly data.\\n\\n        Parameters\\n        ----------\\n        endog_monthly : array_like\\n            Monthly dataset. If a quarterly dataset is given, then this must\\n            be a Pandas object with a PeriodIndex or DatetimeIndex at a monthly\\n            frequency.\\n        endog_quarterly : array_like or None\\n            Quarterly dataset. If not None, then this must be a Pandas object\\n            with a PeriodIndex or DatetimeIndex at a quarterly frequency.\\n\\n        Returns\\n        -------\\n        endog : array_like\\n            If both endog_monthly and endog_quarterly were given, this is a\\n            Pandas DataFrame with a PeriodIndex at the monthly frequency, with\\n            all of the columns from `endog_monthly` ordered first and the\\n            columns from `endog_quarterly` ordered afterwards. Otherwise it is\\n            simply the input `endog_monthly` dataset.\\n        k_endog_monthly : int\\n            The number of monthly variables (which are ordered first) in the\\n            returned `endog` dataset.\\n        '\n    if endog_quarterly is not None:\n        base_msg = 'If given both monthly and quarterly data then the monthly dataset must be a Pandas object with a date index at a monthly frequency.'\n        if not isinstance(endog_monthly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given monthly dataset is not a Pandas object. ' + base_msg)\n        elif endog_monthly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given monthly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_monthly.index, 'freqstr', 'N')[0] == 'M':\n            freqstr = getattr(endog_monthly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given monthly dataset has a non-monthly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with M if it is monthly). Got {freqstr}. ' + base_msg)\n        base_msg = 'If a quarterly dataset is given, then it must be a Pandas object with a date index at a quarterly frequency.'\n        if not isinstance(endog_quarterly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given quarterly dataset is not a Pandas object. ' + base_msg)\n        elif endog_quarterly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given quarterly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_quarterly.index, 'freqstr', 'N')[0] == 'Q':\n            freqstr = getattr(endog_quarterly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given quarterly dataset has a non-quarterly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with Q if it is quarterly). Got {freqstr}. ' + base_msg)\n        if hasattr(endog_monthly.index, 'to_period'):\n            endog_monthly = endog_monthly.to_period('M')\n        if hasattr(endog_quarterly.index, 'to_period'):\n            endog_quarterly = endog_quarterly.to_period('Q')\n        endog = pd.concat([endog_monthly, endog_quarterly.resample('M', convention='end').first()], axis=1)\n        column_counts = endog.columns.value_counts()\n        if column_counts.max() > 1:\n            columns = endog.columns.values.astype(object)\n            for name in column_counts.index:\n                count = column_counts.loc[name]\n                if count == 1:\n                    continue\n                mask = columns == name\n                columns[mask] = [f'{name}{i + 1}' for i in range(count)]\n            endog.columns = columns\n    else:\n        endog = endog_monthly.copy()\n    shape = endog_monthly.shape\n    k_endog_monthly = shape[1] if len(shape) == 2 else 1\n    return (endog, k_endog_monthly)",
            "@classmethod\ndef construct_endog(cls, endog_monthly, endog_quarterly):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Construct a combined dataset from separate monthly and quarterly data.\\n\\n        Parameters\\n        ----------\\n        endog_monthly : array_like\\n            Monthly dataset. If a quarterly dataset is given, then this must\\n            be a Pandas object with a PeriodIndex or DatetimeIndex at a monthly\\n            frequency.\\n        endog_quarterly : array_like or None\\n            Quarterly dataset. If not None, then this must be a Pandas object\\n            with a PeriodIndex or DatetimeIndex at a quarterly frequency.\\n\\n        Returns\\n        -------\\n        endog : array_like\\n            If both endog_monthly and endog_quarterly were given, this is a\\n            Pandas DataFrame with a PeriodIndex at the monthly frequency, with\\n            all of the columns from `endog_monthly` ordered first and the\\n            columns from `endog_quarterly` ordered afterwards. Otherwise it is\\n            simply the input `endog_monthly` dataset.\\n        k_endog_monthly : int\\n            The number of monthly variables (which are ordered first) in the\\n            returned `endog` dataset.\\n        '\n    if endog_quarterly is not None:\n        base_msg = 'If given both monthly and quarterly data then the monthly dataset must be a Pandas object with a date index at a monthly frequency.'\n        if not isinstance(endog_monthly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given monthly dataset is not a Pandas object. ' + base_msg)\n        elif endog_monthly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given monthly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_monthly.index, 'freqstr', 'N')[0] == 'M':\n            freqstr = getattr(endog_monthly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given monthly dataset has a non-monthly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with M if it is monthly). Got {freqstr}. ' + base_msg)\n        base_msg = 'If a quarterly dataset is given, then it must be a Pandas object with a date index at a quarterly frequency.'\n        if not isinstance(endog_quarterly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given quarterly dataset is not a Pandas object. ' + base_msg)\n        elif endog_quarterly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given quarterly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_quarterly.index, 'freqstr', 'N')[0] == 'Q':\n            freqstr = getattr(endog_quarterly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given quarterly dataset has a non-quarterly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with Q if it is quarterly). Got {freqstr}. ' + base_msg)\n        if hasattr(endog_monthly.index, 'to_period'):\n            endog_monthly = endog_monthly.to_period('M')\n        if hasattr(endog_quarterly.index, 'to_period'):\n            endog_quarterly = endog_quarterly.to_period('Q')\n        endog = pd.concat([endog_monthly, endog_quarterly.resample('M', convention='end').first()], axis=1)\n        column_counts = endog.columns.value_counts()\n        if column_counts.max() > 1:\n            columns = endog.columns.values.astype(object)\n            for name in column_counts.index:\n                count = column_counts.loc[name]\n                if count == 1:\n                    continue\n                mask = columns == name\n                columns[mask] = [f'{name}{i + 1}' for i in range(count)]\n            endog.columns = columns\n    else:\n        endog = endog_monthly.copy()\n    shape = endog_monthly.shape\n    k_endog_monthly = shape[1] if len(shape) == 2 else 1\n    return (endog, k_endog_monthly)",
            "@classmethod\ndef construct_endog(cls, endog_monthly, endog_quarterly):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Construct a combined dataset from separate monthly and quarterly data.\\n\\n        Parameters\\n        ----------\\n        endog_monthly : array_like\\n            Monthly dataset. If a quarterly dataset is given, then this must\\n            be a Pandas object with a PeriodIndex or DatetimeIndex at a monthly\\n            frequency.\\n        endog_quarterly : array_like or None\\n            Quarterly dataset. If not None, then this must be a Pandas object\\n            with a PeriodIndex or DatetimeIndex at a quarterly frequency.\\n\\n        Returns\\n        -------\\n        endog : array_like\\n            If both endog_monthly and endog_quarterly were given, this is a\\n            Pandas DataFrame with a PeriodIndex at the monthly frequency, with\\n            all of the columns from `endog_monthly` ordered first and the\\n            columns from `endog_quarterly` ordered afterwards. Otherwise it is\\n            simply the input `endog_monthly` dataset.\\n        k_endog_monthly : int\\n            The number of monthly variables (which are ordered first) in the\\n            returned `endog` dataset.\\n        '\n    if endog_quarterly is not None:\n        base_msg = 'If given both monthly and quarterly data then the monthly dataset must be a Pandas object with a date index at a monthly frequency.'\n        if not isinstance(endog_monthly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given monthly dataset is not a Pandas object. ' + base_msg)\n        elif endog_monthly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given monthly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_monthly.index, 'freqstr', 'N')[0] == 'M':\n            freqstr = getattr(endog_monthly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given monthly dataset has a non-monthly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with M if it is monthly). Got {freqstr}. ' + base_msg)\n        base_msg = 'If a quarterly dataset is given, then it must be a Pandas object with a date index at a quarterly frequency.'\n        if not isinstance(endog_quarterly, (pd.Series, pd.DataFrame)):\n            raise ValueError('Given quarterly dataset is not a Pandas object. ' + base_msg)\n        elif endog_quarterly.index.inferred_type not in ('datetime64', 'period'):\n            raise ValueError('Given quarterly dataset has an index with non-date values. ' + base_msg)\n        elif not getattr(endog_quarterly.index, 'freqstr', 'N')[0] == 'Q':\n            freqstr = getattr(endog_quarterly.index, 'freqstr', 'None')\n            raise ValueError(f'Index of given quarterly dataset has a non-quarterly frequency (to check this, examine the `freqstr` attribute of the index of the dataset - it should start with Q if it is quarterly). Got {freqstr}. ' + base_msg)\n        if hasattr(endog_monthly.index, 'to_period'):\n            endog_monthly = endog_monthly.to_period('M')\n        if hasattr(endog_quarterly.index, 'to_period'):\n            endog_quarterly = endog_quarterly.to_period('Q')\n        endog = pd.concat([endog_monthly, endog_quarterly.resample('M', convention='end').first()], axis=1)\n        column_counts = endog.columns.value_counts()\n        if column_counts.max() > 1:\n            columns = endog.columns.values.astype(object)\n            for name in column_counts.index:\n                count = column_counts.loc[name]\n                if count == 1:\n                    continue\n                mask = columns == name\n                columns[mask] = [f'{name}{i + 1}' for i in range(count)]\n            endog.columns = columns\n    else:\n        endog = endog_monthly.copy()\n    shape = endog_monthly.shape\n    k_endog_monthly = shape[1] if len(shape) == 2 else 1\n    return (endog, k_endog_monthly)"
        ]
    },
    {
        "func_name": "clone",
        "original": "def clone(self, endog, k_endog_monthly=None, endog_quarterly=None, retain_standardization=False, **kwargs):\n    \"\"\"\n        Clone state space model with new data and optionally new specification.\n\n        Parameters\n        ----------\n        endog : array_like\n            The observed time-series process :math:`y`\n        k_endog_monthly : int, optional\n            If specifying a monthly/quarterly mixed frequency model in which\n            the provided `endog` dataset contains both the monthly and\n            quarterly data, this variable should be used to indicate how many\n            of the variables are monthly.\n        endog_quarterly : array_like, optional\n            Observations of quarterly variables. If provided, must be a\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\n            the quarterly frequency.\n        kwargs\n            Keyword arguments to pass to the new model class to change the\n            model specification.\n\n        Returns\n        -------\n        model : DynamicFactorMQ instance\n        \"\"\"\n    if retain_standardization and self.standardize:\n        kwargs['standardize'] = (self._endog_mean, self._endog_std)\n    mod = self._clone_from_init_kwds(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, **kwargs)\n    return mod",
        "mutated": [
            "def clone(self, endog, k_endog_monthly=None, endog_quarterly=None, retain_standardization=False, **kwargs):\n    if False:\n        i = 10\n    '\\n        Clone state space model with new data and optionally new specification.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            The observed time-series process :math:`y`\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            Observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        kwargs\\n            Keyword arguments to pass to the new model class to change the\\n            model specification.\\n\\n        Returns\\n        -------\\n        model : DynamicFactorMQ instance\\n        '\n    if retain_standardization and self.standardize:\n        kwargs['standardize'] = (self._endog_mean, self._endog_std)\n    mod = self._clone_from_init_kwds(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, **kwargs)\n    return mod",
            "def clone(self, endog, k_endog_monthly=None, endog_quarterly=None, retain_standardization=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Clone state space model with new data and optionally new specification.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            The observed time-series process :math:`y`\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            Observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        kwargs\\n            Keyword arguments to pass to the new model class to change the\\n            model specification.\\n\\n        Returns\\n        -------\\n        model : DynamicFactorMQ instance\\n        '\n    if retain_standardization and self.standardize:\n        kwargs['standardize'] = (self._endog_mean, self._endog_std)\n    mod = self._clone_from_init_kwds(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, **kwargs)\n    return mod",
            "def clone(self, endog, k_endog_monthly=None, endog_quarterly=None, retain_standardization=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Clone state space model with new data and optionally new specification.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            The observed time-series process :math:`y`\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            Observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        kwargs\\n            Keyword arguments to pass to the new model class to change the\\n            model specification.\\n\\n        Returns\\n        -------\\n        model : DynamicFactorMQ instance\\n        '\n    if retain_standardization and self.standardize:\n        kwargs['standardize'] = (self._endog_mean, self._endog_std)\n    mod = self._clone_from_init_kwds(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, **kwargs)\n    return mod",
            "def clone(self, endog, k_endog_monthly=None, endog_quarterly=None, retain_standardization=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Clone state space model with new data and optionally new specification.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            The observed time-series process :math:`y`\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            Observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        kwargs\\n            Keyword arguments to pass to the new model class to change the\\n            model specification.\\n\\n        Returns\\n        -------\\n        model : DynamicFactorMQ instance\\n        '\n    if retain_standardization and self.standardize:\n        kwargs['standardize'] = (self._endog_mean, self._endog_std)\n    mod = self._clone_from_init_kwds(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, **kwargs)\n    return mod",
            "def clone(self, endog, k_endog_monthly=None, endog_quarterly=None, retain_standardization=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Clone state space model with new data and optionally new specification.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            The observed time-series process :math:`y`\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            Observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        kwargs\\n            Keyword arguments to pass to the new model class to change the\\n            model specification.\\n\\n        Returns\\n        -------\\n        model : DynamicFactorMQ instance\\n        '\n    if retain_standardization and self.standardize:\n        kwargs['standardize'] = (self._endog_mean, self._endog_std)\n    mod = self._clone_from_init_kwds(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, **kwargs)\n    return mod"
        ]
    },
    {
        "func_name": "_res_classes",
        "original": "@property\ndef _res_classes(self):\n    return {'fit': (DynamicFactorMQResults, mlemodel.MLEResultsWrapper)}",
        "mutated": [
            "@property\ndef _res_classes(self):\n    if False:\n        i = 10\n    return {'fit': (DynamicFactorMQResults, mlemodel.MLEResultsWrapper)}",
            "@property\ndef _res_classes(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'fit': (DynamicFactorMQResults, mlemodel.MLEResultsWrapper)}",
            "@property\ndef _res_classes(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'fit': (DynamicFactorMQResults, mlemodel.MLEResultsWrapper)}",
            "@property\ndef _res_classes(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'fit': (DynamicFactorMQResults, mlemodel.MLEResultsWrapper)}",
            "@property\ndef _res_classes(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'fit': (DynamicFactorMQResults, mlemodel.MLEResultsWrapper)}"
        ]
    },
    {
        "func_name": "_default_initialization",
        "original": "def _default_initialization(self):\n    s = self._s\n    init = initialization.Initialization(self.k_states)\n    for block in s.factor_blocks:\n        init.set(block['factors'], 'stationary')\n    if self.idiosyncratic_ar1:\n        for i in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n            init.set(i, 'stationary')\n    init.set(s['idio_ar_Q'], 'stationary')\n    return init",
        "mutated": [
            "def _default_initialization(self):\n    if False:\n        i = 10\n    s = self._s\n    init = initialization.Initialization(self.k_states)\n    for block in s.factor_blocks:\n        init.set(block['factors'], 'stationary')\n    if self.idiosyncratic_ar1:\n        for i in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n            init.set(i, 'stationary')\n    init.set(s['idio_ar_Q'], 'stationary')\n    return init",
            "def _default_initialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    s = self._s\n    init = initialization.Initialization(self.k_states)\n    for block in s.factor_blocks:\n        init.set(block['factors'], 'stationary')\n    if self.idiosyncratic_ar1:\n        for i in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n            init.set(i, 'stationary')\n    init.set(s['idio_ar_Q'], 'stationary')\n    return init",
            "def _default_initialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    s = self._s\n    init = initialization.Initialization(self.k_states)\n    for block in s.factor_blocks:\n        init.set(block['factors'], 'stationary')\n    if self.idiosyncratic_ar1:\n        for i in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n            init.set(i, 'stationary')\n    init.set(s['idio_ar_Q'], 'stationary')\n    return init",
            "def _default_initialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    s = self._s\n    init = initialization.Initialization(self.k_states)\n    for block in s.factor_blocks:\n        init.set(block['factors'], 'stationary')\n    if self.idiosyncratic_ar1:\n        for i in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n            init.set(i, 'stationary')\n    init.set(s['idio_ar_Q'], 'stationary')\n    return init",
            "def _default_initialization(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    s = self._s\n    init = initialization.Initialization(self.k_states)\n    for block in s.factor_blocks:\n        init.set(block['factors'], 'stationary')\n    if self.idiosyncratic_ar1:\n        for i in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n            init.set(i, 'stationary')\n    init.set(s['idio_ar_Q'], 'stationary')\n    return init"
        ]
    },
    {
        "func_name": "_get_endog_names",
        "original": "def _get_endog_names(self, truncate=None, as_string=None):\n    if truncate is None:\n        truncate = False if as_string is False or self.k_endog == 1 else 24\n    if as_string is False and truncate is not False:\n        raise ValueError('Can only truncate endog names if they are returned as a string.')\n    if as_string is None:\n        as_string = truncate is not False\n    endog_names = self.endog_names\n    if not isinstance(endog_names, list):\n        endog_names = [endog_names]\n    if as_string:\n        endog_names = [str(name) for name in endog_names]\n    if truncate is not False:\n        n = truncate\n        endog_names = [name if len(name) <= n else name[:n] + '...' for name in endog_names]\n    return endog_names",
        "mutated": [
            "def _get_endog_names(self, truncate=None, as_string=None):\n    if False:\n        i = 10\n    if truncate is None:\n        truncate = False if as_string is False or self.k_endog == 1 else 24\n    if as_string is False and truncate is not False:\n        raise ValueError('Can only truncate endog names if they are returned as a string.')\n    if as_string is None:\n        as_string = truncate is not False\n    endog_names = self.endog_names\n    if not isinstance(endog_names, list):\n        endog_names = [endog_names]\n    if as_string:\n        endog_names = [str(name) for name in endog_names]\n    if truncate is not False:\n        n = truncate\n        endog_names = [name if len(name) <= n else name[:n] + '...' for name in endog_names]\n    return endog_names",
            "def _get_endog_names(self, truncate=None, as_string=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if truncate is None:\n        truncate = False if as_string is False or self.k_endog == 1 else 24\n    if as_string is False and truncate is not False:\n        raise ValueError('Can only truncate endog names if they are returned as a string.')\n    if as_string is None:\n        as_string = truncate is not False\n    endog_names = self.endog_names\n    if not isinstance(endog_names, list):\n        endog_names = [endog_names]\n    if as_string:\n        endog_names = [str(name) for name in endog_names]\n    if truncate is not False:\n        n = truncate\n        endog_names = [name if len(name) <= n else name[:n] + '...' for name in endog_names]\n    return endog_names",
            "def _get_endog_names(self, truncate=None, as_string=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if truncate is None:\n        truncate = False if as_string is False or self.k_endog == 1 else 24\n    if as_string is False and truncate is not False:\n        raise ValueError('Can only truncate endog names if they are returned as a string.')\n    if as_string is None:\n        as_string = truncate is not False\n    endog_names = self.endog_names\n    if not isinstance(endog_names, list):\n        endog_names = [endog_names]\n    if as_string:\n        endog_names = [str(name) for name in endog_names]\n    if truncate is not False:\n        n = truncate\n        endog_names = [name if len(name) <= n else name[:n] + '...' for name in endog_names]\n    return endog_names",
            "def _get_endog_names(self, truncate=None, as_string=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if truncate is None:\n        truncate = False if as_string is False or self.k_endog == 1 else 24\n    if as_string is False and truncate is not False:\n        raise ValueError('Can only truncate endog names if they are returned as a string.')\n    if as_string is None:\n        as_string = truncate is not False\n    endog_names = self.endog_names\n    if not isinstance(endog_names, list):\n        endog_names = [endog_names]\n    if as_string:\n        endog_names = [str(name) for name in endog_names]\n    if truncate is not False:\n        n = truncate\n        endog_names = [name if len(name) <= n else name[:n] + '...' for name in endog_names]\n    return endog_names",
            "def _get_endog_names(self, truncate=None, as_string=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if truncate is None:\n        truncate = False if as_string is False or self.k_endog == 1 else 24\n    if as_string is False and truncate is not False:\n        raise ValueError('Can only truncate endog names if they are returned as a string.')\n    if as_string is None:\n        as_string = truncate is not False\n    endog_names = self.endog_names\n    if not isinstance(endog_names, list):\n        endog_names = [endog_names]\n    if as_string:\n        endog_names = [str(name) for name in endog_names]\n    if truncate is not False:\n        n = truncate\n        endog_names = [name if len(name) <= n else name[:n] + '...' for name in endog_names]\n    return endog_names"
        ]
    },
    {
        "func_name": "_model_name",
        "original": "@property\ndef _model_name(self):\n    model_name = ['Dynamic Factor Model', f'{self.k_factors} factors in {self.k_factor_blocks} blocks']\n    if self.k_endog_Q > 0:\n        model_name.append('Mixed frequency (M/Q)')\n    error_type = 'AR(1)' if self.idiosyncratic_ar1 else 'iid'\n    model_name.append(f'{error_type} idiosyncratic')\n    return model_name",
        "mutated": [
            "@property\ndef _model_name(self):\n    if False:\n        i = 10\n    model_name = ['Dynamic Factor Model', f'{self.k_factors} factors in {self.k_factor_blocks} blocks']\n    if self.k_endog_Q > 0:\n        model_name.append('Mixed frequency (M/Q)')\n    error_type = 'AR(1)' if self.idiosyncratic_ar1 else 'iid'\n    model_name.append(f'{error_type} idiosyncratic')\n    return model_name",
            "@property\ndef _model_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model_name = ['Dynamic Factor Model', f'{self.k_factors} factors in {self.k_factor_blocks} blocks']\n    if self.k_endog_Q > 0:\n        model_name.append('Mixed frequency (M/Q)')\n    error_type = 'AR(1)' if self.idiosyncratic_ar1 else 'iid'\n    model_name.append(f'{error_type} idiosyncratic')\n    return model_name",
            "@property\ndef _model_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model_name = ['Dynamic Factor Model', f'{self.k_factors} factors in {self.k_factor_blocks} blocks']\n    if self.k_endog_Q > 0:\n        model_name.append('Mixed frequency (M/Q)')\n    error_type = 'AR(1)' if self.idiosyncratic_ar1 else 'iid'\n    model_name.append(f'{error_type} idiosyncratic')\n    return model_name",
            "@property\ndef _model_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model_name = ['Dynamic Factor Model', f'{self.k_factors} factors in {self.k_factor_blocks} blocks']\n    if self.k_endog_Q > 0:\n        model_name.append('Mixed frequency (M/Q)')\n    error_type = 'AR(1)' if self.idiosyncratic_ar1 else 'iid'\n    model_name.append(f'{error_type} idiosyncratic')\n    return model_name",
            "@property\ndef _model_name(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model_name = ['Dynamic Factor Model', f'{self.k_factors} factors in {self.k_factor_blocks} blocks']\n    if self.k_endog_Q > 0:\n        model_name.append('Mixed frequency (M/Q)')\n    error_type = 'AR(1)' if self.idiosyncratic_ar1 else 'iid'\n    model_name.append(f'{error_type} idiosyncratic')\n    return model_name"
        ]
    },
    {
        "func_name": "summary",
        "original": "def summary(self, truncate_endog_names=None):\n    \"\"\"\n        Create a summary table describing the model.\n\n        Parameters\n        ----------\n        truncate_endog_names : int, optional\n            The number of characters to show for names of observed variables.\n            Default is 24 if there is more than one observed variable, or\n            an unlimited number of there is only one.\n        \"\"\"\n    endog_names = self._get_endog_names(truncate=truncate_endog_names, as_string=True)\n    title = 'Model Specification: Dynamic Factor Model'\n    if self._index_dates:\n        ix = self._index\n        d = ix[0]\n        sample = ['%s' % d]\n        d = ix[-1]\n        sample += ['- ' + '%s' % d]\n    else:\n        sample = [str(0), ' - ' + str(self.nobs)]\n    model_name = self._model_name\n    top_left = []\n    top_left.append(('Model:', [model_name[0]]))\n    for i in range(1, len(model_name)):\n        top_left.append(('', ['+ ' + model_name[i]]))\n    top_left += [('Sample:', [sample[0]]), ('', [sample[1]])]\n    top_right = []\n    if self.k_endog_Q > 0:\n        top_right += [('# of monthly variables:', [self.k_endog_M]), ('# of quarterly variables:', [self.k_endog_Q])]\n    else:\n        top_right += [('# of observed variables:', [self.k_endog])]\n    if self.k_factor_blocks == 1:\n        top_right += [('# of factors:', [self.k_factors])]\n    else:\n        top_right += [('# of factor blocks:', [self.k_factor_blocks])]\n    top_right += [('Idiosyncratic disturbances:', ['AR(1)' if self.idiosyncratic_ar1 else 'iid']), ('Standardize variables:', [self.standardize])]\n    summary = Summary()\n    self.model = self\n    summary.add_table_2cols(self, gleft=top_left, gright=top_right, title=title)\n    table_ix = 1\n    del self.model\n    data = self.endog_factor_map.replace({True: 'X', False: ''})\n    data.index = endog_names\n    try:\n        items = data.items()\n    except AttributeError:\n        items = data.iteritems()\n    for (name, col) in items:\n        data[name] = data[name] + ' ' * (len(name) // 2)\n    data.index.name = 'Dep. variable'\n    data = data.reset_index()\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Observed variables / factor loadings'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    data = self.factor_block_orders.reset_index()\n    data['block'] = data['block'].map(lambda factor_names: ', '.join(factor_names))\n    try:\n        data[['order']] = data[['order']].map(str)\n    except AttributeError:\n        data[['order']] = data[['order']].applymap(str)\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Factor blocks:'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    return summary",
        "mutated": [
            "def summary(self, truncate_endog_names=None):\n    if False:\n        i = 10\n    '\\n        Create a summary table describing the model.\\n\\n        Parameters\\n        ----------\\n        truncate_endog_names : int, optional\\n            The number of characters to show for names of observed variables.\\n            Default is 24 if there is more than one observed variable, or\\n            an unlimited number of there is only one.\\n        '\n    endog_names = self._get_endog_names(truncate=truncate_endog_names, as_string=True)\n    title = 'Model Specification: Dynamic Factor Model'\n    if self._index_dates:\n        ix = self._index\n        d = ix[0]\n        sample = ['%s' % d]\n        d = ix[-1]\n        sample += ['- ' + '%s' % d]\n    else:\n        sample = [str(0), ' - ' + str(self.nobs)]\n    model_name = self._model_name\n    top_left = []\n    top_left.append(('Model:', [model_name[0]]))\n    for i in range(1, len(model_name)):\n        top_left.append(('', ['+ ' + model_name[i]]))\n    top_left += [('Sample:', [sample[0]]), ('', [sample[1]])]\n    top_right = []\n    if self.k_endog_Q > 0:\n        top_right += [('# of monthly variables:', [self.k_endog_M]), ('# of quarterly variables:', [self.k_endog_Q])]\n    else:\n        top_right += [('# of observed variables:', [self.k_endog])]\n    if self.k_factor_blocks == 1:\n        top_right += [('# of factors:', [self.k_factors])]\n    else:\n        top_right += [('# of factor blocks:', [self.k_factor_blocks])]\n    top_right += [('Idiosyncratic disturbances:', ['AR(1)' if self.idiosyncratic_ar1 else 'iid']), ('Standardize variables:', [self.standardize])]\n    summary = Summary()\n    self.model = self\n    summary.add_table_2cols(self, gleft=top_left, gright=top_right, title=title)\n    table_ix = 1\n    del self.model\n    data = self.endog_factor_map.replace({True: 'X', False: ''})\n    data.index = endog_names\n    try:\n        items = data.items()\n    except AttributeError:\n        items = data.iteritems()\n    for (name, col) in items:\n        data[name] = data[name] + ' ' * (len(name) // 2)\n    data.index.name = 'Dep. variable'\n    data = data.reset_index()\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Observed variables / factor loadings'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    data = self.factor_block_orders.reset_index()\n    data['block'] = data['block'].map(lambda factor_names: ', '.join(factor_names))\n    try:\n        data[['order']] = data[['order']].map(str)\n    except AttributeError:\n        data[['order']] = data[['order']].applymap(str)\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Factor blocks:'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    return summary",
            "def summary(self, truncate_endog_names=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a summary table describing the model.\\n\\n        Parameters\\n        ----------\\n        truncate_endog_names : int, optional\\n            The number of characters to show for names of observed variables.\\n            Default is 24 if there is more than one observed variable, or\\n            an unlimited number of there is only one.\\n        '\n    endog_names = self._get_endog_names(truncate=truncate_endog_names, as_string=True)\n    title = 'Model Specification: Dynamic Factor Model'\n    if self._index_dates:\n        ix = self._index\n        d = ix[0]\n        sample = ['%s' % d]\n        d = ix[-1]\n        sample += ['- ' + '%s' % d]\n    else:\n        sample = [str(0), ' - ' + str(self.nobs)]\n    model_name = self._model_name\n    top_left = []\n    top_left.append(('Model:', [model_name[0]]))\n    for i in range(1, len(model_name)):\n        top_left.append(('', ['+ ' + model_name[i]]))\n    top_left += [('Sample:', [sample[0]]), ('', [sample[1]])]\n    top_right = []\n    if self.k_endog_Q > 0:\n        top_right += [('# of monthly variables:', [self.k_endog_M]), ('# of quarterly variables:', [self.k_endog_Q])]\n    else:\n        top_right += [('# of observed variables:', [self.k_endog])]\n    if self.k_factor_blocks == 1:\n        top_right += [('# of factors:', [self.k_factors])]\n    else:\n        top_right += [('# of factor blocks:', [self.k_factor_blocks])]\n    top_right += [('Idiosyncratic disturbances:', ['AR(1)' if self.idiosyncratic_ar1 else 'iid']), ('Standardize variables:', [self.standardize])]\n    summary = Summary()\n    self.model = self\n    summary.add_table_2cols(self, gleft=top_left, gright=top_right, title=title)\n    table_ix = 1\n    del self.model\n    data = self.endog_factor_map.replace({True: 'X', False: ''})\n    data.index = endog_names\n    try:\n        items = data.items()\n    except AttributeError:\n        items = data.iteritems()\n    for (name, col) in items:\n        data[name] = data[name] + ' ' * (len(name) // 2)\n    data.index.name = 'Dep. variable'\n    data = data.reset_index()\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Observed variables / factor loadings'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    data = self.factor_block_orders.reset_index()\n    data['block'] = data['block'].map(lambda factor_names: ', '.join(factor_names))\n    try:\n        data[['order']] = data[['order']].map(str)\n    except AttributeError:\n        data[['order']] = data[['order']].applymap(str)\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Factor blocks:'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    return summary",
            "def summary(self, truncate_endog_names=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a summary table describing the model.\\n\\n        Parameters\\n        ----------\\n        truncate_endog_names : int, optional\\n            The number of characters to show for names of observed variables.\\n            Default is 24 if there is more than one observed variable, or\\n            an unlimited number of there is only one.\\n        '\n    endog_names = self._get_endog_names(truncate=truncate_endog_names, as_string=True)\n    title = 'Model Specification: Dynamic Factor Model'\n    if self._index_dates:\n        ix = self._index\n        d = ix[0]\n        sample = ['%s' % d]\n        d = ix[-1]\n        sample += ['- ' + '%s' % d]\n    else:\n        sample = [str(0), ' - ' + str(self.nobs)]\n    model_name = self._model_name\n    top_left = []\n    top_left.append(('Model:', [model_name[0]]))\n    for i in range(1, len(model_name)):\n        top_left.append(('', ['+ ' + model_name[i]]))\n    top_left += [('Sample:', [sample[0]]), ('', [sample[1]])]\n    top_right = []\n    if self.k_endog_Q > 0:\n        top_right += [('# of monthly variables:', [self.k_endog_M]), ('# of quarterly variables:', [self.k_endog_Q])]\n    else:\n        top_right += [('# of observed variables:', [self.k_endog])]\n    if self.k_factor_blocks == 1:\n        top_right += [('# of factors:', [self.k_factors])]\n    else:\n        top_right += [('# of factor blocks:', [self.k_factor_blocks])]\n    top_right += [('Idiosyncratic disturbances:', ['AR(1)' if self.idiosyncratic_ar1 else 'iid']), ('Standardize variables:', [self.standardize])]\n    summary = Summary()\n    self.model = self\n    summary.add_table_2cols(self, gleft=top_left, gright=top_right, title=title)\n    table_ix = 1\n    del self.model\n    data = self.endog_factor_map.replace({True: 'X', False: ''})\n    data.index = endog_names\n    try:\n        items = data.items()\n    except AttributeError:\n        items = data.iteritems()\n    for (name, col) in items:\n        data[name] = data[name] + ' ' * (len(name) // 2)\n    data.index.name = 'Dep. variable'\n    data = data.reset_index()\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Observed variables / factor loadings'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    data = self.factor_block_orders.reset_index()\n    data['block'] = data['block'].map(lambda factor_names: ', '.join(factor_names))\n    try:\n        data[['order']] = data[['order']].map(str)\n    except AttributeError:\n        data[['order']] = data[['order']].applymap(str)\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Factor blocks:'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    return summary",
            "def summary(self, truncate_endog_names=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a summary table describing the model.\\n\\n        Parameters\\n        ----------\\n        truncate_endog_names : int, optional\\n            The number of characters to show for names of observed variables.\\n            Default is 24 if there is more than one observed variable, or\\n            an unlimited number of there is only one.\\n        '\n    endog_names = self._get_endog_names(truncate=truncate_endog_names, as_string=True)\n    title = 'Model Specification: Dynamic Factor Model'\n    if self._index_dates:\n        ix = self._index\n        d = ix[0]\n        sample = ['%s' % d]\n        d = ix[-1]\n        sample += ['- ' + '%s' % d]\n    else:\n        sample = [str(0), ' - ' + str(self.nobs)]\n    model_name = self._model_name\n    top_left = []\n    top_left.append(('Model:', [model_name[0]]))\n    for i in range(1, len(model_name)):\n        top_left.append(('', ['+ ' + model_name[i]]))\n    top_left += [('Sample:', [sample[0]]), ('', [sample[1]])]\n    top_right = []\n    if self.k_endog_Q > 0:\n        top_right += [('# of monthly variables:', [self.k_endog_M]), ('# of quarterly variables:', [self.k_endog_Q])]\n    else:\n        top_right += [('# of observed variables:', [self.k_endog])]\n    if self.k_factor_blocks == 1:\n        top_right += [('# of factors:', [self.k_factors])]\n    else:\n        top_right += [('# of factor blocks:', [self.k_factor_blocks])]\n    top_right += [('Idiosyncratic disturbances:', ['AR(1)' if self.idiosyncratic_ar1 else 'iid']), ('Standardize variables:', [self.standardize])]\n    summary = Summary()\n    self.model = self\n    summary.add_table_2cols(self, gleft=top_left, gright=top_right, title=title)\n    table_ix = 1\n    del self.model\n    data = self.endog_factor_map.replace({True: 'X', False: ''})\n    data.index = endog_names\n    try:\n        items = data.items()\n    except AttributeError:\n        items = data.iteritems()\n    for (name, col) in items:\n        data[name] = data[name] + ' ' * (len(name) // 2)\n    data.index.name = 'Dep. variable'\n    data = data.reset_index()\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Observed variables / factor loadings'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    data = self.factor_block_orders.reset_index()\n    data['block'] = data['block'].map(lambda factor_names: ', '.join(factor_names))\n    try:\n        data[['order']] = data[['order']].map(str)\n    except AttributeError:\n        data[['order']] = data[['order']].applymap(str)\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Factor blocks:'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    return summary",
            "def summary(self, truncate_endog_names=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a summary table describing the model.\\n\\n        Parameters\\n        ----------\\n        truncate_endog_names : int, optional\\n            The number of characters to show for names of observed variables.\\n            Default is 24 if there is more than one observed variable, or\\n            an unlimited number of there is only one.\\n        '\n    endog_names = self._get_endog_names(truncate=truncate_endog_names, as_string=True)\n    title = 'Model Specification: Dynamic Factor Model'\n    if self._index_dates:\n        ix = self._index\n        d = ix[0]\n        sample = ['%s' % d]\n        d = ix[-1]\n        sample += ['- ' + '%s' % d]\n    else:\n        sample = [str(0), ' - ' + str(self.nobs)]\n    model_name = self._model_name\n    top_left = []\n    top_left.append(('Model:', [model_name[0]]))\n    for i in range(1, len(model_name)):\n        top_left.append(('', ['+ ' + model_name[i]]))\n    top_left += [('Sample:', [sample[0]]), ('', [sample[1]])]\n    top_right = []\n    if self.k_endog_Q > 0:\n        top_right += [('# of monthly variables:', [self.k_endog_M]), ('# of quarterly variables:', [self.k_endog_Q])]\n    else:\n        top_right += [('# of observed variables:', [self.k_endog])]\n    if self.k_factor_blocks == 1:\n        top_right += [('# of factors:', [self.k_factors])]\n    else:\n        top_right += [('# of factor blocks:', [self.k_factor_blocks])]\n    top_right += [('Idiosyncratic disturbances:', ['AR(1)' if self.idiosyncratic_ar1 else 'iid']), ('Standardize variables:', [self.standardize])]\n    summary = Summary()\n    self.model = self\n    summary.add_table_2cols(self, gleft=top_left, gright=top_right, title=title)\n    table_ix = 1\n    del self.model\n    data = self.endog_factor_map.replace({True: 'X', False: ''})\n    data.index = endog_names\n    try:\n        items = data.items()\n    except AttributeError:\n        items = data.iteritems()\n    for (name, col) in items:\n        data[name] = data[name] + ' ' * (len(name) // 2)\n    data.index.name = 'Dep. variable'\n    data = data.reset_index()\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Observed variables / factor loadings'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    data = self.factor_block_orders.reset_index()\n    data['block'] = data['block'].map(lambda factor_names: ', '.join(factor_names))\n    try:\n        data[['order']] = data[['order']].map(str)\n    except AttributeError:\n        data[['order']] = data[['order']].applymap(str)\n    params_data = data.values\n    params_header = data.columns.map(str).tolist()\n    params_stubs = None\n    title = 'Factor blocks:'\n    table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n    summary.tables.insert(table_ix, table)\n    table_ix += 1\n    return summary"
        ]
    },
    {
        "func_name": "__str__",
        "original": "def __str__(self):\n    \"\"\"Summary tables showing model specification.\"\"\"\n    return str(self.summary())",
        "mutated": [
            "def __str__(self):\n    if False:\n        i = 10\n    'Summary tables showing model specification.'\n    return str(self.summary())",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Summary tables showing model specification.'\n    return str(self.summary())",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Summary tables showing model specification.'\n    return str(self.summary())",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Summary tables showing model specification.'\n    return str(self.summary())",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Summary tables showing model specification.'\n    return str(self.summary())"
        ]
    },
    {
        "func_name": "state_names",
        "original": "@property\ndef state_names(self):\n    \"\"\"(list of str) List of human readable names for unobserved states.\"\"\"\n    state_names = []\n    for block in self._s.factor_blocks:\n        state_names += [f'{name}' for name in block.factor_names[:]]\n        for s in range(1, block._factor_order):\n            state_names += [f'L{s}.{name}' for name in block.factor_names]\n    endog_names = self._get_endog_names()\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        state_names += [f'eps_M.{name}' for name in endog_names_M]\n    endog_names_Q = endog_names[self._o['Q']]\n    state_names += [f'eps_Q.{name}' for name in endog_names_Q]\n    for s in range(1, 5):\n        state_names += [f'L{s}.eps_Q.{name}' for name in endog_names_Q]\n    return state_names",
        "mutated": [
            "@property\ndef state_names(self):\n    if False:\n        i = 10\n    '(list of str) List of human readable names for unobserved states.'\n    state_names = []\n    for block in self._s.factor_blocks:\n        state_names += [f'{name}' for name in block.factor_names[:]]\n        for s in range(1, block._factor_order):\n            state_names += [f'L{s}.{name}' for name in block.factor_names]\n    endog_names = self._get_endog_names()\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        state_names += [f'eps_M.{name}' for name in endog_names_M]\n    endog_names_Q = endog_names[self._o['Q']]\n    state_names += [f'eps_Q.{name}' for name in endog_names_Q]\n    for s in range(1, 5):\n        state_names += [f'L{s}.eps_Q.{name}' for name in endog_names_Q]\n    return state_names",
            "@property\ndef state_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '(list of str) List of human readable names for unobserved states.'\n    state_names = []\n    for block in self._s.factor_blocks:\n        state_names += [f'{name}' for name in block.factor_names[:]]\n        for s in range(1, block._factor_order):\n            state_names += [f'L{s}.{name}' for name in block.factor_names]\n    endog_names = self._get_endog_names()\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        state_names += [f'eps_M.{name}' for name in endog_names_M]\n    endog_names_Q = endog_names[self._o['Q']]\n    state_names += [f'eps_Q.{name}' for name in endog_names_Q]\n    for s in range(1, 5):\n        state_names += [f'L{s}.eps_Q.{name}' for name in endog_names_Q]\n    return state_names",
            "@property\ndef state_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '(list of str) List of human readable names for unobserved states.'\n    state_names = []\n    for block in self._s.factor_blocks:\n        state_names += [f'{name}' for name in block.factor_names[:]]\n        for s in range(1, block._factor_order):\n            state_names += [f'L{s}.{name}' for name in block.factor_names]\n    endog_names = self._get_endog_names()\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        state_names += [f'eps_M.{name}' for name in endog_names_M]\n    endog_names_Q = endog_names[self._o['Q']]\n    state_names += [f'eps_Q.{name}' for name in endog_names_Q]\n    for s in range(1, 5):\n        state_names += [f'L{s}.eps_Q.{name}' for name in endog_names_Q]\n    return state_names",
            "@property\ndef state_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '(list of str) List of human readable names for unobserved states.'\n    state_names = []\n    for block in self._s.factor_blocks:\n        state_names += [f'{name}' for name in block.factor_names[:]]\n        for s in range(1, block._factor_order):\n            state_names += [f'L{s}.{name}' for name in block.factor_names]\n    endog_names = self._get_endog_names()\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        state_names += [f'eps_M.{name}' for name in endog_names_M]\n    endog_names_Q = endog_names[self._o['Q']]\n    state_names += [f'eps_Q.{name}' for name in endog_names_Q]\n    for s in range(1, 5):\n        state_names += [f'L{s}.eps_Q.{name}' for name in endog_names_Q]\n    return state_names",
            "@property\ndef state_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '(list of str) List of human readable names for unobserved states.'\n    state_names = []\n    for block in self._s.factor_blocks:\n        state_names += [f'{name}' for name in block.factor_names[:]]\n        for s in range(1, block._factor_order):\n            state_names += [f'L{s}.{name}' for name in block.factor_names]\n    endog_names = self._get_endog_names()\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        state_names += [f'eps_M.{name}' for name in endog_names_M]\n    endog_names_Q = endog_names[self._o['Q']]\n    state_names += [f'eps_Q.{name}' for name in endog_names_Q]\n    for s in range(1, 5):\n        state_names += [f'L{s}.eps_Q.{name}' for name in endog_names_Q]\n    return state_names"
        ]
    },
    {
        "func_name": "param_names",
        "original": "@property\ndef param_names(self):\n    \"\"\"(list of str) List of human readable parameter names.\"\"\"\n    param_names = []\n    endog_names = self._get_endog_names(as_string=False)\n    for endog_name in endog_names:\n        for block in self._s.factor_blocks:\n            for factor_name in block.factor_names:\n                if self.endog_factor_map.loc[endog_name, factor_name]:\n                    param_names.append(f'loading.{factor_name}->{endog_name}')\n    for block in self._s.factor_blocks:\n        for to_factor in block.factor_names:\n            param_names += [f'L{i}.{from_factor}->{to_factor}' for i in range(1, block.factor_order + 1) for from_factor in block.factor_names]\n    for i in range(len(self._s.factor_blocks)):\n        block = self._s.factor_blocks[i]\n        param_names += [f'fb({i}).cov.chol[{j + 1},{k + 1}]' for j in range(block.k_factors) for k in range(j + 1)]\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        param_names += [f'L1.eps_M.{name}' for name in endog_names_M]\n        endog_names_Q = endog_names[self._o['Q']]\n        param_names += [f'L1.eps_Q.{name}' for name in endog_names_Q]\n    param_names += [f'sigma2.{name}' for name in endog_names]\n    return param_names",
        "mutated": [
            "@property\ndef param_names(self):\n    if False:\n        i = 10\n    '(list of str) List of human readable parameter names.'\n    param_names = []\n    endog_names = self._get_endog_names(as_string=False)\n    for endog_name in endog_names:\n        for block in self._s.factor_blocks:\n            for factor_name in block.factor_names:\n                if self.endog_factor_map.loc[endog_name, factor_name]:\n                    param_names.append(f'loading.{factor_name}->{endog_name}')\n    for block in self._s.factor_blocks:\n        for to_factor in block.factor_names:\n            param_names += [f'L{i}.{from_factor}->{to_factor}' for i in range(1, block.factor_order + 1) for from_factor in block.factor_names]\n    for i in range(len(self._s.factor_blocks)):\n        block = self._s.factor_blocks[i]\n        param_names += [f'fb({i}).cov.chol[{j + 1},{k + 1}]' for j in range(block.k_factors) for k in range(j + 1)]\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        param_names += [f'L1.eps_M.{name}' for name in endog_names_M]\n        endog_names_Q = endog_names[self._o['Q']]\n        param_names += [f'L1.eps_Q.{name}' for name in endog_names_Q]\n    param_names += [f'sigma2.{name}' for name in endog_names]\n    return param_names",
            "@property\ndef param_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '(list of str) List of human readable parameter names.'\n    param_names = []\n    endog_names = self._get_endog_names(as_string=False)\n    for endog_name in endog_names:\n        for block in self._s.factor_blocks:\n            for factor_name in block.factor_names:\n                if self.endog_factor_map.loc[endog_name, factor_name]:\n                    param_names.append(f'loading.{factor_name}->{endog_name}')\n    for block in self._s.factor_blocks:\n        for to_factor in block.factor_names:\n            param_names += [f'L{i}.{from_factor}->{to_factor}' for i in range(1, block.factor_order + 1) for from_factor in block.factor_names]\n    for i in range(len(self._s.factor_blocks)):\n        block = self._s.factor_blocks[i]\n        param_names += [f'fb({i}).cov.chol[{j + 1},{k + 1}]' for j in range(block.k_factors) for k in range(j + 1)]\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        param_names += [f'L1.eps_M.{name}' for name in endog_names_M]\n        endog_names_Q = endog_names[self._o['Q']]\n        param_names += [f'L1.eps_Q.{name}' for name in endog_names_Q]\n    param_names += [f'sigma2.{name}' for name in endog_names]\n    return param_names",
            "@property\ndef param_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '(list of str) List of human readable parameter names.'\n    param_names = []\n    endog_names = self._get_endog_names(as_string=False)\n    for endog_name in endog_names:\n        for block in self._s.factor_blocks:\n            for factor_name in block.factor_names:\n                if self.endog_factor_map.loc[endog_name, factor_name]:\n                    param_names.append(f'loading.{factor_name}->{endog_name}')\n    for block in self._s.factor_blocks:\n        for to_factor in block.factor_names:\n            param_names += [f'L{i}.{from_factor}->{to_factor}' for i in range(1, block.factor_order + 1) for from_factor in block.factor_names]\n    for i in range(len(self._s.factor_blocks)):\n        block = self._s.factor_blocks[i]\n        param_names += [f'fb({i}).cov.chol[{j + 1},{k + 1}]' for j in range(block.k_factors) for k in range(j + 1)]\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        param_names += [f'L1.eps_M.{name}' for name in endog_names_M]\n        endog_names_Q = endog_names[self._o['Q']]\n        param_names += [f'L1.eps_Q.{name}' for name in endog_names_Q]\n    param_names += [f'sigma2.{name}' for name in endog_names]\n    return param_names",
            "@property\ndef param_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '(list of str) List of human readable parameter names.'\n    param_names = []\n    endog_names = self._get_endog_names(as_string=False)\n    for endog_name in endog_names:\n        for block in self._s.factor_blocks:\n            for factor_name in block.factor_names:\n                if self.endog_factor_map.loc[endog_name, factor_name]:\n                    param_names.append(f'loading.{factor_name}->{endog_name}')\n    for block in self._s.factor_blocks:\n        for to_factor in block.factor_names:\n            param_names += [f'L{i}.{from_factor}->{to_factor}' for i in range(1, block.factor_order + 1) for from_factor in block.factor_names]\n    for i in range(len(self._s.factor_blocks)):\n        block = self._s.factor_blocks[i]\n        param_names += [f'fb({i}).cov.chol[{j + 1},{k + 1}]' for j in range(block.k_factors) for k in range(j + 1)]\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        param_names += [f'L1.eps_M.{name}' for name in endog_names_M]\n        endog_names_Q = endog_names[self._o['Q']]\n        param_names += [f'L1.eps_Q.{name}' for name in endog_names_Q]\n    param_names += [f'sigma2.{name}' for name in endog_names]\n    return param_names",
            "@property\ndef param_names(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '(list of str) List of human readable parameter names.'\n    param_names = []\n    endog_names = self._get_endog_names(as_string=False)\n    for endog_name in endog_names:\n        for block in self._s.factor_blocks:\n            for factor_name in block.factor_names:\n                if self.endog_factor_map.loc[endog_name, factor_name]:\n                    param_names.append(f'loading.{factor_name}->{endog_name}')\n    for block in self._s.factor_blocks:\n        for to_factor in block.factor_names:\n            param_names += [f'L{i}.{from_factor}->{to_factor}' for i in range(1, block.factor_order + 1) for from_factor in block.factor_names]\n    for i in range(len(self._s.factor_blocks)):\n        block = self._s.factor_blocks[i]\n        param_names += [f'fb({i}).cov.chol[{j + 1},{k + 1}]' for j in range(block.k_factors) for k in range(j + 1)]\n    if self.idiosyncratic_ar1:\n        endog_names_M = endog_names[self._o['M']]\n        param_names += [f'L1.eps_M.{name}' for name in endog_names_M]\n        endog_names_Q = endog_names[self._o['Q']]\n        param_names += [f'L1.eps_Q.{name}' for name in endog_names_Q]\n    param_names += [f'sigma2.{name}' for name in endog_names]\n    return param_names"
        ]
    },
    {
        "func_name": "start_params",
        "original": "@property\ndef start_params(self):\n    \"\"\"(array) Starting parameters for maximum likelihood estimation.\"\"\"\n    params = np.zeros(self.k_params, dtype=np.float64)\n    endog_factor_map_M = self.endog_factor_map.iloc[:self.k_endog_M]\n    factors = []\n    endog = np.require(pd.DataFrame(self.endog).interpolate().bfill(), requirements='W')\n    for name in self.factor_names:\n        endog_ix = np.where(endog_factor_map_M.loc[:, name])[0]\n        if len(endog_ix) == 0:\n            endog_ix = np.where(self.endog_factor_map.loc[:, name])[0]\n        factor_endog = endog[:, endog_ix]\n        res_pca = PCA(factor_endog, ncomp=1, method='eig', normalize=False)\n        factors.append(res_pca.factors)\n        endog[:, endog_ix] -= res_pca.projection\n    factors = np.concatenate(factors, axis=1)\n    loadings = []\n    resid = []\n    for i in range(self.k_endog_M):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = factors[:, factor_ix]\n        mod_ols = OLS(self.endog[:, i], exog=factor_exog, missing='drop')\n        res_ols = mod_ols.fit()\n        loadings += res_ols.params.tolist()\n        resid.append(res_ols.resid)\n    for i in range(self.k_endog_M, self.k_endog):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = lagmat(factors[:, factor_ix], 4, original='in')\n        mod_glm = GLM(self.endog[:, i], factor_exog, missing='drop')\n        res_glm = mod_glm.fit_constrained(self.loading_constraints(i))\n        loadings += res_glm.params[:len(factor_ix)].tolist()\n        resid.append(res_glm.resid_response)\n    params[self._p['loadings']] = loadings\n    stationary = True\n    factor_ar = []\n    factor_cov = []\n    i = 0\n    for block in self._s.factor_blocks:\n        factors_endog = factors[:, i:i + block.k_factors]\n        i += block.k_factors\n        if block.factor_order == 0:\n            continue\n        if block.k_factors == 1:\n            mod_factors = SARIMAX(factors_endog, order=(block.factor_order, 0, 0))\n            sp = mod_factors.start_params\n            block_factor_ar = sp[:-1]\n            block_factor_cov = sp[-1:]\n            coefficient_matrices = mod_factors.start_params[:-1]\n        elif block.k_factors > 1:\n            mod_factors = VAR(factors_endog)\n            res_factors = mod_factors.fit(maxlags=block.factor_order, ic=None, trend='n')\n            block_factor_ar = res_factors.params.T.ravel()\n            L = np.linalg.cholesky(res_factors.sigma_u)\n            block_factor_cov = L[np.tril_indices_from(L)]\n            coefficient_matrices = np.transpose(np.reshape(block_factor_ar, (block.k_factors, block.k_factors, block.factor_order)), (2, 0, 1))\n        stationary = is_invertible([1] + list(-coefficient_matrices))\n        if not stationary:\n            warn(f'Non-stationary starting factor autoregressive parameters found for factor block {block.factor_names}. Using zeros as starting parameters.')\n            block_factor_ar[:] = 0\n            cov_factor = np.diag(factors_endog.std(axis=0))\n            block_factor_cov = cov_factor[np.tril_indices(block.k_factors)]\n        factor_ar += block_factor_ar.tolist()\n        factor_cov += block_factor_cov.tolist()\n    params[self._p['factor_ar']] = factor_ar\n    params[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        idio_ar1 = []\n        idio_var = []\n        for i in range(self.k_endog_M):\n            mod_idio = SARIMAX(resid[i], order=(1, 0, 0), trend='c')\n            sp = mod_idio.start_params\n            idio_ar1.append(np.clip(sp[1], -0.99, 0.99))\n            idio_var.append(np.clip(sp[-1], 1e-05, np.inf))\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(maxiter=10, return_params=True, disp=False)\n            res_idio = mod_idio.fit_em(res_idio, maxiter=5, return_params=True)\n            idio_ar1.append(np.clip(res_idio[0], -0.99, 0.99))\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_ar1']] = idio_ar1\n        params[self._p['idiosyncratic_var']] = idio_var\n    else:\n        idio_var = [np.var(resid[i]) for i in range(self.k_endog_M)]\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(return_params=True, disp=False)\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_var']] = idio_var\n    return params",
        "mutated": [
            "@property\ndef start_params(self):\n    if False:\n        i = 10\n    '(array) Starting parameters for maximum likelihood estimation.'\n    params = np.zeros(self.k_params, dtype=np.float64)\n    endog_factor_map_M = self.endog_factor_map.iloc[:self.k_endog_M]\n    factors = []\n    endog = np.require(pd.DataFrame(self.endog).interpolate().bfill(), requirements='W')\n    for name in self.factor_names:\n        endog_ix = np.where(endog_factor_map_M.loc[:, name])[0]\n        if len(endog_ix) == 0:\n            endog_ix = np.where(self.endog_factor_map.loc[:, name])[0]\n        factor_endog = endog[:, endog_ix]\n        res_pca = PCA(factor_endog, ncomp=1, method='eig', normalize=False)\n        factors.append(res_pca.factors)\n        endog[:, endog_ix] -= res_pca.projection\n    factors = np.concatenate(factors, axis=1)\n    loadings = []\n    resid = []\n    for i in range(self.k_endog_M):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = factors[:, factor_ix]\n        mod_ols = OLS(self.endog[:, i], exog=factor_exog, missing='drop')\n        res_ols = mod_ols.fit()\n        loadings += res_ols.params.tolist()\n        resid.append(res_ols.resid)\n    for i in range(self.k_endog_M, self.k_endog):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = lagmat(factors[:, factor_ix], 4, original='in')\n        mod_glm = GLM(self.endog[:, i], factor_exog, missing='drop')\n        res_glm = mod_glm.fit_constrained(self.loading_constraints(i))\n        loadings += res_glm.params[:len(factor_ix)].tolist()\n        resid.append(res_glm.resid_response)\n    params[self._p['loadings']] = loadings\n    stationary = True\n    factor_ar = []\n    factor_cov = []\n    i = 0\n    for block in self._s.factor_blocks:\n        factors_endog = factors[:, i:i + block.k_factors]\n        i += block.k_factors\n        if block.factor_order == 0:\n            continue\n        if block.k_factors == 1:\n            mod_factors = SARIMAX(factors_endog, order=(block.factor_order, 0, 0))\n            sp = mod_factors.start_params\n            block_factor_ar = sp[:-1]\n            block_factor_cov = sp[-1:]\n            coefficient_matrices = mod_factors.start_params[:-1]\n        elif block.k_factors > 1:\n            mod_factors = VAR(factors_endog)\n            res_factors = mod_factors.fit(maxlags=block.factor_order, ic=None, trend='n')\n            block_factor_ar = res_factors.params.T.ravel()\n            L = np.linalg.cholesky(res_factors.sigma_u)\n            block_factor_cov = L[np.tril_indices_from(L)]\n            coefficient_matrices = np.transpose(np.reshape(block_factor_ar, (block.k_factors, block.k_factors, block.factor_order)), (2, 0, 1))\n        stationary = is_invertible([1] + list(-coefficient_matrices))\n        if not stationary:\n            warn(f'Non-stationary starting factor autoregressive parameters found for factor block {block.factor_names}. Using zeros as starting parameters.')\n            block_factor_ar[:] = 0\n            cov_factor = np.diag(factors_endog.std(axis=0))\n            block_factor_cov = cov_factor[np.tril_indices(block.k_factors)]\n        factor_ar += block_factor_ar.tolist()\n        factor_cov += block_factor_cov.tolist()\n    params[self._p['factor_ar']] = factor_ar\n    params[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        idio_ar1 = []\n        idio_var = []\n        for i in range(self.k_endog_M):\n            mod_idio = SARIMAX(resid[i], order=(1, 0, 0), trend='c')\n            sp = mod_idio.start_params\n            idio_ar1.append(np.clip(sp[1], -0.99, 0.99))\n            idio_var.append(np.clip(sp[-1], 1e-05, np.inf))\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(maxiter=10, return_params=True, disp=False)\n            res_idio = mod_idio.fit_em(res_idio, maxiter=5, return_params=True)\n            idio_ar1.append(np.clip(res_idio[0], -0.99, 0.99))\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_ar1']] = idio_ar1\n        params[self._p['idiosyncratic_var']] = idio_var\n    else:\n        idio_var = [np.var(resid[i]) for i in range(self.k_endog_M)]\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(return_params=True, disp=False)\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_var']] = idio_var\n    return params",
            "@property\ndef start_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '(array) Starting parameters for maximum likelihood estimation.'\n    params = np.zeros(self.k_params, dtype=np.float64)\n    endog_factor_map_M = self.endog_factor_map.iloc[:self.k_endog_M]\n    factors = []\n    endog = np.require(pd.DataFrame(self.endog).interpolate().bfill(), requirements='W')\n    for name in self.factor_names:\n        endog_ix = np.where(endog_factor_map_M.loc[:, name])[0]\n        if len(endog_ix) == 0:\n            endog_ix = np.where(self.endog_factor_map.loc[:, name])[0]\n        factor_endog = endog[:, endog_ix]\n        res_pca = PCA(factor_endog, ncomp=1, method='eig', normalize=False)\n        factors.append(res_pca.factors)\n        endog[:, endog_ix] -= res_pca.projection\n    factors = np.concatenate(factors, axis=1)\n    loadings = []\n    resid = []\n    for i in range(self.k_endog_M):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = factors[:, factor_ix]\n        mod_ols = OLS(self.endog[:, i], exog=factor_exog, missing='drop')\n        res_ols = mod_ols.fit()\n        loadings += res_ols.params.tolist()\n        resid.append(res_ols.resid)\n    for i in range(self.k_endog_M, self.k_endog):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = lagmat(factors[:, factor_ix], 4, original='in')\n        mod_glm = GLM(self.endog[:, i], factor_exog, missing='drop')\n        res_glm = mod_glm.fit_constrained(self.loading_constraints(i))\n        loadings += res_glm.params[:len(factor_ix)].tolist()\n        resid.append(res_glm.resid_response)\n    params[self._p['loadings']] = loadings\n    stationary = True\n    factor_ar = []\n    factor_cov = []\n    i = 0\n    for block in self._s.factor_blocks:\n        factors_endog = factors[:, i:i + block.k_factors]\n        i += block.k_factors\n        if block.factor_order == 0:\n            continue\n        if block.k_factors == 1:\n            mod_factors = SARIMAX(factors_endog, order=(block.factor_order, 0, 0))\n            sp = mod_factors.start_params\n            block_factor_ar = sp[:-1]\n            block_factor_cov = sp[-1:]\n            coefficient_matrices = mod_factors.start_params[:-1]\n        elif block.k_factors > 1:\n            mod_factors = VAR(factors_endog)\n            res_factors = mod_factors.fit(maxlags=block.factor_order, ic=None, trend='n')\n            block_factor_ar = res_factors.params.T.ravel()\n            L = np.linalg.cholesky(res_factors.sigma_u)\n            block_factor_cov = L[np.tril_indices_from(L)]\n            coefficient_matrices = np.transpose(np.reshape(block_factor_ar, (block.k_factors, block.k_factors, block.factor_order)), (2, 0, 1))\n        stationary = is_invertible([1] + list(-coefficient_matrices))\n        if not stationary:\n            warn(f'Non-stationary starting factor autoregressive parameters found for factor block {block.factor_names}. Using zeros as starting parameters.')\n            block_factor_ar[:] = 0\n            cov_factor = np.diag(factors_endog.std(axis=0))\n            block_factor_cov = cov_factor[np.tril_indices(block.k_factors)]\n        factor_ar += block_factor_ar.tolist()\n        factor_cov += block_factor_cov.tolist()\n    params[self._p['factor_ar']] = factor_ar\n    params[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        idio_ar1 = []\n        idio_var = []\n        for i in range(self.k_endog_M):\n            mod_idio = SARIMAX(resid[i], order=(1, 0, 0), trend='c')\n            sp = mod_idio.start_params\n            idio_ar1.append(np.clip(sp[1], -0.99, 0.99))\n            idio_var.append(np.clip(sp[-1], 1e-05, np.inf))\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(maxiter=10, return_params=True, disp=False)\n            res_idio = mod_idio.fit_em(res_idio, maxiter=5, return_params=True)\n            idio_ar1.append(np.clip(res_idio[0], -0.99, 0.99))\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_ar1']] = idio_ar1\n        params[self._p['idiosyncratic_var']] = idio_var\n    else:\n        idio_var = [np.var(resid[i]) for i in range(self.k_endog_M)]\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(return_params=True, disp=False)\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_var']] = idio_var\n    return params",
            "@property\ndef start_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '(array) Starting parameters for maximum likelihood estimation.'\n    params = np.zeros(self.k_params, dtype=np.float64)\n    endog_factor_map_M = self.endog_factor_map.iloc[:self.k_endog_M]\n    factors = []\n    endog = np.require(pd.DataFrame(self.endog).interpolate().bfill(), requirements='W')\n    for name in self.factor_names:\n        endog_ix = np.where(endog_factor_map_M.loc[:, name])[0]\n        if len(endog_ix) == 0:\n            endog_ix = np.where(self.endog_factor_map.loc[:, name])[0]\n        factor_endog = endog[:, endog_ix]\n        res_pca = PCA(factor_endog, ncomp=1, method='eig', normalize=False)\n        factors.append(res_pca.factors)\n        endog[:, endog_ix] -= res_pca.projection\n    factors = np.concatenate(factors, axis=1)\n    loadings = []\n    resid = []\n    for i in range(self.k_endog_M):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = factors[:, factor_ix]\n        mod_ols = OLS(self.endog[:, i], exog=factor_exog, missing='drop')\n        res_ols = mod_ols.fit()\n        loadings += res_ols.params.tolist()\n        resid.append(res_ols.resid)\n    for i in range(self.k_endog_M, self.k_endog):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = lagmat(factors[:, factor_ix], 4, original='in')\n        mod_glm = GLM(self.endog[:, i], factor_exog, missing='drop')\n        res_glm = mod_glm.fit_constrained(self.loading_constraints(i))\n        loadings += res_glm.params[:len(factor_ix)].tolist()\n        resid.append(res_glm.resid_response)\n    params[self._p['loadings']] = loadings\n    stationary = True\n    factor_ar = []\n    factor_cov = []\n    i = 0\n    for block in self._s.factor_blocks:\n        factors_endog = factors[:, i:i + block.k_factors]\n        i += block.k_factors\n        if block.factor_order == 0:\n            continue\n        if block.k_factors == 1:\n            mod_factors = SARIMAX(factors_endog, order=(block.factor_order, 0, 0))\n            sp = mod_factors.start_params\n            block_factor_ar = sp[:-1]\n            block_factor_cov = sp[-1:]\n            coefficient_matrices = mod_factors.start_params[:-1]\n        elif block.k_factors > 1:\n            mod_factors = VAR(factors_endog)\n            res_factors = mod_factors.fit(maxlags=block.factor_order, ic=None, trend='n')\n            block_factor_ar = res_factors.params.T.ravel()\n            L = np.linalg.cholesky(res_factors.sigma_u)\n            block_factor_cov = L[np.tril_indices_from(L)]\n            coefficient_matrices = np.transpose(np.reshape(block_factor_ar, (block.k_factors, block.k_factors, block.factor_order)), (2, 0, 1))\n        stationary = is_invertible([1] + list(-coefficient_matrices))\n        if not stationary:\n            warn(f'Non-stationary starting factor autoregressive parameters found for factor block {block.factor_names}. Using zeros as starting parameters.')\n            block_factor_ar[:] = 0\n            cov_factor = np.diag(factors_endog.std(axis=0))\n            block_factor_cov = cov_factor[np.tril_indices(block.k_factors)]\n        factor_ar += block_factor_ar.tolist()\n        factor_cov += block_factor_cov.tolist()\n    params[self._p['factor_ar']] = factor_ar\n    params[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        idio_ar1 = []\n        idio_var = []\n        for i in range(self.k_endog_M):\n            mod_idio = SARIMAX(resid[i], order=(1, 0, 0), trend='c')\n            sp = mod_idio.start_params\n            idio_ar1.append(np.clip(sp[1], -0.99, 0.99))\n            idio_var.append(np.clip(sp[-1], 1e-05, np.inf))\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(maxiter=10, return_params=True, disp=False)\n            res_idio = mod_idio.fit_em(res_idio, maxiter=5, return_params=True)\n            idio_ar1.append(np.clip(res_idio[0], -0.99, 0.99))\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_ar1']] = idio_ar1\n        params[self._p['idiosyncratic_var']] = idio_var\n    else:\n        idio_var = [np.var(resid[i]) for i in range(self.k_endog_M)]\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(return_params=True, disp=False)\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_var']] = idio_var\n    return params",
            "@property\ndef start_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '(array) Starting parameters for maximum likelihood estimation.'\n    params = np.zeros(self.k_params, dtype=np.float64)\n    endog_factor_map_M = self.endog_factor_map.iloc[:self.k_endog_M]\n    factors = []\n    endog = np.require(pd.DataFrame(self.endog).interpolate().bfill(), requirements='W')\n    for name in self.factor_names:\n        endog_ix = np.where(endog_factor_map_M.loc[:, name])[0]\n        if len(endog_ix) == 0:\n            endog_ix = np.where(self.endog_factor_map.loc[:, name])[0]\n        factor_endog = endog[:, endog_ix]\n        res_pca = PCA(factor_endog, ncomp=1, method='eig', normalize=False)\n        factors.append(res_pca.factors)\n        endog[:, endog_ix] -= res_pca.projection\n    factors = np.concatenate(factors, axis=1)\n    loadings = []\n    resid = []\n    for i in range(self.k_endog_M):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = factors[:, factor_ix]\n        mod_ols = OLS(self.endog[:, i], exog=factor_exog, missing='drop')\n        res_ols = mod_ols.fit()\n        loadings += res_ols.params.tolist()\n        resid.append(res_ols.resid)\n    for i in range(self.k_endog_M, self.k_endog):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = lagmat(factors[:, factor_ix], 4, original='in')\n        mod_glm = GLM(self.endog[:, i], factor_exog, missing='drop')\n        res_glm = mod_glm.fit_constrained(self.loading_constraints(i))\n        loadings += res_glm.params[:len(factor_ix)].tolist()\n        resid.append(res_glm.resid_response)\n    params[self._p['loadings']] = loadings\n    stationary = True\n    factor_ar = []\n    factor_cov = []\n    i = 0\n    for block in self._s.factor_blocks:\n        factors_endog = factors[:, i:i + block.k_factors]\n        i += block.k_factors\n        if block.factor_order == 0:\n            continue\n        if block.k_factors == 1:\n            mod_factors = SARIMAX(factors_endog, order=(block.factor_order, 0, 0))\n            sp = mod_factors.start_params\n            block_factor_ar = sp[:-1]\n            block_factor_cov = sp[-1:]\n            coefficient_matrices = mod_factors.start_params[:-1]\n        elif block.k_factors > 1:\n            mod_factors = VAR(factors_endog)\n            res_factors = mod_factors.fit(maxlags=block.factor_order, ic=None, trend='n')\n            block_factor_ar = res_factors.params.T.ravel()\n            L = np.linalg.cholesky(res_factors.sigma_u)\n            block_factor_cov = L[np.tril_indices_from(L)]\n            coefficient_matrices = np.transpose(np.reshape(block_factor_ar, (block.k_factors, block.k_factors, block.factor_order)), (2, 0, 1))\n        stationary = is_invertible([1] + list(-coefficient_matrices))\n        if not stationary:\n            warn(f'Non-stationary starting factor autoregressive parameters found for factor block {block.factor_names}. Using zeros as starting parameters.')\n            block_factor_ar[:] = 0\n            cov_factor = np.diag(factors_endog.std(axis=0))\n            block_factor_cov = cov_factor[np.tril_indices(block.k_factors)]\n        factor_ar += block_factor_ar.tolist()\n        factor_cov += block_factor_cov.tolist()\n    params[self._p['factor_ar']] = factor_ar\n    params[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        idio_ar1 = []\n        idio_var = []\n        for i in range(self.k_endog_M):\n            mod_idio = SARIMAX(resid[i], order=(1, 0, 0), trend='c')\n            sp = mod_idio.start_params\n            idio_ar1.append(np.clip(sp[1], -0.99, 0.99))\n            idio_var.append(np.clip(sp[-1], 1e-05, np.inf))\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(maxiter=10, return_params=True, disp=False)\n            res_idio = mod_idio.fit_em(res_idio, maxiter=5, return_params=True)\n            idio_ar1.append(np.clip(res_idio[0], -0.99, 0.99))\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_ar1']] = idio_ar1\n        params[self._p['idiosyncratic_var']] = idio_var\n    else:\n        idio_var = [np.var(resid[i]) for i in range(self.k_endog_M)]\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(return_params=True, disp=False)\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_var']] = idio_var\n    return params",
            "@property\ndef start_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '(array) Starting parameters for maximum likelihood estimation.'\n    params = np.zeros(self.k_params, dtype=np.float64)\n    endog_factor_map_M = self.endog_factor_map.iloc[:self.k_endog_M]\n    factors = []\n    endog = np.require(pd.DataFrame(self.endog).interpolate().bfill(), requirements='W')\n    for name in self.factor_names:\n        endog_ix = np.where(endog_factor_map_M.loc[:, name])[0]\n        if len(endog_ix) == 0:\n            endog_ix = np.where(self.endog_factor_map.loc[:, name])[0]\n        factor_endog = endog[:, endog_ix]\n        res_pca = PCA(factor_endog, ncomp=1, method='eig', normalize=False)\n        factors.append(res_pca.factors)\n        endog[:, endog_ix] -= res_pca.projection\n    factors = np.concatenate(factors, axis=1)\n    loadings = []\n    resid = []\n    for i in range(self.k_endog_M):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = factors[:, factor_ix]\n        mod_ols = OLS(self.endog[:, i], exog=factor_exog, missing='drop')\n        res_ols = mod_ols.fit()\n        loadings += res_ols.params.tolist()\n        resid.append(res_ols.resid)\n    for i in range(self.k_endog_M, self.k_endog):\n        factor_ix = self._s.endog_factor_iloc[i]\n        factor_exog = lagmat(factors[:, factor_ix], 4, original='in')\n        mod_glm = GLM(self.endog[:, i], factor_exog, missing='drop')\n        res_glm = mod_glm.fit_constrained(self.loading_constraints(i))\n        loadings += res_glm.params[:len(factor_ix)].tolist()\n        resid.append(res_glm.resid_response)\n    params[self._p['loadings']] = loadings\n    stationary = True\n    factor_ar = []\n    factor_cov = []\n    i = 0\n    for block in self._s.factor_blocks:\n        factors_endog = factors[:, i:i + block.k_factors]\n        i += block.k_factors\n        if block.factor_order == 0:\n            continue\n        if block.k_factors == 1:\n            mod_factors = SARIMAX(factors_endog, order=(block.factor_order, 0, 0))\n            sp = mod_factors.start_params\n            block_factor_ar = sp[:-1]\n            block_factor_cov = sp[-1:]\n            coefficient_matrices = mod_factors.start_params[:-1]\n        elif block.k_factors > 1:\n            mod_factors = VAR(factors_endog)\n            res_factors = mod_factors.fit(maxlags=block.factor_order, ic=None, trend='n')\n            block_factor_ar = res_factors.params.T.ravel()\n            L = np.linalg.cholesky(res_factors.sigma_u)\n            block_factor_cov = L[np.tril_indices_from(L)]\n            coefficient_matrices = np.transpose(np.reshape(block_factor_ar, (block.k_factors, block.k_factors, block.factor_order)), (2, 0, 1))\n        stationary = is_invertible([1] + list(-coefficient_matrices))\n        if not stationary:\n            warn(f'Non-stationary starting factor autoregressive parameters found for factor block {block.factor_names}. Using zeros as starting parameters.')\n            block_factor_ar[:] = 0\n            cov_factor = np.diag(factors_endog.std(axis=0))\n            block_factor_cov = cov_factor[np.tril_indices(block.k_factors)]\n        factor_ar += block_factor_ar.tolist()\n        factor_cov += block_factor_cov.tolist()\n    params[self._p['factor_ar']] = factor_ar\n    params[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        idio_ar1 = []\n        idio_var = []\n        for i in range(self.k_endog_M):\n            mod_idio = SARIMAX(resid[i], order=(1, 0, 0), trend='c')\n            sp = mod_idio.start_params\n            idio_ar1.append(np.clip(sp[1], -0.99, 0.99))\n            idio_var.append(np.clip(sp[-1], 1e-05, np.inf))\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(maxiter=10, return_params=True, disp=False)\n            res_idio = mod_idio.fit_em(res_idio, maxiter=5, return_params=True)\n            idio_ar1.append(np.clip(res_idio[0], -0.99, 0.99))\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_ar1']] = idio_ar1\n        params[self._p['idiosyncratic_var']] = idio_var\n    else:\n        idio_var = [np.var(resid[i]) for i in range(self.k_endog_M)]\n        for i in range(self.k_endog_M, self.k_endog):\n            y = self.endog[:, i].copy()\n            y[~np.isnan(y)] = resid[i]\n            mod_idio = QuarterlyAR1(y)\n            res_idio = mod_idio.fit(return_params=True, disp=False)\n            idio_var.append(np.clip(res_idio[1], 1e-05, np.inf))\n        params[self._p['idiosyncratic_var']] = idio_var\n    return params"
        ]
    },
    {
        "func_name": "transform_params",
        "original": "def transform_params(self, unconstrained):\n    \"\"\"\n        Transform parameters from optimizer space to model space.\n\n        Transform unconstrained parameters used by the optimizer to constrained\n        parameters used in likelihood evaluation.\n\n        Parameters\n        ----------\n        unconstrained : array_like\n            Array of unconstrained parameters used by the optimizer, to be\n            transformed.\n\n        Returns\n        -------\n        constrained : array_like\n            Array of constrained parameters which may be used in likelihood\n            evaluation.\n        \"\"\"\n    constrained = unconstrained.copy()\n    unconstrained_factor_ar = unconstrained[self._p['factor_ar']]\n    constrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(unconstrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = constrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        constrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    constrained[self._p['factor_ar']] = constrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = unconstrained[self._p['idiosyncratic_ar1']]\n        constrained[self._p['idiosyncratic_ar1']] = [constrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    constrained[self._p['idiosyncratic_var']] = constrained[self._p['idiosyncratic_var']] ** 2\n    return constrained",
        "mutated": [
            "def transform_params(self, unconstrained):\n    if False:\n        i = 10\n    '\\n        Transform parameters from optimizer space to model space.\\n\\n        Transform unconstrained parameters used by the optimizer to constrained\\n        parameters used in likelihood evaluation.\\n\\n        Parameters\\n        ----------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer, to be\\n            transformed.\\n\\n        Returns\\n        -------\\n        constrained : array_like\\n            Array of constrained parameters which may be used in likelihood\\n            evaluation.\\n        '\n    constrained = unconstrained.copy()\n    unconstrained_factor_ar = unconstrained[self._p['factor_ar']]\n    constrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(unconstrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = constrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        constrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    constrained[self._p['factor_ar']] = constrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = unconstrained[self._p['idiosyncratic_ar1']]\n        constrained[self._p['idiosyncratic_ar1']] = [constrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    constrained[self._p['idiosyncratic_var']] = constrained[self._p['idiosyncratic_var']] ** 2\n    return constrained",
            "def transform_params(self, unconstrained):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Transform parameters from optimizer space to model space.\\n\\n        Transform unconstrained parameters used by the optimizer to constrained\\n        parameters used in likelihood evaluation.\\n\\n        Parameters\\n        ----------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer, to be\\n            transformed.\\n\\n        Returns\\n        -------\\n        constrained : array_like\\n            Array of constrained parameters which may be used in likelihood\\n            evaluation.\\n        '\n    constrained = unconstrained.copy()\n    unconstrained_factor_ar = unconstrained[self._p['factor_ar']]\n    constrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(unconstrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = constrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        constrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    constrained[self._p['factor_ar']] = constrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = unconstrained[self._p['idiosyncratic_ar1']]\n        constrained[self._p['idiosyncratic_ar1']] = [constrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    constrained[self._p['idiosyncratic_var']] = constrained[self._p['idiosyncratic_var']] ** 2\n    return constrained",
            "def transform_params(self, unconstrained):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Transform parameters from optimizer space to model space.\\n\\n        Transform unconstrained parameters used by the optimizer to constrained\\n        parameters used in likelihood evaluation.\\n\\n        Parameters\\n        ----------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer, to be\\n            transformed.\\n\\n        Returns\\n        -------\\n        constrained : array_like\\n            Array of constrained parameters which may be used in likelihood\\n            evaluation.\\n        '\n    constrained = unconstrained.copy()\n    unconstrained_factor_ar = unconstrained[self._p['factor_ar']]\n    constrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(unconstrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = constrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        constrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    constrained[self._p['factor_ar']] = constrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = unconstrained[self._p['idiosyncratic_ar1']]\n        constrained[self._p['idiosyncratic_ar1']] = [constrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    constrained[self._p['idiosyncratic_var']] = constrained[self._p['idiosyncratic_var']] ** 2\n    return constrained",
            "def transform_params(self, unconstrained):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Transform parameters from optimizer space to model space.\\n\\n        Transform unconstrained parameters used by the optimizer to constrained\\n        parameters used in likelihood evaluation.\\n\\n        Parameters\\n        ----------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer, to be\\n            transformed.\\n\\n        Returns\\n        -------\\n        constrained : array_like\\n            Array of constrained parameters which may be used in likelihood\\n            evaluation.\\n        '\n    constrained = unconstrained.copy()\n    unconstrained_factor_ar = unconstrained[self._p['factor_ar']]\n    constrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(unconstrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = constrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        constrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    constrained[self._p['factor_ar']] = constrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = unconstrained[self._p['idiosyncratic_ar1']]\n        constrained[self._p['idiosyncratic_ar1']] = [constrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    constrained[self._p['idiosyncratic_var']] = constrained[self._p['idiosyncratic_var']] ** 2\n    return constrained",
            "def transform_params(self, unconstrained):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Transform parameters from optimizer space to model space.\\n\\n        Transform unconstrained parameters used by the optimizer to constrained\\n        parameters used in likelihood evaluation.\\n\\n        Parameters\\n        ----------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer, to be\\n            transformed.\\n\\n        Returns\\n        -------\\n        constrained : array_like\\n            Array of constrained parameters which may be used in likelihood\\n            evaluation.\\n        '\n    constrained = unconstrained.copy()\n    unconstrained_factor_ar = unconstrained[self._p['factor_ar']]\n    constrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(unconstrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = constrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        constrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    constrained[self._p['factor_ar']] = constrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = unconstrained[self._p['idiosyncratic_ar1']]\n        constrained[self._p['idiosyncratic_ar1']] = [constrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    constrained[self._p['idiosyncratic_var']] = constrained[self._p['idiosyncratic_var']] ** 2\n    return constrained"
        ]
    },
    {
        "func_name": "untransform_params",
        "original": "def untransform_params(self, constrained):\n    \"\"\"\n        Transform parameters from model space to optimizer space.\n\n        Transform constrained parameters used in likelihood evaluation\n        to unconstrained parameters used by the optimizer.\n\n        Parameters\n        ----------\n        constrained : array_like\n            Array of constrained parameters used in likelihood evaluation, to\n            be transformed.\n\n        Returns\n        -------\n        unconstrained : array_like\n            Array of unconstrained parameters used by the optimizer.\n        \"\"\"\n    unconstrained = constrained.copy()\n    constrained_factor_ar = constrained[self._p['factor_ar']]\n    unconstrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(constrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = unconstrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        unconstrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    unconstrained[self._p['factor_ar']] = unconstrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = constrained[self._p['idiosyncratic_ar1']]\n        unconstrained[self._p['idiosyncratic_ar1']] = [unconstrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    unconstrained[self._p['idiosyncratic_var']] = unconstrained[self._p['idiosyncratic_var']] ** 0.5\n    return unconstrained",
        "mutated": [
            "def untransform_params(self, constrained):\n    if False:\n        i = 10\n    '\\n        Transform parameters from model space to optimizer space.\\n\\n        Transform constrained parameters used in likelihood evaluation\\n        to unconstrained parameters used by the optimizer.\\n\\n        Parameters\\n        ----------\\n        constrained : array_like\\n            Array of constrained parameters used in likelihood evaluation, to\\n            be transformed.\\n\\n        Returns\\n        -------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer.\\n        '\n    unconstrained = constrained.copy()\n    constrained_factor_ar = constrained[self._p['factor_ar']]\n    unconstrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(constrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = unconstrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        unconstrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    unconstrained[self._p['factor_ar']] = unconstrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = constrained[self._p['idiosyncratic_ar1']]\n        unconstrained[self._p['idiosyncratic_ar1']] = [unconstrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    unconstrained[self._p['idiosyncratic_var']] = unconstrained[self._p['idiosyncratic_var']] ** 0.5\n    return unconstrained",
            "def untransform_params(self, constrained):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Transform parameters from model space to optimizer space.\\n\\n        Transform constrained parameters used in likelihood evaluation\\n        to unconstrained parameters used by the optimizer.\\n\\n        Parameters\\n        ----------\\n        constrained : array_like\\n            Array of constrained parameters used in likelihood evaluation, to\\n            be transformed.\\n\\n        Returns\\n        -------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer.\\n        '\n    unconstrained = constrained.copy()\n    constrained_factor_ar = constrained[self._p['factor_ar']]\n    unconstrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(constrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = unconstrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        unconstrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    unconstrained[self._p['factor_ar']] = unconstrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = constrained[self._p['idiosyncratic_ar1']]\n        unconstrained[self._p['idiosyncratic_ar1']] = [unconstrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    unconstrained[self._p['idiosyncratic_var']] = unconstrained[self._p['idiosyncratic_var']] ** 0.5\n    return unconstrained",
            "def untransform_params(self, constrained):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Transform parameters from model space to optimizer space.\\n\\n        Transform constrained parameters used in likelihood evaluation\\n        to unconstrained parameters used by the optimizer.\\n\\n        Parameters\\n        ----------\\n        constrained : array_like\\n            Array of constrained parameters used in likelihood evaluation, to\\n            be transformed.\\n\\n        Returns\\n        -------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer.\\n        '\n    unconstrained = constrained.copy()\n    constrained_factor_ar = constrained[self._p['factor_ar']]\n    unconstrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(constrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = unconstrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        unconstrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    unconstrained[self._p['factor_ar']] = unconstrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = constrained[self._p['idiosyncratic_ar1']]\n        unconstrained[self._p['idiosyncratic_ar1']] = [unconstrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    unconstrained[self._p['idiosyncratic_var']] = unconstrained[self._p['idiosyncratic_var']] ** 0.5\n    return unconstrained",
            "def untransform_params(self, constrained):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Transform parameters from model space to optimizer space.\\n\\n        Transform constrained parameters used in likelihood evaluation\\n        to unconstrained parameters used by the optimizer.\\n\\n        Parameters\\n        ----------\\n        constrained : array_like\\n            Array of constrained parameters used in likelihood evaluation, to\\n            be transformed.\\n\\n        Returns\\n        -------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer.\\n        '\n    unconstrained = constrained.copy()\n    constrained_factor_ar = constrained[self._p['factor_ar']]\n    unconstrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(constrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = unconstrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        unconstrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    unconstrained[self._p['factor_ar']] = unconstrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = constrained[self._p['idiosyncratic_ar1']]\n        unconstrained[self._p['idiosyncratic_ar1']] = [unconstrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    unconstrained[self._p['idiosyncratic_var']] = unconstrained[self._p['idiosyncratic_var']] ** 0.5\n    return unconstrained",
            "def untransform_params(self, constrained):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Transform parameters from model space to optimizer space.\\n\\n        Transform constrained parameters used in likelihood evaluation\\n        to unconstrained parameters used by the optimizer.\\n\\n        Parameters\\n        ----------\\n        constrained : array_like\\n            Array of constrained parameters used in likelihood evaluation, to\\n            be transformed.\\n\\n        Returns\\n        -------\\n        unconstrained : array_like\\n            Array of unconstrained parameters used by the optimizer.\\n        '\n    unconstrained = constrained.copy()\n    constrained_factor_ar = constrained[self._p['factor_ar']]\n    unconstrained_factor_ar = []\n    i = 0\n    for block in self._s.factor_blocks:\n        length = block.k_factors ** 2 * block.factor_order\n        tmp_coeff = np.reshape(constrained_factor_ar[i:i + length], (block.k_factors, block.k_factors * block.factor_order))\n        tmp_cov = np.eye(block.k_factors)\n        (tmp_coeff, _) = unconstrain_stationary_multivariate(tmp_coeff, tmp_cov)\n        unconstrained_factor_ar += tmp_coeff.ravel().tolist()\n        i += length\n    unconstrained[self._p['factor_ar']] = unconstrained_factor_ar\n    if self.idiosyncratic_ar1:\n        idio_ar1 = constrained[self._p['idiosyncratic_ar1']]\n        unconstrained[self._p['idiosyncratic_ar1']] = [unconstrain_stationary_univariate(idio_ar1[i:i + 1])[0] for i in range(self.k_endog)]\n    unconstrained[self._p['idiosyncratic_var']] = unconstrained[self._p['idiosyncratic_var']] ** 0.5\n    return unconstrained"
        ]
    },
    {
        "func_name": "update",
        "original": "def update(self, params, **kwargs):\n    \"\"\"\n        Update the parameters of the model.\n\n        Parameters\n        ----------\n        params : array_like\n            Array of new parameters.\n        transformed : bool, optional\n            Whether or not `params` is already transformed. If set to False,\n            `transform_params` is called. Default is True.\n\n        \"\"\"\n    params = super().update(params, **kwargs)\n    o = self._o\n    s = self._s\n    p = self._p\n    loadings = params[p['loadings']]\n    start = 0\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1'][iloc]\n        self['design', i, factor_ix] = loadings[start:start + k_factors]\n        start += k_factors\n    multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n    for i in range(self.k_endog_M, self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1_5_ix'][:, iloc]\n        self['design', i, factor_ix.ravel()] = np.ravel(loadings[start:start + k_factors] * multipliers)\n        start += k_factors\n    factor_ar = params[p['factor_ar']]\n    start = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors ** 2 * block.factor_order\n        A = np.reshape(factor_ar[start:start + k_params], (block.k_factors, block.k_factors * block.factor_order))\n        start += k_params\n        self['transition', block['factors_L1'], block['factors_ar']] = A\n    factor_cov = params[p['factor_cov']]\n    start = 0\n    ix1 = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors * (block.k_factors + 1) // 2\n        L = np.zeros((block.k_factors, block.k_factors), dtype=params.dtype)\n        L[np.tril_indices_from(L)] = factor_cov[start:start + k_params]\n        start += k_params\n        Q = L @ L.T\n        ix2 = ix1 + block.k_factors\n        self['state_cov', ix1:ix2, ix1:ix2] = Q\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        alpha = np.diag(params[p['idiosyncratic_ar1']])\n        self['transition', s['idio_ar_L1'], s['idio_ar_L1']] = alpha\n    if self.idiosyncratic_ar1:\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(params[p['idiosyncratic_var']])\n    else:\n        idio_var = params[p['idiosyncratic_var']]\n        self['obs_cov', o['M'], o['M']] = np.diag(idio_var[o['M']])\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(idio_var[o['Q']])",
        "mutated": [
            "def update(self, params, **kwargs):\n    if False:\n        i = 10\n    '\\n        Update the parameters of the model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of new parameters.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. If set to False,\\n            `transform_params` is called. Default is True.\\n\\n        '\n    params = super().update(params, **kwargs)\n    o = self._o\n    s = self._s\n    p = self._p\n    loadings = params[p['loadings']]\n    start = 0\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1'][iloc]\n        self['design', i, factor_ix] = loadings[start:start + k_factors]\n        start += k_factors\n    multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n    for i in range(self.k_endog_M, self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1_5_ix'][:, iloc]\n        self['design', i, factor_ix.ravel()] = np.ravel(loadings[start:start + k_factors] * multipliers)\n        start += k_factors\n    factor_ar = params[p['factor_ar']]\n    start = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors ** 2 * block.factor_order\n        A = np.reshape(factor_ar[start:start + k_params], (block.k_factors, block.k_factors * block.factor_order))\n        start += k_params\n        self['transition', block['factors_L1'], block['factors_ar']] = A\n    factor_cov = params[p['factor_cov']]\n    start = 0\n    ix1 = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors * (block.k_factors + 1) // 2\n        L = np.zeros((block.k_factors, block.k_factors), dtype=params.dtype)\n        L[np.tril_indices_from(L)] = factor_cov[start:start + k_params]\n        start += k_params\n        Q = L @ L.T\n        ix2 = ix1 + block.k_factors\n        self['state_cov', ix1:ix2, ix1:ix2] = Q\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        alpha = np.diag(params[p['idiosyncratic_ar1']])\n        self['transition', s['idio_ar_L1'], s['idio_ar_L1']] = alpha\n    if self.idiosyncratic_ar1:\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(params[p['idiosyncratic_var']])\n    else:\n        idio_var = params[p['idiosyncratic_var']]\n        self['obs_cov', o['M'], o['M']] = np.diag(idio_var[o['M']])\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(idio_var[o['Q']])",
            "def update(self, params, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update the parameters of the model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of new parameters.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. If set to False,\\n            `transform_params` is called. Default is True.\\n\\n        '\n    params = super().update(params, **kwargs)\n    o = self._o\n    s = self._s\n    p = self._p\n    loadings = params[p['loadings']]\n    start = 0\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1'][iloc]\n        self['design', i, factor_ix] = loadings[start:start + k_factors]\n        start += k_factors\n    multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n    for i in range(self.k_endog_M, self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1_5_ix'][:, iloc]\n        self['design', i, factor_ix.ravel()] = np.ravel(loadings[start:start + k_factors] * multipliers)\n        start += k_factors\n    factor_ar = params[p['factor_ar']]\n    start = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors ** 2 * block.factor_order\n        A = np.reshape(factor_ar[start:start + k_params], (block.k_factors, block.k_factors * block.factor_order))\n        start += k_params\n        self['transition', block['factors_L1'], block['factors_ar']] = A\n    factor_cov = params[p['factor_cov']]\n    start = 0\n    ix1 = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors * (block.k_factors + 1) // 2\n        L = np.zeros((block.k_factors, block.k_factors), dtype=params.dtype)\n        L[np.tril_indices_from(L)] = factor_cov[start:start + k_params]\n        start += k_params\n        Q = L @ L.T\n        ix2 = ix1 + block.k_factors\n        self['state_cov', ix1:ix2, ix1:ix2] = Q\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        alpha = np.diag(params[p['idiosyncratic_ar1']])\n        self['transition', s['idio_ar_L1'], s['idio_ar_L1']] = alpha\n    if self.idiosyncratic_ar1:\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(params[p['idiosyncratic_var']])\n    else:\n        idio_var = params[p['idiosyncratic_var']]\n        self['obs_cov', o['M'], o['M']] = np.diag(idio_var[o['M']])\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(idio_var[o['Q']])",
            "def update(self, params, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update the parameters of the model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of new parameters.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. If set to False,\\n            `transform_params` is called. Default is True.\\n\\n        '\n    params = super().update(params, **kwargs)\n    o = self._o\n    s = self._s\n    p = self._p\n    loadings = params[p['loadings']]\n    start = 0\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1'][iloc]\n        self['design', i, factor_ix] = loadings[start:start + k_factors]\n        start += k_factors\n    multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n    for i in range(self.k_endog_M, self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1_5_ix'][:, iloc]\n        self['design', i, factor_ix.ravel()] = np.ravel(loadings[start:start + k_factors] * multipliers)\n        start += k_factors\n    factor_ar = params[p['factor_ar']]\n    start = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors ** 2 * block.factor_order\n        A = np.reshape(factor_ar[start:start + k_params], (block.k_factors, block.k_factors * block.factor_order))\n        start += k_params\n        self['transition', block['factors_L1'], block['factors_ar']] = A\n    factor_cov = params[p['factor_cov']]\n    start = 0\n    ix1 = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors * (block.k_factors + 1) // 2\n        L = np.zeros((block.k_factors, block.k_factors), dtype=params.dtype)\n        L[np.tril_indices_from(L)] = factor_cov[start:start + k_params]\n        start += k_params\n        Q = L @ L.T\n        ix2 = ix1 + block.k_factors\n        self['state_cov', ix1:ix2, ix1:ix2] = Q\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        alpha = np.diag(params[p['idiosyncratic_ar1']])\n        self['transition', s['idio_ar_L1'], s['idio_ar_L1']] = alpha\n    if self.idiosyncratic_ar1:\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(params[p['idiosyncratic_var']])\n    else:\n        idio_var = params[p['idiosyncratic_var']]\n        self['obs_cov', o['M'], o['M']] = np.diag(idio_var[o['M']])\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(idio_var[o['Q']])",
            "def update(self, params, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update the parameters of the model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of new parameters.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. If set to False,\\n            `transform_params` is called. Default is True.\\n\\n        '\n    params = super().update(params, **kwargs)\n    o = self._o\n    s = self._s\n    p = self._p\n    loadings = params[p['loadings']]\n    start = 0\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1'][iloc]\n        self['design', i, factor_ix] = loadings[start:start + k_factors]\n        start += k_factors\n    multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n    for i in range(self.k_endog_M, self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1_5_ix'][:, iloc]\n        self['design', i, factor_ix.ravel()] = np.ravel(loadings[start:start + k_factors] * multipliers)\n        start += k_factors\n    factor_ar = params[p['factor_ar']]\n    start = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors ** 2 * block.factor_order\n        A = np.reshape(factor_ar[start:start + k_params], (block.k_factors, block.k_factors * block.factor_order))\n        start += k_params\n        self['transition', block['factors_L1'], block['factors_ar']] = A\n    factor_cov = params[p['factor_cov']]\n    start = 0\n    ix1 = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors * (block.k_factors + 1) // 2\n        L = np.zeros((block.k_factors, block.k_factors), dtype=params.dtype)\n        L[np.tril_indices_from(L)] = factor_cov[start:start + k_params]\n        start += k_params\n        Q = L @ L.T\n        ix2 = ix1 + block.k_factors\n        self['state_cov', ix1:ix2, ix1:ix2] = Q\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        alpha = np.diag(params[p['idiosyncratic_ar1']])\n        self['transition', s['idio_ar_L1'], s['idio_ar_L1']] = alpha\n    if self.idiosyncratic_ar1:\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(params[p['idiosyncratic_var']])\n    else:\n        idio_var = params[p['idiosyncratic_var']]\n        self['obs_cov', o['M'], o['M']] = np.diag(idio_var[o['M']])\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(idio_var[o['Q']])",
            "def update(self, params, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update the parameters of the model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of new parameters.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. If set to False,\\n            `transform_params` is called. Default is True.\\n\\n        '\n    params = super().update(params, **kwargs)\n    o = self._o\n    s = self._s\n    p = self._p\n    loadings = params[p['loadings']]\n    start = 0\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1'][iloc]\n        self['design', i, factor_ix] = loadings[start:start + k_factors]\n        start += k_factors\n    multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n    for i in range(self.k_endog_M, self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        k_factors = len(iloc)\n        factor_ix = s['factors_L1_5_ix'][:, iloc]\n        self['design', i, factor_ix.ravel()] = np.ravel(loadings[start:start + k_factors] * multipliers)\n        start += k_factors\n    factor_ar = params[p['factor_ar']]\n    start = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors ** 2 * block.factor_order\n        A = np.reshape(factor_ar[start:start + k_params], (block.k_factors, block.k_factors * block.factor_order))\n        start += k_params\n        self['transition', block['factors_L1'], block['factors_ar']] = A\n    factor_cov = params[p['factor_cov']]\n    start = 0\n    ix1 = 0\n    for block in s.factor_blocks:\n        k_params = block.k_factors * (block.k_factors + 1) // 2\n        L = np.zeros((block.k_factors, block.k_factors), dtype=params.dtype)\n        L[np.tril_indices_from(L)] = factor_cov[start:start + k_params]\n        start += k_params\n        Q = L @ L.T\n        ix2 = ix1 + block.k_factors\n        self['state_cov', ix1:ix2, ix1:ix2] = Q\n        ix1 = ix2\n    if self.idiosyncratic_ar1:\n        alpha = np.diag(params[p['idiosyncratic_ar1']])\n        self['transition', s['idio_ar_L1'], s['idio_ar_L1']] = alpha\n    if self.idiosyncratic_ar1:\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(params[p['idiosyncratic_var']])\n    else:\n        idio_var = params[p['idiosyncratic_var']]\n        self['obs_cov', o['M'], o['M']] = np.diag(idio_var[o['M']])\n        self['state_cov', self.k_factors:, self.k_factors:] = np.diag(idio_var[o['Q']])"
        ]
    },
    {
        "func_name": "loglike_constant",
        "original": "@property\ndef loglike_constant(self):\n    \"\"\"\n        Constant term in the joint log-likelihood function.\n\n        Useful in facilitating comparisons to other packages that exclude the\n        constant from the log-likelihood computation.\n        \"\"\"\n    return -0.5 * (1 - np.isnan(self.endog)).sum() * np.log(2 * np.pi)",
        "mutated": [
            "@property\ndef loglike_constant(self):\n    if False:\n        i = 10\n    '\\n        Constant term in the joint log-likelihood function.\\n\\n        Useful in facilitating comparisons to other packages that exclude the\\n        constant from the log-likelihood computation.\\n        '\n    return -0.5 * (1 - np.isnan(self.endog)).sum() * np.log(2 * np.pi)",
            "@property\ndef loglike_constant(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Constant term in the joint log-likelihood function.\\n\\n        Useful in facilitating comparisons to other packages that exclude the\\n        constant from the log-likelihood computation.\\n        '\n    return -0.5 * (1 - np.isnan(self.endog)).sum() * np.log(2 * np.pi)",
            "@property\ndef loglike_constant(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Constant term in the joint log-likelihood function.\\n\\n        Useful in facilitating comparisons to other packages that exclude the\\n        constant from the log-likelihood computation.\\n        '\n    return -0.5 * (1 - np.isnan(self.endog)).sum() * np.log(2 * np.pi)",
            "@property\ndef loglike_constant(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Constant term in the joint log-likelihood function.\\n\\n        Useful in facilitating comparisons to other packages that exclude the\\n        constant from the log-likelihood computation.\\n        '\n    return -0.5 * (1 - np.isnan(self.endog)).sum() * np.log(2 * np.pi)",
            "@property\ndef loglike_constant(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Constant term in the joint log-likelihood function.\\n\\n        Useful in facilitating comparisons to other packages that exclude the\\n        constant from the log-likelihood computation.\\n        '\n    return -0.5 * (1 - np.isnan(self.endog)).sum() * np.log(2 * np.pi)"
        ]
    },
    {
        "func_name": "loading_constraints",
        "original": "def loading_constraints(self, i):\n    \"\"\"\n        Matrix formulation of quarterly variables' factor loading constraints.\n\n        Parameters\n        ----------\n        i : int\n            Index of the `endog` variable to compute constraints for.\n\n        Returns\n        -------\n        R : array (k_constraints, k_factors * 5)\n        q : array (k_constraints,)\n\n        Notes\n        -----\n        If the factors were known, then the factor loadings for the ith\n        quarterly variable would be computed by a linear regression of the form\n\n        y_i = A_i' f + B_i' L1.f + C_i' L2.f + D_i' L3.f + E_i' L4.f\n\n        where:\n\n        - f is (k_i x 1) and collects all of the factors that load on y_i\n        - L{j}.f is (k_i x 1) and collects the jth lag of each factor\n        - A_i, ..., E_i are (k_i x 1) and collect factor loadings\n\n        As the observed variable is quarterly while the factors are monthly, we\n        want to restrict the estimated regression coefficients to be:\n\n        y_i = A_i f + 2 A_i L1.f + 3 A_i L2.f + 2 A_i L3.f + A_i L4.f\n\n        Stack the unconstrained coefficients: \\\\Lambda_i = [A_i' B_i' ... E_i']'\n\n        Then the constraints can be written as follows, for l = 1, ..., k_i\n\n        - 2 A_{i,l} - B_{i,l} = 0\n        - 3 A_{i,l} - C_{i,l} = 0\n        - 2 A_{i,l} - D_{i,l} = 0\n        - A_{i,l} - E_{i,l} = 0\n\n        So that k_constraints = 4 * k_i. In matrix form the constraints are:\n\n        .. math::\n\n            R \\\\Lambda_i = q\n\n        where :math:`\\\\Lambda_i` is shaped `(k_i * 5,)`, :math:`R` is shaped\n        `(k_constraints, k_i * 5)`, and :math:`q` is shaped `(k_constraints,)`.\n\n\n        For example, for the case that k_i = 2, we can write:\n\n        |  2 0   -1  0    0  0    0  0    0  0  |   | A_{i,1} |     | 0 |\n        |  0 2    0 -1    0  0    0  0    0  0  |   | A_{i,2} |     | 0 |\n        |  3 0    0  0   -1  0    0  0    0  0  |   | B_{i,1} |     | 0 |\n        |  0 3    0  0    0 -1    0  0    0  0  |   | B_{i,2} |     | 0 |\n        |  2 0    0  0    0  0   -1  0    0  0  |   | C_{i,1} |  =  | 0 |\n        |  0 2    0  0    0  0    0 -1    0  0  |   | C_{i,2} |     | 0 |\n        |  1 0    0  0    0  0    0  0   -1  0  |   | D_{i,1} |     | 0 |\n        |  0 1    0  0    0  0    0  0    0 -1  |   | D_{i,2} |     | 0 |\n                                                    | E_{i,1} |     | 0 |\n                                                    | E_{i,2} |     | 0 |\n\n        \"\"\"\n    if i < self.k_endog_M:\n        raise ValueError('No constraints for monthly variables.')\n    if i not in self._loading_constraints:\n        k_factors = self.endog_factor_map.iloc[i].sum()\n        R = np.zeros((k_factors * 4, k_factors * 5))\n        q = np.zeros(R.shape[0])\n        multipliers = np.array([1, 2, 3, 2, 1])\n        R[:, :k_factors] = np.reshape((multipliers[1:] * np.eye(k_factors)[..., None]).T, (k_factors * 4, k_factors))\n        R[:, k_factors:] = np.diag([-1] * (k_factors * 4))\n        self._loading_constraints[i] = (R, q)\n    return self._loading_constraints[i]",
        "mutated": [
            "def loading_constraints(self, i):\n    if False:\n        i = 10\n    \"\\n        Matrix formulation of quarterly variables' factor loading constraints.\\n\\n        Parameters\\n        ----------\\n        i : int\\n            Index of the `endog` variable to compute constraints for.\\n\\n        Returns\\n        -------\\n        R : array (k_constraints, k_factors * 5)\\n        q : array (k_constraints,)\\n\\n        Notes\\n        -----\\n        If the factors were known, then the factor loadings for the ith\\n        quarterly variable would be computed by a linear regression of the form\\n\\n        y_i = A_i' f + B_i' L1.f + C_i' L2.f + D_i' L3.f + E_i' L4.f\\n\\n        where:\\n\\n        - f is (k_i x 1) and collects all of the factors that load on y_i\\n        - L{j}.f is (k_i x 1) and collects the jth lag of each factor\\n        - A_i, ..., E_i are (k_i x 1) and collect factor loadings\\n\\n        As the observed variable is quarterly while the factors are monthly, we\\n        want to restrict the estimated regression coefficients to be:\\n\\n        y_i = A_i f + 2 A_i L1.f + 3 A_i L2.f + 2 A_i L3.f + A_i L4.f\\n\\n        Stack the unconstrained coefficients: \\\\Lambda_i = [A_i' B_i' ... E_i']'\\n\\n        Then the constraints can be written as follows, for l = 1, ..., k_i\\n\\n        - 2 A_{i,l} - B_{i,l} = 0\\n        - 3 A_{i,l} - C_{i,l} = 0\\n        - 2 A_{i,l} - D_{i,l} = 0\\n        - A_{i,l} - E_{i,l} = 0\\n\\n        So that k_constraints = 4 * k_i. In matrix form the constraints are:\\n\\n        .. math::\\n\\n            R \\\\Lambda_i = q\\n\\n        where :math:`\\\\Lambda_i` is shaped `(k_i * 5,)`, :math:`R` is shaped\\n        `(k_constraints, k_i * 5)`, and :math:`q` is shaped `(k_constraints,)`.\\n\\n\\n        For example, for the case that k_i = 2, we can write:\\n\\n        |  2 0   -1  0    0  0    0  0    0  0  |   | A_{i,1} |     | 0 |\\n        |  0 2    0 -1    0  0    0  0    0  0  |   | A_{i,2} |     | 0 |\\n        |  3 0    0  0   -1  0    0  0    0  0  |   | B_{i,1} |     | 0 |\\n        |  0 3    0  0    0 -1    0  0    0  0  |   | B_{i,2} |     | 0 |\\n        |  2 0    0  0    0  0   -1  0    0  0  |   | C_{i,1} |  =  | 0 |\\n        |  0 2    0  0    0  0    0 -1    0  0  |   | C_{i,2} |     | 0 |\\n        |  1 0    0  0    0  0    0  0   -1  0  |   | D_{i,1} |     | 0 |\\n        |  0 1    0  0    0  0    0  0    0 -1  |   | D_{i,2} |     | 0 |\\n                                                    | E_{i,1} |     | 0 |\\n                                                    | E_{i,2} |     | 0 |\\n\\n        \"\n    if i < self.k_endog_M:\n        raise ValueError('No constraints for monthly variables.')\n    if i not in self._loading_constraints:\n        k_factors = self.endog_factor_map.iloc[i].sum()\n        R = np.zeros((k_factors * 4, k_factors * 5))\n        q = np.zeros(R.shape[0])\n        multipliers = np.array([1, 2, 3, 2, 1])\n        R[:, :k_factors] = np.reshape((multipliers[1:] * np.eye(k_factors)[..., None]).T, (k_factors * 4, k_factors))\n        R[:, k_factors:] = np.diag([-1] * (k_factors * 4))\n        self._loading_constraints[i] = (R, q)\n    return self._loading_constraints[i]",
            "def loading_constraints(self, i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Matrix formulation of quarterly variables' factor loading constraints.\\n\\n        Parameters\\n        ----------\\n        i : int\\n            Index of the `endog` variable to compute constraints for.\\n\\n        Returns\\n        -------\\n        R : array (k_constraints, k_factors * 5)\\n        q : array (k_constraints,)\\n\\n        Notes\\n        -----\\n        If the factors were known, then the factor loadings for the ith\\n        quarterly variable would be computed by a linear regression of the form\\n\\n        y_i = A_i' f + B_i' L1.f + C_i' L2.f + D_i' L3.f + E_i' L4.f\\n\\n        where:\\n\\n        - f is (k_i x 1) and collects all of the factors that load on y_i\\n        - L{j}.f is (k_i x 1) and collects the jth lag of each factor\\n        - A_i, ..., E_i are (k_i x 1) and collect factor loadings\\n\\n        As the observed variable is quarterly while the factors are monthly, we\\n        want to restrict the estimated regression coefficients to be:\\n\\n        y_i = A_i f + 2 A_i L1.f + 3 A_i L2.f + 2 A_i L3.f + A_i L4.f\\n\\n        Stack the unconstrained coefficients: \\\\Lambda_i = [A_i' B_i' ... E_i']'\\n\\n        Then the constraints can be written as follows, for l = 1, ..., k_i\\n\\n        - 2 A_{i,l} - B_{i,l} = 0\\n        - 3 A_{i,l} - C_{i,l} = 0\\n        - 2 A_{i,l} - D_{i,l} = 0\\n        - A_{i,l} - E_{i,l} = 0\\n\\n        So that k_constraints = 4 * k_i. In matrix form the constraints are:\\n\\n        .. math::\\n\\n            R \\\\Lambda_i = q\\n\\n        where :math:`\\\\Lambda_i` is shaped `(k_i * 5,)`, :math:`R` is shaped\\n        `(k_constraints, k_i * 5)`, and :math:`q` is shaped `(k_constraints,)`.\\n\\n\\n        For example, for the case that k_i = 2, we can write:\\n\\n        |  2 0   -1  0    0  0    0  0    0  0  |   | A_{i,1} |     | 0 |\\n        |  0 2    0 -1    0  0    0  0    0  0  |   | A_{i,2} |     | 0 |\\n        |  3 0    0  0   -1  0    0  0    0  0  |   | B_{i,1} |     | 0 |\\n        |  0 3    0  0    0 -1    0  0    0  0  |   | B_{i,2} |     | 0 |\\n        |  2 0    0  0    0  0   -1  0    0  0  |   | C_{i,1} |  =  | 0 |\\n        |  0 2    0  0    0  0    0 -1    0  0  |   | C_{i,2} |     | 0 |\\n        |  1 0    0  0    0  0    0  0   -1  0  |   | D_{i,1} |     | 0 |\\n        |  0 1    0  0    0  0    0  0    0 -1  |   | D_{i,2} |     | 0 |\\n                                                    | E_{i,1} |     | 0 |\\n                                                    | E_{i,2} |     | 0 |\\n\\n        \"\n    if i < self.k_endog_M:\n        raise ValueError('No constraints for monthly variables.')\n    if i not in self._loading_constraints:\n        k_factors = self.endog_factor_map.iloc[i].sum()\n        R = np.zeros((k_factors * 4, k_factors * 5))\n        q = np.zeros(R.shape[0])\n        multipliers = np.array([1, 2, 3, 2, 1])\n        R[:, :k_factors] = np.reshape((multipliers[1:] * np.eye(k_factors)[..., None]).T, (k_factors * 4, k_factors))\n        R[:, k_factors:] = np.diag([-1] * (k_factors * 4))\n        self._loading_constraints[i] = (R, q)\n    return self._loading_constraints[i]",
            "def loading_constraints(self, i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Matrix formulation of quarterly variables' factor loading constraints.\\n\\n        Parameters\\n        ----------\\n        i : int\\n            Index of the `endog` variable to compute constraints for.\\n\\n        Returns\\n        -------\\n        R : array (k_constraints, k_factors * 5)\\n        q : array (k_constraints,)\\n\\n        Notes\\n        -----\\n        If the factors were known, then the factor loadings for the ith\\n        quarterly variable would be computed by a linear regression of the form\\n\\n        y_i = A_i' f + B_i' L1.f + C_i' L2.f + D_i' L3.f + E_i' L4.f\\n\\n        where:\\n\\n        - f is (k_i x 1) and collects all of the factors that load on y_i\\n        - L{j}.f is (k_i x 1) and collects the jth lag of each factor\\n        - A_i, ..., E_i are (k_i x 1) and collect factor loadings\\n\\n        As the observed variable is quarterly while the factors are monthly, we\\n        want to restrict the estimated regression coefficients to be:\\n\\n        y_i = A_i f + 2 A_i L1.f + 3 A_i L2.f + 2 A_i L3.f + A_i L4.f\\n\\n        Stack the unconstrained coefficients: \\\\Lambda_i = [A_i' B_i' ... E_i']'\\n\\n        Then the constraints can be written as follows, for l = 1, ..., k_i\\n\\n        - 2 A_{i,l} - B_{i,l} = 0\\n        - 3 A_{i,l} - C_{i,l} = 0\\n        - 2 A_{i,l} - D_{i,l} = 0\\n        - A_{i,l} - E_{i,l} = 0\\n\\n        So that k_constraints = 4 * k_i. In matrix form the constraints are:\\n\\n        .. math::\\n\\n            R \\\\Lambda_i = q\\n\\n        where :math:`\\\\Lambda_i` is shaped `(k_i * 5,)`, :math:`R` is shaped\\n        `(k_constraints, k_i * 5)`, and :math:`q` is shaped `(k_constraints,)`.\\n\\n\\n        For example, for the case that k_i = 2, we can write:\\n\\n        |  2 0   -1  0    0  0    0  0    0  0  |   | A_{i,1} |     | 0 |\\n        |  0 2    0 -1    0  0    0  0    0  0  |   | A_{i,2} |     | 0 |\\n        |  3 0    0  0   -1  0    0  0    0  0  |   | B_{i,1} |     | 0 |\\n        |  0 3    0  0    0 -1    0  0    0  0  |   | B_{i,2} |     | 0 |\\n        |  2 0    0  0    0  0   -1  0    0  0  |   | C_{i,1} |  =  | 0 |\\n        |  0 2    0  0    0  0    0 -1    0  0  |   | C_{i,2} |     | 0 |\\n        |  1 0    0  0    0  0    0  0   -1  0  |   | D_{i,1} |     | 0 |\\n        |  0 1    0  0    0  0    0  0    0 -1  |   | D_{i,2} |     | 0 |\\n                                                    | E_{i,1} |     | 0 |\\n                                                    | E_{i,2} |     | 0 |\\n\\n        \"\n    if i < self.k_endog_M:\n        raise ValueError('No constraints for monthly variables.')\n    if i not in self._loading_constraints:\n        k_factors = self.endog_factor_map.iloc[i].sum()\n        R = np.zeros((k_factors * 4, k_factors * 5))\n        q = np.zeros(R.shape[0])\n        multipliers = np.array([1, 2, 3, 2, 1])\n        R[:, :k_factors] = np.reshape((multipliers[1:] * np.eye(k_factors)[..., None]).T, (k_factors * 4, k_factors))\n        R[:, k_factors:] = np.diag([-1] * (k_factors * 4))\n        self._loading_constraints[i] = (R, q)\n    return self._loading_constraints[i]",
            "def loading_constraints(self, i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Matrix formulation of quarterly variables' factor loading constraints.\\n\\n        Parameters\\n        ----------\\n        i : int\\n            Index of the `endog` variable to compute constraints for.\\n\\n        Returns\\n        -------\\n        R : array (k_constraints, k_factors * 5)\\n        q : array (k_constraints,)\\n\\n        Notes\\n        -----\\n        If the factors were known, then the factor loadings for the ith\\n        quarterly variable would be computed by a linear regression of the form\\n\\n        y_i = A_i' f + B_i' L1.f + C_i' L2.f + D_i' L3.f + E_i' L4.f\\n\\n        where:\\n\\n        - f is (k_i x 1) and collects all of the factors that load on y_i\\n        - L{j}.f is (k_i x 1) and collects the jth lag of each factor\\n        - A_i, ..., E_i are (k_i x 1) and collect factor loadings\\n\\n        As the observed variable is quarterly while the factors are monthly, we\\n        want to restrict the estimated regression coefficients to be:\\n\\n        y_i = A_i f + 2 A_i L1.f + 3 A_i L2.f + 2 A_i L3.f + A_i L4.f\\n\\n        Stack the unconstrained coefficients: \\\\Lambda_i = [A_i' B_i' ... E_i']'\\n\\n        Then the constraints can be written as follows, for l = 1, ..., k_i\\n\\n        - 2 A_{i,l} - B_{i,l} = 0\\n        - 3 A_{i,l} - C_{i,l} = 0\\n        - 2 A_{i,l} - D_{i,l} = 0\\n        - A_{i,l} - E_{i,l} = 0\\n\\n        So that k_constraints = 4 * k_i. In matrix form the constraints are:\\n\\n        .. math::\\n\\n            R \\\\Lambda_i = q\\n\\n        where :math:`\\\\Lambda_i` is shaped `(k_i * 5,)`, :math:`R` is shaped\\n        `(k_constraints, k_i * 5)`, and :math:`q` is shaped `(k_constraints,)`.\\n\\n\\n        For example, for the case that k_i = 2, we can write:\\n\\n        |  2 0   -1  0    0  0    0  0    0  0  |   | A_{i,1} |     | 0 |\\n        |  0 2    0 -1    0  0    0  0    0  0  |   | A_{i,2} |     | 0 |\\n        |  3 0    0  0   -1  0    0  0    0  0  |   | B_{i,1} |     | 0 |\\n        |  0 3    0  0    0 -1    0  0    0  0  |   | B_{i,2} |     | 0 |\\n        |  2 0    0  0    0  0   -1  0    0  0  |   | C_{i,1} |  =  | 0 |\\n        |  0 2    0  0    0  0    0 -1    0  0  |   | C_{i,2} |     | 0 |\\n        |  1 0    0  0    0  0    0  0   -1  0  |   | D_{i,1} |     | 0 |\\n        |  0 1    0  0    0  0    0  0    0 -1  |   | D_{i,2} |     | 0 |\\n                                                    | E_{i,1} |     | 0 |\\n                                                    | E_{i,2} |     | 0 |\\n\\n        \"\n    if i < self.k_endog_M:\n        raise ValueError('No constraints for monthly variables.')\n    if i not in self._loading_constraints:\n        k_factors = self.endog_factor_map.iloc[i].sum()\n        R = np.zeros((k_factors * 4, k_factors * 5))\n        q = np.zeros(R.shape[0])\n        multipliers = np.array([1, 2, 3, 2, 1])\n        R[:, :k_factors] = np.reshape((multipliers[1:] * np.eye(k_factors)[..., None]).T, (k_factors * 4, k_factors))\n        R[:, k_factors:] = np.diag([-1] * (k_factors * 4))\n        self._loading_constraints[i] = (R, q)\n    return self._loading_constraints[i]",
            "def loading_constraints(self, i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Matrix formulation of quarterly variables' factor loading constraints.\\n\\n        Parameters\\n        ----------\\n        i : int\\n            Index of the `endog` variable to compute constraints for.\\n\\n        Returns\\n        -------\\n        R : array (k_constraints, k_factors * 5)\\n        q : array (k_constraints,)\\n\\n        Notes\\n        -----\\n        If the factors were known, then the factor loadings for the ith\\n        quarterly variable would be computed by a linear regression of the form\\n\\n        y_i = A_i' f + B_i' L1.f + C_i' L2.f + D_i' L3.f + E_i' L4.f\\n\\n        where:\\n\\n        - f is (k_i x 1) and collects all of the factors that load on y_i\\n        - L{j}.f is (k_i x 1) and collects the jth lag of each factor\\n        - A_i, ..., E_i are (k_i x 1) and collect factor loadings\\n\\n        As the observed variable is quarterly while the factors are monthly, we\\n        want to restrict the estimated regression coefficients to be:\\n\\n        y_i = A_i f + 2 A_i L1.f + 3 A_i L2.f + 2 A_i L3.f + A_i L4.f\\n\\n        Stack the unconstrained coefficients: \\\\Lambda_i = [A_i' B_i' ... E_i']'\\n\\n        Then the constraints can be written as follows, for l = 1, ..., k_i\\n\\n        - 2 A_{i,l} - B_{i,l} = 0\\n        - 3 A_{i,l} - C_{i,l} = 0\\n        - 2 A_{i,l} - D_{i,l} = 0\\n        - A_{i,l} - E_{i,l} = 0\\n\\n        So that k_constraints = 4 * k_i. In matrix form the constraints are:\\n\\n        .. math::\\n\\n            R \\\\Lambda_i = q\\n\\n        where :math:`\\\\Lambda_i` is shaped `(k_i * 5,)`, :math:`R` is shaped\\n        `(k_constraints, k_i * 5)`, and :math:`q` is shaped `(k_constraints,)`.\\n\\n\\n        For example, for the case that k_i = 2, we can write:\\n\\n        |  2 0   -1  0    0  0    0  0    0  0  |   | A_{i,1} |     | 0 |\\n        |  0 2    0 -1    0  0    0  0    0  0  |   | A_{i,2} |     | 0 |\\n        |  3 0    0  0   -1  0    0  0    0  0  |   | B_{i,1} |     | 0 |\\n        |  0 3    0  0    0 -1    0  0    0  0  |   | B_{i,2} |     | 0 |\\n        |  2 0    0  0    0  0   -1  0    0  0  |   | C_{i,1} |  =  | 0 |\\n        |  0 2    0  0    0  0    0 -1    0  0  |   | C_{i,2} |     | 0 |\\n        |  1 0    0  0    0  0    0  0   -1  0  |   | D_{i,1} |     | 0 |\\n        |  0 1    0  0    0  0    0  0    0 -1  |   | D_{i,2} |     | 0 |\\n                                                    | E_{i,1} |     | 0 |\\n                                                    | E_{i,2} |     | 0 |\\n\\n        \"\n    if i < self.k_endog_M:\n        raise ValueError('No constraints for monthly variables.')\n    if i not in self._loading_constraints:\n        k_factors = self.endog_factor_map.iloc[i].sum()\n        R = np.zeros((k_factors * 4, k_factors * 5))\n        q = np.zeros(R.shape[0])\n        multipliers = np.array([1, 2, 3, 2, 1])\n        R[:, :k_factors] = np.reshape((multipliers[1:] * np.eye(k_factors)[..., None]).T, (k_factors * 4, k_factors))\n        R[:, k_factors:] = np.diag([-1] * (k_factors * 4))\n        self._loading_constraints[i] = (R, q)\n    return self._loading_constraints[i]"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, start_params=None, transformed=True, includes_fixed=False, cov_type='none', cov_kwds=None, method='em', maxiter=500, tolerance=1e-06, em_initialization=True, mstep_method=None, full_output=1, disp=False, callback=None, return_params=False, optim_score=None, optim_complex_step=None, optim_hessian=None, flags=None, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001, **kwargs):\n    \"\"\"\n        Fits the model by maximum likelihood via Kalman filter.\n\n        Parameters\n        ----------\n        start_params : array_like, optional\n            Initial guess of the solution for the loglikelihood maximization.\n            If None, the default is given by Model.start_params.\n        transformed : bool, optional\n            Whether or not `start_params` is already transformed. Default is\n            True.\n        includes_fixed : bool, optional\n            If parameters were previously fixed with the `fix_params` method,\n            this argument describes whether or not `start_params` also includes\n            the fixed parameters, in addition to the free parameters. Default\n            is False.\n        cov_type : str, optional\n            The `cov_type` keyword governs the method for calculating the\n            covariance matrix of parameter estimates. Can be one of:\n\n            - 'opg' for the outer product of gradient estimator\n            - 'oim' for the observed information matrix estimator, calculated\n              using the method of Harvey (1989)\n            - 'approx' for the observed information matrix estimator,\n              calculated using a numerical approximation of the Hessian matrix.\n            - 'robust' for an approximate (quasi-maximum likelihood) covariance\n              matrix that may be valid even in the presence of some\n              misspecifications. Intermediate calculations use the 'oim'\n              method.\n            - 'robust_approx' is the same as 'robust' except that the\n              intermediate calculations use the 'approx' method.\n            - 'none' for no covariance matrix calculation.\n\n            Default is 'none', since computing this matrix can be very slow\n            when there are a large number of parameters.\n        cov_kwds : dict or None, optional\n            A dictionary of arguments affecting covariance matrix computation.\n\n            **opg, oim, approx, robust, robust_approx**\n\n            - 'approx_complex_step' : bool, optional - If True, numerical\n              approximations are computed using complex-step methods. If False,\n              numerical approximations are computed using finite difference\n              methods. Default is True.\n            - 'approx_centered' : bool, optional - If True, numerical\n              approximations computed using finite difference methods use a\n              centered approximation. Default is False.\n        method : str, optional\n            The `method` determines which solver from `scipy.optimize`\n            is used, and it can be chosen from among the following strings:\n\n            - 'em' for the EM algorithm\n            - 'newton' for Newton-Raphson\n            - 'nm' for Nelder-Mead\n            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n            - 'lbfgs' for limited-memory BFGS with optional box constraints\n            - 'powell' for modified Powell's method\n            - 'cg' for conjugate gradient\n            - 'ncg' for Newton-conjugate gradient\n            - 'basinhopping' for global basin-hopping solver\n\n            The explicit arguments in `fit` are passed to the solver,\n            with the exception of the basin-hopping solver. Each\n            solver has several optional arguments that are not the same across\n            solvers. See the notes section below (or scipy.optimize) for the\n            available arguments and for the list of explicit arguments that the\n            basin-hopping solver supports.\n        maxiter : int, optional\n            The maximum number of iterations to perform.\n        tolerance : float, optional\n            Tolerance to use for convergence checking when using the EM\n            algorithm. To set the tolerance for other methods, pass\n            the optimizer-specific keyword argument(s).\n        full_output : bool, optional\n            Set to True to have all available output in the Results object's\n            mle_retvals attribute. The output is dependent on the solver.\n            See LikelihoodModelResults notes section for more information.\n        disp : bool, optional\n            Set to True to print convergence messages.\n        callback : callable callback(xk), optional\n            Called after each iteration, as callback(xk), where xk is the\n            current parameter vector.\n        return_params : bool, optional\n            Whether or not to return only the array of maximizing parameters.\n            Default is False.\n        optim_score : {'harvey', 'approx'} or None, optional\n            The method by which the score vector is calculated. 'harvey' uses\n            the method from Harvey (1989), 'approx' uses either finite\n            difference or complex step differentiation depending upon the\n            value of `optim_complex_step`, and None uses the built-in gradient\n            approximation of the optimizer. Default is None. This keyword is\n            only relevant if the optimization method uses the score.\n        optim_complex_step : bool, optional\n            Whether or not to use complex step differentiation when\n            approximating the score; if False, finite difference approximation\n            is used. Default is True. This keyword is only relevant if\n            `optim_score` is set to 'harvey' or 'approx'.\n        optim_hessian : {'opg','oim','approx'}, optional\n            The method by which the Hessian is numerically approximated. 'opg'\n            uses outer product of gradients, 'oim' uses the information\n            matrix formula from Harvey (1989), and 'approx' uses numerical\n            approximation. This keyword is only relevant if the\n            optimization method uses the Hessian matrix.\n        low_memory : bool, optional\n            If set to True, techniques are applied to substantially reduce\n            memory usage. If used, some features of the results object will\n            not be available (including smoothed results and in-sample\n            prediction), although out-of-sample forecasting is possible.\n            Note that this option is not available when using the EM algorithm\n            (which is the default for this model). Default is False.\n        llf_decrease_action : {'ignore', 'warn', 'revert'}, optional\n            Action to take if the log-likelihood decreases in an EM iteration.\n            'ignore' continues the iterations, 'warn' issues a warning but\n            continues the iterations, while 'revert' ends the iterations and\n            returns the result from the last good iteration. Default is 'warn'.\n        llf_decrease_tolerance : float, optional\n            Minimum size of the log-likelihood decrease required to trigger a\n            warning or to end the EM iterations. Setting this value slightly\n            larger than zero allows small decreases in the log-likelihood that\n            may be caused by numerical issues. If set to zero, then any\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\n        **kwargs\n            Additional keyword arguments to pass to the optimizer.\n\n        Returns\n        -------\n        MLEResults\n\n        See Also\n        --------\n        statsmodels.base.model.LikelihoodModel.fit\n        statsmodels.tsa.statespace.mlemodel.MLEResults\n        \"\"\"\n    if method == 'em':\n        return self.fit_em(start_params=start_params, transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance, **kwargs)\n    else:\n        return super().fit(start_params=start_params, transformed=transformed, includes_fixed=includes_fixed, cov_type=cov_type, cov_kwds=cov_kwds, method=method, maxiter=maxiter, full_output=full_output, disp=disp, callback=callback, return_params=return_params, optim_score=optim_score, optim_complex_step=optim_complex_step, optim_hessian=optim_hessian, flags=flags, low_memory=low_memory, **kwargs)",
        "mutated": [
            "def fit(self, start_params=None, transformed=True, includes_fixed=False, cov_type='none', cov_kwds=None, method='em', maxiter=500, tolerance=1e-06, em_initialization=True, mstep_method=None, full_output=1, disp=False, callback=None, return_params=False, optim_score=None, optim_complex_step=None, optim_hessian=None, flags=None, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001, **kwargs):\n    if False:\n        i = 10\n    \"\\n        Fits the model by maximum likelihood via Kalman filter.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            If None, the default is given by Model.start_params.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `start_params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - 'opg' for the outer product of gradient estimator\\n            - 'oim' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - 'approx' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - 'robust' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the 'oim'\\n              method.\\n            - 'robust_approx' is the same as 'robust' except that the\\n              intermediate calculations use the 'approx' method.\\n            - 'none' for no covariance matrix calculation.\\n\\n            Default is 'none', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - 'approx_complex_step' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - 'approx_centered' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        method : str, optional\\n            The `method` determines which solver from `scipy.optimize`\\n            is used, and it can be chosen from among the following strings:\\n\\n            - 'em' for the EM algorithm\\n            - 'newton' for Newton-Raphson\\n            - 'nm' for Nelder-Mead\\n            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)\\n            - 'lbfgs' for limited-memory BFGS with optional box constraints\\n            - 'powell' for modified Powell's method\\n            - 'cg' for conjugate gradient\\n            - 'ncg' for Newton-conjugate gradient\\n            - 'basinhopping' for global basin-hopping solver\\n\\n            The explicit arguments in `fit` are passed to the solver,\\n            with the exception of the basin-hopping solver. Each\\n            solver has several optional arguments that are not the same across\\n            solvers. See the notes section below (or scipy.optimize) for the\\n            available arguments and for the list of explicit arguments that the\\n            basin-hopping solver supports.\\n        maxiter : int, optional\\n            The maximum number of iterations to perform.\\n        tolerance : float, optional\\n            Tolerance to use for convergence checking when using the EM\\n            algorithm. To set the tolerance for other methods, pass\\n            the optimizer-specific keyword argument(s).\\n        full_output : bool, optional\\n            Set to True to have all available output in the Results object's\\n            mle_retvals attribute. The output is dependent on the solver.\\n            See LikelihoodModelResults notes section for more information.\\n        disp : bool, optional\\n            Set to True to print convergence messages.\\n        callback : callable callback(xk), optional\\n            Called after each iteration, as callback(xk), where xk is the\\n            current parameter vector.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        optim_score : {'harvey', 'approx'} or None, optional\\n            The method by which the score vector is calculated. 'harvey' uses\\n            the method from Harvey (1989), 'approx' uses either finite\\n            difference or complex step differentiation depending upon the\\n            value of `optim_complex_step`, and None uses the built-in gradient\\n            approximation of the optimizer. Default is None. This keyword is\\n            only relevant if the optimization method uses the score.\\n        optim_complex_step : bool, optional\\n            Whether or not to use complex step differentiation when\\n            approximating the score; if False, finite difference approximation\\n            is used. Default is True. This keyword is only relevant if\\n            `optim_score` is set to 'harvey' or 'approx'.\\n        optim_hessian : {'opg','oim','approx'}, optional\\n            The method by which the Hessian is numerically approximated. 'opg'\\n            uses outer product of gradients, 'oim' uses the information\\n            matrix formula from Harvey (1989), and 'approx' uses numerical\\n            approximation. This keyword is only relevant if the\\n            optimization method uses the Hessian matrix.\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including smoothed results and in-sample\\n            prediction), although out-of-sample forecasting is possible.\\n            Note that this option is not available when using the EM algorithm\\n            (which is the default for this model). Default is False.\\n        llf_decrease_action : {'ignore', 'warn', 'revert'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            'ignore' continues the iterations, 'warn' issues a warning but\\n            continues the iterations, while 'revert' ends the iterations and\\n            returns the result from the last good iteration. Default is 'warn'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n        **kwargs\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        MLEResults\\n\\n        See Also\\n        --------\\n        statsmodels.base.model.LikelihoodModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        \"\n    if method == 'em':\n        return self.fit_em(start_params=start_params, transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance, **kwargs)\n    else:\n        return super().fit(start_params=start_params, transformed=transformed, includes_fixed=includes_fixed, cov_type=cov_type, cov_kwds=cov_kwds, method=method, maxiter=maxiter, full_output=full_output, disp=disp, callback=callback, return_params=return_params, optim_score=optim_score, optim_complex_step=optim_complex_step, optim_hessian=optim_hessian, flags=flags, low_memory=low_memory, **kwargs)",
            "def fit(self, start_params=None, transformed=True, includes_fixed=False, cov_type='none', cov_kwds=None, method='em', maxiter=500, tolerance=1e-06, em_initialization=True, mstep_method=None, full_output=1, disp=False, callback=None, return_params=False, optim_score=None, optim_complex_step=None, optim_hessian=None, flags=None, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Fits the model by maximum likelihood via Kalman filter.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            If None, the default is given by Model.start_params.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `start_params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - 'opg' for the outer product of gradient estimator\\n            - 'oim' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - 'approx' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - 'robust' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the 'oim'\\n              method.\\n            - 'robust_approx' is the same as 'robust' except that the\\n              intermediate calculations use the 'approx' method.\\n            - 'none' for no covariance matrix calculation.\\n\\n            Default is 'none', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - 'approx_complex_step' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - 'approx_centered' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        method : str, optional\\n            The `method` determines which solver from `scipy.optimize`\\n            is used, and it can be chosen from among the following strings:\\n\\n            - 'em' for the EM algorithm\\n            - 'newton' for Newton-Raphson\\n            - 'nm' for Nelder-Mead\\n            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)\\n            - 'lbfgs' for limited-memory BFGS with optional box constraints\\n            - 'powell' for modified Powell's method\\n            - 'cg' for conjugate gradient\\n            - 'ncg' for Newton-conjugate gradient\\n            - 'basinhopping' for global basin-hopping solver\\n\\n            The explicit arguments in `fit` are passed to the solver,\\n            with the exception of the basin-hopping solver. Each\\n            solver has several optional arguments that are not the same across\\n            solvers. See the notes section below (or scipy.optimize) for the\\n            available arguments and for the list of explicit arguments that the\\n            basin-hopping solver supports.\\n        maxiter : int, optional\\n            The maximum number of iterations to perform.\\n        tolerance : float, optional\\n            Tolerance to use for convergence checking when using the EM\\n            algorithm. To set the tolerance for other methods, pass\\n            the optimizer-specific keyword argument(s).\\n        full_output : bool, optional\\n            Set to True to have all available output in the Results object's\\n            mle_retvals attribute. The output is dependent on the solver.\\n            See LikelihoodModelResults notes section for more information.\\n        disp : bool, optional\\n            Set to True to print convergence messages.\\n        callback : callable callback(xk), optional\\n            Called after each iteration, as callback(xk), where xk is the\\n            current parameter vector.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        optim_score : {'harvey', 'approx'} or None, optional\\n            The method by which the score vector is calculated. 'harvey' uses\\n            the method from Harvey (1989), 'approx' uses either finite\\n            difference or complex step differentiation depending upon the\\n            value of `optim_complex_step`, and None uses the built-in gradient\\n            approximation of the optimizer. Default is None. This keyword is\\n            only relevant if the optimization method uses the score.\\n        optim_complex_step : bool, optional\\n            Whether or not to use complex step differentiation when\\n            approximating the score; if False, finite difference approximation\\n            is used. Default is True. This keyword is only relevant if\\n            `optim_score` is set to 'harvey' or 'approx'.\\n        optim_hessian : {'opg','oim','approx'}, optional\\n            The method by which the Hessian is numerically approximated. 'opg'\\n            uses outer product of gradients, 'oim' uses the information\\n            matrix formula from Harvey (1989), and 'approx' uses numerical\\n            approximation. This keyword is only relevant if the\\n            optimization method uses the Hessian matrix.\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including smoothed results and in-sample\\n            prediction), although out-of-sample forecasting is possible.\\n            Note that this option is not available when using the EM algorithm\\n            (which is the default for this model). Default is False.\\n        llf_decrease_action : {'ignore', 'warn', 'revert'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            'ignore' continues the iterations, 'warn' issues a warning but\\n            continues the iterations, while 'revert' ends the iterations and\\n            returns the result from the last good iteration. Default is 'warn'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n        **kwargs\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        MLEResults\\n\\n        See Also\\n        --------\\n        statsmodels.base.model.LikelihoodModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        \"\n    if method == 'em':\n        return self.fit_em(start_params=start_params, transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance, **kwargs)\n    else:\n        return super().fit(start_params=start_params, transformed=transformed, includes_fixed=includes_fixed, cov_type=cov_type, cov_kwds=cov_kwds, method=method, maxiter=maxiter, full_output=full_output, disp=disp, callback=callback, return_params=return_params, optim_score=optim_score, optim_complex_step=optim_complex_step, optim_hessian=optim_hessian, flags=flags, low_memory=low_memory, **kwargs)",
            "def fit(self, start_params=None, transformed=True, includes_fixed=False, cov_type='none', cov_kwds=None, method='em', maxiter=500, tolerance=1e-06, em_initialization=True, mstep_method=None, full_output=1, disp=False, callback=None, return_params=False, optim_score=None, optim_complex_step=None, optim_hessian=None, flags=None, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Fits the model by maximum likelihood via Kalman filter.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            If None, the default is given by Model.start_params.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `start_params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - 'opg' for the outer product of gradient estimator\\n            - 'oim' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - 'approx' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - 'robust' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the 'oim'\\n              method.\\n            - 'robust_approx' is the same as 'robust' except that the\\n              intermediate calculations use the 'approx' method.\\n            - 'none' for no covariance matrix calculation.\\n\\n            Default is 'none', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - 'approx_complex_step' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - 'approx_centered' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        method : str, optional\\n            The `method` determines which solver from `scipy.optimize`\\n            is used, and it can be chosen from among the following strings:\\n\\n            - 'em' for the EM algorithm\\n            - 'newton' for Newton-Raphson\\n            - 'nm' for Nelder-Mead\\n            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)\\n            - 'lbfgs' for limited-memory BFGS with optional box constraints\\n            - 'powell' for modified Powell's method\\n            - 'cg' for conjugate gradient\\n            - 'ncg' for Newton-conjugate gradient\\n            - 'basinhopping' for global basin-hopping solver\\n\\n            The explicit arguments in `fit` are passed to the solver,\\n            with the exception of the basin-hopping solver. Each\\n            solver has several optional arguments that are not the same across\\n            solvers. See the notes section below (or scipy.optimize) for the\\n            available arguments and for the list of explicit arguments that the\\n            basin-hopping solver supports.\\n        maxiter : int, optional\\n            The maximum number of iterations to perform.\\n        tolerance : float, optional\\n            Tolerance to use for convergence checking when using the EM\\n            algorithm. To set the tolerance for other methods, pass\\n            the optimizer-specific keyword argument(s).\\n        full_output : bool, optional\\n            Set to True to have all available output in the Results object's\\n            mle_retvals attribute. The output is dependent on the solver.\\n            See LikelihoodModelResults notes section for more information.\\n        disp : bool, optional\\n            Set to True to print convergence messages.\\n        callback : callable callback(xk), optional\\n            Called after each iteration, as callback(xk), where xk is the\\n            current parameter vector.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        optim_score : {'harvey', 'approx'} or None, optional\\n            The method by which the score vector is calculated. 'harvey' uses\\n            the method from Harvey (1989), 'approx' uses either finite\\n            difference or complex step differentiation depending upon the\\n            value of `optim_complex_step`, and None uses the built-in gradient\\n            approximation of the optimizer. Default is None. This keyword is\\n            only relevant if the optimization method uses the score.\\n        optim_complex_step : bool, optional\\n            Whether or not to use complex step differentiation when\\n            approximating the score; if False, finite difference approximation\\n            is used. Default is True. This keyword is only relevant if\\n            `optim_score` is set to 'harvey' or 'approx'.\\n        optim_hessian : {'opg','oim','approx'}, optional\\n            The method by which the Hessian is numerically approximated. 'opg'\\n            uses outer product of gradients, 'oim' uses the information\\n            matrix formula from Harvey (1989), and 'approx' uses numerical\\n            approximation. This keyword is only relevant if the\\n            optimization method uses the Hessian matrix.\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including smoothed results and in-sample\\n            prediction), although out-of-sample forecasting is possible.\\n            Note that this option is not available when using the EM algorithm\\n            (which is the default for this model). Default is False.\\n        llf_decrease_action : {'ignore', 'warn', 'revert'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            'ignore' continues the iterations, 'warn' issues a warning but\\n            continues the iterations, while 'revert' ends the iterations and\\n            returns the result from the last good iteration. Default is 'warn'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n        **kwargs\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        MLEResults\\n\\n        See Also\\n        --------\\n        statsmodels.base.model.LikelihoodModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        \"\n    if method == 'em':\n        return self.fit_em(start_params=start_params, transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance, **kwargs)\n    else:\n        return super().fit(start_params=start_params, transformed=transformed, includes_fixed=includes_fixed, cov_type=cov_type, cov_kwds=cov_kwds, method=method, maxiter=maxiter, full_output=full_output, disp=disp, callback=callback, return_params=return_params, optim_score=optim_score, optim_complex_step=optim_complex_step, optim_hessian=optim_hessian, flags=flags, low_memory=low_memory, **kwargs)",
            "def fit(self, start_params=None, transformed=True, includes_fixed=False, cov_type='none', cov_kwds=None, method='em', maxiter=500, tolerance=1e-06, em_initialization=True, mstep_method=None, full_output=1, disp=False, callback=None, return_params=False, optim_score=None, optim_complex_step=None, optim_hessian=None, flags=None, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Fits the model by maximum likelihood via Kalman filter.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            If None, the default is given by Model.start_params.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `start_params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - 'opg' for the outer product of gradient estimator\\n            - 'oim' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - 'approx' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - 'robust' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the 'oim'\\n              method.\\n            - 'robust_approx' is the same as 'robust' except that the\\n              intermediate calculations use the 'approx' method.\\n            - 'none' for no covariance matrix calculation.\\n\\n            Default is 'none', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - 'approx_complex_step' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - 'approx_centered' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        method : str, optional\\n            The `method` determines which solver from `scipy.optimize`\\n            is used, and it can be chosen from among the following strings:\\n\\n            - 'em' for the EM algorithm\\n            - 'newton' for Newton-Raphson\\n            - 'nm' for Nelder-Mead\\n            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)\\n            - 'lbfgs' for limited-memory BFGS with optional box constraints\\n            - 'powell' for modified Powell's method\\n            - 'cg' for conjugate gradient\\n            - 'ncg' for Newton-conjugate gradient\\n            - 'basinhopping' for global basin-hopping solver\\n\\n            The explicit arguments in `fit` are passed to the solver,\\n            with the exception of the basin-hopping solver. Each\\n            solver has several optional arguments that are not the same across\\n            solvers. See the notes section below (or scipy.optimize) for the\\n            available arguments and for the list of explicit arguments that the\\n            basin-hopping solver supports.\\n        maxiter : int, optional\\n            The maximum number of iterations to perform.\\n        tolerance : float, optional\\n            Tolerance to use for convergence checking when using the EM\\n            algorithm. To set the tolerance for other methods, pass\\n            the optimizer-specific keyword argument(s).\\n        full_output : bool, optional\\n            Set to True to have all available output in the Results object's\\n            mle_retvals attribute. The output is dependent on the solver.\\n            See LikelihoodModelResults notes section for more information.\\n        disp : bool, optional\\n            Set to True to print convergence messages.\\n        callback : callable callback(xk), optional\\n            Called after each iteration, as callback(xk), where xk is the\\n            current parameter vector.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        optim_score : {'harvey', 'approx'} or None, optional\\n            The method by which the score vector is calculated. 'harvey' uses\\n            the method from Harvey (1989), 'approx' uses either finite\\n            difference or complex step differentiation depending upon the\\n            value of `optim_complex_step`, and None uses the built-in gradient\\n            approximation of the optimizer. Default is None. This keyword is\\n            only relevant if the optimization method uses the score.\\n        optim_complex_step : bool, optional\\n            Whether or not to use complex step differentiation when\\n            approximating the score; if False, finite difference approximation\\n            is used. Default is True. This keyword is only relevant if\\n            `optim_score` is set to 'harvey' or 'approx'.\\n        optim_hessian : {'opg','oim','approx'}, optional\\n            The method by which the Hessian is numerically approximated. 'opg'\\n            uses outer product of gradients, 'oim' uses the information\\n            matrix formula from Harvey (1989), and 'approx' uses numerical\\n            approximation. This keyword is only relevant if the\\n            optimization method uses the Hessian matrix.\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including smoothed results and in-sample\\n            prediction), although out-of-sample forecasting is possible.\\n            Note that this option is not available when using the EM algorithm\\n            (which is the default for this model). Default is False.\\n        llf_decrease_action : {'ignore', 'warn', 'revert'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            'ignore' continues the iterations, 'warn' issues a warning but\\n            continues the iterations, while 'revert' ends the iterations and\\n            returns the result from the last good iteration. Default is 'warn'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n        **kwargs\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        MLEResults\\n\\n        See Also\\n        --------\\n        statsmodels.base.model.LikelihoodModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        \"\n    if method == 'em':\n        return self.fit_em(start_params=start_params, transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance, **kwargs)\n    else:\n        return super().fit(start_params=start_params, transformed=transformed, includes_fixed=includes_fixed, cov_type=cov_type, cov_kwds=cov_kwds, method=method, maxiter=maxiter, full_output=full_output, disp=disp, callback=callback, return_params=return_params, optim_score=optim_score, optim_complex_step=optim_complex_step, optim_hessian=optim_hessian, flags=flags, low_memory=low_memory, **kwargs)",
            "def fit(self, start_params=None, transformed=True, includes_fixed=False, cov_type='none', cov_kwds=None, method='em', maxiter=500, tolerance=1e-06, em_initialization=True, mstep_method=None, full_output=1, disp=False, callback=None, return_params=False, optim_score=None, optim_complex_step=None, optim_hessian=None, flags=None, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Fits the model by maximum likelihood via Kalman filter.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            If None, the default is given by Model.start_params.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `start_params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - 'opg' for the outer product of gradient estimator\\n            - 'oim' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - 'approx' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - 'robust' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the 'oim'\\n              method.\\n            - 'robust_approx' is the same as 'robust' except that the\\n              intermediate calculations use the 'approx' method.\\n            - 'none' for no covariance matrix calculation.\\n\\n            Default is 'none', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - 'approx_complex_step' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - 'approx_centered' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        method : str, optional\\n            The `method` determines which solver from `scipy.optimize`\\n            is used, and it can be chosen from among the following strings:\\n\\n            - 'em' for the EM algorithm\\n            - 'newton' for Newton-Raphson\\n            - 'nm' for Nelder-Mead\\n            - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)\\n            - 'lbfgs' for limited-memory BFGS with optional box constraints\\n            - 'powell' for modified Powell's method\\n            - 'cg' for conjugate gradient\\n            - 'ncg' for Newton-conjugate gradient\\n            - 'basinhopping' for global basin-hopping solver\\n\\n            The explicit arguments in `fit` are passed to the solver,\\n            with the exception of the basin-hopping solver. Each\\n            solver has several optional arguments that are not the same across\\n            solvers. See the notes section below (or scipy.optimize) for the\\n            available arguments and for the list of explicit arguments that the\\n            basin-hopping solver supports.\\n        maxiter : int, optional\\n            The maximum number of iterations to perform.\\n        tolerance : float, optional\\n            Tolerance to use for convergence checking when using the EM\\n            algorithm. To set the tolerance for other methods, pass\\n            the optimizer-specific keyword argument(s).\\n        full_output : bool, optional\\n            Set to True to have all available output in the Results object's\\n            mle_retvals attribute. The output is dependent on the solver.\\n            See LikelihoodModelResults notes section for more information.\\n        disp : bool, optional\\n            Set to True to print convergence messages.\\n        callback : callable callback(xk), optional\\n            Called after each iteration, as callback(xk), where xk is the\\n            current parameter vector.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        optim_score : {'harvey', 'approx'} or None, optional\\n            The method by which the score vector is calculated. 'harvey' uses\\n            the method from Harvey (1989), 'approx' uses either finite\\n            difference or complex step differentiation depending upon the\\n            value of `optim_complex_step`, and None uses the built-in gradient\\n            approximation of the optimizer. Default is None. This keyword is\\n            only relevant if the optimization method uses the score.\\n        optim_complex_step : bool, optional\\n            Whether or not to use complex step differentiation when\\n            approximating the score; if False, finite difference approximation\\n            is used. Default is True. This keyword is only relevant if\\n            `optim_score` is set to 'harvey' or 'approx'.\\n        optim_hessian : {'opg','oim','approx'}, optional\\n            The method by which the Hessian is numerically approximated. 'opg'\\n            uses outer product of gradients, 'oim' uses the information\\n            matrix formula from Harvey (1989), and 'approx' uses numerical\\n            approximation. This keyword is only relevant if the\\n            optimization method uses the Hessian matrix.\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including smoothed results and in-sample\\n            prediction), although out-of-sample forecasting is possible.\\n            Note that this option is not available when using the EM algorithm\\n            (which is the default for this model). Default is False.\\n        llf_decrease_action : {'ignore', 'warn', 'revert'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            'ignore' continues the iterations, 'warn' issues a warning but\\n            continues the iterations, while 'revert' ends the iterations and\\n            returns the result from the last good iteration. Default is 'warn'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n        **kwargs\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        MLEResults\\n\\n        See Also\\n        --------\\n        statsmodels.base.model.LikelihoodModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        \"\n    if method == 'em':\n        return self.fit_em(start_params=start_params, transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance, **kwargs)\n    else:\n        return super().fit(start_params=start_params, transformed=transformed, includes_fixed=includes_fixed, cov_type=cov_type, cov_kwds=cov_kwds, method=method, maxiter=maxiter, full_output=full_output, disp=disp, callback=callback, return_params=return_params, optim_score=optim_score, optim_complex_step=optim_complex_step, optim_hessian=optim_hessian, flags=flags, low_memory=low_memory, **kwargs)"
        ]
    },
    {
        "func_name": "fit_em",
        "original": "def fit_em(self, start_params=None, transformed=True, cov_type='none', cov_kwds=None, maxiter=500, tolerance=1e-06, disp=False, em_initialization=True, mstep_method=None, full_output=True, return_params=False, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001):\n    \"\"\"\n        Fits the model by maximum likelihood via the EM algorithm.\n\n        Parameters\n        ----------\n        start_params : array_like, optional\n            Initial guess of the solution for the loglikelihood maximization.\n            The default is to use `DynamicFactorMQ.start_params`.\n        transformed : bool, optional\n            Whether or not `start_params` is already transformed. Default is\n            True.\n        cov_type : str, optional\n            The `cov_type` keyword governs the method for calculating the\n            covariance matrix of parameter estimates. Can be one of:\n\n            - 'opg' for the outer product of gradient estimator\n            - 'oim' for the observed information matrix estimator, calculated\n              using the method of Harvey (1989)\n            - 'approx' for the observed information matrix estimator,\n              calculated using a numerical approximation of the Hessian matrix.\n            - 'robust' for an approximate (quasi-maximum likelihood) covariance\n              matrix that may be valid even in the presence of some\n              misspecifications. Intermediate calculations use the 'oim'\n              method.\n            - 'robust_approx' is the same as 'robust' except that the\n              intermediate calculations use the 'approx' method.\n            - 'none' for no covariance matrix calculation.\n\n            Default is 'none', since computing this matrix can be very slow\n            when there are a large number of parameters.\n        cov_kwds : dict or None, optional\n            A dictionary of arguments affecting covariance matrix computation.\n\n            **opg, oim, approx, robust, robust_approx**\n\n            - 'approx_complex_step' : bool, optional - If True, numerical\n              approximations are computed using complex-step methods. If False,\n              numerical approximations are computed using finite difference\n              methods. Default is True.\n            - 'approx_centered' : bool, optional - If True, numerical\n              approximations computed using finite difference methods use a\n              centered approximation. Default is False.\n        maxiter : int, optional\n            The maximum number of EM iterations to perform.\n        tolerance : float, optional\n            Parameter governing convergence of the EM algorithm. The\n            `tolerance` is the minimum relative increase in the likelihood\n            for which convergence will be declared. A smaller value for the\n            `tolerance` will typically yield more precise parameter estimates,\n            but will typically require more EM iterations. Default is 1e-6.\n        disp : int or bool, optional\n            Controls printing of EM iteration progress. If an integer, progress\n            is printed at every `disp` iterations. A value of True is\n            interpreted as the value of 1. Default is False (nothing will be\n            printed).\n        em_initialization : bool, optional\n            Whether or not to also update the Kalman filter initialization\n            using the EM algorithm. Default is True.\n        mstep_method : {None, 'missing', 'nonmissing'}, optional\n            The EM algorithm maximization step. If there are no NaN values\n            in the dataset, this can be set to \"nonmissing\" (which is slightly\n            faster) or \"missing\", otherwise it must be \"missing\". Default is\n            \"nonmissing\" if there are no NaN values or \"missing\" if there are.\n        full_output : bool, optional\n            Set to True to have all available output from EM iterations in\n            the Results object's mle_retvals attribute.\n        return_params : bool, optional\n            Whether or not to return only the array of maximizing parameters.\n            Default is False.\n        low_memory : bool, optional\n            This option cannot be used with the EM algorithm and will raise an\n            error if set to True. Default is False.\n        llf_decrease_action : {'ignore', 'warn', 'revert'}, optional\n            Action to take if the log-likelihood decreases in an EM iteration.\n            'ignore' continues the iterations, 'warn' issues a warning but\n            continues the iterations, while 'revert' ends the iterations and\n            returns the result from the last good iteration. Default is 'warn'.\n        llf_decrease_tolerance : float, optional\n            Minimum size of the log-likelihood decrease required to trigger a\n            warning or to end the EM iterations. Setting this value slightly\n            larger than zero allows small decreases in the log-likelihood that\n            may be caused by numerical issues. If set to zero, then any\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\n\n        Returns\n        -------\n        DynamicFactorMQResults\n\n        See Also\n        --------\n        statsmodels.tsa.statespace.mlemodel.MLEModel.fit\n        statsmodels.tsa.statespace.mlemodel.MLEResults\n        \"\"\"\n    if self._has_fixed_params:\n        raise NotImplementedError('Cannot fit using the EM algorithm while holding some parameters fixed.')\n    if low_memory:\n        raise ValueError('Cannot fit using the EM algorithm when using low_memory option.')\n    if start_params is None:\n        start_params = self.start_params\n        transformed = True\n    else:\n        start_params = np.array(start_params, ndmin=1)\n    if not transformed:\n        start_params = self.transform_params(start_params)\n    llf_decrease_action = string_like(llf_decrease_action, 'llf_decrease_action', options=['ignore', 'warn', 'revert'])\n    disp = int(disp)\n    s = self._s\n    llf = []\n    params = [start_params]\n    init = None\n    inits = [self.ssm.initialization]\n    i = 0\n    delta = 0\n    terminate = False\n    while i < maxiter and (not terminate) and (i < 1 or delta > tolerance):\n        out = self._em_iteration(params[-1], init=init, mstep_method=mstep_method)\n        new_llf = out[0].llf_obs.sum()\n        if not em_initialization:\n            self.update(out[1])\n            switch_init = []\n            T = self['transition']\n            init = self.ssm.initialization\n            iloc = np.arange(self.k_states)\n            if self.k_endog_Q == 0 and (not self.idiosyncratic_ar1):\n                block = s.factor_blocks[0]\n                if init.initialization_type == 'stationary':\n                    Tb = T[block['factors'], block['factors']]\n                    if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                        init.set(block['factors'], 'diffuse')\n                        switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            else:\n                for block in s.factor_blocks:\n                    b = tuple(iloc[block['factors']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[block['factors'], block['factors']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(block['factors'], 'diffuse')\n                            switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            if self.idiosyncratic_ar1:\n                endog_names = self._get_endog_names(as_string=True)\n                for j in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n                    init_type = init.blocks[j,].initialization_type\n                    if init_type == 'stationary':\n                        if not np.abs(T[j, j]) < 1 - 1e-10:\n                            init.set(j, 'diffuse')\n                            name = endog_names[j - s['idio_ar_M'].start]\n                            switch_init.append(f'idiosyncratic AR(1) for monthly variable: {name}')\n                if self.k_endog_Q > 0:\n                    b = tuple(iloc[s['idio_ar_Q']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[s['idio_ar_Q'], s['idio_ar_Q']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(s['idio_ar_Q'], 'diffuse')\n                            switch_init.append('idiosyncratic AR(1) for the block of quarterly variables')\n            if len(switch_init) > 0:\n                warn(f'Non-stationary parameters found at EM iteration {i + 1}, which is not compatible with stationary initialization. Initialization was switched to diffuse for the following:  {switch_init}, and fitting was restarted.')\n                results = self.fit_em(start_params=params[-1], transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance)\n                self.ssm.initialize(self._default_initialization())\n                return results\n        llf_decrease = i > 0 and new_llf - llf[-1] < -llf_decrease_tolerance\n        if llf_decrease_action == 'revert' and llf_decrease:\n            warn(f'Log-likelihood decreased at EM iteration {i + 1}. Reverting to the results from EM iteration {i} (prior to the decrease) and returning the solution.')\n            i -= 1\n            terminate = True\n        else:\n            if llf_decrease_action == 'warn' and llf_decrease:\n                warn(f'Log-likelihood decreased at EM iteration {i + 1}, which can indicate numerical issues.')\n            llf.append(new_llf)\n            params.append(out[1])\n            if em_initialization:\n                init = initialization.Initialization(self.k_states, 'known', constant=out[0].smoothed_state[..., 0], stationary_cov=out[0].smoothed_state_cov[..., 0])\n                inits.append(init)\n            if i > 0:\n                delta = 2 * np.abs(llf[-1] - llf[-2]) / (np.abs(llf[-1]) + np.abs(llf[-2]))\n            else:\n                delta = np.inf\n            if disp and i == 0:\n                print(f'EM start iterations, llf={llf[-1]:.5g}')\n            elif disp and (i + 1) % disp == 0:\n                print(f'EM iteration {i + 1}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g}')\n        i += 1\n    not_converged = i == maxiter and delta > tolerance\n    if not_converged:\n        warn(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n    if disp:\n        if terminate:\n            print(f'EM terminated at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        elif not_converged:\n            print(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        else:\n            print(f'EM converged at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} < tolerance={tolerance:.5g}')\n    if return_params:\n        result = params[-1]\n    else:\n        if em_initialization:\n            base_init = self.ssm.initialization\n            self.ssm.initialization = init\n        result = self.smooth(params[-1], transformed=True, cov_type=cov_type, cov_kwds=cov_kwds)\n        if em_initialization:\n            self.ssm.initialization = base_init\n        if full_output:\n            llf.append(result.llf)\n            em_retvals = Bunch(**{'params': np.array(params), 'llf': np.array(llf), 'iter': i, 'inits': inits})\n            em_settings = Bunch(**{'method': 'em', 'tolerance': tolerance, 'maxiter': maxiter})\n        else:\n            em_retvals = None\n            em_settings = None\n        result._results.mle_retvals = em_retvals\n        result._results.mle_settings = em_settings\n    return result",
        "mutated": [
            "def fit_em(self, start_params=None, transformed=True, cov_type='none', cov_kwds=None, maxiter=500, tolerance=1e-06, disp=False, em_initialization=True, mstep_method=None, full_output=True, return_params=False, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001):\n    if False:\n        i = 10\n    '\\n        Fits the model by maximum likelihood via the EM algorithm.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            The default is to use `DynamicFactorMQ.start_params`.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - \\'opg\\' for the outer product of gradient estimator\\n            - \\'oim\\' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - \\'approx\\' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - \\'robust\\' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the \\'oim\\'\\n              method.\\n            - \\'robust_approx\\' is the same as \\'robust\\' except that the\\n              intermediate calculations use the \\'approx\\' method.\\n            - \\'none\\' for no covariance matrix calculation.\\n\\n            Default is \\'none\\', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - \\'approx_complex_step\\' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - \\'approx_centered\\' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        maxiter : int, optional\\n            The maximum number of EM iterations to perform.\\n        tolerance : float, optional\\n            Parameter governing convergence of the EM algorithm. The\\n            `tolerance` is the minimum relative increase in the likelihood\\n            for which convergence will be declared. A smaller value for the\\n            `tolerance` will typically yield more precise parameter estimates,\\n            but will typically require more EM iterations. Default is 1e-6.\\n        disp : int or bool, optional\\n            Controls printing of EM iteration progress. If an integer, progress\\n            is printed at every `disp` iterations. A value of True is\\n            interpreted as the value of 1. Default is False (nothing will be\\n            printed).\\n        em_initialization : bool, optional\\n            Whether or not to also update the Kalman filter initialization\\n            using the EM algorithm. Default is True.\\n        mstep_method : {None, \\'missing\\', \\'nonmissing\\'}, optional\\n            The EM algorithm maximization step. If there are no NaN values\\n            in the dataset, this can be set to \"nonmissing\" (which is slightly\\n            faster) or \"missing\", otherwise it must be \"missing\". Default is\\n            \"nonmissing\" if there are no NaN values or \"missing\" if there are.\\n        full_output : bool, optional\\n            Set to True to have all available output from EM iterations in\\n            the Results object\\'s mle_retvals attribute.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        low_memory : bool, optional\\n            This option cannot be used with the EM algorithm and will raise an\\n            error if set to True. Default is False.\\n        llf_decrease_action : {\\'ignore\\', \\'warn\\', \\'revert\\'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            \\'ignore\\' continues the iterations, \\'warn\\' issues a warning but\\n            continues the iterations, while \\'revert\\' ends the iterations and\\n            returns the result from the last good iteration. Default is \\'warn\\'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n\\n        Returns\\n        -------\\n        DynamicFactorMQResults\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        '\n    if self._has_fixed_params:\n        raise NotImplementedError('Cannot fit using the EM algorithm while holding some parameters fixed.')\n    if low_memory:\n        raise ValueError('Cannot fit using the EM algorithm when using low_memory option.')\n    if start_params is None:\n        start_params = self.start_params\n        transformed = True\n    else:\n        start_params = np.array(start_params, ndmin=1)\n    if not transformed:\n        start_params = self.transform_params(start_params)\n    llf_decrease_action = string_like(llf_decrease_action, 'llf_decrease_action', options=['ignore', 'warn', 'revert'])\n    disp = int(disp)\n    s = self._s\n    llf = []\n    params = [start_params]\n    init = None\n    inits = [self.ssm.initialization]\n    i = 0\n    delta = 0\n    terminate = False\n    while i < maxiter and (not terminate) and (i < 1 or delta > tolerance):\n        out = self._em_iteration(params[-1], init=init, mstep_method=mstep_method)\n        new_llf = out[0].llf_obs.sum()\n        if not em_initialization:\n            self.update(out[1])\n            switch_init = []\n            T = self['transition']\n            init = self.ssm.initialization\n            iloc = np.arange(self.k_states)\n            if self.k_endog_Q == 0 and (not self.idiosyncratic_ar1):\n                block = s.factor_blocks[0]\n                if init.initialization_type == 'stationary':\n                    Tb = T[block['factors'], block['factors']]\n                    if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                        init.set(block['factors'], 'diffuse')\n                        switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            else:\n                for block in s.factor_blocks:\n                    b = tuple(iloc[block['factors']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[block['factors'], block['factors']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(block['factors'], 'diffuse')\n                            switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            if self.idiosyncratic_ar1:\n                endog_names = self._get_endog_names(as_string=True)\n                for j in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n                    init_type = init.blocks[j,].initialization_type\n                    if init_type == 'stationary':\n                        if not np.abs(T[j, j]) < 1 - 1e-10:\n                            init.set(j, 'diffuse')\n                            name = endog_names[j - s['idio_ar_M'].start]\n                            switch_init.append(f'idiosyncratic AR(1) for monthly variable: {name}')\n                if self.k_endog_Q > 0:\n                    b = tuple(iloc[s['idio_ar_Q']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[s['idio_ar_Q'], s['idio_ar_Q']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(s['idio_ar_Q'], 'diffuse')\n                            switch_init.append('idiosyncratic AR(1) for the block of quarterly variables')\n            if len(switch_init) > 0:\n                warn(f'Non-stationary parameters found at EM iteration {i + 1}, which is not compatible with stationary initialization. Initialization was switched to diffuse for the following:  {switch_init}, and fitting was restarted.')\n                results = self.fit_em(start_params=params[-1], transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance)\n                self.ssm.initialize(self._default_initialization())\n                return results\n        llf_decrease = i > 0 and new_llf - llf[-1] < -llf_decrease_tolerance\n        if llf_decrease_action == 'revert' and llf_decrease:\n            warn(f'Log-likelihood decreased at EM iteration {i + 1}. Reverting to the results from EM iteration {i} (prior to the decrease) and returning the solution.')\n            i -= 1\n            terminate = True\n        else:\n            if llf_decrease_action == 'warn' and llf_decrease:\n                warn(f'Log-likelihood decreased at EM iteration {i + 1}, which can indicate numerical issues.')\n            llf.append(new_llf)\n            params.append(out[1])\n            if em_initialization:\n                init = initialization.Initialization(self.k_states, 'known', constant=out[0].smoothed_state[..., 0], stationary_cov=out[0].smoothed_state_cov[..., 0])\n                inits.append(init)\n            if i > 0:\n                delta = 2 * np.abs(llf[-1] - llf[-2]) / (np.abs(llf[-1]) + np.abs(llf[-2]))\n            else:\n                delta = np.inf\n            if disp and i == 0:\n                print(f'EM start iterations, llf={llf[-1]:.5g}')\n            elif disp and (i + 1) % disp == 0:\n                print(f'EM iteration {i + 1}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g}')\n        i += 1\n    not_converged = i == maxiter and delta > tolerance\n    if not_converged:\n        warn(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n    if disp:\n        if terminate:\n            print(f'EM terminated at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        elif not_converged:\n            print(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        else:\n            print(f'EM converged at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} < tolerance={tolerance:.5g}')\n    if return_params:\n        result = params[-1]\n    else:\n        if em_initialization:\n            base_init = self.ssm.initialization\n            self.ssm.initialization = init\n        result = self.smooth(params[-1], transformed=True, cov_type=cov_type, cov_kwds=cov_kwds)\n        if em_initialization:\n            self.ssm.initialization = base_init\n        if full_output:\n            llf.append(result.llf)\n            em_retvals = Bunch(**{'params': np.array(params), 'llf': np.array(llf), 'iter': i, 'inits': inits})\n            em_settings = Bunch(**{'method': 'em', 'tolerance': tolerance, 'maxiter': maxiter})\n        else:\n            em_retvals = None\n            em_settings = None\n        result._results.mle_retvals = em_retvals\n        result._results.mle_settings = em_settings\n    return result",
            "def fit_em(self, start_params=None, transformed=True, cov_type='none', cov_kwds=None, maxiter=500, tolerance=1e-06, disp=False, em_initialization=True, mstep_method=None, full_output=True, return_params=False, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fits the model by maximum likelihood via the EM algorithm.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            The default is to use `DynamicFactorMQ.start_params`.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - \\'opg\\' for the outer product of gradient estimator\\n            - \\'oim\\' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - \\'approx\\' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - \\'robust\\' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the \\'oim\\'\\n              method.\\n            - \\'robust_approx\\' is the same as \\'robust\\' except that the\\n              intermediate calculations use the \\'approx\\' method.\\n            - \\'none\\' for no covariance matrix calculation.\\n\\n            Default is \\'none\\', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - \\'approx_complex_step\\' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - \\'approx_centered\\' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        maxiter : int, optional\\n            The maximum number of EM iterations to perform.\\n        tolerance : float, optional\\n            Parameter governing convergence of the EM algorithm. The\\n            `tolerance` is the minimum relative increase in the likelihood\\n            for which convergence will be declared. A smaller value for the\\n            `tolerance` will typically yield more precise parameter estimates,\\n            but will typically require more EM iterations. Default is 1e-6.\\n        disp : int or bool, optional\\n            Controls printing of EM iteration progress. If an integer, progress\\n            is printed at every `disp` iterations. A value of True is\\n            interpreted as the value of 1. Default is False (nothing will be\\n            printed).\\n        em_initialization : bool, optional\\n            Whether or not to also update the Kalman filter initialization\\n            using the EM algorithm. Default is True.\\n        mstep_method : {None, \\'missing\\', \\'nonmissing\\'}, optional\\n            The EM algorithm maximization step. If there are no NaN values\\n            in the dataset, this can be set to \"nonmissing\" (which is slightly\\n            faster) or \"missing\", otherwise it must be \"missing\". Default is\\n            \"nonmissing\" if there are no NaN values or \"missing\" if there are.\\n        full_output : bool, optional\\n            Set to True to have all available output from EM iterations in\\n            the Results object\\'s mle_retvals attribute.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        low_memory : bool, optional\\n            This option cannot be used with the EM algorithm and will raise an\\n            error if set to True. Default is False.\\n        llf_decrease_action : {\\'ignore\\', \\'warn\\', \\'revert\\'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            \\'ignore\\' continues the iterations, \\'warn\\' issues a warning but\\n            continues the iterations, while \\'revert\\' ends the iterations and\\n            returns the result from the last good iteration. Default is \\'warn\\'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n\\n        Returns\\n        -------\\n        DynamicFactorMQResults\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        '\n    if self._has_fixed_params:\n        raise NotImplementedError('Cannot fit using the EM algorithm while holding some parameters fixed.')\n    if low_memory:\n        raise ValueError('Cannot fit using the EM algorithm when using low_memory option.')\n    if start_params is None:\n        start_params = self.start_params\n        transformed = True\n    else:\n        start_params = np.array(start_params, ndmin=1)\n    if not transformed:\n        start_params = self.transform_params(start_params)\n    llf_decrease_action = string_like(llf_decrease_action, 'llf_decrease_action', options=['ignore', 'warn', 'revert'])\n    disp = int(disp)\n    s = self._s\n    llf = []\n    params = [start_params]\n    init = None\n    inits = [self.ssm.initialization]\n    i = 0\n    delta = 0\n    terminate = False\n    while i < maxiter and (not terminate) and (i < 1 or delta > tolerance):\n        out = self._em_iteration(params[-1], init=init, mstep_method=mstep_method)\n        new_llf = out[0].llf_obs.sum()\n        if not em_initialization:\n            self.update(out[1])\n            switch_init = []\n            T = self['transition']\n            init = self.ssm.initialization\n            iloc = np.arange(self.k_states)\n            if self.k_endog_Q == 0 and (not self.idiosyncratic_ar1):\n                block = s.factor_blocks[0]\n                if init.initialization_type == 'stationary':\n                    Tb = T[block['factors'], block['factors']]\n                    if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                        init.set(block['factors'], 'diffuse')\n                        switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            else:\n                for block in s.factor_blocks:\n                    b = tuple(iloc[block['factors']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[block['factors'], block['factors']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(block['factors'], 'diffuse')\n                            switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            if self.idiosyncratic_ar1:\n                endog_names = self._get_endog_names(as_string=True)\n                for j in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n                    init_type = init.blocks[j,].initialization_type\n                    if init_type == 'stationary':\n                        if not np.abs(T[j, j]) < 1 - 1e-10:\n                            init.set(j, 'diffuse')\n                            name = endog_names[j - s['idio_ar_M'].start]\n                            switch_init.append(f'idiosyncratic AR(1) for monthly variable: {name}')\n                if self.k_endog_Q > 0:\n                    b = tuple(iloc[s['idio_ar_Q']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[s['idio_ar_Q'], s['idio_ar_Q']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(s['idio_ar_Q'], 'diffuse')\n                            switch_init.append('idiosyncratic AR(1) for the block of quarterly variables')\n            if len(switch_init) > 0:\n                warn(f'Non-stationary parameters found at EM iteration {i + 1}, which is not compatible with stationary initialization. Initialization was switched to diffuse for the following:  {switch_init}, and fitting was restarted.')\n                results = self.fit_em(start_params=params[-1], transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance)\n                self.ssm.initialize(self._default_initialization())\n                return results\n        llf_decrease = i > 0 and new_llf - llf[-1] < -llf_decrease_tolerance\n        if llf_decrease_action == 'revert' and llf_decrease:\n            warn(f'Log-likelihood decreased at EM iteration {i + 1}. Reverting to the results from EM iteration {i} (prior to the decrease) and returning the solution.')\n            i -= 1\n            terminate = True\n        else:\n            if llf_decrease_action == 'warn' and llf_decrease:\n                warn(f'Log-likelihood decreased at EM iteration {i + 1}, which can indicate numerical issues.')\n            llf.append(new_llf)\n            params.append(out[1])\n            if em_initialization:\n                init = initialization.Initialization(self.k_states, 'known', constant=out[0].smoothed_state[..., 0], stationary_cov=out[0].smoothed_state_cov[..., 0])\n                inits.append(init)\n            if i > 0:\n                delta = 2 * np.abs(llf[-1] - llf[-2]) / (np.abs(llf[-1]) + np.abs(llf[-2]))\n            else:\n                delta = np.inf\n            if disp and i == 0:\n                print(f'EM start iterations, llf={llf[-1]:.5g}')\n            elif disp and (i + 1) % disp == 0:\n                print(f'EM iteration {i + 1}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g}')\n        i += 1\n    not_converged = i == maxiter and delta > tolerance\n    if not_converged:\n        warn(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n    if disp:\n        if terminate:\n            print(f'EM terminated at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        elif not_converged:\n            print(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        else:\n            print(f'EM converged at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} < tolerance={tolerance:.5g}')\n    if return_params:\n        result = params[-1]\n    else:\n        if em_initialization:\n            base_init = self.ssm.initialization\n            self.ssm.initialization = init\n        result = self.smooth(params[-1], transformed=True, cov_type=cov_type, cov_kwds=cov_kwds)\n        if em_initialization:\n            self.ssm.initialization = base_init\n        if full_output:\n            llf.append(result.llf)\n            em_retvals = Bunch(**{'params': np.array(params), 'llf': np.array(llf), 'iter': i, 'inits': inits})\n            em_settings = Bunch(**{'method': 'em', 'tolerance': tolerance, 'maxiter': maxiter})\n        else:\n            em_retvals = None\n            em_settings = None\n        result._results.mle_retvals = em_retvals\n        result._results.mle_settings = em_settings\n    return result",
            "def fit_em(self, start_params=None, transformed=True, cov_type='none', cov_kwds=None, maxiter=500, tolerance=1e-06, disp=False, em_initialization=True, mstep_method=None, full_output=True, return_params=False, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fits the model by maximum likelihood via the EM algorithm.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            The default is to use `DynamicFactorMQ.start_params`.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - \\'opg\\' for the outer product of gradient estimator\\n            - \\'oim\\' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - \\'approx\\' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - \\'robust\\' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the \\'oim\\'\\n              method.\\n            - \\'robust_approx\\' is the same as \\'robust\\' except that the\\n              intermediate calculations use the \\'approx\\' method.\\n            - \\'none\\' for no covariance matrix calculation.\\n\\n            Default is \\'none\\', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - \\'approx_complex_step\\' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - \\'approx_centered\\' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        maxiter : int, optional\\n            The maximum number of EM iterations to perform.\\n        tolerance : float, optional\\n            Parameter governing convergence of the EM algorithm. The\\n            `tolerance` is the minimum relative increase in the likelihood\\n            for which convergence will be declared. A smaller value for the\\n            `tolerance` will typically yield more precise parameter estimates,\\n            but will typically require more EM iterations. Default is 1e-6.\\n        disp : int or bool, optional\\n            Controls printing of EM iteration progress. If an integer, progress\\n            is printed at every `disp` iterations. A value of True is\\n            interpreted as the value of 1. Default is False (nothing will be\\n            printed).\\n        em_initialization : bool, optional\\n            Whether or not to also update the Kalman filter initialization\\n            using the EM algorithm. Default is True.\\n        mstep_method : {None, \\'missing\\', \\'nonmissing\\'}, optional\\n            The EM algorithm maximization step. If there are no NaN values\\n            in the dataset, this can be set to \"nonmissing\" (which is slightly\\n            faster) or \"missing\", otherwise it must be \"missing\". Default is\\n            \"nonmissing\" if there are no NaN values or \"missing\" if there are.\\n        full_output : bool, optional\\n            Set to True to have all available output from EM iterations in\\n            the Results object\\'s mle_retvals attribute.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        low_memory : bool, optional\\n            This option cannot be used with the EM algorithm and will raise an\\n            error if set to True. Default is False.\\n        llf_decrease_action : {\\'ignore\\', \\'warn\\', \\'revert\\'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            \\'ignore\\' continues the iterations, \\'warn\\' issues a warning but\\n            continues the iterations, while \\'revert\\' ends the iterations and\\n            returns the result from the last good iteration. Default is \\'warn\\'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n\\n        Returns\\n        -------\\n        DynamicFactorMQResults\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        '\n    if self._has_fixed_params:\n        raise NotImplementedError('Cannot fit using the EM algorithm while holding some parameters fixed.')\n    if low_memory:\n        raise ValueError('Cannot fit using the EM algorithm when using low_memory option.')\n    if start_params is None:\n        start_params = self.start_params\n        transformed = True\n    else:\n        start_params = np.array(start_params, ndmin=1)\n    if not transformed:\n        start_params = self.transform_params(start_params)\n    llf_decrease_action = string_like(llf_decrease_action, 'llf_decrease_action', options=['ignore', 'warn', 'revert'])\n    disp = int(disp)\n    s = self._s\n    llf = []\n    params = [start_params]\n    init = None\n    inits = [self.ssm.initialization]\n    i = 0\n    delta = 0\n    terminate = False\n    while i < maxiter and (not terminate) and (i < 1 or delta > tolerance):\n        out = self._em_iteration(params[-1], init=init, mstep_method=mstep_method)\n        new_llf = out[0].llf_obs.sum()\n        if not em_initialization:\n            self.update(out[1])\n            switch_init = []\n            T = self['transition']\n            init = self.ssm.initialization\n            iloc = np.arange(self.k_states)\n            if self.k_endog_Q == 0 and (not self.idiosyncratic_ar1):\n                block = s.factor_blocks[0]\n                if init.initialization_type == 'stationary':\n                    Tb = T[block['factors'], block['factors']]\n                    if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                        init.set(block['factors'], 'diffuse')\n                        switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            else:\n                for block in s.factor_blocks:\n                    b = tuple(iloc[block['factors']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[block['factors'], block['factors']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(block['factors'], 'diffuse')\n                            switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            if self.idiosyncratic_ar1:\n                endog_names = self._get_endog_names(as_string=True)\n                for j in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n                    init_type = init.blocks[j,].initialization_type\n                    if init_type == 'stationary':\n                        if not np.abs(T[j, j]) < 1 - 1e-10:\n                            init.set(j, 'diffuse')\n                            name = endog_names[j - s['idio_ar_M'].start]\n                            switch_init.append(f'idiosyncratic AR(1) for monthly variable: {name}')\n                if self.k_endog_Q > 0:\n                    b = tuple(iloc[s['idio_ar_Q']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[s['idio_ar_Q'], s['idio_ar_Q']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(s['idio_ar_Q'], 'diffuse')\n                            switch_init.append('idiosyncratic AR(1) for the block of quarterly variables')\n            if len(switch_init) > 0:\n                warn(f'Non-stationary parameters found at EM iteration {i + 1}, which is not compatible with stationary initialization. Initialization was switched to diffuse for the following:  {switch_init}, and fitting was restarted.')\n                results = self.fit_em(start_params=params[-1], transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance)\n                self.ssm.initialize(self._default_initialization())\n                return results\n        llf_decrease = i > 0 and new_llf - llf[-1] < -llf_decrease_tolerance\n        if llf_decrease_action == 'revert' and llf_decrease:\n            warn(f'Log-likelihood decreased at EM iteration {i + 1}. Reverting to the results from EM iteration {i} (prior to the decrease) and returning the solution.')\n            i -= 1\n            terminate = True\n        else:\n            if llf_decrease_action == 'warn' and llf_decrease:\n                warn(f'Log-likelihood decreased at EM iteration {i + 1}, which can indicate numerical issues.')\n            llf.append(new_llf)\n            params.append(out[1])\n            if em_initialization:\n                init = initialization.Initialization(self.k_states, 'known', constant=out[0].smoothed_state[..., 0], stationary_cov=out[0].smoothed_state_cov[..., 0])\n                inits.append(init)\n            if i > 0:\n                delta = 2 * np.abs(llf[-1] - llf[-2]) / (np.abs(llf[-1]) + np.abs(llf[-2]))\n            else:\n                delta = np.inf\n            if disp and i == 0:\n                print(f'EM start iterations, llf={llf[-1]:.5g}')\n            elif disp and (i + 1) % disp == 0:\n                print(f'EM iteration {i + 1}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g}')\n        i += 1\n    not_converged = i == maxiter and delta > tolerance\n    if not_converged:\n        warn(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n    if disp:\n        if terminate:\n            print(f'EM terminated at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        elif not_converged:\n            print(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        else:\n            print(f'EM converged at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} < tolerance={tolerance:.5g}')\n    if return_params:\n        result = params[-1]\n    else:\n        if em_initialization:\n            base_init = self.ssm.initialization\n            self.ssm.initialization = init\n        result = self.smooth(params[-1], transformed=True, cov_type=cov_type, cov_kwds=cov_kwds)\n        if em_initialization:\n            self.ssm.initialization = base_init\n        if full_output:\n            llf.append(result.llf)\n            em_retvals = Bunch(**{'params': np.array(params), 'llf': np.array(llf), 'iter': i, 'inits': inits})\n            em_settings = Bunch(**{'method': 'em', 'tolerance': tolerance, 'maxiter': maxiter})\n        else:\n            em_retvals = None\n            em_settings = None\n        result._results.mle_retvals = em_retvals\n        result._results.mle_settings = em_settings\n    return result",
            "def fit_em(self, start_params=None, transformed=True, cov_type='none', cov_kwds=None, maxiter=500, tolerance=1e-06, disp=False, em_initialization=True, mstep_method=None, full_output=True, return_params=False, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fits the model by maximum likelihood via the EM algorithm.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            The default is to use `DynamicFactorMQ.start_params`.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - \\'opg\\' for the outer product of gradient estimator\\n            - \\'oim\\' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - \\'approx\\' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - \\'robust\\' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the \\'oim\\'\\n              method.\\n            - \\'robust_approx\\' is the same as \\'robust\\' except that the\\n              intermediate calculations use the \\'approx\\' method.\\n            - \\'none\\' for no covariance matrix calculation.\\n\\n            Default is \\'none\\', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - \\'approx_complex_step\\' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - \\'approx_centered\\' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        maxiter : int, optional\\n            The maximum number of EM iterations to perform.\\n        tolerance : float, optional\\n            Parameter governing convergence of the EM algorithm. The\\n            `tolerance` is the minimum relative increase in the likelihood\\n            for which convergence will be declared. A smaller value for the\\n            `tolerance` will typically yield more precise parameter estimates,\\n            but will typically require more EM iterations. Default is 1e-6.\\n        disp : int or bool, optional\\n            Controls printing of EM iteration progress. If an integer, progress\\n            is printed at every `disp` iterations. A value of True is\\n            interpreted as the value of 1. Default is False (nothing will be\\n            printed).\\n        em_initialization : bool, optional\\n            Whether or not to also update the Kalman filter initialization\\n            using the EM algorithm. Default is True.\\n        mstep_method : {None, \\'missing\\', \\'nonmissing\\'}, optional\\n            The EM algorithm maximization step. If there are no NaN values\\n            in the dataset, this can be set to \"nonmissing\" (which is slightly\\n            faster) or \"missing\", otherwise it must be \"missing\". Default is\\n            \"nonmissing\" if there are no NaN values or \"missing\" if there are.\\n        full_output : bool, optional\\n            Set to True to have all available output from EM iterations in\\n            the Results object\\'s mle_retvals attribute.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        low_memory : bool, optional\\n            This option cannot be used with the EM algorithm and will raise an\\n            error if set to True. Default is False.\\n        llf_decrease_action : {\\'ignore\\', \\'warn\\', \\'revert\\'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            \\'ignore\\' continues the iterations, \\'warn\\' issues a warning but\\n            continues the iterations, while \\'revert\\' ends the iterations and\\n            returns the result from the last good iteration. Default is \\'warn\\'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n\\n        Returns\\n        -------\\n        DynamicFactorMQResults\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        '\n    if self._has_fixed_params:\n        raise NotImplementedError('Cannot fit using the EM algorithm while holding some parameters fixed.')\n    if low_memory:\n        raise ValueError('Cannot fit using the EM algorithm when using low_memory option.')\n    if start_params is None:\n        start_params = self.start_params\n        transformed = True\n    else:\n        start_params = np.array(start_params, ndmin=1)\n    if not transformed:\n        start_params = self.transform_params(start_params)\n    llf_decrease_action = string_like(llf_decrease_action, 'llf_decrease_action', options=['ignore', 'warn', 'revert'])\n    disp = int(disp)\n    s = self._s\n    llf = []\n    params = [start_params]\n    init = None\n    inits = [self.ssm.initialization]\n    i = 0\n    delta = 0\n    terminate = False\n    while i < maxiter and (not terminate) and (i < 1 or delta > tolerance):\n        out = self._em_iteration(params[-1], init=init, mstep_method=mstep_method)\n        new_llf = out[0].llf_obs.sum()\n        if not em_initialization:\n            self.update(out[1])\n            switch_init = []\n            T = self['transition']\n            init = self.ssm.initialization\n            iloc = np.arange(self.k_states)\n            if self.k_endog_Q == 0 and (not self.idiosyncratic_ar1):\n                block = s.factor_blocks[0]\n                if init.initialization_type == 'stationary':\n                    Tb = T[block['factors'], block['factors']]\n                    if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                        init.set(block['factors'], 'diffuse')\n                        switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            else:\n                for block in s.factor_blocks:\n                    b = tuple(iloc[block['factors']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[block['factors'], block['factors']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(block['factors'], 'diffuse')\n                            switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            if self.idiosyncratic_ar1:\n                endog_names = self._get_endog_names(as_string=True)\n                for j in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n                    init_type = init.blocks[j,].initialization_type\n                    if init_type == 'stationary':\n                        if not np.abs(T[j, j]) < 1 - 1e-10:\n                            init.set(j, 'diffuse')\n                            name = endog_names[j - s['idio_ar_M'].start]\n                            switch_init.append(f'idiosyncratic AR(1) for monthly variable: {name}')\n                if self.k_endog_Q > 0:\n                    b = tuple(iloc[s['idio_ar_Q']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[s['idio_ar_Q'], s['idio_ar_Q']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(s['idio_ar_Q'], 'diffuse')\n                            switch_init.append('idiosyncratic AR(1) for the block of quarterly variables')\n            if len(switch_init) > 0:\n                warn(f'Non-stationary parameters found at EM iteration {i + 1}, which is not compatible with stationary initialization. Initialization was switched to diffuse for the following:  {switch_init}, and fitting was restarted.')\n                results = self.fit_em(start_params=params[-1], transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance)\n                self.ssm.initialize(self._default_initialization())\n                return results\n        llf_decrease = i > 0 and new_llf - llf[-1] < -llf_decrease_tolerance\n        if llf_decrease_action == 'revert' and llf_decrease:\n            warn(f'Log-likelihood decreased at EM iteration {i + 1}. Reverting to the results from EM iteration {i} (prior to the decrease) and returning the solution.')\n            i -= 1\n            terminate = True\n        else:\n            if llf_decrease_action == 'warn' and llf_decrease:\n                warn(f'Log-likelihood decreased at EM iteration {i + 1}, which can indicate numerical issues.')\n            llf.append(new_llf)\n            params.append(out[1])\n            if em_initialization:\n                init = initialization.Initialization(self.k_states, 'known', constant=out[0].smoothed_state[..., 0], stationary_cov=out[0].smoothed_state_cov[..., 0])\n                inits.append(init)\n            if i > 0:\n                delta = 2 * np.abs(llf[-1] - llf[-2]) / (np.abs(llf[-1]) + np.abs(llf[-2]))\n            else:\n                delta = np.inf\n            if disp and i == 0:\n                print(f'EM start iterations, llf={llf[-1]:.5g}')\n            elif disp and (i + 1) % disp == 0:\n                print(f'EM iteration {i + 1}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g}')\n        i += 1\n    not_converged = i == maxiter and delta > tolerance\n    if not_converged:\n        warn(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n    if disp:\n        if terminate:\n            print(f'EM terminated at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        elif not_converged:\n            print(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        else:\n            print(f'EM converged at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} < tolerance={tolerance:.5g}')\n    if return_params:\n        result = params[-1]\n    else:\n        if em_initialization:\n            base_init = self.ssm.initialization\n            self.ssm.initialization = init\n        result = self.smooth(params[-1], transformed=True, cov_type=cov_type, cov_kwds=cov_kwds)\n        if em_initialization:\n            self.ssm.initialization = base_init\n        if full_output:\n            llf.append(result.llf)\n            em_retvals = Bunch(**{'params': np.array(params), 'llf': np.array(llf), 'iter': i, 'inits': inits})\n            em_settings = Bunch(**{'method': 'em', 'tolerance': tolerance, 'maxiter': maxiter})\n        else:\n            em_retvals = None\n            em_settings = None\n        result._results.mle_retvals = em_retvals\n        result._results.mle_settings = em_settings\n    return result",
            "def fit_em(self, start_params=None, transformed=True, cov_type='none', cov_kwds=None, maxiter=500, tolerance=1e-06, disp=False, em_initialization=True, mstep_method=None, full_output=True, return_params=False, low_memory=False, llf_decrease_action='revert', llf_decrease_tolerance=0.0001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fits the model by maximum likelihood via the EM algorithm.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like, optional\\n            Initial guess of the solution for the loglikelihood maximization.\\n            The default is to use `DynamicFactorMQ.start_params`.\\n        transformed : bool, optional\\n            Whether or not `start_params` is already transformed. Default is\\n            True.\\n        cov_type : str, optional\\n            The `cov_type` keyword governs the method for calculating the\\n            covariance matrix of parameter estimates. Can be one of:\\n\\n            - \\'opg\\' for the outer product of gradient estimator\\n            - \\'oim\\' for the observed information matrix estimator, calculated\\n              using the method of Harvey (1989)\\n            - \\'approx\\' for the observed information matrix estimator,\\n              calculated using a numerical approximation of the Hessian matrix.\\n            - \\'robust\\' for an approximate (quasi-maximum likelihood) covariance\\n              matrix that may be valid even in the presence of some\\n              misspecifications. Intermediate calculations use the \\'oim\\'\\n              method.\\n            - \\'robust_approx\\' is the same as \\'robust\\' except that the\\n              intermediate calculations use the \\'approx\\' method.\\n            - \\'none\\' for no covariance matrix calculation.\\n\\n            Default is \\'none\\', since computing this matrix can be very slow\\n            when there are a large number of parameters.\\n        cov_kwds : dict or None, optional\\n            A dictionary of arguments affecting covariance matrix computation.\\n\\n            **opg, oim, approx, robust, robust_approx**\\n\\n            - \\'approx_complex_step\\' : bool, optional - If True, numerical\\n              approximations are computed using complex-step methods. If False,\\n              numerical approximations are computed using finite difference\\n              methods. Default is True.\\n            - \\'approx_centered\\' : bool, optional - If True, numerical\\n              approximations computed using finite difference methods use a\\n              centered approximation. Default is False.\\n        maxiter : int, optional\\n            The maximum number of EM iterations to perform.\\n        tolerance : float, optional\\n            Parameter governing convergence of the EM algorithm. The\\n            `tolerance` is the minimum relative increase in the likelihood\\n            for which convergence will be declared. A smaller value for the\\n            `tolerance` will typically yield more precise parameter estimates,\\n            but will typically require more EM iterations. Default is 1e-6.\\n        disp : int or bool, optional\\n            Controls printing of EM iteration progress. If an integer, progress\\n            is printed at every `disp` iterations. A value of True is\\n            interpreted as the value of 1. Default is False (nothing will be\\n            printed).\\n        em_initialization : bool, optional\\n            Whether or not to also update the Kalman filter initialization\\n            using the EM algorithm. Default is True.\\n        mstep_method : {None, \\'missing\\', \\'nonmissing\\'}, optional\\n            The EM algorithm maximization step. If there are no NaN values\\n            in the dataset, this can be set to \"nonmissing\" (which is slightly\\n            faster) or \"missing\", otherwise it must be \"missing\". Default is\\n            \"nonmissing\" if there are no NaN values or \"missing\" if there are.\\n        full_output : bool, optional\\n            Set to True to have all available output from EM iterations in\\n            the Results object\\'s mle_retvals attribute.\\n        return_params : bool, optional\\n            Whether or not to return only the array of maximizing parameters.\\n            Default is False.\\n        low_memory : bool, optional\\n            This option cannot be used with the EM algorithm and will raise an\\n            error if set to True. Default is False.\\n        llf_decrease_action : {\\'ignore\\', \\'warn\\', \\'revert\\'}, optional\\n            Action to take if the log-likelihood decreases in an EM iteration.\\n            \\'ignore\\' continues the iterations, \\'warn\\' issues a warning but\\n            continues the iterations, while \\'revert\\' ends the iterations and\\n            returns the result from the last good iteration. Default is \\'warn\\'.\\n        llf_decrease_tolerance : float, optional\\n            Minimum size of the log-likelihood decrease required to trigger a\\n            warning or to end the EM iterations. Setting this value slightly\\n            larger than zero allows small decreases in the log-likelihood that\\n            may be caused by numerical issues. If set to zero, then any\\n            decrease will trigger the `llf_decrease_action`. Default is 1e-4.\\n\\n        Returns\\n        -------\\n        DynamicFactorMQResults\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEModel.fit\\n        statsmodels.tsa.statespace.mlemodel.MLEResults\\n        '\n    if self._has_fixed_params:\n        raise NotImplementedError('Cannot fit using the EM algorithm while holding some parameters fixed.')\n    if low_memory:\n        raise ValueError('Cannot fit using the EM algorithm when using low_memory option.')\n    if start_params is None:\n        start_params = self.start_params\n        transformed = True\n    else:\n        start_params = np.array(start_params, ndmin=1)\n    if not transformed:\n        start_params = self.transform_params(start_params)\n    llf_decrease_action = string_like(llf_decrease_action, 'llf_decrease_action', options=['ignore', 'warn', 'revert'])\n    disp = int(disp)\n    s = self._s\n    llf = []\n    params = [start_params]\n    init = None\n    inits = [self.ssm.initialization]\n    i = 0\n    delta = 0\n    terminate = False\n    while i < maxiter and (not terminate) and (i < 1 or delta > tolerance):\n        out = self._em_iteration(params[-1], init=init, mstep_method=mstep_method)\n        new_llf = out[0].llf_obs.sum()\n        if not em_initialization:\n            self.update(out[1])\n            switch_init = []\n            T = self['transition']\n            init = self.ssm.initialization\n            iloc = np.arange(self.k_states)\n            if self.k_endog_Q == 0 and (not self.idiosyncratic_ar1):\n                block = s.factor_blocks[0]\n                if init.initialization_type == 'stationary':\n                    Tb = T[block['factors'], block['factors']]\n                    if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                        init.set(block['factors'], 'diffuse')\n                        switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            else:\n                for block in s.factor_blocks:\n                    b = tuple(iloc[block['factors']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[block['factors'], block['factors']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(block['factors'], 'diffuse')\n                            switch_init.append(f'factor block: {tuple(block.factor_names)}')\n            if self.idiosyncratic_ar1:\n                endog_names = self._get_endog_names(as_string=True)\n                for j in range(s['idio_ar_M'].start, s['idio_ar_M'].stop):\n                    init_type = init.blocks[j,].initialization_type\n                    if init_type == 'stationary':\n                        if not np.abs(T[j, j]) < 1 - 1e-10:\n                            init.set(j, 'diffuse')\n                            name = endog_names[j - s['idio_ar_M'].start]\n                            switch_init.append(f'idiosyncratic AR(1) for monthly variable: {name}')\n                if self.k_endog_Q > 0:\n                    b = tuple(iloc[s['idio_ar_Q']])\n                    init_type = init.blocks[b].initialization_type\n                    if init_type == 'stationary':\n                        Tb = T[s['idio_ar_Q'], s['idio_ar_Q']]\n                        if not np.all(np.linalg.eigvals(Tb) < 1 - 1e-10):\n                            init.set(s['idio_ar_Q'], 'diffuse')\n                            switch_init.append('idiosyncratic AR(1) for the block of quarterly variables')\n            if len(switch_init) > 0:\n                warn(f'Non-stationary parameters found at EM iteration {i + 1}, which is not compatible with stationary initialization. Initialization was switched to diffuse for the following:  {switch_init}, and fitting was restarted.')\n                results = self.fit_em(start_params=params[-1], transformed=transformed, cov_type=cov_type, cov_kwds=cov_kwds, maxiter=maxiter, tolerance=tolerance, em_initialization=em_initialization, mstep_method=mstep_method, full_output=full_output, disp=disp, return_params=return_params, low_memory=low_memory, llf_decrease_action=llf_decrease_action, llf_decrease_tolerance=llf_decrease_tolerance)\n                self.ssm.initialize(self._default_initialization())\n                return results\n        llf_decrease = i > 0 and new_llf - llf[-1] < -llf_decrease_tolerance\n        if llf_decrease_action == 'revert' and llf_decrease:\n            warn(f'Log-likelihood decreased at EM iteration {i + 1}. Reverting to the results from EM iteration {i} (prior to the decrease) and returning the solution.')\n            i -= 1\n            terminate = True\n        else:\n            if llf_decrease_action == 'warn' and llf_decrease:\n                warn(f'Log-likelihood decreased at EM iteration {i + 1}, which can indicate numerical issues.')\n            llf.append(new_llf)\n            params.append(out[1])\n            if em_initialization:\n                init = initialization.Initialization(self.k_states, 'known', constant=out[0].smoothed_state[..., 0], stationary_cov=out[0].smoothed_state_cov[..., 0])\n                inits.append(init)\n            if i > 0:\n                delta = 2 * np.abs(llf[-1] - llf[-2]) / (np.abs(llf[-1]) + np.abs(llf[-2]))\n            else:\n                delta = np.inf\n            if disp and i == 0:\n                print(f'EM start iterations, llf={llf[-1]:.5g}')\n            elif disp and (i + 1) % disp == 0:\n                print(f'EM iteration {i + 1}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g}')\n        i += 1\n    not_converged = i == maxiter and delta > tolerance\n    if not_converged:\n        warn(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n    if disp:\n        if terminate:\n            print(f'EM terminated at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        elif not_converged:\n            print(f'EM reached maximum number of iterations ({maxiter}), without achieving convergence: llf={llf[-1]:.5g}, convergence criterion={delta:.5g} (while specified tolerance was {tolerance:.5g})')\n        else:\n            print(f'EM converged at iteration {i}, llf={llf[-1]:.5g}, convergence criterion={delta:.5g} < tolerance={tolerance:.5g}')\n    if return_params:\n        result = params[-1]\n    else:\n        if em_initialization:\n            base_init = self.ssm.initialization\n            self.ssm.initialization = init\n        result = self.smooth(params[-1], transformed=True, cov_type=cov_type, cov_kwds=cov_kwds)\n        if em_initialization:\n            self.ssm.initialization = base_init\n        if full_output:\n            llf.append(result.llf)\n            em_retvals = Bunch(**{'params': np.array(params), 'llf': np.array(llf), 'iter': i, 'inits': inits})\n            em_settings = Bunch(**{'method': 'em', 'tolerance': tolerance, 'maxiter': maxiter})\n        else:\n            em_retvals = None\n            em_settings = None\n        result._results.mle_retvals = em_retvals\n        result._results.mle_settings = em_settings\n    return result"
        ]
    },
    {
        "func_name": "_em_iteration",
        "original": "def _em_iteration(self, params0, init=None, mstep_method=None):\n    \"\"\"EM iteration.\"\"\"\n    res = self._em_expectation_step(params0, init=init)\n    params1 = self._em_maximization_step(res, params0, mstep_method=mstep_method)\n    return (res, params1)",
        "mutated": [
            "def _em_iteration(self, params0, init=None, mstep_method=None):\n    if False:\n        i = 10\n    'EM iteration.'\n    res = self._em_expectation_step(params0, init=init)\n    params1 = self._em_maximization_step(res, params0, mstep_method=mstep_method)\n    return (res, params1)",
            "def _em_iteration(self, params0, init=None, mstep_method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'EM iteration.'\n    res = self._em_expectation_step(params0, init=init)\n    params1 = self._em_maximization_step(res, params0, mstep_method=mstep_method)\n    return (res, params1)",
            "def _em_iteration(self, params0, init=None, mstep_method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'EM iteration.'\n    res = self._em_expectation_step(params0, init=init)\n    params1 = self._em_maximization_step(res, params0, mstep_method=mstep_method)\n    return (res, params1)",
            "def _em_iteration(self, params0, init=None, mstep_method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'EM iteration.'\n    res = self._em_expectation_step(params0, init=init)\n    params1 = self._em_maximization_step(res, params0, mstep_method=mstep_method)\n    return (res, params1)",
            "def _em_iteration(self, params0, init=None, mstep_method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'EM iteration.'\n    res = self._em_expectation_step(params0, init=init)\n    params1 = self._em_maximization_step(res, params0, mstep_method=mstep_method)\n    return (res, params1)"
        ]
    },
    {
        "func_name": "_em_expectation_step",
        "original": "def _em_expectation_step(self, params0, init=None):\n    \"\"\"EM expectation step.\"\"\"\n    self.update(params0)\n    if init is not None:\n        base_init = self.ssm.initialization\n        self.ssm.initialization = init\n    res = self.ssm.smooth(SMOOTHER_STATE | SMOOTHER_STATE_COV | SMOOTHER_STATE_AUTOCOV, update_filter=False)\n    res.llf_obs = np.array(self.ssm._kalman_filter.loglikelihood, copy=True)\n    if init is not None:\n        self.ssm.initialization = base_init\n    return res",
        "mutated": [
            "def _em_expectation_step(self, params0, init=None):\n    if False:\n        i = 10\n    'EM expectation step.'\n    self.update(params0)\n    if init is not None:\n        base_init = self.ssm.initialization\n        self.ssm.initialization = init\n    res = self.ssm.smooth(SMOOTHER_STATE | SMOOTHER_STATE_COV | SMOOTHER_STATE_AUTOCOV, update_filter=False)\n    res.llf_obs = np.array(self.ssm._kalman_filter.loglikelihood, copy=True)\n    if init is not None:\n        self.ssm.initialization = base_init\n    return res",
            "def _em_expectation_step(self, params0, init=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'EM expectation step.'\n    self.update(params0)\n    if init is not None:\n        base_init = self.ssm.initialization\n        self.ssm.initialization = init\n    res = self.ssm.smooth(SMOOTHER_STATE | SMOOTHER_STATE_COV | SMOOTHER_STATE_AUTOCOV, update_filter=False)\n    res.llf_obs = np.array(self.ssm._kalman_filter.loglikelihood, copy=True)\n    if init is not None:\n        self.ssm.initialization = base_init\n    return res",
            "def _em_expectation_step(self, params0, init=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'EM expectation step.'\n    self.update(params0)\n    if init is not None:\n        base_init = self.ssm.initialization\n        self.ssm.initialization = init\n    res = self.ssm.smooth(SMOOTHER_STATE | SMOOTHER_STATE_COV | SMOOTHER_STATE_AUTOCOV, update_filter=False)\n    res.llf_obs = np.array(self.ssm._kalman_filter.loglikelihood, copy=True)\n    if init is not None:\n        self.ssm.initialization = base_init\n    return res",
            "def _em_expectation_step(self, params0, init=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'EM expectation step.'\n    self.update(params0)\n    if init is not None:\n        base_init = self.ssm.initialization\n        self.ssm.initialization = init\n    res = self.ssm.smooth(SMOOTHER_STATE | SMOOTHER_STATE_COV | SMOOTHER_STATE_AUTOCOV, update_filter=False)\n    res.llf_obs = np.array(self.ssm._kalman_filter.loglikelihood, copy=True)\n    if init is not None:\n        self.ssm.initialization = base_init\n    return res",
            "def _em_expectation_step(self, params0, init=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'EM expectation step.'\n    self.update(params0)\n    if init is not None:\n        base_init = self.ssm.initialization\n        self.ssm.initialization = init\n    res = self.ssm.smooth(SMOOTHER_STATE | SMOOTHER_STATE_COV | SMOOTHER_STATE_AUTOCOV, update_filter=False)\n    res.llf_obs = np.array(self.ssm._kalman_filter.loglikelihood, copy=True)\n    if init is not None:\n        self.ssm.initialization = base_init\n    return res"
        ]
    },
    {
        "func_name": "_em_maximization_step",
        "original": "def _em_maximization_step(self, res, params0, mstep_method=None):\n    \"\"\"EM maximization step.\"\"\"\n    s = self._s\n    a = res.smoothed_state.T[..., None]\n    cov_a = res.smoothed_state_cov.transpose(2, 0, 1)\n    acov_a = res.smoothed_state_autocov.transpose(2, 0, 1)\n    Eaa = cov_a.copy() + np.matmul(a, a.transpose(0, 2, 1))\n    Eaa1 = acov_a[:-1] + np.matmul(a[1:], a[:-1].transpose(0, 2, 1))\n    has_missing = np.any(res.nmissing)\n    if mstep_method is None:\n        mstep_method = 'missing' if has_missing else 'nonmissing'\n    mstep_method = mstep_method.lower()\n    if mstep_method == 'nonmissing' and has_missing:\n        raise ValueError('Cannot use EM algorithm option `mstep_method=\"nonmissing\"` with missing data.')\n    if mstep_method == 'nonmissing':\n        func = self._em_maximization_obs_nonmissing\n    elif mstep_method == 'missing':\n        func = self._em_maximization_obs_missing\n    else:\n        raise ValueError('Invalid maximization step method: \"%s\".' % mstep_method)\n    (Lambda, H) = func(res, Eaa, a, compute_H=not self.idiosyncratic_ar1)\n    factor_ar = []\n    factor_cov = []\n    for b in s.factor_blocks:\n        A = Eaa[:-1, b['factors_ar'], b['factors_ar']].sum(axis=0)\n        B = Eaa1[:, b['factors_L1'], b['factors_ar']].sum(axis=0)\n        C = Eaa[1:, b['factors_L1'], b['factors_L1']].sum(axis=0)\n        nobs = Eaa.shape[0] - 1\n        try:\n            f_A = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            f_A = np.linalg.solve(A, B.T).T\n        f_Q = (C - f_A @ B.T) / nobs\n        factor_ar += f_A.ravel().tolist()\n        factor_cov += np.linalg.cholesky(f_Q)[np.tril_indices_from(f_Q)].tolist()\n    if self.idiosyncratic_ar1:\n        ix = s['idio_ar_L1']\n        Ad = Eaa[:-1, ix, ix].sum(axis=0).diagonal()\n        Bd = Eaa1[:, ix, ix].sum(axis=0).diagonal()\n        Cd = Eaa[1:, ix, ix].sum(axis=0).diagonal()\n        nobs = Eaa.shape[0] - 1\n        alpha = Bd / Ad\n        sigma2 = (Cd - alpha * Bd) / nobs\n    else:\n        ix = s['idio_ar_L1']\n        C = Eaa[:, ix, ix].sum(axis=0)\n        sigma2 = np.r_[H.diagonal()[self._o['M']], C.diagonal() / Eaa.shape[0]]\n    params1 = np.zeros_like(params0)\n    loadings = []\n    for i in range(self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        loadings += Lambda[i, factor_ix].tolist()\n    params1[self._p['loadings']] = loadings\n    params1[self._p['factor_ar']] = factor_ar\n    params1[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        params1[self._p['idiosyncratic_ar1']] = alpha\n    params1[self._p['idiosyncratic_var']] = sigma2\n    return params1",
        "mutated": [
            "def _em_maximization_step(self, res, params0, mstep_method=None):\n    if False:\n        i = 10\n    'EM maximization step.'\n    s = self._s\n    a = res.smoothed_state.T[..., None]\n    cov_a = res.smoothed_state_cov.transpose(2, 0, 1)\n    acov_a = res.smoothed_state_autocov.transpose(2, 0, 1)\n    Eaa = cov_a.copy() + np.matmul(a, a.transpose(0, 2, 1))\n    Eaa1 = acov_a[:-1] + np.matmul(a[1:], a[:-1].transpose(0, 2, 1))\n    has_missing = np.any(res.nmissing)\n    if mstep_method is None:\n        mstep_method = 'missing' if has_missing else 'nonmissing'\n    mstep_method = mstep_method.lower()\n    if mstep_method == 'nonmissing' and has_missing:\n        raise ValueError('Cannot use EM algorithm option `mstep_method=\"nonmissing\"` with missing data.')\n    if mstep_method == 'nonmissing':\n        func = self._em_maximization_obs_nonmissing\n    elif mstep_method == 'missing':\n        func = self._em_maximization_obs_missing\n    else:\n        raise ValueError('Invalid maximization step method: \"%s\".' % mstep_method)\n    (Lambda, H) = func(res, Eaa, a, compute_H=not self.idiosyncratic_ar1)\n    factor_ar = []\n    factor_cov = []\n    for b in s.factor_blocks:\n        A = Eaa[:-1, b['factors_ar'], b['factors_ar']].sum(axis=0)\n        B = Eaa1[:, b['factors_L1'], b['factors_ar']].sum(axis=0)\n        C = Eaa[1:, b['factors_L1'], b['factors_L1']].sum(axis=0)\n        nobs = Eaa.shape[0] - 1\n        try:\n            f_A = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            f_A = np.linalg.solve(A, B.T).T\n        f_Q = (C - f_A @ B.T) / nobs\n        factor_ar += f_A.ravel().tolist()\n        factor_cov += np.linalg.cholesky(f_Q)[np.tril_indices_from(f_Q)].tolist()\n    if self.idiosyncratic_ar1:\n        ix = s['idio_ar_L1']\n        Ad = Eaa[:-1, ix, ix].sum(axis=0).diagonal()\n        Bd = Eaa1[:, ix, ix].sum(axis=0).diagonal()\n        Cd = Eaa[1:, ix, ix].sum(axis=0).diagonal()\n        nobs = Eaa.shape[0] - 1\n        alpha = Bd / Ad\n        sigma2 = (Cd - alpha * Bd) / nobs\n    else:\n        ix = s['idio_ar_L1']\n        C = Eaa[:, ix, ix].sum(axis=0)\n        sigma2 = np.r_[H.diagonal()[self._o['M']], C.diagonal() / Eaa.shape[0]]\n    params1 = np.zeros_like(params0)\n    loadings = []\n    for i in range(self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        loadings += Lambda[i, factor_ix].tolist()\n    params1[self._p['loadings']] = loadings\n    params1[self._p['factor_ar']] = factor_ar\n    params1[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        params1[self._p['idiosyncratic_ar1']] = alpha\n    params1[self._p['idiosyncratic_var']] = sigma2\n    return params1",
            "def _em_maximization_step(self, res, params0, mstep_method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'EM maximization step.'\n    s = self._s\n    a = res.smoothed_state.T[..., None]\n    cov_a = res.smoothed_state_cov.transpose(2, 0, 1)\n    acov_a = res.smoothed_state_autocov.transpose(2, 0, 1)\n    Eaa = cov_a.copy() + np.matmul(a, a.transpose(0, 2, 1))\n    Eaa1 = acov_a[:-1] + np.matmul(a[1:], a[:-1].transpose(0, 2, 1))\n    has_missing = np.any(res.nmissing)\n    if mstep_method is None:\n        mstep_method = 'missing' if has_missing else 'nonmissing'\n    mstep_method = mstep_method.lower()\n    if mstep_method == 'nonmissing' and has_missing:\n        raise ValueError('Cannot use EM algorithm option `mstep_method=\"nonmissing\"` with missing data.')\n    if mstep_method == 'nonmissing':\n        func = self._em_maximization_obs_nonmissing\n    elif mstep_method == 'missing':\n        func = self._em_maximization_obs_missing\n    else:\n        raise ValueError('Invalid maximization step method: \"%s\".' % mstep_method)\n    (Lambda, H) = func(res, Eaa, a, compute_H=not self.idiosyncratic_ar1)\n    factor_ar = []\n    factor_cov = []\n    for b in s.factor_blocks:\n        A = Eaa[:-1, b['factors_ar'], b['factors_ar']].sum(axis=0)\n        B = Eaa1[:, b['factors_L1'], b['factors_ar']].sum(axis=0)\n        C = Eaa[1:, b['factors_L1'], b['factors_L1']].sum(axis=0)\n        nobs = Eaa.shape[0] - 1\n        try:\n            f_A = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            f_A = np.linalg.solve(A, B.T).T\n        f_Q = (C - f_A @ B.T) / nobs\n        factor_ar += f_A.ravel().tolist()\n        factor_cov += np.linalg.cholesky(f_Q)[np.tril_indices_from(f_Q)].tolist()\n    if self.idiosyncratic_ar1:\n        ix = s['idio_ar_L1']\n        Ad = Eaa[:-1, ix, ix].sum(axis=0).diagonal()\n        Bd = Eaa1[:, ix, ix].sum(axis=0).diagonal()\n        Cd = Eaa[1:, ix, ix].sum(axis=0).diagonal()\n        nobs = Eaa.shape[0] - 1\n        alpha = Bd / Ad\n        sigma2 = (Cd - alpha * Bd) / nobs\n    else:\n        ix = s['idio_ar_L1']\n        C = Eaa[:, ix, ix].sum(axis=0)\n        sigma2 = np.r_[H.diagonal()[self._o['M']], C.diagonal() / Eaa.shape[0]]\n    params1 = np.zeros_like(params0)\n    loadings = []\n    for i in range(self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        loadings += Lambda[i, factor_ix].tolist()\n    params1[self._p['loadings']] = loadings\n    params1[self._p['factor_ar']] = factor_ar\n    params1[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        params1[self._p['idiosyncratic_ar1']] = alpha\n    params1[self._p['idiosyncratic_var']] = sigma2\n    return params1",
            "def _em_maximization_step(self, res, params0, mstep_method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'EM maximization step.'\n    s = self._s\n    a = res.smoothed_state.T[..., None]\n    cov_a = res.smoothed_state_cov.transpose(2, 0, 1)\n    acov_a = res.smoothed_state_autocov.transpose(2, 0, 1)\n    Eaa = cov_a.copy() + np.matmul(a, a.transpose(0, 2, 1))\n    Eaa1 = acov_a[:-1] + np.matmul(a[1:], a[:-1].transpose(0, 2, 1))\n    has_missing = np.any(res.nmissing)\n    if mstep_method is None:\n        mstep_method = 'missing' if has_missing else 'nonmissing'\n    mstep_method = mstep_method.lower()\n    if mstep_method == 'nonmissing' and has_missing:\n        raise ValueError('Cannot use EM algorithm option `mstep_method=\"nonmissing\"` with missing data.')\n    if mstep_method == 'nonmissing':\n        func = self._em_maximization_obs_nonmissing\n    elif mstep_method == 'missing':\n        func = self._em_maximization_obs_missing\n    else:\n        raise ValueError('Invalid maximization step method: \"%s\".' % mstep_method)\n    (Lambda, H) = func(res, Eaa, a, compute_H=not self.idiosyncratic_ar1)\n    factor_ar = []\n    factor_cov = []\n    for b in s.factor_blocks:\n        A = Eaa[:-1, b['factors_ar'], b['factors_ar']].sum(axis=0)\n        B = Eaa1[:, b['factors_L1'], b['factors_ar']].sum(axis=0)\n        C = Eaa[1:, b['factors_L1'], b['factors_L1']].sum(axis=0)\n        nobs = Eaa.shape[0] - 1\n        try:\n            f_A = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            f_A = np.linalg.solve(A, B.T).T\n        f_Q = (C - f_A @ B.T) / nobs\n        factor_ar += f_A.ravel().tolist()\n        factor_cov += np.linalg.cholesky(f_Q)[np.tril_indices_from(f_Q)].tolist()\n    if self.idiosyncratic_ar1:\n        ix = s['idio_ar_L1']\n        Ad = Eaa[:-1, ix, ix].sum(axis=0).diagonal()\n        Bd = Eaa1[:, ix, ix].sum(axis=0).diagonal()\n        Cd = Eaa[1:, ix, ix].sum(axis=0).diagonal()\n        nobs = Eaa.shape[0] - 1\n        alpha = Bd / Ad\n        sigma2 = (Cd - alpha * Bd) / nobs\n    else:\n        ix = s['idio_ar_L1']\n        C = Eaa[:, ix, ix].sum(axis=0)\n        sigma2 = np.r_[H.diagonal()[self._o['M']], C.diagonal() / Eaa.shape[0]]\n    params1 = np.zeros_like(params0)\n    loadings = []\n    for i in range(self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        loadings += Lambda[i, factor_ix].tolist()\n    params1[self._p['loadings']] = loadings\n    params1[self._p['factor_ar']] = factor_ar\n    params1[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        params1[self._p['idiosyncratic_ar1']] = alpha\n    params1[self._p['idiosyncratic_var']] = sigma2\n    return params1",
            "def _em_maximization_step(self, res, params0, mstep_method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'EM maximization step.'\n    s = self._s\n    a = res.smoothed_state.T[..., None]\n    cov_a = res.smoothed_state_cov.transpose(2, 0, 1)\n    acov_a = res.smoothed_state_autocov.transpose(2, 0, 1)\n    Eaa = cov_a.copy() + np.matmul(a, a.transpose(0, 2, 1))\n    Eaa1 = acov_a[:-1] + np.matmul(a[1:], a[:-1].transpose(0, 2, 1))\n    has_missing = np.any(res.nmissing)\n    if mstep_method is None:\n        mstep_method = 'missing' if has_missing else 'nonmissing'\n    mstep_method = mstep_method.lower()\n    if mstep_method == 'nonmissing' and has_missing:\n        raise ValueError('Cannot use EM algorithm option `mstep_method=\"nonmissing\"` with missing data.')\n    if mstep_method == 'nonmissing':\n        func = self._em_maximization_obs_nonmissing\n    elif mstep_method == 'missing':\n        func = self._em_maximization_obs_missing\n    else:\n        raise ValueError('Invalid maximization step method: \"%s\".' % mstep_method)\n    (Lambda, H) = func(res, Eaa, a, compute_H=not self.idiosyncratic_ar1)\n    factor_ar = []\n    factor_cov = []\n    for b in s.factor_blocks:\n        A = Eaa[:-1, b['factors_ar'], b['factors_ar']].sum(axis=0)\n        B = Eaa1[:, b['factors_L1'], b['factors_ar']].sum(axis=0)\n        C = Eaa[1:, b['factors_L1'], b['factors_L1']].sum(axis=0)\n        nobs = Eaa.shape[0] - 1\n        try:\n            f_A = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            f_A = np.linalg.solve(A, B.T).T\n        f_Q = (C - f_A @ B.T) / nobs\n        factor_ar += f_A.ravel().tolist()\n        factor_cov += np.linalg.cholesky(f_Q)[np.tril_indices_from(f_Q)].tolist()\n    if self.idiosyncratic_ar1:\n        ix = s['idio_ar_L1']\n        Ad = Eaa[:-1, ix, ix].sum(axis=0).diagonal()\n        Bd = Eaa1[:, ix, ix].sum(axis=0).diagonal()\n        Cd = Eaa[1:, ix, ix].sum(axis=0).diagonal()\n        nobs = Eaa.shape[0] - 1\n        alpha = Bd / Ad\n        sigma2 = (Cd - alpha * Bd) / nobs\n    else:\n        ix = s['idio_ar_L1']\n        C = Eaa[:, ix, ix].sum(axis=0)\n        sigma2 = np.r_[H.diagonal()[self._o['M']], C.diagonal() / Eaa.shape[0]]\n    params1 = np.zeros_like(params0)\n    loadings = []\n    for i in range(self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        loadings += Lambda[i, factor_ix].tolist()\n    params1[self._p['loadings']] = loadings\n    params1[self._p['factor_ar']] = factor_ar\n    params1[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        params1[self._p['idiosyncratic_ar1']] = alpha\n    params1[self._p['idiosyncratic_var']] = sigma2\n    return params1",
            "def _em_maximization_step(self, res, params0, mstep_method=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'EM maximization step.'\n    s = self._s\n    a = res.smoothed_state.T[..., None]\n    cov_a = res.smoothed_state_cov.transpose(2, 0, 1)\n    acov_a = res.smoothed_state_autocov.transpose(2, 0, 1)\n    Eaa = cov_a.copy() + np.matmul(a, a.transpose(0, 2, 1))\n    Eaa1 = acov_a[:-1] + np.matmul(a[1:], a[:-1].transpose(0, 2, 1))\n    has_missing = np.any(res.nmissing)\n    if mstep_method is None:\n        mstep_method = 'missing' if has_missing else 'nonmissing'\n    mstep_method = mstep_method.lower()\n    if mstep_method == 'nonmissing' and has_missing:\n        raise ValueError('Cannot use EM algorithm option `mstep_method=\"nonmissing\"` with missing data.')\n    if mstep_method == 'nonmissing':\n        func = self._em_maximization_obs_nonmissing\n    elif mstep_method == 'missing':\n        func = self._em_maximization_obs_missing\n    else:\n        raise ValueError('Invalid maximization step method: \"%s\".' % mstep_method)\n    (Lambda, H) = func(res, Eaa, a, compute_H=not self.idiosyncratic_ar1)\n    factor_ar = []\n    factor_cov = []\n    for b in s.factor_blocks:\n        A = Eaa[:-1, b['factors_ar'], b['factors_ar']].sum(axis=0)\n        B = Eaa1[:, b['factors_L1'], b['factors_ar']].sum(axis=0)\n        C = Eaa[1:, b['factors_L1'], b['factors_L1']].sum(axis=0)\n        nobs = Eaa.shape[0] - 1\n        try:\n            f_A = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            f_A = np.linalg.solve(A, B.T).T\n        f_Q = (C - f_A @ B.T) / nobs\n        factor_ar += f_A.ravel().tolist()\n        factor_cov += np.linalg.cholesky(f_Q)[np.tril_indices_from(f_Q)].tolist()\n    if self.idiosyncratic_ar1:\n        ix = s['idio_ar_L1']\n        Ad = Eaa[:-1, ix, ix].sum(axis=0).diagonal()\n        Bd = Eaa1[:, ix, ix].sum(axis=0).diagonal()\n        Cd = Eaa[1:, ix, ix].sum(axis=0).diagonal()\n        nobs = Eaa.shape[0] - 1\n        alpha = Bd / Ad\n        sigma2 = (Cd - alpha * Bd) / nobs\n    else:\n        ix = s['idio_ar_L1']\n        C = Eaa[:, ix, ix].sum(axis=0)\n        sigma2 = np.r_[H.diagonal()[self._o['M']], C.diagonal() / Eaa.shape[0]]\n    params1 = np.zeros_like(params0)\n    loadings = []\n    for i in range(self.k_endog):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        loadings += Lambda[i, factor_ix].tolist()\n    params1[self._p['loadings']] = loadings\n    params1[self._p['factor_ar']] = factor_ar\n    params1[self._p['factor_cov']] = factor_cov\n    if self.idiosyncratic_ar1:\n        params1[self._p['idiosyncratic_ar1']] = alpha\n    params1[self._p['idiosyncratic_var']] = sigma2\n    return params1"
        ]
    },
    {
        "func_name": "_em_maximization_obs_nonmissing",
        "original": "def _em_maximization_obs_nonmissing(self, res, Eaa, a, compute_H=False):\n    \"\"\"EM maximization step, observation equation without missing data.\"\"\"\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    for i in range(self.k_endog):\n        y = self.endog[:, i:i + 1]\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        ix = (np.s_[:],) + np.ix_(factor_ix, factor_ix)\n        A = Eaa[ix].sum(axis=0)\n        B = y.T @ a[:, factor_ix, 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            B -= Eaa[:, ix1:ix2, factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(A, B.T).T\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :k] = Lambda\n        BL = self.endog.T @ a[..., 0] @ Z.T\n        C = self.endog.T @ self.endog\n        H = (C + -BL - BL.T + Z @ Eaa.sum(axis=0) @ Z.T) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
        "mutated": [
            "def _em_maximization_obs_nonmissing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n    'EM maximization step, observation equation without missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    for i in range(self.k_endog):\n        y = self.endog[:, i:i + 1]\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        ix = (np.s_[:],) + np.ix_(factor_ix, factor_ix)\n        A = Eaa[ix].sum(axis=0)\n        B = y.T @ a[:, factor_ix, 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            B -= Eaa[:, ix1:ix2, factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(A, B.T).T\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :k] = Lambda\n        BL = self.endog.T @ a[..., 0] @ Z.T\n        C = self.endog.T @ self.endog\n        H = (C + -BL - BL.T + Z @ Eaa.sum(axis=0) @ Z.T) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
            "def _em_maximization_obs_nonmissing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'EM maximization step, observation equation without missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    for i in range(self.k_endog):\n        y = self.endog[:, i:i + 1]\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        ix = (np.s_[:],) + np.ix_(factor_ix, factor_ix)\n        A = Eaa[ix].sum(axis=0)\n        B = y.T @ a[:, factor_ix, 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            B -= Eaa[:, ix1:ix2, factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(A, B.T).T\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :k] = Lambda\n        BL = self.endog.T @ a[..., 0] @ Z.T\n        C = self.endog.T @ self.endog\n        H = (C + -BL - BL.T + Z @ Eaa.sum(axis=0) @ Z.T) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
            "def _em_maximization_obs_nonmissing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'EM maximization step, observation equation without missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    for i in range(self.k_endog):\n        y = self.endog[:, i:i + 1]\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        ix = (np.s_[:],) + np.ix_(factor_ix, factor_ix)\n        A = Eaa[ix].sum(axis=0)\n        B = y.T @ a[:, factor_ix, 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            B -= Eaa[:, ix1:ix2, factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(A, B.T).T\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :k] = Lambda\n        BL = self.endog.T @ a[..., 0] @ Z.T\n        C = self.endog.T @ self.endog\n        H = (C + -BL - BL.T + Z @ Eaa.sum(axis=0) @ Z.T) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
            "def _em_maximization_obs_nonmissing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'EM maximization step, observation equation without missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    for i in range(self.k_endog):\n        y = self.endog[:, i:i + 1]\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        ix = (np.s_[:],) + np.ix_(factor_ix, factor_ix)\n        A = Eaa[ix].sum(axis=0)\n        B = y.T @ a[:, factor_ix, 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            B -= Eaa[:, ix1:ix2, factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(A, B.T).T\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :k] = Lambda\n        BL = self.endog.T @ a[..., 0] @ Z.T\n        C = self.endog.T @ self.endog\n        H = (C + -BL - BL.T + Z @ Eaa.sum(axis=0) @ Z.T) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
            "def _em_maximization_obs_nonmissing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'EM maximization step, observation equation without missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    for i in range(self.k_endog):\n        y = self.endog[:, i:i + 1]\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        ix = (np.s_[:],) + np.ix_(factor_ix, factor_ix)\n        A = Eaa[ix].sum(axis=0)\n        B = y.T @ a[:, factor_ix, 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            B -= Eaa[:, ix1:ix2, factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(A), B.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(A, B.T).T\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :k] = Lambda\n        BL = self.endog.T @ a[..., 0] @ Z.T\n        C = self.endog.T @ self.endog\n        H = (C + -BL - BL.T + Z @ Eaa.sum(axis=0) @ Z.T) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)"
        ]
    },
    {
        "func_name": "_em_maximization_obs_missing",
        "original": "def _em_maximization_obs_missing(self, res, Eaa, a, compute_H=False):\n    \"\"\"EM maximization step, observation equation with missing data.\"\"\"\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    W = 1 - res.missing.T\n    mask = W.astype(bool)\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        m = mask[:, i]\n        yt = self.endog[m, i:i + 1]\n        ix = np.ix_(m, factor_ix, factor_ix)\n        Ai = Eaa[ix].sum(axis=0)\n        Bi = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            Bi -= Eaa[m, ix1:ix2][..., factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(Ai), Bi.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(Ai, Bi.T).T\n    if self.k_endog_Q > 0:\n        multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n        for i in range(self.k_endog_M, self.k_endog):\n            iloc = self._s.endog_factor_iloc[i]\n            factor_ix = s['factors_L1_5_ix'][:, iloc].ravel().tolist()\n            (R, _) = self.loading_constraints(i)\n            iQ = i - self.k_endog_M\n            m = mask[:, i]\n            yt = self.endog[m, i:i + 1]\n            ix = np.ix_(m, factor_ix, factor_ix)\n            Ai = Eaa[ix].sum(axis=0)\n            BiQ = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n            if self.idiosyncratic_ar1:\n                ix = (np.s_[:],) + np.ix_(s['idio_ar_Q_ix'][iQ], factor_ix)\n                Eepsf = Eaa[ix]\n                BiQ -= (multipliers * Eepsf[m].sum(axis=0)).sum(axis=0)\n            try:\n                L_and_lower = cho_factor(Ai)\n                unrestricted = cho_solve(L_and_lower, BiQ.T).T[0]\n                AiiRT = cho_solve(L_and_lower, R.T)\n                L_and_lower = cho_factor(R @ AiiRT)\n                RAiiRTiR = cho_solve(L_and_lower, R)\n                restricted = unrestricted - AiiRT @ RAiiRTiR @ unrestricted\n            except LinAlgError:\n                Aii = np.linalg.inv(Ai)\n                unrestricted = (BiQ @ Aii)[0]\n                RARi = np.linalg.inv(R @ Aii @ R.T)\n                restricted = unrestricted - Aii @ R.T @ RARi @ R @ unrestricted\n            Lambda[i, factor_ix] = restricted\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :Lambda.shape[1]] = Lambda\n        y = np.nan_to_num(self.endog)\n        C = y.T @ y\n        W = W[..., None]\n        IW = 1 - W\n        WL = W * Z\n        WLT = WL.transpose(0, 2, 1)\n        BL = y[..., None] @ a.transpose(0, 2, 1) @ WLT\n        A = Eaa\n        BLT = BL.transpose(0, 2, 1)\n        IWT = IW.transpose(0, 2, 1)\n        H = (C + (-BL - BLT + WL @ A @ WLT + IW * self['obs_cov'] * IWT).sum(axis=0)) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
        "mutated": [
            "def _em_maximization_obs_missing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n    'EM maximization step, observation equation with missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    W = 1 - res.missing.T\n    mask = W.astype(bool)\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        m = mask[:, i]\n        yt = self.endog[m, i:i + 1]\n        ix = np.ix_(m, factor_ix, factor_ix)\n        Ai = Eaa[ix].sum(axis=0)\n        Bi = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            Bi -= Eaa[m, ix1:ix2][..., factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(Ai), Bi.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(Ai, Bi.T).T\n    if self.k_endog_Q > 0:\n        multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n        for i in range(self.k_endog_M, self.k_endog):\n            iloc = self._s.endog_factor_iloc[i]\n            factor_ix = s['factors_L1_5_ix'][:, iloc].ravel().tolist()\n            (R, _) = self.loading_constraints(i)\n            iQ = i - self.k_endog_M\n            m = mask[:, i]\n            yt = self.endog[m, i:i + 1]\n            ix = np.ix_(m, factor_ix, factor_ix)\n            Ai = Eaa[ix].sum(axis=0)\n            BiQ = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n            if self.idiosyncratic_ar1:\n                ix = (np.s_[:],) + np.ix_(s['idio_ar_Q_ix'][iQ], factor_ix)\n                Eepsf = Eaa[ix]\n                BiQ -= (multipliers * Eepsf[m].sum(axis=0)).sum(axis=0)\n            try:\n                L_and_lower = cho_factor(Ai)\n                unrestricted = cho_solve(L_and_lower, BiQ.T).T[0]\n                AiiRT = cho_solve(L_and_lower, R.T)\n                L_and_lower = cho_factor(R @ AiiRT)\n                RAiiRTiR = cho_solve(L_and_lower, R)\n                restricted = unrestricted - AiiRT @ RAiiRTiR @ unrestricted\n            except LinAlgError:\n                Aii = np.linalg.inv(Ai)\n                unrestricted = (BiQ @ Aii)[0]\n                RARi = np.linalg.inv(R @ Aii @ R.T)\n                restricted = unrestricted - Aii @ R.T @ RARi @ R @ unrestricted\n            Lambda[i, factor_ix] = restricted\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :Lambda.shape[1]] = Lambda\n        y = np.nan_to_num(self.endog)\n        C = y.T @ y\n        W = W[..., None]\n        IW = 1 - W\n        WL = W * Z\n        WLT = WL.transpose(0, 2, 1)\n        BL = y[..., None] @ a.transpose(0, 2, 1) @ WLT\n        A = Eaa\n        BLT = BL.transpose(0, 2, 1)\n        IWT = IW.transpose(0, 2, 1)\n        H = (C + (-BL - BLT + WL @ A @ WLT + IW * self['obs_cov'] * IWT).sum(axis=0)) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
            "def _em_maximization_obs_missing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'EM maximization step, observation equation with missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    W = 1 - res.missing.T\n    mask = W.astype(bool)\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        m = mask[:, i]\n        yt = self.endog[m, i:i + 1]\n        ix = np.ix_(m, factor_ix, factor_ix)\n        Ai = Eaa[ix].sum(axis=0)\n        Bi = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            Bi -= Eaa[m, ix1:ix2][..., factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(Ai), Bi.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(Ai, Bi.T).T\n    if self.k_endog_Q > 0:\n        multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n        for i in range(self.k_endog_M, self.k_endog):\n            iloc = self._s.endog_factor_iloc[i]\n            factor_ix = s['factors_L1_5_ix'][:, iloc].ravel().tolist()\n            (R, _) = self.loading_constraints(i)\n            iQ = i - self.k_endog_M\n            m = mask[:, i]\n            yt = self.endog[m, i:i + 1]\n            ix = np.ix_(m, factor_ix, factor_ix)\n            Ai = Eaa[ix].sum(axis=0)\n            BiQ = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n            if self.idiosyncratic_ar1:\n                ix = (np.s_[:],) + np.ix_(s['idio_ar_Q_ix'][iQ], factor_ix)\n                Eepsf = Eaa[ix]\n                BiQ -= (multipliers * Eepsf[m].sum(axis=0)).sum(axis=0)\n            try:\n                L_and_lower = cho_factor(Ai)\n                unrestricted = cho_solve(L_and_lower, BiQ.T).T[0]\n                AiiRT = cho_solve(L_and_lower, R.T)\n                L_and_lower = cho_factor(R @ AiiRT)\n                RAiiRTiR = cho_solve(L_and_lower, R)\n                restricted = unrestricted - AiiRT @ RAiiRTiR @ unrestricted\n            except LinAlgError:\n                Aii = np.linalg.inv(Ai)\n                unrestricted = (BiQ @ Aii)[0]\n                RARi = np.linalg.inv(R @ Aii @ R.T)\n                restricted = unrestricted - Aii @ R.T @ RARi @ R @ unrestricted\n            Lambda[i, factor_ix] = restricted\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :Lambda.shape[1]] = Lambda\n        y = np.nan_to_num(self.endog)\n        C = y.T @ y\n        W = W[..., None]\n        IW = 1 - W\n        WL = W * Z\n        WLT = WL.transpose(0, 2, 1)\n        BL = y[..., None] @ a.transpose(0, 2, 1) @ WLT\n        A = Eaa\n        BLT = BL.transpose(0, 2, 1)\n        IWT = IW.transpose(0, 2, 1)\n        H = (C + (-BL - BLT + WL @ A @ WLT + IW * self['obs_cov'] * IWT).sum(axis=0)) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
            "def _em_maximization_obs_missing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'EM maximization step, observation equation with missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    W = 1 - res.missing.T\n    mask = W.astype(bool)\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        m = mask[:, i]\n        yt = self.endog[m, i:i + 1]\n        ix = np.ix_(m, factor_ix, factor_ix)\n        Ai = Eaa[ix].sum(axis=0)\n        Bi = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            Bi -= Eaa[m, ix1:ix2][..., factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(Ai), Bi.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(Ai, Bi.T).T\n    if self.k_endog_Q > 0:\n        multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n        for i in range(self.k_endog_M, self.k_endog):\n            iloc = self._s.endog_factor_iloc[i]\n            factor_ix = s['factors_L1_5_ix'][:, iloc].ravel().tolist()\n            (R, _) = self.loading_constraints(i)\n            iQ = i - self.k_endog_M\n            m = mask[:, i]\n            yt = self.endog[m, i:i + 1]\n            ix = np.ix_(m, factor_ix, factor_ix)\n            Ai = Eaa[ix].sum(axis=0)\n            BiQ = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n            if self.idiosyncratic_ar1:\n                ix = (np.s_[:],) + np.ix_(s['idio_ar_Q_ix'][iQ], factor_ix)\n                Eepsf = Eaa[ix]\n                BiQ -= (multipliers * Eepsf[m].sum(axis=0)).sum(axis=0)\n            try:\n                L_and_lower = cho_factor(Ai)\n                unrestricted = cho_solve(L_and_lower, BiQ.T).T[0]\n                AiiRT = cho_solve(L_and_lower, R.T)\n                L_and_lower = cho_factor(R @ AiiRT)\n                RAiiRTiR = cho_solve(L_and_lower, R)\n                restricted = unrestricted - AiiRT @ RAiiRTiR @ unrestricted\n            except LinAlgError:\n                Aii = np.linalg.inv(Ai)\n                unrestricted = (BiQ @ Aii)[0]\n                RARi = np.linalg.inv(R @ Aii @ R.T)\n                restricted = unrestricted - Aii @ R.T @ RARi @ R @ unrestricted\n            Lambda[i, factor_ix] = restricted\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :Lambda.shape[1]] = Lambda\n        y = np.nan_to_num(self.endog)\n        C = y.T @ y\n        W = W[..., None]\n        IW = 1 - W\n        WL = W * Z\n        WLT = WL.transpose(0, 2, 1)\n        BL = y[..., None] @ a.transpose(0, 2, 1) @ WLT\n        A = Eaa\n        BLT = BL.transpose(0, 2, 1)\n        IWT = IW.transpose(0, 2, 1)\n        H = (C + (-BL - BLT + WL @ A @ WLT + IW * self['obs_cov'] * IWT).sum(axis=0)) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
            "def _em_maximization_obs_missing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'EM maximization step, observation equation with missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    W = 1 - res.missing.T\n    mask = W.astype(bool)\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        m = mask[:, i]\n        yt = self.endog[m, i:i + 1]\n        ix = np.ix_(m, factor_ix, factor_ix)\n        Ai = Eaa[ix].sum(axis=0)\n        Bi = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            Bi -= Eaa[m, ix1:ix2][..., factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(Ai), Bi.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(Ai, Bi.T).T\n    if self.k_endog_Q > 0:\n        multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n        for i in range(self.k_endog_M, self.k_endog):\n            iloc = self._s.endog_factor_iloc[i]\n            factor_ix = s['factors_L1_5_ix'][:, iloc].ravel().tolist()\n            (R, _) = self.loading_constraints(i)\n            iQ = i - self.k_endog_M\n            m = mask[:, i]\n            yt = self.endog[m, i:i + 1]\n            ix = np.ix_(m, factor_ix, factor_ix)\n            Ai = Eaa[ix].sum(axis=0)\n            BiQ = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n            if self.idiosyncratic_ar1:\n                ix = (np.s_[:],) + np.ix_(s['idio_ar_Q_ix'][iQ], factor_ix)\n                Eepsf = Eaa[ix]\n                BiQ -= (multipliers * Eepsf[m].sum(axis=0)).sum(axis=0)\n            try:\n                L_and_lower = cho_factor(Ai)\n                unrestricted = cho_solve(L_and_lower, BiQ.T).T[0]\n                AiiRT = cho_solve(L_and_lower, R.T)\n                L_and_lower = cho_factor(R @ AiiRT)\n                RAiiRTiR = cho_solve(L_and_lower, R)\n                restricted = unrestricted - AiiRT @ RAiiRTiR @ unrestricted\n            except LinAlgError:\n                Aii = np.linalg.inv(Ai)\n                unrestricted = (BiQ @ Aii)[0]\n                RARi = np.linalg.inv(R @ Aii @ R.T)\n                restricted = unrestricted - Aii @ R.T @ RARi @ R @ unrestricted\n            Lambda[i, factor_ix] = restricted\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :Lambda.shape[1]] = Lambda\n        y = np.nan_to_num(self.endog)\n        C = y.T @ y\n        W = W[..., None]\n        IW = 1 - W\n        WL = W * Z\n        WLT = WL.transpose(0, 2, 1)\n        BL = y[..., None] @ a.transpose(0, 2, 1) @ WLT\n        A = Eaa\n        BLT = BL.transpose(0, 2, 1)\n        IWT = IW.transpose(0, 2, 1)\n        H = (C + (-BL - BLT + WL @ A @ WLT + IW * self['obs_cov'] * IWT).sum(axis=0)) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)",
            "def _em_maximization_obs_missing(self, res, Eaa, a, compute_H=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'EM maximization step, observation equation with missing data.'\n    s = self._s\n    dtype = Eaa.dtype\n    k = s.k_states_factors\n    Lambda = np.zeros((self.k_endog, k), dtype=dtype)\n    W = 1 - res.missing.T\n    mask = W.astype(bool)\n    for i in range(self.k_endog_M):\n        iloc = self._s.endog_factor_iloc[i]\n        factor_ix = s['factors_L1'][iloc]\n        m = mask[:, i]\n        yt = self.endog[m, i:i + 1]\n        ix = np.ix_(m, factor_ix, factor_ix)\n        Ai = Eaa[ix].sum(axis=0)\n        Bi = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n        if self.idiosyncratic_ar1:\n            ix1 = s.k_states_factors + i\n            ix2 = ix1 + 1\n            Bi -= Eaa[m, ix1:ix2][..., factor_ix].sum(axis=0)\n        try:\n            Lambda[i, factor_ix] = cho_solve(cho_factor(Ai), Bi.T).T\n        except LinAlgError:\n            Lambda[i, factor_ix] = np.linalg.solve(Ai, Bi.T).T\n    if self.k_endog_Q > 0:\n        multipliers = np.array([1, 2, 3, 2, 1])[:, None]\n        for i in range(self.k_endog_M, self.k_endog):\n            iloc = self._s.endog_factor_iloc[i]\n            factor_ix = s['factors_L1_5_ix'][:, iloc].ravel().tolist()\n            (R, _) = self.loading_constraints(i)\n            iQ = i - self.k_endog_M\n            m = mask[:, i]\n            yt = self.endog[m, i:i + 1]\n            ix = np.ix_(m, factor_ix, factor_ix)\n            Ai = Eaa[ix].sum(axis=0)\n            BiQ = yt.T @ a[np.ix_(m, factor_ix)][..., 0]\n            if self.idiosyncratic_ar1:\n                ix = (np.s_[:],) + np.ix_(s['idio_ar_Q_ix'][iQ], factor_ix)\n                Eepsf = Eaa[ix]\n                BiQ -= (multipliers * Eepsf[m].sum(axis=0)).sum(axis=0)\n            try:\n                L_and_lower = cho_factor(Ai)\n                unrestricted = cho_solve(L_and_lower, BiQ.T).T[0]\n                AiiRT = cho_solve(L_and_lower, R.T)\n                L_and_lower = cho_factor(R @ AiiRT)\n                RAiiRTiR = cho_solve(L_and_lower, R)\n                restricted = unrestricted - AiiRT @ RAiiRTiR @ unrestricted\n            except LinAlgError:\n                Aii = np.linalg.inv(Ai)\n                unrestricted = (BiQ @ Aii)[0]\n                RARi = np.linalg.inv(R @ Aii @ R.T)\n                restricted = unrestricted - Aii @ R.T @ RARi @ R @ unrestricted\n            Lambda[i, factor_ix] = restricted\n    if compute_H:\n        Z = self['design'].copy()\n        Z[:, :Lambda.shape[1]] = Lambda\n        y = np.nan_to_num(self.endog)\n        C = y.T @ y\n        W = W[..., None]\n        IW = 1 - W\n        WL = W * Z\n        WLT = WL.transpose(0, 2, 1)\n        BL = y[..., None] @ a.transpose(0, 2, 1) @ WLT\n        A = Eaa\n        BLT = BL.transpose(0, 2, 1)\n        IWT = IW.transpose(0, 2, 1)\n        H = (C + (-BL - BLT + WL @ A @ WLT + IW * self['obs_cov'] * IWT).sum(axis=0)) / self.nobs\n    else:\n        H = np.zeros((self.k_endog, self.k_endog), dtype=dtype) * np.nan\n    return (Lambda, H)"
        ]
    },
    {
        "func_name": "smooth",
        "original": "def smooth(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, **kwargs):\n    \"\"\"\n        Kalman smoothing.\n\n        Parameters\n        ----------\n        params : array_like\n            Array of parameters at which to evaluate the loglikelihood\n            function.\n        transformed : bool, optional\n            Whether or not `params` is already transformed. Default is True.\n        return_ssm : bool,optional\n            Whether or not to return only the state space output or a full\n            results object. Default is to return a full results object.\n        cov_type : str, optional\n            See `MLEResults.fit` for a description of covariance matrix types\n            for results object. Default is None.\n        cov_kwds : dict or None, optional\n            See `MLEResults.get_robustcov_results` for a description required\n            keywords for alternative covariance estimators\n        **kwargs\n            Additional keyword arguments to pass to the Kalman filter. See\n            `KalmanFilter.filter` for more details.\n        \"\"\"\n    return super().smooth(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
        "mutated": [
            "def smooth(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, **kwargs):\n    if False:\n        i = 10\n    '\\n        Kalman smoothing.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is None.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        '\n    return super().smooth(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
            "def smooth(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Kalman smoothing.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is None.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        '\n    return super().smooth(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
            "def smooth(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Kalman smoothing.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is None.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        '\n    return super().smooth(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
            "def smooth(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Kalman smoothing.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is None.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        '\n    return super().smooth(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
            "def smooth(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Kalman smoothing.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is None.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        '\n    return super().smooth(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)"
        ]
    },
    {
        "func_name": "filter",
        "original": "def filter(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, low_memory=False, **kwargs):\n    \"\"\"\n        Kalman filtering.\n\n        Parameters\n        ----------\n        params : array_like\n            Array of parameters at which to evaluate the loglikelihood\n            function.\n        transformed : bool, optional\n            Whether or not `params` is already transformed. Default is True.\n        return_ssm : bool,optional\n            Whether or not to return only the state space output or a full\n            results object. Default is to return a full results object.\n        cov_type : str, optional\n            See `MLEResults.fit` for a description of covariance matrix types\n            for results object. Default is 'none'.\n        cov_kwds : dict or None, optional\n            See `MLEResults.get_robustcov_results` for a description required\n            keywords for alternative covariance estimators\n        low_memory : bool, optional\n            If set to True, techniques are applied to substantially reduce\n            memory usage. If used, some features of the results object will\n            not be available (including in-sample prediction), although\n            out-of-sample forecasting is possible. Default is False.\n        **kwargs\n            Additional keyword arguments to pass to the Kalman filter. See\n            `KalmanFilter.filter` for more details.\n        \"\"\"\n    return super().filter(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
        "mutated": [
            "def filter(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, low_memory=False, **kwargs):\n    if False:\n        i = 10\n    \"\\n        Kalman filtering.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is 'none'.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including in-sample prediction), although\\n            out-of-sample forecasting is possible. Default is False.\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        \"\n    return super().filter(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
            "def filter(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, low_memory=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Kalman filtering.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is 'none'.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including in-sample prediction), although\\n            out-of-sample forecasting is possible. Default is False.\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        \"\n    return super().filter(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
            "def filter(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, low_memory=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Kalman filtering.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is 'none'.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including in-sample prediction), although\\n            out-of-sample forecasting is possible. Default is False.\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        \"\n    return super().filter(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
            "def filter(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, low_memory=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Kalman filtering.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is 'none'.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including in-sample prediction), although\\n            out-of-sample forecasting is possible. Default is False.\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        \"\n    return super().filter(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)",
            "def filter(self, params, transformed=True, includes_fixed=False, complex_step=False, cov_type='none', cov_kwds=None, return_ssm=False, results_class=None, results_wrapper_class=None, low_memory=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Kalman filtering.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters at which to evaluate the loglikelihood\\n            function.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is True.\\n        return_ssm : bool,optional\\n            Whether or not to return only the state space output or a full\\n            results object. Default is to return a full results object.\\n        cov_type : str, optional\\n            See `MLEResults.fit` for a description of covariance matrix types\\n            for results object. Default is 'none'.\\n        cov_kwds : dict or None, optional\\n            See `MLEResults.get_robustcov_results` for a description required\\n            keywords for alternative covariance estimators\\n        low_memory : bool, optional\\n            If set to True, techniques are applied to substantially reduce\\n            memory usage. If used, some features of the results object will\\n            not be available (including in-sample prediction), although\\n            out-of-sample forecasting is possible. Default is False.\\n        **kwargs\\n            Additional keyword arguments to pass to the Kalman filter. See\\n            `KalmanFilter.filter` for more details.\\n        \"\n    return super().filter(params, transformed=transformed, includes_fixed=includes_fixed, complex_step=complex_step, cov_type=cov_type, cov_kwds=cov_kwds, return_ssm=return_ssm, results_class=results_class, results_wrapper_class=results_wrapper_class, **kwargs)"
        ]
    },
    {
        "func_name": "simulate",
        "original": "def simulate(self, params, nsimulations, measurement_shocks=None, state_shocks=None, initial_state=None, anchor=None, repetitions=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    \"\"\"\n        Simulate a new time series following the state space model.\n\n        Parameters\n        ----------\n        params : array_like\n            Array of parameters to use in constructing the state space\n            representation to use when simulating.\n        nsimulations : int\n            The number of observations to simulate. If the model is\n            time-invariant this can be any number. If the model is\n            time-varying, then this number must be less than or equal to the\n            number of observations.\n        measurement_shocks : array_like, optional\n            If specified, these are the shocks to the measurement equation,\n            :math:`\\\\varepsilon_t`. If unspecified, these are automatically\n            generated using a pseudo-random number generator. If specified,\n            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the\n            same as in the state space model.\n        state_shocks : array_like, optional\n            If specified, these are the shocks to the state equation,\n            :math:`\\\\eta_t`. If unspecified, these are automatically\n            generated using a pseudo-random number generator. If specified,\n            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the\n            same as in the state space model.\n        initial_state : array_like, optional\n            If specified, this is the initial state vector to use in\n            simulation, which should be shaped (`k_states` x 1), where\n            `k_states` is the same as in the state space model. If unspecified,\n            but the model has been initialized, then that initialization is\n            used. This must be specified if `anchor` is anything other than\n            \"start\" or 0 (or else you can use the `simulate` method on a\n            results object rather than on the model object).\n        anchor : int, str, or datetime, optional\n            First period for simulation. The simulation will be conditional on\n            all existing datapoints prior to the `anchor`.  Type depends on the\n            index of the given `endog` in the model. Two special cases are the\n            strings 'start' and 'end'. `start` refers to beginning the\n            simulation at the first period of the sample, and `end` refers to\n            beginning the simulation at the first period after the sample.\n            Integer values can run from 0 to `nobs`, or can be negative to\n            apply negative indexing. Finally, if a date/time index was provided\n            to the model, then this argument can be a date string to parse or a\n            datetime type. Default is 'start'.\n        repetitions : int, optional\n            Number of simulated paths to generate. Default is 1 simulated path.\n        exog : array_like, optional\n            New observations of exogenous regressors, if applicable.\n        transformed : bool, optional\n            Whether or not `params` is already transformed. Default is\n            True.\n        includes_fixed : bool, optional\n            If parameters were previously fixed with the `fix_params` method,\n            this argument describes whether or not `params` also includes\n            the fixed parameters, in addition to the free parameters. Default\n            is False.\n        original_scale : bool, optional\n            If the model specification standardized the data, whether or not\n            to return simulations in the original scale of the data (i.e.\n            before it was standardized by the model). Default is True.\n\n        Returns\n        -------\n        simulated_obs : ndarray\n            An array of simulated observations. If `repetitions=None`, then it\n            will be shaped (nsimulations x k_endog) or (nsimulations,) if\n            `k_endog=1`. Otherwise it will be shaped\n            (nsimulations x k_endog x repetitions). If the model was given\n            Pandas input then the output will be a Pandas object. If\n            `k_endog > 1` and `repetitions` is not None, then the output will\n            be a Pandas DataFrame that has a MultiIndex for the columns, with\n            the first level containing the names of the `endog` variables and\n            the second level containing the repetition number.\n        \"\"\"\n    sim = super().simulate(params, nsimulations, measurement_shocks=measurement_shocks, state_shocks=state_shocks, initial_state=initial_state, anchor=anchor, repetitions=repetitions, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = sim.shape\n        if use_pandas:\n            if len(shape) == 1:\n                sim *= self._endog_std.iloc[0]\n                sim += self._endog_mean.iloc[0]\n            elif len(shape) == 2:\n                sim = sim.multiply(self._endog_std, axis=1, level=0).add(self._endog_mean, axis=1, level=0)\n        elif len(shape) == 1:\n            sim = sim * self._endog_std + self._endog_mean\n        elif len(shape) == 2:\n            sim = sim * self._endog_std + self._endog_mean\n        else:\n            std = np.atleast_2d(self._endog_std)[..., None]\n            mean = np.atleast_2d(self._endog_mean)[..., None]\n            sim = sim * std + mean\n    return sim",
        "mutated": [
            "def simulate(self, params, nsimulations, measurement_shocks=None, state_shocks=None, initial_state=None, anchor=None, repetitions=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n    '\\n        Simulate a new time series following the state space model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters to use in constructing the state space\\n            representation to use when simulating.\\n        nsimulations : int\\n            The number of observations to simulate. If the model is\\n            time-invariant this can be any number. If the model is\\n            time-varying, then this number must be less than or equal to the\\n            number of observations.\\n        measurement_shocks : array_like, optional\\n            If specified, these are the shocks to the measurement equation,\\n            :math:`\\\\varepsilon_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the\\n            same as in the state space model.\\n        state_shocks : array_like, optional\\n            If specified, these are the shocks to the state equation,\\n            :math:`\\\\eta_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the\\n            same as in the state space model.\\n        initial_state : array_like, optional\\n            If specified, this is the initial state vector to use in\\n            simulation, which should be shaped (`k_states` x 1), where\\n            `k_states` is the same as in the state space model. If unspecified,\\n            but the model has been initialized, then that initialization is\\n            used. This must be specified if `anchor` is anything other than\\n            \"start\" or 0 (or else you can use the `simulate` method on a\\n            results object rather than on the model object).\\n        anchor : int, str, or datetime, optional\\n            First period for simulation. The simulation will be conditional on\\n            all existing datapoints prior to the `anchor`.  Type depends on the\\n            index of the given `endog` in the model. Two special cases are the\\n            strings \\'start\\' and \\'end\\'. `start` refers to beginning the\\n            simulation at the first period of the sample, and `end` refers to\\n            beginning the simulation at the first period after the sample.\\n            Integer values can run from 0 to `nobs`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        repetitions : int, optional\\n            Number of simulated paths to generate. Default is 1 simulated path.\\n        exog : array_like, optional\\n            New observations of exogenous regressors, if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        simulated_obs : ndarray\\n            An array of simulated observations. If `repetitions=None`, then it\\n            will be shaped (nsimulations x k_endog) or (nsimulations,) if\\n            `k_endog=1`. Otherwise it will be shaped\\n            (nsimulations x k_endog x repetitions). If the model was given\\n            Pandas input then the output will be a Pandas object. If\\n            `k_endog > 1` and `repetitions` is not None, then the output will\\n            be a Pandas DataFrame that has a MultiIndex for the columns, with\\n            the first level containing the names of the `endog` variables and\\n            the second level containing the repetition number.\\n        '\n    sim = super().simulate(params, nsimulations, measurement_shocks=measurement_shocks, state_shocks=state_shocks, initial_state=initial_state, anchor=anchor, repetitions=repetitions, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = sim.shape\n        if use_pandas:\n            if len(shape) == 1:\n                sim *= self._endog_std.iloc[0]\n                sim += self._endog_mean.iloc[0]\n            elif len(shape) == 2:\n                sim = sim.multiply(self._endog_std, axis=1, level=0).add(self._endog_mean, axis=1, level=0)\n        elif len(shape) == 1:\n            sim = sim * self._endog_std + self._endog_mean\n        elif len(shape) == 2:\n            sim = sim * self._endog_std + self._endog_mean\n        else:\n            std = np.atleast_2d(self._endog_std)[..., None]\n            mean = np.atleast_2d(self._endog_mean)[..., None]\n            sim = sim * std + mean\n    return sim",
            "def simulate(self, params, nsimulations, measurement_shocks=None, state_shocks=None, initial_state=None, anchor=None, repetitions=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Simulate a new time series following the state space model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters to use in constructing the state space\\n            representation to use when simulating.\\n        nsimulations : int\\n            The number of observations to simulate. If the model is\\n            time-invariant this can be any number. If the model is\\n            time-varying, then this number must be less than or equal to the\\n            number of observations.\\n        measurement_shocks : array_like, optional\\n            If specified, these are the shocks to the measurement equation,\\n            :math:`\\\\varepsilon_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the\\n            same as in the state space model.\\n        state_shocks : array_like, optional\\n            If specified, these are the shocks to the state equation,\\n            :math:`\\\\eta_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the\\n            same as in the state space model.\\n        initial_state : array_like, optional\\n            If specified, this is the initial state vector to use in\\n            simulation, which should be shaped (`k_states` x 1), where\\n            `k_states` is the same as in the state space model. If unspecified,\\n            but the model has been initialized, then that initialization is\\n            used. This must be specified if `anchor` is anything other than\\n            \"start\" or 0 (or else you can use the `simulate` method on a\\n            results object rather than on the model object).\\n        anchor : int, str, or datetime, optional\\n            First period for simulation. The simulation will be conditional on\\n            all existing datapoints prior to the `anchor`.  Type depends on the\\n            index of the given `endog` in the model. Two special cases are the\\n            strings \\'start\\' and \\'end\\'. `start` refers to beginning the\\n            simulation at the first period of the sample, and `end` refers to\\n            beginning the simulation at the first period after the sample.\\n            Integer values can run from 0 to `nobs`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        repetitions : int, optional\\n            Number of simulated paths to generate. Default is 1 simulated path.\\n        exog : array_like, optional\\n            New observations of exogenous regressors, if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        simulated_obs : ndarray\\n            An array of simulated observations. If `repetitions=None`, then it\\n            will be shaped (nsimulations x k_endog) or (nsimulations,) if\\n            `k_endog=1`. Otherwise it will be shaped\\n            (nsimulations x k_endog x repetitions). If the model was given\\n            Pandas input then the output will be a Pandas object. If\\n            `k_endog > 1` and `repetitions` is not None, then the output will\\n            be a Pandas DataFrame that has a MultiIndex for the columns, with\\n            the first level containing the names of the `endog` variables and\\n            the second level containing the repetition number.\\n        '\n    sim = super().simulate(params, nsimulations, measurement_shocks=measurement_shocks, state_shocks=state_shocks, initial_state=initial_state, anchor=anchor, repetitions=repetitions, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = sim.shape\n        if use_pandas:\n            if len(shape) == 1:\n                sim *= self._endog_std.iloc[0]\n                sim += self._endog_mean.iloc[0]\n            elif len(shape) == 2:\n                sim = sim.multiply(self._endog_std, axis=1, level=0).add(self._endog_mean, axis=1, level=0)\n        elif len(shape) == 1:\n            sim = sim * self._endog_std + self._endog_mean\n        elif len(shape) == 2:\n            sim = sim * self._endog_std + self._endog_mean\n        else:\n            std = np.atleast_2d(self._endog_std)[..., None]\n            mean = np.atleast_2d(self._endog_mean)[..., None]\n            sim = sim * std + mean\n    return sim",
            "def simulate(self, params, nsimulations, measurement_shocks=None, state_shocks=None, initial_state=None, anchor=None, repetitions=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Simulate a new time series following the state space model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters to use in constructing the state space\\n            representation to use when simulating.\\n        nsimulations : int\\n            The number of observations to simulate. If the model is\\n            time-invariant this can be any number. If the model is\\n            time-varying, then this number must be less than or equal to the\\n            number of observations.\\n        measurement_shocks : array_like, optional\\n            If specified, these are the shocks to the measurement equation,\\n            :math:`\\\\varepsilon_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the\\n            same as in the state space model.\\n        state_shocks : array_like, optional\\n            If specified, these are the shocks to the state equation,\\n            :math:`\\\\eta_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the\\n            same as in the state space model.\\n        initial_state : array_like, optional\\n            If specified, this is the initial state vector to use in\\n            simulation, which should be shaped (`k_states` x 1), where\\n            `k_states` is the same as in the state space model. If unspecified,\\n            but the model has been initialized, then that initialization is\\n            used. This must be specified if `anchor` is anything other than\\n            \"start\" or 0 (or else you can use the `simulate` method on a\\n            results object rather than on the model object).\\n        anchor : int, str, or datetime, optional\\n            First period for simulation. The simulation will be conditional on\\n            all existing datapoints prior to the `anchor`.  Type depends on the\\n            index of the given `endog` in the model. Two special cases are the\\n            strings \\'start\\' and \\'end\\'. `start` refers to beginning the\\n            simulation at the first period of the sample, and `end` refers to\\n            beginning the simulation at the first period after the sample.\\n            Integer values can run from 0 to `nobs`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        repetitions : int, optional\\n            Number of simulated paths to generate. Default is 1 simulated path.\\n        exog : array_like, optional\\n            New observations of exogenous regressors, if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        simulated_obs : ndarray\\n            An array of simulated observations. If `repetitions=None`, then it\\n            will be shaped (nsimulations x k_endog) or (nsimulations,) if\\n            `k_endog=1`. Otherwise it will be shaped\\n            (nsimulations x k_endog x repetitions). If the model was given\\n            Pandas input then the output will be a Pandas object. If\\n            `k_endog > 1` and `repetitions` is not None, then the output will\\n            be a Pandas DataFrame that has a MultiIndex for the columns, with\\n            the first level containing the names of the `endog` variables and\\n            the second level containing the repetition number.\\n        '\n    sim = super().simulate(params, nsimulations, measurement_shocks=measurement_shocks, state_shocks=state_shocks, initial_state=initial_state, anchor=anchor, repetitions=repetitions, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = sim.shape\n        if use_pandas:\n            if len(shape) == 1:\n                sim *= self._endog_std.iloc[0]\n                sim += self._endog_mean.iloc[0]\n            elif len(shape) == 2:\n                sim = sim.multiply(self._endog_std, axis=1, level=0).add(self._endog_mean, axis=1, level=0)\n        elif len(shape) == 1:\n            sim = sim * self._endog_std + self._endog_mean\n        elif len(shape) == 2:\n            sim = sim * self._endog_std + self._endog_mean\n        else:\n            std = np.atleast_2d(self._endog_std)[..., None]\n            mean = np.atleast_2d(self._endog_mean)[..., None]\n            sim = sim * std + mean\n    return sim",
            "def simulate(self, params, nsimulations, measurement_shocks=None, state_shocks=None, initial_state=None, anchor=None, repetitions=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Simulate a new time series following the state space model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters to use in constructing the state space\\n            representation to use when simulating.\\n        nsimulations : int\\n            The number of observations to simulate. If the model is\\n            time-invariant this can be any number. If the model is\\n            time-varying, then this number must be less than or equal to the\\n            number of observations.\\n        measurement_shocks : array_like, optional\\n            If specified, these are the shocks to the measurement equation,\\n            :math:`\\\\varepsilon_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the\\n            same as in the state space model.\\n        state_shocks : array_like, optional\\n            If specified, these are the shocks to the state equation,\\n            :math:`\\\\eta_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the\\n            same as in the state space model.\\n        initial_state : array_like, optional\\n            If specified, this is the initial state vector to use in\\n            simulation, which should be shaped (`k_states` x 1), where\\n            `k_states` is the same as in the state space model. If unspecified,\\n            but the model has been initialized, then that initialization is\\n            used. This must be specified if `anchor` is anything other than\\n            \"start\" or 0 (or else you can use the `simulate` method on a\\n            results object rather than on the model object).\\n        anchor : int, str, or datetime, optional\\n            First period for simulation. The simulation will be conditional on\\n            all existing datapoints prior to the `anchor`.  Type depends on the\\n            index of the given `endog` in the model. Two special cases are the\\n            strings \\'start\\' and \\'end\\'. `start` refers to beginning the\\n            simulation at the first period of the sample, and `end` refers to\\n            beginning the simulation at the first period after the sample.\\n            Integer values can run from 0 to `nobs`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        repetitions : int, optional\\n            Number of simulated paths to generate. Default is 1 simulated path.\\n        exog : array_like, optional\\n            New observations of exogenous regressors, if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        simulated_obs : ndarray\\n            An array of simulated observations. If `repetitions=None`, then it\\n            will be shaped (nsimulations x k_endog) or (nsimulations,) if\\n            `k_endog=1`. Otherwise it will be shaped\\n            (nsimulations x k_endog x repetitions). If the model was given\\n            Pandas input then the output will be a Pandas object. If\\n            `k_endog > 1` and `repetitions` is not None, then the output will\\n            be a Pandas DataFrame that has a MultiIndex for the columns, with\\n            the first level containing the names of the `endog` variables and\\n            the second level containing the repetition number.\\n        '\n    sim = super().simulate(params, nsimulations, measurement_shocks=measurement_shocks, state_shocks=state_shocks, initial_state=initial_state, anchor=anchor, repetitions=repetitions, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = sim.shape\n        if use_pandas:\n            if len(shape) == 1:\n                sim *= self._endog_std.iloc[0]\n                sim += self._endog_mean.iloc[0]\n            elif len(shape) == 2:\n                sim = sim.multiply(self._endog_std, axis=1, level=0).add(self._endog_mean, axis=1, level=0)\n        elif len(shape) == 1:\n            sim = sim * self._endog_std + self._endog_mean\n        elif len(shape) == 2:\n            sim = sim * self._endog_std + self._endog_mean\n        else:\n            std = np.atleast_2d(self._endog_std)[..., None]\n            mean = np.atleast_2d(self._endog_mean)[..., None]\n            sim = sim * std + mean\n    return sim",
            "def simulate(self, params, nsimulations, measurement_shocks=None, state_shocks=None, initial_state=None, anchor=None, repetitions=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Simulate a new time series following the state space model.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of parameters to use in constructing the state space\\n            representation to use when simulating.\\n        nsimulations : int\\n            The number of observations to simulate. If the model is\\n            time-invariant this can be any number. If the model is\\n            time-varying, then this number must be less than or equal to the\\n            number of observations.\\n        measurement_shocks : array_like, optional\\n            If specified, these are the shocks to the measurement equation,\\n            :math:`\\\\varepsilon_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_endog`, where `k_endog` is the\\n            same as in the state space model.\\n        state_shocks : array_like, optional\\n            If specified, these are the shocks to the state equation,\\n            :math:`\\\\eta_t`. If unspecified, these are automatically\\n            generated using a pseudo-random number generator. If specified,\\n            must be shaped `nsimulations` x `k_posdef` where `k_posdef` is the\\n            same as in the state space model.\\n        initial_state : array_like, optional\\n            If specified, this is the initial state vector to use in\\n            simulation, which should be shaped (`k_states` x 1), where\\n            `k_states` is the same as in the state space model. If unspecified,\\n            but the model has been initialized, then that initialization is\\n            used. This must be specified if `anchor` is anything other than\\n            \"start\" or 0 (or else you can use the `simulate` method on a\\n            results object rather than on the model object).\\n        anchor : int, str, or datetime, optional\\n            First period for simulation. The simulation will be conditional on\\n            all existing datapoints prior to the `anchor`.  Type depends on the\\n            index of the given `endog` in the model. Two special cases are the\\n            strings \\'start\\' and \\'end\\'. `start` refers to beginning the\\n            simulation at the first period of the sample, and `end` refers to\\n            beginning the simulation at the first period after the sample.\\n            Integer values can run from 0 to `nobs`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        repetitions : int, optional\\n            Number of simulated paths to generate. Default is 1 simulated path.\\n        exog : array_like, optional\\n            New observations of exogenous regressors, if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        simulated_obs : ndarray\\n            An array of simulated observations. If `repetitions=None`, then it\\n            will be shaped (nsimulations x k_endog) or (nsimulations,) if\\n            `k_endog=1`. Otherwise it will be shaped\\n            (nsimulations x k_endog x repetitions). If the model was given\\n            Pandas input then the output will be a Pandas object. If\\n            `k_endog > 1` and `repetitions` is not None, then the output will\\n            be a Pandas DataFrame that has a MultiIndex for the columns, with\\n            the first level containing the names of the `endog` variables and\\n            the second level containing the repetition number.\\n        '\n    sim = super().simulate(params, nsimulations, measurement_shocks=measurement_shocks, state_shocks=state_shocks, initial_state=initial_state, anchor=anchor, repetitions=repetitions, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = sim.shape\n        if use_pandas:\n            if len(shape) == 1:\n                sim *= self._endog_std.iloc[0]\n                sim += self._endog_mean.iloc[0]\n            elif len(shape) == 2:\n                sim = sim.multiply(self._endog_std, axis=1, level=0).add(self._endog_mean, axis=1, level=0)\n        elif len(shape) == 1:\n            sim = sim * self._endog_std + self._endog_mean\n        elif len(shape) == 2:\n            sim = sim * self._endog_std + self._endog_mean\n        else:\n            std = np.atleast_2d(self._endog_std)[..., None]\n            mean = np.atleast_2d(self._endog_mean)[..., None]\n            sim = sim * std + mean\n    return sim"
        ]
    },
    {
        "func_name": "impulse_responses",
        "original": "def impulse_responses(self, params, steps=1, impulse=0, orthogonalized=False, cumulative=False, anchor=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    \"\"\"\n        Impulse response function.\n\n        Parameters\n        ----------\n        params : array_like\n            Array of model parameters.\n        steps : int, optional\n            The number of steps for which impulse responses are calculated.\n            Default is 1. Note that for time-invariant models, the initial\n            impulse is not counted as a step, so if `steps=1`, the output will\n            have 2 entries.\n        impulse : int or array_like\n            If an integer, the state innovation to pulse; must be between 0\n            and `k_posdef-1`. Alternatively, a custom impulse vector may be\n            provided; must be shaped `k_posdef x 1`.\n        orthogonalized : bool, optional\n            Whether or not to perform impulse using orthogonalized innovations.\n            Note that this will also affect custum `impulse` vectors. Default\n            is False.\n        cumulative : bool, optional\n            Whether or not to return cumulative impulse responses. Default is\n            False.\n        anchor : int, str, or datetime, optional\n            Time point within the sample for the state innovation impulse. Type\n            depends on the index of the given `endog` in the model. Two special\n            cases are the strings 'start' and 'end', which refer to setting the\n            impulse at the first and last points of the sample, respectively.\n            Integer values can run from 0 to `nobs - 1`, or can be negative to\n            apply negative indexing. Finally, if a date/time index was provided\n            to the model, then this argument can be a date string to parse or a\n            datetime type. Default is 'start'.\n        exog : array_like, optional\n            New observations of exogenous regressors for our-of-sample periods,\n            if applicable.\n        transformed : bool, optional\n            Whether or not `params` is already transformed. Default is\n            True.\n        includes_fixed : bool, optional\n            If parameters were previously fixed with the `fix_params` method,\n            this argument describes whether or not `params` also includes\n            the fixed parameters, in addition to the free parameters. Default\n            is False.\n        original_scale : bool, optional\n            If the model specification standardized the data, whether or not\n            to return impulse responses in the original scale of the data (i.e.\n            before it was standardized by the model). Default is True.\n        **kwargs\n            If the model has time-varying design or transition matrices and the\n            combination of `anchor` and `steps` implies creating impulse\n            responses for the out-of-sample period, then these matrices must\n            have updated values provided for the out-of-sample steps. For\n            example, if `design` is a time-varying component, `nobs` is 10,\n            `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7)\n            matrix must be provided with the new design matrix values.\n\n        Returns\n        -------\n        impulse_responses : ndarray\n            Responses for each endogenous variable due to the impulse\n            given by the `impulse` argument. For a time-invariant model, the\n            impulse responses are given for `steps + 1` elements (this gives\n            the \"initial impulse\" followed by `steps` responses for the\n            important cases of VAR and SARIMAX models), while for time-varying\n            models the impulse responses are only given for `steps` elements\n            (to avoid having to unexpectedly provide updated time-varying\n            matrices).\n\n        \"\"\"\n    irfs = super().impulse_responses(params, steps=steps, impulse=impulse, orthogonalized=orthogonalized, cumulative=cumulative, anchor=anchor, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = irfs.shape\n        if use_pandas:\n            if len(shape) == 1:\n                irfs = irfs * self._endog_std.iloc[0]\n            elif len(shape) == 2:\n                irfs = irfs.multiply(self._endog_std, axis=1, level=0)\n        elif len(shape) == 1:\n            irfs = irfs * self._endog_std\n        elif len(shape) == 2:\n            irfs = irfs * self._endog_std\n    return irfs",
        "mutated": [
            "def impulse_responses(self, params, steps=1, impulse=0, orthogonalized=False, cumulative=False, anchor=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n    '\\n        Impulse response function.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of model parameters.\\n        steps : int, optional\\n            The number of steps for which impulse responses are calculated.\\n            Default is 1. Note that for time-invariant models, the initial\\n            impulse is not counted as a step, so if `steps=1`, the output will\\n            have 2 entries.\\n        impulse : int or array_like\\n            If an integer, the state innovation to pulse; must be between 0\\n            and `k_posdef-1`. Alternatively, a custom impulse vector may be\\n            provided; must be shaped `k_posdef x 1`.\\n        orthogonalized : bool, optional\\n            Whether or not to perform impulse using orthogonalized innovations.\\n            Note that this will also affect custum `impulse` vectors. Default\\n            is False.\\n        cumulative : bool, optional\\n            Whether or not to return cumulative impulse responses. Default is\\n            False.\\n        anchor : int, str, or datetime, optional\\n            Time point within the sample for the state innovation impulse. Type\\n            depends on the index of the given `endog` in the model. Two special\\n            cases are the strings \\'start\\' and \\'end\\', which refer to setting the\\n            impulse at the first and last points of the sample, respectively.\\n            Integer values can run from 0 to `nobs - 1`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        exog : array_like, optional\\n            New observations of exogenous regressors for our-of-sample periods,\\n            if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return impulse responses in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            If the model has time-varying design or transition matrices and the\\n            combination of `anchor` and `steps` implies creating impulse\\n            responses for the out-of-sample period, then these matrices must\\n            have updated values provided for the out-of-sample steps. For\\n            example, if `design` is a time-varying component, `nobs` is 10,\\n            `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7)\\n            matrix must be provided with the new design matrix values.\\n\\n        Returns\\n        -------\\n        impulse_responses : ndarray\\n            Responses for each endogenous variable due to the impulse\\n            given by the `impulse` argument. For a time-invariant model, the\\n            impulse responses are given for `steps + 1` elements (this gives\\n            the \"initial impulse\" followed by `steps` responses for the\\n            important cases of VAR and SARIMAX models), while for time-varying\\n            models the impulse responses are only given for `steps` elements\\n            (to avoid having to unexpectedly provide updated time-varying\\n            matrices).\\n\\n        '\n    irfs = super().impulse_responses(params, steps=steps, impulse=impulse, orthogonalized=orthogonalized, cumulative=cumulative, anchor=anchor, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = irfs.shape\n        if use_pandas:\n            if len(shape) == 1:\n                irfs = irfs * self._endog_std.iloc[0]\n            elif len(shape) == 2:\n                irfs = irfs.multiply(self._endog_std, axis=1, level=0)\n        elif len(shape) == 1:\n            irfs = irfs * self._endog_std\n        elif len(shape) == 2:\n            irfs = irfs * self._endog_std\n    return irfs",
            "def impulse_responses(self, params, steps=1, impulse=0, orthogonalized=False, cumulative=False, anchor=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Impulse response function.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of model parameters.\\n        steps : int, optional\\n            The number of steps for which impulse responses are calculated.\\n            Default is 1. Note that for time-invariant models, the initial\\n            impulse is not counted as a step, so if `steps=1`, the output will\\n            have 2 entries.\\n        impulse : int or array_like\\n            If an integer, the state innovation to pulse; must be between 0\\n            and `k_posdef-1`. Alternatively, a custom impulse vector may be\\n            provided; must be shaped `k_posdef x 1`.\\n        orthogonalized : bool, optional\\n            Whether or not to perform impulse using orthogonalized innovations.\\n            Note that this will also affect custum `impulse` vectors. Default\\n            is False.\\n        cumulative : bool, optional\\n            Whether or not to return cumulative impulse responses. Default is\\n            False.\\n        anchor : int, str, or datetime, optional\\n            Time point within the sample for the state innovation impulse. Type\\n            depends on the index of the given `endog` in the model. Two special\\n            cases are the strings \\'start\\' and \\'end\\', which refer to setting the\\n            impulse at the first and last points of the sample, respectively.\\n            Integer values can run from 0 to `nobs - 1`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        exog : array_like, optional\\n            New observations of exogenous regressors for our-of-sample periods,\\n            if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return impulse responses in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            If the model has time-varying design or transition matrices and the\\n            combination of `anchor` and `steps` implies creating impulse\\n            responses for the out-of-sample period, then these matrices must\\n            have updated values provided for the out-of-sample steps. For\\n            example, if `design` is a time-varying component, `nobs` is 10,\\n            `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7)\\n            matrix must be provided with the new design matrix values.\\n\\n        Returns\\n        -------\\n        impulse_responses : ndarray\\n            Responses for each endogenous variable due to the impulse\\n            given by the `impulse` argument. For a time-invariant model, the\\n            impulse responses are given for `steps + 1` elements (this gives\\n            the \"initial impulse\" followed by `steps` responses for the\\n            important cases of VAR and SARIMAX models), while for time-varying\\n            models the impulse responses are only given for `steps` elements\\n            (to avoid having to unexpectedly provide updated time-varying\\n            matrices).\\n\\n        '\n    irfs = super().impulse_responses(params, steps=steps, impulse=impulse, orthogonalized=orthogonalized, cumulative=cumulative, anchor=anchor, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = irfs.shape\n        if use_pandas:\n            if len(shape) == 1:\n                irfs = irfs * self._endog_std.iloc[0]\n            elif len(shape) == 2:\n                irfs = irfs.multiply(self._endog_std, axis=1, level=0)\n        elif len(shape) == 1:\n            irfs = irfs * self._endog_std\n        elif len(shape) == 2:\n            irfs = irfs * self._endog_std\n    return irfs",
            "def impulse_responses(self, params, steps=1, impulse=0, orthogonalized=False, cumulative=False, anchor=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Impulse response function.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of model parameters.\\n        steps : int, optional\\n            The number of steps for which impulse responses are calculated.\\n            Default is 1. Note that for time-invariant models, the initial\\n            impulse is not counted as a step, so if `steps=1`, the output will\\n            have 2 entries.\\n        impulse : int or array_like\\n            If an integer, the state innovation to pulse; must be between 0\\n            and `k_posdef-1`. Alternatively, a custom impulse vector may be\\n            provided; must be shaped `k_posdef x 1`.\\n        orthogonalized : bool, optional\\n            Whether or not to perform impulse using orthogonalized innovations.\\n            Note that this will also affect custum `impulse` vectors. Default\\n            is False.\\n        cumulative : bool, optional\\n            Whether or not to return cumulative impulse responses. Default is\\n            False.\\n        anchor : int, str, or datetime, optional\\n            Time point within the sample for the state innovation impulse. Type\\n            depends on the index of the given `endog` in the model. Two special\\n            cases are the strings \\'start\\' and \\'end\\', which refer to setting the\\n            impulse at the first and last points of the sample, respectively.\\n            Integer values can run from 0 to `nobs - 1`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        exog : array_like, optional\\n            New observations of exogenous regressors for our-of-sample periods,\\n            if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return impulse responses in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            If the model has time-varying design or transition matrices and the\\n            combination of `anchor` and `steps` implies creating impulse\\n            responses for the out-of-sample period, then these matrices must\\n            have updated values provided for the out-of-sample steps. For\\n            example, if `design` is a time-varying component, `nobs` is 10,\\n            `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7)\\n            matrix must be provided with the new design matrix values.\\n\\n        Returns\\n        -------\\n        impulse_responses : ndarray\\n            Responses for each endogenous variable due to the impulse\\n            given by the `impulse` argument. For a time-invariant model, the\\n            impulse responses are given for `steps + 1` elements (this gives\\n            the \"initial impulse\" followed by `steps` responses for the\\n            important cases of VAR and SARIMAX models), while for time-varying\\n            models the impulse responses are only given for `steps` elements\\n            (to avoid having to unexpectedly provide updated time-varying\\n            matrices).\\n\\n        '\n    irfs = super().impulse_responses(params, steps=steps, impulse=impulse, orthogonalized=orthogonalized, cumulative=cumulative, anchor=anchor, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = irfs.shape\n        if use_pandas:\n            if len(shape) == 1:\n                irfs = irfs * self._endog_std.iloc[0]\n            elif len(shape) == 2:\n                irfs = irfs.multiply(self._endog_std, axis=1, level=0)\n        elif len(shape) == 1:\n            irfs = irfs * self._endog_std\n        elif len(shape) == 2:\n            irfs = irfs * self._endog_std\n    return irfs",
            "def impulse_responses(self, params, steps=1, impulse=0, orthogonalized=False, cumulative=False, anchor=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Impulse response function.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of model parameters.\\n        steps : int, optional\\n            The number of steps for which impulse responses are calculated.\\n            Default is 1. Note that for time-invariant models, the initial\\n            impulse is not counted as a step, so if `steps=1`, the output will\\n            have 2 entries.\\n        impulse : int or array_like\\n            If an integer, the state innovation to pulse; must be between 0\\n            and `k_posdef-1`. Alternatively, a custom impulse vector may be\\n            provided; must be shaped `k_posdef x 1`.\\n        orthogonalized : bool, optional\\n            Whether or not to perform impulse using orthogonalized innovations.\\n            Note that this will also affect custum `impulse` vectors. Default\\n            is False.\\n        cumulative : bool, optional\\n            Whether or not to return cumulative impulse responses. Default is\\n            False.\\n        anchor : int, str, or datetime, optional\\n            Time point within the sample for the state innovation impulse. Type\\n            depends on the index of the given `endog` in the model. Two special\\n            cases are the strings \\'start\\' and \\'end\\', which refer to setting the\\n            impulse at the first and last points of the sample, respectively.\\n            Integer values can run from 0 to `nobs - 1`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        exog : array_like, optional\\n            New observations of exogenous regressors for our-of-sample periods,\\n            if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return impulse responses in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            If the model has time-varying design or transition matrices and the\\n            combination of `anchor` and `steps` implies creating impulse\\n            responses for the out-of-sample period, then these matrices must\\n            have updated values provided for the out-of-sample steps. For\\n            example, if `design` is a time-varying component, `nobs` is 10,\\n            `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7)\\n            matrix must be provided with the new design matrix values.\\n\\n        Returns\\n        -------\\n        impulse_responses : ndarray\\n            Responses for each endogenous variable due to the impulse\\n            given by the `impulse` argument. For a time-invariant model, the\\n            impulse responses are given for `steps + 1` elements (this gives\\n            the \"initial impulse\" followed by `steps` responses for the\\n            important cases of VAR and SARIMAX models), while for time-varying\\n            models the impulse responses are only given for `steps` elements\\n            (to avoid having to unexpectedly provide updated time-varying\\n            matrices).\\n\\n        '\n    irfs = super().impulse_responses(params, steps=steps, impulse=impulse, orthogonalized=orthogonalized, cumulative=cumulative, anchor=anchor, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = irfs.shape\n        if use_pandas:\n            if len(shape) == 1:\n                irfs = irfs * self._endog_std.iloc[0]\n            elif len(shape) == 2:\n                irfs = irfs.multiply(self._endog_std, axis=1, level=0)\n        elif len(shape) == 1:\n            irfs = irfs * self._endog_std\n        elif len(shape) == 2:\n            irfs = irfs * self._endog_std\n    return irfs",
            "def impulse_responses(self, params, steps=1, impulse=0, orthogonalized=False, cumulative=False, anchor=None, exog=None, extend_model=None, extend_kwargs=None, transformed=True, includes_fixed=False, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Impulse response function.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Array of model parameters.\\n        steps : int, optional\\n            The number of steps for which impulse responses are calculated.\\n            Default is 1. Note that for time-invariant models, the initial\\n            impulse is not counted as a step, so if `steps=1`, the output will\\n            have 2 entries.\\n        impulse : int or array_like\\n            If an integer, the state innovation to pulse; must be between 0\\n            and `k_posdef-1`. Alternatively, a custom impulse vector may be\\n            provided; must be shaped `k_posdef x 1`.\\n        orthogonalized : bool, optional\\n            Whether or not to perform impulse using orthogonalized innovations.\\n            Note that this will also affect custum `impulse` vectors. Default\\n            is False.\\n        cumulative : bool, optional\\n            Whether or not to return cumulative impulse responses. Default is\\n            False.\\n        anchor : int, str, or datetime, optional\\n            Time point within the sample for the state innovation impulse. Type\\n            depends on the index of the given `endog` in the model. Two special\\n            cases are the strings \\'start\\' and \\'end\\', which refer to setting the\\n            impulse at the first and last points of the sample, respectively.\\n            Integer values can run from 0 to `nobs - 1`, or can be negative to\\n            apply negative indexing. Finally, if a date/time index was provided\\n            to the model, then this argument can be a date string to parse or a\\n            datetime type. Default is \\'start\\'.\\n        exog : array_like, optional\\n            New observations of exogenous regressors for our-of-sample periods,\\n            if applicable.\\n        transformed : bool, optional\\n            Whether or not `params` is already transformed. Default is\\n            True.\\n        includes_fixed : bool, optional\\n            If parameters were previously fixed with the `fix_params` method,\\n            this argument describes whether or not `params` also includes\\n            the fixed parameters, in addition to the free parameters. Default\\n            is False.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return impulse responses in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            If the model has time-varying design or transition matrices and the\\n            combination of `anchor` and `steps` implies creating impulse\\n            responses for the out-of-sample period, then these matrices must\\n            have updated values provided for the out-of-sample steps. For\\n            example, if `design` is a time-varying component, `nobs` is 10,\\n            `anchor=1`, and `steps` is 15, a (`k_endog` x `k_states` x 7)\\n            matrix must be provided with the new design matrix values.\\n\\n        Returns\\n        -------\\n        impulse_responses : ndarray\\n            Responses for each endogenous variable due to the impulse\\n            given by the `impulse` argument. For a time-invariant model, the\\n            impulse responses are given for `steps + 1` elements (this gives\\n            the \"initial impulse\" followed by `steps` responses for the\\n            important cases of VAR and SARIMAX models), while for time-varying\\n            models the impulse responses are only given for `steps` elements\\n            (to avoid having to unexpectedly provide updated time-varying\\n            matrices).\\n\\n        '\n    irfs = super().impulse_responses(params, steps=steps, impulse=impulse, orthogonalized=orthogonalized, cumulative=cumulative, anchor=anchor, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, transformed=transformed, includes_fixed=includes_fixed, **kwargs)\n    if self.standardize and original_scale:\n        use_pandas = isinstance(self.data, PandasData)\n        shape = irfs.shape\n        if use_pandas:\n            if len(shape) == 1:\n                irfs = irfs * self._endog_std.iloc[0]\n            elif len(shape) == 2:\n                irfs = irfs.multiply(self._endog_std, axis=1, level=0)\n        elif len(shape) == 1:\n            irfs = irfs * self._endog_std\n        elif len(shape) == 2:\n            irfs = irfs * self._endog_std\n    return irfs"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model, params, filter_results, cov_type=None, **kwargs):\n    super(DynamicFactorMQResults, self).__init__(model, params, filter_results, cov_type, **kwargs)",
        "mutated": [
            "def __init__(self, model, params, filter_results, cov_type=None, **kwargs):\n    if False:\n        i = 10\n    super(DynamicFactorMQResults, self).__init__(model, params, filter_results, cov_type, **kwargs)",
            "def __init__(self, model, params, filter_results, cov_type=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(DynamicFactorMQResults, self).__init__(model, params, filter_results, cov_type, **kwargs)",
            "def __init__(self, model, params, filter_results, cov_type=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(DynamicFactorMQResults, self).__init__(model, params, filter_results, cov_type, **kwargs)",
            "def __init__(self, model, params, filter_results, cov_type=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(DynamicFactorMQResults, self).__init__(model, params, filter_results, cov_type, **kwargs)",
            "def __init__(self, model, params, filter_results, cov_type=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(DynamicFactorMQResults, self).__init__(model, params, filter_results, cov_type, **kwargs)"
        ]
    },
    {
        "func_name": "factors",
        "original": "@property\ndef factors(self):\n    \"\"\"\n        Estimates of unobserved factors.\n\n        Returns\n        -------\n        out : Bunch\n            Has the following attributes shown in Notes.\n\n        Notes\n        -----\n        The output is a bunch of the following format:\n\n        - `filtered`: a time series array with the filtered estimate of\n          the component\n        - `filtered_cov`: a time series array with the filtered estimate of\n          the variance/covariance of the component\n        - `smoothed`: a time series array with the smoothed estimate of\n          the component\n        - `smoothed_cov`: a time series array with the smoothed estimate of\n          the variance/covariance of the component\n        - `offset`: an integer giving the offset in the state vector where\n          this component begins\n        \"\"\"\n    out = None\n    if self.model.k_factors > 0:\n        iloc = self.model._s.factors_L1\n        ix = np.array(self.model.state_names)[iloc].tolist()\n        out = Bunch(filtered=self.states.filtered.loc[:, ix], filtered_cov=self.states.filtered_cov.loc[np.s_[ix, :], ix], smoothed=None, smoothed_cov=None)\n        if self.smoothed_state is not None:\n            out.smoothed = self.states.smoothed.loc[:, ix]\n        if self.smoothed_state_cov is not None:\n            out.smoothed_cov = self.states.smoothed_cov.loc[np.s_[ix, :], ix]\n    return out",
        "mutated": [
            "@property\ndef factors(self):\n    if False:\n        i = 10\n    '\\n        Estimates of unobserved factors.\\n\\n        Returns\\n        -------\\n        out : Bunch\\n            Has the following attributes shown in Notes.\\n\\n        Notes\\n        -----\\n        The output is a bunch of the following format:\\n\\n        - `filtered`: a time series array with the filtered estimate of\\n          the component\\n        - `filtered_cov`: a time series array with the filtered estimate of\\n          the variance/covariance of the component\\n        - `smoothed`: a time series array with the smoothed estimate of\\n          the component\\n        - `smoothed_cov`: a time series array with the smoothed estimate of\\n          the variance/covariance of the component\\n        - `offset`: an integer giving the offset in the state vector where\\n          this component begins\\n        '\n    out = None\n    if self.model.k_factors > 0:\n        iloc = self.model._s.factors_L1\n        ix = np.array(self.model.state_names)[iloc].tolist()\n        out = Bunch(filtered=self.states.filtered.loc[:, ix], filtered_cov=self.states.filtered_cov.loc[np.s_[ix, :], ix], smoothed=None, smoothed_cov=None)\n        if self.smoothed_state is not None:\n            out.smoothed = self.states.smoothed.loc[:, ix]\n        if self.smoothed_state_cov is not None:\n            out.smoothed_cov = self.states.smoothed_cov.loc[np.s_[ix, :], ix]\n    return out",
            "@property\ndef factors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Estimates of unobserved factors.\\n\\n        Returns\\n        -------\\n        out : Bunch\\n            Has the following attributes shown in Notes.\\n\\n        Notes\\n        -----\\n        The output is a bunch of the following format:\\n\\n        - `filtered`: a time series array with the filtered estimate of\\n          the component\\n        - `filtered_cov`: a time series array with the filtered estimate of\\n          the variance/covariance of the component\\n        - `smoothed`: a time series array with the smoothed estimate of\\n          the component\\n        - `smoothed_cov`: a time series array with the smoothed estimate of\\n          the variance/covariance of the component\\n        - `offset`: an integer giving the offset in the state vector where\\n          this component begins\\n        '\n    out = None\n    if self.model.k_factors > 0:\n        iloc = self.model._s.factors_L1\n        ix = np.array(self.model.state_names)[iloc].tolist()\n        out = Bunch(filtered=self.states.filtered.loc[:, ix], filtered_cov=self.states.filtered_cov.loc[np.s_[ix, :], ix], smoothed=None, smoothed_cov=None)\n        if self.smoothed_state is not None:\n            out.smoothed = self.states.smoothed.loc[:, ix]\n        if self.smoothed_state_cov is not None:\n            out.smoothed_cov = self.states.smoothed_cov.loc[np.s_[ix, :], ix]\n    return out",
            "@property\ndef factors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Estimates of unobserved factors.\\n\\n        Returns\\n        -------\\n        out : Bunch\\n            Has the following attributes shown in Notes.\\n\\n        Notes\\n        -----\\n        The output is a bunch of the following format:\\n\\n        - `filtered`: a time series array with the filtered estimate of\\n          the component\\n        - `filtered_cov`: a time series array with the filtered estimate of\\n          the variance/covariance of the component\\n        - `smoothed`: a time series array with the smoothed estimate of\\n          the component\\n        - `smoothed_cov`: a time series array with the smoothed estimate of\\n          the variance/covariance of the component\\n        - `offset`: an integer giving the offset in the state vector where\\n          this component begins\\n        '\n    out = None\n    if self.model.k_factors > 0:\n        iloc = self.model._s.factors_L1\n        ix = np.array(self.model.state_names)[iloc].tolist()\n        out = Bunch(filtered=self.states.filtered.loc[:, ix], filtered_cov=self.states.filtered_cov.loc[np.s_[ix, :], ix], smoothed=None, smoothed_cov=None)\n        if self.smoothed_state is not None:\n            out.smoothed = self.states.smoothed.loc[:, ix]\n        if self.smoothed_state_cov is not None:\n            out.smoothed_cov = self.states.smoothed_cov.loc[np.s_[ix, :], ix]\n    return out",
            "@property\ndef factors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Estimates of unobserved factors.\\n\\n        Returns\\n        -------\\n        out : Bunch\\n            Has the following attributes shown in Notes.\\n\\n        Notes\\n        -----\\n        The output is a bunch of the following format:\\n\\n        - `filtered`: a time series array with the filtered estimate of\\n          the component\\n        - `filtered_cov`: a time series array with the filtered estimate of\\n          the variance/covariance of the component\\n        - `smoothed`: a time series array with the smoothed estimate of\\n          the component\\n        - `smoothed_cov`: a time series array with the smoothed estimate of\\n          the variance/covariance of the component\\n        - `offset`: an integer giving the offset in the state vector where\\n          this component begins\\n        '\n    out = None\n    if self.model.k_factors > 0:\n        iloc = self.model._s.factors_L1\n        ix = np.array(self.model.state_names)[iloc].tolist()\n        out = Bunch(filtered=self.states.filtered.loc[:, ix], filtered_cov=self.states.filtered_cov.loc[np.s_[ix, :], ix], smoothed=None, smoothed_cov=None)\n        if self.smoothed_state is not None:\n            out.smoothed = self.states.smoothed.loc[:, ix]\n        if self.smoothed_state_cov is not None:\n            out.smoothed_cov = self.states.smoothed_cov.loc[np.s_[ix, :], ix]\n    return out",
            "@property\ndef factors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Estimates of unobserved factors.\\n\\n        Returns\\n        -------\\n        out : Bunch\\n            Has the following attributes shown in Notes.\\n\\n        Notes\\n        -----\\n        The output is a bunch of the following format:\\n\\n        - `filtered`: a time series array with the filtered estimate of\\n          the component\\n        - `filtered_cov`: a time series array with the filtered estimate of\\n          the variance/covariance of the component\\n        - `smoothed`: a time series array with the smoothed estimate of\\n          the component\\n        - `smoothed_cov`: a time series array with the smoothed estimate of\\n          the variance/covariance of the component\\n        - `offset`: an integer giving the offset in the state vector where\\n          this component begins\\n        '\n    out = None\n    if self.model.k_factors > 0:\n        iloc = self.model._s.factors_L1\n        ix = np.array(self.model.state_names)[iloc].tolist()\n        out = Bunch(filtered=self.states.filtered.loc[:, ix], filtered_cov=self.states.filtered_cov.loc[np.s_[ix, :], ix], smoothed=None, smoothed_cov=None)\n        if self.smoothed_state is not None:\n            out.smoothed = self.states.smoothed.loc[:, ix]\n        if self.smoothed_state_cov is not None:\n            out.smoothed_cov = self.states.smoothed_cov.loc[np.s_[ix, :], ix]\n    return out"
        ]
    },
    {
        "func_name": "get_coefficients_of_determination",
        "original": "def get_coefficients_of_determination(self, method='individual', which=None):\n    \"\"\"\n        Get coefficients of determination (R-squared) for variables / factors.\n\n        Parameters\n        ----------\n        method : {'individual', 'joint', 'cumulative'}, optional\n            The type of R-squared values to generate. \"individual\" plots\n            the R-squared of each variable on each factor; \"joint\" plots the\n            R-squared of each variable on each factor that it loads on;\n            \"cumulative\" plots the successive R-squared values as each\n            additional factor is added to the regression, for each variable.\n            Default is 'individual'.\n        which: {None, 'filtered', 'smoothed'}, optional\n            Whether to compute R-squared values based on filtered or smoothed\n            estimates of the factors. Default is 'smoothed' if smoothed results\n            are available and 'filtered' otherwise.\n\n        Returns\n        -------\n        rsquared : pd.DataFrame or pd.Series\n            The R-squared values from regressions of observed variables on\n            one or more of the factors. If method='individual' or\n            method='cumulative', this will be a Pandas DataFrame with observed\n            variables as the index and factors as the columns . If\n            method='joint', will be a Pandas Series with observed variables as\n            the index.\n\n        See Also\n        --------\n        plot_coefficients_of_determination\n        coefficients_of_determination\n        \"\"\"\n    from statsmodels.tools import add_constant\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if which is None:\n        which = 'filtered' if self.smoothed_state is None else 'smoothed'\n    k_endog = self.model.k_endog\n    k_factors = self.model.k_factors\n    ef_map = self.model._s.endog_factor_map\n    endog_names = self.model.endog_names\n    factor_names = self.model.factor_names\n    if method == 'individual':\n        coefficients = np.zeros((k_endog, k_factors))\n        for i in range(k_factors):\n            exog = add_constant(self.factors[which].iloc[:, i])\n            for j in range(k_endog):\n                if ef_map.iloc[j, i]:\n                    endog = self.filter_results.endog[j]\n                    coefficients[j, i] = OLS(endog, exog, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    elif method == 'joint':\n        coefficients = np.zeros((k_endog,))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            ix = np.r_[True, ef_map.iloc[j]].tolist()\n            X = exog.loc[:, ix]\n            coefficients[j] = OLS(endog, X, missing='drop').fit().rsquared\n        coefficients = pd.Series(coefficients, index=endog_names)\n    elif method == 'cumulative':\n        coefficients = np.zeros((k_endog, k_factors))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            for i in range(k_factors):\n                if self.model._s.endog_factor_map.iloc[j, i]:\n                    ix = np.r_[True, ef_map.iloc[j, :i + 1], [False] * (k_factors - i - 1)]\n                    X = exog.loc[:, ix.astype(bool).tolist()]\n                    coefficients[j, i] = OLS(endog, X, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    return coefficients",
        "mutated": [
            "def get_coefficients_of_determination(self, method='individual', which=None):\n    if False:\n        i = 10\n    '\\n        Get coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n\\n        Returns\\n        -------\\n        rsquared : pd.DataFrame or pd.Series\\n            The R-squared values from regressions of observed variables on\\n            one or more of the factors. If method=\\'individual\\' or\\n            method=\\'cumulative\\', this will be a Pandas DataFrame with observed\\n            variables as the index and factors as the columns . If\\n            method=\\'joint\\', will be a Pandas Series with observed variables as\\n            the index.\\n\\n        See Also\\n        --------\\n        plot_coefficients_of_determination\\n        coefficients_of_determination\\n        '\n    from statsmodels.tools import add_constant\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if which is None:\n        which = 'filtered' if self.smoothed_state is None else 'smoothed'\n    k_endog = self.model.k_endog\n    k_factors = self.model.k_factors\n    ef_map = self.model._s.endog_factor_map\n    endog_names = self.model.endog_names\n    factor_names = self.model.factor_names\n    if method == 'individual':\n        coefficients = np.zeros((k_endog, k_factors))\n        for i in range(k_factors):\n            exog = add_constant(self.factors[which].iloc[:, i])\n            for j in range(k_endog):\n                if ef_map.iloc[j, i]:\n                    endog = self.filter_results.endog[j]\n                    coefficients[j, i] = OLS(endog, exog, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    elif method == 'joint':\n        coefficients = np.zeros((k_endog,))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            ix = np.r_[True, ef_map.iloc[j]].tolist()\n            X = exog.loc[:, ix]\n            coefficients[j] = OLS(endog, X, missing='drop').fit().rsquared\n        coefficients = pd.Series(coefficients, index=endog_names)\n    elif method == 'cumulative':\n        coefficients = np.zeros((k_endog, k_factors))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            for i in range(k_factors):\n                if self.model._s.endog_factor_map.iloc[j, i]:\n                    ix = np.r_[True, ef_map.iloc[j, :i + 1], [False] * (k_factors - i - 1)]\n                    X = exog.loc[:, ix.astype(bool).tolist()]\n                    coefficients[j, i] = OLS(endog, X, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    return coefficients",
            "def get_coefficients_of_determination(self, method='individual', which=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Get coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n\\n        Returns\\n        -------\\n        rsquared : pd.DataFrame or pd.Series\\n            The R-squared values from regressions of observed variables on\\n            one or more of the factors. If method=\\'individual\\' or\\n            method=\\'cumulative\\', this will be a Pandas DataFrame with observed\\n            variables as the index and factors as the columns . If\\n            method=\\'joint\\', will be a Pandas Series with observed variables as\\n            the index.\\n\\n        See Also\\n        --------\\n        plot_coefficients_of_determination\\n        coefficients_of_determination\\n        '\n    from statsmodels.tools import add_constant\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if which is None:\n        which = 'filtered' if self.smoothed_state is None else 'smoothed'\n    k_endog = self.model.k_endog\n    k_factors = self.model.k_factors\n    ef_map = self.model._s.endog_factor_map\n    endog_names = self.model.endog_names\n    factor_names = self.model.factor_names\n    if method == 'individual':\n        coefficients = np.zeros((k_endog, k_factors))\n        for i in range(k_factors):\n            exog = add_constant(self.factors[which].iloc[:, i])\n            for j in range(k_endog):\n                if ef_map.iloc[j, i]:\n                    endog = self.filter_results.endog[j]\n                    coefficients[j, i] = OLS(endog, exog, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    elif method == 'joint':\n        coefficients = np.zeros((k_endog,))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            ix = np.r_[True, ef_map.iloc[j]].tolist()\n            X = exog.loc[:, ix]\n            coefficients[j] = OLS(endog, X, missing='drop').fit().rsquared\n        coefficients = pd.Series(coefficients, index=endog_names)\n    elif method == 'cumulative':\n        coefficients = np.zeros((k_endog, k_factors))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            for i in range(k_factors):\n                if self.model._s.endog_factor_map.iloc[j, i]:\n                    ix = np.r_[True, ef_map.iloc[j, :i + 1], [False] * (k_factors - i - 1)]\n                    X = exog.loc[:, ix.astype(bool).tolist()]\n                    coefficients[j, i] = OLS(endog, X, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    return coefficients",
            "def get_coefficients_of_determination(self, method='individual', which=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Get coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n\\n        Returns\\n        -------\\n        rsquared : pd.DataFrame or pd.Series\\n            The R-squared values from regressions of observed variables on\\n            one or more of the factors. If method=\\'individual\\' or\\n            method=\\'cumulative\\', this will be a Pandas DataFrame with observed\\n            variables as the index and factors as the columns . If\\n            method=\\'joint\\', will be a Pandas Series with observed variables as\\n            the index.\\n\\n        See Also\\n        --------\\n        plot_coefficients_of_determination\\n        coefficients_of_determination\\n        '\n    from statsmodels.tools import add_constant\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if which is None:\n        which = 'filtered' if self.smoothed_state is None else 'smoothed'\n    k_endog = self.model.k_endog\n    k_factors = self.model.k_factors\n    ef_map = self.model._s.endog_factor_map\n    endog_names = self.model.endog_names\n    factor_names = self.model.factor_names\n    if method == 'individual':\n        coefficients = np.zeros((k_endog, k_factors))\n        for i in range(k_factors):\n            exog = add_constant(self.factors[which].iloc[:, i])\n            for j in range(k_endog):\n                if ef_map.iloc[j, i]:\n                    endog = self.filter_results.endog[j]\n                    coefficients[j, i] = OLS(endog, exog, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    elif method == 'joint':\n        coefficients = np.zeros((k_endog,))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            ix = np.r_[True, ef_map.iloc[j]].tolist()\n            X = exog.loc[:, ix]\n            coefficients[j] = OLS(endog, X, missing='drop').fit().rsquared\n        coefficients = pd.Series(coefficients, index=endog_names)\n    elif method == 'cumulative':\n        coefficients = np.zeros((k_endog, k_factors))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            for i in range(k_factors):\n                if self.model._s.endog_factor_map.iloc[j, i]:\n                    ix = np.r_[True, ef_map.iloc[j, :i + 1], [False] * (k_factors - i - 1)]\n                    X = exog.loc[:, ix.astype(bool).tolist()]\n                    coefficients[j, i] = OLS(endog, X, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    return coefficients",
            "def get_coefficients_of_determination(self, method='individual', which=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Get coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n\\n        Returns\\n        -------\\n        rsquared : pd.DataFrame or pd.Series\\n            The R-squared values from regressions of observed variables on\\n            one or more of the factors. If method=\\'individual\\' or\\n            method=\\'cumulative\\', this will be a Pandas DataFrame with observed\\n            variables as the index and factors as the columns . If\\n            method=\\'joint\\', will be a Pandas Series with observed variables as\\n            the index.\\n\\n        See Also\\n        --------\\n        plot_coefficients_of_determination\\n        coefficients_of_determination\\n        '\n    from statsmodels.tools import add_constant\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if which is None:\n        which = 'filtered' if self.smoothed_state is None else 'smoothed'\n    k_endog = self.model.k_endog\n    k_factors = self.model.k_factors\n    ef_map = self.model._s.endog_factor_map\n    endog_names = self.model.endog_names\n    factor_names = self.model.factor_names\n    if method == 'individual':\n        coefficients = np.zeros((k_endog, k_factors))\n        for i in range(k_factors):\n            exog = add_constant(self.factors[which].iloc[:, i])\n            for j in range(k_endog):\n                if ef_map.iloc[j, i]:\n                    endog = self.filter_results.endog[j]\n                    coefficients[j, i] = OLS(endog, exog, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    elif method == 'joint':\n        coefficients = np.zeros((k_endog,))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            ix = np.r_[True, ef_map.iloc[j]].tolist()\n            X = exog.loc[:, ix]\n            coefficients[j] = OLS(endog, X, missing='drop').fit().rsquared\n        coefficients = pd.Series(coefficients, index=endog_names)\n    elif method == 'cumulative':\n        coefficients = np.zeros((k_endog, k_factors))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            for i in range(k_factors):\n                if self.model._s.endog_factor_map.iloc[j, i]:\n                    ix = np.r_[True, ef_map.iloc[j, :i + 1], [False] * (k_factors - i - 1)]\n                    X = exog.loc[:, ix.astype(bool).tolist()]\n                    coefficients[j, i] = OLS(endog, X, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    return coefficients",
            "def get_coefficients_of_determination(self, method='individual', which=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Get coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n\\n        Returns\\n        -------\\n        rsquared : pd.DataFrame or pd.Series\\n            The R-squared values from regressions of observed variables on\\n            one or more of the factors. If method=\\'individual\\' or\\n            method=\\'cumulative\\', this will be a Pandas DataFrame with observed\\n            variables as the index and factors as the columns . If\\n            method=\\'joint\\', will be a Pandas Series with observed variables as\\n            the index.\\n\\n        See Also\\n        --------\\n        plot_coefficients_of_determination\\n        coefficients_of_determination\\n        '\n    from statsmodels.tools import add_constant\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if which is None:\n        which = 'filtered' if self.smoothed_state is None else 'smoothed'\n    k_endog = self.model.k_endog\n    k_factors = self.model.k_factors\n    ef_map = self.model._s.endog_factor_map\n    endog_names = self.model.endog_names\n    factor_names = self.model.factor_names\n    if method == 'individual':\n        coefficients = np.zeros((k_endog, k_factors))\n        for i in range(k_factors):\n            exog = add_constant(self.factors[which].iloc[:, i])\n            for j in range(k_endog):\n                if ef_map.iloc[j, i]:\n                    endog = self.filter_results.endog[j]\n                    coefficients[j, i] = OLS(endog, exog, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    elif method == 'joint':\n        coefficients = np.zeros((k_endog,))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            ix = np.r_[True, ef_map.iloc[j]].tolist()\n            X = exog.loc[:, ix]\n            coefficients[j] = OLS(endog, X, missing='drop').fit().rsquared\n        coefficients = pd.Series(coefficients, index=endog_names)\n    elif method == 'cumulative':\n        coefficients = np.zeros((k_endog, k_factors))\n        exog = add_constant(self.factors[which])\n        for j in range(k_endog):\n            endog = self.filter_results.endog[j]\n            for i in range(k_factors):\n                if self.model._s.endog_factor_map.iloc[j, i]:\n                    ix = np.r_[True, ef_map.iloc[j, :i + 1], [False] * (k_factors - i - 1)]\n                    X = exog.loc[:, ix.astype(bool).tolist()]\n                    coefficients[j, i] = OLS(endog, X, missing='drop').fit().rsquared\n                else:\n                    coefficients[j, i] = np.nan\n        coefficients = pd.DataFrame(coefficients, index=endog_names, columns=factor_names)\n    return coefficients"
        ]
    },
    {
        "func_name": "coefficients_of_determination",
        "original": "@cache_readonly\ndef coefficients_of_determination(self):\n    \"\"\"\n        Individual coefficients of determination (:math:`R^2`).\n\n        Coefficients of determination (:math:`R^2`) from regressions of\n        endogenous variables on individual estimated factors.\n\n        Returns\n        -------\n        coefficients_of_determination : ndarray\n            A `k_endog` x `k_factors` array, where\n            `coefficients_of_determination[i, j]` represents the :math:`R^2`\n            value from a regression of factor `j` and a constant on endogenous\n            variable `i`.\n\n        Notes\n        -----\n        Although it can be difficult to interpret the estimated factor loadings\n        and factors, it is often helpful to use the coefficients of\n        determination from univariate regressions to assess the importance of\n        each factor in explaining the variation in each endogenous variable.\n\n        In models with many variables and factors, this can sometimes lend\n        interpretation to the factors (for example sometimes one factor will\n        load primarily on real variables and another on nominal variables).\n\n        See Also\n        --------\n        get_coefficients_of_determination\n        plot_coefficients_of_determination\n        \"\"\"\n    return self.get_coefficients_of_determination(method='individual')",
        "mutated": [
            "@cache_readonly\ndef coefficients_of_determination(self):\n    if False:\n        i = 10\n    '\\n        Individual coefficients of determination (:math:`R^2`).\\n\\n        Coefficients of determination (:math:`R^2`) from regressions of\\n        endogenous variables on individual estimated factors.\\n\\n        Returns\\n        -------\\n        coefficients_of_determination : ndarray\\n            A `k_endog` x `k_factors` array, where\\n            `coefficients_of_determination[i, j]` represents the :math:`R^2`\\n            value from a regression of factor `j` and a constant on endogenous\\n            variable `i`.\\n\\n        Notes\\n        -----\\n        Although it can be difficult to interpret the estimated factor loadings\\n        and factors, it is often helpful to use the coefficients of\\n        determination from univariate regressions to assess the importance of\\n        each factor in explaining the variation in each endogenous variable.\\n\\n        In models with many variables and factors, this can sometimes lend\\n        interpretation to the factors (for example sometimes one factor will\\n        load primarily on real variables and another on nominal variables).\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        plot_coefficients_of_determination\\n        '\n    return self.get_coefficients_of_determination(method='individual')",
            "@cache_readonly\ndef coefficients_of_determination(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Individual coefficients of determination (:math:`R^2`).\\n\\n        Coefficients of determination (:math:`R^2`) from regressions of\\n        endogenous variables on individual estimated factors.\\n\\n        Returns\\n        -------\\n        coefficients_of_determination : ndarray\\n            A `k_endog` x `k_factors` array, where\\n            `coefficients_of_determination[i, j]` represents the :math:`R^2`\\n            value from a regression of factor `j` and a constant on endogenous\\n            variable `i`.\\n\\n        Notes\\n        -----\\n        Although it can be difficult to interpret the estimated factor loadings\\n        and factors, it is often helpful to use the coefficients of\\n        determination from univariate regressions to assess the importance of\\n        each factor in explaining the variation in each endogenous variable.\\n\\n        In models with many variables and factors, this can sometimes lend\\n        interpretation to the factors (for example sometimes one factor will\\n        load primarily on real variables and another on nominal variables).\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        plot_coefficients_of_determination\\n        '\n    return self.get_coefficients_of_determination(method='individual')",
            "@cache_readonly\ndef coefficients_of_determination(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Individual coefficients of determination (:math:`R^2`).\\n\\n        Coefficients of determination (:math:`R^2`) from regressions of\\n        endogenous variables on individual estimated factors.\\n\\n        Returns\\n        -------\\n        coefficients_of_determination : ndarray\\n            A `k_endog` x `k_factors` array, where\\n            `coefficients_of_determination[i, j]` represents the :math:`R^2`\\n            value from a regression of factor `j` and a constant on endogenous\\n            variable `i`.\\n\\n        Notes\\n        -----\\n        Although it can be difficult to interpret the estimated factor loadings\\n        and factors, it is often helpful to use the coefficients of\\n        determination from univariate regressions to assess the importance of\\n        each factor in explaining the variation in each endogenous variable.\\n\\n        In models with many variables and factors, this can sometimes lend\\n        interpretation to the factors (for example sometimes one factor will\\n        load primarily on real variables and another on nominal variables).\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        plot_coefficients_of_determination\\n        '\n    return self.get_coefficients_of_determination(method='individual')",
            "@cache_readonly\ndef coefficients_of_determination(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Individual coefficients of determination (:math:`R^2`).\\n\\n        Coefficients of determination (:math:`R^2`) from regressions of\\n        endogenous variables on individual estimated factors.\\n\\n        Returns\\n        -------\\n        coefficients_of_determination : ndarray\\n            A `k_endog` x `k_factors` array, where\\n            `coefficients_of_determination[i, j]` represents the :math:`R^2`\\n            value from a regression of factor `j` and a constant on endogenous\\n            variable `i`.\\n\\n        Notes\\n        -----\\n        Although it can be difficult to interpret the estimated factor loadings\\n        and factors, it is often helpful to use the coefficients of\\n        determination from univariate regressions to assess the importance of\\n        each factor in explaining the variation in each endogenous variable.\\n\\n        In models with many variables and factors, this can sometimes lend\\n        interpretation to the factors (for example sometimes one factor will\\n        load primarily on real variables and another on nominal variables).\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        plot_coefficients_of_determination\\n        '\n    return self.get_coefficients_of_determination(method='individual')",
            "@cache_readonly\ndef coefficients_of_determination(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Individual coefficients of determination (:math:`R^2`).\\n\\n        Coefficients of determination (:math:`R^2`) from regressions of\\n        endogenous variables on individual estimated factors.\\n\\n        Returns\\n        -------\\n        coefficients_of_determination : ndarray\\n            A `k_endog` x `k_factors` array, where\\n            `coefficients_of_determination[i, j]` represents the :math:`R^2`\\n            value from a regression of factor `j` and a constant on endogenous\\n            variable `i`.\\n\\n        Notes\\n        -----\\n        Although it can be difficult to interpret the estimated factor loadings\\n        and factors, it is often helpful to use the coefficients of\\n        determination from univariate regressions to assess the importance of\\n        each factor in explaining the variation in each endogenous variable.\\n\\n        In models with many variables and factors, this can sometimes lend\\n        interpretation to the factors (for example sometimes one factor will\\n        load primarily on real variables and another on nominal variables).\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        plot_coefficients_of_determination\\n        '\n    return self.get_coefficients_of_determination(method='individual')"
        ]
    },
    {
        "func_name": "plot_coefficients_of_determination",
        "original": "def plot_coefficients_of_determination(self, method='individual', which=None, endog_labels=None, fig=None, figsize=None):\n    \"\"\"\n        Plot coefficients of determination (R-squared) for variables / factors.\n\n        Parameters\n        ----------\n        method : {'individual', 'joint', 'cumulative'}, optional\n            The type of R-squared values to generate. \"individual\" plots\n            the R-squared of each variable on each factor; \"joint\" plots the\n            R-squared of each variable on each factor that it loads on;\n            \"cumulative\" plots the successive R-squared values as each\n            additional factor is added to the regression, for each variable.\n            Default is 'individual'.\n        which: {None, 'filtered', 'smoothed'}, optional\n            Whether to compute R-squared values based on filtered or smoothed\n            estimates of the factors. Default is 'smoothed' if smoothed results\n            are available and 'filtered' otherwise.\n        endog_labels : bool, optional\n            Whether or not to label the endogenous variables along the x-axis\n            of the plots. Default is to include labels if there are 5 or fewer\n            endogenous variables.\n        fig : Figure, optional\n            If given, subplots are created in this figure instead of in a new\n            figure. Note that the grid will be created in the provided\n            figure using `fig.add_subplot()`.\n        figsize : tuple, optional\n            If a figure is created, this argument allows specifying a size.\n            The tuple is (width, height).\n\n        Notes\n        -----\n        The endogenous variables are arranged along the x-axis according to\n        their position in the model's `endog` array.\n\n        See Also\n        --------\n        get_coefficients_of_determination\n        \"\"\"\n    from statsmodels.graphics.utils import _import_mpl, create_mpl_fig\n    _import_mpl()\n    fig = create_mpl_fig(fig, figsize)\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if endog_labels is None:\n        endog_labels = self.model.k_endog <= 5\n    rsquared = self.get_coefficients_of_determination(method=method, which=which)\n    if method in ['individual', 'cumulative']:\n        plot_idx = 1\n        for (factor_name, coeffs) in rsquared.T.iterrows():\n            ax = fig.add_subplot(self.model.k_factors, 1, plot_idx)\n            ax.set_ylim((0, 1))\n            ax.set(title=f'{factor_name}', ylabel='$R^2$')\n            coeffs.plot(ax=ax, kind='bar')\n            if plot_idx < len(rsquared.columns) or not endog_labels:\n                ax.xaxis.set_ticklabels([])\n            plot_idx += 1\n    elif method == 'joint':\n        ax = fig.add_subplot(1, 1, 1)\n        ax.set_ylim((0, 1))\n        ax.set(title='$R^2$ - regression on all loaded factors', ylabel='$R^2$')\n        rsquared.plot(ax=ax, kind='bar')\n        if not endog_labels:\n            ax.xaxis.set_ticklabels([])\n    return fig",
        "mutated": [
            "def plot_coefficients_of_determination(self, method='individual', which=None, endog_labels=None, fig=None, figsize=None):\n    if False:\n        i = 10\n    '\\n        Plot coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n        endog_labels : bool, optional\\n            Whether or not to label the endogenous variables along the x-axis\\n            of the plots. Default is to include labels if there are 5 or fewer\\n            endogenous variables.\\n        fig : Figure, optional\\n            If given, subplots are created in this figure instead of in a new\\n            figure. Note that the grid will be created in the provided\\n            figure using `fig.add_subplot()`.\\n        figsize : tuple, optional\\n            If a figure is created, this argument allows specifying a size.\\n            The tuple is (width, height).\\n\\n        Notes\\n        -----\\n        The endogenous variables are arranged along the x-axis according to\\n        their position in the model\\'s `endog` array.\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        '\n    from statsmodels.graphics.utils import _import_mpl, create_mpl_fig\n    _import_mpl()\n    fig = create_mpl_fig(fig, figsize)\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if endog_labels is None:\n        endog_labels = self.model.k_endog <= 5\n    rsquared = self.get_coefficients_of_determination(method=method, which=which)\n    if method in ['individual', 'cumulative']:\n        plot_idx = 1\n        for (factor_name, coeffs) in rsquared.T.iterrows():\n            ax = fig.add_subplot(self.model.k_factors, 1, plot_idx)\n            ax.set_ylim((0, 1))\n            ax.set(title=f'{factor_name}', ylabel='$R^2$')\n            coeffs.plot(ax=ax, kind='bar')\n            if plot_idx < len(rsquared.columns) or not endog_labels:\n                ax.xaxis.set_ticklabels([])\n            plot_idx += 1\n    elif method == 'joint':\n        ax = fig.add_subplot(1, 1, 1)\n        ax.set_ylim((0, 1))\n        ax.set(title='$R^2$ - regression on all loaded factors', ylabel='$R^2$')\n        rsquared.plot(ax=ax, kind='bar')\n        if not endog_labels:\n            ax.xaxis.set_ticklabels([])\n    return fig",
            "def plot_coefficients_of_determination(self, method='individual', which=None, endog_labels=None, fig=None, figsize=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Plot coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n        endog_labels : bool, optional\\n            Whether or not to label the endogenous variables along the x-axis\\n            of the plots. Default is to include labels if there are 5 or fewer\\n            endogenous variables.\\n        fig : Figure, optional\\n            If given, subplots are created in this figure instead of in a new\\n            figure. Note that the grid will be created in the provided\\n            figure using `fig.add_subplot()`.\\n        figsize : tuple, optional\\n            If a figure is created, this argument allows specifying a size.\\n            The tuple is (width, height).\\n\\n        Notes\\n        -----\\n        The endogenous variables are arranged along the x-axis according to\\n        their position in the model\\'s `endog` array.\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        '\n    from statsmodels.graphics.utils import _import_mpl, create_mpl_fig\n    _import_mpl()\n    fig = create_mpl_fig(fig, figsize)\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if endog_labels is None:\n        endog_labels = self.model.k_endog <= 5\n    rsquared = self.get_coefficients_of_determination(method=method, which=which)\n    if method in ['individual', 'cumulative']:\n        plot_idx = 1\n        for (factor_name, coeffs) in rsquared.T.iterrows():\n            ax = fig.add_subplot(self.model.k_factors, 1, plot_idx)\n            ax.set_ylim((0, 1))\n            ax.set(title=f'{factor_name}', ylabel='$R^2$')\n            coeffs.plot(ax=ax, kind='bar')\n            if plot_idx < len(rsquared.columns) or not endog_labels:\n                ax.xaxis.set_ticklabels([])\n            plot_idx += 1\n    elif method == 'joint':\n        ax = fig.add_subplot(1, 1, 1)\n        ax.set_ylim((0, 1))\n        ax.set(title='$R^2$ - regression on all loaded factors', ylabel='$R^2$')\n        rsquared.plot(ax=ax, kind='bar')\n        if not endog_labels:\n            ax.xaxis.set_ticklabels([])\n    return fig",
            "def plot_coefficients_of_determination(self, method='individual', which=None, endog_labels=None, fig=None, figsize=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Plot coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n        endog_labels : bool, optional\\n            Whether or not to label the endogenous variables along the x-axis\\n            of the plots. Default is to include labels if there are 5 or fewer\\n            endogenous variables.\\n        fig : Figure, optional\\n            If given, subplots are created in this figure instead of in a new\\n            figure. Note that the grid will be created in the provided\\n            figure using `fig.add_subplot()`.\\n        figsize : tuple, optional\\n            If a figure is created, this argument allows specifying a size.\\n            The tuple is (width, height).\\n\\n        Notes\\n        -----\\n        The endogenous variables are arranged along the x-axis according to\\n        their position in the model\\'s `endog` array.\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        '\n    from statsmodels.graphics.utils import _import_mpl, create_mpl_fig\n    _import_mpl()\n    fig = create_mpl_fig(fig, figsize)\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if endog_labels is None:\n        endog_labels = self.model.k_endog <= 5\n    rsquared = self.get_coefficients_of_determination(method=method, which=which)\n    if method in ['individual', 'cumulative']:\n        plot_idx = 1\n        for (factor_name, coeffs) in rsquared.T.iterrows():\n            ax = fig.add_subplot(self.model.k_factors, 1, plot_idx)\n            ax.set_ylim((0, 1))\n            ax.set(title=f'{factor_name}', ylabel='$R^2$')\n            coeffs.plot(ax=ax, kind='bar')\n            if plot_idx < len(rsquared.columns) or not endog_labels:\n                ax.xaxis.set_ticklabels([])\n            plot_idx += 1\n    elif method == 'joint':\n        ax = fig.add_subplot(1, 1, 1)\n        ax.set_ylim((0, 1))\n        ax.set(title='$R^2$ - regression on all loaded factors', ylabel='$R^2$')\n        rsquared.plot(ax=ax, kind='bar')\n        if not endog_labels:\n            ax.xaxis.set_ticklabels([])\n    return fig",
            "def plot_coefficients_of_determination(self, method='individual', which=None, endog_labels=None, fig=None, figsize=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Plot coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n        endog_labels : bool, optional\\n            Whether or not to label the endogenous variables along the x-axis\\n            of the plots. Default is to include labels if there are 5 or fewer\\n            endogenous variables.\\n        fig : Figure, optional\\n            If given, subplots are created in this figure instead of in a new\\n            figure. Note that the grid will be created in the provided\\n            figure using `fig.add_subplot()`.\\n        figsize : tuple, optional\\n            If a figure is created, this argument allows specifying a size.\\n            The tuple is (width, height).\\n\\n        Notes\\n        -----\\n        The endogenous variables are arranged along the x-axis according to\\n        their position in the model\\'s `endog` array.\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        '\n    from statsmodels.graphics.utils import _import_mpl, create_mpl_fig\n    _import_mpl()\n    fig = create_mpl_fig(fig, figsize)\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if endog_labels is None:\n        endog_labels = self.model.k_endog <= 5\n    rsquared = self.get_coefficients_of_determination(method=method, which=which)\n    if method in ['individual', 'cumulative']:\n        plot_idx = 1\n        for (factor_name, coeffs) in rsquared.T.iterrows():\n            ax = fig.add_subplot(self.model.k_factors, 1, plot_idx)\n            ax.set_ylim((0, 1))\n            ax.set(title=f'{factor_name}', ylabel='$R^2$')\n            coeffs.plot(ax=ax, kind='bar')\n            if plot_idx < len(rsquared.columns) or not endog_labels:\n                ax.xaxis.set_ticklabels([])\n            plot_idx += 1\n    elif method == 'joint':\n        ax = fig.add_subplot(1, 1, 1)\n        ax.set_ylim((0, 1))\n        ax.set(title='$R^2$ - regression on all loaded factors', ylabel='$R^2$')\n        rsquared.plot(ax=ax, kind='bar')\n        if not endog_labels:\n            ax.xaxis.set_ticklabels([])\n    return fig",
            "def plot_coefficients_of_determination(self, method='individual', which=None, endog_labels=None, fig=None, figsize=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Plot coefficients of determination (R-squared) for variables / factors.\\n\\n        Parameters\\n        ----------\\n        method : {\\'individual\\', \\'joint\\', \\'cumulative\\'}, optional\\n            The type of R-squared values to generate. \"individual\" plots\\n            the R-squared of each variable on each factor; \"joint\" plots the\\n            R-squared of each variable on each factor that it loads on;\\n            \"cumulative\" plots the successive R-squared values as each\\n            additional factor is added to the regression, for each variable.\\n            Default is \\'individual\\'.\\n        which: {None, \\'filtered\\', \\'smoothed\\'}, optional\\n            Whether to compute R-squared values based on filtered or smoothed\\n            estimates of the factors. Default is \\'smoothed\\' if smoothed results\\n            are available and \\'filtered\\' otherwise.\\n        endog_labels : bool, optional\\n            Whether or not to label the endogenous variables along the x-axis\\n            of the plots. Default is to include labels if there are 5 or fewer\\n            endogenous variables.\\n        fig : Figure, optional\\n            If given, subplots are created in this figure instead of in a new\\n            figure. Note that the grid will be created in the provided\\n            figure using `fig.add_subplot()`.\\n        figsize : tuple, optional\\n            If a figure is created, this argument allows specifying a size.\\n            The tuple is (width, height).\\n\\n        Notes\\n        -----\\n        The endogenous variables are arranged along the x-axis according to\\n        their position in the model\\'s `endog` array.\\n\\n        See Also\\n        --------\\n        get_coefficients_of_determination\\n        '\n    from statsmodels.graphics.utils import _import_mpl, create_mpl_fig\n    _import_mpl()\n    fig = create_mpl_fig(fig, figsize)\n    method = string_like(method, 'method', options=['individual', 'joint', 'cumulative'])\n    if endog_labels is None:\n        endog_labels = self.model.k_endog <= 5\n    rsquared = self.get_coefficients_of_determination(method=method, which=which)\n    if method in ['individual', 'cumulative']:\n        plot_idx = 1\n        for (factor_name, coeffs) in rsquared.T.iterrows():\n            ax = fig.add_subplot(self.model.k_factors, 1, plot_idx)\n            ax.set_ylim((0, 1))\n            ax.set(title=f'{factor_name}', ylabel='$R^2$')\n            coeffs.plot(ax=ax, kind='bar')\n            if plot_idx < len(rsquared.columns) or not endog_labels:\n                ax.xaxis.set_ticklabels([])\n            plot_idx += 1\n    elif method == 'joint':\n        ax = fig.add_subplot(1, 1, 1)\n        ax.set_ylim((0, 1))\n        ax.set(title='$R^2$ - regression on all loaded factors', ylabel='$R^2$')\n        rsquared.plot(ax=ax, kind='bar')\n        if not endog_labels:\n            ax.xaxis.set_ticklabels([])\n    return fig"
        ]
    },
    {
        "func_name": "get_prediction",
        "original": "def get_prediction(self, start=None, end=None, dynamic=False, information_set='predicted', signal_only=False, original_scale=True, index=None, exog=None, extend_model=None, extend_kwargs=None, **kwargs):\n    \"\"\"\n        In-sample prediction and out-of-sample forecasting.\n\n        Parameters\n        ----------\n        start : int, str, or datetime, optional\n            Zero-indexed observation number at which to start forecasting,\n            i.e., the first forecast is start. Can also be a date string to\n            parse or a datetime type. Default is the the zeroth observation.\n        end : int, str, or datetime, optional\n            Zero-indexed observation number at which to end forecasting, i.e.,\n            the last forecast is end. Can also be a date string to\n            parse or a datetime type. However, if the dates index does not\n            have a fixed frequency, end must be an integer index if you\n            want out of sample prediction. Default is the last observation in\n            the sample.\n        dynamic : bool, int, str, or datetime, optional\n            Integer offset relative to `start` at which to begin dynamic\n            prediction. Can also be an absolute date string to parse or a\n            datetime type (these are not interpreted as offsets).\n            Prior to this observation, true endogenous values will be used for\n            prediction; starting with this observation and continuing through\n            the end of prediction, forecasted endogenous values will be used\n            instead.\n        information_set : str, optional\n            The information set to condition each prediction on. Default is\n            \"predicted\", which computes predictions of period t values\n            conditional on observed data through period t-1; these are\n            one-step-ahead predictions, and correspond with the typical\n            `fittedvalues` results attribute. Alternatives are \"filtered\",\n            which computes predictions of period t values conditional on\n            observed data through period t, and \"smoothed\", which computes\n            predictions of period t values conditional on the entire dataset\n            (including also future observations t+1, t+2, ...).\n        signal_only : bool, optional\n            Whether to compute forecasts of only the \"signal\" component of\n            the observation equation. Default is False. For example, the\n            observation equation of a time-invariant model is\n            :math:`y_t = d + Z \\\\alpha_t + \\\\varepsilon_t`, and the \"signal\"\n            component is then :math:`Z \\\\alpha_t`. If this argument is set to\n            True, then forecasts of the \"signal\" :math:`Z \\\\alpha_t` will be\n            returned. Otherwise, the default is for forecasts of :math:`y_t`\n            to be returned.\n        original_scale : bool, optional\n            If the model specification standardized the data, whether or not\n            to return predictions in the original scale of the data (i.e.\n            before it was standardized by the model). Default is True.\n        **kwargs\n            Additional arguments may required for forecasting beyond the end\n            of the sample. See `FilterResults.predict` for more details.\n\n        Returns\n        -------\n        forecast : ndarray\n            Array of out of in-sample predictions and / or out-of-sample\n            forecasts. An (npredict x k_endog) array.\n        \"\"\"\n    res = super().get_prediction(start=start, end=end, dynamic=dynamic, information_set=information_set, signal_only=signal_only, index=index, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, **kwargs)\n    if self.model.standardize and original_scale:\n        prediction_results = res.prediction_results\n        (k_endog, _) = prediction_results.endog.shape\n        mean = np.array(self.model._endog_mean)\n        std = np.array(self.model._endog_std)\n        if self.model.k_endog > 1:\n            mean = mean[None, :]\n            std = std[None, :]\n        res._results._predicted_mean = res._results._predicted_mean * std + mean\n        if k_endog == 1:\n            res._results._var_pred_mean *= std ** 2\n        else:\n            res._results._var_pred_mean = std * res._results._var_pred_mean * std.T\n    return res",
        "mutated": [
            "def get_prediction(self, start=None, end=None, dynamic=False, information_set='predicted', signal_only=False, original_scale=True, index=None, exog=None, extend_model=None, extend_kwargs=None, **kwargs):\n    if False:\n        i = 10\n    '\\n        In-sample prediction and out-of-sample forecasting.\\n\\n        Parameters\\n        ----------\\n        start : int, str, or datetime, optional\\n            Zero-indexed observation number at which to start forecasting,\\n            i.e., the first forecast is start. Can also be a date string to\\n            parse or a datetime type. Default is the the zeroth observation.\\n        end : int, str, or datetime, optional\\n            Zero-indexed observation number at which to end forecasting, i.e.,\\n            the last forecast is end. Can also be a date string to\\n            parse or a datetime type. However, if the dates index does not\\n            have a fixed frequency, end must be an integer index if you\\n            want out of sample prediction. Default is the last observation in\\n            the sample.\\n        dynamic : bool, int, str, or datetime, optional\\n            Integer offset relative to `start` at which to begin dynamic\\n            prediction. Can also be an absolute date string to parse or a\\n            datetime type (these are not interpreted as offsets).\\n            Prior to this observation, true endogenous values will be used for\\n            prediction; starting with this observation and continuing through\\n            the end of prediction, forecasted endogenous values will be used\\n            instead.\\n        information_set : str, optional\\n            The information set to condition each prediction on. Default is\\n            \"predicted\", which computes predictions of period t values\\n            conditional on observed data through period t-1; these are\\n            one-step-ahead predictions, and correspond with the typical\\n            `fittedvalues` results attribute. Alternatives are \"filtered\",\\n            which computes predictions of period t values conditional on\\n            observed data through period t, and \"smoothed\", which computes\\n            predictions of period t values conditional on the entire dataset\\n            (including also future observations t+1, t+2, ...).\\n        signal_only : bool, optional\\n            Whether to compute forecasts of only the \"signal\" component of\\n            the observation equation. Default is False. For example, the\\n            observation equation of a time-invariant model is\\n            :math:`y_t = d + Z \\\\alpha_t + \\\\varepsilon_t`, and the \"signal\"\\n            component is then :math:`Z \\\\alpha_t`. If this argument is set to\\n            True, then forecasts of the \"signal\" :math:`Z \\\\alpha_t` will be\\n            returned. Otherwise, the default is for forecasts of :math:`y_t`\\n            to be returned.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return predictions in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            Additional arguments may required for forecasting beyond the end\\n            of the sample. See `FilterResults.predict` for more details.\\n\\n        Returns\\n        -------\\n        forecast : ndarray\\n            Array of out of in-sample predictions and / or out-of-sample\\n            forecasts. An (npredict x k_endog) array.\\n        '\n    res = super().get_prediction(start=start, end=end, dynamic=dynamic, information_set=information_set, signal_only=signal_only, index=index, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, **kwargs)\n    if self.model.standardize and original_scale:\n        prediction_results = res.prediction_results\n        (k_endog, _) = prediction_results.endog.shape\n        mean = np.array(self.model._endog_mean)\n        std = np.array(self.model._endog_std)\n        if self.model.k_endog > 1:\n            mean = mean[None, :]\n            std = std[None, :]\n        res._results._predicted_mean = res._results._predicted_mean * std + mean\n        if k_endog == 1:\n            res._results._var_pred_mean *= std ** 2\n        else:\n            res._results._var_pred_mean = std * res._results._var_pred_mean * std.T\n    return res",
            "def get_prediction(self, start=None, end=None, dynamic=False, information_set='predicted', signal_only=False, original_scale=True, index=None, exog=None, extend_model=None, extend_kwargs=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        In-sample prediction and out-of-sample forecasting.\\n\\n        Parameters\\n        ----------\\n        start : int, str, or datetime, optional\\n            Zero-indexed observation number at which to start forecasting,\\n            i.e., the first forecast is start. Can also be a date string to\\n            parse or a datetime type. Default is the the zeroth observation.\\n        end : int, str, or datetime, optional\\n            Zero-indexed observation number at which to end forecasting, i.e.,\\n            the last forecast is end. Can also be a date string to\\n            parse or a datetime type. However, if the dates index does not\\n            have a fixed frequency, end must be an integer index if you\\n            want out of sample prediction. Default is the last observation in\\n            the sample.\\n        dynamic : bool, int, str, or datetime, optional\\n            Integer offset relative to `start` at which to begin dynamic\\n            prediction. Can also be an absolute date string to parse or a\\n            datetime type (these are not interpreted as offsets).\\n            Prior to this observation, true endogenous values will be used for\\n            prediction; starting with this observation and continuing through\\n            the end of prediction, forecasted endogenous values will be used\\n            instead.\\n        information_set : str, optional\\n            The information set to condition each prediction on. Default is\\n            \"predicted\", which computes predictions of period t values\\n            conditional on observed data through period t-1; these are\\n            one-step-ahead predictions, and correspond with the typical\\n            `fittedvalues` results attribute. Alternatives are \"filtered\",\\n            which computes predictions of period t values conditional on\\n            observed data through period t, and \"smoothed\", which computes\\n            predictions of period t values conditional on the entire dataset\\n            (including also future observations t+1, t+2, ...).\\n        signal_only : bool, optional\\n            Whether to compute forecasts of only the \"signal\" component of\\n            the observation equation. Default is False. For example, the\\n            observation equation of a time-invariant model is\\n            :math:`y_t = d + Z \\\\alpha_t + \\\\varepsilon_t`, and the \"signal\"\\n            component is then :math:`Z \\\\alpha_t`. If this argument is set to\\n            True, then forecasts of the \"signal\" :math:`Z \\\\alpha_t` will be\\n            returned. Otherwise, the default is for forecasts of :math:`y_t`\\n            to be returned.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return predictions in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            Additional arguments may required for forecasting beyond the end\\n            of the sample. See `FilterResults.predict` for more details.\\n\\n        Returns\\n        -------\\n        forecast : ndarray\\n            Array of out of in-sample predictions and / or out-of-sample\\n            forecasts. An (npredict x k_endog) array.\\n        '\n    res = super().get_prediction(start=start, end=end, dynamic=dynamic, information_set=information_set, signal_only=signal_only, index=index, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, **kwargs)\n    if self.model.standardize and original_scale:\n        prediction_results = res.prediction_results\n        (k_endog, _) = prediction_results.endog.shape\n        mean = np.array(self.model._endog_mean)\n        std = np.array(self.model._endog_std)\n        if self.model.k_endog > 1:\n            mean = mean[None, :]\n            std = std[None, :]\n        res._results._predicted_mean = res._results._predicted_mean * std + mean\n        if k_endog == 1:\n            res._results._var_pred_mean *= std ** 2\n        else:\n            res._results._var_pred_mean = std * res._results._var_pred_mean * std.T\n    return res",
            "def get_prediction(self, start=None, end=None, dynamic=False, information_set='predicted', signal_only=False, original_scale=True, index=None, exog=None, extend_model=None, extend_kwargs=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        In-sample prediction and out-of-sample forecasting.\\n\\n        Parameters\\n        ----------\\n        start : int, str, or datetime, optional\\n            Zero-indexed observation number at which to start forecasting,\\n            i.e., the first forecast is start. Can also be a date string to\\n            parse or a datetime type. Default is the the zeroth observation.\\n        end : int, str, or datetime, optional\\n            Zero-indexed observation number at which to end forecasting, i.e.,\\n            the last forecast is end. Can also be a date string to\\n            parse or a datetime type. However, if the dates index does not\\n            have a fixed frequency, end must be an integer index if you\\n            want out of sample prediction. Default is the last observation in\\n            the sample.\\n        dynamic : bool, int, str, or datetime, optional\\n            Integer offset relative to `start` at which to begin dynamic\\n            prediction. Can also be an absolute date string to parse or a\\n            datetime type (these are not interpreted as offsets).\\n            Prior to this observation, true endogenous values will be used for\\n            prediction; starting with this observation and continuing through\\n            the end of prediction, forecasted endogenous values will be used\\n            instead.\\n        information_set : str, optional\\n            The information set to condition each prediction on. Default is\\n            \"predicted\", which computes predictions of period t values\\n            conditional on observed data through period t-1; these are\\n            one-step-ahead predictions, and correspond with the typical\\n            `fittedvalues` results attribute. Alternatives are \"filtered\",\\n            which computes predictions of period t values conditional on\\n            observed data through period t, and \"smoothed\", which computes\\n            predictions of period t values conditional on the entire dataset\\n            (including also future observations t+1, t+2, ...).\\n        signal_only : bool, optional\\n            Whether to compute forecasts of only the \"signal\" component of\\n            the observation equation. Default is False. For example, the\\n            observation equation of a time-invariant model is\\n            :math:`y_t = d + Z \\\\alpha_t + \\\\varepsilon_t`, and the \"signal\"\\n            component is then :math:`Z \\\\alpha_t`. If this argument is set to\\n            True, then forecasts of the \"signal\" :math:`Z \\\\alpha_t` will be\\n            returned. Otherwise, the default is for forecasts of :math:`y_t`\\n            to be returned.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return predictions in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            Additional arguments may required for forecasting beyond the end\\n            of the sample. See `FilterResults.predict` for more details.\\n\\n        Returns\\n        -------\\n        forecast : ndarray\\n            Array of out of in-sample predictions and / or out-of-sample\\n            forecasts. An (npredict x k_endog) array.\\n        '\n    res = super().get_prediction(start=start, end=end, dynamic=dynamic, information_set=information_set, signal_only=signal_only, index=index, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, **kwargs)\n    if self.model.standardize and original_scale:\n        prediction_results = res.prediction_results\n        (k_endog, _) = prediction_results.endog.shape\n        mean = np.array(self.model._endog_mean)\n        std = np.array(self.model._endog_std)\n        if self.model.k_endog > 1:\n            mean = mean[None, :]\n            std = std[None, :]\n        res._results._predicted_mean = res._results._predicted_mean * std + mean\n        if k_endog == 1:\n            res._results._var_pred_mean *= std ** 2\n        else:\n            res._results._var_pred_mean = std * res._results._var_pred_mean * std.T\n    return res",
            "def get_prediction(self, start=None, end=None, dynamic=False, information_set='predicted', signal_only=False, original_scale=True, index=None, exog=None, extend_model=None, extend_kwargs=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        In-sample prediction and out-of-sample forecasting.\\n\\n        Parameters\\n        ----------\\n        start : int, str, or datetime, optional\\n            Zero-indexed observation number at which to start forecasting,\\n            i.e., the first forecast is start. Can also be a date string to\\n            parse or a datetime type. Default is the the zeroth observation.\\n        end : int, str, or datetime, optional\\n            Zero-indexed observation number at which to end forecasting, i.e.,\\n            the last forecast is end. Can also be a date string to\\n            parse or a datetime type. However, if the dates index does not\\n            have a fixed frequency, end must be an integer index if you\\n            want out of sample prediction. Default is the last observation in\\n            the sample.\\n        dynamic : bool, int, str, or datetime, optional\\n            Integer offset relative to `start` at which to begin dynamic\\n            prediction. Can also be an absolute date string to parse or a\\n            datetime type (these are not interpreted as offsets).\\n            Prior to this observation, true endogenous values will be used for\\n            prediction; starting with this observation and continuing through\\n            the end of prediction, forecasted endogenous values will be used\\n            instead.\\n        information_set : str, optional\\n            The information set to condition each prediction on. Default is\\n            \"predicted\", which computes predictions of period t values\\n            conditional on observed data through period t-1; these are\\n            one-step-ahead predictions, and correspond with the typical\\n            `fittedvalues` results attribute. Alternatives are \"filtered\",\\n            which computes predictions of period t values conditional on\\n            observed data through period t, and \"smoothed\", which computes\\n            predictions of period t values conditional on the entire dataset\\n            (including also future observations t+1, t+2, ...).\\n        signal_only : bool, optional\\n            Whether to compute forecasts of only the \"signal\" component of\\n            the observation equation. Default is False. For example, the\\n            observation equation of a time-invariant model is\\n            :math:`y_t = d + Z \\\\alpha_t + \\\\varepsilon_t`, and the \"signal\"\\n            component is then :math:`Z \\\\alpha_t`. If this argument is set to\\n            True, then forecasts of the \"signal\" :math:`Z \\\\alpha_t` will be\\n            returned. Otherwise, the default is for forecasts of :math:`y_t`\\n            to be returned.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return predictions in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            Additional arguments may required for forecasting beyond the end\\n            of the sample. See `FilterResults.predict` for more details.\\n\\n        Returns\\n        -------\\n        forecast : ndarray\\n            Array of out of in-sample predictions and / or out-of-sample\\n            forecasts. An (npredict x k_endog) array.\\n        '\n    res = super().get_prediction(start=start, end=end, dynamic=dynamic, information_set=information_set, signal_only=signal_only, index=index, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, **kwargs)\n    if self.model.standardize and original_scale:\n        prediction_results = res.prediction_results\n        (k_endog, _) = prediction_results.endog.shape\n        mean = np.array(self.model._endog_mean)\n        std = np.array(self.model._endog_std)\n        if self.model.k_endog > 1:\n            mean = mean[None, :]\n            std = std[None, :]\n        res._results._predicted_mean = res._results._predicted_mean * std + mean\n        if k_endog == 1:\n            res._results._var_pred_mean *= std ** 2\n        else:\n            res._results._var_pred_mean = std * res._results._var_pred_mean * std.T\n    return res",
            "def get_prediction(self, start=None, end=None, dynamic=False, information_set='predicted', signal_only=False, original_scale=True, index=None, exog=None, extend_model=None, extend_kwargs=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        In-sample prediction and out-of-sample forecasting.\\n\\n        Parameters\\n        ----------\\n        start : int, str, or datetime, optional\\n            Zero-indexed observation number at which to start forecasting,\\n            i.e., the first forecast is start. Can also be a date string to\\n            parse or a datetime type. Default is the the zeroth observation.\\n        end : int, str, or datetime, optional\\n            Zero-indexed observation number at which to end forecasting, i.e.,\\n            the last forecast is end. Can also be a date string to\\n            parse or a datetime type. However, if the dates index does not\\n            have a fixed frequency, end must be an integer index if you\\n            want out of sample prediction. Default is the last observation in\\n            the sample.\\n        dynamic : bool, int, str, or datetime, optional\\n            Integer offset relative to `start` at which to begin dynamic\\n            prediction. Can also be an absolute date string to parse or a\\n            datetime type (these are not interpreted as offsets).\\n            Prior to this observation, true endogenous values will be used for\\n            prediction; starting with this observation and continuing through\\n            the end of prediction, forecasted endogenous values will be used\\n            instead.\\n        information_set : str, optional\\n            The information set to condition each prediction on. Default is\\n            \"predicted\", which computes predictions of period t values\\n            conditional on observed data through period t-1; these are\\n            one-step-ahead predictions, and correspond with the typical\\n            `fittedvalues` results attribute. Alternatives are \"filtered\",\\n            which computes predictions of period t values conditional on\\n            observed data through period t, and \"smoothed\", which computes\\n            predictions of period t values conditional on the entire dataset\\n            (including also future observations t+1, t+2, ...).\\n        signal_only : bool, optional\\n            Whether to compute forecasts of only the \"signal\" component of\\n            the observation equation. Default is False. For example, the\\n            observation equation of a time-invariant model is\\n            :math:`y_t = d + Z \\\\alpha_t + \\\\varepsilon_t`, and the \"signal\"\\n            component is then :math:`Z \\\\alpha_t`. If this argument is set to\\n            True, then forecasts of the \"signal\" :math:`Z \\\\alpha_t` will be\\n            returned. Otherwise, the default is for forecasts of :math:`y_t`\\n            to be returned.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return predictions in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n        **kwargs\\n            Additional arguments may required for forecasting beyond the end\\n            of the sample. See `FilterResults.predict` for more details.\\n\\n        Returns\\n        -------\\n        forecast : ndarray\\n            Array of out of in-sample predictions and / or out-of-sample\\n            forecasts. An (npredict x k_endog) array.\\n        '\n    res = super().get_prediction(start=start, end=end, dynamic=dynamic, information_set=information_set, signal_only=signal_only, index=index, exog=exog, extend_model=extend_model, extend_kwargs=extend_kwargs, **kwargs)\n    if self.model.standardize and original_scale:\n        prediction_results = res.prediction_results\n        (k_endog, _) = prediction_results.endog.shape\n        mean = np.array(self.model._endog_mean)\n        std = np.array(self.model._endog_std)\n        if self.model.k_endog > 1:\n            mean = mean[None, :]\n            std = std[None, :]\n        res._results._predicted_mean = res._results._predicted_mean * std + mean\n        if k_endog == 1:\n            res._results._var_pred_mean *= std ** 2\n        else:\n            res._results._var_pred_mean = std * res._results._var_pred_mean * std.T\n    return res"
        ]
    },
    {
        "func_name": "news",
        "original": "def news(self, comparison, impact_date=None, impacted_variable=None, start=None, end=None, periods=None, exog=None, comparison_type=None, revisions_details_start=False, state_index=None, return_raw=False, tolerance=1e-10, endog_quarterly=None, original_scale=True, **kwargs):\n    \"\"\"\n        Compute impacts from updated data (news and revisions).\n\n        Parameters\n        ----------\n        comparison : array_like or MLEResults\n            An updated dataset with updated and/or revised data from which the\n            news can be computed, or an updated or previous results object\n            to use in computing the news.\n        impact_date : int, str, or datetime, optional\n            A single specific period of impacts from news and revisions to\n            compute. Can also be a date string to parse or a datetime type.\n            This argument cannot be used in combination with `start`, `end`, or\n            `periods`. Default is the first out-of-sample observation.\n        impacted_variable : str, list, array, or slice, optional\n            Observation variable label or slice of labels specifying that only\n            specific impacted variables should be shown in the News output. The\n            impacted variable(s) describe the variables that were *affected* by\n            the news. If you do not know the labels for the variables, check\n            the `endog_names` attribute of the model instance.\n        start : int, str, or datetime, optional\n            The first period of impacts from news and revisions to compute.\n            Can also be a date string to parse or a datetime type. Default is\n            the first out-of-sample observation.\n        end : int, str, or datetime, optional\n            The last period of impacts from news and revisions to compute.\n            Can also be a date string to parse or a datetime type. Default is\n            the first out-of-sample observation.\n        periods : int, optional\n            The number of periods of impacts from news and revisions to\n            compute.\n        exog : array_like, optional\n            Array of exogenous regressors for the out-of-sample period, if\n            applicable.\n        comparison_type : {None, 'previous', 'updated'}\n            This denotes whether the `comparison` argument represents a\n            *previous* results object or dataset or an *updated* results object\n            or dataset. If not specified, then an attempt is made to determine\n            the comparison type.\n        state_index : array_like or \"common\", optional\n            An optional index specifying a subset of states to use when\n            constructing the impacts of revisions and news. For example, if\n            `state_index=[0, 1]` is passed, then only the impacts to the\n            observed variables arising from the impacts to the first two\n            states will be returned. If the string \"common\" is passed and the\n            model includes idiosyncratic AR(1) components, news will only be\n            computed based on the common states. Default is to use all states.\n        return_raw : bool, optional\n            Whether or not to return only the specific output or a full\n            results object. Default is to return a full results object.\n        tolerance : float, optional\n            The numerical threshold for determining zero impact. Default is\n            that any impact less than 1e-10 is assumed to be zero.\n        endog_quarterly : array_like, optional\n            New observations of quarterly variables, if `comparison` was\n            provided as an updated monthly dataset. If this argument is\n            provided, it must be a Pandas Series or DataFrame with a\n            DatetimeIndex or PeriodIndex at the quarterly frequency.\n\n        References\n        ----------\n        .. [1] Ba\u0144bura, Marta, and Michele Modugno.\n               \"Maximum likelihood estimation of factor models on datasets with\n               arbitrary pattern of missing data.\"\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\n        .. [2] Ba\u0144bura, Marta, Domenico Giannone, and Lucrezia Reichlin.\n               \"Nowcasting.\"\n               The Oxford Handbook of Economic Forecasting. July 8, 2011.\n        .. [3] Ba\u0144bura, Marta, Domenico Giannone, Michele Modugno, and Lucrezia\n               Reichlin.\n               \"Now-casting and the real-time data flow.\"\n               In Handbook of economic forecasting, vol. 2, pp. 195-237.\n               Elsevier, 2013.\n        \"\"\"\n    if state_index == 'common':\n        state_index = np.arange(self.model.k_states - self.model.k_endog)\n    news_results = super().news(comparison, impact_date=impact_date, impacted_variable=impacted_variable, start=start, end=end, periods=periods, exog=exog, comparison_type=comparison_type, revisions_details_start=revisions_details_start, state_index=state_index, return_raw=return_raw, tolerance=tolerance, endog_quarterly=endog_quarterly, **kwargs)\n    if not return_raw and self.model.standardize and original_scale:\n        endog_mean = self.model._endog_mean\n        endog_std = self.model._endog_std\n        news_results.total_impacts = news_results.total_impacts * endog_std\n        news_results.update_impacts = news_results.update_impacts * endog_std\n        if news_results.revision_impacts is not None:\n            news_results.revision_impacts = news_results.revision_impacts * endog_std\n        if news_results.revision_detailed_impacts is not None:\n            news_results.revision_detailed_impacts = news_results.revision_detailed_impacts * endog_std\n        if news_results.revision_grouped_impacts is not None:\n            news_results.revision_grouped_impacts = news_results.revision_grouped_impacts * endog_std\n        for name in ['prev_impacted_forecasts', 'news', 'revisions', 'update_realized', 'update_forecasts', 'revised', 'revised_prev', 'post_impacted_forecasts', 'revisions_all', 'revised_all', 'revised_prev_all']:\n            dta = getattr(news_results, name)\n            orig_name = None\n            if hasattr(dta, 'name'):\n                orig_name = dta.name\n            dta = dta.multiply(endog_std, level=1)\n            if name not in ['news', 'revisions']:\n                dta = dta.add(endog_mean, level=1)\n            if orig_name is not None:\n                dta.name = orig_name\n            setattr(news_results, name, dta)\n        news_results.weights = news_results.weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n        news_results.revision_weights = news_results.revision_weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n    return news_results",
        "mutated": [
            "def news(self, comparison, impact_date=None, impacted_variable=None, start=None, end=None, periods=None, exog=None, comparison_type=None, revisions_details_start=False, state_index=None, return_raw=False, tolerance=1e-10, endog_quarterly=None, original_scale=True, **kwargs):\n    if False:\n        i = 10\n    '\\n        Compute impacts from updated data (news and revisions).\\n\\n        Parameters\\n        ----------\\n        comparison : array_like or MLEResults\\n            An updated dataset with updated and/or revised data from which the\\n            news can be computed, or an updated or previous results object\\n            to use in computing the news.\\n        impact_date : int, str, or datetime, optional\\n            A single specific period of impacts from news and revisions to\\n            compute. Can also be a date string to parse or a datetime type.\\n            This argument cannot be used in combination with `start`, `end`, or\\n            `periods`. Default is the first out-of-sample observation.\\n        impacted_variable : str, list, array, or slice, optional\\n            Observation variable label or slice of labels specifying that only\\n            specific impacted variables should be shown in the News output. The\\n            impacted variable(s) describe the variables that were *affected* by\\n            the news. If you do not know the labels for the variables, check\\n            the `endog_names` attribute of the model instance.\\n        start : int, str, or datetime, optional\\n            The first period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        end : int, str, or datetime, optional\\n            The last period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        periods : int, optional\\n            The number of periods of impacts from news and revisions to\\n            compute.\\n        exog : array_like, optional\\n            Array of exogenous regressors for the out-of-sample period, if\\n            applicable.\\n        comparison_type : {None, \\'previous\\', \\'updated\\'}\\n            This denotes whether the `comparison` argument represents a\\n            *previous* results object or dataset or an *updated* results object\\n            or dataset. If not specified, then an attempt is made to determine\\n            the comparison type.\\n        state_index : array_like or \"common\", optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned. If the string \"common\" is passed and the\\n            model includes idiosyncratic AR(1) components, news will only be\\n            computed based on the common states. Default is to use all states.\\n        return_raw : bool, optional\\n            Whether or not to return only the specific output or a full\\n            results object. Default is to return a full results object.\\n        tolerance : float, optional\\n            The numerical threshold for determining zero impact. Default is\\n            that any impact less than 1e-10 is assumed to be zero.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables, if `comparison` was\\n            provided as an updated monthly dataset. If this argument is\\n            provided, it must be a Pandas Series or DataFrame with a\\n            DatetimeIndex or PeriodIndex at the quarterly frequency.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n        .. [2] Ba\u0144bura, Marta, Domenico Giannone, and Lucrezia Reichlin.\\n               \"Nowcasting.\"\\n               The Oxford Handbook of Economic Forecasting. July 8, 2011.\\n        .. [3] Ba\u0144bura, Marta, Domenico Giannone, Michele Modugno, and Lucrezia\\n               Reichlin.\\n               \"Now-casting and the real-time data flow.\"\\n               In Handbook of economic forecasting, vol. 2, pp. 195-237.\\n               Elsevier, 2013.\\n        '\n    if state_index == 'common':\n        state_index = np.arange(self.model.k_states - self.model.k_endog)\n    news_results = super().news(comparison, impact_date=impact_date, impacted_variable=impacted_variable, start=start, end=end, periods=periods, exog=exog, comparison_type=comparison_type, revisions_details_start=revisions_details_start, state_index=state_index, return_raw=return_raw, tolerance=tolerance, endog_quarterly=endog_quarterly, **kwargs)\n    if not return_raw and self.model.standardize and original_scale:\n        endog_mean = self.model._endog_mean\n        endog_std = self.model._endog_std\n        news_results.total_impacts = news_results.total_impacts * endog_std\n        news_results.update_impacts = news_results.update_impacts * endog_std\n        if news_results.revision_impacts is not None:\n            news_results.revision_impacts = news_results.revision_impacts * endog_std\n        if news_results.revision_detailed_impacts is not None:\n            news_results.revision_detailed_impacts = news_results.revision_detailed_impacts * endog_std\n        if news_results.revision_grouped_impacts is not None:\n            news_results.revision_grouped_impacts = news_results.revision_grouped_impacts * endog_std\n        for name in ['prev_impacted_forecasts', 'news', 'revisions', 'update_realized', 'update_forecasts', 'revised', 'revised_prev', 'post_impacted_forecasts', 'revisions_all', 'revised_all', 'revised_prev_all']:\n            dta = getattr(news_results, name)\n            orig_name = None\n            if hasattr(dta, 'name'):\n                orig_name = dta.name\n            dta = dta.multiply(endog_std, level=1)\n            if name not in ['news', 'revisions']:\n                dta = dta.add(endog_mean, level=1)\n            if orig_name is not None:\n                dta.name = orig_name\n            setattr(news_results, name, dta)\n        news_results.weights = news_results.weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n        news_results.revision_weights = news_results.revision_weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n    return news_results",
            "def news(self, comparison, impact_date=None, impacted_variable=None, start=None, end=None, periods=None, exog=None, comparison_type=None, revisions_details_start=False, state_index=None, return_raw=False, tolerance=1e-10, endog_quarterly=None, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute impacts from updated data (news and revisions).\\n\\n        Parameters\\n        ----------\\n        comparison : array_like or MLEResults\\n            An updated dataset with updated and/or revised data from which the\\n            news can be computed, or an updated or previous results object\\n            to use in computing the news.\\n        impact_date : int, str, or datetime, optional\\n            A single specific period of impacts from news and revisions to\\n            compute. Can also be a date string to parse or a datetime type.\\n            This argument cannot be used in combination with `start`, `end`, or\\n            `periods`. Default is the first out-of-sample observation.\\n        impacted_variable : str, list, array, or slice, optional\\n            Observation variable label or slice of labels specifying that only\\n            specific impacted variables should be shown in the News output. The\\n            impacted variable(s) describe the variables that were *affected* by\\n            the news. If you do not know the labels for the variables, check\\n            the `endog_names` attribute of the model instance.\\n        start : int, str, or datetime, optional\\n            The first period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        end : int, str, or datetime, optional\\n            The last period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        periods : int, optional\\n            The number of periods of impacts from news and revisions to\\n            compute.\\n        exog : array_like, optional\\n            Array of exogenous regressors for the out-of-sample period, if\\n            applicable.\\n        comparison_type : {None, \\'previous\\', \\'updated\\'}\\n            This denotes whether the `comparison` argument represents a\\n            *previous* results object or dataset or an *updated* results object\\n            or dataset. If not specified, then an attempt is made to determine\\n            the comparison type.\\n        state_index : array_like or \"common\", optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned. If the string \"common\" is passed and the\\n            model includes idiosyncratic AR(1) components, news will only be\\n            computed based on the common states. Default is to use all states.\\n        return_raw : bool, optional\\n            Whether or not to return only the specific output or a full\\n            results object. Default is to return a full results object.\\n        tolerance : float, optional\\n            The numerical threshold for determining zero impact. Default is\\n            that any impact less than 1e-10 is assumed to be zero.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables, if `comparison` was\\n            provided as an updated monthly dataset. If this argument is\\n            provided, it must be a Pandas Series or DataFrame with a\\n            DatetimeIndex or PeriodIndex at the quarterly frequency.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n        .. [2] Ba\u0144bura, Marta, Domenico Giannone, and Lucrezia Reichlin.\\n               \"Nowcasting.\"\\n               The Oxford Handbook of Economic Forecasting. July 8, 2011.\\n        .. [3] Ba\u0144bura, Marta, Domenico Giannone, Michele Modugno, and Lucrezia\\n               Reichlin.\\n               \"Now-casting and the real-time data flow.\"\\n               In Handbook of economic forecasting, vol. 2, pp. 195-237.\\n               Elsevier, 2013.\\n        '\n    if state_index == 'common':\n        state_index = np.arange(self.model.k_states - self.model.k_endog)\n    news_results = super().news(comparison, impact_date=impact_date, impacted_variable=impacted_variable, start=start, end=end, periods=periods, exog=exog, comparison_type=comparison_type, revisions_details_start=revisions_details_start, state_index=state_index, return_raw=return_raw, tolerance=tolerance, endog_quarterly=endog_quarterly, **kwargs)\n    if not return_raw and self.model.standardize and original_scale:\n        endog_mean = self.model._endog_mean\n        endog_std = self.model._endog_std\n        news_results.total_impacts = news_results.total_impacts * endog_std\n        news_results.update_impacts = news_results.update_impacts * endog_std\n        if news_results.revision_impacts is not None:\n            news_results.revision_impacts = news_results.revision_impacts * endog_std\n        if news_results.revision_detailed_impacts is not None:\n            news_results.revision_detailed_impacts = news_results.revision_detailed_impacts * endog_std\n        if news_results.revision_grouped_impacts is not None:\n            news_results.revision_grouped_impacts = news_results.revision_grouped_impacts * endog_std\n        for name in ['prev_impacted_forecasts', 'news', 'revisions', 'update_realized', 'update_forecasts', 'revised', 'revised_prev', 'post_impacted_forecasts', 'revisions_all', 'revised_all', 'revised_prev_all']:\n            dta = getattr(news_results, name)\n            orig_name = None\n            if hasattr(dta, 'name'):\n                orig_name = dta.name\n            dta = dta.multiply(endog_std, level=1)\n            if name not in ['news', 'revisions']:\n                dta = dta.add(endog_mean, level=1)\n            if orig_name is not None:\n                dta.name = orig_name\n            setattr(news_results, name, dta)\n        news_results.weights = news_results.weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n        news_results.revision_weights = news_results.revision_weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n    return news_results",
            "def news(self, comparison, impact_date=None, impacted_variable=None, start=None, end=None, periods=None, exog=None, comparison_type=None, revisions_details_start=False, state_index=None, return_raw=False, tolerance=1e-10, endog_quarterly=None, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute impacts from updated data (news and revisions).\\n\\n        Parameters\\n        ----------\\n        comparison : array_like or MLEResults\\n            An updated dataset with updated and/or revised data from which the\\n            news can be computed, or an updated or previous results object\\n            to use in computing the news.\\n        impact_date : int, str, or datetime, optional\\n            A single specific period of impacts from news and revisions to\\n            compute. Can also be a date string to parse or a datetime type.\\n            This argument cannot be used in combination with `start`, `end`, or\\n            `periods`. Default is the first out-of-sample observation.\\n        impacted_variable : str, list, array, or slice, optional\\n            Observation variable label or slice of labels specifying that only\\n            specific impacted variables should be shown in the News output. The\\n            impacted variable(s) describe the variables that were *affected* by\\n            the news. If you do not know the labels for the variables, check\\n            the `endog_names` attribute of the model instance.\\n        start : int, str, or datetime, optional\\n            The first period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        end : int, str, or datetime, optional\\n            The last period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        periods : int, optional\\n            The number of periods of impacts from news and revisions to\\n            compute.\\n        exog : array_like, optional\\n            Array of exogenous regressors for the out-of-sample period, if\\n            applicable.\\n        comparison_type : {None, \\'previous\\', \\'updated\\'}\\n            This denotes whether the `comparison` argument represents a\\n            *previous* results object or dataset or an *updated* results object\\n            or dataset. If not specified, then an attempt is made to determine\\n            the comparison type.\\n        state_index : array_like or \"common\", optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned. If the string \"common\" is passed and the\\n            model includes idiosyncratic AR(1) components, news will only be\\n            computed based on the common states. Default is to use all states.\\n        return_raw : bool, optional\\n            Whether or not to return only the specific output or a full\\n            results object. Default is to return a full results object.\\n        tolerance : float, optional\\n            The numerical threshold for determining zero impact. Default is\\n            that any impact less than 1e-10 is assumed to be zero.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables, if `comparison` was\\n            provided as an updated monthly dataset. If this argument is\\n            provided, it must be a Pandas Series or DataFrame with a\\n            DatetimeIndex or PeriodIndex at the quarterly frequency.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n        .. [2] Ba\u0144bura, Marta, Domenico Giannone, and Lucrezia Reichlin.\\n               \"Nowcasting.\"\\n               The Oxford Handbook of Economic Forecasting. July 8, 2011.\\n        .. [3] Ba\u0144bura, Marta, Domenico Giannone, Michele Modugno, and Lucrezia\\n               Reichlin.\\n               \"Now-casting and the real-time data flow.\"\\n               In Handbook of economic forecasting, vol. 2, pp. 195-237.\\n               Elsevier, 2013.\\n        '\n    if state_index == 'common':\n        state_index = np.arange(self.model.k_states - self.model.k_endog)\n    news_results = super().news(comparison, impact_date=impact_date, impacted_variable=impacted_variable, start=start, end=end, periods=periods, exog=exog, comparison_type=comparison_type, revisions_details_start=revisions_details_start, state_index=state_index, return_raw=return_raw, tolerance=tolerance, endog_quarterly=endog_quarterly, **kwargs)\n    if not return_raw and self.model.standardize and original_scale:\n        endog_mean = self.model._endog_mean\n        endog_std = self.model._endog_std\n        news_results.total_impacts = news_results.total_impacts * endog_std\n        news_results.update_impacts = news_results.update_impacts * endog_std\n        if news_results.revision_impacts is not None:\n            news_results.revision_impacts = news_results.revision_impacts * endog_std\n        if news_results.revision_detailed_impacts is not None:\n            news_results.revision_detailed_impacts = news_results.revision_detailed_impacts * endog_std\n        if news_results.revision_grouped_impacts is not None:\n            news_results.revision_grouped_impacts = news_results.revision_grouped_impacts * endog_std\n        for name in ['prev_impacted_forecasts', 'news', 'revisions', 'update_realized', 'update_forecasts', 'revised', 'revised_prev', 'post_impacted_forecasts', 'revisions_all', 'revised_all', 'revised_prev_all']:\n            dta = getattr(news_results, name)\n            orig_name = None\n            if hasattr(dta, 'name'):\n                orig_name = dta.name\n            dta = dta.multiply(endog_std, level=1)\n            if name not in ['news', 'revisions']:\n                dta = dta.add(endog_mean, level=1)\n            if orig_name is not None:\n                dta.name = orig_name\n            setattr(news_results, name, dta)\n        news_results.weights = news_results.weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n        news_results.revision_weights = news_results.revision_weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n    return news_results",
            "def news(self, comparison, impact_date=None, impacted_variable=None, start=None, end=None, periods=None, exog=None, comparison_type=None, revisions_details_start=False, state_index=None, return_raw=False, tolerance=1e-10, endog_quarterly=None, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute impacts from updated data (news and revisions).\\n\\n        Parameters\\n        ----------\\n        comparison : array_like or MLEResults\\n            An updated dataset with updated and/or revised data from which the\\n            news can be computed, or an updated or previous results object\\n            to use in computing the news.\\n        impact_date : int, str, or datetime, optional\\n            A single specific period of impacts from news and revisions to\\n            compute. Can also be a date string to parse or a datetime type.\\n            This argument cannot be used in combination with `start`, `end`, or\\n            `periods`. Default is the first out-of-sample observation.\\n        impacted_variable : str, list, array, or slice, optional\\n            Observation variable label or slice of labels specifying that only\\n            specific impacted variables should be shown in the News output. The\\n            impacted variable(s) describe the variables that were *affected* by\\n            the news. If you do not know the labels for the variables, check\\n            the `endog_names` attribute of the model instance.\\n        start : int, str, or datetime, optional\\n            The first period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        end : int, str, or datetime, optional\\n            The last period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        periods : int, optional\\n            The number of periods of impacts from news and revisions to\\n            compute.\\n        exog : array_like, optional\\n            Array of exogenous regressors for the out-of-sample period, if\\n            applicable.\\n        comparison_type : {None, \\'previous\\', \\'updated\\'}\\n            This denotes whether the `comparison` argument represents a\\n            *previous* results object or dataset or an *updated* results object\\n            or dataset. If not specified, then an attempt is made to determine\\n            the comparison type.\\n        state_index : array_like or \"common\", optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned. If the string \"common\" is passed and the\\n            model includes idiosyncratic AR(1) components, news will only be\\n            computed based on the common states. Default is to use all states.\\n        return_raw : bool, optional\\n            Whether or not to return only the specific output or a full\\n            results object. Default is to return a full results object.\\n        tolerance : float, optional\\n            The numerical threshold for determining zero impact. Default is\\n            that any impact less than 1e-10 is assumed to be zero.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables, if `comparison` was\\n            provided as an updated monthly dataset. If this argument is\\n            provided, it must be a Pandas Series or DataFrame with a\\n            DatetimeIndex or PeriodIndex at the quarterly frequency.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n        .. [2] Ba\u0144bura, Marta, Domenico Giannone, and Lucrezia Reichlin.\\n               \"Nowcasting.\"\\n               The Oxford Handbook of Economic Forecasting. July 8, 2011.\\n        .. [3] Ba\u0144bura, Marta, Domenico Giannone, Michele Modugno, and Lucrezia\\n               Reichlin.\\n               \"Now-casting and the real-time data flow.\"\\n               In Handbook of economic forecasting, vol. 2, pp. 195-237.\\n               Elsevier, 2013.\\n        '\n    if state_index == 'common':\n        state_index = np.arange(self.model.k_states - self.model.k_endog)\n    news_results = super().news(comparison, impact_date=impact_date, impacted_variable=impacted_variable, start=start, end=end, periods=periods, exog=exog, comparison_type=comparison_type, revisions_details_start=revisions_details_start, state_index=state_index, return_raw=return_raw, tolerance=tolerance, endog_quarterly=endog_quarterly, **kwargs)\n    if not return_raw and self.model.standardize and original_scale:\n        endog_mean = self.model._endog_mean\n        endog_std = self.model._endog_std\n        news_results.total_impacts = news_results.total_impacts * endog_std\n        news_results.update_impacts = news_results.update_impacts * endog_std\n        if news_results.revision_impacts is not None:\n            news_results.revision_impacts = news_results.revision_impacts * endog_std\n        if news_results.revision_detailed_impacts is not None:\n            news_results.revision_detailed_impacts = news_results.revision_detailed_impacts * endog_std\n        if news_results.revision_grouped_impacts is not None:\n            news_results.revision_grouped_impacts = news_results.revision_grouped_impacts * endog_std\n        for name in ['prev_impacted_forecasts', 'news', 'revisions', 'update_realized', 'update_forecasts', 'revised', 'revised_prev', 'post_impacted_forecasts', 'revisions_all', 'revised_all', 'revised_prev_all']:\n            dta = getattr(news_results, name)\n            orig_name = None\n            if hasattr(dta, 'name'):\n                orig_name = dta.name\n            dta = dta.multiply(endog_std, level=1)\n            if name not in ['news', 'revisions']:\n                dta = dta.add(endog_mean, level=1)\n            if orig_name is not None:\n                dta.name = orig_name\n            setattr(news_results, name, dta)\n        news_results.weights = news_results.weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n        news_results.revision_weights = news_results.revision_weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n    return news_results",
            "def news(self, comparison, impact_date=None, impacted_variable=None, start=None, end=None, periods=None, exog=None, comparison_type=None, revisions_details_start=False, state_index=None, return_raw=False, tolerance=1e-10, endog_quarterly=None, original_scale=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute impacts from updated data (news and revisions).\\n\\n        Parameters\\n        ----------\\n        comparison : array_like or MLEResults\\n            An updated dataset with updated and/or revised data from which the\\n            news can be computed, or an updated or previous results object\\n            to use in computing the news.\\n        impact_date : int, str, or datetime, optional\\n            A single specific period of impacts from news and revisions to\\n            compute. Can also be a date string to parse or a datetime type.\\n            This argument cannot be used in combination with `start`, `end`, or\\n            `periods`. Default is the first out-of-sample observation.\\n        impacted_variable : str, list, array, or slice, optional\\n            Observation variable label or slice of labels specifying that only\\n            specific impacted variables should be shown in the News output. The\\n            impacted variable(s) describe the variables that were *affected* by\\n            the news. If you do not know the labels for the variables, check\\n            the `endog_names` attribute of the model instance.\\n        start : int, str, or datetime, optional\\n            The first period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        end : int, str, or datetime, optional\\n            The last period of impacts from news and revisions to compute.\\n            Can also be a date string to parse or a datetime type. Default is\\n            the first out-of-sample observation.\\n        periods : int, optional\\n            The number of periods of impacts from news and revisions to\\n            compute.\\n        exog : array_like, optional\\n            Array of exogenous regressors for the out-of-sample period, if\\n            applicable.\\n        comparison_type : {None, \\'previous\\', \\'updated\\'}\\n            This denotes whether the `comparison` argument represents a\\n            *previous* results object or dataset or an *updated* results object\\n            or dataset. If not specified, then an attempt is made to determine\\n            the comparison type.\\n        state_index : array_like or \"common\", optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned. If the string \"common\" is passed and the\\n            model includes idiosyncratic AR(1) components, news will only be\\n            computed based on the common states. Default is to use all states.\\n        return_raw : bool, optional\\n            Whether or not to return only the specific output or a full\\n            results object. Default is to return a full results object.\\n        tolerance : float, optional\\n            The numerical threshold for determining zero impact. Default is\\n            that any impact less than 1e-10 is assumed to be zero.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables, if `comparison` was\\n            provided as an updated monthly dataset. If this argument is\\n            provided, it must be a Pandas Series or DataFrame with a\\n            DatetimeIndex or PeriodIndex at the quarterly frequency.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n        .. [2] Ba\u0144bura, Marta, Domenico Giannone, and Lucrezia Reichlin.\\n               \"Nowcasting.\"\\n               The Oxford Handbook of Economic Forecasting. July 8, 2011.\\n        .. [3] Ba\u0144bura, Marta, Domenico Giannone, Michele Modugno, and Lucrezia\\n               Reichlin.\\n               \"Now-casting and the real-time data flow.\"\\n               In Handbook of economic forecasting, vol. 2, pp. 195-237.\\n               Elsevier, 2013.\\n        '\n    if state_index == 'common':\n        state_index = np.arange(self.model.k_states - self.model.k_endog)\n    news_results = super().news(comparison, impact_date=impact_date, impacted_variable=impacted_variable, start=start, end=end, periods=periods, exog=exog, comparison_type=comparison_type, revisions_details_start=revisions_details_start, state_index=state_index, return_raw=return_raw, tolerance=tolerance, endog_quarterly=endog_quarterly, **kwargs)\n    if not return_raw and self.model.standardize and original_scale:\n        endog_mean = self.model._endog_mean\n        endog_std = self.model._endog_std\n        news_results.total_impacts = news_results.total_impacts * endog_std\n        news_results.update_impacts = news_results.update_impacts * endog_std\n        if news_results.revision_impacts is not None:\n            news_results.revision_impacts = news_results.revision_impacts * endog_std\n        if news_results.revision_detailed_impacts is not None:\n            news_results.revision_detailed_impacts = news_results.revision_detailed_impacts * endog_std\n        if news_results.revision_grouped_impacts is not None:\n            news_results.revision_grouped_impacts = news_results.revision_grouped_impacts * endog_std\n        for name in ['prev_impacted_forecasts', 'news', 'revisions', 'update_realized', 'update_forecasts', 'revised', 'revised_prev', 'post_impacted_forecasts', 'revisions_all', 'revised_all', 'revised_prev_all']:\n            dta = getattr(news_results, name)\n            orig_name = None\n            if hasattr(dta, 'name'):\n                orig_name = dta.name\n            dta = dta.multiply(endog_std, level=1)\n            if name not in ['news', 'revisions']:\n                dta = dta.add(endog_mean, level=1)\n            if orig_name is not None:\n                dta.name = orig_name\n            setattr(news_results, name, dta)\n        news_results.weights = news_results.weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n        news_results.revision_weights = news_results.revision_weights.divide(endog_std, axis=0, level=1).multiply(endog_std, axis=1, level=1)\n    return news_results"
        ]
    },
    {
        "func_name": "get_smoothed_decomposition",
        "original": "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None, original_scale=True):\n    \"\"\"\n        Decompose smoothed output into contributions from observations\n\n        Parameters\n        ----------\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\n            The object to perform a decomposition of. If it is set to\n            \"smoothed_state\", then the elements of the smoothed state vector\n            are decomposed into the contributions of each observation. If it\n            is set to \"smoothed_signal\", then the predictions of the\n            observation vector based on the smoothed state vector are\n            decomposed. Default is \"smoothed_state\".\n        state_index : array_like, optional\n            An optional index specifying a subset of states to use when\n            constructing the decomposition of the \"smoothed_signal\". For\n            example, if `state_index=[0, 1]` is passed, then only the\n            contributions of observed variables to the smoothed signal arising\n            from the first two states will be returned. Note that if not all\n            states are used, the contributions will not sum to the smoothed\n            signal. Default is to use all states.\n        original_scale : bool, optional\n            If the model specification standardized the data, whether or not\n            to return simulations in the original scale of the data (i.e.\n            before it was standardized by the model). Default is True.\n\n        Returns\n        -------\n        data_contributions : pd.DataFrame\n            Contributions of observations to the decomposed object. If the\n            smoothed state is being decomposed, then `data_contributions` is\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\n            columns corresponding to `variable_from x date_from`. If the\n            smoothed signal is being decomposed, then `data_contributions` is\n            shaped `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\n            corresponding to `variable_to x date_to` and\n            `variable_from x date_from`.\n        obs_intercept_contributions : pd.DataFrame\n            Contributions of the observation intercept to the decomposed\n            object. If the smoothed state is being decomposed, then\n            `obs_intercept_contributions` is\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\n            columns corresponding to `obs_intercept_from x date_from`. If the\n            smoothed signal is being decomposed, then\n            `obs_intercept_contributions` is shaped\n            `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\n            corresponding to `variable_to x date_to` and\n            `obs_intercept_from x date_from`.\n        state_intercept_contributions : pd.DataFrame\n            Contributions of the state intercept to the decomposed\n            object. If the smoothed state is being decomposed, then\n            `state_intercept_contributions` is\n            shaped `(k_states x nobs, k_states x nobs)` with a `pd.MultiIndex`\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\n            columns corresponding to `state_intercept_from x date_from`. If the\n            smoothed signal is being decomposed, then\n            `state_intercept_contributions` is shaped\n            `(k_endog x nobs, k_states x nobs)` with `pd.MultiIndex`-es\n            corresponding to `variable_to x date_to` and\n            `state_intercept_from x date_from`.\n        prior_contributions : pd.DataFrame\n            Contributions of the prior to the decomposed object. If the\n            smoothed state is being decomposed, then `prior_contributions` is\n            shaped `(nobs x k_states, k_states)`, with a `pd.MultiIndex`\n            index corresponding to `state_to x date_to` and columns\n            corresponding to elements of the prior mean (aka \"initial state\").\n            If the smoothed signal is being decomposed, then\n            `prior_contributions` is shaped `(nobs x k_endog, k_states)`,\n            with a `pd.MultiIndex` index corresponding to\n            `variable_to x date_to` and columns corresponding to elements of\n            the prior mean.\n\n        Notes\n        -----\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\n        design matrix operative at time :math:`t`.\n        \"\"\"\n    if self.model.standardize and original_scale:\n        cache_obs_intercept = self.model['obs_intercept']\n        self.model['obs_intercept'] = self.model._endog_mean\n    (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions) = super().get_smoothed_decomposition(decomposition_of=decomposition_of, state_index=state_index)\n    if self.model.standardize and original_scale:\n        self.model['obs_intercept'] = cache_obs_intercept\n    if decomposition_of == 'smoothed_signal' and self.model.standardize and original_scale:\n        endog_std = self.model._endog_std\n        data_contributions = data_contributions.multiply(endog_std, axis=0, level=0)\n        obs_intercept_contributions = obs_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        state_intercept_contributions = state_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        prior_contributions = prior_contributions.multiply(endog_std, axis=0, level=0)\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
        "mutated": [
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None, original_scale=True):\n    if False:\n        i = 10\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        data_contributions : pd.DataFrame\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `variable_from x date_from`. If the\\n            smoothed signal is being decomposed, then `data_contributions` is\\n            shaped `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `variable_from x date_from`.\\n        obs_intercept_contributions : pd.DataFrame\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `obs_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `obs_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `obs_intercept_from x date_from`.\\n        state_intercept_contributions : pd.DataFrame\\n            Contributions of the state intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `state_intercept_contributions` is\\n            shaped `(k_states x nobs, k_states x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `state_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `state_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_states x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `state_intercept_from x date_from`.\\n        prior_contributions : pd.DataFrame\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` is\\n            shaped `(nobs x k_states, k_states)`, with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and columns\\n            corresponding to elements of the prior mean (aka \"initial state\").\\n            If the smoothed signal is being decomposed, then\\n            `prior_contributions` is shaped `(nobs x k_endog, k_states)`,\\n            with a `pd.MultiIndex` index corresponding to\\n            `variable_to x date_to` and columns corresponding to elements of\\n            the prior mean.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if self.model.standardize and original_scale:\n        cache_obs_intercept = self.model['obs_intercept']\n        self.model['obs_intercept'] = self.model._endog_mean\n    (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions) = super().get_smoothed_decomposition(decomposition_of=decomposition_of, state_index=state_index)\n    if self.model.standardize and original_scale:\n        self.model['obs_intercept'] = cache_obs_intercept\n    if decomposition_of == 'smoothed_signal' and self.model.standardize and original_scale:\n        endog_std = self.model._endog_std\n        data_contributions = data_contributions.multiply(endog_std, axis=0, level=0)\n        obs_intercept_contributions = obs_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        state_intercept_contributions = state_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        prior_contributions = prior_contributions.multiply(endog_std, axis=0, level=0)\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None, original_scale=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        data_contributions : pd.DataFrame\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `variable_from x date_from`. If the\\n            smoothed signal is being decomposed, then `data_contributions` is\\n            shaped `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `variable_from x date_from`.\\n        obs_intercept_contributions : pd.DataFrame\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `obs_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `obs_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `obs_intercept_from x date_from`.\\n        state_intercept_contributions : pd.DataFrame\\n            Contributions of the state intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `state_intercept_contributions` is\\n            shaped `(k_states x nobs, k_states x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `state_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `state_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_states x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `state_intercept_from x date_from`.\\n        prior_contributions : pd.DataFrame\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` is\\n            shaped `(nobs x k_states, k_states)`, with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and columns\\n            corresponding to elements of the prior mean (aka \"initial state\").\\n            If the smoothed signal is being decomposed, then\\n            `prior_contributions` is shaped `(nobs x k_endog, k_states)`,\\n            with a `pd.MultiIndex` index corresponding to\\n            `variable_to x date_to` and columns corresponding to elements of\\n            the prior mean.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if self.model.standardize and original_scale:\n        cache_obs_intercept = self.model['obs_intercept']\n        self.model['obs_intercept'] = self.model._endog_mean\n    (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions) = super().get_smoothed_decomposition(decomposition_of=decomposition_of, state_index=state_index)\n    if self.model.standardize and original_scale:\n        self.model['obs_intercept'] = cache_obs_intercept\n    if decomposition_of == 'smoothed_signal' and self.model.standardize and original_scale:\n        endog_std = self.model._endog_std\n        data_contributions = data_contributions.multiply(endog_std, axis=0, level=0)\n        obs_intercept_contributions = obs_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        state_intercept_contributions = state_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        prior_contributions = prior_contributions.multiply(endog_std, axis=0, level=0)\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None, original_scale=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        data_contributions : pd.DataFrame\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `variable_from x date_from`. If the\\n            smoothed signal is being decomposed, then `data_contributions` is\\n            shaped `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `variable_from x date_from`.\\n        obs_intercept_contributions : pd.DataFrame\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `obs_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `obs_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `obs_intercept_from x date_from`.\\n        state_intercept_contributions : pd.DataFrame\\n            Contributions of the state intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `state_intercept_contributions` is\\n            shaped `(k_states x nobs, k_states x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `state_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `state_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_states x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `state_intercept_from x date_from`.\\n        prior_contributions : pd.DataFrame\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` is\\n            shaped `(nobs x k_states, k_states)`, with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and columns\\n            corresponding to elements of the prior mean (aka \"initial state\").\\n            If the smoothed signal is being decomposed, then\\n            `prior_contributions` is shaped `(nobs x k_endog, k_states)`,\\n            with a `pd.MultiIndex` index corresponding to\\n            `variable_to x date_to` and columns corresponding to elements of\\n            the prior mean.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if self.model.standardize and original_scale:\n        cache_obs_intercept = self.model['obs_intercept']\n        self.model['obs_intercept'] = self.model._endog_mean\n    (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions) = super().get_smoothed_decomposition(decomposition_of=decomposition_of, state_index=state_index)\n    if self.model.standardize and original_scale:\n        self.model['obs_intercept'] = cache_obs_intercept\n    if decomposition_of == 'smoothed_signal' and self.model.standardize and original_scale:\n        endog_std = self.model._endog_std\n        data_contributions = data_contributions.multiply(endog_std, axis=0, level=0)\n        obs_intercept_contributions = obs_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        state_intercept_contributions = state_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        prior_contributions = prior_contributions.multiply(endog_std, axis=0, level=0)\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None, original_scale=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        data_contributions : pd.DataFrame\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `variable_from x date_from`. If the\\n            smoothed signal is being decomposed, then `data_contributions` is\\n            shaped `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `variable_from x date_from`.\\n        obs_intercept_contributions : pd.DataFrame\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `obs_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `obs_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `obs_intercept_from x date_from`.\\n        state_intercept_contributions : pd.DataFrame\\n            Contributions of the state intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `state_intercept_contributions` is\\n            shaped `(k_states x nobs, k_states x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `state_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `state_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_states x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `state_intercept_from x date_from`.\\n        prior_contributions : pd.DataFrame\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` is\\n            shaped `(nobs x k_states, k_states)`, with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and columns\\n            corresponding to elements of the prior mean (aka \"initial state\").\\n            If the smoothed signal is being decomposed, then\\n            `prior_contributions` is shaped `(nobs x k_endog, k_states)`,\\n            with a `pd.MultiIndex` index corresponding to\\n            `variable_to x date_to` and columns corresponding to elements of\\n            the prior mean.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if self.model.standardize and original_scale:\n        cache_obs_intercept = self.model['obs_intercept']\n        self.model['obs_intercept'] = self.model._endog_mean\n    (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions) = super().get_smoothed_decomposition(decomposition_of=decomposition_of, state_index=state_index)\n    if self.model.standardize and original_scale:\n        self.model['obs_intercept'] = cache_obs_intercept\n    if decomposition_of == 'smoothed_signal' and self.model.standardize and original_scale:\n        endog_std = self.model._endog_std\n        data_contributions = data_contributions.multiply(endog_std, axis=0, level=0)\n        obs_intercept_contributions = obs_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        state_intercept_contributions = state_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        prior_contributions = prior_contributions.multiply(endog_std, axis=0, level=0)\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None, original_scale=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n        original_scale : bool, optional\\n            If the model specification standardized the data, whether or not\\n            to return simulations in the original scale of the data (i.e.\\n            before it was standardized by the model). Default is True.\\n\\n        Returns\\n        -------\\n        data_contributions : pd.DataFrame\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `variable_from x date_from`. If the\\n            smoothed signal is being decomposed, then `data_contributions` is\\n            shaped `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `variable_from x date_from`.\\n        obs_intercept_contributions : pd.DataFrame\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` is\\n            shaped `(k_states x nobs, k_endog x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `obs_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `obs_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_endog x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `obs_intercept_from x date_from`.\\n        state_intercept_contributions : pd.DataFrame\\n            Contributions of the state intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `state_intercept_contributions` is\\n            shaped `(k_states x nobs, k_states x nobs)` with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and `pd.MultiIndex`\\n            columns corresponding to `state_intercept_from x date_from`. If the\\n            smoothed signal is being decomposed, then\\n            `state_intercept_contributions` is shaped\\n            `(k_endog x nobs, k_states x nobs)` with `pd.MultiIndex`-es\\n            corresponding to `variable_to x date_to` and\\n            `state_intercept_from x date_from`.\\n        prior_contributions : pd.DataFrame\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` is\\n            shaped `(nobs x k_states, k_states)`, with a `pd.MultiIndex`\\n            index corresponding to `state_to x date_to` and columns\\n            corresponding to elements of the prior mean (aka \"initial state\").\\n            If the smoothed signal is being decomposed, then\\n            `prior_contributions` is shaped `(nobs x k_endog, k_states)`,\\n            with a `pd.MultiIndex` index corresponding to\\n            `variable_to x date_to` and columns corresponding to elements of\\n            the prior mean.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if self.model.standardize and original_scale:\n        cache_obs_intercept = self.model['obs_intercept']\n        self.model['obs_intercept'] = self.model._endog_mean\n    (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions) = super().get_smoothed_decomposition(decomposition_of=decomposition_of, state_index=state_index)\n    if self.model.standardize and original_scale:\n        self.model['obs_intercept'] = cache_obs_intercept\n    if decomposition_of == 'smoothed_signal' and self.model.standardize and original_scale:\n        endog_std = self.model._endog_std\n        data_contributions = data_contributions.multiply(endog_std, axis=0, level=0)\n        obs_intercept_contributions = obs_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        state_intercept_contributions = state_intercept_contributions.multiply(endog_std, axis=0, level=0)\n        prior_contributions = prior_contributions.multiply(endog_std, axis=0, level=0)\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)"
        ]
    },
    {
        "func_name": "append",
        "original": "def append(self, endog, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=True, retain_standardization=True, **kwargs):\n    \"\"\"\n        Recreate the results object with new data appended to original data.\n\n        Creates a new result object applied to a dataset that is created by\n        appending new data to the end of the model's original data. The new\n        results can then be used for analysis or forecasting.\n\n        Parameters\n        ----------\n        endog : array_like\n            New observations from the modeled time-series process.\n        endog_quarterly : array_like, optional\n            New observations of quarterly variables. If provided, must be a\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\n            the quarterly frequency.\n        refit : bool, optional\n            Whether to re-fit the parameters, based on the combined dataset.\n            Default is False (so parameters from the current results object\n            are used to create the new results object).\n        fit_kwargs : dict, optional\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\n            `smooth`.\n        copy_initialization : bool, optional\n            Whether or not to copy the initialization from the current results\n            set to the new model. Default is True.\n        retain_standardization : bool, optional\n            Whether or not to use the mean and standard deviations that were\n            used to standardize the data in the current model in the new model.\n            Default is True.\n        **kwargs\n            Keyword arguments may be used to modify model specification\n            arguments when created the new model object.\n\n        Returns\n        -------\n        results\n            Updated Results object, that includes results from both the\n            original dataset and the new dataset.\n\n        Notes\n        -----\n        The `endog` and `exog` arguments to this method must be formatted in\n        the same way (e.g. Pandas Series versus Numpy array) as were the\n        `endog` and `exog` arrays passed to the original model.\n\n        The `endog` (and, if applicable, `endog_quarterly`) arguments to this\n        method should consist of new observations that occurred directly after\n        the last element of `endog`. For any other kind of dataset, see the\n        `apply` method.\n\n        This method will apply filtering to all of the original data as well\n        as to the new data. To apply filtering only to the new data (which\n        can be much faster if the original dataset is large), see the `extend`\n        method.\n\n        See Also\n        --------\n        extend\n        apply\n        \"\"\"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().append(endog, refit=refit, fit_kwargs=fit_kwargs, copy_initialization=copy_initialization, retain_standardization=retain_standardization, **kwargs)",
        "mutated": [
            "def append(self, endog, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=True, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n    \"\\n        Recreate the results object with new data appended to original data.\\n\\n        Creates a new result object applied to a dataset that is created by\\n        appending new data to the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, based on the combined dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is True.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results from both the\\n            original dataset and the new dataset.\\n\\n        Notes\\n        -----\\n        The `endog` and `exog` arguments to this method must be formatted in\\n        the same way (e.g. Pandas Series versus Numpy array) as were the\\n        `endog` and `exog` arrays passed to the original model.\\n\\n        The `endog` (and, if applicable, `endog_quarterly`) arguments to this\\n        method should consist of new observations that occurred directly after\\n        the last element of `endog`. For any other kind of dataset, see the\\n        `apply` method.\\n\\n        This method will apply filtering to all of the original data as well\\n        as to the new data. To apply filtering only to the new data (which\\n        can be much faster if the original dataset is large), see the `extend`\\n        method.\\n\\n        See Also\\n        --------\\n        extend\\n        apply\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().append(endog, refit=refit, fit_kwargs=fit_kwargs, copy_initialization=copy_initialization, retain_standardization=retain_standardization, **kwargs)",
            "def append(self, endog, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=True, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Recreate the results object with new data appended to original data.\\n\\n        Creates a new result object applied to a dataset that is created by\\n        appending new data to the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, based on the combined dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is True.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results from both the\\n            original dataset and the new dataset.\\n\\n        Notes\\n        -----\\n        The `endog` and `exog` arguments to this method must be formatted in\\n        the same way (e.g. Pandas Series versus Numpy array) as were the\\n        `endog` and `exog` arrays passed to the original model.\\n\\n        The `endog` (and, if applicable, `endog_quarterly`) arguments to this\\n        method should consist of new observations that occurred directly after\\n        the last element of `endog`. For any other kind of dataset, see the\\n        `apply` method.\\n\\n        This method will apply filtering to all of the original data as well\\n        as to the new data. To apply filtering only to the new data (which\\n        can be much faster if the original dataset is large), see the `extend`\\n        method.\\n\\n        See Also\\n        --------\\n        extend\\n        apply\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().append(endog, refit=refit, fit_kwargs=fit_kwargs, copy_initialization=copy_initialization, retain_standardization=retain_standardization, **kwargs)",
            "def append(self, endog, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=True, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Recreate the results object with new data appended to original data.\\n\\n        Creates a new result object applied to a dataset that is created by\\n        appending new data to the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, based on the combined dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is True.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results from both the\\n            original dataset and the new dataset.\\n\\n        Notes\\n        -----\\n        The `endog` and `exog` arguments to this method must be formatted in\\n        the same way (e.g. Pandas Series versus Numpy array) as were the\\n        `endog` and `exog` arrays passed to the original model.\\n\\n        The `endog` (and, if applicable, `endog_quarterly`) arguments to this\\n        method should consist of new observations that occurred directly after\\n        the last element of `endog`. For any other kind of dataset, see the\\n        `apply` method.\\n\\n        This method will apply filtering to all of the original data as well\\n        as to the new data. To apply filtering only to the new data (which\\n        can be much faster if the original dataset is large), see the `extend`\\n        method.\\n\\n        See Also\\n        --------\\n        extend\\n        apply\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().append(endog, refit=refit, fit_kwargs=fit_kwargs, copy_initialization=copy_initialization, retain_standardization=retain_standardization, **kwargs)",
            "def append(self, endog, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=True, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Recreate the results object with new data appended to original data.\\n\\n        Creates a new result object applied to a dataset that is created by\\n        appending new data to the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, based on the combined dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is True.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results from both the\\n            original dataset and the new dataset.\\n\\n        Notes\\n        -----\\n        The `endog` and `exog` arguments to this method must be formatted in\\n        the same way (e.g. Pandas Series versus Numpy array) as were the\\n        `endog` and `exog` arrays passed to the original model.\\n\\n        The `endog` (and, if applicable, `endog_quarterly`) arguments to this\\n        method should consist of new observations that occurred directly after\\n        the last element of `endog`. For any other kind of dataset, see the\\n        `apply` method.\\n\\n        This method will apply filtering to all of the original data as well\\n        as to the new data. To apply filtering only to the new data (which\\n        can be much faster if the original dataset is large), see the `extend`\\n        method.\\n\\n        See Also\\n        --------\\n        extend\\n        apply\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().append(endog, refit=refit, fit_kwargs=fit_kwargs, copy_initialization=copy_initialization, retain_standardization=retain_standardization, **kwargs)",
            "def append(self, endog, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=True, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Recreate the results object with new data appended to original data.\\n\\n        Creates a new result object applied to a dataset that is created by\\n        appending new data to the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, based on the combined dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is True.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results from both the\\n            original dataset and the new dataset.\\n\\n        Notes\\n        -----\\n        The `endog` and `exog` arguments to this method must be formatted in\\n        the same way (e.g. Pandas Series versus Numpy array) as were the\\n        `endog` and `exog` arrays passed to the original model.\\n\\n        The `endog` (and, if applicable, `endog_quarterly`) arguments to this\\n        method should consist of new observations that occurred directly after\\n        the last element of `endog`. For any other kind of dataset, see the\\n        `apply` method.\\n\\n        This method will apply filtering to all of the original data as well\\n        as to the new data. To apply filtering only to the new data (which\\n        can be much faster if the original dataset is large), see the `extend`\\n        method.\\n\\n        See Also\\n        --------\\n        extend\\n        apply\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().append(endog, refit=refit, fit_kwargs=fit_kwargs, copy_initialization=copy_initialization, retain_standardization=retain_standardization, **kwargs)"
        ]
    },
    {
        "func_name": "extend",
        "original": "def extend(self, endog, endog_quarterly=None, fit_kwargs=None, retain_standardization=True, **kwargs):\n    \"\"\"\n        Recreate the results object for new data that extends original data.\n\n        Creates a new result object applied to a new dataset that is assumed to\n        follow directly from the end of the model's original data. The new\n        results can then be used for analysis or forecasting.\n\n        Parameters\n        ----------\n        endog : array_like\n            New observations from the modeled time-series process.\n        endog_quarterly : array_like, optional\n            New observations of quarterly variables. If provided, must be a\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\n            the quarterly frequency.\n        fit_kwargs : dict, optional\n            Keyword arguments to pass to `filter` or `smooth`.\n        retain_standardization : bool, optional\n            Whether or not to use the mean and standard deviations that were\n            used to standardize the data in the current model in the new model.\n            Default is True.\n        **kwargs\n            Keyword arguments may be used to modify model specification\n            arguments when created the new model object.\n\n        Returns\n        -------\n        results\n            Updated Results object, that includes results only for the new\n            dataset.\n\n        See Also\n        --------\n        append\n        apply\n\n        Notes\n        -----\n        The `endog` argument to this method should consist of new observations\n        that occurred directly after the last element of the model's original\n        `endog` array. For any other kind of dataset, see the `apply` method.\n\n        This method will apply filtering only to the new data provided by the\n        `endog` argument, which can be much faster than re-filtering the entire\n        dataset. However, the returned results object will only have results\n        for the new data. To retrieve results for both the new data and the\n        original data, see the `append` method.\n        \"\"\"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().extend(endog, fit_kwargs=fit_kwargs, retain_standardization=retain_standardization, **kwargs)",
        "mutated": [
            "def extend(self, endog, endog_quarterly=None, fit_kwargs=None, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n    \"\\n        Recreate the results object for new data that extends original data.\\n\\n        Creates a new result object applied to a new dataset that is assumed to\\n        follow directly from the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `filter` or `smooth`.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        append\\n        apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that occurred directly after the last element of the model's original\\n        `endog` array. For any other kind of dataset, see the `apply` method.\\n\\n        This method will apply filtering only to the new data provided by the\\n        `endog` argument, which can be much faster than re-filtering the entire\\n        dataset. However, the returned results object will only have results\\n        for the new data. To retrieve results for both the new data and the\\n        original data, see the `append` method.\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().extend(endog, fit_kwargs=fit_kwargs, retain_standardization=retain_standardization, **kwargs)",
            "def extend(self, endog, endog_quarterly=None, fit_kwargs=None, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Recreate the results object for new data that extends original data.\\n\\n        Creates a new result object applied to a new dataset that is assumed to\\n        follow directly from the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `filter` or `smooth`.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        append\\n        apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that occurred directly after the last element of the model's original\\n        `endog` array. For any other kind of dataset, see the `apply` method.\\n\\n        This method will apply filtering only to the new data provided by the\\n        `endog` argument, which can be much faster than re-filtering the entire\\n        dataset. However, the returned results object will only have results\\n        for the new data. To retrieve results for both the new data and the\\n        original data, see the `append` method.\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().extend(endog, fit_kwargs=fit_kwargs, retain_standardization=retain_standardization, **kwargs)",
            "def extend(self, endog, endog_quarterly=None, fit_kwargs=None, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Recreate the results object for new data that extends original data.\\n\\n        Creates a new result object applied to a new dataset that is assumed to\\n        follow directly from the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `filter` or `smooth`.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        append\\n        apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that occurred directly after the last element of the model's original\\n        `endog` array. For any other kind of dataset, see the `apply` method.\\n\\n        This method will apply filtering only to the new data provided by the\\n        `endog` argument, which can be much faster than re-filtering the entire\\n        dataset. However, the returned results object will only have results\\n        for the new data. To retrieve results for both the new data and the\\n        original data, see the `append` method.\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().extend(endog, fit_kwargs=fit_kwargs, retain_standardization=retain_standardization, **kwargs)",
            "def extend(self, endog, endog_quarterly=None, fit_kwargs=None, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Recreate the results object for new data that extends original data.\\n\\n        Creates a new result object applied to a new dataset that is assumed to\\n        follow directly from the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `filter` or `smooth`.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        append\\n        apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that occurred directly after the last element of the model's original\\n        `endog` array. For any other kind of dataset, see the `apply` method.\\n\\n        This method will apply filtering only to the new data provided by the\\n        `endog` argument, which can be much faster than re-filtering the entire\\n        dataset. However, the returned results object will only have results\\n        for the new data. To retrieve results for both the new data and the\\n        original data, see the `append` method.\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().extend(endog, fit_kwargs=fit_kwargs, retain_standardization=retain_standardization, **kwargs)",
            "def extend(self, endog, endog_quarterly=None, fit_kwargs=None, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Recreate the results object for new data that extends original data.\\n\\n        Creates a new result object applied to a new dataset that is assumed to\\n        follow directly from the end of the model's original data. The new\\n        results can then be used for analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `filter` or `smooth`.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        append\\n        apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that occurred directly after the last element of the model's original\\n        `endog` array. For any other kind of dataset, see the `apply` method.\\n\\n        This method will apply filtering only to the new data provided by the\\n        `endog` argument, which can be much faster than re-filtering the entire\\n        dataset. However, the returned results object will only have results\\n        for the new data. To retrieve results for both the new data and the\\n        original data, see the `append` method.\\n        \"\n    (endog, k_endog_monthly) = DynamicFactorMQ.construct_endog(endog, endog_quarterly)\n    k_endog = endog.shape[1] if len(endog.shape) == 2 else 1\n    if k_endog_monthly != self.model.k_endog_M or k_endog != self.model.k_endog:\n        raise ValueError('Cannot append data of a different dimension to a model.')\n    kwargs['k_endog_monthly'] = k_endog_monthly\n    return super().extend(endog, fit_kwargs=fit_kwargs, retain_standardization=retain_standardization, **kwargs)"
        ]
    },
    {
        "func_name": "apply",
        "original": "def apply(self, endog, k_endog_monthly=None, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=False, retain_standardization=True, **kwargs):\n    \"\"\"\n        Apply the fitted parameters to new data unrelated to the original data.\n\n        Creates a new result object using the current fitted parameters,\n        applied to a completely new dataset that is assumed to be unrelated to\n        the model's original data. The new results can then be used for\n        analysis or forecasting.\n\n        Parameters\n        ----------\n        endog : array_like\n            New observations from the modeled time-series process.\n        k_endog_monthly : int, optional\n            If specifying a monthly/quarterly mixed frequency model in which\n            the provided `endog` dataset contains both the monthly and\n            quarterly data, this variable should be used to indicate how many\n            of the variables are monthly.\n        endog_quarterly : array_like, optional\n            New observations of quarterly variables. If provided, must be a\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\n            the quarterly frequency.\n        refit : bool, optional\n            Whether to re-fit the parameters, using the new dataset.\n            Default is False (so parameters from the current results object\n            are used to create the new results object).\n        fit_kwargs : dict, optional\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\n            `smooth`.\n        copy_initialization : bool, optional\n            Whether or not to copy the initialization from the current results\n            set to the new model. Default is False.\n        retain_standardization : bool, optional\n            Whether or not to use the mean and standard deviations that were\n            used to standardize the data in the current model in the new model.\n            Default is True.\n        **kwargs\n            Keyword arguments may be used to modify model specification\n            arguments when created the new model object.\n\n        Returns\n        -------\n        results\n            Updated Results object, that includes results only for the new\n            dataset.\n\n        See Also\n        --------\n        statsmodels.tsa.statespace.mlemodel.MLEResults.append\n        statsmodels.tsa.statespace.mlemodel.MLEResults.apply\n\n        Notes\n        -----\n        The `endog` argument to this method should consist of new observations\n        that are not necessarily related to the original model's `endog`\n        dataset. For observations that continue that original dataset by follow\n        directly after its last element, see the `append` and `extend` methods.\n        \"\"\"\n    mod = self.model.clone(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, retain_standardization=retain_standardization, **kwargs)\n    if copy_initialization:\n        init = initialization.Initialization.from_results(self.filter_results)\n        mod.ssm.initialization = init\n    res = self._apply(mod, refit=refit, fit_kwargs=fit_kwargs)\n    return res",
        "mutated": [
            "def apply(self, endog, k_endog_monthly=None, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=False, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n    \"\\n        Apply the fitted parameters to new data unrelated to the original data.\\n\\n        Creates a new result object using the current fitted parameters,\\n        applied to a completely new dataset that is assumed to be unrelated to\\n        the model's original data. The new results can then be used for\\n        analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, using the new dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is False.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.append\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that are not necessarily related to the original model's `endog`\\n        dataset. For observations that continue that original dataset by follow\\n        directly after its last element, see the `append` and `extend` methods.\\n        \"\n    mod = self.model.clone(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, retain_standardization=retain_standardization, **kwargs)\n    if copy_initialization:\n        init = initialization.Initialization.from_results(self.filter_results)\n        mod.ssm.initialization = init\n    res = self._apply(mod, refit=refit, fit_kwargs=fit_kwargs)\n    return res",
            "def apply(self, endog, k_endog_monthly=None, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=False, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Apply the fitted parameters to new data unrelated to the original data.\\n\\n        Creates a new result object using the current fitted parameters,\\n        applied to a completely new dataset that is assumed to be unrelated to\\n        the model's original data. The new results can then be used for\\n        analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, using the new dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is False.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.append\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that are not necessarily related to the original model's `endog`\\n        dataset. For observations that continue that original dataset by follow\\n        directly after its last element, see the `append` and `extend` methods.\\n        \"\n    mod = self.model.clone(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, retain_standardization=retain_standardization, **kwargs)\n    if copy_initialization:\n        init = initialization.Initialization.from_results(self.filter_results)\n        mod.ssm.initialization = init\n    res = self._apply(mod, refit=refit, fit_kwargs=fit_kwargs)\n    return res",
            "def apply(self, endog, k_endog_monthly=None, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=False, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Apply the fitted parameters to new data unrelated to the original data.\\n\\n        Creates a new result object using the current fitted parameters,\\n        applied to a completely new dataset that is assumed to be unrelated to\\n        the model's original data. The new results can then be used for\\n        analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, using the new dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is False.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.append\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that are not necessarily related to the original model's `endog`\\n        dataset. For observations that continue that original dataset by follow\\n        directly after its last element, see the `append` and `extend` methods.\\n        \"\n    mod = self.model.clone(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, retain_standardization=retain_standardization, **kwargs)\n    if copy_initialization:\n        init = initialization.Initialization.from_results(self.filter_results)\n        mod.ssm.initialization = init\n    res = self._apply(mod, refit=refit, fit_kwargs=fit_kwargs)\n    return res",
            "def apply(self, endog, k_endog_monthly=None, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=False, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Apply the fitted parameters to new data unrelated to the original data.\\n\\n        Creates a new result object using the current fitted parameters,\\n        applied to a completely new dataset that is assumed to be unrelated to\\n        the model's original data. The new results can then be used for\\n        analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, using the new dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is False.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.append\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that are not necessarily related to the original model's `endog`\\n        dataset. For observations that continue that original dataset by follow\\n        directly after its last element, see the `append` and `extend` methods.\\n        \"\n    mod = self.model.clone(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, retain_standardization=retain_standardization, **kwargs)\n    if copy_initialization:\n        init = initialization.Initialization.from_results(self.filter_results)\n        mod.ssm.initialization = init\n    res = self._apply(mod, refit=refit, fit_kwargs=fit_kwargs)\n    return res",
            "def apply(self, endog, k_endog_monthly=None, endog_quarterly=None, refit=False, fit_kwargs=None, copy_initialization=False, retain_standardization=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Apply the fitted parameters to new data unrelated to the original data.\\n\\n        Creates a new result object using the current fitted parameters,\\n        applied to a completely new dataset that is assumed to be unrelated to\\n        the model's original data. The new results can then be used for\\n        analysis or forecasting.\\n\\n        Parameters\\n        ----------\\n        endog : array_like\\n            New observations from the modeled time-series process.\\n        k_endog_monthly : int, optional\\n            If specifying a monthly/quarterly mixed frequency model in which\\n            the provided `endog` dataset contains both the monthly and\\n            quarterly data, this variable should be used to indicate how many\\n            of the variables are monthly.\\n        endog_quarterly : array_like, optional\\n            New observations of quarterly variables. If provided, must be a\\n            Pandas Series or DataFrame with a DatetimeIndex or PeriodIndex at\\n            the quarterly frequency.\\n        refit : bool, optional\\n            Whether to re-fit the parameters, using the new dataset.\\n            Default is False (so parameters from the current results object\\n            are used to create the new results object).\\n        fit_kwargs : dict, optional\\n            Keyword arguments to pass to `fit` (if `refit=True`) or `filter` /\\n            `smooth`.\\n        copy_initialization : bool, optional\\n            Whether or not to copy the initialization from the current results\\n            set to the new model. Default is False.\\n        retain_standardization : bool, optional\\n            Whether or not to use the mean and standard deviations that were\\n            used to standardize the data in the current model in the new model.\\n            Default is True.\\n        **kwargs\\n            Keyword arguments may be used to modify model specification\\n            arguments when created the new model object.\\n\\n        Returns\\n        -------\\n        results\\n            Updated Results object, that includes results only for the new\\n            dataset.\\n\\n        See Also\\n        --------\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.append\\n        statsmodels.tsa.statespace.mlemodel.MLEResults.apply\\n\\n        Notes\\n        -----\\n        The `endog` argument to this method should consist of new observations\\n        that are not necessarily related to the original model's `endog`\\n        dataset. For observations that continue that original dataset by follow\\n        directly after its last element, see the `append` and `extend` methods.\\n        \"\n    mod = self.model.clone(endog, k_endog_monthly=k_endog_monthly, endog_quarterly=endog_quarterly, retain_standardization=retain_standardization, **kwargs)\n    if copy_initialization:\n        init = initialization.Initialization.from_results(self.filter_results)\n        mod.ssm.initialization = init\n    res = self._apply(mod, refit=refit, fit_kwargs=fit_kwargs)\n    return res"
        ]
    },
    {
        "func_name": "summary",
        "original": "def summary(self, alpha=0.05, start=None, title=None, model_name=None, display_params=True, display_diagnostics=False, display_params_as_list=False, truncate_endog_names=None, display_max_endog=3):\n    \"\"\"\n        Summarize the Model.\n\n        Parameters\n        ----------\n        alpha : float, optional\n            Significance level for the confidence intervals. Default is 0.05.\n        start : int, optional\n            Integer of the start observation. Default is 0.\n        title : str, optional\n            The title used for the summary table.\n        model_name : str, optional\n            The name of the model used. Default is to use model class name.\n\n        Returns\n        -------\n        summary : Summary instance\n            This holds the summary table and text, which can be printed or\n            converted to various output formats.\n\n        See Also\n        --------\n        statsmodels.iolib.summary.Summary\n        \"\"\"\n    mod = self.model\n    if title is None:\n        title = 'Dynamic Factor Results'\n    if model_name is None:\n        model_name = self.model._model_name\n    endog_names = self.model._get_endog_names(truncate=truncate_endog_names)\n    extra_top_left = None\n    extra_top_right = []\n    mle_retvals = getattr(self, 'mle_retvals', None)\n    mle_settings = getattr(self, 'mle_settings', None)\n    if mle_settings is not None and mle_settings.method == 'em':\n        extra_top_right += [('EM Iterations', [f'{mle_retvals.iter}'])]\n    summary = super().summary(alpha=alpha, start=start, title=title, model_name=model_name, display_params=display_params and display_params_as_list, display_diagnostics=display_diagnostics, truncate_endog_names=truncate_endog_names, display_max_endog=display_max_endog, extra_top_left=extra_top_left, extra_top_right=extra_top_right)\n    table_ix = 1\n    if not display_params_as_list:\n        data = pd.DataFrame(self.filter_results.design[:, mod._s['factors_L1'], 0], index=endog_names, columns=mod.factor_names)\n        try:\n            data = data.map(lambda s: '%.2f' % s)\n        except AttributeError:\n            data = data.applymap(lambda s: '%.2f' % s)\n        k_idio = 1\n        if mod.idiosyncratic_ar1:\n            data['   idiosyncratic: AR(1)'] = self.params[mod._p['idiosyncratic_ar1']]\n            k_idio += 1\n        data['var.'] = self.params[mod._p['idiosyncratic_var']]\n        try:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].map(lambda s: f'{s:.2f}')\n        except AttributeError:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].applymap(lambda s: f'{s:.2f}')\n        data.index.name = 'Factor loadings:'\n        base_iloc = np.arange(mod.k_factors)\n        for i in range(mod.k_endog):\n            iloc = [j for j in base_iloc if j not in mod._s.endog_factor_iloc[i]]\n            data.iloc[i, iloc] = '.'\n        data = data.reset_index()\n        params_data = data.values\n        params_header = data.columns.tolist()\n        params_stubs = None\n        title = 'Observation equation:'\n        table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n        summary.tables.insert(table_ix, table)\n        table_ix += 1\n        ix1 = 0\n        ix2 = 0\n        for i in range(len(mod._s.factor_blocks)):\n            block = mod._s.factor_blocks[i]\n            ix2 += block.k_factors\n            T = self.filter_results.transition\n            lag_names = []\n            for j in range(block.factor_order):\n                lag_names += [f'L{j + 1}.{name}' for name in block.factor_names]\n            data = pd.DataFrame(T[block.factors_L1, block.factors_ar, 0], index=block.factor_names, columns=lag_names)\n            data.index.name = ''\n            try:\n                data = data.map(lambda s: '%.2f' % s)\n            except AttributeError:\n                data = data.applymap(lambda s: '%.2f' % s)\n            Q = self.filter_results.state_cov\n            if block.k_factors == 1:\n                data['   error variance'] = Q[ix1, ix1]\n            else:\n                data['   error covariance'] = block.factor_names\n                for j in range(block.k_factors):\n                    data[block.factor_names[j]] = Q[ix1:ix2, ix1 + j]\n            try:\n                formatted_vals = data.iloc[:, -block.k_factors:].map(lambda s: f'{s:.2f}')\n            except AttributeError:\n                formatted_vals = data.iloc[:, -block.k_factors:].applymap(lambda s: f'{s:.2f}')\n            data.iloc[:, -block.k_factors:] = formatted_vals\n            data = data.reset_index()\n            params_data = data.values\n            params_header = data.columns.tolist()\n            params_stubs = None\n            title = f'Transition: Factor block {i}'\n            table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n            summary.tables.insert(table_ix, table)\n            table_ix += 1\n            ix1 = ix2\n    return summary",
        "mutated": [
            "def summary(self, alpha=0.05, start=None, title=None, model_name=None, display_params=True, display_diagnostics=False, display_params_as_list=False, truncate_endog_names=None, display_max_endog=3):\n    if False:\n        i = 10\n    '\\n        Summarize the Model.\\n\\n        Parameters\\n        ----------\\n        alpha : float, optional\\n            Significance level for the confidence intervals. Default is 0.05.\\n        start : int, optional\\n            Integer of the start observation. Default is 0.\\n        title : str, optional\\n            The title used for the summary table.\\n        model_name : str, optional\\n            The name of the model used. Default is to use model class name.\\n\\n        Returns\\n        -------\\n        summary : Summary instance\\n            This holds the summary table and text, which can be printed or\\n            converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary.Summary\\n        '\n    mod = self.model\n    if title is None:\n        title = 'Dynamic Factor Results'\n    if model_name is None:\n        model_name = self.model._model_name\n    endog_names = self.model._get_endog_names(truncate=truncate_endog_names)\n    extra_top_left = None\n    extra_top_right = []\n    mle_retvals = getattr(self, 'mle_retvals', None)\n    mle_settings = getattr(self, 'mle_settings', None)\n    if mle_settings is not None and mle_settings.method == 'em':\n        extra_top_right += [('EM Iterations', [f'{mle_retvals.iter}'])]\n    summary = super().summary(alpha=alpha, start=start, title=title, model_name=model_name, display_params=display_params and display_params_as_list, display_diagnostics=display_diagnostics, truncate_endog_names=truncate_endog_names, display_max_endog=display_max_endog, extra_top_left=extra_top_left, extra_top_right=extra_top_right)\n    table_ix = 1\n    if not display_params_as_list:\n        data = pd.DataFrame(self.filter_results.design[:, mod._s['factors_L1'], 0], index=endog_names, columns=mod.factor_names)\n        try:\n            data = data.map(lambda s: '%.2f' % s)\n        except AttributeError:\n            data = data.applymap(lambda s: '%.2f' % s)\n        k_idio = 1\n        if mod.idiosyncratic_ar1:\n            data['   idiosyncratic: AR(1)'] = self.params[mod._p['idiosyncratic_ar1']]\n            k_idio += 1\n        data['var.'] = self.params[mod._p['idiosyncratic_var']]\n        try:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].map(lambda s: f'{s:.2f}')\n        except AttributeError:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].applymap(lambda s: f'{s:.2f}')\n        data.index.name = 'Factor loadings:'\n        base_iloc = np.arange(mod.k_factors)\n        for i in range(mod.k_endog):\n            iloc = [j for j in base_iloc if j not in mod._s.endog_factor_iloc[i]]\n            data.iloc[i, iloc] = '.'\n        data = data.reset_index()\n        params_data = data.values\n        params_header = data.columns.tolist()\n        params_stubs = None\n        title = 'Observation equation:'\n        table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n        summary.tables.insert(table_ix, table)\n        table_ix += 1\n        ix1 = 0\n        ix2 = 0\n        for i in range(len(mod._s.factor_blocks)):\n            block = mod._s.factor_blocks[i]\n            ix2 += block.k_factors\n            T = self.filter_results.transition\n            lag_names = []\n            for j in range(block.factor_order):\n                lag_names += [f'L{j + 1}.{name}' for name in block.factor_names]\n            data = pd.DataFrame(T[block.factors_L1, block.factors_ar, 0], index=block.factor_names, columns=lag_names)\n            data.index.name = ''\n            try:\n                data = data.map(lambda s: '%.2f' % s)\n            except AttributeError:\n                data = data.applymap(lambda s: '%.2f' % s)\n            Q = self.filter_results.state_cov\n            if block.k_factors == 1:\n                data['   error variance'] = Q[ix1, ix1]\n            else:\n                data['   error covariance'] = block.factor_names\n                for j in range(block.k_factors):\n                    data[block.factor_names[j]] = Q[ix1:ix2, ix1 + j]\n            try:\n                formatted_vals = data.iloc[:, -block.k_factors:].map(lambda s: f'{s:.2f}')\n            except AttributeError:\n                formatted_vals = data.iloc[:, -block.k_factors:].applymap(lambda s: f'{s:.2f}')\n            data.iloc[:, -block.k_factors:] = formatted_vals\n            data = data.reset_index()\n            params_data = data.values\n            params_header = data.columns.tolist()\n            params_stubs = None\n            title = f'Transition: Factor block {i}'\n            table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n            summary.tables.insert(table_ix, table)\n            table_ix += 1\n            ix1 = ix2\n    return summary",
            "def summary(self, alpha=0.05, start=None, title=None, model_name=None, display_params=True, display_diagnostics=False, display_params_as_list=False, truncate_endog_names=None, display_max_endog=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Summarize the Model.\\n\\n        Parameters\\n        ----------\\n        alpha : float, optional\\n            Significance level for the confidence intervals. Default is 0.05.\\n        start : int, optional\\n            Integer of the start observation. Default is 0.\\n        title : str, optional\\n            The title used for the summary table.\\n        model_name : str, optional\\n            The name of the model used. Default is to use model class name.\\n\\n        Returns\\n        -------\\n        summary : Summary instance\\n            This holds the summary table and text, which can be printed or\\n            converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary.Summary\\n        '\n    mod = self.model\n    if title is None:\n        title = 'Dynamic Factor Results'\n    if model_name is None:\n        model_name = self.model._model_name\n    endog_names = self.model._get_endog_names(truncate=truncate_endog_names)\n    extra_top_left = None\n    extra_top_right = []\n    mle_retvals = getattr(self, 'mle_retvals', None)\n    mle_settings = getattr(self, 'mle_settings', None)\n    if mle_settings is not None and mle_settings.method == 'em':\n        extra_top_right += [('EM Iterations', [f'{mle_retvals.iter}'])]\n    summary = super().summary(alpha=alpha, start=start, title=title, model_name=model_name, display_params=display_params and display_params_as_list, display_diagnostics=display_diagnostics, truncate_endog_names=truncate_endog_names, display_max_endog=display_max_endog, extra_top_left=extra_top_left, extra_top_right=extra_top_right)\n    table_ix = 1\n    if not display_params_as_list:\n        data = pd.DataFrame(self.filter_results.design[:, mod._s['factors_L1'], 0], index=endog_names, columns=mod.factor_names)\n        try:\n            data = data.map(lambda s: '%.2f' % s)\n        except AttributeError:\n            data = data.applymap(lambda s: '%.2f' % s)\n        k_idio = 1\n        if mod.idiosyncratic_ar1:\n            data['   idiosyncratic: AR(1)'] = self.params[mod._p['idiosyncratic_ar1']]\n            k_idio += 1\n        data['var.'] = self.params[mod._p['idiosyncratic_var']]\n        try:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].map(lambda s: f'{s:.2f}')\n        except AttributeError:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].applymap(lambda s: f'{s:.2f}')\n        data.index.name = 'Factor loadings:'\n        base_iloc = np.arange(mod.k_factors)\n        for i in range(mod.k_endog):\n            iloc = [j for j in base_iloc if j not in mod._s.endog_factor_iloc[i]]\n            data.iloc[i, iloc] = '.'\n        data = data.reset_index()\n        params_data = data.values\n        params_header = data.columns.tolist()\n        params_stubs = None\n        title = 'Observation equation:'\n        table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n        summary.tables.insert(table_ix, table)\n        table_ix += 1\n        ix1 = 0\n        ix2 = 0\n        for i in range(len(mod._s.factor_blocks)):\n            block = mod._s.factor_blocks[i]\n            ix2 += block.k_factors\n            T = self.filter_results.transition\n            lag_names = []\n            for j in range(block.factor_order):\n                lag_names += [f'L{j + 1}.{name}' for name in block.factor_names]\n            data = pd.DataFrame(T[block.factors_L1, block.factors_ar, 0], index=block.factor_names, columns=lag_names)\n            data.index.name = ''\n            try:\n                data = data.map(lambda s: '%.2f' % s)\n            except AttributeError:\n                data = data.applymap(lambda s: '%.2f' % s)\n            Q = self.filter_results.state_cov\n            if block.k_factors == 1:\n                data['   error variance'] = Q[ix1, ix1]\n            else:\n                data['   error covariance'] = block.factor_names\n                for j in range(block.k_factors):\n                    data[block.factor_names[j]] = Q[ix1:ix2, ix1 + j]\n            try:\n                formatted_vals = data.iloc[:, -block.k_factors:].map(lambda s: f'{s:.2f}')\n            except AttributeError:\n                formatted_vals = data.iloc[:, -block.k_factors:].applymap(lambda s: f'{s:.2f}')\n            data.iloc[:, -block.k_factors:] = formatted_vals\n            data = data.reset_index()\n            params_data = data.values\n            params_header = data.columns.tolist()\n            params_stubs = None\n            title = f'Transition: Factor block {i}'\n            table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n            summary.tables.insert(table_ix, table)\n            table_ix += 1\n            ix1 = ix2\n    return summary",
            "def summary(self, alpha=0.05, start=None, title=None, model_name=None, display_params=True, display_diagnostics=False, display_params_as_list=False, truncate_endog_names=None, display_max_endog=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Summarize the Model.\\n\\n        Parameters\\n        ----------\\n        alpha : float, optional\\n            Significance level for the confidence intervals. Default is 0.05.\\n        start : int, optional\\n            Integer of the start observation. Default is 0.\\n        title : str, optional\\n            The title used for the summary table.\\n        model_name : str, optional\\n            The name of the model used. Default is to use model class name.\\n\\n        Returns\\n        -------\\n        summary : Summary instance\\n            This holds the summary table and text, which can be printed or\\n            converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary.Summary\\n        '\n    mod = self.model\n    if title is None:\n        title = 'Dynamic Factor Results'\n    if model_name is None:\n        model_name = self.model._model_name\n    endog_names = self.model._get_endog_names(truncate=truncate_endog_names)\n    extra_top_left = None\n    extra_top_right = []\n    mle_retvals = getattr(self, 'mle_retvals', None)\n    mle_settings = getattr(self, 'mle_settings', None)\n    if mle_settings is not None and mle_settings.method == 'em':\n        extra_top_right += [('EM Iterations', [f'{mle_retvals.iter}'])]\n    summary = super().summary(alpha=alpha, start=start, title=title, model_name=model_name, display_params=display_params and display_params_as_list, display_diagnostics=display_diagnostics, truncate_endog_names=truncate_endog_names, display_max_endog=display_max_endog, extra_top_left=extra_top_left, extra_top_right=extra_top_right)\n    table_ix = 1\n    if not display_params_as_list:\n        data = pd.DataFrame(self.filter_results.design[:, mod._s['factors_L1'], 0], index=endog_names, columns=mod.factor_names)\n        try:\n            data = data.map(lambda s: '%.2f' % s)\n        except AttributeError:\n            data = data.applymap(lambda s: '%.2f' % s)\n        k_idio = 1\n        if mod.idiosyncratic_ar1:\n            data['   idiosyncratic: AR(1)'] = self.params[mod._p['idiosyncratic_ar1']]\n            k_idio += 1\n        data['var.'] = self.params[mod._p['idiosyncratic_var']]\n        try:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].map(lambda s: f'{s:.2f}')\n        except AttributeError:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].applymap(lambda s: f'{s:.2f}')\n        data.index.name = 'Factor loadings:'\n        base_iloc = np.arange(mod.k_factors)\n        for i in range(mod.k_endog):\n            iloc = [j for j in base_iloc if j not in mod._s.endog_factor_iloc[i]]\n            data.iloc[i, iloc] = '.'\n        data = data.reset_index()\n        params_data = data.values\n        params_header = data.columns.tolist()\n        params_stubs = None\n        title = 'Observation equation:'\n        table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n        summary.tables.insert(table_ix, table)\n        table_ix += 1\n        ix1 = 0\n        ix2 = 0\n        for i in range(len(mod._s.factor_blocks)):\n            block = mod._s.factor_blocks[i]\n            ix2 += block.k_factors\n            T = self.filter_results.transition\n            lag_names = []\n            for j in range(block.factor_order):\n                lag_names += [f'L{j + 1}.{name}' for name in block.factor_names]\n            data = pd.DataFrame(T[block.factors_L1, block.factors_ar, 0], index=block.factor_names, columns=lag_names)\n            data.index.name = ''\n            try:\n                data = data.map(lambda s: '%.2f' % s)\n            except AttributeError:\n                data = data.applymap(lambda s: '%.2f' % s)\n            Q = self.filter_results.state_cov\n            if block.k_factors == 1:\n                data['   error variance'] = Q[ix1, ix1]\n            else:\n                data['   error covariance'] = block.factor_names\n                for j in range(block.k_factors):\n                    data[block.factor_names[j]] = Q[ix1:ix2, ix1 + j]\n            try:\n                formatted_vals = data.iloc[:, -block.k_factors:].map(lambda s: f'{s:.2f}')\n            except AttributeError:\n                formatted_vals = data.iloc[:, -block.k_factors:].applymap(lambda s: f'{s:.2f}')\n            data.iloc[:, -block.k_factors:] = formatted_vals\n            data = data.reset_index()\n            params_data = data.values\n            params_header = data.columns.tolist()\n            params_stubs = None\n            title = f'Transition: Factor block {i}'\n            table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n            summary.tables.insert(table_ix, table)\n            table_ix += 1\n            ix1 = ix2\n    return summary",
            "def summary(self, alpha=0.05, start=None, title=None, model_name=None, display_params=True, display_diagnostics=False, display_params_as_list=False, truncate_endog_names=None, display_max_endog=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Summarize the Model.\\n\\n        Parameters\\n        ----------\\n        alpha : float, optional\\n            Significance level for the confidence intervals. Default is 0.05.\\n        start : int, optional\\n            Integer of the start observation. Default is 0.\\n        title : str, optional\\n            The title used for the summary table.\\n        model_name : str, optional\\n            The name of the model used. Default is to use model class name.\\n\\n        Returns\\n        -------\\n        summary : Summary instance\\n            This holds the summary table and text, which can be printed or\\n            converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary.Summary\\n        '\n    mod = self.model\n    if title is None:\n        title = 'Dynamic Factor Results'\n    if model_name is None:\n        model_name = self.model._model_name\n    endog_names = self.model._get_endog_names(truncate=truncate_endog_names)\n    extra_top_left = None\n    extra_top_right = []\n    mle_retvals = getattr(self, 'mle_retvals', None)\n    mle_settings = getattr(self, 'mle_settings', None)\n    if mle_settings is not None and mle_settings.method == 'em':\n        extra_top_right += [('EM Iterations', [f'{mle_retvals.iter}'])]\n    summary = super().summary(alpha=alpha, start=start, title=title, model_name=model_name, display_params=display_params and display_params_as_list, display_diagnostics=display_diagnostics, truncate_endog_names=truncate_endog_names, display_max_endog=display_max_endog, extra_top_left=extra_top_left, extra_top_right=extra_top_right)\n    table_ix = 1\n    if not display_params_as_list:\n        data = pd.DataFrame(self.filter_results.design[:, mod._s['factors_L1'], 0], index=endog_names, columns=mod.factor_names)\n        try:\n            data = data.map(lambda s: '%.2f' % s)\n        except AttributeError:\n            data = data.applymap(lambda s: '%.2f' % s)\n        k_idio = 1\n        if mod.idiosyncratic_ar1:\n            data['   idiosyncratic: AR(1)'] = self.params[mod._p['idiosyncratic_ar1']]\n            k_idio += 1\n        data['var.'] = self.params[mod._p['idiosyncratic_var']]\n        try:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].map(lambda s: f'{s:.2f}')\n        except AttributeError:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].applymap(lambda s: f'{s:.2f}')\n        data.index.name = 'Factor loadings:'\n        base_iloc = np.arange(mod.k_factors)\n        for i in range(mod.k_endog):\n            iloc = [j for j in base_iloc if j not in mod._s.endog_factor_iloc[i]]\n            data.iloc[i, iloc] = '.'\n        data = data.reset_index()\n        params_data = data.values\n        params_header = data.columns.tolist()\n        params_stubs = None\n        title = 'Observation equation:'\n        table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n        summary.tables.insert(table_ix, table)\n        table_ix += 1\n        ix1 = 0\n        ix2 = 0\n        for i in range(len(mod._s.factor_blocks)):\n            block = mod._s.factor_blocks[i]\n            ix2 += block.k_factors\n            T = self.filter_results.transition\n            lag_names = []\n            for j in range(block.factor_order):\n                lag_names += [f'L{j + 1}.{name}' for name in block.factor_names]\n            data = pd.DataFrame(T[block.factors_L1, block.factors_ar, 0], index=block.factor_names, columns=lag_names)\n            data.index.name = ''\n            try:\n                data = data.map(lambda s: '%.2f' % s)\n            except AttributeError:\n                data = data.applymap(lambda s: '%.2f' % s)\n            Q = self.filter_results.state_cov\n            if block.k_factors == 1:\n                data['   error variance'] = Q[ix1, ix1]\n            else:\n                data['   error covariance'] = block.factor_names\n                for j in range(block.k_factors):\n                    data[block.factor_names[j]] = Q[ix1:ix2, ix1 + j]\n            try:\n                formatted_vals = data.iloc[:, -block.k_factors:].map(lambda s: f'{s:.2f}')\n            except AttributeError:\n                formatted_vals = data.iloc[:, -block.k_factors:].applymap(lambda s: f'{s:.2f}')\n            data.iloc[:, -block.k_factors:] = formatted_vals\n            data = data.reset_index()\n            params_data = data.values\n            params_header = data.columns.tolist()\n            params_stubs = None\n            title = f'Transition: Factor block {i}'\n            table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n            summary.tables.insert(table_ix, table)\n            table_ix += 1\n            ix1 = ix2\n    return summary",
            "def summary(self, alpha=0.05, start=None, title=None, model_name=None, display_params=True, display_diagnostics=False, display_params_as_list=False, truncate_endog_names=None, display_max_endog=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Summarize the Model.\\n\\n        Parameters\\n        ----------\\n        alpha : float, optional\\n            Significance level for the confidence intervals. Default is 0.05.\\n        start : int, optional\\n            Integer of the start observation. Default is 0.\\n        title : str, optional\\n            The title used for the summary table.\\n        model_name : str, optional\\n            The name of the model used. Default is to use model class name.\\n\\n        Returns\\n        -------\\n        summary : Summary instance\\n            This holds the summary table and text, which can be printed or\\n            converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary.Summary\\n        '\n    mod = self.model\n    if title is None:\n        title = 'Dynamic Factor Results'\n    if model_name is None:\n        model_name = self.model._model_name\n    endog_names = self.model._get_endog_names(truncate=truncate_endog_names)\n    extra_top_left = None\n    extra_top_right = []\n    mle_retvals = getattr(self, 'mle_retvals', None)\n    mle_settings = getattr(self, 'mle_settings', None)\n    if mle_settings is not None and mle_settings.method == 'em':\n        extra_top_right += [('EM Iterations', [f'{mle_retvals.iter}'])]\n    summary = super().summary(alpha=alpha, start=start, title=title, model_name=model_name, display_params=display_params and display_params_as_list, display_diagnostics=display_diagnostics, truncate_endog_names=truncate_endog_names, display_max_endog=display_max_endog, extra_top_left=extra_top_left, extra_top_right=extra_top_right)\n    table_ix = 1\n    if not display_params_as_list:\n        data = pd.DataFrame(self.filter_results.design[:, mod._s['factors_L1'], 0], index=endog_names, columns=mod.factor_names)\n        try:\n            data = data.map(lambda s: '%.2f' % s)\n        except AttributeError:\n            data = data.applymap(lambda s: '%.2f' % s)\n        k_idio = 1\n        if mod.idiosyncratic_ar1:\n            data['   idiosyncratic: AR(1)'] = self.params[mod._p['idiosyncratic_ar1']]\n            k_idio += 1\n        data['var.'] = self.params[mod._p['idiosyncratic_var']]\n        try:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].map(lambda s: f'{s:.2f}')\n        except AttributeError:\n            data.iloc[:, -k_idio:] = data.iloc[:, -k_idio:].applymap(lambda s: f'{s:.2f}')\n        data.index.name = 'Factor loadings:'\n        base_iloc = np.arange(mod.k_factors)\n        for i in range(mod.k_endog):\n            iloc = [j for j in base_iloc if j not in mod._s.endog_factor_iloc[i]]\n            data.iloc[i, iloc] = '.'\n        data = data.reset_index()\n        params_data = data.values\n        params_header = data.columns.tolist()\n        params_stubs = None\n        title = 'Observation equation:'\n        table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n        summary.tables.insert(table_ix, table)\n        table_ix += 1\n        ix1 = 0\n        ix2 = 0\n        for i in range(len(mod._s.factor_blocks)):\n            block = mod._s.factor_blocks[i]\n            ix2 += block.k_factors\n            T = self.filter_results.transition\n            lag_names = []\n            for j in range(block.factor_order):\n                lag_names += [f'L{j + 1}.{name}' for name in block.factor_names]\n            data = pd.DataFrame(T[block.factors_L1, block.factors_ar, 0], index=block.factor_names, columns=lag_names)\n            data.index.name = ''\n            try:\n                data = data.map(lambda s: '%.2f' % s)\n            except AttributeError:\n                data = data.applymap(lambda s: '%.2f' % s)\n            Q = self.filter_results.state_cov\n            if block.k_factors == 1:\n                data['   error variance'] = Q[ix1, ix1]\n            else:\n                data['   error covariance'] = block.factor_names\n                for j in range(block.k_factors):\n                    data[block.factor_names[j]] = Q[ix1:ix2, ix1 + j]\n            try:\n                formatted_vals = data.iloc[:, -block.k_factors:].map(lambda s: f'{s:.2f}')\n            except AttributeError:\n                formatted_vals = data.iloc[:, -block.k_factors:].applymap(lambda s: f'{s:.2f}')\n            data.iloc[:, -block.k_factors:] = formatted_vals\n            data = data.reset_index()\n            params_data = data.values\n            params_header = data.columns.tolist()\n            params_stubs = None\n            title = f'Transition: Factor block {i}'\n            table = SimpleTable(params_data, params_header, params_stubs, txt_fmt=fmt_params, title=title)\n            summary.tables.insert(table_ix, table)\n            table_ix += 1\n            ix1 = ix2\n    return summary"
        ]
    }
]