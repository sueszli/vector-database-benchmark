[
    {
        "func_name": "test_window_with_timecontext",
        "original": "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], (-3600, 0)), ([(ibis.interval(hours=2), 0)], (-7200, 0)), ([(0, ibis.interval(hours=1))], (0, 3600)), ([(ibis.interval(hours=1), ibis.interval(hours=1))], (-3600, 3600))], indirect=['ibis_windows'])\ndef test_window_with_timecontext(con, ibis_windows, spark_range):\n    \"\"\"Test context adjustment for trailing / range window.\n\n    We expand context according to window sizes, for example, for a table of:\n    time       value\n    2020-01-01   a\n    2020-01-02   b\n    2020-01-03   c\n    2020-01-04   d\n    with context = (2020-01-03, 2002-01-04) trailing count for 1 day will be:\n    time       value  count\n    2020-01-03   c      2\n    2020-01-04   d      2\n    trailing count for 2 days will be:\n    time       value  count\n    2020-01-03   c      3\n    2020-01-04   d      3\n    with context = (2020-01-01, 2002-01-02) count for 1 day forward looking\n    window will be:\n    time       value  count\n    2020-01-01   a      2\n    2020-01-02   b      2\n    \"\"\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170103', tz='UTC'))\n    result_pd = table.mutate(count=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
        "mutated": [
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], (-3600, 0)), ([(ibis.interval(hours=2), 0)], (-7200, 0)), ([(0, ibis.interval(hours=1))], (0, 3600)), ([(ibis.interval(hours=1), ibis.interval(hours=1))], (-3600, 3600))], indirect=['ibis_windows'])\ndef test_window_with_timecontext(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n    'Test context adjustment for trailing / range window.\\n\\n    We expand context according to window sizes, for example, for a table of:\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-03, 2002-01-04) trailing count for 1 day will be:\\n    time       value  count\\n    2020-01-03   c      2\\n    2020-01-04   d      2\\n    trailing count for 2 days will be:\\n    time       value  count\\n    2020-01-03   c      3\\n    2020-01-04   d      3\\n    with context = (2020-01-01, 2002-01-02) count for 1 day forward looking\\n    window will be:\\n    time       value  count\\n    2020-01-01   a      2\\n    2020-01-02   b      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170103', tz='UTC'))\n    result_pd = table.mutate(count=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], (-3600, 0)), ([(ibis.interval(hours=2), 0)], (-7200, 0)), ([(0, ibis.interval(hours=1))], (0, 3600)), ([(ibis.interval(hours=1), ibis.interval(hours=1))], (-3600, 3600))], indirect=['ibis_windows'])\ndef test_window_with_timecontext(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test context adjustment for trailing / range window.\\n\\n    We expand context according to window sizes, for example, for a table of:\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-03, 2002-01-04) trailing count for 1 day will be:\\n    time       value  count\\n    2020-01-03   c      2\\n    2020-01-04   d      2\\n    trailing count for 2 days will be:\\n    time       value  count\\n    2020-01-03   c      3\\n    2020-01-04   d      3\\n    with context = (2020-01-01, 2002-01-02) count for 1 day forward looking\\n    window will be:\\n    time       value  count\\n    2020-01-01   a      2\\n    2020-01-02   b      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170103', tz='UTC'))\n    result_pd = table.mutate(count=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], (-3600, 0)), ([(ibis.interval(hours=2), 0)], (-7200, 0)), ([(0, ibis.interval(hours=1))], (0, 3600)), ([(ibis.interval(hours=1), ibis.interval(hours=1))], (-3600, 3600))], indirect=['ibis_windows'])\ndef test_window_with_timecontext(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test context adjustment for trailing / range window.\\n\\n    We expand context according to window sizes, for example, for a table of:\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-03, 2002-01-04) trailing count for 1 day will be:\\n    time       value  count\\n    2020-01-03   c      2\\n    2020-01-04   d      2\\n    trailing count for 2 days will be:\\n    time       value  count\\n    2020-01-03   c      3\\n    2020-01-04   d      3\\n    with context = (2020-01-01, 2002-01-02) count for 1 day forward looking\\n    window will be:\\n    time       value  count\\n    2020-01-01   a      2\\n    2020-01-02   b      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170103', tz='UTC'))\n    result_pd = table.mutate(count=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], (-3600, 0)), ([(ibis.interval(hours=2), 0)], (-7200, 0)), ([(0, ibis.interval(hours=1))], (0, 3600)), ([(ibis.interval(hours=1), ibis.interval(hours=1))], (-3600, 3600))], indirect=['ibis_windows'])\ndef test_window_with_timecontext(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test context adjustment for trailing / range window.\\n\\n    We expand context according to window sizes, for example, for a table of:\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-03, 2002-01-04) trailing count for 1 day will be:\\n    time       value  count\\n    2020-01-03   c      2\\n    2020-01-04   d      2\\n    trailing count for 2 days will be:\\n    time       value  count\\n    2020-01-03   c      3\\n    2020-01-04   d      3\\n    with context = (2020-01-01, 2002-01-02) count for 1 day forward looking\\n    window will be:\\n    time       value  count\\n    2020-01-01   a      2\\n    2020-01-02   b      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170103', tz='UTC'))\n    result_pd = table.mutate(count=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], (-3600, 0)), ([(ibis.interval(hours=2), 0)], (-7200, 0)), ([(0, ibis.interval(hours=1))], (0, 3600)), ([(ibis.interval(hours=1), ibis.interval(hours=1))], (-3600, 3600))], indirect=['ibis_windows'])\ndef test_window_with_timecontext(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test context adjustment for trailing / range window.\\n\\n    We expand context according to window sizes, for example, for a table of:\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-03, 2002-01-04) trailing count for 1 day will be:\\n    time       value  count\\n    2020-01-03   c      2\\n    2020-01-04   d      2\\n    trailing count for 2 days will be:\\n    time       value  count\\n    2020-01-03   c      3\\n    2020-01-04   d      3\\n    with context = (2020-01-01, 2002-01-02) count for 1 day forward looking\\n    window will be:\\n    time       value  count\\n    2020-01-01   a      2\\n    2020-01-02   b      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170103', tz='UTC'))\n    result_pd = table.mutate(count=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)"
        ]
    },
    {
        "func_name": "test_cumulative_window",
        "original": "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(None, 0)], (Window.unboundedPreceding, 0))], indirect=['ibis_windows'])\ndef test_cumulative_window(con, ibis_windows, spark_range):\n    \"\"\"Test context adjustment for cumulative window.\n\n    For cumulative window, by definition we should look back infinitely.\n    When data is trimmed by time context, we define the limit of looking\n    back is the start time of given time context. Thus for a table of\n    time       value\n    2020-01-01   a\n    2020-01-02   b\n    2020-01-03   c\n    2020-01-04   d\n    with context = (2020-01-02, 2002-01-03) cumulative count will be:\n    time       value  count\n    2020-01-02   b      1\n    2020-01-03   c      2\n    \"\"\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_cum=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile(timecontext=context)\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count_cum', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
        "mutated": [
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(None, 0)], (Window.unboundedPreceding, 0))], indirect=['ibis_windows'])\ndef test_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n    'Test context adjustment for cumulative window.\\n\\n    For cumulative window, by definition we should look back infinitely.\\n    When data is trimmed by time context, we define the limit of looking\\n    back is the start time of given time context. Thus for a table of\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03) cumulative count will be:\\n    time       value  count\\n    2020-01-02   b      1\\n    2020-01-03   c      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_cum=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile(timecontext=context)\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count_cum', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(None, 0)], (Window.unboundedPreceding, 0))], indirect=['ibis_windows'])\ndef test_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test context adjustment for cumulative window.\\n\\n    For cumulative window, by definition we should look back infinitely.\\n    When data is trimmed by time context, we define the limit of looking\\n    back is the start time of given time context. Thus for a table of\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03) cumulative count will be:\\n    time       value  count\\n    2020-01-02   b      1\\n    2020-01-03   c      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_cum=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile(timecontext=context)\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count_cum', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(None, 0)], (Window.unboundedPreceding, 0))], indirect=['ibis_windows'])\ndef test_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test context adjustment for cumulative window.\\n\\n    For cumulative window, by definition we should look back infinitely.\\n    When data is trimmed by time context, we define the limit of looking\\n    back is the start time of given time context. Thus for a table of\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03) cumulative count will be:\\n    time       value  count\\n    2020-01-02   b      1\\n    2020-01-03   c      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_cum=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile(timecontext=context)\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count_cum', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(None, 0)], (Window.unboundedPreceding, 0))], indirect=['ibis_windows'])\ndef test_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test context adjustment for cumulative window.\\n\\n    For cumulative window, by definition we should look back infinitely.\\n    When data is trimmed by time context, we define the limit of looking\\n    back is the start time of given time context. Thus for a table of\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03) cumulative count will be:\\n    time       value  count\\n    2020-01-02   b      1\\n    2020-01-03   c      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_cum=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile(timecontext=context)\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count_cum', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(None, 0)], (Window.unboundedPreceding, 0))], indirect=['ibis_windows'])\ndef test_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test context adjustment for cumulative window.\\n\\n    For cumulative window, by definition we should look back infinitely.\\n    When data is trimmed by time context, we define the limit of looking\\n    back is the start time of given time context. Thus for a table of\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03) cumulative count will be:\\n    time       value  count\\n    2020-01-02   b      1\\n    2020-01-03   c      2\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_cum=table['value'].count().over(ibis_windows[0])).execute(timecontext=context)\n    spark_table = table.compile(timecontext=context)\n    spark_window = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range)\n    expected = spark_table.withColumn('count_cum', F.count(spark_table['value']).over(spark_window)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)"
        ]
    },
    {
        "func_name": "test_multiple_trailing_window",
        "original": "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_multiple_trailing_window(con, ibis_windows, spark_range):\n    \"\"\"Test context adjustment for multiple trailing window.\n\n    When there are multiple window ops, we need to verify contexts are\n    adjusted correctly for all windows. In this tests we are constructing\n    one trailing window for 1h and another trailing window for 2h\n    \"\"\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_2h=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_2h', F.count(spark_table['value']).over(spark_window_2h)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
        "mutated": [
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_multiple_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n    'Test context adjustment for multiple trailing window.\\n\\n    When there are multiple window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and another trailing window for 2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_2h=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_2h', F.count(spark_table['value']).over(spark_window_2h)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_multiple_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test context adjustment for multiple trailing window.\\n\\n    When there are multiple window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and another trailing window for 2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_2h=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_2h', F.count(spark_table['value']).over(spark_window_2h)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_multiple_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test context adjustment for multiple trailing window.\\n\\n    When there are multiple window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and another trailing window for 2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_2h=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_2h', F.count(spark_table['value']).over(spark_window_2h)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_multiple_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test context adjustment for multiple trailing window.\\n\\n    When there are multiple window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and another trailing window for 2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_2h=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_2h', F.count(spark_table['value']).over(spark_window_2h)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_multiple_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test context adjustment for multiple trailing window.\\n\\n    When there are multiple window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and another trailing window for 2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_2h=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_2h', F.count(spark_table['value']).over(spark_window_2h)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)"
        ]
    },
    {
        "func_name": "test_chained_trailing_window",
        "original": "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_chained_trailing_window(con, ibis_windows, spark_range):\n    \"\"\"Test context adjustment for chained windows.\n\n    When there are chained window ops, we need to verify contexts are\n    adjusted correctly for all windows. In this tests we are constructing\n    one trailing window for 1h and trailing window on the new column for\n    2h\n    \"\"\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    table = table.mutate(new_col=table['value'].count().over(ibis_windows[0]))\n    table = table.mutate(count=table['new_col'].count().over(ibis_windows[1]))\n    result_pd = table.execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    spark_table = spark_table.withColumn('new_col', F.count(spark_table['value']).over(spark_window_1h))\n    spark_table = spark_table.withColumn('count', F.count(spark_table['new_col']).over(spark_window_2h))\n    expected = spark_table.toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
        "mutated": [
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_chained_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n    'Test context adjustment for chained windows.\\n\\n    When there are chained window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and trailing window on the new column for\\n    2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    table = table.mutate(new_col=table['value'].count().over(ibis_windows[0]))\n    table = table.mutate(count=table['new_col'].count().over(ibis_windows[1]))\n    result_pd = table.execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    spark_table = spark_table.withColumn('new_col', F.count(spark_table['value']).over(spark_window_1h))\n    spark_table = spark_table.withColumn('count', F.count(spark_table['new_col']).over(spark_window_2h))\n    expected = spark_table.toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_chained_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test context adjustment for chained windows.\\n\\n    When there are chained window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and trailing window on the new column for\\n    2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    table = table.mutate(new_col=table['value'].count().over(ibis_windows[0]))\n    table = table.mutate(count=table['new_col'].count().over(ibis_windows[1]))\n    result_pd = table.execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    spark_table = spark_table.withColumn('new_col', F.count(spark_table['value']).over(spark_window_1h))\n    spark_table = spark_table.withColumn('count', F.count(spark_table['new_col']).over(spark_window_2h))\n    expected = spark_table.toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_chained_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test context adjustment for chained windows.\\n\\n    When there are chained window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and trailing window on the new column for\\n    2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    table = table.mutate(new_col=table['value'].count().over(ibis_windows[0]))\n    table = table.mutate(count=table['new_col'].count().over(ibis_windows[1]))\n    result_pd = table.execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    spark_table = spark_table.withColumn('new_col', F.count(spark_table['value']).over(spark_window_1h))\n    spark_table = spark_table.withColumn('count', F.count(spark_table['new_col']).over(spark_window_2h))\n    expected = spark_table.toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_chained_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test context adjustment for chained windows.\\n\\n    When there are chained window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and trailing window on the new column for\\n    2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    table = table.mutate(new_col=table['value'].count().over(ibis_windows[0]))\n    table = table.mutate(count=table['new_col'].count().over(ibis_windows[1]))\n    result_pd = table.execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    spark_table = spark_table.withColumn('new_col', F.count(spark_table['value']).over(spark_window_1h))\n    spark_table = spark_table.withColumn('count', F.count(spark_table['new_col']).over(spark_window_2h))\n    expected = spark_table.toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (ibis.interval(hours=2), 0)], [(-3600, 0), (-7200, 0)])], indirect=['ibis_windows'])\ndef test_chained_trailing_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test context adjustment for chained windows.\\n\\n    When there are chained window ops, we need to verify contexts are\\n    adjusted correctly for all windows. In this tests we are constructing\\n    one trailing window for 1h and trailing window on the new column for\\n    2h\\n    '\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    table = table.mutate(new_col=table['value'].count().over(ibis_windows[0]))\n    table = table.mutate(count=table['new_col'].count().over(ibis_windows[1]))\n    result_pd = table.execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_2h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    spark_table = spark_table.withColumn('new_col', F.count(spark_table['value']).over(spark_window_1h))\n    spark_table = spark_table.withColumn('count', F.count(spark_table['new_col']).over(spark_window_2h))\n    expected = spark_table.toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)"
        ]
    },
    {
        "func_name": "test_rolling_with_cumulative_window",
        "original": "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (None, 0)], [(-3600, 0), (Window.unboundedPreceding, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_cumulative_window(con, ibis_windows, spark_range):\n    \"\"\"Test context adjustment for rolling window and cumulative window.\n\n    cumulative window should calculate only with in user's context,\n    while rolling window should calculate on expanded context.\n    For a rolling window of 1 day,\n    time       value\n    2020-01-01   a\n    2020-01-02   b\n    2020-01-03   c\n    2020-01-04   d\n    with context = (2020-01-02, 2002-01-03), count will be:\n    time       value  roll_count cum_count\n    2020-01-02   b      2            1\n    2020-01-03   c      2            2\n    \"\"\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_cum=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_cum = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_cum', F.count(spark_table['value']).over(spark_window_cum)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
        "mutated": [
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (None, 0)], [(-3600, 0), (Window.unboundedPreceding, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n    \"Test context adjustment for rolling window and cumulative window.\\n\\n    cumulative window should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day,\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03), count will be:\\n    time       value  roll_count cum_count\\n    2020-01-02   b      2            1\\n    2020-01-03   c      2            2\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_cum=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_cum = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_cum', F.count(spark_table['value']).over(spark_window_cum)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (None, 0)], [(-3600, 0), (Window.unboundedPreceding, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Test context adjustment for rolling window and cumulative window.\\n\\n    cumulative window should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day,\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03), count will be:\\n    time       value  roll_count cum_count\\n    2020-01-02   b      2            1\\n    2020-01-03   c      2            2\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_cum=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_cum = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_cum', F.count(spark_table['value']).over(spark_window_cum)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (None, 0)], [(-3600, 0), (Window.unboundedPreceding, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Test context adjustment for rolling window and cumulative window.\\n\\n    cumulative window should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day,\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03), count will be:\\n    time       value  roll_count cum_count\\n    2020-01-02   b      2            1\\n    2020-01-03   c      2            2\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_cum=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_cum = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_cum', F.count(spark_table['value']).over(spark_window_cum)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (None, 0)], [(-3600, 0), (Window.unboundedPreceding, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Test context adjustment for rolling window and cumulative window.\\n\\n    cumulative window should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day,\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03), count will be:\\n    time       value  roll_count cum_count\\n    2020-01-02   b      2            1\\n    2020-01-03   c      2            2\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_cum=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_cum = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_cum', F.count(spark_table['value']).over(spark_window_cum)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0), (None, 0)], [(-3600, 0), (Window.unboundedPreceding, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_cumulative_window(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Test context adjustment for rolling window and cumulative window.\\n\\n    cumulative window should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day,\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-03), count will be:\\n    time       value  roll_count cum_count\\n    2020-01-02   b      2            1\\n    2020-01-03   c      2            2\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count_cum=table['value'].count().over(ibis_windows[1])).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    spark_window_cum = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[1])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count_cum', F.count(spark_table['value']).over(spark_window_cum)).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)"
        ]
    },
    {
        "func_name": "test_rolling_with_non_window_op",
        "original": "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], [(-3600, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_non_window_op(con, ibis_windows, spark_range):\n    \"\"\"Test context adjustment for rolling window and non window ops.\n\n    non window ops should calculate only with in user's context,\n    while rolling window should calculate on expanded context.\n    For a rolling window of 1 day, and a `count` aggregation\n    time       value\n    2020-01-01   a\n    2020-01-02   b\n    2020-01-03   c\n    2020-01-04   d\n    with context = (2020-01-02, 2002-01-04), result will be:\n    time       value  roll_count    count\n    2020-01-02   b      2            3\n    2020-01-03   c      2            3\n    2020-01-04   d      2            3\n    Because there are 3 rows within user context (01-02, 01-04),\n    count should return 3 for every row, rather 4, based on the\n    adjusted context (01-01, 01-04).\n    \"\"\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count=table['value'].count()).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count', F.count(spark_table['value'])).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
        "mutated": [
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], [(-3600, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_non_window_op(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n    \"Test context adjustment for rolling window and non window ops.\\n\\n    non window ops should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day, and a `count` aggregation\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-04), result will be:\\n    time       value  roll_count    count\\n    2020-01-02   b      2            3\\n    2020-01-03   c      2            3\\n    2020-01-04   d      2            3\\n    Because there are 3 rows within user context (01-02, 01-04),\\n    count should return 3 for every row, rather 4, based on the\\n    adjusted context (01-01, 01-04).\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count=table['value'].count()).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count', F.count(spark_table['value'])).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], [(-3600, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_non_window_op(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Test context adjustment for rolling window and non window ops.\\n\\n    non window ops should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day, and a `count` aggregation\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-04), result will be:\\n    time       value  roll_count    count\\n    2020-01-02   b      2            3\\n    2020-01-03   c      2            3\\n    2020-01-04   d      2            3\\n    Because there are 3 rows within user context (01-02, 01-04),\\n    count should return 3 for every row, rather 4, based on the\\n    adjusted context (01-01, 01-04).\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count=table['value'].count()).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count', F.count(spark_table['value'])).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], [(-3600, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_non_window_op(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Test context adjustment for rolling window and non window ops.\\n\\n    non window ops should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day, and a `count` aggregation\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-04), result will be:\\n    time       value  roll_count    count\\n    2020-01-02   b      2            3\\n    2020-01-03   c      2            3\\n    2020-01-04   d      2            3\\n    Because there are 3 rows within user context (01-02, 01-04),\\n    count should return 3 for every row, rather 4, based on the\\n    adjusted context (01-01, 01-04).\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count=table['value'].count()).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count', F.count(spark_table['value'])).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], [(-3600, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_non_window_op(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Test context adjustment for rolling window and non window ops.\\n\\n    non window ops should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day, and a `count` aggregation\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-04), result will be:\\n    time       value  roll_count    count\\n    2020-01-02   b      2            3\\n    2020-01-03   c      2            3\\n    2020-01-04   d      2            3\\n    Because there are 3 rows within user context (01-02, 01-04),\\n    count should return 3 for every row, rather 4, based on the\\n    adjusted context (01-01, 01-04).\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count=table['value'].count()).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count', F.count(spark_table['value'])).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "@pytest.mark.xfail(reason='Issue #2457 Adjust context properly for mixed rolling window, cumulative window and non window ops', strict=True)\n@pytest.mark.parametrize(('ibis_windows', 'spark_range'), [([(ibis.interval(hours=1), 0)], [(-3600, 0)])], indirect=['ibis_windows'])\ndef test_rolling_with_non_window_op(con, ibis_windows, spark_range):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Test context adjustment for rolling window and non window ops.\\n\\n    non window ops should calculate only with in user's context,\\n    while rolling window should calculate on expanded context.\\n    For a rolling window of 1 day, and a `count` aggregation\\n    time       value\\n    2020-01-01   a\\n    2020-01-02   b\\n    2020-01-03   c\\n    2020-01-04   d\\n    with context = (2020-01-02, 2002-01-04), result will be:\\n    time       value  roll_count    count\\n    2020-01-02   b      2            3\\n    2020-01-03   c      2            3\\n    2020-01-04   d      2            3\\n    Because there are 3 rows within user context (01-02, 01-04),\\n    count should return 3 for every row, rather 4, based on the\\n    adjusted context (01-01, 01-04).\\n    \"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    result_pd = table.mutate(count_1h=table['value'].count().over(ibis_windows[0]), count=table['value'].count()).execute(timecontext=context)\n    spark_table = table.compile()\n    spark_window_1h = Window.partitionBy('key').orderBy(F.col('time').cast('long')).rangeBetween(*spark_range[0])\n    expected = spark_table.withColumn('count_1h', F.count(spark_table['value']).over(spark_window_1h)).withColumn('count', F.count(spark_table['value'])).toPandas()\n    expected = expected[expected.time.between(*(t.tz_convert(None) for t in context))].reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)"
        ]
    },
    {
        "func_name": "test_complex_window",
        "original": "def test_complex_window(con):\n    \"\"\"Test window with different sizes mix context adjustment for window op\n    that require context adjustment and non window op that doesn't adjust\n    context.\"\"\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    window = ibis.trailing_window(preceding=ibis.interval(hours=1), order_by='time', group_by='key')\n    window2 = ibis.trailing_window(preceding=ibis.interval(hours=2), order_by='time', group_by='key')\n    window_cum = ibis.cumulative_window(order_by='time', group_by='key')\n    result_pd = table.mutate(count_1h=table['value'].count().over(window), count_2h=table['value'].count().over(window2), count_cum=table['value'].count().over(window_cum)).mutate(count=table['value'].count()).execute(timecontext=context)\n    df = table.execute()\n    expected_win_1h = df.set_index('time').groupby('key').value.rolling('1h', closed='both').count().rename('count_1h').astype(int)\n    expected_win_2h = df.set_index('time').groupby('key').value.rolling('2h', closed='both').count().rename('count_2h').astype(int)\n    expected_cum_win = df.set_index('time').groupby('key').value.expanding().count().rename('count_cum').astype(int)\n    df = df.set_index('time')\n    df = df.assign(count_1h=expected_win_1h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_2h=expected_win_2h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_cum=expected_cum_win.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df['count'] = df.groupby(['key'])['value'].transform('count')\n    df = df.reset_index()\n    expected = df[df.time.between(*(t.tz_convert(None) for t in context))].sort_values(['key']).reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
        "mutated": [
            "def test_complex_window(con):\n    if False:\n        i = 10\n    \"Test window with different sizes mix context adjustment for window op\\n    that require context adjustment and non window op that doesn't adjust\\n    context.\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    window = ibis.trailing_window(preceding=ibis.interval(hours=1), order_by='time', group_by='key')\n    window2 = ibis.trailing_window(preceding=ibis.interval(hours=2), order_by='time', group_by='key')\n    window_cum = ibis.cumulative_window(order_by='time', group_by='key')\n    result_pd = table.mutate(count_1h=table['value'].count().over(window), count_2h=table['value'].count().over(window2), count_cum=table['value'].count().over(window_cum)).mutate(count=table['value'].count()).execute(timecontext=context)\n    df = table.execute()\n    expected_win_1h = df.set_index('time').groupby('key').value.rolling('1h', closed='both').count().rename('count_1h').astype(int)\n    expected_win_2h = df.set_index('time').groupby('key').value.rolling('2h', closed='both').count().rename('count_2h').astype(int)\n    expected_cum_win = df.set_index('time').groupby('key').value.expanding().count().rename('count_cum').astype(int)\n    df = df.set_index('time')\n    df = df.assign(count_1h=expected_win_1h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_2h=expected_win_2h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_cum=expected_cum_win.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df['count'] = df.groupby(['key'])['value'].transform('count')\n    df = df.reset_index()\n    expected = df[df.time.between(*(t.tz_convert(None) for t in context))].sort_values(['key']).reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "def test_complex_window(con):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Test window with different sizes mix context adjustment for window op\\n    that require context adjustment and non window op that doesn't adjust\\n    context.\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    window = ibis.trailing_window(preceding=ibis.interval(hours=1), order_by='time', group_by='key')\n    window2 = ibis.trailing_window(preceding=ibis.interval(hours=2), order_by='time', group_by='key')\n    window_cum = ibis.cumulative_window(order_by='time', group_by='key')\n    result_pd = table.mutate(count_1h=table['value'].count().over(window), count_2h=table['value'].count().over(window2), count_cum=table['value'].count().over(window_cum)).mutate(count=table['value'].count()).execute(timecontext=context)\n    df = table.execute()\n    expected_win_1h = df.set_index('time').groupby('key').value.rolling('1h', closed='both').count().rename('count_1h').astype(int)\n    expected_win_2h = df.set_index('time').groupby('key').value.rolling('2h', closed='both').count().rename('count_2h').astype(int)\n    expected_cum_win = df.set_index('time').groupby('key').value.expanding().count().rename('count_cum').astype(int)\n    df = df.set_index('time')\n    df = df.assign(count_1h=expected_win_1h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_2h=expected_win_2h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_cum=expected_cum_win.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df['count'] = df.groupby(['key'])['value'].transform('count')\n    df = df.reset_index()\n    expected = df[df.time.between(*(t.tz_convert(None) for t in context))].sort_values(['key']).reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "def test_complex_window(con):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Test window with different sizes mix context adjustment for window op\\n    that require context adjustment and non window op that doesn't adjust\\n    context.\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    window = ibis.trailing_window(preceding=ibis.interval(hours=1), order_by='time', group_by='key')\n    window2 = ibis.trailing_window(preceding=ibis.interval(hours=2), order_by='time', group_by='key')\n    window_cum = ibis.cumulative_window(order_by='time', group_by='key')\n    result_pd = table.mutate(count_1h=table['value'].count().over(window), count_2h=table['value'].count().over(window2), count_cum=table['value'].count().over(window_cum)).mutate(count=table['value'].count()).execute(timecontext=context)\n    df = table.execute()\n    expected_win_1h = df.set_index('time').groupby('key').value.rolling('1h', closed='both').count().rename('count_1h').astype(int)\n    expected_win_2h = df.set_index('time').groupby('key').value.rolling('2h', closed='both').count().rename('count_2h').astype(int)\n    expected_cum_win = df.set_index('time').groupby('key').value.expanding().count().rename('count_cum').astype(int)\n    df = df.set_index('time')\n    df = df.assign(count_1h=expected_win_1h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_2h=expected_win_2h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_cum=expected_cum_win.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df['count'] = df.groupby(['key'])['value'].transform('count')\n    df = df.reset_index()\n    expected = df[df.time.between(*(t.tz_convert(None) for t in context))].sort_values(['key']).reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "def test_complex_window(con):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Test window with different sizes mix context adjustment for window op\\n    that require context adjustment and non window op that doesn't adjust\\n    context.\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    window = ibis.trailing_window(preceding=ibis.interval(hours=1), order_by='time', group_by='key')\n    window2 = ibis.trailing_window(preceding=ibis.interval(hours=2), order_by='time', group_by='key')\n    window_cum = ibis.cumulative_window(order_by='time', group_by='key')\n    result_pd = table.mutate(count_1h=table['value'].count().over(window), count_2h=table['value'].count().over(window2), count_cum=table['value'].count().over(window_cum)).mutate(count=table['value'].count()).execute(timecontext=context)\n    df = table.execute()\n    expected_win_1h = df.set_index('time').groupby('key').value.rolling('1h', closed='both').count().rename('count_1h').astype(int)\n    expected_win_2h = df.set_index('time').groupby('key').value.rolling('2h', closed='both').count().rename('count_2h').astype(int)\n    expected_cum_win = df.set_index('time').groupby('key').value.expanding().count().rename('count_cum').astype(int)\n    df = df.set_index('time')\n    df = df.assign(count_1h=expected_win_1h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_2h=expected_win_2h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_cum=expected_cum_win.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df['count'] = df.groupby(['key'])['value'].transform('count')\n    df = df.reset_index()\n    expected = df[df.time.between(*(t.tz_convert(None) for t in context))].sort_values(['key']).reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)",
            "def test_complex_window(con):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Test window with different sizes mix context adjustment for window op\\n    that require context adjustment and non window op that doesn't adjust\\n    context.\"\n    table = con.table('time_indexed_table')\n    context = (pd.Timestamp('20170102 07:00:00', tz='UTC'), pd.Timestamp('20170105', tz='UTC'))\n    window = ibis.trailing_window(preceding=ibis.interval(hours=1), order_by='time', group_by='key')\n    window2 = ibis.trailing_window(preceding=ibis.interval(hours=2), order_by='time', group_by='key')\n    window_cum = ibis.cumulative_window(order_by='time', group_by='key')\n    result_pd = table.mutate(count_1h=table['value'].count().over(window), count_2h=table['value'].count().over(window2), count_cum=table['value'].count().over(window_cum)).mutate(count=table['value'].count()).execute(timecontext=context)\n    df = table.execute()\n    expected_win_1h = df.set_index('time').groupby('key').value.rolling('1h', closed='both').count().rename('count_1h').astype(int)\n    expected_win_2h = df.set_index('time').groupby('key').value.rolling('2h', closed='both').count().rename('count_2h').astype(int)\n    expected_cum_win = df.set_index('time').groupby('key').value.expanding().count().rename('count_cum').astype(int)\n    df = df.set_index('time')\n    df = df.assign(count_1h=expected_win_1h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_2h=expected_win_2h.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df = df.assign(count_cum=expected_cum_win.sort_index(level=['time', 'key']).reset_index(level='key', drop=True))\n    df['count'] = df.groupby(['key'])['value'].transform('count')\n    df = df.reset_index()\n    expected = df[df.time.between(*(t.tz_convert(None) for t in context))].sort_values(['key']).reset_index(drop=True)\n    tm.assert_frame_equal(result_pd, expected)"
        ]
    }
]