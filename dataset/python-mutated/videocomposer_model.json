[
    {
        "func_name": "__init__",
        "original": "def __init__(self, model_dir, *args, **kwargs):\n    \"\"\"\n        Args:\n            model_dir (`str` or `os.PathLike`)\n                Can be either:\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on modelscope\n                      or modelscope.cn. Valid model ids can be located at the root-level, like `bert-base-uncased`,\n                      or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\n                    - A path to a *directory* containing model weights saved using\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\n                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n                      `True`.\n        \"\"\"\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.duration = kwargs.pop('duration', 200)\n    clip_checkpoint = kwargs.pop('clip_checkpoint', 'open_clip_pytorch_model.bin')\n    sd_checkpoint = kwargs.pop('sd_checkpoint', 'v2-1_512-ema-pruned.ckpt')\n    cfg_file_name = kwargs.pop('cfg_file_name', 'exp06_text_depths_vs_style.yaml')\n    _cfg = Config(load=True, cfg_dict=None, cfg_level=None, model_dir=model_dir, cfg_file_name=cfg_file_name)\n    cfg.update(_cfg.cfg_dict)\n    l1 = len(cfg.frame_lens)\n    l2 = len(cfg.feature_framerates)\n    cfg.max_frames = cfg.frame_lens[0 % (l1 * l2) // l2]\n    cfg.batch_size = cfg.batch_sizes[str(cfg.max_frames)]\n    self.cfg = cfg\n    if 'MASTER_ADDR' not in os.environ:\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = find_free_port()\n    self.cfg.pmi_rank = int(os.getenv('RANK', 0))\n    self.cfg.pmi_world_size = int(os.getenv('WORLD_SIZE', 1))\n    setup_seed(self.cfg.seed)\n    self.read_image = kwargs.pop('read_image', False)\n    self.read_style = kwargs.pop('read_style', True)\n    self.read_sketch = kwargs.pop('read_sketch', False)\n    self.save_origin_video = kwargs.pop('save_origin_video', True)\n    self.video_compositions = kwargs.pop('video_compositions', ['text', 'mask', 'depthmap', 'sketch', 'motion', 'image', 'local_image', 'single_sketch'])\n    self.viz_num = self.cfg.batch_size\n    self.clip_encoder = FrozenOpenCLIPEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder = self.clip_encoder.to(self.device)\n    self.clip_encoder_visual = FrozenOpenCLIPVisualEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder_visual.model.to(self.device)\n    ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n    self.autoencoder = AutoencoderKL(ddconfig, 4, ckpt_path=os.path.join(model_dir, sd_checkpoint))\n    self.zero_y = self.clip_encoder('').detach()\n    black_image_feature = self.clip_encoder_visual(self.clip_encoder_visual.black_image).unsqueeze(1)\n    black_image_feature = torch.zeros_like(black_image_feature)\n    self.autoencoder.eval()\n    for param in self.autoencoder.parameters():\n        param.requires_grad = False\n    self.autoencoder.cuda()\n    self.model = UNetSD_temporal(cfg=self.cfg, in_dim=self.cfg.unet_in_dim, concat_dim=self.cfg.unet_concat_dim, dim=self.cfg.unet_dim, y_dim=self.cfg.unet_y_dim, context_dim=self.cfg.unet_context_dim, out_dim=self.cfg.unet_out_dim, dim_mult=self.cfg.unet_dim_mult, num_heads=self.cfg.unet_num_heads, head_dim=self.cfg.unet_head_dim, num_res_blocks=self.cfg.unet_res_blocks, attn_scales=self.cfg.unet_attn_scales, dropout=self.cfg.unet_dropout, temporal_attention=self.cfg.temporal_attention, temporal_attn_times=self.cfg.temporal_attn_times, use_checkpoint=self.cfg.use_checkpoint, use_fps_condition=self.cfg.use_fps_condition, use_sim_mask=self.cfg.use_sim_mask, video_compositions=self.cfg.video_compositions, misc_dropout=self.cfg.misc_dropout, p_all_zero=self.cfg.p_all_zero, p_all_keep=self.cfg.p_all_zero, zero_y=self.zero_y, black_image_feature=black_image_feature).to(self.device)\n    if self.cfg.resume and self.cfg.resume_checkpoint:\n        if hasattr(self.cfg, 'text_to_video_pretrain') and self.cfg.text_to_video_pretrain:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            ss = torch.load(os.path.join(self.model_dir, cfg.resume_checkpoint))\n            ss = {key: p for (key, p) in ss.items() if 'input_blocks.0.0' not in key}\n            self.model.load_state_dict(ss, strict=False)\n        else:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, checkpoint_name), map_location='cpu'), strict=False)\n        torch.cuda.empty_cache()\n    else:\n        raise ValueError(f'The checkpoint file {self.cfg.resume_checkpoint} is wrong ')\n    betas = beta_schedule('linear_sd', self.cfg.num_timesteps, init_beta=0.00085, last_beta=0.012)\n    self.diffusion = GaussianDiffusion(betas=betas, mean_type=self.cfg.mean_type, var_type=self.cfg.var_type, loss_type=self.cfg.loss_type, rescale_timesteps=False)",
        "mutated": [
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n    '\\n        Args:\\n            model_dir (`str` or `os.PathLike`)\\n                Can be either:\\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on modelscope\\n                      or modelscope.cn. Valid model ids can be located at the root-level, like `bert-base-uncased`,\\n                      or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\\n                    - A path to a *directory* containing model weights saved using\\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\\n                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\\n                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\\n                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\\n                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\\n                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\\n                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\\n                      `True`.\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.duration = kwargs.pop('duration', 200)\n    clip_checkpoint = kwargs.pop('clip_checkpoint', 'open_clip_pytorch_model.bin')\n    sd_checkpoint = kwargs.pop('sd_checkpoint', 'v2-1_512-ema-pruned.ckpt')\n    cfg_file_name = kwargs.pop('cfg_file_name', 'exp06_text_depths_vs_style.yaml')\n    _cfg = Config(load=True, cfg_dict=None, cfg_level=None, model_dir=model_dir, cfg_file_name=cfg_file_name)\n    cfg.update(_cfg.cfg_dict)\n    l1 = len(cfg.frame_lens)\n    l2 = len(cfg.feature_framerates)\n    cfg.max_frames = cfg.frame_lens[0 % (l1 * l2) // l2]\n    cfg.batch_size = cfg.batch_sizes[str(cfg.max_frames)]\n    self.cfg = cfg\n    if 'MASTER_ADDR' not in os.environ:\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = find_free_port()\n    self.cfg.pmi_rank = int(os.getenv('RANK', 0))\n    self.cfg.pmi_world_size = int(os.getenv('WORLD_SIZE', 1))\n    setup_seed(self.cfg.seed)\n    self.read_image = kwargs.pop('read_image', False)\n    self.read_style = kwargs.pop('read_style', True)\n    self.read_sketch = kwargs.pop('read_sketch', False)\n    self.save_origin_video = kwargs.pop('save_origin_video', True)\n    self.video_compositions = kwargs.pop('video_compositions', ['text', 'mask', 'depthmap', 'sketch', 'motion', 'image', 'local_image', 'single_sketch'])\n    self.viz_num = self.cfg.batch_size\n    self.clip_encoder = FrozenOpenCLIPEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder = self.clip_encoder.to(self.device)\n    self.clip_encoder_visual = FrozenOpenCLIPVisualEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder_visual.model.to(self.device)\n    ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n    self.autoencoder = AutoencoderKL(ddconfig, 4, ckpt_path=os.path.join(model_dir, sd_checkpoint))\n    self.zero_y = self.clip_encoder('').detach()\n    black_image_feature = self.clip_encoder_visual(self.clip_encoder_visual.black_image).unsqueeze(1)\n    black_image_feature = torch.zeros_like(black_image_feature)\n    self.autoencoder.eval()\n    for param in self.autoencoder.parameters():\n        param.requires_grad = False\n    self.autoencoder.cuda()\n    self.model = UNetSD_temporal(cfg=self.cfg, in_dim=self.cfg.unet_in_dim, concat_dim=self.cfg.unet_concat_dim, dim=self.cfg.unet_dim, y_dim=self.cfg.unet_y_dim, context_dim=self.cfg.unet_context_dim, out_dim=self.cfg.unet_out_dim, dim_mult=self.cfg.unet_dim_mult, num_heads=self.cfg.unet_num_heads, head_dim=self.cfg.unet_head_dim, num_res_blocks=self.cfg.unet_res_blocks, attn_scales=self.cfg.unet_attn_scales, dropout=self.cfg.unet_dropout, temporal_attention=self.cfg.temporal_attention, temporal_attn_times=self.cfg.temporal_attn_times, use_checkpoint=self.cfg.use_checkpoint, use_fps_condition=self.cfg.use_fps_condition, use_sim_mask=self.cfg.use_sim_mask, video_compositions=self.cfg.video_compositions, misc_dropout=self.cfg.misc_dropout, p_all_zero=self.cfg.p_all_zero, p_all_keep=self.cfg.p_all_zero, zero_y=self.zero_y, black_image_feature=black_image_feature).to(self.device)\n    if self.cfg.resume and self.cfg.resume_checkpoint:\n        if hasattr(self.cfg, 'text_to_video_pretrain') and self.cfg.text_to_video_pretrain:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            ss = torch.load(os.path.join(self.model_dir, cfg.resume_checkpoint))\n            ss = {key: p for (key, p) in ss.items() if 'input_blocks.0.0' not in key}\n            self.model.load_state_dict(ss, strict=False)\n        else:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, checkpoint_name), map_location='cpu'), strict=False)\n        torch.cuda.empty_cache()\n    else:\n        raise ValueError(f'The checkpoint file {self.cfg.resume_checkpoint} is wrong ')\n    betas = beta_schedule('linear_sd', self.cfg.num_timesteps, init_beta=0.00085, last_beta=0.012)\n    self.diffusion = GaussianDiffusion(betas=betas, mean_type=self.cfg.mean_type, var_type=self.cfg.var_type, loss_type=self.cfg.loss_type, rescale_timesteps=False)",
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Args:\\n            model_dir (`str` or `os.PathLike`)\\n                Can be either:\\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on modelscope\\n                      or modelscope.cn. Valid model ids can be located at the root-level, like `bert-base-uncased`,\\n                      or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\\n                    - A path to a *directory* containing model weights saved using\\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\\n                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\\n                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\\n                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\\n                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\\n                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\\n                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\\n                      `True`.\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.duration = kwargs.pop('duration', 200)\n    clip_checkpoint = kwargs.pop('clip_checkpoint', 'open_clip_pytorch_model.bin')\n    sd_checkpoint = kwargs.pop('sd_checkpoint', 'v2-1_512-ema-pruned.ckpt')\n    cfg_file_name = kwargs.pop('cfg_file_name', 'exp06_text_depths_vs_style.yaml')\n    _cfg = Config(load=True, cfg_dict=None, cfg_level=None, model_dir=model_dir, cfg_file_name=cfg_file_name)\n    cfg.update(_cfg.cfg_dict)\n    l1 = len(cfg.frame_lens)\n    l2 = len(cfg.feature_framerates)\n    cfg.max_frames = cfg.frame_lens[0 % (l1 * l2) // l2]\n    cfg.batch_size = cfg.batch_sizes[str(cfg.max_frames)]\n    self.cfg = cfg\n    if 'MASTER_ADDR' not in os.environ:\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = find_free_port()\n    self.cfg.pmi_rank = int(os.getenv('RANK', 0))\n    self.cfg.pmi_world_size = int(os.getenv('WORLD_SIZE', 1))\n    setup_seed(self.cfg.seed)\n    self.read_image = kwargs.pop('read_image', False)\n    self.read_style = kwargs.pop('read_style', True)\n    self.read_sketch = kwargs.pop('read_sketch', False)\n    self.save_origin_video = kwargs.pop('save_origin_video', True)\n    self.video_compositions = kwargs.pop('video_compositions', ['text', 'mask', 'depthmap', 'sketch', 'motion', 'image', 'local_image', 'single_sketch'])\n    self.viz_num = self.cfg.batch_size\n    self.clip_encoder = FrozenOpenCLIPEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder = self.clip_encoder.to(self.device)\n    self.clip_encoder_visual = FrozenOpenCLIPVisualEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder_visual.model.to(self.device)\n    ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n    self.autoencoder = AutoencoderKL(ddconfig, 4, ckpt_path=os.path.join(model_dir, sd_checkpoint))\n    self.zero_y = self.clip_encoder('').detach()\n    black_image_feature = self.clip_encoder_visual(self.clip_encoder_visual.black_image).unsqueeze(1)\n    black_image_feature = torch.zeros_like(black_image_feature)\n    self.autoencoder.eval()\n    for param in self.autoencoder.parameters():\n        param.requires_grad = False\n    self.autoencoder.cuda()\n    self.model = UNetSD_temporal(cfg=self.cfg, in_dim=self.cfg.unet_in_dim, concat_dim=self.cfg.unet_concat_dim, dim=self.cfg.unet_dim, y_dim=self.cfg.unet_y_dim, context_dim=self.cfg.unet_context_dim, out_dim=self.cfg.unet_out_dim, dim_mult=self.cfg.unet_dim_mult, num_heads=self.cfg.unet_num_heads, head_dim=self.cfg.unet_head_dim, num_res_blocks=self.cfg.unet_res_blocks, attn_scales=self.cfg.unet_attn_scales, dropout=self.cfg.unet_dropout, temporal_attention=self.cfg.temporal_attention, temporal_attn_times=self.cfg.temporal_attn_times, use_checkpoint=self.cfg.use_checkpoint, use_fps_condition=self.cfg.use_fps_condition, use_sim_mask=self.cfg.use_sim_mask, video_compositions=self.cfg.video_compositions, misc_dropout=self.cfg.misc_dropout, p_all_zero=self.cfg.p_all_zero, p_all_keep=self.cfg.p_all_zero, zero_y=self.zero_y, black_image_feature=black_image_feature).to(self.device)\n    if self.cfg.resume and self.cfg.resume_checkpoint:\n        if hasattr(self.cfg, 'text_to_video_pretrain') and self.cfg.text_to_video_pretrain:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            ss = torch.load(os.path.join(self.model_dir, cfg.resume_checkpoint))\n            ss = {key: p for (key, p) in ss.items() if 'input_blocks.0.0' not in key}\n            self.model.load_state_dict(ss, strict=False)\n        else:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, checkpoint_name), map_location='cpu'), strict=False)\n        torch.cuda.empty_cache()\n    else:\n        raise ValueError(f'The checkpoint file {self.cfg.resume_checkpoint} is wrong ')\n    betas = beta_schedule('linear_sd', self.cfg.num_timesteps, init_beta=0.00085, last_beta=0.012)\n    self.diffusion = GaussianDiffusion(betas=betas, mean_type=self.cfg.mean_type, var_type=self.cfg.var_type, loss_type=self.cfg.loss_type, rescale_timesteps=False)",
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Args:\\n            model_dir (`str` or `os.PathLike`)\\n                Can be either:\\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on modelscope\\n                      or modelscope.cn. Valid model ids can be located at the root-level, like `bert-base-uncased`,\\n                      or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\\n                    - A path to a *directory* containing model weights saved using\\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\\n                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\\n                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\\n                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\\n                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\\n                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\\n                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\\n                      `True`.\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.duration = kwargs.pop('duration', 200)\n    clip_checkpoint = kwargs.pop('clip_checkpoint', 'open_clip_pytorch_model.bin')\n    sd_checkpoint = kwargs.pop('sd_checkpoint', 'v2-1_512-ema-pruned.ckpt')\n    cfg_file_name = kwargs.pop('cfg_file_name', 'exp06_text_depths_vs_style.yaml')\n    _cfg = Config(load=True, cfg_dict=None, cfg_level=None, model_dir=model_dir, cfg_file_name=cfg_file_name)\n    cfg.update(_cfg.cfg_dict)\n    l1 = len(cfg.frame_lens)\n    l2 = len(cfg.feature_framerates)\n    cfg.max_frames = cfg.frame_lens[0 % (l1 * l2) // l2]\n    cfg.batch_size = cfg.batch_sizes[str(cfg.max_frames)]\n    self.cfg = cfg\n    if 'MASTER_ADDR' not in os.environ:\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = find_free_port()\n    self.cfg.pmi_rank = int(os.getenv('RANK', 0))\n    self.cfg.pmi_world_size = int(os.getenv('WORLD_SIZE', 1))\n    setup_seed(self.cfg.seed)\n    self.read_image = kwargs.pop('read_image', False)\n    self.read_style = kwargs.pop('read_style', True)\n    self.read_sketch = kwargs.pop('read_sketch', False)\n    self.save_origin_video = kwargs.pop('save_origin_video', True)\n    self.video_compositions = kwargs.pop('video_compositions', ['text', 'mask', 'depthmap', 'sketch', 'motion', 'image', 'local_image', 'single_sketch'])\n    self.viz_num = self.cfg.batch_size\n    self.clip_encoder = FrozenOpenCLIPEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder = self.clip_encoder.to(self.device)\n    self.clip_encoder_visual = FrozenOpenCLIPVisualEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder_visual.model.to(self.device)\n    ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n    self.autoencoder = AutoencoderKL(ddconfig, 4, ckpt_path=os.path.join(model_dir, sd_checkpoint))\n    self.zero_y = self.clip_encoder('').detach()\n    black_image_feature = self.clip_encoder_visual(self.clip_encoder_visual.black_image).unsqueeze(1)\n    black_image_feature = torch.zeros_like(black_image_feature)\n    self.autoencoder.eval()\n    for param in self.autoencoder.parameters():\n        param.requires_grad = False\n    self.autoencoder.cuda()\n    self.model = UNetSD_temporal(cfg=self.cfg, in_dim=self.cfg.unet_in_dim, concat_dim=self.cfg.unet_concat_dim, dim=self.cfg.unet_dim, y_dim=self.cfg.unet_y_dim, context_dim=self.cfg.unet_context_dim, out_dim=self.cfg.unet_out_dim, dim_mult=self.cfg.unet_dim_mult, num_heads=self.cfg.unet_num_heads, head_dim=self.cfg.unet_head_dim, num_res_blocks=self.cfg.unet_res_blocks, attn_scales=self.cfg.unet_attn_scales, dropout=self.cfg.unet_dropout, temporal_attention=self.cfg.temporal_attention, temporal_attn_times=self.cfg.temporal_attn_times, use_checkpoint=self.cfg.use_checkpoint, use_fps_condition=self.cfg.use_fps_condition, use_sim_mask=self.cfg.use_sim_mask, video_compositions=self.cfg.video_compositions, misc_dropout=self.cfg.misc_dropout, p_all_zero=self.cfg.p_all_zero, p_all_keep=self.cfg.p_all_zero, zero_y=self.zero_y, black_image_feature=black_image_feature).to(self.device)\n    if self.cfg.resume and self.cfg.resume_checkpoint:\n        if hasattr(self.cfg, 'text_to_video_pretrain') and self.cfg.text_to_video_pretrain:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            ss = torch.load(os.path.join(self.model_dir, cfg.resume_checkpoint))\n            ss = {key: p for (key, p) in ss.items() if 'input_blocks.0.0' not in key}\n            self.model.load_state_dict(ss, strict=False)\n        else:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, checkpoint_name), map_location='cpu'), strict=False)\n        torch.cuda.empty_cache()\n    else:\n        raise ValueError(f'The checkpoint file {self.cfg.resume_checkpoint} is wrong ')\n    betas = beta_schedule('linear_sd', self.cfg.num_timesteps, init_beta=0.00085, last_beta=0.012)\n    self.diffusion = GaussianDiffusion(betas=betas, mean_type=self.cfg.mean_type, var_type=self.cfg.var_type, loss_type=self.cfg.loss_type, rescale_timesteps=False)",
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Args:\\n            model_dir (`str` or `os.PathLike`)\\n                Can be either:\\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on modelscope\\n                      or modelscope.cn. Valid model ids can be located at the root-level, like `bert-base-uncased`,\\n                      or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\\n                    - A path to a *directory* containing model weights saved using\\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\\n                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\\n                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\\n                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\\n                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\\n                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\\n                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\\n                      `True`.\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.duration = kwargs.pop('duration', 200)\n    clip_checkpoint = kwargs.pop('clip_checkpoint', 'open_clip_pytorch_model.bin')\n    sd_checkpoint = kwargs.pop('sd_checkpoint', 'v2-1_512-ema-pruned.ckpt')\n    cfg_file_name = kwargs.pop('cfg_file_name', 'exp06_text_depths_vs_style.yaml')\n    _cfg = Config(load=True, cfg_dict=None, cfg_level=None, model_dir=model_dir, cfg_file_name=cfg_file_name)\n    cfg.update(_cfg.cfg_dict)\n    l1 = len(cfg.frame_lens)\n    l2 = len(cfg.feature_framerates)\n    cfg.max_frames = cfg.frame_lens[0 % (l1 * l2) // l2]\n    cfg.batch_size = cfg.batch_sizes[str(cfg.max_frames)]\n    self.cfg = cfg\n    if 'MASTER_ADDR' not in os.environ:\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = find_free_port()\n    self.cfg.pmi_rank = int(os.getenv('RANK', 0))\n    self.cfg.pmi_world_size = int(os.getenv('WORLD_SIZE', 1))\n    setup_seed(self.cfg.seed)\n    self.read_image = kwargs.pop('read_image', False)\n    self.read_style = kwargs.pop('read_style', True)\n    self.read_sketch = kwargs.pop('read_sketch', False)\n    self.save_origin_video = kwargs.pop('save_origin_video', True)\n    self.video_compositions = kwargs.pop('video_compositions', ['text', 'mask', 'depthmap', 'sketch', 'motion', 'image', 'local_image', 'single_sketch'])\n    self.viz_num = self.cfg.batch_size\n    self.clip_encoder = FrozenOpenCLIPEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder = self.clip_encoder.to(self.device)\n    self.clip_encoder_visual = FrozenOpenCLIPVisualEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder_visual.model.to(self.device)\n    ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n    self.autoencoder = AutoencoderKL(ddconfig, 4, ckpt_path=os.path.join(model_dir, sd_checkpoint))\n    self.zero_y = self.clip_encoder('').detach()\n    black_image_feature = self.clip_encoder_visual(self.clip_encoder_visual.black_image).unsqueeze(1)\n    black_image_feature = torch.zeros_like(black_image_feature)\n    self.autoencoder.eval()\n    for param in self.autoencoder.parameters():\n        param.requires_grad = False\n    self.autoencoder.cuda()\n    self.model = UNetSD_temporal(cfg=self.cfg, in_dim=self.cfg.unet_in_dim, concat_dim=self.cfg.unet_concat_dim, dim=self.cfg.unet_dim, y_dim=self.cfg.unet_y_dim, context_dim=self.cfg.unet_context_dim, out_dim=self.cfg.unet_out_dim, dim_mult=self.cfg.unet_dim_mult, num_heads=self.cfg.unet_num_heads, head_dim=self.cfg.unet_head_dim, num_res_blocks=self.cfg.unet_res_blocks, attn_scales=self.cfg.unet_attn_scales, dropout=self.cfg.unet_dropout, temporal_attention=self.cfg.temporal_attention, temporal_attn_times=self.cfg.temporal_attn_times, use_checkpoint=self.cfg.use_checkpoint, use_fps_condition=self.cfg.use_fps_condition, use_sim_mask=self.cfg.use_sim_mask, video_compositions=self.cfg.video_compositions, misc_dropout=self.cfg.misc_dropout, p_all_zero=self.cfg.p_all_zero, p_all_keep=self.cfg.p_all_zero, zero_y=self.zero_y, black_image_feature=black_image_feature).to(self.device)\n    if self.cfg.resume and self.cfg.resume_checkpoint:\n        if hasattr(self.cfg, 'text_to_video_pretrain') and self.cfg.text_to_video_pretrain:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            ss = torch.load(os.path.join(self.model_dir, cfg.resume_checkpoint))\n            ss = {key: p for (key, p) in ss.items() if 'input_blocks.0.0' not in key}\n            self.model.load_state_dict(ss, strict=False)\n        else:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, checkpoint_name), map_location='cpu'), strict=False)\n        torch.cuda.empty_cache()\n    else:\n        raise ValueError(f'The checkpoint file {self.cfg.resume_checkpoint} is wrong ')\n    betas = beta_schedule('linear_sd', self.cfg.num_timesteps, init_beta=0.00085, last_beta=0.012)\n    self.diffusion = GaussianDiffusion(betas=betas, mean_type=self.cfg.mean_type, var_type=self.cfg.var_type, loss_type=self.cfg.loss_type, rescale_timesteps=False)",
            "def __init__(self, model_dir, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Args:\\n            model_dir (`str` or `os.PathLike`)\\n                Can be either:\\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on modelscope\\n                      or modelscope.cn. Valid model ids can be located at the root-level, like `bert-base-uncased`,\\n                      or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\\n                    - A path to a *directory* containing model weights saved using\\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\\n                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\\n                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\\n                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\\n                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\\n                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\\n                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\\n                      `True`.\\n        '\n    super().__init__(*args, model_dir=model_dir, **kwargs)\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.duration = kwargs.pop('duration', 200)\n    clip_checkpoint = kwargs.pop('clip_checkpoint', 'open_clip_pytorch_model.bin')\n    sd_checkpoint = kwargs.pop('sd_checkpoint', 'v2-1_512-ema-pruned.ckpt')\n    cfg_file_name = kwargs.pop('cfg_file_name', 'exp06_text_depths_vs_style.yaml')\n    _cfg = Config(load=True, cfg_dict=None, cfg_level=None, model_dir=model_dir, cfg_file_name=cfg_file_name)\n    cfg.update(_cfg.cfg_dict)\n    l1 = len(cfg.frame_lens)\n    l2 = len(cfg.feature_framerates)\n    cfg.max_frames = cfg.frame_lens[0 % (l1 * l2) // l2]\n    cfg.batch_size = cfg.batch_sizes[str(cfg.max_frames)]\n    self.cfg = cfg\n    if 'MASTER_ADDR' not in os.environ:\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = find_free_port()\n    self.cfg.pmi_rank = int(os.getenv('RANK', 0))\n    self.cfg.pmi_world_size = int(os.getenv('WORLD_SIZE', 1))\n    setup_seed(self.cfg.seed)\n    self.read_image = kwargs.pop('read_image', False)\n    self.read_style = kwargs.pop('read_style', True)\n    self.read_sketch = kwargs.pop('read_sketch', False)\n    self.save_origin_video = kwargs.pop('save_origin_video', True)\n    self.video_compositions = kwargs.pop('video_compositions', ['text', 'mask', 'depthmap', 'sketch', 'motion', 'image', 'local_image', 'single_sketch'])\n    self.viz_num = self.cfg.batch_size\n    self.clip_encoder = FrozenOpenCLIPEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder = self.clip_encoder.to(self.device)\n    self.clip_encoder_visual = FrozenOpenCLIPVisualEmbedder(layer='penultimate', pretrained=os.path.join(model_dir, clip_checkpoint))\n    self.clip_encoder_visual.model.to(self.device)\n    ddconfig = {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}\n    self.autoencoder = AutoencoderKL(ddconfig, 4, ckpt_path=os.path.join(model_dir, sd_checkpoint))\n    self.zero_y = self.clip_encoder('').detach()\n    black_image_feature = self.clip_encoder_visual(self.clip_encoder_visual.black_image).unsqueeze(1)\n    black_image_feature = torch.zeros_like(black_image_feature)\n    self.autoencoder.eval()\n    for param in self.autoencoder.parameters():\n        param.requires_grad = False\n    self.autoencoder.cuda()\n    self.model = UNetSD_temporal(cfg=self.cfg, in_dim=self.cfg.unet_in_dim, concat_dim=self.cfg.unet_concat_dim, dim=self.cfg.unet_dim, y_dim=self.cfg.unet_y_dim, context_dim=self.cfg.unet_context_dim, out_dim=self.cfg.unet_out_dim, dim_mult=self.cfg.unet_dim_mult, num_heads=self.cfg.unet_num_heads, head_dim=self.cfg.unet_head_dim, num_res_blocks=self.cfg.unet_res_blocks, attn_scales=self.cfg.unet_attn_scales, dropout=self.cfg.unet_dropout, temporal_attention=self.cfg.temporal_attention, temporal_attn_times=self.cfg.temporal_attn_times, use_checkpoint=self.cfg.use_checkpoint, use_fps_condition=self.cfg.use_fps_condition, use_sim_mask=self.cfg.use_sim_mask, video_compositions=self.cfg.video_compositions, misc_dropout=self.cfg.misc_dropout, p_all_zero=self.cfg.p_all_zero, p_all_keep=self.cfg.p_all_zero, zero_y=self.zero_y, black_image_feature=black_image_feature).to(self.device)\n    if self.cfg.resume and self.cfg.resume_checkpoint:\n        if hasattr(self.cfg, 'text_to_video_pretrain') and self.cfg.text_to_video_pretrain:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            ss = torch.load(os.path.join(self.model_dir, cfg.resume_checkpoint))\n            ss = {key: p for (key, p) in ss.items() if 'input_blocks.0.0' not in key}\n            self.model.load_state_dict(ss, strict=False)\n        else:\n            checkpoint_name = cfg.resume_checkpoint.split('/')[-1]\n            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, checkpoint_name), map_location='cpu'), strict=False)\n        torch.cuda.empty_cache()\n    else:\n        raise ValueError(f'The checkpoint file {self.cfg.resume_checkpoint} is wrong ')\n    betas = beta_schedule('linear_sd', self.cfg.num_timesteps, init_beta=0.00085, last_beta=0.012)\n    self.diffusion = GaussianDiffusion(betas=betas, mean_type=self.cfg.mean_type, var_type=self.cfg.var_type, loss_type=self.cfg.loss_type, rescale_timesteps=False)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, input: Dict[str, Any]):\n    frame_in = None\n    if self.read_image:\n        image_key = input['style_image']\n        frame = load_image(image_key)\n        frame_in = misc_transforms([frame])\n    frame_sketch = None\n    if self.read_sketch:\n        sketch_key = self.cfg.sketch_path\n        frame_sketch = load_image(sketch_key)\n        frame_sketch = misc_transforms([frame_sketch])\n    frame_style = None\n    if self.read_style:\n        frame_style = load_image(input['style_image'])\n    if 'depthmap' in self.video_compositions:\n        midas = models.midas_v3(pretrained=True, model_dir=self.model_dir).eval().requires_grad_(False).to(memory_format=torch.channels_last).half().to(self.device)\n    if 'canny' in self.video_compositions:\n        canny_detector = CannyDetector()\n    if 'sketch' in self.video_compositions:\n        pidinet = pidinet_bsd(self.model_dir, pretrained=True, vanilla_cnn=True).eval().requires_grad_(False).to(self.device)\n        cleaner = sketch_simplification_gan(self.model_dir, pretrained=True).eval().requires_grad_(False).to(self.device)\n        pidi_mean = torch.tensor(self.cfg.sketch_mean).view(1, -1, 1, 1).to(self.device)\n        pidi_std = torch.tensor(self.cfg.sketch_std).view(1, -1, 1, 1).to(self.device)\n    palette = None\n    self.model.eval()\n    caps = input['cap_txt']\n    if self.cfg.max_frames == 1 and self.cfg.use_image_dataset:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    else:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    misc_backups = copy(misc_data)\n    misc_backups = rearrange(misc_backups, 'b f c h w -> b c f h w')\n    mv_data_video = []\n    if 'motion' in self.cfg.video_compositions:\n        mv_data_video = rearrange(mv_data, 'b f c h w -> b c f h w')\n    masked_video = []\n    if 'mask' in self.cfg.video_compositions:\n        masked_video = make_masked_images(misc_data.sub(0.5).div_(0.5), mask)\n        masked_video = rearrange(masked_video, 'b f c h w -> b c f h w')\n    image_local = []\n    if 'local_image' in self.cfg.video_compositions:\n        frames_num = misc_data.shape[1]\n        bs_vd_local = misc_data.shape[0]\n        if self.cfg.read_image:\n            image_local = frame_in.unsqueeze(0).repeat(bs_vd_local, frames_num, 1, 1, 1).cuda()\n        else:\n            image_local = misc_data[:, :1].clone().repeat(1, frames_num, 1, 1, 1)\n        image_local = rearrange(image_local, 'b f c h w -> b c f h w', b=bs_vd_local)\n    bs_vd = video_data.shape[0]\n    video_data = rearrange(video_data, 'b f c h w -> (b f) c h w')\n    misc_data = rearrange(misc_data, 'b f c h w -> (b f) c h w')\n    video_data_list = torch.chunk(video_data, video_data.shape[0] // self.cfg.chunk_size, dim=0)\n    misc_data_list = torch.chunk(misc_data, misc_data.shape[0] // self.cfg.chunk_size, dim=0)\n    with torch.no_grad():\n        decode_data = []\n        for vd_data in video_data_list:\n            encoder_posterior = self.autoencoder.encode(vd_data)\n            tmp = get_first_stage_encoding(encoder_posterior).detach()\n            decode_data.append(tmp)\n        video_data = torch.cat(decode_data, dim=0)\n        video_data = rearrange(video_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        depth_data = []\n        if 'depthmap' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                depth = midas(misc_imgs.sub(0.5).div_(0.5).to(memory_format=torch.channels_last).half())\n                depth = (depth / self.cfg.depth_std).clamp_(0, self.cfg.depth_clamp)\n                depth_data.append(depth)\n            depth_data = torch.cat(depth_data, dim=0)\n            depth_data = rearrange(depth_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        canny_data = []\n        if 'canny' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                misc_imgs = rearrange(misc_imgs.clone(), 'k c h w -> k h w c')\n                canny_condition = torch.stack([canny_detector(misc_img) for misc_img in misc_imgs])\n                canny_condition = rearrange(canny_condition, 'k h w c-> k c h w')\n                canny_data.append(canny_condition)\n            canny_data = torch.cat(canny_data, dim=0)\n            canny_data = rearrange(canny_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        sketch_data = []\n        if 'sketch' in self.cfg.video_compositions:\n            sketch_list = misc_data_list\n            if self.cfg.read_sketch:\n                sketch_repeat = frame_sketch.repeat(frames_num, 1, 1, 1).cuda()\n                sketch_list = [sketch_repeat]\n            for misc_imgs in sketch_list:\n                sketch = pidinet(misc_imgs.sub(pidi_mean).div_(pidi_std))\n                sketch = 1.0 - cleaner(1.0 - sketch)\n                sketch_data.append(sketch)\n            sketch_data = torch.cat(sketch_data, dim=0)\n            sketch_data = rearrange(sketch_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        single_sketch_data = []\n        if 'single_sketch' in self.cfg.video_compositions:\n            single_sketch_data = sketch_data.clone()[:, :, :1].repeat(1, 1, frames_num, 1, 1)\n    y = self.clip_encoder(caps).detach()\n    y0 = y.clone()\n    y_visual = []\n    if 'image' in self.cfg.video_compositions:\n        with torch.no_grad():\n            if self.cfg.read_style:\n                y_visual = self.clip_encoder_visual(self.clip_encoder_visual.preprocess(frame_style).unsqueeze(0).cuda()).unsqueeze(0)\n                y_visual0 = y_visual.clone()\n            else:\n                ref_imgs = ref_imgs.squeeze(1)\n                y_visual = self.clip_encoder_visual(ref_imgs).unsqueeze(1)\n                y_visual0 = y_visual.clone()\n    with torch.no_grad():\n        pynvml.nvmlInit()\n        with amp.autocast(enabled=self.cfg.use_fp16):\n            if self.cfg.share_noise:\n                (b, c, f, h, w) = video_data.shape\n                noise = torch.randn((self.viz_num, c, h, w), device=self.device)\n                noise = noise.repeat_interleave(repeats=f, dim=0)\n                noise = rearrange(noise, '(b f) c h w->b c f h w', b=self.viz_num)\n                noise = noise.contiguous()\n            else:\n                noise = torch.randn_like(video_data[:self.viz_num])\n            full_model_kwargs = [{'y': y0[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else y_visual0[:self.viz_num], 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}, {'y': self.zero_y.repeat(self.viz_num, 1, 1) if not self.cfg.use_fps_condition else torch.zeros_like(y0)[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else torch.zeros_like(y_visual0[:self.viz_num]), 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}]\n            partial_keys = self.cfg.guidances\n            noise_motion = noise.clone()\n            model_kwargs = prepare_model_kwargs(partial_keys=partial_keys, full_model_kwargs=full_model_kwargs, use_fps_condition=self.cfg.use_fps_condition)\n            video_output = self.diffusion.ddim_sample_loop(noise=noise_motion, model=self.model.eval(), model_kwargs=model_kwargs, guide_scale=9.0, ddim_timesteps=self.cfg.ddim_timesteps, eta=0.0)\n            save_with_model_kwargs(model_kwargs=model_kwargs, video_data=video_output, autoencoder=self.autoencoder, ori_video=misc_backups, viz_num=self.viz_num, step=0, caps=caps, palette=palette, cfg=self.cfg, duration=self.duration)\n    return {'video': video_output.type(torch.float32).cpu(), 'video_path': self.cfg}",
        "mutated": [
            "def forward(self, input: Dict[str, Any]):\n    if False:\n        i = 10\n    frame_in = None\n    if self.read_image:\n        image_key = input['style_image']\n        frame = load_image(image_key)\n        frame_in = misc_transforms([frame])\n    frame_sketch = None\n    if self.read_sketch:\n        sketch_key = self.cfg.sketch_path\n        frame_sketch = load_image(sketch_key)\n        frame_sketch = misc_transforms([frame_sketch])\n    frame_style = None\n    if self.read_style:\n        frame_style = load_image(input['style_image'])\n    if 'depthmap' in self.video_compositions:\n        midas = models.midas_v3(pretrained=True, model_dir=self.model_dir).eval().requires_grad_(False).to(memory_format=torch.channels_last).half().to(self.device)\n    if 'canny' in self.video_compositions:\n        canny_detector = CannyDetector()\n    if 'sketch' in self.video_compositions:\n        pidinet = pidinet_bsd(self.model_dir, pretrained=True, vanilla_cnn=True).eval().requires_grad_(False).to(self.device)\n        cleaner = sketch_simplification_gan(self.model_dir, pretrained=True).eval().requires_grad_(False).to(self.device)\n        pidi_mean = torch.tensor(self.cfg.sketch_mean).view(1, -1, 1, 1).to(self.device)\n        pidi_std = torch.tensor(self.cfg.sketch_std).view(1, -1, 1, 1).to(self.device)\n    palette = None\n    self.model.eval()\n    caps = input['cap_txt']\n    if self.cfg.max_frames == 1 and self.cfg.use_image_dataset:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    else:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    misc_backups = copy(misc_data)\n    misc_backups = rearrange(misc_backups, 'b f c h w -> b c f h w')\n    mv_data_video = []\n    if 'motion' in self.cfg.video_compositions:\n        mv_data_video = rearrange(mv_data, 'b f c h w -> b c f h w')\n    masked_video = []\n    if 'mask' in self.cfg.video_compositions:\n        masked_video = make_masked_images(misc_data.sub(0.5).div_(0.5), mask)\n        masked_video = rearrange(masked_video, 'b f c h w -> b c f h w')\n    image_local = []\n    if 'local_image' in self.cfg.video_compositions:\n        frames_num = misc_data.shape[1]\n        bs_vd_local = misc_data.shape[0]\n        if self.cfg.read_image:\n            image_local = frame_in.unsqueeze(0).repeat(bs_vd_local, frames_num, 1, 1, 1).cuda()\n        else:\n            image_local = misc_data[:, :1].clone().repeat(1, frames_num, 1, 1, 1)\n        image_local = rearrange(image_local, 'b f c h w -> b c f h w', b=bs_vd_local)\n    bs_vd = video_data.shape[0]\n    video_data = rearrange(video_data, 'b f c h w -> (b f) c h w')\n    misc_data = rearrange(misc_data, 'b f c h w -> (b f) c h w')\n    video_data_list = torch.chunk(video_data, video_data.shape[0] // self.cfg.chunk_size, dim=0)\n    misc_data_list = torch.chunk(misc_data, misc_data.shape[0] // self.cfg.chunk_size, dim=0)\n    with torch.no_grad():\n        decode_data = []\n        for vd_data in video_data_list:\n            encoder_posterior = self.autoencoder.encode(vd_data)\n            tmp = get_first_stage_encoding(encoder_posterior).detach()\n            decode_data.append(tmp)\n        video_data = torch.cat(decode_data, dim=0)\n        video_data = rearrange(video_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        depth_data = []\n        if 'depthmap' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                depth = midas(misc_imgs.sub(0.5).div_(0.5).to(memory_format=torch.channels_last).half())\n                depth = (depth / self.cfg.depth_std).clamp_(0, self.cfg.depth_clamp)\n                depth_data.append(depth)\n            depth_data = torch.cat(depth_data, dim=0)\n            depth_data = rearrange(depth_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        canny_data = []\n        if 'canny' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                misc_imgs = rearrange(misc_imgs.clone(), 'k c h w -> k h w c')\n                canny_condition = torch.stack([canny_detector(misc_img) for misc_img in misc_imgs])\n                canny_condition = rearrange(canny_condition, 'k h w c-> k c h w')\n                canny_data.append(canny_condition)\n            canny_data = torch.cat(canny_data, dim=0)\n            canny_data = rearrange(canny_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        sketch_data = []\n        if 'sketch' in self.cfg.video_compositions:\n            sketch_list = misc_data_list\n            if self.cfg.read_sketch:\n                sketch_repeat = frame_sketch.repeat(frames_num, 1, 1, 1).cuda()\n                sketch_list = [sketch_repeat]\n            for misc_imgs in sketch_list:\n                sketch = pidinet(misc_imgs.sub(pidi_mean).div_(pidi_std))\n                sketch = 1.0 - cleaner(1.0 - sketch)\n                sketch_data.append(sketch)\n            sketch_data = torch.cat(sketch_data, dim=0)\n            sketch_data = rearrange(sketch_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        single_sketch_data = []\n        if 'single_sketch' in self.cfg.video_compositions:\n            single_sketch_data = sketch_data.clone()[:, :, :1].repeat(1, 1, frames_num, 1, 1)\n    y = self.clip_encoder(caps).detach()\n    y0 = y.clone()\n    y_visual = []\n    if 'image' in self.cfg.video_compositions:\n        with torch.no_grad():\n            if self.cfg.read_style:\n                y_visual = self.clip_encoder_visual(self.clip_encoder_visual.preprocess(frame_style).unsqueeze(0).cuda()).unsqueeze(0)\n                y_visual0 = y_visual.clone()\n            else:\n                ref_imgs = ref_imgs.squeeze(1)\n                y_visual = self.clip_encoder_visual(ref_imgs).unsqueeze(1)\n                y_visual0 = y_visual.clone()\n    with torch.no_grad():\n        pynvml.nvmlInit()\n        with amp.autocast(enabled=self.cfg.use_fp16):\n            if self.cfg.share_noise:\n                (b, c, f, h, w) = video_data.shape\n                noise = torch.randn((self.viz_num, c, h, w), device=self.device)\n                noise = noise.repeat_interleave(repeats=f, dim=0)\n                noise = rearrange(noise, '(b f) c h w->b c f h w', b=self.viz_num)\n                noise = noise.contiguous()\n            else:\n                noise = torch.randn_like(video_data[:self.viz_num])\n            full_model_kwargs = [{'y': y0[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else y_visual0[:self.viz_num], 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}, {'y': self.zero_y.repeat(self.viz_num, 1, 1) if not self.cfg.use_fps_condition else torch.zeros_like(y0)[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else torch.zeros_like(y_visual0[:self.viz_num]), 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}]\n            partial_keys = self.cfg.guidances\n            noise_motion = noise.clone()\n            model_kwargs = prepare_model_kwargs(partial_keys=partial_keys, full_model_kwargs=full_model_kwargs, use_fps_condition=self.cfg.use_fps_condition)\n            video_output = self.diffusion.ddim_sample_loop(noise=noise_motion, model=self.model.eval(), model_kwargs=model_kwargs, guide_scale=9.0, ddim_timesteps=self.cfg.ddim_timesteps, eta=0.0)\n            save_with_model_kwargs(model_kwargs=model_kwargs, video_data=video_output, autoencoder=self.autoencoder, ori_video=misc_backups, viz_num=self.viz_num, step=0, caps=caps, palette=palette, cfg=self.cfg, duration=self.duration)\n    return {'video': video_output.type(torch.float32).cpu(), 'video_path': self.cfg}",
            "def forward(self, input: Dict[str, Any]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    frame_in = None\n    if self.read_image:\n        image_key = input['style_image']\n        frame = load_image(image_key)\n        frame_in = misc_transforms([frame])\n    frame_sketch = None\n    if self.read_sketch:\n        sketch_key = self.cfg.sketch_path\n        frame_sketch = load_image(sketch_key)\n        frame_sketch = misc_transforms([frame_sketch])\n    frame_style = None\n    if self.read_style:\n        frame_style = load_image(input['style_image'])\n    if 'depthmap' in self.video_compositions:\n        midas = models.midas_v3(pretrained=True, model_dir=self.model_dir).eval().requires_grad_(False).to(memory_format=torch.channels_last).half().to(self.device)\n    if 'canny' in self.video_compositions:\n        canny_detector = CannyDetector()\n    if 'sketch' in self.video_compositions:\n        pidinet = pidinet_bsd(self.model_dir, pretrained=True, vanilla_cnn=True).eval().requires_grad_(False).to(self.device)\n        cleaner = sketch_simplification_gan(self.model_dir, pretrained=True).eval().requires_grad_(False).to(self.device)\n        pidi_mean = torch.tensor(self.cfg.sketch_mean).view(1, -1, 1, 1).to(self.device)\n        pidi_std = torch.tensor(self.cfg.sketch_std).view(1, -1, 1, 1).to(self.device)\n    palette = None\n    self.model.eval()\n    caps = input['cap_txt']\n    if self.cfg.max_frames == 1 and self.cfg.use_image_dataset:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    else:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    misc_backups = copy(misc_data)\n    misc_backups = rearrange(misc_backups, 'b f c h w -> b c f h w')\n    mv_data_video = []\n    if 'motion' in self.cfg.video_compositions:\n        mv_data_video = rearrange(mv_data, 'b f c h w -> b c f h w')\n    masked_video = []\n    if 'mask' in self.cfg.video_compositions:\n        masked_video = make_masked_images(misc_data.sub(0.5).div_(0.5), mask)\n        masked_video = rearrange(masked_video, 'b f c h w -> b c f h w')\n    image_local = []\n    if 'local_image' in self.cfg.video_compositions:\n        frames_num = misc_data.shape[1]\n        bs_vd_local = misc_data.shape[0]\n        if self.cfg.read_image:\n            image_local = frame_in.unsqueeze(0).repeat(bs_vd_local, frames_num, 1, 1, 1).cuda()\n        else:\n            image_local = misc_data[:, :1].clone().repeat(1, frames_num, 1, 1, 1)\n        image_local = rearrange(image_local, 'b f c h w -> b c f h w', b=bs_vd_local)\n    bs_vd = video_data.shape[0]\n    video_data = rearrange(video_data, 'b f c h w -> (b f) c h w')\n    misc_data = rearrange(misc_data, 'b f c h w -> (b f) c h w')\n    video_data_list = torch.chunk(video_data, video_data.shape[0] // self.cfg.chunk_size, dim=0)\n    misc_data_list = torch.chunk(misc_data, misc_data.shape[0] // self.cfg.chunk_size, dim=0)\n    with torch.no_grad():\n        decode_data = []\n        for vd_data in video_data_list:\n            encoder_posterior = self.autoencoder.encode(vd_data)\n            tmp = get_first_stage_encoding(encoder_posterior).detach()\n            decode_data.append(tmp)\n        video_data = torch.cat(decode_data, dim=0)\n        video_data = rearrange(video_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        depth_data = []\n        if 'depthmap' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                depth = midas(misc_imgs.sub(0.5).div_(0.5).to(memory_format=torch.channels_last).half())\n                depth = (depth / self.cfg.depth_std).clamp_(0, self.cfg.depth_clamp)\n                depth_data.append(depth)\n            depth_data = torch.cat(depth_data, dim=0)\n            depth_data = rearrange(depth_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        canny_data = []\n        if 'canny' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                misc_imgs = rearrange(misc_imgs.clone(), 'k c h w -> k h w c')\n                canny_condition = torch.stack([canny_detector(misc_img) for misc_img in misc_imgs])\n                canny_condition = rearrange(canny_condition, 'k h w c-> k c h w')\n                canny_data.append(canny_condition)\n            canny_data = torch.cat(canny_data, dim=0)\n            canny_data = rearrange(canny_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        sketch_data = []\n        if 'sketch' in self.cfg.video_compositions:\n            sketch_list = misc_data_list\n            if self.cfg.read_sketch:\n                sketch_repeat = frame_sketch.repeat(frames_num, 1, 1, 1).cuda()\n                sketch_list = [sketch_repeat]\n            for misc_imgs in sketch_list:\n                sketch = pidinet(misc_imgs.sub(pidi_mean).div_(pidi_std))\n                sketch = 1.0 - cleaner(1.0 - sketch)\n                sketch_data.append(sketch)\n            sketch_data = torch.cat(sketch_data, dim=0)\n            sketch_data = rearrange(sketch_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        single_sketch_data = []\n        if 'single_sketch' in self.cfg.video_compositions:\n            single_sketch_data = sketch_data.clone()[:, :, :1].repeat(1, 1, frames_num, 1, 1)\n    y = self.clip_encoder(caps).detach()\n    y0 = y.clone()\n    y_visual = []\n    if 'image' in self.cfg.video_compositions:\n        with torch.no_grad():\n            if self.cfg.read_style:\n                y_visual = self.clip_encoder_visual(self.clip_encoder_visual.preprocess(frame_style).unsqueeze(0).cuda()).unsqueeze(0)\n                y_visual0 = y_visual.clone()\n            else:\n                ref_imgs = ref_imgs.squeeze(1)\n                y_visual = self.clip_encoder_visual(ref_imgs).unsqueeze(1)\n                y_visual0 = y_visual.clone()\n    with torch.no_grad():\n        pynvml.nvmlInit()\n        with amp.autocast(enabled=self.cfg.use_fp16):\n            if self.cfg.share_noise:\n                (b, c, f, h, w) = video_data.shape\n                noise = torch.randn((self.viz_num, c, h, w), device=self.device)\n                noise = noise.repeat_interleave(repeats=f, dim=0)\n                noise = rearrange(noise, '(b f) c h w->b c f h w', b=self.viz_num)\n                noise = noise.contiguous()\n            else:\n                noise = torch.randn_like(video_data[:self.viz_num])\n            full_model_kwargs = [{'y': y0[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else y_visual0[:self.viz_num], 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}, {'y': self.zero_y.repeat(self.viz_num, 1, 1) if not self.cfg.use_fps_condition else torch.zeros_like(y0)[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else torch.zeros_like(y_visual0[:self.viz_num]), 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}]\n            partial_keys = self.cfg.guidances\n            noise_motion = noise.clone()\n            model_kwargs = prepare_model_kwargs(partial_keys=partial_keys, full_model_kwargs=full_model_kwargs, use_fps_condition=self.cfg.use_fps_condition)\n            video_output = self.diffusion.ddim_sample_loop(noise=noise_motion, model=self.model.eval(), model_kwargs=model_kwargs, guide_scale=9.0, ddim_timesteps=self.cfg.ddim_timesteps, eta=0.0)\n            save_with_model_kwargs(model_kwargs=model_kwargs, video_data=video_output, autoencoder=self.autoencoder, ori_video=misc_backups, viz_num=self.viz_num, step=0, caps=caps, palette=palette, cfg=self.cfg, duration=self.duration)\n    return {'video': video_output.type(torch.float32).cpu(), 'video_path': self.cfg}",
            "def forward(self, input: Dict[str, Any]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    frame_in = None\n    if self.read_image:\n        image_key = input['style_image']\n        frame = load_image(image_key)\n        frame_in = misc_transforms([frame])\n    frame_sketch = None\n    if self.read_sketch:\n        sketch_key = self.cfg.sketch_path\n        frame_sketch = load_image(sketch_key)\n        frame_sketch = misc_transforms([frame_sketch])\n    frame_style = None\n    if self.read_style:\n        frame_style = load_image(input['style_image'])\n    if 'depthmap' in self.video_compositions:\n        midas = models.midas_v3(pretrained=True, model_dir=self.model_dir).eval().requires_grad_(False).to(memory_format=torch.channels_last).half().to(self.device)\n    if 'canny' in self.video_compositions:\n        canny_detector = CannyDetector()\n    if 'sketch' in self.video_compositions:\n        pidinet = pidinet_bsd(self.model_dir, pretrained=True, vanilla_cnn=True).eval().requires_grad_(False).to(self.device)\n        cleaner = sketch_simplification_gan(self.model_dir, pretrained=True).eval().requires_grad_(False).to(self.device)\n        pidi_mean = torch.tensor(self.cfg.sketch_mean).view(1, -1, 1, 1).to(self.device)\n        pidi_std = torch.tensor(self.cfg.sketch_std).view(1, -1, 1, 1).to(self.device)\n    palette = None\n    self.model.eval()\n    caps = input['cap_txt']\n    if self.cfg.max_frames == 1 and self.cfg.use_image_dataset:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    else:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    misc_backups = copy(misc_data)\n    misc_backups = rearrange(misc_backups, 'b f c h w -> b c f h w')\n    mv_data_video = []\n    if 'motion' in self.cfg.video_compositions:\n        mv_data_video = rearrange(mv_data, 'b f c h w -> b c f h w')\n    masked_video = []\n    if 'mask' in self.cfg.video_compositions:\n        masked_video = make_masked_images(misc_data.sub(0.5).div_(0.5), mask)\n        masked_video = rearrange(masked_video, 'b f c h w -> b c f h w')\n    image_local = []\n    if 'local_image' in self.cfg.video_compositions:\n        frames_num = misc_data.shape[1]\n        bs_vd_local = misc_data.shape[0]\n        if self.cfg.read_image:\n            image_local = frame_in.unsqueeze(0).repeat(bs_vd_local, frames_num, 1, 1, 1).cuda()\n        else:\n            image_local = misc_data[:, :1].clone().repeat(1, frames_num, 1, 1, 1)\n        image_local = rearrange(image_local, 'b f c h w -> b c f h w', b=bs_vd_local)\n    bs_vd = video_data.shape[0]\n    video_data = rearrange(video_data, 'b f c h w -> (b f) c h w')\n    misc_data = rearrange(misc_data, 'b f c h w -> (b f) c h w')\n    video_data_list = torch.chunk(video_data, video_data.shape[0] // self.cfg.chunk_size, dim=0)\n    misc_data_list = torch.chunk(misc_data, misc_data.shape[0] // self.cfg.chunk_size, dim=0)\n    with torch.no_grad():\n        decode_data = []\n        for vd_data in video_data_list:\n            encoder_posterior = self.autoencoder.encode(vd_data)\n            tmp = get_first_stage_encoding(encoder_posterior).detach()\n            decode_data.append(tmp)\n        video_data = torch.cat(decode_data, dim=0)\n        video_data = rearrange(video_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        depth_data = []\n        if 'depthmap' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                depth = midas(misc_imgs.sub(0.5).div_(0.5).to(memory_format=torch.channels_last).half())\n                depth = (depth / self.cfg.depth_std).clamp_(0, self.cfg.depth_clamp)\n                depth_data.append(depth)\n            depth_data = torch.cat(depth_data, dim=0)\n            depth_data = rearrange(depth_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        canny_data = []\n        if 'canny' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                misc_imgs = rearrange(misc_imgs.clone(), 'k c h w -> k h w c')\n                canny_condition = torch.stack([canny_detector(misc_img) for misc_img in misc_imgs])\n                canny_condition = rearrange(canny_condition, 'k h w c-> k c h w')\n                canny_data.append(canny_condition)\n            canny_data = torch.cat(canny_data, dim=0)\n            canny_data = rearrange(canny_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        sketch_data = []\n        if 'sketch' in self.cfg.video_compositions:\n            sketch_list = misc_data_list\n            if self.cfg.read_sketch:\n                sketch_repeat = frame_sketch.repeat(frames_num, 1, 1, 1).cuda()\n                sketch_list = [sketch_repeat]\n            for misc_imgs in sketch_list:\n                sketch = pidinet(misc_imgs.sub(pidi_mean).div_(pidi_std))\n                sketch = 1.0 - cleaner(1.0 - sketch)\n                sketch_data.append(sketch)\n            sketch_data = torch.cat(sketch_data, dim=0)\n            sketch_data = rearrange(sketch_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        single_sketch_data = []\n        if 'single_sketch' in self.cfg.video_compositions:\n            single_sketch_data = sketch_data.clone()[:, :, :1].repeat(1, 1, frames_num, 1, 1)\n    y = self.clip_encoder(caps).detach()\n    y0 = y.clone()\n    y_visual = []\n    if 'image' in self.cfg.video_compositions:\n        with torch.no_grad():\n            if self.cfg.read_style:\n                y_visual = self.clip_encoder_visual(self.clip_encoder_visual.preprocess(frame_style).unsqueeze(0).cuda()).unsqueeze(0)\n                y_visual0 = y_visual.clone()\n            else:\n                ref_imgs = ref_imgs.squeeze(1)\n                y_visual = self.clip_encoder_visual(ref_imgs).unsqueeze(1)\n                y_visual0 = y_visual.clone()\n    with torch.no_grad():\n        pynvml.nvmlInit()\n        with amp.autocast(enabled=self.cfg.use_fp16):\n            if self.cfg.share_noise:\n                (b, c, f, h, w) = video_data.shape\n                noise = torch.randn((self.viz_num, c, h, w), device=self.device)\n                noise = noise.repeat_interleave(repeats=f, dim=0)\n                noise = rearrange(noise, '(b f) c h w->b c f h w', b=self.viz_num)\n                noise = noise.contiguous()\n            else:\n                noise = torch.randn_like(video_data[:self.viz_num])\n            full_model_kwargs = [{'y': y0[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else y_visual0[:self.viz_num], 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}, {'y': self.zero_y.repeat(self.viz_num, 1, 1) if not self.cfg.use_fps_condition else torch.zeros_like(y0)[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else torch.zeros_like(y_visual0[:self.viz_num]), 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}]\n            partial_keys = self.cfg.guidances\n            noise_motion = noise.clone()\n            model_kwargs = prepare_model_kwargs(partial_keys=partial_keys, full_model_kwargs=full_model_kwargs, use_fps_condition=self.cfg.use_fps_condition)\n            video_output = self.diffusion.ddim_sample_loop(noise=noise_motion, model=self.model.eval(), model_kwargs=model_kwargs, guide_scale=9.0, ddim_timesteps=self.cfg.ddim_timesteps, eta=0.0)\n            save_with_model_kwargs(model_kwargs=model_kwargs, video_data=video_output, autoencoder=self.autoencoder, ori_video=misc_backups, viz_num=self.viz_num, step=0, caps=caps, palette=palette, cfg=self.cfg, duration=self.duration)\n    return {'video': video_output.type(torch.float32).cpu(), 'video_path': self.cfg}",
            "def forward(self, input: Dict[str, Any]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    frame_in = None\n    if self.read_image:\n        image_key = input['style_image']\n        frame = load_image(image_key)\n        frame_in = misc_transforms([frame])\n    frame_sketch = None\n    if self.read_sketch:\n        sketch_key = self.cfg.sketch_path\n        frame_sketch = load_image(sketch_key)\n        frame_sketch = misc_transforms([frame_sketch])\n    frame_style = None\n    if self.read_style:\n        frame_style = load_image(input['style_image'])\n    if 'depthmap' in self.video_compositions:\n        midas = models.midas_v3(pretrained=True, model_dir=self.model_dir).eval().requires_grad_(False).to(memory_format=torch.channels_last).half().to(self.device)\n    if 'canny' in self.video_compositions:\n        canny_detector = CannyDetector()\n    if 'sketch' in self.video_compositions:\n        pidinet = pidinet_bsd(self.model_dir, pretrained=True, vanilla_cnn=True).eval().requires_grad_(False).to(self.device)\n        cleaner = sketch_simplification_gan(self.model_dir, pretrained=True).eval().requires_grad_(False).to(self.device)\n        pidi_mean = torch.tensor(self.cfg.sketch_mean).view(1, -1, 1, 1).to(self.device)\n        pidi_std = torch.tensor(self.cfg.sketch_std).view(1, -1, 1, 1).to(self.device)\n    palette = None\n    self.model.eval()\n    caps = input['cap_txt']\n    if self.cfg.max_frames == 1 and self.cfg.use_image_dataset:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    else:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    misc_backups = copy(misc_data)\n    misc_backups = rearrange(misc_backups, 'b f c h w -> b c f h w')\n    mv_data_video = []\n    if 'motion' in self.cfg.video_compositions:\n        mv_data_video = rearrange(mv_data, 'b f c h w -> b c f h w')\n    masked_video = []\n    if 'mask' in self.cfg.video_compositions:\n        masked_video = make_masked_images(misc_data.sub(0.5).div_(0.5), mask)\n        masked_video = rearrange(masked_video, 'b f c h w -> b c f h w')\n    image_local = []\n    if 'local_image' in self.cfg.video_compositions:\n        frames_num = misc_data.shape[1]\n        bs_vd_local = misc_data.shape[0]\n        if self.cfg.read_image:\n            image_local = frame_in.unsqueeze(0).repeat(bs_vd_local, frames_num, 1, 1, 1).cuda()\n        else:\n            image_local = misc_data[:, :1].clone().repeat(1, frames_num, 1, 1, 1)\n        image_local = rearrange(image_local, 'b f c h w -> b c f h w', b=bs_vd_local)\n    bs_vd = video_data.shape[0]\n    video_data = rearrange(video_data, 'b f c h w -> (b f) c h w')\n    misc_data = rearrange(misc_data, 'b f c h w -> (b f) c h w')\n    video_data_list = torch.chunk(video_data, video_data.shape[0] // self.cfg.chunk_size, dim=0)\n    misc_data_list = torch.chunk(misc_data, misc_data.shape[0] // self.cfg.chunk_size, dim=0)\n    with torch.no_grad():\n        decode_data = []\n        for vd_data in video_data_list:\n            encoder_posterior = self.autoencoder.encode(vd_data)\n            tmp = get_first_stage_encoding(encoder_posterior).detach()\n            decode_data.append(tmp)\n        video_data = torch.cat(decode_data, dim=0)\n        video_data = rearrange(video_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        depth_data = []\n        if 'depthmap' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                depth = midas(misc_imgs.sub(0.5).div_(0.5).to(memory_format=torch.channels_last).half())\n                depth = (depth / self.cfg.depth_std).clamp_(0, self.cfg.depth_clamp)\n                depth_data.append(depth)\n            depth_data = torch.cat(depth_data, dim=0)\n            depth_data = rearrange(depth_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        canny_data = []\n        if 'canny' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                misc_imgs = rearrange(misc_imgs.clone(), 'k c h w -> k h w c')\n                canny_condition = torch.stack([canny_detector(misc_img) for misc_img in misc_imgs])\n                canny_condition = rearrange(canny_condition, 'k h w c-> k c h w')\n                canny_data.append(canny_condition)\n            canny_data = torch.cat(canny_data, dim=0)\n            canny_data = rearrange(canny_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        sketch_data = []\n        if 'sketch' in self.cfg.video_compositions:\n            sketch_list = misc_data_list\n            if self.cfg.read_sketch:\n                sketch_repeat = frame_sketch.repeat(frames_num, 1, 1, 1).cuda()\n                sketch_list = [sketch_repeat]\n            for misc_imgs in sketch_list:\n                sketch = pidinet(misc_imgs.sub(pidi_mean).div_(pidi_std))\n                sketch = 1.0 - cleaner(1.0 - sketch)\n                sketch_data.append(sketch)\n            sketch_data = torch.cat(sketch_data, dim=0)\n            sketch_data = rearrange(sketch_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        single_sketch_data = []\n        if 'single_sketch' in self.cfg.video_compositions:\n            single_sketch_data = sketch_data.clone()[:, :, :1].repeat(1, 1, frames_num, 1, 1)\n    y = self.clip_encoder(caps).detach()\n    y0 = y.clone()\n    y_visual = []\n    if 'image' in self.cfg.video_compositions:\n        with torch.no_grad():\n            if self.cfg.read_style:\n                y_visual = self.clip_encoder_visual(self.clip_encoder_visual.preprocess(frame_style).unsqueeze(0).cuda()).unsqueeze(0)\n                y_visual0 = y_visual.clone()\n            else:\n                ref_imgs = ref_imgs.squeeze(1)\n                y_visual = self.clip_encoder_visual(ref_imgs).unsqueeze(1)\n                y_visual0 = y_visual.clone()\n    with torch.no_grad():\n        pynvml.nvmlInit()\n        with amp.autocast(enabled=self.cfg.use_fp16):\n            if self.cfg.share_noise:\n                (b, c, f, h, w) = video_data.shape\n                noise = torch.randn((self.viz_num, c, h, w), device=self.device)\n                noise = noise.repeat_interleave(repeats=f, dim=0)\n                noise = rearrange(noise, '(b f) c h w->b c f h w', b=self.viz_num)\n                noise = noise.contiguous()\n            else:\n                noise = torch.randn_like(video_data[:self.viz_num])\n            full_model_kwargs = [{'y': y0[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else y_visual0[:self.viz_num], 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}, {'y': self.zero_y.repeat(self.viz_num, 1, 1) if not self.cfg.use_fps_condition else torch.zeros_like(y0)[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else torch.zeros_like(y_visual0[:self.viz_num]), 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}]\n            partial_keys = self.cfg.guidances\n            noise_motion = noise.clone()\n            model_kwargs = prepare_model_kwargs(partial_keys=partial_keys, full_model_kwargs=full_model_kwargs, use_fps_condition=self.cfg.use_fps_condition)\n            video_output = self.diffusion.ddim_sample_loop(noise=noise_motion, model=self.model.eval(), model_kwargs=model_kwargs, guide_scale=9.0, ddim_timesteps=self.cfg.ddim_timesteps, eta=0.0)\n            save_with_model_kwargs(model_kwargs=model_kwargs, video_data=video_output, autoencoder=self.autoencoder, ori_video=misc_backups, viz_num=self.viz_num, step=0, caps=caps, palette=palette, cfg=self.cfg, duration=self.duration)\n    return {'video': video_output.type(torch.float32).cpu(), 'video_path': self.cfg}",
            "def forward(self, input: Dict[str, Any]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    frame_in = None\n    if self.read_image:\n        image_key = input['style_image']\n        frame = load_image(image_key)\n        frame_in = misc_transforms([frame])\n    frame_sketch = None\n    if self.read_sketch:\n        sketch_key = self.cfg.sketch_path\n        frame_sketch = load_image(sketch_key)\n        frame_sketch = misc_transforms([frame_sketch])\n    frame_style = None\n    if self.read_style:\n        frame_style = load_image(input['style_image'])\n    if 'depthmap' in self.video_compositions:\n        midas = models.midas_v3(pretrained=True, model_dir=self.model_dir).eval().requires_grad_(False).to(memory_format=torch.channels_last).half().to(self.device)\n    if 'canny' in self.video_compositions:\n        canny_detector = CannyDetector()\n    if 'sketch' in self.video_compositions:\n        pidinet = pidinet_bsd(self.model_dir, pretrained=True, vanilla_cnn=True).eval().requires_grad_(False).to(self.device)\n        cleaner = sketch_simplification_gan(self.model_dir, pretrained=True).eval().requires_grad_(False).to(self.device)\n        pidi_mean = torch.tensor(self.cfg.sketch_mean).view(1, -1, 1, 1).to(self.device)\n        pidi_std = torch.tensor(self.cfg.sketch_std).view(1, -1, 1, 1).to(self.device)\n    palette = None\n    self.model.eval()\n    caps = input['cap_txt']\n    if self.cfg.max_frames == 1 and self.cfg.use_image_dataset:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    else:\n        ref_imgs = input['ref_frame']\n        video_data = input['video_data']\n        misc_data = input['misc_data']\n        mask = input['mask']\n        mv_data = input['mv_data']\n        fps = torch.tensor([self.cfg.feature_framerate] * self.cfg.batch_size, dtype=torch.long, device=self.device)\n    misc_backups = copy(misc_data)\n    misc_backups = rearrange(misc_backups, 'b f c h w -> b c f h w')\n    mv_data_video = []\n    if 'motion' in self.cfg.video_compositions:\n        mv_data_video = rearrange(mv_data, 'b f c h w -> b c f h w')\n    masked_video = []\n    if 'mask' in self.cfg.video_compositions:\n        masked_video = make_masked_images(misc_data.sub(0.5).div_(0.5), mask)\n        masked_video = rearrange(masked_video, 'b f c h w -> b c f h w')\n    image_local = []\n    if 'local_image' in self.cfg.video_compositions:\n        frames_num = misc_data.shape[1]\n        bs_vd_local = misc_data.shape[0]\n        if self.cfg.read_image:\n            image_local = frame_in.unsqueeze(0).repeat(bs_vd_local, frames_num, 1, 1, 1).cuda()\n        else:\n            image_local = misc_data[:, :1].clone().repeat(1, frames_num, 1, 1, 1)\n        image_local = rearrange(image_local, 'b f c h w -> b c f h w', b=bs_vd_local)\n    bs_vd = video_data.shape[0]\n    video_data = rearrange(video_data, 'b f c h w -> (b f) c h w')\n    misc_data = rearrange(misc_data, 'b f c h w -> (b f) c h w')\n    video_data_list = torch.chunk(video_data, video_data.shape[0] // self.cfg.chunk_size, dim=0)\n    misc_data_list = torch.chunk(misc_data, misc_data.shape[0] // self.cfg.chunk_size, dim=0)\n    with torch.no_grad():\n        decode_data = []\n        for vd_data in video_data_list:\n            encoder_posterior = self.autoencoder.encode(vd_data)\n            tmp = get_first_stage_encoding(encoder_posterior).detach()\n            decode_data.append(tmp)\n        video_data = torch.cat(decode_data, dim=0)\n        video_data = rearrange(video_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        depth_data = []\n        if 'depthmap' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                depth = midas(misc_imgs.sub(0.5).div_(0.5).to(memory_format=torch.channels_last).half())\n                depth = (depth / self.cfg.depth_std).clamp_(0, self.cfg.depth_clamp)\n                depth_data.append(depth)\n            depth_data = torch.cat(depth_data, dim=0)\n            depth_data = rearrange(depth_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        canny_data = []\n        if 'canny' in self.cfg.video_compositions:\n            for misc_imgs in misc_data_list:\n                misc_imgs = rearrange(misc_imgs.clone(), 'k c h w -> k h w c')\n                canny_condition = torch.stack([canny_detector(misc_img) for misc_img in misc_imgs])\n                canny_condition = rearrange(canny_condition, 'k h w c-> k c h w')\n                canny_data.append(canny_condition)\n            canny_data = torch.cat(canny_data, dim=0)\n            canny_data = rearrange(canny_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        sketch_data = []\n        if 'sketch' in self.cfg.video_compositions:\n            sketch_list = misc_data_list\n            if self.cfg.read_sketch:\n                sketch_repeat = frame_sketch.repeat(frames_num, 1, 1, 1).cuda()\n                sketch_list = [sketch_repeat]\n            for misc_imgs in sketch_list:\n                sketch = pidinet(misc_imgs.sub(pidi_mean).div_(pidi_std))\n                sketch = 1.0 - cleaner(1.0 - sketch)\n                sketch_data.append(sketch)\n            sketch_data = torch.cat(sketch_data, dim=0)\n            sketch_data = rearrange(sketch_data, '(b f) c h w -> b c f h w', b=bs_vd)\n        single_sketch_data = []\n        if 'single_sketch' in self.cfg.video_compositions:\n            single_sketch_data = sketch_data.clone()[:, :, :1].repeat(1, 1, frames_num, 1, 1)\n    y = self.clip_encoder(caps).detach()\n    y0 = y.clone()\n    y_visual = []\n    if 'image' in self.cfg.video_compositions:\n        with torch.no_grad():\n            if self.cfg.read_style:\n                y_visual = self.clip_encoder_visual(self.clip_encoder_visual.preprocess(frame_style).unsqueeze(0).cuda()).unsqueeze(0)\n                y_visual0 = y_visual.clone()\n            else:\n                ref_imgs = ref_imgs.squeeze(1)\n                y_visual = self.clip_encoder_visual(ref_imgs).unsqueeze(1)\n                y_visual0 = y_visual.clone()\n    with torch.no_grad():\n        pynvml.nvmlInit()\n        with amp.autocast(enabled=self.cfg.use_fp16):\n            if self.cfg.share_noise:\n                (b, c, f, h, w) = video_data.shape\n                noise = torch.randn((self.viz_num, c, h, w), device=self.device)\n                noise = noise.repeat_interleave(repeats=f, dim=0)\n                noise = rearrange(noise, '(b f) c h w->b c f h w', b=self.viz_num)\n                noise = noise.contiguous()\n            else:\n                noise = torch.randn_like(video_data[:self.viz_num])\n            full_model_kwargs = [{'y': y0[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else y_visual0[:self.viz_num], 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}, {'y': self.zero_y.repeat(self.viz_num, 1, 1) if not self.cfg.use_fps_condition else torch.zeros_like(y0)[:self.viz_num], 'local_image': None if len(image_local) == 0 else image_local[:self.viz_num], 'image': None if len(y_visual) == 0 else torch.zeros_like(y_visual0[:self.viz_num]), 'depth': None if len(depth_data) == 0 else depth_data[:self.viz_num], 'canny': None if len(canny_data) == 0 else canny_data[:self.viz_num], 'sketch': None if len(sketch_data) == 0 else sketch_data[:self.viz_num], 'masked': None if len(masked_video) == 0 else masked_video[:self.viz_num], 'motion': None if len(mv_data_video) == 0 else mv_data_video[:self.viz_num], 'single_sketch': None if len(single_sketch_data) == 0 else single_sketch_data[:self.viz_num], 'fps': fps[:self.viz_num]}]\n            partial_keys = self.cfg.guidances\n            noise_motion = noise.clone()\n            model_kwargs = prepare_model_kwargs(partial_keys=partial_keys, full_model_kwargs=full_model_kwargs, use_fps_condition=self.cfg.use_fps_condition)\n            video_output = self.diffusion.ddim_sample_loop(noise=noise_motion, model=self.model.eval(), model_kwargs=model_kwargs, guide_scale=9.0, ddim_timesteps=self.cfg.ddim_timesteps, eta=0.0)\n            save_with_model_kwargs(model_kwargs=model_kwargs, video_data=video_output, autoencoder=self.autoencoder, ori_video=misc_backups, viz_num=self.viz_num, step=0, caps=caps, palette=palette, cfg=self.cfg, duration=self.duration)\n    return {'video': video_output.type(torch.float32).cpu(), 'video_path': self.cfg}"
        ]
    }
]