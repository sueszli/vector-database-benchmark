[
    {
        "func_name": "__init__",
        "original": "def __init__(self, columns: Union[Hashable, List[Hashable], None]=None, ignore_columns: Union[Hashable, List[Hashable], None]=None, n_top_features: Optional[int]=10, segment_minimum_size_ratio: float=0.05, alternative_scorer: Dict[str, Union[str, Callable]]=None, loss_per_sample: Union[np.ndarray, pd.Series, None]=None, score_per_sample: Union[np.ndarray, pd.Series, None]=None, n_samples: int=10000, categorical_aggregation_threshold: float=0.05, n_to_show: int=3, random_state: int=42, multiple_segments_per_feature: bool=True, **kwargs):\n    super().__init__(**kwargs)\n    if loss_per_sample is not None and score_per_sample is None:\n        warnings.warn(f'{self.__class__.__name__}: loss_per_sample is deprecated. Please use score_per_sample instead.', DeprecationWarning)\n        score_per_sample = -np.asarray(loss_per_sample)\n    if score_per_sample is not None and alternative_scorer:\n        raise DeepchecksValueError('Cannot use both score_per_sample and alternative_scorer')\n    self.columns = columns\n    self.ignore_columns = ignore_columns\n    self.n_top_features = n_top_features\n    self.segment_minimum_size_ratio = segment_minimum_size_ratio\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.score_per_sample = score_per_sample\n    self.loss_per_sample = loss_per_sample\n    self.alternative_scorer = alternative_scorer\n    self.categorical_aggregation_threshold = categorical_aggregation_threshold\n    self.multiple_segments_per_feature = multiple_segments_per_feature",
        "mutated": [
            "def __init__(self, columns: Union[Hashable, List[Hashable], None]=None, ignore_columns: Union[Hashable, List[Hashable], None]=None, n_top_features: Optional[int]=10, segment_minimum_size_ratio: float=0.05, alternative_scorer: Dict[str, Union[str, Callable]]=None, loss_per_sample: Union[np.ndarray, pd.Series, None]=None, score_per_sample: Union[np.ndarray, pd.Series, None]=None, n_samples: int=10000, categorical_aggregation_threshold: float=0.05, n_to_show: int=3, random_state: int=42, multiple_segments_per_feature: bool=True, **kwargs):\n    if False:\n        i = 10\n    super().__init__(**kwargs)\n    if loss_per_sample is not None and score_per_sample is None:\n        warnings.warn(f'{self.__class__.__name__}: loss_per_sample is deprecated. Please use score_per_sample instead.', DeprecationWarning)\n        score_per_sample = -np.asarray(loss_per_sample)\n    if score_per_sample is not None and alternative_scorer:\n        raise DeepchecksValueError('Cannot use both score_per_sample and alternative_scorer')\n    self.columns = columns\n    self.ignore_columns = ignore_columns\n    self.n_top_features = n_top_features\n    self.segment_minimum_size_ratio = segment_minimum_size_ratio\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.score_per_sample = score_per_sample\n    self.loss_per_sample = loss_per_sample\n    self.alternative_scorer = alternative_scorer\n    self.categorical_aggregation_threshold = categorical_aggregation_threshold\n    self.multiple_segments_per_feature = multiple_segments_per_feature",
            "def __init__(self, columns: Union[Hashable, List[Hashable], None]=None, ignore_columns: Union[Hashable, List[Hashable], None]=None, n_top_features: Optional[int]=10, segment_minimum_size_ratio: float=0.05, alternative_scorer: Dict[str, Union[str, Callable]]=None, loss_per_sample: Union[np.ndarray, pd.Series, None]=None, score_per_sample: Union[np.ndarray, pd.Series, None]=None, n_samples: int=10000, categorical_aggregation_threshold: float=0.05, n_to_show: int=3, random_state: int=42, multiple_segments_per_feature: bool=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(**kwargs)\n    if loss_per_sample is not None and score_per_sample is None:\n        warnings.warn(f'{self.__class__.__name__}: loss_per_sample is deprecated. Please use score_per_sample instead.', DeprecationWarning)\n        score_per_sample = -np.asarray(loss_per_sample)\n    if score_per_sample is not None and alternative_scorer:\n        raise DeepchecksValueError('Cannot use both score_per_sample and alternative_scorer')\n    self.columns = columns\n    self.ignore_columns = ignore_columns\n    self.n_top_features = n_top_features\n    self.segment_minimum_size_ratio = segment_minimum_size_ratio\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.score_per_sample = score_per_sample\n    self.loss_per_sample = loss_per_sample\n    self.alternative_scorer = alternative_scorer\n    self.categorical_aggregation_threshold = categorical_aggregation_threshold\n    self.multiple_segments_per_feature = multiple_segments_per_feature",
            "def __init__(self, columns: Union[Hashable, List[Hashable], None]=None, ignore_columns: Union[Hashable, List[Hashable], None]=None, n_top_features: Optional[int]=10, segment_minimum_size_ratio: float=0.05, alternative_scorer: Dict[str, Union[str, Callable]]=None, loss_per_sample: Union[np.ndarray, pd.Series, None]=None, score_per_sample: Union[np.ndarray, pd.Series, None]=None, n_samples: int=10000, categorical_aggregation_threshold: float=0.05, n_to_show: int=3, random_state: int=42, multiple_segments_per_feature: bool=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(**kwargs)\n    if loss_per_sample is not None and score_per_sample is None:\n        warnings.warn(f'{self.__class__.__name__}: loss_per_sample is deprecated. Please use score_per_sample instead.', DeprecationWarning)\n        score_per_sample = -np.asarray(loss_per_sample)\n    if score_per_sample is not None and alternative_scorer:\n        raise DeepchecksValueError('Cannot use both score_per_sample and alternative_scorer')\n    self.columns = columns\n    self.ignore_columns = ignore_columns\n    self.n_top_features = n_top_features\n    self.segment_minimum_size_ratio = segment_minimum_size_ratio\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.score_per_sample = score_per_sample\n    self.loss_per_sample = loss_per_sample\n    self.alternative_scorer = alternative_scorer\n    self.categorical_aggregation_threshold = categorical_aggregation_threshold\n    self.multiple_segments_per_feature = multiple_segments_per_feature",
            "def __init__(self, columns: Union[Hashable, List[Hashable], None]=None, ignore_columns: Union[Hashable, List[Hashable], None]=None, n_top_features: Optional[int]=10, segment_minimum_size_ratio: float=0.05, alternative_scorer: Dict[str, Union[str, Callable]]=None, loss_per_sample: Union[np.ndarray, pd.Series, None]=None, score_per_sample: Union[np.ndarray, pd.Series, None]=None, n_samples: int=10000, categorical_aggregation_threshold: float=0.05, n_to_show: int=3, random_state: int=42, multiple_segments_per_feature: bool=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(**kwargs)\n    if loss_per_sample is not None and score_per_sample is None:\n        warnings.warn(f'{self.__class__.__name__}: loss_per_sample is deprecated. Please use score_per_sample instead.', DeprecationWarning)\n        score_per_sample = -np.asarray(loss_per_sample)\n    if score_per_sample is not None and alternative_scorer:\n        raise DeepchecksValueError('Cannot use both score_per_sample and alternative_scorer')\n    self.columns = columns\n    self.ignore_columns = ignore_columns\n    self.n_top_features = n_top_features\n    self.segment_minimum_size_ratio = segment_minimum_size_ratio\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.score_per_sample = score_per_sample\n    self.loss_per_sample = loss_per_sample\n    self.alternative_scorer = alternative_scorer\n    self.categorical_aggregation_threshold = categorical_aggregation_threshold\n    self.multiple_segments_per_feature = multiple_segments_per_feature",
            "def __init__(self, columns: Union[Hashable, List[Hashable], None]=None, ignore_columns: Union[Hashable, List[Hashable], None]=None, n_top_features: Optional[int]=10, segment_minimum_size_ratio: float=0.05, alternative_scorer: Dict[str, Union[str, Callable]]=None, loss_per_sample: Union[np.ndarray, pd.Series, None]=None, score_per_sample: Union[np.ndarray, pd.Series, None]=None, n_samples: int=10000, categorical_aggregation_threshold: float=0.05, n_to_show: int=3, random_state: int=42, multiple_segments_per_feature: bool=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(**kwargs)\n    if loss_per_sample is not None and score_per_sample is None:\n        warnings.warn(f'{self.__class__.__name__}: loss_per_sample is deprecated. Please use score_per_sample instead.', DeprecationWarning)\n        score_per_sample = -np.asarray(loss_per_sample)\n    if score_per_sample is not None and alternative_scorer:\n        raise DeepchecksValueError('Cannot use both score_per_sample and alternative_scorer')\n    self.columns = columns\n    self.ignore_columns = ignore_columns\n    self.n_top_features = n_top_features\n    self.segment_minimum_size_ratio = segment_minimum_size_ratio\n    self.n_samples = n_samples\n    self.n_to_show = n_to_show\n    self.random_state = random_state\n    self.score_per_sample = score_per_sample\n    self.loss_per_sample = loss_per_sample\n    self.alternative_scorer = alternative_scorer\n    self.categorical_aggregation_threshold = categorical_aggregation_threshold\n    self.multiple_segments_per_feature = multiple_segments_per_feature"
        ]
    },
    {
        "func_name": "run_logic",
        "original": "def run_logic(self, context: Context, dataset_kind) -> CheckResult:\n    \"\"\"Run check.\"\"\"\n    dataset = context.get_data_by_kind(dataset_kind)\n    dataset = dataset.sample(self.n_samples, random_state=self.random_state).drop_na_labels()\n    dataset_subset = dataset.select(self.columns, self.ignore_columns, keep_label=True)\n    if len(dataset.features) < 2:\n        raise DeepchecksNotSupportedError('Check requires data to have at least two features in order to run.')\n    features_data = dataset_subset.features_columns[dataset_subset.numerical_features + dataset_subset.cat_features]\n    encoded_dataset = self._target_encode_categorical_features_fill_na(features_data, dataset.label_col, dataset_subset.cat_features, context.task_type != TaskType.REGRESSION)\n    if self.score_per_sample is not None:\n        score_per_sample = self.score_per_sample[list(dataset.data.index)]\n        (scorer, dummy_model) = (None, None)\n        avg_score = round(score_per_sample.mean(), 3)\n    else:\n        predictions = context.model.predict(dataset.features_columns)\n        if context.task_type == TaskType.REGRESSION:\n            y_proba = None\n            score_per_sample = calculate_neg_mse_per_sample(dataset_subset.label_col, predictions)\n        elif context.task_type in [TaskType.MULTICLASS, TaskType.BINARY]:\n            if not hasattr(context.model, 'predict_proba'):\n                raise DeepchecksNotSupportedError('Predicted probabilities not supplied. The weak segment checks relies on cross entropy error that requires predicted probabilities, rather than only predicted classes.')\n            y_proba = context.model.predict_proba(dataset.features_columns)\n            score_per_sample = calculate_neg_cross_entropy_per_sample(dataset.label_col, y_proba, context.model_classes)\n        dummy_model = _DummyModel(test=encoded_dataset, y_pred_test=predictions, y_proba_test=y_proba, validate_data_on_predict=False)\n        scorer = context.get_single_scorer(self.alternative_scorer)\n        avg_score = round(scorer(dummy_model, encoded_dataset), 3)\n    relevant_features = dataset_subset.cat_features + dataset_subset.numerical_features\n    if context.feature_importance is not None:\n        feature_rank = context.feature_importance.sort_values(ascending=False).keys()\n        feature_rank = np.asarray([col for col in feature_rank if col in relevant_features], dtype='object')\n    else:\n        feature_rank = np.asarray(relevant_features, dtype='object')\n    weak_segments = self._weak_segments_search(data=encoded_dataset.data, score_per_sample=score_per_sample, label_col=dataset_subset.label_col, feature_rank_for_search=feature_rank, dummy_model=dummy_model, scorer=scorer, multiple_segments_per_feature=self.multiple_segments_per_feature)\n    if len(weak_segments) == 0:\n        raise DeepchecksProcessError('WeakSegmentsPerformance was unable to train an error model to find weak segments. Try increasing n_samples or supply additional features.')\n    if context.with_display:\n        display = self._create_heatmap_display(data=encoded_dataset.data, weak_segments=weak_segments, score_per_sample=score_per_sample, avg_score=avg_score, label_col=dataset_subset.label_col, dummy_model=dummy_model, scorer=scorer)\n    else:\n        display = []\n    check_result_value = self._generate_check_result_value(weak_segments, dataset_subset.cat_features, avg_score)\n    display_msg = 'Showcasing intersections of features with weakest detected segments.<br> The full list of weak segments can be observed in the check result value. '\n    return CheckResult(value=check_result_value, display=[display_msg, DisplayMap(display)])",
        "mutated": [
            "def run_logic(self, context: Context, dataset_kind) -> CheckResult:\n    if False:\n        i = 10\n    'Run check.'\n    dataset = context.get_data_by_kind(dataset_kind)\n    dataset = dataset.sample(self.n_samples, random_state=self.random_state).drop_na_labels()\n    dataset_subset = dataset.select(self.columns, self.ignore_columns, keep_label=True)\n    if len(dataset.features) < 2:\n        raise DeepchecksNotSupportedError('Check requires data to have at least two features in order to run.')\n    features_data = dataset_subset.features_columns[dataset_subset.numerical_features + dataset_subset.cat_features]\n    encoded_dataset = self._target_encode_categorical_features_fill_na(features_data, dataset.label_col, dataset_subset.cat_features, context.task_type != TaskType.REGRESSION)\n    if self.score_per_sample is not None:\n        score_per_sample = self.score_per_sample[list(dataset.data.index)]\n        (scorer, dummy_model) = (None, None)\n        avg_score = round(score_per_sample.mean(), 3)\n    else:\n        predictions = context.model.predict(dataset.features_columns)\n        if context.task_type == TaskType.REGRESSION:\n            y_proba = None\n            score_per_sample = calculate_neg_mse_per_sample(dataset_subset.label_col, predictions)\n        elif context.task_type in [TaskType.MULTICLASS, TaskType.BINARY]:\n            if not hasattr(context.model, 'predict_proba'):\n                raise DeepchecksNotSupportedError('Predicted probabilities not supplied. The weak segment checks relies on cross entropy error that requires predicted probabilities, rather than only predicted classes.')\n            y_proba = context.model.predict_proba(dataset.features_columns)\n            score_per_sample = calculate_neg_cross_entropy_per_sample(dataset.label_col, y_proba, context.model_classes)\n        dummy_model = _DummyModel(test=encoded_dataset, y_pred_test=predictions, y_proba_test=y_proba, validate_data_on_predict=False)\n        scorer = context.get_single_scorer(self.alternative_scorer)\n        avg_score = round(scorer(dummy_model, encoded_dataset), 3)\n    relevant_features = dataset_subset.cat_features + dataset_subset.numerical_features\n    if context.feature_importance is not None:\n        feature_rank = context.feature_importance.sort_values(ascending=False).keys()\n        feature_rank = np.asarray([col for col in feature_rank if col in relevant_features], dtype='object')\n    else:\n        feature_rank = np.asarray(relevant_features, dtype='object')\n    weak_segments = self._weak_segments_search(data=encoded_dataset.data, score_per_sample=score_per_sample, label_col=dataset_subset.label_col, feature_rank_for_search=feature_rank, dummy_model=dummy_model, scorer=scorer, multiple_segments_per_feature=self.multiple_segments_per_feature)\n    if len(weak_segments) == 0:\n        raise DeepchecksProcessError('WeakSegmentsPerformance was unable to train an error model to find weak segments. Try increasing n_samples or supply additional features.')\n    if context.with_display:\n        display = self._create_heatmap_display(data=encoded_dataset.data, weak_segments=weak_segments, score_per_sample=score_per_sample, avg_score=avg_score, label_col=dataset_subset.label_col, dummy_model=dummy_model, scorer=scorer)\n    else:\n        display = []\n    check_result_value = self._generate_check_result_value(weak_segments, dataset_subset.cat_features, avg_score)\n    display_msg = 'Showcasing intersections of features with weakest detected segments.<br> The full list of weak segments can be observed in the check result value. '\n    return CheckResult(value=check_result_value, display=[display_msg, DisplayMap(display)])",
            "def run_logic(self, context: Context, dataset_kind) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Run check.'\n    dataset = context.get_data_by_kind(dataset_kind)\n    dataset = dataset.sample(self.n_samples, random_state=self.random_state).drop_na_labels()\n    dataset_subset = dataset.select(self.columns, self.ignore_columns, keep_label=True)\n    if len(dataset.features) < 2:\n        raise DeepchecksNotSupportedError('Check requires data to have at least two features in order to run.')\n    features_data = dataset_subset.features_columns[dataset_subset.numerical_features + dataset_subset.cat_features]\n    encoded_dataset = self._target_encode_categorical_features_fill_na(features_data, dataset.label_col, dataset_subset.cat_features, context.task_type != TaskType.REGRESSION)\n    if self.score_per_sample is not None:\n        score_per_sample = self.score_per_sample[list(dataset.data.index)]\n        (scorer, dummy_model) = (None, None)\n        avg_score = round(score_per_sample.mean(), 3)\n    else:\n        predictions = context.model.predict(dataset.features_columns)\n        if context.task_type == TaskType.REGRESSION:\n            y_proba = None\n            score_per_sample = calculate_neg_mse_per_sample(dataset_subset.label_col, predictions)\n        elif context.task_type in [TaskType.MULTICLASS, TaskType.BINARY]:\n            if not hasattr(context.model, 'predict_proba'):\n                raise DeepchecksNotSupportedError('Predicted probabilities not supplied. The weak segment checks relies on cross entropy error that requires predicted probabilities, rather than only predicted classes.')\n            y_proba = context.model.predict_proba(dataset.features_columns)\n            score_per_sample = calculate_neg_cross_entropy_per_sample(dataset.label_col, y_proba, context.model_classes)\n        dummy_model = _DummyModel(test=encoded_dataset, y_pred_test=predictions, y_proba_test=y_proba, validate_data_on_predict=False)\n        scorer = context.get_single_scorer(self.alternative_scorer)\n        avg_score = round(scorer(dummy_model, encoded_dataset), 3)\n    relevant_features = dataset_subset.cat_features + dataset_subset.numerical_features\n    if context.feature_importance is not None:\n        feature_rank = context.feature_importance.sort_values(ascending=False).keys()\n        feature_rank = np.asarray([col for col in feature_rank if col in relevant_features], dtype='object')\n    else:\n        feature_rank = np.asarray(relevant_features, dtype='object')\n    weak_segments = self._weak_segments_search(data=encoded_dataset.data, score_per_sample=score_per_sample, label_col=dataset_subset.label_col, feature_rank_for_search=feature_rank, dummy_model=dummy_model, scorer=scorer, multiple_segments_per_feature=self.multiple_segments_per_feature)\n    if len(weak_segments) == 0:\n        raise DeepchecksProcessError('WeakSegmentsPerformance was unable to train an error model to find weak segments. Try increasing n_samples or supply additional features.')\n    if context.with_display:\n        display = self._create_heatmap_display(data=encoded_dataset.data, weak_segments=weak_segments, score_per_sample=score_per_sample, avg_score=avg_score, label_col=dataset_subset.label_col, dummy_model=dummy_model, scorer=scorer)\n    else:\n        display = []\n    check_result_value = self._generate_check_result_value(weak_segments, dataset_subset.cat_features, avg_score)\n    display_msg = 'Showcasing intersections of features with weakest detected segments.<br> The full list of weak segments can be observed in the check result value. '\n    return CheckResult(value=check_result_value, display=[display_msg, DisplayMap(display)])",
            "def run_logic(self, context: Context, dataset_kind) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Run check.'\n    dataset = context.get_data_by_kind(dataset_kind)\n    dataset = dataset.sample(self.n_samples, random_state=self.random_state).drop_na_labels()\n    dataset_subset = dataset.select(self.columns, self.ignore_columns, keep_label=True)\n    if len(dataset.features) < 2:\n        raise DeepchecksNotSupportedError('Check requires data to have at least two features in order to run.')\n    features_data = dataset_subset.features_columns[dataset_subset.numerical_features + dataset_subset.cat_features]\n    encoded_dataset = self._target_encode_categorical_features_fill_na(features_data, dataset.label_col, dataset_subset.cat_features, context.task_type != TaskType.REGRESSION)\n    if self.score_per_sample is not None:\n        score_per_sample = self.score_per_sample[list(dataset.data.index)]\n        (scorer, dummy_model) = (None, None)\n        avg_score = round(score_per_sample.mean(), 3)\n    else:\n        predictions = context.model.predict(dataset.features_columns)\n        if context.task_type == TaskType.REGRESSION:\n            y_proba = None\n            score_per_sample = calculate_neg_mse_per_sample(dataset_subset.label_col, predictions)\n        elif context.task_type in [TaskType.MULTICLASS, TaskType.BINARY]:\n            if not hasattr(context.model, 'predict_proba'):\n                raise DeepchecksNotSupportedError('Predicted probabilities not supplied. The weak segment checks relies on cross entropy error that requires predicted probabilities, rather than only predicted classes.')\n            y_proba = context.model.predict_proba(dataset.features_columns)\n            score_per_sample = calculate_neg_cross_entropy_per_sample(dataset.label_col, y_proba, context.model_classes)\n        dummy_model = _DummyModel(test=encoded_dataset, y_pred_test=predictions, y_proba_test=y_proba, validate_data_on_predict=False)\n        scorer = context.get_single_scorer(self.alternative_scorer)\n        avg_score = round(scorer(dummy_model, encoded_dataset), 3)\n    relevant_features = dataset_subset.cat_features + dataset_subset.numerical_features\n    if context.feature_importance is not None:\n        feature_rank = context.feature_importance.sort_values(ascending=False).keys()\n        feature_rank = np.asarray([col for col in feature_rank if col in relevant_features], dtype='object')\n    else:\n        feature_rank = np.asarray(relevant_features, dtype='object')\n    weak_segments = self._weak_segments_search(data=encoded_dataset.data, score_per_sample=score_per_sample, label_col=dataset_subset.label_col, feature_rank_for_search=feature_rank, dummy_model=dummy_model, scorer=scorer, multiple_segments_per_feature=self.multiple_segments_per_feature)\n    if len(weak_segments) == 0:\n        raise DeepchecksProcessError('WeakSegmentsPerformance was unable to train an error model to find weak segments. Try increasing n_samples or supply additional features.')\n    if context.with_display:\n        display = self._create_heatmap_display(data=encoded_dataset.data, weak_segments=weak_segments, score_per_sample=score_per_sample, avg_score=avg_score, label_col=dataset_subset.label_col, dummy_model=dummy_model, scorer=scorer)\n    else:\n        display = []\n    check_result_value = self._generate_check_result_value(weak_segments, dataset_subset.cat_features, avg_score)\n    display_msg = 'Showcasing intersections of features with weakest detected segments.<br> The full list of weak segments can be observed in the check result value. '\n    return CheckResult(value=check_result_value, display=[display_msg, DisplayMap(display)])",
            "def run_logic(self, context: Context, dataset_kind) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Run check.'\n    dataset = context.get_data_by_kind(dataset_kind)\n    dataset = dataset.sample(self.n_samples, random_state=self.random_state).drop_na_labels()\n    dataset_subset = dataset.select(self.columns, self.ignore_columns, keep_label=True)\n    if len(dataset.features) < 2:\n        raise DeepchecksNotSupportedError('Check requires data to have at least two features in order to run.')\n    features_data = dataset_subset.features_columns[dataset_subset.numerical_features + dataset_subset.cat_features]\n    encoded_dataset = self._target_encode_categorical_features_fill_na(features_data, dataset.label_col, dataset_subset.cat_features, context.task_type != TaskType.REGRESSION)\n    if self.score_per_sample is not None:\n        score_per_sample = self.score_per_sample[list(dataset.data.index)]\n        (scorer, dummy_model) = (None, None)\n        avg_score = round(score_per_sample.mean(), 3)\n    else:\n        predictions = context.model.predict(dataset.features_columns)\n        if context.task_type == TaskType.REGRESSION:\n            y_proba = None\n            score_per_sample = calculate_neg_mse_per_sample(dataset_subset.label_col, predictions)\n        elif context.task_type in [TaskType.MULTICLASS, TaskType.BINARY]:\n            if not hasattr(context.model, 'predict_proba'):\n                raise DeepchecksNotSupportedError('Predicted probabilities not supplied. The weak segment checks relies on cross entropy error that requires predicted probabilities, rather than only predicted classes.')\n            y_proba = context.model.predict_proba(dataset.features_columns)\n            score_per_sample = calculate_neg_cross_entropy_per_sample(dataset.label_col, y_proba, context.model_classes)\n        dummy_model = _DummyModel(test=encoded_dataset, y_pred_test=predictions, y_proba_test=y_proba, validate_data_on_predict=False)\n        scorer = context.get_single_scorer(self.alternative_scorer)\n        avg_score = round(scorer(dummy_model, encoded_dataset), 3)\n    relevant_features = dataset_subset.cat_features + dataset_subset.numerical_features\n    if context.feature_importance is not None:\n        feature_rank = context.feature_importance.sort_values(ascending=False).keys()\n        feature_rank = np.asarray([col for col in feature_rank if col in relevant_features], dtype='object')\n    else:\n        feature_rank = np.asarray(relevant_features, dtype='object')\n    weak_segments = self._weak_segments_search(data=encoded_dataset.data, score_per_sample=score_per_sample, label_col=dataset_subset.label_col, feature_rank_for_search=feature_rank, dummy_model=dummy_model, scorer=scorer, multiple_segments_per_feature=self.multiple_segments_per_feature)\n    if len(weak_segments) == 0:\n        raise DeepchecksProcessError('WeakSegmentsPerformance was unable to train an error model to find weak segments. Try increasing n_samples or supply additional features.')\n    if context.with_display:\n        display = self._create_heatmap_display(data=encoded_dataset.data, weak_segments=weak_segments, score_per_sample=score_per_sample, avg_score=avg_score, label_col=dataset_subset.label_col, dummy_model=dummy_model, scorer=scorer)\n    else:\n        display = []\n    check_result_value = self._generate_check_result_value(weak_segments, dataset_subset.cat_features, avg_score)\n    display_msg = 'Showcasing intersections of features with weakest detected segments.<br> The full list of weak segments can be observed in the check result value. '\n    return CheckResult(value=check_result_value, display=[display_msg, DisplayMap(display)])",
            "def run_logic(self, context: Context, dataset_kind) -> CheckResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Run check.'\n    dataset = context.get_data_by_kind(dataset_kind)\n    dataset = dataset.sample(self.n_samples, random_state=self.random_state).drop_na_labels()\n    dataset_subset = dataset.select(self.columns, self.ignore_columns, keep_label=True)\n    if len(dataset.features) < 2:\n        raise DeepchecksNotSupportedError('Check requires data to have at least two features in order to run.')\n    features_data = dataset_subset.features_columns[dataset_subset.numerical_features + dataset_subset.cat_features]\n    encoded_dataset = self._target_encode_categorical_features_fill_na(features_data, dataset.label_col, dataset_subset.cat_features, context.task_type != TaskType.REGRESSION)\n    if self.score_per_sample is not None:\n        score_per_sample = self.score_per_sample[list(dataset.data.index)]\n        (scorer, dummy_model) = (None, None)\n        avg_score = round(score_per_sample.mean(), 3)\n    else:\n        predictions = context.model.predict(dataset.features_columns)\n        if context.task_type == TaskType.REGRESSION:\n            y_proba = None\n            score_per_sample = calculate_neg_mse_per_sample(dataset_subset.label_col, predictions)\n        elif context.task_type in [TaskType.MULTICLASS, TaskType.BINARY]:\n            if not hasattr(context.model, 'predict_proba'):\n                raise DeepchecksNotSupportedError('Predicted probabilities not supplied. The weak segment checks relies on cross entropy error that requires predicted probabilities, rather than only predicted classes.')\n            y_proba = context.model.predict_proba(dataset.features_columns)\n            score_per_sample = calculate_neg_cross_entropy_per_sample(dataset.label_col, y_proba, context.model_classes)\n        dummy_model = _DummyModel(test=encoded_dataset, y_pred_test=predictions, y_proba_test=y_proba, validate_data_on_predict=False)\n        scorer = context.get_single_scorer(self.alternative_scorer)\n        avg_score = round(scorer(dummy_model, encoded_dataset), 3)\n    relevant_features = dataset_subset.cat_features + dataset_subset.numerical_features\n    if context.feature_importance is not None:\n        feature_rank = context.feature_importance.sort_values(ascending=False).keys()\n        feature_rank = np.asarray([col for col in feature_rank if col in relevant_features], dtype='object')\n    else:\n        feature_rank = np.asarray(relevant_features, dtype='object')\n    weak_segments = self._weak_segments_search(data=encoded_dataset.data, score_per_sample=score_per_sample, label_col=dataset_subset.label_col, feature_rank_for_search=feature_rank, dummy_model=dummy_model, scorer=scorer, multiple_segments_per_feature=self.multiple_segments_per_feature)\n    if len(weak_segments) == 0:\n        raise DeepchecksProcessError('WeakSegmentsPerformance was unable to train an error model to find weak segments. Try increasing n_samples or supply additional features.')\n    if context.with_display:\n        display = self._create_heatmap_display(data=encoded_dataset.data, weak_segments=weak_segments, score_per_sample=score_per_sample, avg_score=avg_score, label_col=dataset_subset.label_col, dummy_model=dummy_model, scorer=scorer)\n    else:\n        display = []\n    check_result_value = self._generate_check_result_value(weak_segments, dataset_subset.cat_features, avg_score)\n    display_msg = 'Showcasing intersections of features with weakest detected segments.<br> The full list of weak segments can be observed in the check result value. '\n    return CheckResult(value=check_result_value, display=[display_msg, DisplayMap(display)])"
        ]
    },
    {
        "func_name": "config",
        "original": "def config(self, include_version: bool=True, include_defaults: bool=True) -> 'CheckConfig':\n    \"\"\"Return checks instance config.\"\"\"\n    if isinstance(self.alternative_scorer, dict):\n        for (k, v) in self.alternative_scorer.items():\n            if callable(v):\n                reference = doclink('supported-metrics-by-string', template='For a list of built-in scorers please refer to {link}. ')\n                raise ValueError(f'Only built-in scorers are allowed when serializing check instances. {reference}Scorer name: {k}')\n    return super().config(include_version, include_defaults=include_defaults)",
        "mutated": [
            "def config(self, include_version: bool=True, include_defaults: bool=True) -> 'CheckConfig':\n    if False:\n        i = 10\n    'Return checks instance config.'\n    if isinstance(self.alternative_scorer, dict):\n        for (k, v) in self.alternative_scorer.items():\n            if callable(v):\n                reference = doclink('supported-metrics-by-string', template='For a list of built-in scorers please refer to {link}. ')\n                raise ValueError(f'Only built-in scorers are allowed when serializing check instances. {reference}Scorer name: {k}')\n    return super().config(include_version, include_defaults=include_defaults)",
            "def config(self, include_version: bool=True, include_defaults: bool=True) -> 'CheckConfig':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return checks instance config.'\n    if isinstance(self.alternative_scorer, dict):\n        for (k, v) in self.alternative_scorer.items():\n            if callable(v):\n                reference = doclink('supported-metrics-by-string', template='For a list of built-in scorers please refer to {link}. ')\n                raise ValueError(f'Only built-in scorers are allowed when serializing check instances. {reference}Scorer name: {k}')\n    return super().config(include_version, include_defaults=include_defaults)",
            "def config(self, include_version: bool=True, include_defaults: bool=True) -> 'CheckConfig':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return checks instance config.'\n    if isinstance(self.alternative_scorer, dict):\n        for (k, v) in self.alternative_scorer.items():\n            if callable(v):\n                reference = doclink('supported-metrics-by-string', template='For a list of built-in scorers please refer to {link}. ')\n                raise ValueError(f'Only built-in scorers are allowed when serializing check instances. {reference}Scorer name: {k}')\n    return super().config(include_version, include_defaults=include_defaults)",
            "def config(self, include_version: bool=True, include_defaults: bool=True) -> 'CheckConfig':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return checks instance config.'\n    if isinstance(self.alternative_scorer, dict):\n        for (k, v) in self.alternative_scorer.items():\n            if callable(v):\n                reference = doclink('supported-metrics-by-string', template='For a list of built-in scorers please refer to {link}. ')\n                raise ValueError(f'Only built-in scorers are allowed when serializing check instances. {reference}Scorer name: {k}')\n    return super().config(include_version, include_defaults=include_defaults)",
            "def config(self, include_version: bool=True, include_defaults: bool=True) -> 'CheckConfig':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return checks instance config.'\n    if isinstance(self.alternative_scorer, dict):\n        for (k, v) in self.alternative_scorer.items():\n            if callable(v):\n                reference = doclink('supported-metrics-by-string', template='For a list of built-in scorers please refer to {link}. ')\n                raise ValueError(f'Only built-in scorers are allowed when serializing check instances. {reference}Scorer name: {k}')\n    return super().config(include_version, include_defaults=include_defaults)"
        ]
    }
]