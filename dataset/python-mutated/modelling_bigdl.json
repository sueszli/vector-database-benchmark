[
    {
        "func_name": "from_pretrained",
        "original": "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, model_family: str='llama', dtype: str='int4', **kwargs):\n    \"\"\"\n        :param pretrained_model_name_or_path: Path for converted BigDL-LLM optimized ggml\n               binary checkpoint. The checkpoint should be converted by ``bigdl.llm.llm_convert``.\n        :param model_family: The model family of the pretrained checkpoint.\n               Currently we support ``\"llama\"``, ``\"bloom\"``, ``\"gptneox\"``, ``\"starcoder\"``\n               and ``\"chatglm\"``.\n        :param dtype: Which quantized precision will be converted.\n                Now only `int4` and `int8` are supported, and `int8` only works for `llama`\n                , `gptneox` and `starcoder`.\n        :param cache_dir: (optional) This parameter will only be used when\n               ``pretrained_model_name_or_path`` is a huggingface checkpoint or hub repo id.\n               It indicates the saving path for the converted low precision model.\n        :param tmp_path: (optional) Which path to store the intermediate fp16 model during the\n               conversion process. Default to `None` so that intermediate model will not be saved.\n        :param kwargs: keyword arguments which will be passed to the model instance\n\n        :return: a model instance\n        \"\"\"\n    logging.warning('BigdlNativeForCausalLM has been deprecated, please switch to the new CausalLM API for sepcific models.')\n    invalidInputError(model_family in ['llama', 'gptneox', 'bloom', 'starcoder', 'chatglm'], \"Now we only support model family: 'llama', 'gptneox', 'bloom', 'starcoder', 'chatglm', '{}' is not in the list.\".format(model_family))\n    invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n    ggml_model_path = pretrained_model_name_or_path\n    if model_family == 'llama':\n        from bigdl.llm.ggml.model.llama import Llama\n        return Llama(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'gptneox':\n        from bigdl.llm.ggml.model.gptneox import Gptneox\n        return Gptneox(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'bloom':\n        from bigdl.llm.ggml.model.bloom import Bloom\n        return Bloom(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'starcoder':\n        from bigdl.llm.ggml.model.starcoder import Starcoder\n        return Starcoder(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'chatglm':\n        from bigdl.llm.ggml.model.chatglm import ChatGLM\n        return ChatGLM(model_path=ggml_model_path, **kwargs)",
        "mutated": [
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, model_family: str='llama', dtype: str='int4', **kwargs):\n    if False:\n        i = 10\n    '\\n        :param pretrained_model_name_or_path: Path for converted BigDL-LLM optimized ggml\\n               binary checkpoint. The checkpoint should be converted by ``bigdl.llm.llm_convert``.\\n        :param model_family: The model family of the pretrained checkpoint.\\n               Currently we support ``\"llama\"``, ``\"bloom\"``, ``\"gptneox\"``, ``\"starcoder\"``\\n               and ``\"chatglm\"``.\\n        :param dtype: Which quantized precision will be converted.\\n                Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n                , `gptneox` and `starcoder`.\\n        :param cache_dir: (optional) This parameter will only be used when\\n               ``pretrained_model_name_or_path`` is a huggingface checkpoint or hub repo id.\\n               It indicates the saving path for the converted low precision model.\\n        :param tmp_path: (optional) Which path to store the intermediate fp16 model during the\\n               conversion process. Default to `None` so that intermediate model will not be saved.\\n        :param kwargs: keyword arguments which will be passed to the model instance\\n\\n        :return: a model instance\\n        '\n    logging.warning('BigdlNativeForCausalLM has been deprecated, please switch to the new CausalLM API for sepcific models.')\n    invalidInputError(model_family in ['llama', 'gptneox', 'bloom', 'starcoder', 'chatglm'], \"Now we only support model family: 'llama', 'gptneox', 'bloom', 'starcoder', 'chatglm', '{}' is not in the list.\".format(model_family))\n    invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n    ggml_model_path = pretrained_model_name_or_path\n    if model_family == 'llama':\n        from bigdl.llm.ggml.model.llama import Llama\n        return Llama(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'gptneox':\n        from bigdl.llm.ggml.model.gptneox import Gptneox\n        return Gptneox(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'bloom':\n        from bigdl.llm.ggml.model.bloom import Bloom\n        return Bloom(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'starcoder':\n        from bigdl.llm.ggml.model.starcoder import Starcoder\n        return Starcoder(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'chatglm':\n        from bigdl.llm.ggml.model.chatglm import ChatGLM\n        return ChatGLM(model_path=ggml_model_path, **kwargs)",
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, model_family: str='llama', dtype: str='int4', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        :param pretrained_model_name_or_path: Path for converted BigDL-LLM optimized ggml\\n               binary checkpoint. The checkpoint should be converted by ``bigdl.llm.llm_convert``.\\n        :param model_family: The model family of the pretrained checkpoint.\\n               Currently we support ``\"llama\"``, ``\"bloom\"``, ``\"gptneox\"``, ``\"starcoder\"``\\n               and ``\"chatglm\"``.\\n        :param dtype: Which quantized precision will be converted.\\n                Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n                , `gptneox` and `starcoder`.\\n        :param cache_dir: (optional) This parameter will only be used when\\n               ``pretrained_model_name_or_path`` is a huggingface checkpoint or hub repo id.\\n               It indicates the saving path for the converted low precision model.\\n        :param tmp_path: (optional) Which path to store the intermediate fp16 model during the\\n               conversion process. Default to `None` so that intermediate model will not be saved.\\n        :param kwargs: keyword arguments which will be passed to the model instance\\n\\n        :return: a model instance\\n        '\n    logging.warning('BigdlNativeForCausalLM has been deprecated, please switch to the new CausalLM API for sepcific models.')\n    invalidInputError(model_family in ['llama', 'gptneox', 'bloom', 'starcoder', 'chatglm'], \"Now we only support model family: 'llama', 'gptneox', 'bloom', 'starcoder', 'chatglm', '{}' is not in the list.\".format(model_family))\n    invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n    ggml_model_path = pretrained_model_name_or_path\n    if model_family == 'llama':\n        from bigdl.llm.ggml.model.llama import Llama\n        return Llama(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'gptneox':\n        from bigdl.llm.ggml.model.gptneox import Gptneox\n        return Gptneox(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'bloom':\n        from bigdl.llm.ggml.model.bloom import Bloom\n        return Bloom(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'starcoder':\n        from bigdl.llm.ggml.model.starcoder import Starcoder\n        return Starcoder(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'chatglm':\n        from bigdl.llm.ggml.model.chatglm import ChatGLM\n        return ChatGLM(model_path=ggml_model_path, **kwargs)",
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, model_family: str='llama', dtype: str='int4', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        :param pretrained_model_name_or_path: Path for converted BigDL-LLM optimized ggml\\n               binary checkpoint. The checkpoint should be converted by ``bigdl.llm.llm_convert``.\\n        :param model_family: The model family of the pretrained checkpoint.\\n               Currently we support ``\"llama\"``, ``\"bloom\"``, ``\"gptneox\"``, ``\"starcoder\"``\\n               and ``\"chatglm\"``.\\n        :param dtype: Which quantized precision will be converted.\\n                Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n                , `gptneox` and `starcoder`.\\n        :param cache_dir: (optional) This parameter will only be used when\\n               ``pretrained_model_name_or_path`` is a huggingface checkpoint or hub repo id.\\n               It indicates the saving path for the converted low precision model.\\n        :param tmp_path: (optional) Which path to store the intermediate fp16 model during the\\n               conversion process. Default to `None` so that intermediate model will not be saved.\\n        :param kwargs: keyword arguments which will be passed to the model instance\\n\\n        :return: a model instance\\n        '\n    logging.warning('BigdlNativeForCausalLM has been deprecated, please switch to the new CausalLM API for sepcific models.')\n    invalidInputError(model_family in ['llama', 'gptneox', 'bloom', 'starcoder', 'chatglm'], \"Now we only support model family: 'llama', 'gptneox', 'bloom', 'starcoder', 'chatglm', '{}' is not in the list.\".format(model_family))\n    invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n    ggml_model_path = pretrained_model_name_or_path\n    if model_family == 'llama':\n        from bigdl.llm.ggml.model.llama import Llama\n        return Llama(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'gptneox':\n        from bigdl.llm.ggml.model.gptneox import Gptneox\n        return Gptneox(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'bloom':\n        from bigdl.llm.ggml.model.bloom import Bloom\n        return Bloom(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'starcoder':\n        from bigdl.llm.ggml.model.starcoder import Starcoder\n        return Starcoder(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'chatglm':\n        from bigdl.llm.ggml.model.chatglm import ChatGLM\n        return ChatGLM(model_path=ggml_model_path, **kwargs)",
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, model_family: str='llama', dtype: str='int4', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        :param pretrained_model_name_or_path: Path for converted BigDL-LLM optimized ggml\\n               binary checkpoint. The checkpoint should be converted by ``bigdl.llm.llm_convert``.\\n        :param model_family: The model family of the pretrained checkpoint.\\n               Currently we support ``\"llama\"``, ``\"bloom\"``, ``\"gptneox\"``, ``\"starcoder\"``\\n               and ``\"chatglm\"``.\\n        :param dtype: Which quantized precision will be converted.\\n                Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n                , `gptneox` and `starcoder`.\\n        :param cache_dir: (optional) This parameter will only be used when\\n               ``pretrained_model_name_or_path`` is a huggingface checkpoint or hub repo id.\\n               It indicates the saving path for the converted low precision model.\\n        :param tmp_path: (optional) Which path to store the intermediate fp16 model during the\\n               conversion process. Default to `None` so that intermediate model will not be saved.\\n        :param kwargs: keyword arguments which will be passed to the model instance\\n\\n        :return: a model instance\\n        '\n    logging.warning('BigdlNativeForCausalLM has been deprecated, please switch to the new CausalLM API for sepcific models.')\n    invalidInputError(model_family in ['llama', 'gptneox', 'bloom', 'starcoder', 'chatglm'], \"Now we only support model family: 'llama', 'gptneox', 'bloom', 'starcoder', 'chatglm', '{}' is not in the list.\".format(model_family))\n    invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n    ggml_model_path = pretrained_model_name_or_path\n    if model_family == 'llama':\n        from bigdl.llm.ggml.model.llama import Llama\n        return Llama(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'gptneox':\n        from bigdl.llm.ggml.model.gptneox import Gptneox\n        return Gptneox(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'bloom':\n        from bigdl.llm.ggml.model.bloom import Bloom\n        return Bloom(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'starcoder':\n        from bigdl.llm.ggml.model.starcoder import Starcoder\n        return Starcoder(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'chatglm':\n        from bigdl.llm.ggml.model.chatglm import ChatGLM\n        return ChatGLM(model_path=ggml_model_path, **kwargs)",
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, model_family: str='llama', dtype: str='int4', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        :param pretrained_model_name_or_path: Path for converted BigDL-LLM optimized ggml\\n               binary checkpoint. The checkpoint should be converted by ``bigdl.llm.llm_convert``.\\n        :param model_family: The model family of the pretrained checkpoint.\\n               Currently we support ``\"llama\"``, ``\"bloom\"``, ``\"gptneox\"``, ``\"starcoder\"``\\n               and ``\"chatglm\"``.\\n        :param dtype: Which quantized precision will be converted.\\n                Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n                , `gptneox` and `starcoder`.\\n        :param cache_dir: (optional) This parameter will only be used when\\n               ``pretrained_model_name_or_path`` is a huggingface checkpoint or hub repo id.\\n               It indicates the saving path for the converted low precision model.\\n        :param tmp_path: (optional) Which path to store the intermediate fp16 model during the\\n               conversion process. Default to `None` so that intermediate model will not be saved.\\n        :param kwargs: keyword arguments which will be passed to the model instance\\n\\n        :return: a model instance\\n        '\n    logging.warning('BigdlNativeForCausalLM has been deprecated, please switch to the new CausalLM API for sepcific models.')\n    invalidInputError(model_family in ['llama', 'gptneox', 'bloom', 'starcoder', 'chatglm'], \"Now we only support model family: 'llama', 'gptneox', 'bloom', 'starcoder', 'chatglm', '{}' is not in the list.\".format(model_family))\n    invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n    ggml_model_path = pretrained_model_name_or_path\n    if model_family == 'llama':\n        from bigdl.llm.ggml.model.llama import Llama\n        return Llama(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'gptneox':\n        from bigdl.llm.ggml.model.gptneox import Gptneox\n        return Gptneox(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'bloom':\n        from bigdl.llm.ggml.model.bloom import Bloom\n        return Bloom(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'starcoder':\n        from bigdl.llm.ggml.model.starcoder import Starcoder\n        return Starcoder(model_path=ggml_model_path, **kwargs)\n    elif model_family == 'chatglm':\n        from bigdl.llm.ggml.model.chatglm import ChatGLM\n        return ChatGLM(model_path=ggml_model_path, **kwargs)"
        ]
    },
    {
        "func_name": "from_pretrained",
        "original": "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, native: bool=True, dtype: str='int4', *args, **kwargs):\n    \"\"\"\n        :param pretrained_model_name_or_path: Path for model checkpoint.\n               If running with ``native int4``, the path should be converted BigDL-LLM optimized\n               ggml binary checkpoint, which should be converted by ``bigdl.llm.llm_convert``.\n               If running with ``transformers int4``, the path should be the huggingface repo id\n               to be downloaded or the huggingface checkpoint folder.\n        :param native: Load model to either BigDL-LLM optimized Transformer or Native (ggml) int4.\n        :param dtype: Which quantized precision will be converted.\n               Now only `int4` and `int8` are supported, and `int8` only works for `llama`\n               , `gptneox` and `starcoder`.\n        :param kwargs: keyword arguments which will be passed to the model instance.\n\n        :return: a model instance\n        \"\"\"\n    try:\n        module = importlib.import_module(cls.GGML_Module)\n        class_ = getattr(module, cls.GGML_Model)\n        if native:\n            invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n            ggml_model_path = pretrained_model_name_or_path\n            model = class_(model_path=ggml_model_path, **kwargs)\n        else:\n            model = cls.HF_Class.from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n    except Exception as e:\n        invalidInputError(False, f'Could not load model from path: {pretrained_model_name_or_path}. Please make sure the CausalLM class matches the model you want to load.Received error {e}')\n    return model",
        "mutated": [
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, native: bool=True, dtype: str='int4', *args, **kwargs):\n    if False:\n        i = 10\n    '\\n        :param pretrained_model_name_or_path: Path for model checkpoint.\\n               If running with ``native int4``, the path should be converted BigDL-LLM optimized\\n               ggml binary checkpoint, which should be converted by ``bigdl.llm.llm_convert``.\\n               If running with ``transformers int4``, the path should be the huggingface repo id\\n               to be downloaded or the huggingface checkpoint folder.\\n        :param native: Load model to either BigDL-LLM optimized Transformer or Native (ggml) int4.\\n        :param dtype: Which quantized precision will be converted.\\n               Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n               , `gptneox` and `starcoder`.\\n        :param kwargs: keyword arguments which will be passed to the model instance.\\n\\n        :return: a model instance\\n        '\n    try:\n        module = importlib.import_module(cls.GGML_Module)\n        class_ = getattr(module, cls.GGML_Model)\n        if native:\n            invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n            ggml_model_path = pretrained_model_name_or_path\n            model = class_(model_path=ggml_model_path, **kwargs)\n        else:\n            model = cls.HF_Class.from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n    except Exception as e:\n        invalidInputError(False, f'Could not load model from path: {pretrained_model_name_or_path}. Please make sure the CausalLM class matches the model you want to load.Received error {e}')\n    return model",
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, native: bool=True, dtype: str='int4', *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        :param pretrained_model_name_or_path: Path for model checkpoint.\\n               If running with ``native int4``, the path should be converted BigDL-LLM optimized\\n               ggml binary checkpoint, which should be converted by ``bigdl.llm.llm_convert``.\\n               If running with ``transformers int4``, the path should be the huggingface repo id\\n               to be downloaded or the huggingface checkpoint folder.\\n        :param native: Load model to either BigDL-LLM optimized Transformer or Native (ggml) int4.\\n        :param dtype: Which quantized precision will be converted.\\n               Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n               , `gptneox` and `starcoder`.\\n        :param kwargs: keyword arguments which will be passed to the model instance.\\n\\n        :return: a model instance\\n        '\n    try:\n        module = importlib.import_module(cls.GGML_Module)\n        class_ = getattr(module, cls.GGML_Model)\n        if native:\n            invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n            ggml_model_path = pretrained_model_name_or_path\n            model = class_(model_path=ggml_model_path, **kwargs)\n        else:\n            model = cls.HF_Class.from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n    except Exception as e:\n        invalidInputError(False, f'Could not load model from path: {pretrained_model_name_or_path}. Please make sure the CausalLM class matches the model you want to load.Received error {e}')\n    return model",
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, native: bool=True, dtype: str='int4', *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        :param pretrained_model_name_or_path: Path for model checkpoint.\\n               If running with ``native int4``, the path should be converted BigDL-LLM optimized\\n               ggml binary checkpoint, which should be converted by ``bigdl.llm.llm_convert``.\\n               If running with ``transformers int4``, the path should be the huggingface repo id\\n               to be downloaded or the huggingface checkpoint folder.\\n        :param native: Load model to either BigDL-LLM optimized Transformer or Native (ggml) int4.\\n        :param dtype: Which quantized precision will be converted.\\n               Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n               , `gptneox` and `starcoder`.\\n        :param kwargs: keyword arguments which will be passed to the model instance.\\n\\n        :return: a model instance\\n        '\n    try:\n        module = importlib.import_module(cls.GGML_Module)\n        class_ = getattr(module, cls.GGML_Model)\n        if native:\n            invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n            ggml_model_path = pretrained_model_name_or_path\n            model = class_(model_path=ggml_model_path, **kwargs)\n        else:\n            model = cls.HF_Class.from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n    except Exception as e:\n        invalidInputError(False, f'Could not load model from path: {pretrained_model_name_or_path}. Please make sure the CausalLM class matches the model you want to load.Received error {e}')\n    return model",
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, native: bool=True, dtype: str='int4', *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        :param pretrained_model_name_or_path: Path for model checkpoint.\\n               If running with ``native int4``, the path should be converted BigDL-LLM optimized\\n               ggml binary checkpoint, which should be converted by ``bigdl.llm.llm_convert``.\\n               If running with ``transformers int4``, the path should be the huggingface repo id\\n               to be downloaded or the huggingface checkpoint folder.\\n        :param native: Load model to either BigDL-LLM optimized Transformer or Native (ggml) int4.\\n        :param dtype: Which quantized precision will be converted.\\n               Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n               , `gptneox` and `starcoder`.\\n        :param kwargs: keyword arguments which will be passed to the model instance.\\n\\n        :return: a model instance\\n        '\n    try:\n        module = importlib.import_module(cls.GGML_Module)\n        class_ = getattr(module, cls.GGML_Model)\n        if native:\n            invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n            ggml_model_path = pretrained_model_name_or_path\n            model = class_(model_path=ggml_model_path, **kwargs)\n        else:\n            model = cls.HF_Class.from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n    except Exception as e:\n        invalidInputError(False, f'Could not load model from path: {pretrained_model_name_or_path}. Please make sure the CausalLM class matches the model you want to load.Received error {e}')\n    return model",
            "@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path: str, native: bool=True, dtype: str='int4', *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        :param pretrained_model_name_or_path: Path for model checkpoint.\\n               If running with ``native int4``, the path should be converted BigDL-LLM optimized\\n               ggml binary checkpoint, which should be converted by ``bigdl.llm.llm_convert``.\\n               If running with ``transformers int4``, the path should be the huggingface repo id\\n               to be downloaded or the huggingface checkpoint folder.\\n        :param native: Load model to either BigDL-LLM optimized Transformer or Native (ggml) int4.\\n        :param dtype: Which quantized precision will be converted.\\n               Now only `int4` and `int8` are supported, and `int8` only works for `llama`\\n               , `gptneox` and `starcoder`.\\n        :param kwargs: keyword arguments which will be passed to the model instance.\\n\\n        :return: a model instance\\n        '\n    try:\n        module = importlib.import_module(cls.GGML_Module)\n        class_ = getattr(module, cls.GGML_Model)\n        if native:\n            invalidInputError(dtype.lower() in ['int4', 'int8'], 'Now we only support int4 and int8 as date type for weight')\n            ggml_model_path = pretrained_model_name_or_path\n            model = class_(model_path=ggml_model_path, **kwargs)\n        else:\n            model = cls.HF_Class.from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n    except Exception as e:\n        invalidInputError(False, f'Could not load model from path: {pretrained_model_name_or_path}. Please make sure the CausalLM class matches the model you want to load.Received error {e}')\n    return model"
        ]
    }
]