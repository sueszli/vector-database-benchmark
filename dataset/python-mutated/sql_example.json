[
    {
        "func_name": "sql_dataframe_example",
        "original": "def sql_dataframe_example(spark):\n    print('INFO SQL Dataframe Example starts')\n    df = spark.read.parquet('spark-3.1.3/examples/src/main/resources/users.parquet')\n    df.show()\n    res = df.agg({'name': 'max'}).collect()\n    print(res)\n    print('INFO agg API finished')\n    df_as1 = df.alias('df_as1')\n    df_as2 = df.alias('df_as2')\n    joined_df = df_as1.join(df_as2, col('df_as1.name') == col('df_as2.name'), 'inner')\n    res = joined_df.select('df_as1.name', 'df_as2.name', 'df_as2.favorite_color').collect()\n    print(res)\n    print('INFO alias API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.select(df.colRegex('`(Col1)?+.+`')).show()\n    print(df.collect())\n    print(df.columns)\n    print('INFO colRegex, collect and columns API finished')\n    print(df.approxQuantile('Col2', [0.1], 1.0))\n    print('INFO approxQuantile API finished')\n    df.cache()\n    print('INFO cache API finished')\n    res = df.coalesce(1).rdd.getNumPartitions()\n    print(res)\n    print('INFO coalesce API finished')\n    res = df.corr('Col2', 'Col3')\n    print(res)\n    res = df.count()\n    print(res)\n    res = df.cov('Col2', 'Col3')\n    print(res)\n    print('INFO corr, count, cov API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.createGlobalTempView('threecols')\n    df2 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceGlobalTempView('threecols')\n    df3 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO GlobalTempView API finished')\n    df.createTempView('threecols')\n    df2 = spark.sql('select * from threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceTempView('threecols')\n    df3 = spark.sql('select * from threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO TempView API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df2 = spark.createDataFrame([(2, 'Alice'), (5, 'Bob')], ['age', 'name'])\n    df3 = spark.createDataFrame([('Tom', 80), ('Bob', 85)], ['name', 'height'])\n    df2.crossJoin(df3.select('height')).select('age', 'name', 'height').show()\n    res = df.crosstab('Col1', 'Col2')\n    print(res)\n    print('INFO cross API finished')\n    df2.cube('name', df2.age).count().orderBy('name', 'age').show()\n    print('INFO cube API finished')\n    df2.describe().show()\n    print('INFO descibe API finished')\n    res = df.distinct().count()\n    print(res)\n    print('INFO distinct API finished')\n    df3 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df3.dropDuplicates().show()\n    df3.drop('age').show()\n    df3.na.drop().show()\n    print(df3.dtypes)\n    print('INFO drop API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('b', 3)], ['C1', 'C2'])\n    df1.exceptAll(df2).show()\n    df1.explain()\n    df1.na.fill(50).show()\n    res = df1.first()\n    print(res)\n    print('INFO exceptAll, explain, fill and first API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    res = df1.freqItems(['C1'])\n    print(res)\n    res = df1.groupBy().avg().collect()\n    print(res)\n    print('INFO freqItems and groupBy API finished')\n    res = df1.head(1)\n    print(res)\n    print('INFO head API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df1.join(df2.hint('broadcast'), 'C1').show()\n    df1.intersect(df2).sort('C1', 'C2').show()\n    df1.intersectAll(df2).sort('C1', 'C2').show()\n    print('INFO intersect and join API finished')\n    res = df1.isLocal()\n    print(res)\n    print(df1.isStreaming)\n    res = df1.limit(2).collect()\n    print(res)\n    print('INFO isLocal and isStreaming API finished')\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df2.orderBy(df2.C2.desc()).show()\n    df2.persist()\n    df2.unpersist()\n    df2.printSchema()\n    print('INFO orderBy API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    splits = df4.randomSplit([1.0, 2.0], 24)\n    splits[0].show()\n    df4.registerTempTable('people')\n    df2 = spark.sql('select * from people')\n    print(sorted(df4.collect()) == sorted(df2.collect()))\n    spark.catalog.dropTempView('people')\n    print('INFO randomSplit API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    data = df4.union(df4).repartition('age')\n    data.show()\n    data = df4.repartitionByRange('age')\n    data.show()\n    print('INFO repartition API finished')\n    data = df4.na.replace('Alice', None)\n    data.show()\n    df4.rollup('name', df4.age).count().orderBy('name', 'age').show()\n    print('INFO replace API finished')\n    df = spark.range(10)\n    res = df.sample(0.5, 3).count()\n    print(res)\n    res = df.sample(fraction=0.5, seed=3).count()\n    print(res)\n    dataset = spark.range(0, 100).select((col('id') % 3).alias('key'))\n    sampled = dataset.sampleBy('key', fractions={0: 0.1, 1: 0.2}, seed=0)\n    sampled.groupBy('key').count().orderBy('key').show()\n    print(df.schema)\n    print('INFO Sample and sampleBy API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    res = df.select(df.name, (df.age + 10).alias('age')).collect()\n    print(res)\n    res = df.selectExpr('age * 2', 'abs(age)').collect()\n    print(res)\n    df.sortWithinPartitions('age', ascending=False).show()\n    res = df.storageLevel\n    print(res)\n    print('INFO select API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.subtract(df2)\n    df.summary().show()\n    res = df.take(2)\n    print(res)\n    print('INFO substract, summary and take API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.toDF('f1', 'f2').collect()\n    print(res)\n    res = df.toPandas()\n    print(res)\n    print('INFO to API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.union(df2).show()\n    df.unionAll(df2).show()\n    df1 = spark.createDataFrame([[1, 2, 3]], ['col0', 'col1', 'col2'])\n    df2 = spark.createDataFrame([[4, 5, 6]], ['col1', 'col2', 'col0'])\n    df1.unionByName(df2).show()\n    print('INFO union API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.withColumn('age2', df.age + 2).collect()\n    print(res)\n    res = df.withColumnRenamed('age', 'age2').collect()\n    print(res)\n    print('INFO with API finished')\n    res = df.write\n    print(res)\n    print('INFO write API finished')\n    print('INFO SQL Dataframe Example API finished')",
        "mutated": [
            "def sql_dataframe_example(spark):\n    if False:\n        i = 10\n    print('INFO SQL Dataframe Example starts')\n    df = spark.read.parquet('spark-3.1.3/examples/src/main/resources/users.parquet')\n    df.show()\n    res = df.agg({'name': 'max'}).collect()\n    print(res)\n    print('INFO agg API finished')\n    df_as1 = df.alias('df_as1')\n    df_as2 = df.alias('df_as2')\n    joined_df = df_as1.join(df_as2, col('df_as1.name') == col('df_as2.name'), 'inner')\n    res = joined_df.select('df_as1.name', 'df_as2.name', 'df_as2.favorite_color').collect()\n    print(res)\n    print('INFO alias API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.select(df.colRegex('`(Col1)?+.+`')).show()\n    print(df.collect())\n    print(df.columns)\n    print('INFO colRegex, collect and columns API finished')\n    print(df.approxQuantile('Col2', [0.1], 1.0))\n    print('INFO approxQuantile API finished')\n    df.cache()\n    print('INFO cache API finished')\n    res = df.coalesce(1).rdd.getNumPartitions()\n    print(res)\n    print('INFO coalesce API finished')\n    res = df.corr('Col2', 'Col3')\n    print(res)\n    res = df.count()\n    print(res)\n    res = df.cov('Col2', 'Col3')\n    print(res)\n    print('INFO corr, count, cov API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.createGlobalTempView('threecols')\n    df2 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceGlobalTempView('threecols')\n    df3 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO GlobalTempView API finished')\n    df.createTempView('threecols')\n    df2 = spark.sql('select * from threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceTempView('threecols')\n    df3 = spark.sql('select * from threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO TempView API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df2 = spark.createDataFrame([(2, 'Alice'), (5, 'Bob')], ['age', 'name'])\n    df3 = spark.createDataFrame([('Tom', 80), ('Bob', 85)], ['name', 'height'])\n    df2.crossJoin(df3.select('height')).select('age', 'name', 'height').show()\n    res = df.crosstab('Col1', 'Col2')\n    print(res)\n    print('INFO cross API finished')\n    df2.cube('name', df2.age).count().orderBy('name', 'age').show()\n    print('INFO cube API finished')\n    df2.describe().show()\n    print('INFO descibe API finished')\n    res = df.distinct().count()\n    print(res)\n    print('INFO distinct API finished')\n    df3 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df3.dropDuplicates().show()\n    df3.drop('age').show()\n    df3.na.drop().show()\n    print(df3.dtypes)\n    print('INFO drop API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('b', 3)], ['C1', 'C2'])\n    df1.exceptAll(df2).show()\n    df1.explain()\n    df1.na.fill(50).show()\n    res = df1.first()\n    print(res)\n    print('INFO exceptAll, explain, fill and first API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    res = df1.freqItems(['C1'])\n    print(res)\n    res = df1.groupBy().avg().collect()\n    print(res)\n    print('INFO freqItems and groupBy API finished')\n    res = df1.head(1)\n    print(res)\n    print('INFO head API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df1.join(df2.hint('broadcast'), 'C1').show()\n    df1.intersect(df2).sort('C1', 'C2').show()\n    df1.intersectAll(df2).sort('C1', 'C2').show()\n    print('INFO intersect and join API finished')\n    res = df1.isLocal()\n    print(res)\n    print(df1.isStreaming)\n    res = df1.limit(2).collect()\n    print(res)\n    print('INFO isLocal and isStreaming API finished')\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df2.orderBy(df2.C2.desc()).show()\n    df2.persist()\n    df2.unpersist()\n    df2.printSchema()\n    print('INFO orderBy API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    splits = df4.randomSplit([1.0, 2.0], 24)\n    splits[0].show()\n    df4.registerTempTable('people')\n    df2 = spark.sql('select * from people')\n    print(sorted(df4.collect()) == sorted(df2.collect()))\n    spark.catalog.dropTempView('people')\n    print('INFO randomSplit API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    data = df4.union(df4).repartition('age')\n    data.show()\n    data = df4.repartitionByRange('age')\n    data.show()\n    print('INFO repartition API finished')\n    data = df4.na.replace('Alice', None)\n    data.show()\n    df4.rollup('name', df4.age).count().orderBy('name', 'age').show()\n    print('INFO replace API finished')\n    df = spark.range(10)\n    res = df.sample(0.5, 3).count()\n    print(res)\n    res = df.sample(fraction=0.5, seed=3).count()\n    print(res)\n    dataset = spark.range(0, 100).select((col('id') % 3).alias('key'))\n    sampled = dataset.sampleBy('key', fractions={0: 0.1, 1: 0.2}, seed=0)\n    sampled.groupBy('key').count().orderBy('key').show()\n    print(df.schema)\n    print('INFO Sample and sampleBy API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    res = df.select(df.name, (df.age + 10).alias('age')).collect()\n    print(res)\n    res = df.selectExpr('age * 2', 'abs(age)').collect()\n    print(res)\n    df.sortWithinPartitions('age', ascending=False).show()\n    res = df.storageLevel\n    print(res)\n    print('INFO select API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.subtract(df2)\n    df.summary().show()\n    res = df.take(2)\n    print(res)\n    print('INFO substract, summary and take API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.toDF('f1', 'f2').collect()\n    print(res)\n    res = df.toPandas()\n    print(res)\n    print('INFO to API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.union(df2).show()\n    df.unionAll(df2).show()\n    df1 = spark.createDataFrame([[1, 2, 3]], ['col0', 'col1', 'col2'])\n    df2 = spark.createDataFrame([[4, 5, 6]], ['col1', 'col2', 'col0'])\n    df1.unionByName(df2).show()\n    print('INFO union API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.withColumn('age2', df.age + 2).collect()\n    print(res)\n    res = df.withColumnRenamed('age', 'age2').collect()\n    print(res)\n    print('INFO with API finished')\n    res = df.write\n    print(res)\n    print('INFO write API finished')\n    print('INFO SQL Dataframe Example API finished')",
            "def sql_dataframe_example(spark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print('INFO SQL Dataframe Example starts')\n    df = spark.read.parquet('spark-3.1.3/examples/src/main/resources/users.parquet')\n    df.show()\n    res = df.agg({'name': 'max'}).collect()\n    print(res)\n    print('INFO agg API finished')\n    df_as1 = df.alias('df_as1')\n    df_as2 = df.alias('df_as2')\n    joined_df = df_as1.join(df_as2, col('df_as1.name') == col('df_as2.name'), 'inner')\n    res = joined_df.select('df_as1.name', 'df_as2.name', 'df_as2.favorite_color').collect()\n    print(res)\n    print('INFO alias API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.select(df.colRegex('`(Col1)?+.+`')).show()\n    print(df.collect())\n    print(df.columns)\n    print('INFO colRegex, collect and columns API finished')\n    print(df.approxQuantile('Col2', [0.1], 1.0))\n    print('INFO approxQuantile API finished')\n    df.cache()\n    print('INFO cache API finished')\n    res = df.coalesce(1).rdd.getNumPartitions()\n    print(res)\n    print('INFO coalesce API finished')\n    res = df.corr('Col2', 'Col3')\n    print(res)\n    res = df.count()\n    print(res)\n    res = df.cov('Col2', 'Col3')\n    print(res)\n    print('INFO corr, count, cov API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.createGlobalTempView('threecols')\n    df2 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceGlobalTempView('threecols')\n    df3 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO GlobalTempView API finished')\n    df.createTempView('threecols')\n    df2 = spark.sql('select * from threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceTempView('threecols')\n    df3 = spark.sql('select * from threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO TempView API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df2 = spark.createDataFrame([(2, 'Alice'), (5, 'Bob')], ['age', 'name'])\n    df3 = spark.createDataFrame([('Tom', 80), ('Bob', 85)], ['name', 'height'])\n    df2.crossJoin(df3.select('height')).select('age', 'name', 'height').show()\n    res = df.crosstab('Col1', 'Col2')\n    print(res)\n    print('INFO cross API finished')\n    df2.cube('name', df2.age).count().orderBy('name', 'age').show()\n    print('INFO cube API finished')\n    df2.describe().show()\n    print('INFO descibe API finished')\n    res = df.distinct().count()\n    print(res)\n    print('INFO distinct API finished')\n    df3 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df3.dropDuplicates().show()\n    df3.drop('age').show()\n    df3.na.drop().show()\n    print(df3.dtypes)\n    print('INFO drop API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('b', 3)], ['C1', 'C2'])\n    df1.exceptAll(df2).show()\n    df1.explain()\n    df1.na.fill(50).show()\n    res = df1.first()\n    print(res)\n    print('INFO exceptAll, explain, fill and first API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    res = df1.freqItems(['C1'])\n    print(res)\n    res = df1.groupBy().avg().collect()\n    print(res)\n    print('INFO freqItems and groupBy API finished')\n    res = df1.head(1)\n    print(res)\n    print('INFO head API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df1.join(df2.hint('broadcast'), 'C1').show()\n    df1.intersect(df2).sort('C1', 'C2').show()\n    df1.intersectAll(df2).sort('C1', 'C2').show()\n    print('INFO intersect and join API finished')\n    res = df1.isLocal()\n    print(res)\n    print(df1.isStreaming)\n    res = df1.limit(2).collect()\n    print(res)\n    print('INFO isLocal and isStreaming API finished')\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df2.orderBy(df2.C2.desc()).show()\n    df2.persist()\n    df2.unpersist()\n    df2.printSchema()\n    print('INFO orderBy API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    splits = df4.randomSplit([1.0, 2.0], 24)\n    splits[0].show()\n    df4.registerTempTable('people')\n    df2 = spark.sql('select * from people')\n    print(sorted(df4.collect()) == sorted(df2.collect()))\n    spark.catalog.dropTempView('people')\n    print('INFO randomSplit API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    data = df4.union(df4).repartition('age')\n    data.show()\n    data = df4.repartitionByRange('age')\n    data.show()\n    print('INFO repartition API finished')\n    data = df4.na.replace('Alice', None)\n    data.show()\n    df4.rollup('name', df4.age).count().orderBy('name', 'age').show()\n    print('INFO replace API finished')\n    df = spark.range(10)\n    res = df.sample(0.5, 3).count()\n    print(res)\n    res = df.sample(fraction=0.5, seed=3).count()\n    print(res)\n    dataset = spark.range(0, 100).select((col('id') % 3).alias('key'))\n    sampled = dataset.sampleBy('key', fractions={0: 0.1, 1: 0.2}, seed=0)\n    sampled.groupBy('key').count().orderBy('key').show()\n    print(df.schema)\n    print('INFO Sample and sampleBy API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    res = df.select(df.name, (df.age + 10).alias('age')).collect()\n    print(res)\n    res = df.selectExpr('age * 2', 'abs(age)').collect()\n    print(res)\n    df.sortWithinPartitions('age', ascending=False).show()\n    res = df.storageLevel\n    print(res)\n    print('INFO select API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.subtract(df2)\n    df.summary().show()\n    res = df.take(2)\n    print(res)\n    print('INFO substract, summary and take API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.toDF('f1', 'f2').collect()\n    print(res)\n    res = df.toPandas()\n    print(res)\n    print('INFO to API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.union(df2).show()\n    df.unionAll(df2).show()\n    df1 = spark.createDataFrame([[1, 2, 3]], ['col0', 'col1', 'col2'])\n    df2 = spark.createDataFrame([[4, 5, 6]], ['col1', 'col2', 'col0'])\n    df1.unionByName(df2).show()\n    print('INFO union API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.withColumn('age2', df.age + 2).collect()\n    print(res)\n    res = df.withColumnRenamed('age', 'age2').collect()\n    print(res)\n    print('INFO with API finished')\n    res = df.write\n    print(res)\n    print('INFO write API finished')\n    print('INFO SQL Dataframe Example API finished')",
            "def sql_dataframe_example(spark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print('INFO SQL Dataframe Example starts')\n    df = spark.read.parquet('spark-3.1.3/examples/src/main/resources/users.parquet')\n    df.show()\n    res = df.agg({'name': 'max'}).collect()\n    print(res)\n    print('INFO agg API finished')\n    df_as1 = df.alias('df_as1')\n    df_as2 = df.alias('df_as2')\n    joined_df = df_as1.join(df_as2, col('df_as1.name') == col('df_as2.name'), 'inner')\n    res = joined_df.select('df_as1.name', 'df_as2.name', 'df_as2.favorite_color').collect()\n    print(res)\n    print('INFO alias API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.select(df.colRegex('`(Col1)?+.+`')).show()\n    print(df.collect())\n    print(df.columns)\n    print('INFO colRegex, collect and columns API finished')\n    print(df.approxQuantile('Col2', [0.1], 1.0))\n    print('INFO approxQuantile API finished')\n    df.cache()\n    print('INFO cache API finished')\n    res = df.coalesce(1).rdd.getNumPartitions()\n    print(res)\n    print('INFO coalesce API finished')\n    res = df.corr('Col2', 'Col3')\n    print(res)\n    res = df.count()\n    print(res)\n    res = df.cov('Col2', 'Col3')\n    print(res)\n    print('INFO corr, count, cov API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.createGlobalTempView('threecols')\n    df2 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceGlobalTempView('threecols')\n    df3 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO GlobalTempView API finished')\n    df.createTempView('threecols')\n    df2 = spark.sql('select * from threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceTempView('threecols')\n    df3 = spark.sql('select * from threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO TempView API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df2 = spark.createDataFrame([(2, 'Alice'), (5, 'Bob')], ['age', 'name'])\n    df3 = spark.createDataFrame([('Tom', 80), ('Bob', 85)], ['name', 'height'])\n    df2.crossJoin(df3.select('height')).select('age', 'name', 'height').show()\n    res = df.crosstab('Col1', 'Col2')\n    print(res)\n    print('INFO cross API finished')\n    df2.cube('name', df2.age).count().orderBy('name', 'age').show()\n    print('INFO cube API finished')\n    df2.describe().show()\n    print('INFO descibe API finished')\n    res = df.distinct().count()\n    print(res)\n    print('INFO distinct API finished')\n    df3 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df3.dropDuplicates().show()\n    df3.drop('age').show()\n    df3.na.drop().show()\n    print(df3.dtypes)\n    print('INFO drop API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('b', 3)], ['C1', 'C2'])\n    df1.exceptAll(df2).show()\n    df1.explain()\n    df1.na.fill(50).show()\n    res = df1.first()\n    print(res)\n    print('INFO exceptAll, explain, fill and first API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    res = df1.freqItems(['C1'])\n    print(res)\n    res = df1.groupBy().avg().collect()\n    print(res)\n    print('INFO freqItems and groupBy API finished')\n    res = df1.head(1)\n    print(res)\n    print('INFO head API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df1.join(df2.hint('broadcast'), 'C1').show()\n    df1.intersect(df2).sort('C1', 'C2').show()\n    df1.intersectAll(df2).sort('C1', 'C2').show()\n    print('INFO intersect and join API finished')\n    res = df1.isLocal()\n    print(res)\n    print(df1.isStreaming)\n    res = df1.limit(2).collect()\n    print(res)\n    print('INFO isLocal and isStreaming API finished')\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df2.orderBy(df2.C2.desc()).show()\n    df2.persist()\n    df2.unpersist()\n    df2.printSchema()\n    print('INFO orderBy API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    splits = df4.randomSplit([1.0, 2.0], 24)\n    splits[0].show()\n    df4.registerTempTable('people')\n    df2 = spark.sql('select * from people')\n    print(sorted(df4.collect()) == sorted(df2.collect()))\n    spark.catalog.dropTempView('people')\n    print('INFO randomSplit API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    data = df4.union(df4).repartition('age')\n    data.show()\n    data = df4.repartitionByRange('age')\n    data.show()\n    print('INFO repartition API finished')\n    data = df4.na.replace('Alice', None)\n    data.show()\n    df4.rollup('name', df4.age).count().orderBy('name', 'age').show()\n    print('INFO replace API finished')\n    df = spark.range(10)\n    res = df.sample(0.5, 3).count()\n    print(res)\n    res = df.sample(fraction=0.5, seed=3).count()\n    print(res)\n    dataset = spark.range(0, 100).select((col('id') % 3).alias('key'))\n    sampled = dataset.sampleBy('key', fractions={0: 0.1, 1: 0.2}, seed=0)\n    sampled.groupBy('key').count().orderBy('key').show()\n    print(df.schema)\n    print('INFO Sample and sampleBy API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    res = df.select(df.name, (df.age + 10).alias('age')).collect()\n    print(res)\n    res = df.selectExpr('age * 2', 'abs(age)').collect()\n    print(res)\n    df.sortWithinPartitions('age', ascending=False).show()\n    res = df.storageLevel\n    print(res)\n    print('INFO select API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.subtract(df2)\n    df.summary().show()\n    res = df.take(2)\n    print(res)\n    print('INFO substract, summary and take API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.toDF('f1', 'f2').collect()\n    print(res)\n    res = df.toPandas()\n    print(res)\n    print('INFO to API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.union(df2).show()\n    df.unionAll(df2).show()\n    df1 = spark.createDataFrame([[1, 2, 3]], ['col0', 'col1', 'col2'])\n    df2 = spark.createDataFrame([[4, 5, 6]], ['col1', 'col2', 'col0'])\n    df1.unionByName(df2).show()\n    print('INFO union API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.withColumn('age2', df.age + 2).collect()\n    print(res)\n    res = df.withColumnRenamed('age', 'age2').collect()\n    print(res)\n    print('INFO with API finished')\n    res = df.write\n    print(res)\n    print('INFO write API finished')\n    print('INFO SQL Dataframe Example API finished')",
            "def sql_dataframe_example(spark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print('INFO SQL Dataframe Example starts')\n    df = spark.read.parquet('spark-3.1.3/examples/src/main/resources/users.parquet')\n    df.show()\n    res = df.agg({'name': 'max'}).collect()\n    print(res)\n    print('INFO agg API finished')\n    df_as1 = df.alias('df_as1')\n    df_as2 = df.alias('df_as2')\n    joined_df = df_as1.join(df_as2, col('df_as1.name') == col('df_as2.name'), 'inner')\n    res = joined_df.select('df_as1.name', 'df_as2.name', 'df_as2.favorite_color').collect()\n    print(res)\n    print('INFO alias API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.select(df.colRegex('`(Col1)?+.+`')).show()\n    print(df.collect())\n    print(df.columns)\n    print('INFO colRegex, collect and columns API finished')\n    print(df.approxQuantile('Col2', [0.1], 1.0))\n    print('INFO approxQuantile API finished')\n    df.cache()\n    print('INFO cache API finished')\n    res = df.coalesce(1).rdd.getNumPartitions()\n    print(res)\n    print('INFO coalesce API finished')\n    res = df.corr('Col2', 'Col3')\n    print(res)\n    res = df.count()\n    print(res)\n    res = df.cov('Col2', 'Col3')\n    print(res)\n    print('INFO corr, count, cov API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.createGlobalTempView('threecols')\n    df2 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceGlobalTempView('threecols')\n    df3 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO GlobalTempView API finished')\n    df.createTempView('threecols')\n    df2 = spark.sql('select * from threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceTempView('threecols')\n    df3 = spark.sql('select * from threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO TempView API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df2 = spark.createDataFrame([(2, 'Alice'), (5, 'Bob')], ['age', 'name'])\n    df3 = spark.createDataFrame([('Tom', 80), ('Bob', 85)], ['name', 'height'])\n    df2.crossJoin(df3.select('height')).select('age', 'name', 'height').show()\n    res = df.crosstab('Col1', 'Col2')\n    print(res)\n    print('INFO cross API finished')\n    df2.cube('name', df2.age).count().orderBy('name', 'age').show()\n    print('INFO cube API finished')\n    df2.describe().show()\n    print('INFO descibe API finished')\n    res = df.distinct().count()\n    print(res)\n    print('INFO distinct API finished')\n    df3 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df3.dropDuplicates().show()\n    df3.drop('age').show()\n    df3.na.drop().show()\n    print(df3.dtypes)\n    print('INFO drop API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('b', 3)], ['C1', 'C2'])\n    df1.exceptAll(df2).show()\n    df1.explain()\n    df1.na.fill(50).show()\n    res = df1.first()\n    print(res)\n    print('INFO exceptAll, explain, fill and first API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    res = df1.freqItems(['C1'])\n    print(res)\n    res = df1.groupBy().avg().collect()\n    print(res)\n    print('INFO freqItems and groupBy API finished')\n    res = df1.head(1)\n    print(res)\n    print('INFO head API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df1.join(df2.hint('broadcast'), 'C1').show()\n    df1.intersect(df2).sort('C1', 'C2').show()\n    df1.intersectAll(df2).sort('C1', 'C2').show()\n    print('INFO intersect and join API finished')\n    res = df1.isLocal()\n    print(res)\n    print(df1.isStreaming)\n    res = df1.limit(2).collect()\n    print(res)\n    print('INFO isLocal and isStreaming API finished')\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df2.orderBy(df2.C2.desc()).show()\n    df2.persist()\n    df2.unpersist()\n    df2.printSchema()\n    print('INFO orderBy API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    splits = df4.randomSplit([1.0, 2.0], 24)\n    splits[0].show()\n    df4.registerTempTable('people')\n    df2 = spark.sql('select * from people')\n    print(sorted(df4.collect()) == sorted(df2.collect()))\n    spark.catalog.dropTempView('people')\n    print('INFO randomSplit API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    data = df4.union(df4).repartition('age')\n    data.show()\n    data = df4.repartitionByRange('age')\n    data.show()\n    print('INFO repartition API finished')\n    data = df4.na.replace('Alice', None)\n    data.show()\n    df4.rollup('name', df4.age).count().orderBy('name', 'age').show()\n    print('INFO replace API finished')\n    df = spark.range(10)\n    res = df.sample(0.5, 3).count()\n    print(res)\n    res = df.sample(fraction=0.5, seed=3).count()\n    print(res)\n    dataset = spark.range(0, 100).select((col('id') % 3).alias('key'))\n    sampled = dataset.sampleBy('key', fractions={0: 0.1, 1: 0.2}, seed=0)\n    sampled.groupBy('key').count().orderBy('key').show()\n    print(df.schema)\n    print('INFO Sample and sampleBy API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    res = df.select(df.name, (df.age + 10).alias('age')).collect()\n    print(res)\n    res = df.selectExpr('age * 2', 'abs(age)').collect()\n    print(res)\n    df.sortWithinPartitions('age', ascending=False).show()\n    res = df.storageLevel\n    print(res)\n    print('INFO select API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.subtract(df2)\n    df.summary().show()\n    res = df.take(2)\n    print(res)\n    print('INFO substract, summary and take API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.toDF('f1', 'f2').collect()\n    print(res)\n    res = df.toPandas()\n    print(res)\n    print('INFO to API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.union(df2).show()\n    df.unionAll(df2).show()\n    df1 = spark.createDataFrame([[1, 2, 3]], ['col0', 'col1', 'col2'])\n    df2 = spark.createDataFrame([[4, 5, 6]], ['col1', 'col2', 'col0'])\n    df1.unionByName(df2).show()\n    print('INFO union API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.withColumn('age2', df.age + 2).collect()\n    print(res)\n    res = df.withColumnRenamed('age', 'age2').collect()\n    print(res)\n    print('INFO with API finished')\n    res = df.write\n    print(res)\n    print('INFO write API finished')\n    print('INFO SQL Dataframe Example API finished')",
            "def sql_dataframe_example(spark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print('INFO SQL Dataframe Example starts')\n    df = spark.read.parquet('spark-3.1.3/examples/src/main/resources/users.parquet')\n    df.show()\n    res = df.agg({'name': 'max'}).collect()\n    print(res)\n    print('INFO agg API finished')\n    df_as1 = df.alias('df_as1')\n    df_as2 = df.alias('df_as2')\n    joined_df = df_as1.join(df_as2, col('df_as1.name') == col('df_as2.name'), 'inner')\n    res = joined_df.select('df_as1.name', 'df_as2.name', 'df_as2.favorite_color').collect()\n    print(res)\n    print('INFO alias API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.select(df.colRegex('`(Col1)?+.+`')).show()\n    print(df.collect())\n    print(df.columns)\n    print('INFO colRegex, collect and columns API finished')\n    print(df.approxQuantile('Col2', [0.1], 1.0))\n    print('INFO approxQuantile API finished')\n    df.cache()\n    print('INFO cache API finished')\n    res = df.coalesce(1).rdd.getNumPartitions()\n    print(res)\n    print('INFO coalesce API finished')\n    res = df.corr('Col2', 'Col3')\n    print(res)\n    res = df.count()\n    print(res)\n    res = df.cov('Col2', 'Col3')\n    print(res)\n    print('INFO corr, count, cov API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df.createGlobalTempView('threecols')\n    df2 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceGlobalTempView('threecols')\n    df3 = spark.sql('select * from global_temp.threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO GlobalTempView API finished')\n    df.createTempView('threecols')\n    df2 = spark.sql('select * from threecols')\n    print(sorted(df.collect()) == sorted(df2.collect()))\n    df2 = df.filter(df.Col2 > 2.0)\n    df2.createOrReplaceTempView('threecols')\n    df3 = spark.sql('select * from threecols')\n    print(sorted(df3.collect()) == sorted(df2.collect()))\n    print('INFO TempView API finished')\n    df = spark.createDataFrame([('a', 1.1, 1.0), ('b', 2.2, 2.0), ('c', 3.3, 3.0)], ['Col1', 'Col2', 'Col3'])\n    df2 = spark.createDataFrame([(2, 'Alice'), (5, 'Bob')], ['age', 'name'])\n    df3 = spark.createDataFrame([('Tom', 80), ('Bob', 85)], ['name', 'height'])\n    df2.crossJoin(df3.select('height')).select('age', 'name', 'height').show()\n    res = df.crosstab('Col1', 'Col2')\n    print(res)\n    print('INFO cross API finished')\n    df2.cube('name', df2.age).count().orderBy('name', 'age').show()\n    print('INFO cube API finished')\n    df2.describe().show()\n    print('INFO descibe API finished')\n    res = df.distinct().count()\n    print(res)\n    print('INFO distinct API finished')\n    df3 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df3.dropDuplicates().show()\n    df3.drop('age').show()\n    df3.na.drop().show()\n    print(df3.dtypes)\n    print('INFO drop API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('b', 3)], ['C1', 'C2'])\n    df1.exceptAll(df2).show()\n    df1.explain()\n    df1.na.fill(50).show()\n    res = df1.first()\n    print(res)\n    print('INFO exceptAll, explain, fill and first API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('a', 1), ('a', 2), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    res = df1.freqItems(['C1'])\n    print(res)\n    res = df1.groupBy().avg().collect()\n    print(res)\n    print('INFO freqItems and groupBy API finished')\n    res = df1.head(1)\n    print(res)\n    print('INFO head API finished')\n    df1 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 4)], ['C1', 'C2'])\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df1.join(df2.hint('broadcast'), 'C1').show()\n    df1.intersect(df2).sort('C1', 'C2').show()\n    df1.intersectAll(df2).sort('C1', 'C2').show()\n    print('INFO intersect and join API finished')\n    res = df1.isLocal()\n    print(res)\n    print(df1.isStreaming)\n    res = df1.limit(2).collect()\n    print(res)\n    print('INFO isLocal and isStreaming API finished')\n    df2 = spark.createDataFrame([('a', 1), ('a', 1), ('b', 3), ('c', 0)], ['C1', 'C2'])\n    df2.orderBy(df2.C2.desc()).show()\n    df2.persist()\n    df2.unpersist()\n    df2.printSchema()\n    print('INFO orderBy API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    splits = df4.randomSplit([1.0, 2.0], 24)\n    splits[0].show()\n    df4.registerTempTable('people')\n    df2 = spark.sql('select * from people')\n    print(sorted(df4.collect()) == sorted(df2.collect()))\n    spark.catalog.dropTempView('people')\n    print('INFO randomSplit API finished')\n    df4 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    data = df4.union(df4).repartition('age')\n    data.show()\n    data = df4.repartitionByRange('age')\n    data.show()\n    print('INFO repartition API finished')\n    data = df4.na.replace('Alice', None)\n    data.show()\n    df4.rollup('name', df4.age).count().orderBy('name', 'age').show()\n    print('INFO replace API finished')\n    df = spark.range(10)\n    res = df.sample(0.5, 3).count()\n    print(res)\n    res = df.sample(fraction=0.5, seed=3).count()\n    print(res)\n    dataset = spark.range(0, 100).select((col('id') % 3).alias('key'))\n    sampled = dataset.sampleBy('key', fractions={0: 0.1, 1: 0.2}, seed=0)\n    sampled.groupBy('key').count().orderBy('key').show()\n    print(df.schema)\n    print('INFO Sample and sampleBy API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    res = df.select(df.name, (df.age + 10).alias('age')).collect()\n    print(res)\n    res = df.selectExpr('age * 2', 'abs(age)').collect()\n    print(res)\n    df.sortWithinPartitions('age', ascending=False).show()\n    res = df.storageLevel\n    print(res)\n    print('INFO select API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.subtract(df2)\n    df.summary().show()\n    res = df.take(2)\n    print(res)\n    print('INFO substract, summary and take API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.toDF('f1', 'f2').collect()\n    print(res)\n    res = df.toPandas()\n    print(res)\n    print('INFO to API finished')\n    df = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80), ('Alice', 10, 80)], ['name', 'age', 'height'])\n    df2 = spark.createDataFrame([('Alice', 5, 80), ('Alice', 5, 80)], ['name', 'age', 'height'])\n    df.union(df2).show()\n    df.unionAll(df2).show()\n    df1 = spark.createDataFrame([[1, 2, 3]], ['col0', 'col1', 'col2'])\n    df2 = spark.createDataFrame([[4, 5, 6]], ['col1', 'col2', 'col0'])\n    df1.unionByName(df2).show()\n    print('INFO union API finished')\n    df = spark.createDataFrame([('Alice', 5), ('Alice', 5)], ['name', 'age'])\n    res = df.withColumn('age2', df.age + 2).collect()\n    print(res)\n    res = df.withColumnRenamed('age', 'age2').collect()\n    print(res)\n    print('INFO with API finished')\n    res = df.write\n    print(res)\n    print('INFO write API finished')\n    print('INFO SQL Dataframe Example API finished')"
        ]
    }
]