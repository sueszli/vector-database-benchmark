from paddle import _legacy_C_ops
from paddle.framework import in_dynamic_mode

def fused_gate_attention(query, key=None, query_weight=None, key_weight=None, value_weight=None, qkv_weight=None, gate_linear_weight=None, gate_linear_bias=None, out_linear_weight=None, out_linear_bias=None, nonbatched_bias=None, attn_mask=None, has_gating=True, merge_qkv=True, use_flash_attn=False):
    if False:
        i = 10
        return i + 15
    '\n    Attention mapps queries and a set of key-value pairs to outputs, and\n    Gate Attention performs multiple parallel attention to jointly attending\n    to information from different representation subspaces. This API only\n    support self_attention. The pseudo code is as follows:\n\n    .. code-block:: text\n\n        c = c ** (-0.5)\n        q = paddle.einsum(\'nbqa,ahc->nbqhc\', q_data, query_w) * c\n        k = paddle.einsum(\'nbka,ahc->nbkhc\', m_data, key_w)\n        v = paddle.einsum(\'nbka,ahc->nbkhc\', m_data, value_w)\n        logits = paddle.einsum(\'nbqhc,nbkhc->nbhqk\', q, k) + bias\n\n        if nonbatched_bias is not None:\n            logits += paddle.unsqueeze(nonbatched_bias, axis=1)\n\n        weights = paddle.nn.functional.softmax(logits)\n        weighted_avg = paddle.einsum(\'nbhqk,nbkhc->nbqhc\', weights, v)\n\n        if has_gating:\n            gate_values = paddle.einsum(\'nbqc,chv->nbqhv\', q_data, gating_w) + gating_b\n            gate_values = paddle.nn.functional.sigmoid(gate_values)\n            weighted_avg *= gate_values\n\n        output = paddle.einsum(\'nbqhc,hco->nbqo\', weighted_avg, output_w) + output_b\n\n\n    Args:\n        query (Tensor): The input query tensor. The shape is [batch_size, msa_len, res_len, q_dim].\n        key (Tensor, optional): The input key tensor, which can be set when\n            merge_qkv is False. The shape is [batch_size, msa_len, m_size, kv_dim]. Default None.\n        query_weight (Tensor, optional): The weight of query linear, which should be set when input\n            key is not None. The shape is [q_dim, num_heads, head_dim]. Default None.\n        key_weight (Tensor, optional): The weight of key linear, which should be set when input key\n            is not None. The shape is [kv_dim, num_heads, head_dim]. Default None.\n        value_weight (Tensor, optional): The weight of value linear, which should be set when input\n            key is not None. The shape is [kv_dim, num_heads, head_dim]. Default None.\n        qkv_weight (Tensor, optional): The weight of qkv linear, which should be set when merge_qkv\n            is True. The shape is [3, num_heads, head_dim, q_dim]. Default None.\n        gate_linear_weight (Tensor, optional): The weight of gating linear, which should be set when\n            has_gating is True. The shape is [q_dim, num_heads, head_dim]. Default None.\n        gate_linear_bias (Tensor, optional): The bias of gating linear, which should be set when\n            has_gating is True. The shape is [num_heads, head_dim]. Default None.\n        out_linear_weight (Tensor, optional): The weight of output linear. The shape is [num_heads, head_dim, q_dim]. Default None.\n        out_linear_bias (Tensor): The bias of output linear, the shape is [q_dim]. Default None.\n        nonbatched_bias (Tensor, optional): The extra bias. The shape is [batch_size, 1, num_heads, res_len, m_size]. Default None.\n        attn_mask (Tensor, optional):  The attention mask. The shape is [batch_size, msa_len, 1, 1, res_len]. Default None.\n        has_gating (bool, optional): Whether has the gating linear. Default True.\n        merge_qkv (bool, optional): Whether has the gating linear. Default True.\n        use_flash_attn (bool, optional): Whether use flash-attention to speedup. Default False.\n\n    Returns:\n        Tensor: The output Tensor, the data type and shape is same as `query`.\n\n    Examples:\n\n        .. code-block:: python\n\n            >>> # doctest: +REQUIRES(env:GPU)\n            >>> import paddle\n            >>> import paddle.incubate.nn.functional as F\n\n            >>> # batch_size = 2\n            >>> # msa_len = 4\n            >>> # res_len = 2\n            >>> # q_dim = 4\n            >>> # num_heads = 8\n            >>> # head_dim = 4\n            >>> # m_size = res_len (when merge_qkv is True)\n\n            >>> # query: [batch_size, msa_len, res_len, q_dim]\n            >>> query = paddle.rand(shape=[2, 4, 2, 4], dtype="float32")\n\n            >>> # qkv_weight:  [3, n_heads, head_dim, q_dim]\n            >>> qkv_weight = paddle.rand(shape=[3, 8, 4, 4], dtype="float32")\n\n            >>> # nonbatched_bias: [batch_size, 1, num_heads, res_len, m_size]\n            >>> nonbatched_bias = paddle.rand(shape=[2, 1, 8, 2, 2], dtype="float32")\n\n            >>> # attn_mask: [batch_size, msa_len, 1, 1, m_size]\n            >>> attn_mask = paddle.rand(shape=[2, 4, 1, 1, 2], dtype="float32")\n\n            >>> # gate_linear_weight: [q_dim, num_heads, head_dim]\n            >>> gate_linear_weight = paddle.rand(shape=[4, 8, 4], dtype="float32")\n            >>> # gate_bias: [num_heads, head_dim]\n            >>> gate_linear_bias = paddle.rand(shape=[8, 4], dtype="float32")\n\n            >>> # out_linear_weight: [num_heads, head_dim, q_dim]\n            >>> out_linear_weight = paddle.rand(shape=[8, 4, 4], dtype="float32")\n            >>> # out_linear_bias: [q_dim]\n            >>> out_linear_bias = paddle.rand(shape=[4], dtype="float32")\n\n            >>> # output: [batch_size, msa_len, res_len, q_dim]\n            >>> output = F.fused_gate_attention(\n            ...     query=query,\n            ...     qkv_weight=qkv_weight,\n            ...     gate_linear_weight=gate_linear_weight,\n            ...     gate_linear_bias=gate_linear_bias,\n            ...     out_linear_weight=out_linear_weight,\n            ...     out_linear_bias=out_linear_bias,\n            ...     nonbatched_bias=nonbatched_bias,\n            ...     attn_mask=attn_mask,\n            ...     has_gating=True,\n            ...     merge_qkv=True)\n            >>> print(output.shape)\n            [2, 4, 2, 4]\n\n    '
    if in_dynamic_mode():
        (_, _, _, _, _, _, _, _, out) = _legacy_C_ops.fused_gate_attention(query, key, query_weight, key_weight, value_weight, qkv_weight, nonbatched_bias, attn_mask, gate_linear_weight, gate_linear_bias, out_linear_weight, out_linear_bias, 'has_gating', has_gating, 'merge_qkv', merge_qkv, 'use_flash_attn', use_flash_attn)
        return out