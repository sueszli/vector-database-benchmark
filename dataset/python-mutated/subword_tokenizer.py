from __future__ import annotations
import warnings
from typing import Union
import cupy as cp
from cudf._lib.nvtext.subword_tokenize import Hashed_Vocabulary as cpp_hashed_vocabulary, subword_tokenize_inmem_hash as cpp_subword_tokenize

def _cast_to_appropriate_type(ar, cast_type):
    if False:
        i = 10
        return i + 15
    if cast_type == 'cp':
        return ar
    if cast_type == 'pt':
        from torch.utils.dlpack import from_dlpack
    elif cast_type == 'tf':
        from tensorflow.experimental.dlpack import from_dlpack
    return from_dlpack(ar.astype('int32').toDlpack())

class SubwordTokenizer:
    """
    Run CUDA BERT subword tokenizer on cuDF strings column.
    Encodes words to token ids using vocabulary from a pretrained
    tokenizer.
    This function requires about 21x the number of character bytes
    in the input strings column as working memory.

    Parameters
    ----------
    hash_file : str
        Path to hash file containing vocabulary of words with token-ids.
        This can be created from the raw vocabulary
        using the ``cudf.utils.hash_vocab_utils.hash_vocab`` function

    do_lower : bool, Default is True
        If set to True, original text will be lowercased before encoding.

    Returns
    -------
    SubwordTokenizer
    """

    def __init__(self, hash_file: str, do_lower_case: bool=True):
        if False:
            print('Hello World!')
        self.do_lower_case = do_lower_case
        self.vocab_file = cpp_hashed_vocabulary(hash_file)

    def __call__(self, text, max_length: int, max_num_rows: int, add_special_tokens: bool=True, padding: str='max_length', truncation: Union[bool, str]=False, stride: int=0, return_tensors: str='cp', return_token_type_ids: bool=False):
        if False:
            for i in range(10):
                print('nop')
        '\n        Run CUDA BERT subword tokenizer on cuDF strings column.\n        Encodes words to token ids using vocabulary from a\n        pretrained tokenizer.\n\n        Parameters\n        ----------\n        text : cudf string series\n            The batch of sequences to be encoded.\n\n        max_length : int\n            Controls the maximum length to use or pad to.\n\n        max_num_rows : int\n            Maximum number of rows for the output token-ids expected to\n            be generated by the tokenizer.\n            Used for allocating temporary working memory on the GPU device.\n            If the output generates a larger number of rows,\n            behavior is undefined.\n            This will vary based on stride, truncation, and max_length.\n            For example, for non-overlapping sequences output rows will be\n            the same as input rows.\n            A good default can be twice the max_length\n\n        add_special_tokens : bool, optional, defaults to True\n            Whether or not to encode the sequences with the special tokens\n            of the BERT classification model\n\n        padding : "max_length"\n            Pad to a maximum length specified with the argument max_length\n\n        truncation : bool, defaults to False\n            True:\n            Truncate to a maximum length specified with the argument max_length\n            False or \'do_not_truncate\': default\n            No truncation (Output differs from HuggingFace)\n\n        stride : int, optional, defaults to 0\n            The value of this argument defines the number of\n            overlapping tokens.\n            The information about the overlapping tokens is\n            present in the metadata outputted.\n\n        return_tensors : str, {"cp", "pt", "tf"} defaults to "cp"\n            "cp" : Return cupy cp.ndarray objects\n            "tf" : Return TensorFlow tf.constant objects\n            "pt" : Return PyTorch torch.Tensor objects\n\n\n        return_token_type_ids : bool, optional\n            Only False currently supported\n\n        Returns\n        -------\n        An encoding with the following fields:\n            input_ids:(type defined by return_tensors)\n                A tensor of token ids to be fed to the model.\n            attention_mask: (type defined by return_tensors)\n                A tensor of indices specifying which tokens\n                should be attended to by the model\n            metadata: (type defined by return_tensors)\n                Each row contains the index id of the original string and the\n                first and last index of the token-ids that are non-padded and\n                non-overlapping\n\n        Examples\n        --------\n        >>> import cudf\n        >>> from cudf.utils.hash_vocab_utils import hash_vocab\n        >>> hash_vocab(\'bert-base-cased-vocab.txt\', \'voc_hash.txt\')\n\n\n        >>> from cudf.core.subword_tokenizer import SubwordTokenizer\n        >>> cudf_tokenizer = SubwordTokenizer(\'voc_hash.txt\',\n        ...                                    do_lower_case=True)\n        >>> str_series = cudf.Series([\'This is the\', \'best book\'])\n        >>> tokenizer_output = cudf_tokenizer(str_series,\n        ...                                   max_length=8,\n        ...                                   max_num_rows=len(str_series),\n        ...                                   padding=\'max_length\',\n        ...                                   return_tensors=\'pt\',\n        ...                                   truncation=True)\n        >>> tokenizer_output[\'input_ids\']\n        tensor([[ 101, 1142, 1110, 1103,  102,    0,    0,    0],\n                [ 101, 1436, 1520,  102,    0,    0,    0,    0]],\n                device=\'cuda:0\',\n               dtype=torch.int32)\n        >>> tokenizer_output[\'attention_mask\']\n        tensor([[1, 1, 1, 1, 1, 0, 0, 0],\n                [1, 1, 1, 1, 0, 0, 0, 0]],\n                device=\'cuda:0\', dtype=torch.int32)\n        >>> tokenizer_output[\'metadata\']\n        tensor([[0, 1, 3],\n                [1, 1, 2]], device=\'cuda:0\', dtype=torch.int32)\n        '
        if return_token_type_ids:
            error_msg = 'Returning token_type_ids is currently supported'
            raise NotImplementedError(error_msg)
        if truncation in (False, 'do_not_truncate'):
            if add_special_tokens:
                error_msg = f'Adding special tokens is not supported with truncation = {truncation}. '
                recommendation = 'Custom Cupy kernel can potentially be used to add it. For reference see: _bert_add_special_tokens'
                raise NotImplementedError(error_msg + recommendation)
            truncation = False
            warning_msg = 'When truncation is not True, the behavior currently differs from HuggingFace as cudf always returns overflowing tokens'
            warnings.warn(warning_msg)
        if padding != 'max_length':
            error_msg = 'Only padding to the provided max_lengthis currently supported'
            raise NotImplementedError(error_msg)
        if max_length <= stride:
            error_msg = 'Stride should be less than max_length'
            raise ValueError(error_msg)
        if return_tensors not in {'cp', 'pt', 'tf'}:
            error_msg = 'Only cupy(cp), pytorch(pt) and tensorflow(tf) tensors are supported'
            raise NotImplementedError(error_msg)
        stride = max_length - stride
        (input_ids, attention_mask, metadata) = cpp_subword_tokenize(text._column, self.vocab_file, max_sequence_length=max_length, stride=stride, do_lower=self.do_lower_case, do_truncate=truncation)
        tokenizer_output = {'input_ids': cp.asarray(input_ids).reshape(-1, max_length), 'attention_mask': cp.asarray(attention_mask).reshape(-1, max_length), 'metadata': cp.asarray(metadata).reshape(-1, 3)}
        if add_special_tokens:
            tokenizer_output = _bert_add_special_tokens(tokenizer_output)
        tokenizer_output = {k: _cast_to_appropriate_type(v, return_tensors) for (k, v) in tokenizer_output.items()}
        return tokenizer_output

def _bert_add_special_tokens(token_o):
    if False:
        while True:
            i = 10
    '\n    Adds special tokens (CLS,SEP) which are often used by pre-trained BERT\n    models to input_ids and adjusts attention_mask and metadata to account\n    for them.\n    '
    max_length = token_o['input_ids'].shape[1]
    seq_end_col = max_length - (token_o['input_ids'][:, ::-1] != 0).argmax(1)
    seq_end_col = cp.clip(seq_end_col + 1, a_min=None, a_max=max_length - 1)
    _bert_add_special_tokens_input_ids(token_o['input_ids'], seq_end_col)
    _bert_add_special_tokens_attention_mask(token_o['attention_mask'], seq_end_col)
    _bert_add_special_tokens_metadata(token_o['metadata'], max_length)
    return token_o

def _bert_add_special_tokens_input_ids(input_ids, seq_end_col):
    if False:
        print('Hello World!')
    '\n    Add token ids for special tokens ([CLS] and [SEP]) to\n    the start and end of each sequence\n    '
    input_ids[:, 1:-1] = input_ids[:, 0:-2]
    input_ids[:, 0] = 101
    input_ids[cp.arange(0, input_ids.shape[0], dtype=cp.uint32), seq_end_col] = 102

def _bert_add_special_tokens_attention_mask(attention_mask, seq_end_col):
    if False:
        i = 10
        return i + 15
    '\n    Mark attention mask for special tokens ([CLS] and [SEP]) with 1\n    '
    attention_mask[:, 1:-1] = attention_mask[:, 0:-2]
    attention_mask[:, 0] = 1
    attention_mask[cp.arange(0, attention_mask.shape[0], dtype=cp.uint32), seq_end_col] = 1

def _bert_add_special_tokens_metadata(metadata, max_length):
    if False:
        return 10
    '\n    Edit metadata to account for the added special tokens ([CLS] and [SEP])\n    '
    metadata[:, 1] = metadata[:, 1] + 1
    metadata[:, 2] = cp.clip(metadata[:, 2] + 1, a_min=None, a_max=max_length - 2)