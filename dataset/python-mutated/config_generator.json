[
    {
        "func_name": "__init__",
        "original": "def __init__(self, configspace, min_points_in_model=None, top_n_percent=15, num_samples=64, random_fraction=1 / 3, bandwidth_factor=3, min_bandwidth=0.001):\n    \"\"\"Fits for each given budget a kernel density estimator on the best N percent of the\n        evaluated configurations on this budget.\n\n\n        Parameters:\n        -----------\n        configspace: ConfigSpace\n            Configuration space object\n        top_n_percent: int\n            Determines the percentile of configurations that will be used as training data\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\n            for training.\n        min_points_in_model: int\n            minimum number of datapoints needed to fit a model\n        num_samples: int\n            number of samples drawn to optimize EI via sampling\n        random_fraction: float\n            fraction of random configurations returned\n        bandwidth_factor: float\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\n        min_bandwidth: float\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\n        \"\"\"\n    self.top_n_percent = top_n_percent\n    self.configspace = configspace\n    self.bw_factor = bandwidth_factor\n    self.min_bandwidth = min_bandwidth\n    self.min_points_in_model = min_points_in_model\n    if min_points_in_model is None:\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    if self.min_points_in_model < len(self.configspace.get_hyperparameters()) + 1:\n        logger.warning('Invalid min_points_in_model value. Setting it to %i', len(self.configspace.get_hyperparameters()) + 1)\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    self.num_samples = num_samples\n    self.random_fraction = random_fraction\n    hps = self.configspace.get_hyperparameters()\n    self.kde_vartypes = ''\n    self.vartypes = []\n    for h in hps:\n        if hasattr(h, 'choices'):\n            self.kde_vartypes += 'u'\n            self.vartypes += [len(h.choices)]\n        else:\n            self.kde_vartypes += 'c'\n            self.vartypes += [0]\n    self.vartypes = np.array(self.vartypes, dtype=int)\n    self.cat_probs = []\n    self.configs = dict()\n    self.losses = dict()\n    self.good_config_rankings = dict()\n    self.kde_models = dict()",
        "mutated": [
            "def __init__(self, configspace, min_points_in_model=None, top_n_percent=15, num_samples=64, random_fraction=1 / 3, bandwidth_factor=3, min_bandwidth=0.001):\n    if False:\n        i = 10\n    'Fits for each given budget a kernel density estimator on the best N percent of the\\n        evaluated configurations on this budget.\\n\\n\\n        Parameters:\\n        -----------\\n        configspace: ConfigSpace\\n            Configuration space object\\n        top_n_percent: int\\n            Determines the percentile of configurations that will be used as training data\\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\\n            for training.\\n        min_points_in_model: int\\n            minimum number of datapoints needed to fit a model\\n        num_samples: int\\n            number of samples drawn to optimize EI via sampling\\n        random_fraction: float\\n            fraction of random configurations returned\\n        bandwidth_factor: float\\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\\n        min_bandwidth: float\\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\\n        '\n    self.top_n_percent = top_n_percent\n    self.configspace = configspace\n    self.bw_factor = bandwidth_factor\n    self.min_bandwidth = min_bandwidth\n    self.min_points_in_model = min_points_in_model\n    if min_points_in_model is None:\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    if self.min_points_in_model < len(self.configspace.get_hyperparameters()) + 1:\n        logger.warning('Invalid min_points_in_model value. Setting it to %i', len(self.configspace.get_hyperparameters()) + 1)\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    self.num_samples = num_samples\n    self.random_fraction = random_fraction\n    hps = self.configspace.get_hyperparameters()\n    self.kde_vartypes = ''\n    self.vartypes = []\n    for h in hps:\n        if hasattr(h, 'choices'):\n            self.kde_vartypes += 'u'\n            self.vartypes += [len(h.choices)]\n        else:\n            self.kde_vartypes += 'c'\n            self.vartypes += [0]\n    self.vartypes = np.array(self.vartypes, dtype=int)\n    self.cat_probs = []\n    self.configs = dict()\n    self.losses = dict()\n    self.good_config_rankings = dict()\n    self.kde_models = dict()",
            "def __init__(self, configspace, min_points_in_model=None, top_n_percent=15, num_samples=64, random_fraction=1 / 3, bandwidth_factor=3, min_bandwidth=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Fits for each given budget a kernel density estimator on the best N percent of the\\n        evaluated configurations on this budget.\\n\\n\\n        Parameters:\\n        -----------\\n        configspace: ConfigSpace\\n            Configuration space object\\n        top_n_percent: int\\n            Determines the percentile of configurations that will be used as training data\\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\\n            for training.\\n        min_points_in_model: int\\n            minimum number of datapoints needed to fit a model\\n        num_samples: int\\n            number of samples drawn to optimize EI via sampling\\n        random_fraction: float\\n            fraction of random configurations returned\\n        bandwidth_factor: float\\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\\n        min_bandwidth: float\\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\\n        '\n    self.top_n_percent = top_n_percent\n    self.configspace = configspace\n    self.bw_factor = bandwidth_factor\n    self.min_bandwidth = min_bandwidth\n    self.min_points_in_model = min_points_in_model\n    if min_points_in_model is None:\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    if self.min_points_in_model < len(self.configspace.get_hyperparameters()) + 1:\n        logger.warning('Invalid min_points_in_model value. Setting it to %i', len(self.configspace.get_hyperparameters()) + 1)\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    self.num_samples = num_samples\n    self.random_fraction = random_fraction\n    hps = self.configspace.get_hyperparameters()\n    self.kde_vartypes = ''\n    self.vartypes = []\n    for h in hps:\n        if hasattr(h, 'choices'):\n            self.kde_vartypes += 'u'\n            self.vartypes += [len(h.choices)]\n        else:\n            self.kde_vartypes += 'c'\n            self.vartypes += [0]\n    self.vartypes = np.array(self.vartypes, dtype=int)\n    self.cat_probs = []\n    self.configs = dict()\n    self.losses = dict()\n    self.good_config_rankings = dict()\n    self.kde_models = dict()",
            "def __init__(self, configspace, min_points_in_model=None, top_n_percent=15, num_samples=64, random_fraction=1 / 3, bandwidth_factor=3, min_bandwidth=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Fits for each given budget a kernel density estimator on the best N percent of the\\n        evaluated configurations on this budget.\\n\\n\\n        Parameters:\\n        -----------\\n        configspace: ConfigSpace\\n            Configuration space object\\n        top_n_percent: int\\n            Determines the percentile of configurations that will be used as training data\\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\\n            for training.\\n        min_points_in_model: int\\n            minimum number of datapoints needed to fit a model\\n        num_samples: int\\n            number of samples drawn to optimize EI via sampling\\n        random_fraction: float\\n            fraction of random configurations returned\\n        bandwidth_factor: float\\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\\n        min_bandwidth: float\\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\\n        '\n    self.top_n_percent = top_n_percent\n    self.configspace = configspace\n    self.bw_factor = bandwidth_factor\n    self.min_bandwidth = min_bandwidth\n    self.min_points_in_model = min_points_in_model\n    if min_points_in_model is None:\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    if self.min_points_in_model < len(self.configspace.get_hyperparameters()) + 1:\n        logger.warning('Invalid min_points_in_model value. Setting it to %i', len(self.configspace.get_hyperparameters()) + 1)\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    self.num_samples = num_samples\n    self.random_fraction = random_fraction\n    hps = self.configspace.get_hyperparameters()\n    self.kde_vartypes = ''\n    self.vartypes = []\n    for h in hps:\n        if hasattr(h, 'choices'):\n            self.kde_vartypes += 'u'\n            self.vartypes += [len(h.choices)]\n        else:\n            self.kde_vartypes += 'c'\n            self.vartypes += [0]\n    self.vartypes = np.array(self.vartypes, dtype=int)\n    self.cat_probs = []\n    self.configs = dict()\n    self.losses = dict()\n    self.good_config_rankings = dict()\n    self.kde_models = dict()",
            "def __init__(self, configspace, min_points_in_model=None, top_n_percent=15, num_samples=64, random_fraction=1 / 3, bandwidth_factor=3, min_bandwidth=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Fits for each given budget a kernel density estimator on the best N percent of the\\n        evaluated configurations on this budget.\\n\\n\\n        Parameters:\\n        -----------\\n        configspace: ConfigSpace\\n            Configuration space object\\n        top_n_percent: int\\n            Determines the percentile of configurations that will be used as training data\\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\\n            for training.\\n        min_points_in_model: int\\n            minimum number of datapoints needed to fit a model\\n        num_samples: int\\n            number of samples drawn to optimize EI via sampling\\n        random_fraction: float\\n            fraction of random configurations returned\\n        bandwidth_factor: float\\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\\n        min_bandwidth: float\\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\\n        '\n    self.top_n_percent = top_n_percent\n    self.configspace = configspace\n    self.bw_factor = bandwidth_factor\n    self.min_bandwidth = min_bandwidth\n    self.min_points_in_model = min_points_in_model\n    if min_points_in_model is None:\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    if self.min_points_in_model < len(self.configspace.get_hyperparameters()) + 1:\n        logger.warning('Invalid min_points_in_model value. Setting it to %i', len(self.configspace.get_hyperparameters()) + 1)\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    self.num_samples = num_samples\n    self.random_fraction = random_fraction\n    hps = self.configspace.get_hyperparameters()\n    self.kde_vartypes = ''\n    self.vartypes = []\n    for h in hps:\n        if hasattr(h, 'choices'):\n            self.kde_vartypes += 'u'\n            self.vartypes += [len(h.choices)]\n        else:\n            self.kde_vartypes += 'c'\n            self.vartypes += [0]\n    self.vartypes = np.array(self.vartypes, dtype=int)\n    self.cat_probs = []\n    self.configs = dict()\n    self.losses = dict()\n    self.good_config_rankings = dict()\n    self.kde_models = dict()",
            "def __init__(self, configspace, min_points_in_model=None, top_n_percent=15, num_samples=64, random_fraction=1 / 3, bandwidth_factor=3, min_bandwidth=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Fits for each given budget a kernel density estimator on the best N percent of the\\n        evaluated configurations on this budget.\\n\\n\\n        Parameters:\\n        -----------\\n        configspace: ConfigSpace\\n            Configuration space object\\n        top_n_percent: int\\n            Determines the percentile of configurations that will be used as training data\\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\\n            for training.\\n        min_points_in_model: int\\n            minimum number of datapoints needed to fit a model\\n        num_samples: int\\n            number of samples drawn to optimize EI via sampling\\n        random_fraction: float\\n            fraction of random configurations returned\\n        bandwidth_factor: float\\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\\n        min_bandwidth: float\\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\\n        '\n    self.top_n_percent = top_n_percent\n    self.configspace = configspace\n    self.bw_factor = bandwidth_factor\n    self.min_bandwidth = min_bandwidth\n    self.min_points_in_model = min_points_in_model\n    if min_points_in_model is None:\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    if self.min_points_in_model < len(self.configspace.get_hyperparameters()) + 1:\n        logger.warning('Invalid min_points_in_model value. Setting it to %i', len(self.configspace.get_hyperparameters()) + 1)\n        self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n    self.num_samples = num_samples\n    self.random_fraction = random_fraction\n    hps = self.configspace.get_hyperparameters()\n    self.kde_vartypes = ''\n    self.vartypes = []\n    for h in hps:\n        if hasattr(h, 'choices'):\n            self.kde_vartypes += 'u'\n            self.vartypes += [len(h.choices)]\n        else:\n            self.kde_vartypes += 'c'\n            self.vartypes += [0]\n    self.vartypes = np.array(self.vartypes, dtype=int)\n    self.cat_probs = []\n    self.configs = dict()\n    self.losses = dict()\n    self.good_config_rankings = dict()\n    self.kde_models = dict()"
        ]
    },
    {
        "func_name": "largest_budget_with_model",
        "original": "def largest_budget_with_model(self):\n    if not self.kde_models:\n        return -float('inf')\n    return max(self.kde_models.keys())",
        "mutated": [
            "def largest_budget_with_model(self):\n    if False:\n        i = 10\n    if not self.kde_models:\n        return -float('inf')\n    return max(self.kde_models.keys())",
            "def largest_budget_with_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.kde_models:\n        return -float('inf')\n    return max(self.kde_models.keys())",
            "def largest_budget_with_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.kde_models:\n        return -float('inf')\n    return max(self.kde_models.keys())",
            "def largest_budget_with_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.kde_models:\n        return -float('inf')\n    return max(self.kde_models.keys())",
            "def largest_budget_with_model(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.kde_models:\n        return -float('inf')\n    return max(self.kde_models.keys())"
        ]
    },
    {
        "func_name": "sample_from_largest_budget",
        "original": "def sample_from_largest_budget(self, info_dict):\n    \"\"\"We opted for a single multidimensional KDE compared to the\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\n        seperated by budget. This function sample a configuration from\n        largest budget. Firstly we sample \"num_samples\" configurations,\n        then prefer one with the largest l(x)/g(x).\n\n        Parameters:\n        -----------\n        info_dict: dict\n            record the information of this configuration\n\n        Returns\n        -------\n        dict:\n            new configuration named sample\n        dict:\n            info_dict, record the information of this configuration\n        \"\"\"\n    best = np.inf\n    best_vector = None\n    budget = max(self.kde_models.keys())\n    l = self.kde_models[budget]['good'].pdf\n    g = self.kde_models[budget]['bad'].pdf\n    minimize_me = lambda x: max(1e-32, g(x)) / max(l(x), 1e-32)\n    kde_good = self.kde_models[budget]['good']\n    kde_bad = self.kde_models[budget]['bad']\n    for i in range(self.num_samples):\n        idx = np.random.randint(0, len(kde_good.data))\n        datum = kde_good.data[idx]\n        vector = []\n        for (m, bw, t) in zip(datum, kde_good.bw, self.vartypes):\n            bw = max(bw, self.min_bandwidth)\n            if t == 0:\n                bw = self.bw_factor * bw\n                vector.append(sps.truncnorm.rvs(-m / bw, (1 - m) / bw, loc=m, scale=bw))\n            elif np.random.rand() < 1 - bw:\n                vector.append(int(m))\n            else:\n                vector.append(np.random.randint(t))\n        val = minimize_me(vector)\n        if not np.isfinite(val):\n            logger.warning('sampled vector: %s has EI value %s', vector, val)\n            logger.warning('data in the KDEs:\\n%s\\n%s', kde_good.data, kde_bad.data)\n            logger.warning('bandwidth of the KDEs:\\n%s\\n%s', kde_good.bw, kde_bad.bw)\n            logger.warning('l(x) = %s', l(vector))\n            logger.warning('g(x) = %s', g(vector))\n            if np.isfinite(l(vector)):\n                best_vector = vector\n                break\n        if val < best:\n            best = val\n            best_vector = vector\n    if best_vector is None:\n        logger.debug('Sampling based optimization with %i samples failed -> using random configuration', self.num_samples)\n        sample = self.configspace.sample_configuration().get_dictionary()\n        info_dict['model_based_pick'] = False\n    else:\n        logger.debug('best_vector: %s, %s, %s, %s', best_vector, best, l(best_vector), g(best_vector))\n        for (i, _) in enumerate(best_vector):\n            hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n            if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                best_vector[i] = int(np.rint(best_vector[i]))\n        sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample)\n        info_dict['model_based_pick'] = True\n    return (sample, info_dict)",
        "mutated": [
            "def sample_from_largest_budget(self, info_dict):\n    if False:\n        i = 10\n    'We opted for a single multidimensional KDE compared to the\\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\\n        seperated by budget. This function sample a configuration from\\n        largest budget. Firstly we sample \"num_samples\" configurations,\\n        then prefer one with the largest l(x)/g(x).\\n\\n        Parameters:\\n        -----------\\n        info_dict: dict\\n            record the information of this configuration\\n\\n        Returns\\n        -------\\n        dict:\\n            new configuration named sample\\n        dict:\\n            info_dict, record the information of this configuration\\n        '\n    best = np.inf\n    best_vector = None\n    budget = max(self.kde_models.keys())\n    l = self.kde_models[budget]['good'].pdf\n    g = self.kde_models[budget]['bad'].pdf\n    minimize_me = lambda x: max(1e-32, g(x)) / max(l(x), 1e-32)\n    kde_good = self.kde_models[budget]['good']\n    kde_bad = self.kde_models[budget]['bad']\n    for i in range(self.num_samples):\n        idx = np.random.randint(0, len(kde_good.data))\n        datum = kde_good.data[idx]\n        vector = []\n        for (m, bw, t) in zip(datum, kde_good.bw, self.vartypes):\n            bw = max(bw, self.min_bandwidth)\n            if t == 0:\n                bw = self.bw_factor * bw\n                vector.append(sps.truncnorm.rvs(-m / bw, (1 - m) / bw, loc=m, scale=bw))\n            elif np.random.rand() < 1 - bw:\n                vector.append(int(m))\n            else:\n                vector.append(np.random.randint(t))\n        val = minimize_me(vector)\n        if not np.isfinite(val):\n            logger.warning('sampled vector: %s has EI value %s', vector, val)\n            logger.warning('data in the KDEs:\\n%s\\n%s', kde_good.data, kde_bad.data)\n            logger.warning('bandwidth of the KDEs:\\n%s\\n%s', kde_good.bw, kde_bad.bw)\n            logger.warning('l(x) = %s', l(vector))\n            logger.warning('g(x) = %s', g(vector))\n            if np.isfinite(l(vector)):\n                best_vector = vector\n                break\n        if val < best:\n            best = val\n            best_vector = vector\n    if best_vector is None:\n        logger.debug('Sampling based optimization with %i samples failed -> using random configuration', self.num_samples)\n        sample = self.configspace.sample_configuration().get_dictionary()\n        info_dict['model_based_pick'] = False\n    else:\n        logger.debug('best_vector: %s, %s, %s, %s', best_vector, best, l(best_vector), g(best_vector))\n        for (i, _) in enumerate(best_vector):\n            hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n            if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                best_vector[i] = int(np.rint(best_vector[i]))\n        sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample)\n        info_dict['model_based_pick'] = True\n    return (sample, info_dict)",
            "def sample_from_largest_budget(self, info_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'We opted for a single multidimensional KDE compared to the\\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\\n        seperated by budget. This function sample a configuration from\\n        largest budget. Firstly we sample \"num_samples\" configurations,\\n        then prefer one with the largest l(x)/g(x).\\n\\n        Parameters:\\n        -----------\\n        info_dict: dict\\n            record the information of this configuration\\n\\n        Returns\\n        -------\\n        dict:\\n            new configuration named sample\\n        dict:\\n            info_dict, record the information of this configuration\\n        '\n    best = np.inf\n    best_vector = None\n    budget = max(self.kde_models.keys())\n    l = self.kde_models[budget]['good'].pdf\n    g = self.kde_models[budget]['bad'].pdf\n    minimize_me = lambda x: max(1e-32, g(x)) / max(l(x), 1e-32)\n    kde_good = self.kde_models[budget]['good']\n    kde_bad = self.kde_models[budget]['bad']\n    for i in range(self.num_samples):\n        idx = np.random.randint(0, len(kde_good.data))\n        datum = kde_good.data[idx]\n        vector = []\n        for (m, bw, t) in zip(datum, kde_good.bw, self.vartypes):\n            bw = max(bw, self.min_bandwidth)\n            if t == 0:\n                bw = self.bw_factor * bw\n                vector.append(sps.truncnorm.rvs(-m / bw, (1 - m) / bw, loc=m, scale=bw))\n            elif np.random.rand() < 1 - bw:\n                vector.append(int(m))\n            else:\n                vector.append(np.random.randint(t))\n        val = minimize_me(vector)\n        if not np.isfinite(val):\n            logger.warning('sampled vector: %s has EI value %s', vector, val)\n            logger.warning('data in the KDEs:\\n%s\\n%s', kde_good.data, kde_bad.data)\n            logger.warning('bandwidth of the KDEs:\\n%s\\n%s', kde_good.bw, kde_bad.bw)\n            logger.warning('l(x) = %s', l(vector))\n            logger.warning('g(x) = %s', g(vector))\n            if np.isfinite(l(vector)):\n                best_vector = vector\n                break\n        if val < best:\n            best = val\n            best_vector = vector\n    if best_vector is None:\n        logger.debug('Sampling based optimization with %i samples failed -> using random configuration', self.num_samples)\n        sample = self.configspace.sample_configuration().get_dictionary()\n        info_dict['model_based_pick'] = False\n    else:\n        logger.debug('best_vector: %s, %s, %s, %s', best_vector, best, l(best_vector), g(best_vector))\n        for (i, _) in enumerate(best_vector):\n            hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n            if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                best_vector[i] = int(np.rint(best_vector[i]))\n        sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample)\n        info_dict['model_based_pick'] = True\n    return (sample, info_dict)",
            "def sample_from_largest_budget(self, info_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'We opted for a single multidimensional KDE compared to the\\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\\n        seperated by budget. This function sample a configuration from\\n        largest budget. Firstly we sample \"num_samples\" configurations,\\n        then prefer one with the largest l(x)/g(x).\\n\\n        Parameters:\\n        -----------\\n        info_dict: dict\\n            record the information of this configuration\\n\\n        Returns\\n        -------\\n        dict:\\n            new configuration named sample\\n        dict:\\n            info_dict, record the information of this configuration\\n        '\n    best = np.inf\n    best_vector = None\n    budget = max(self.kde_models.keys())\n    l = self.kde_models[budget]['good'].pdf\n    g = self.kde_models[budget]['bad'].pdf\n    minimize_me = lambda x: max(1e-32, g(x)) / max(l(x), 1e-32)\n    kde_good = self.kde_models[budget]['good']\n    kde_bad = self.kde_models[budget]['bad']\n    for i in range(self.num_samples):\n        idx = np.random.randint(0, len(kde_good.data))\n        datum = kde_good.data[idx]\n        vector = []\n        for (m, bw, t) in zip(datum, kde_good.bw, self.vartypes):\n            bw = max(bw, self.min_bandwidth)\n            if t == 0:\n                bw = self.bw_factor * bw\n                vector.append(sps.truncnorm.rvs(-m / bw, (1 - m) / bw, loc=m, scale=bw))\n            elif np.random.rand() < 1 - bw:\n                vector.append(int(m))\n            else:\n                vector.append(np.random.randint(t))\n        val = minimize_me(vector)\n        if not np.isfinite(val):\n            logger.warning('sampled vector: %s has EI value %s', vector, val)\n            logger.warning('data in the KDEs:\\n%s\\n%s', kde_good.data, kde_bad.data)\n            logger.warning('bandwidth of the KDEs:\\n%s\\n%s', kde_good.bw, kde_bad.bw)\n            logger.warning('l(x) = %s', l(vector))\n            logger.warning('g(x) = %s', g(vector))\n            if np.isfinite(l(vector)):\n                best_vector = vector\n                break\n        if val < best:\n            best = val\n            best_vector = vector\n    if best_vector is None:\n        logger.debug('Sampling based optimization with %i samples failed -> using random configuration', self.num_samples)\n        sample = self.configspace.sample_configuration().get_dictionary()\n        info_dict['model_based_pick'] = False\n    else:\n        logger.debug('best_vector: %s, %s, %s, %s', best_vector, best, l(best_vector), g(best_vector))\n        for (i, _) in enumerate(best_vector):\n            hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n            if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                best_vector[i] = int(np.rint(best_vector[i]))\n        sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample)\n        info_dict['model_based_pick'] = True\n    return (sample, info_dict)",
            "def sample_from_largest_budget(self, info_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'We opted for a single multidimensional KDE compared to the\\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\\n        seperated by budget. This function sample a configuration from\\n        largest budget. Firstly we sample \"num_samples\" configurations,\\n        then prefer one with the largest l(x)/g(x).\\n\\n        Parameters:\\n        -----------\\n        info_dict: dict\\n            record the information of this configuration\\n\\n        Returns\\n        -------\\n        dict:\\n            new configuration named sample\\n        dict:\\n            info_dict, record the information of this configuration\\n        '\n    best = np.inf\n    best_vector = None\n    budget = max(self.kde_models.keys())\n    l = self.kde_models[budget]['good'].pdf\n    g = self.kde_models[budget]['bad'].pdf\n    minimize_me = lambda x: max(1e-32, g(x)) / max(l(x), 1e-32)\n    kde_good = self.kde_models[budget]['good']\n    kde_bad = self.kde_models[budget]['bad']\n    for i in range(self.num_samples):\n        idx = np.random.randint(0, len(kde_good.data))\n        datum = kde_good.data[idx]\n        vector = []\n        for (m, bw, t) in zip(datum, kde_good.bw, self.vartypes):\n            bw = max(bw, self.min_bandwidth)\n            if t == 0:\n                bw = self.bw_factor * bw\n                vector.append(sps.truncnorm.rvs(-m / bw, (1 - m) / bw, loc=m, scale=bw))\n            elif np.random.rand() < 1 - bw:\n                vector.append(int(m))\n            else:\n                vector.append(np.random.randint(t))\n        val = minimize_me(vector)\n        if not np.isfinite(val):\n            logger.warning('sampled vector: %s has EI value %s', vector, val)\n            logger.warning('data in the KDEs:\\n%s\\n%s', kde_good.data, kde_bad.data)\n            logger.warning('bandwidth of the KDEs:\\n%s\\n%s', kde_good.bw, kde_bad.bw)\n            logger.warning('l(x) = %s', l(vector))\n            logger.warning('g(x) = %s', g(vector))\n            if np.isfinite(l(vector)):\n                best_vector = vector\n                break\n        if val < best:\n            best = val\n            best_vector = vector\n    if best_vector is None:\n        logger.debug('Sampling based optimization with %i samples failed -> using random configuration', self.num_samples)\n        sample = self.configspace.sample_configuration().get_dictionary()\n        info_dict['model_based_pick'] = False\n    else:\n        logger.debug('best_vector: %s, %s, %s, %s', best_vector, best, l(best_vector), g(best_vector))\n        for (i, _) in enumerate(best_vector):\n            hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n            if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                best_vector[i] = int(np.rint(best_vector[i]))\n        sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample)\n        info_dict['model_based_pick'] = True\n    return (sample, info_dict)",
            "def sample_from_largest_budget(self, info_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'We opted for a single multidimensional KDE compared to the\\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\\n        seperated by budget. This function sample a configuration from\\n        largest budget. Firstly we sample \"num_samples\" configurations,\\n        then prefer one with the largest l(x)/g(x).\\n\\n        Parameters:\\n        -----------\\n        info_dict: dict\\n            record the information of this configuration\\n\\n        Returns\\n        -------\\n        dict:\\n            new configuration named sample\\n        dict:\\n            info_dict, record the information of this configuration\\n        '\n    best = np.inf\n    best_vector = None\n    budget = max(self.kde_models.keys())\n    l = self.kde_models[budget]['good'].pdf\n    g = self.kde_models[budget]['bad'].pdf\n    minimize_me = lambda x: max(1e-32, g(x)) / max(l(x), 1e-32)\n    kde_good = self.kde_models[budget]['good']\n    kde_bad = self.kde_models[budget]['bad']\n    for i in range(self.num_samples):\n        idx = np.random.randint(0, len(kde_good.data))\n        datum = kde_good.data[idx]\n        vector = []\n        for (m, bw, t) in zip(datum, kde_good.bw, self.vartypes):\n            bw = max(bw, self.min_bandwidth)\n            if t == 0:\n                bw = self.bw_factor * bw\n                vector.append(sps.truncnorm.rvs(-m / bw, (1 - m) / bw, loc=m, scale=bw))\n            elif np.random.rand() < 1 - bw:\n                vector.append(int(m))\n            else:\n                vector.append(np.random.randint(t))\n        val = minimize_me(vector)\n        if not np.isfinite(val):\n            logger.warning('sampled vector: %s has EI value %s', vector, val)\n            logger.warning('data in the KDEs:\\n%s\\n%s', kde_good.data, kde_bad.data)\n            logger.warning('bandwidth of the KDEs:\\n%s\\n%s', kde_good.bw, kde_bad.bw)\n            logger.warning('l(x) = %s', l(vector))\n            logger.warning('g(x) = %s', g(vector))\n            if np.isfinite(l(vector)):\n                best_vector = vector\n                break\n        if val < best:\n            best = val\n            best_vector = vector\n    if best_vector is None:\n        logger.debug('Sampling based optimization with %i samples failed -> using random configuration', self.num_samples)\n        sample = self.configspace.sample_configuration().get_dictionary()\n        info_dict['model_based_pick'] = False\n    else:\n        logger.debug('best_vector: %s, %s, %s, %s', best_vector, best, l(best_vector), g(best_vector))\n        for (i, _) in enumerate(best_vector):\n            hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n            if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                best_vector[i] = int(np.rint(best_vector[i]))\n        sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample)\n        info_dict['model_based_pick'] = True\n    return (sample, info_dict)"
        ]
    },
    {
        "func_name": "get_config",
        "original": "def get_config(self, budget):\n    \"\"\"Function to sample a new configuration\n        This function is called inside BOHB to query a new configuration\n\n        Parameters:\n        -----------\n        budget: float\n            the budget for which this configuration is scheduled\n\n        Returns\n        -------\n        config\n            return a valid configuration with parameters and budget\n        \"\"\"\n    logger.debug('start sampling a new configuration.')\n    sample = None\n    info_dict = {}\n    if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n        sample = self.configspace.sample_configuration()\n        info_dict['model_based_pick'] = False\n    if sample is None:\n        (sample, info_dict) = self.sample_from_largest_budget(info_dict)\n    sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample.get_dictionary()).get_dictionary()\n    logger.debug('done sampling a new configuration.')\n    sample['TRIAL_BUDGET'] = budget\n    return sample",
        "mutated": [
            "def get_config(self, budget):\n    if False:\n        i = 10\n    'Function to sample a new configuration\\n        This function is called inside BOHB to query a new configuration\\n\\n        Parameters:\\n        -----------\\n        budget: float\\n            the budget for which this configuration is scheduled\\n\\n        Returns\\n        -------\\n        config\\n            return a valid configuration with parameters and budget\\n        '\n    logger.debug('start sampling a new configuration.')\n    sample = None\n    info_dict = {}\n    if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n        sample = self.configspace.sample_configuration()\n        info_dict['model_based_pick'] = False\n    if sample is None:\n        (sample, info_dict) = self.sample_from_largest_budget(info_dict)\n    sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample.get_dictionary()).get_dictionary()\n    logger.debug('done sampling a new configuration.')\n    sample['TRIAL_BUDGET'] = budget\n    return sample",
            "def get_config(self, budget):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Function to sample a new configuration\\n        This function is called inside BOHB to query a new configuration\\n\\n        Parameters:\\n        -----------\\n        budget: float\\n            the budget for which this configuration is scheduled\\n\\n        Returns\\n        -------\\n        config\\n            return a valid configuration with parameters and budget\\n        '\n    logger.debug('start sampling a new configuration.')\n    sample = None\n    info_dict = {}\n    if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n        sample = self.configspace.sample_configuration()\n        info_dict['model_based_pick'] = False\n    if sample is None:\n        (sample, info_dict) = self.sample_from_largest_budget(info_dict)\n    sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample.get_dictionary()).get_dictionary()\n    logger.debug('done sampling a new configuration.')\n    sample['TRIAL_BUDGET'] = budget\n    return sample",
            "def get_config(self, budget):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Function to sample a new configuration\\n        This function is called inside BOHB to query a new configuration\\n\\n        Parameters:\\n        -----------\\n        budget: float\\n            the budget for which this configuration is scheduled\\n\\n        Returns\\n        -------\\n        config\\n            return a valid configuration with parameters and budget\\n        '\n    logger.debug('start sampling a new configuration.')\n    sample = None\n    info_dict = {}\n    if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n        sample = self.configspace.sample_configuration()\n        info_dict['model_based_pick'] = False\n    if sample is None:\n        (sample, info_dict) = self.sample_from_largest_budget(info_dict)\n    sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample.get_dictionary()).get_dictionary()\n    logger.debug('done sampling a new configuration.')\n    sample['TRIAL_BUDGET'] = budget\n    return sample",
            "def get_config(self, budget):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Function to sample a new configuration\\n        This function is called inside BOHB to query a new configuration\\n\\n        Parameters:\\n        -----------\\n        budget: float\\n            the budget for which this configuration is scheduled\\n\\n        Returns\\n        -------\\n        config\\n            return a valid configuration with parameters and budget\\n        '\n    logger.debug('start sampling a new configuration.')\n    sample = None\n    info_dict = {}\n    if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n        sample = self.configspace.sample_configuration()\n        info_dict['model_based_pick'] = False\n    if sample is None:\n        (sample, info_dict) = self.sample_from_largest_budget(info_dict)\n    sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample.get_dictionary()).get_dictionary()\n    logger.debug('done sampling a new configuration.')\n    sample['TRIAL_BUDGET'] = budget\n    return sample",
            "def get_config(self, budget):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Function to sample a new configuration\\n        This function is called inside BOHB to query a new configuration\\n\\n        Parameters:\\n        -----------\\n        budget: float\\n            the budget for which this configuration is scheduled\\n\\n        Returns\\n        -------\\n        config\\n            return a valid configuration with parameters and budget\\n        '\n    logger.debug('start sampling a new configuration.')\n    sample = None\n    info_dict = {}\n    if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n        sample = self.configspace.sample_configuration()\n        info_dict['model_based_pick'] = False\n    if sample is None:\n        (sample, info_dict) = self.sample_from_largest_budget(info_dict)\n    sample = ConfigSpace.util.deactivate_inactive_hyperparameters(configuration_space=self.configspace, configuration=sample.get_dictionary()).get_dictionary()\n    logger.debug('done sampling a new configuration.')\n    sample['TRIAL_BUDGET'] = budget\n    return sample"
        ]
    },
    {
        "func_name": "impute_conditional_data",
        "original": "def impute_conditional_data(self, array):\n    return_array = np.zeros(array.shape)\n    for i in range(array.shape[0]):\n        datum = np.copy(array[i])\n        nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        while np.any(nan_indices):\n            nan_idx = nan_indices[0]\n            valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n            if len(valid_indices) > 0:\n                row_idx = np.random.choice(valid_indices)\n                datum[nan_indices] = array[row_idx, nan_indices]\n            else:\n                t = self.vartypes[nan_idx]\n                if t == 0:\n                    datum[nan_idx] = np.random.rand()\n                else:\n                    datum[nan_idx] = np.random.randint(t)\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        return_array[i, :] = datum\n    return return_array",
        "mutated": [
            "def impute_conditional_data(self, array):\n    if False:\n        i = 10\n    return_array = np.zeros(array.shape)\n    for i in range(array.shape[0]):\n        datum = np.copy(array[i])\n        nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        while np.any(nan_indices):\n            nan_idx = nan_indices[0]\n            valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n            if len(valid_indices) > 0:\n                row_idx = np.random.choice(valid_indices)\n                datum[nan_indices] = array[row_idx, nan_indices]\n            else:\n                t = self.vartypes[nan_idx]\n                if t == 0:\n                    datum[nan_idx] = np.random.rand()\n                else:\n                    datum[nan_idx] = np.random.randint(t)\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        return_array[i, :] = datum\n    return return_array",
            "def impute_conditional_data(self, array):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return_array = np.zeros(array.shape)\n    for i in range(array.shape[0]):\n        datum = np.copy(array[i])\n        nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        while np.any(nan_indices):\n            nan_idx = nan_indices[0]\n            valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n            if len(valid_indices) > 0:\n                row_idx = np.random.choice(valid_indices)\n                datum[nan_indices] = array[row_idx, nan_indices]\n            else:\n                t = self.vartypes[nan_idx]\n                if t == 0:\n                    datum[nan_idx] = np.random.rand()\n                else:\n                    datum[nan_idx] = np.random.randint(t)\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        return_array[i, :] = datum\n    return return_array",
            "def impute_conditional_data(self, array):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return_array = np.zeros(array.shape)\n    for i in range(array.shape[0]):\n        datum = np.copy(array[i])\n        nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        while np.any(nan_indices):\n            nan_idx = nan_indices[0]\n            valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n            if len(valid_indices) > 0:\n                row_idx = np.random.choice(valid_indices)\n                datum[nan_indices] = array[row_idx, nan_indices]\n            else:\n                t = self.vartypes[nan_idx]\n                if t == 0:\n                    datum[nan_idx] = np.random.rand()\n                else:\n                    datum[nan_idx] = np.random.randint(t)\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        return_array[i, :] = datum\n    return return_array",
            "def impute_conditional_data(self, array):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return_array = np.zeros(array.shape)\n    for i in range(array.shape[0]):\n        datum = np.copy(array[i])\n        nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        while np.any(nan_indices):\n            nan_idx = nan_indices[0]\n            valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n            if len(valid_indices) > 0:\n                row_idx = np.random.choice(valid_indices)\n                datum[nan_indices] = array[row_idx, nan_indices]\n            else:\n                t = self.vartypes[nan_idx]\n                if t == 0:\n                    datum[nan_idx] = np.random.rand()\n                else:\n                    datum[nan_idx] = np.random.randint(t)\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        return_array[i, :] = datum\n    return return_array",
            "def impute_conditional_data(self, array):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return_array = np.zeros(array.shape)\n    for i in range(array.shape[0]):\n        datum = np.copy(array[i])\n        nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        while np.any(nan_indices):\n            nan_idx = nan_indices[0]\n            valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n            if len(valid_indices) > 0:\n                row_idx = np.random.choice(valid_indices)\n                datum[nan_indices] = array[row_idx, nan_indices]\n            else:\n                t = self.vartypes[nan_idx]\n                if t == 0:\n                    datum[nan_idx] = np.random.rand()\n                else:\n                    datum[nan_idx] = np.random.randint(t)\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        return_array[i, :] = datum\n    return return_array"
        ]
    },
    {
        "func_name": "new_result",
        "original": "def new_result(self, loss, budget, parameters, update_model=True):\n    \"\"\"\n        Function to register finished runs. Every time a run has finished, this function should be called\n        to register it with the loss.\n\n        Parameters:\n        -----------\n        loss: float\n            the loss of the parameters\n        budget: float\n            the budget of the parameters\n        parameters: dict\n            the parameters of this trial\n        update_model: bool\n            whether use this parameter to update BP model\n\n        Returns\n        -------\n        None\n        \"\"\"\n    if loss is None:\n        loss = np.inf\n    if budget not in self.configs.keys():\n        self.configs[budget] = []\n        self.losses[budget] = []\n    if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n        return\n    conf = ConfigSpace.Configuration(self.configspace, parameters)\n    self.configs[budget].append(conf.get_array())\n    self.losses[budget].append(loss)\n    if len(self.configs[budget]) <= self.min_points_in_model - 1:\n        logger.debug(\"Only %i run(s) for budget %f available, need more than %s             -> can't build model!\", len(self.configs[budget]), budget, self.min_points_in_model + 1)\n        return\n    if not update_model:\n        return\n    train_configs = np.array(self.configs[budget])\n    train_losses = np.array(self.losses[budget])\n    n_good = max(self.min_points_in_model, self.top_n_percent * train_configs.shape[0] // 100)\n    n_bad = max(self.min_points_in_model, (100 - self.top_n_percent) * train_configs.shape[0] // 100)\n    idx = np.argsort(train_losses)\n    train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n    train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good + n_bad]])\n    if train_data_good.shape[0] <= train_data_good.shape[1]:\n        return\n    if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n        return\n    bw_estimation = 'normal_reference'\n    bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n    good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n    bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n    good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n    self.kde_models[budget] = {'good': good_kde, 'bad': bad_kde}\n    logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n', budget, n_good, n_bad, np.min(train_losses))",
        "mutated": [
            "def new_result(self, loss, budget, parameters, update_model=True):\n    if False:\n        i = 10\n    '\\n        Function to register finished runs. Every time a run has finished, this function should be called\\n        to register it with the loss.\\n\\n        Parameters:\\n        -----------\\n        loss: float\\n            the loss of the parameters\\n        budget: float\\n            the budget of the parameters\\n        parameters: dict\\n            the parameters of this trial\\n        update_model: bool\\n            whether use this parameter to update BP model\\n\\n        Returns\\n        -------\\n        None\\n        '\n    if loss is None:\n        loss = np.inf\n    if budget not in self.configs.keys():\n        self.configs[budget] = []\n        self.losses[budget] = []\n    if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n        return\n    conf = ConfigSpace.Configuration(self.configspace, parameters)\n    self.configs[budget].append(conf.get_array())\n    self.losses[budget].append(loss)\n    if len(self.configs[budget]) <= self.min_points_in_model - 1:\n        logger.debug(\"Only %i run(s) for budget %f available, need more than %s             -> can't build model!\", len(self.configs[budget]), budget, self.min_points_in_model + 1)\n        return\n    if not update_model:\n        return\n    train_configs = np.array(self.configs[budget])\n    train_losses = np.array(self.losses[budget])\n    n_good = max(self.min_points_in_model, self.top_n_percent * train_configs.shape[0] // 100)\n    n_bad = max(self.min_points_in_model, (100 - self.top_n_percent) * train_configs.shape[0] // 100)\n    idx = np.argsort(train_losses)\n    train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n    train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good + n_bad]])\n    if train_data_good.shape[0] <= train_data_good.shape[1]:\n        return\n    if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n        return\n    bw_estimation = 'normal_reference'\n    bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n    good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n    bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n    good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n    self.kde_models[budget] = {'good': good_kde, 'bad': bad_kde}\n    logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n', budget, n_good, n_bad, np.min(train_losses))",
            "def new_result(self, loss, budget, parameters, update_model=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Function to register finished runs. Every time a run has finished, this function should be called\\n        to register it with the loss.\\n\\n        Parameters:\\n        -----------\\n        loss: float\\n            the loss of the parameters\\n        budget: float\\n            the budget of the parameters\\n        parameters: dict\\n            the parameters of this trial\\n        update_model: bool\\n            whether use this parameter to update BP model\\n\\n        Returns\\n        -------\\n        None\\n        '\n    if loss is None:\n        loss = np.inf\n    if budget not in self.configs.keys():\n        self.configs[budget] = []\n        self.losses[budget] = []\n    if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n        return\n    conf = ConfigSpace.Configuration(self.configspace, parameters)\n    self.configs[budget].append(conf.get_array())\n    self.losses[budget].append(loss)\n    if len(self.configs[budget]) <= self.min_points_in_model - 1:\n        logger.debug(\"Only %i run(s) for budget %f available, need more than %s             -> can't build model!\", len(self.configs[budget]), budget, self.min_points_in_model + 1)\n        return\n    if not update_model:\n        return\n    train_configs = np.array(self.configs[budget])\n    train_losses = np.array(self.losses[budget])\n    n_good = max(self.min_points_in_model, self.top_n_percent * train_configs.shape[0] // 100)\n    n_bad = max(self.min_points_in_model, (100 - self.top_n_percent) * train_configs.shape[0] // 100)\n    idx = np.argsort(train_losses)\n    train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n    train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good + n_bad]])\n    if train_data_good.shape[0] <= train_data_good.shape[1]:\n        return\n    if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n        return\n    bw_estimation = 'normal_reference'\n    bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n    good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n    bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n    good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n    self.kde_models[budget] = {'good': good_kde, 'bad': bad_kde}\n    logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n', budget, n_good, n_bad, np.min(train_losses))",
            "def new_result(self, loss, budget, parameters, update_model=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Function to register finished runs. Every time a run has finished, this function should be called\\n        to register it with the loss.\\n\\n        Parameters:\\n        -----------\\n        loss: float\\n            the loss of the parameters\\n        budget: float\\n            the budget of the parameters\\n        parameters: dict\\n            the parameters of this trial\\n        update_model: bool\\n            whether use this parameter to update BP model\\n\\n        Returns\\n        -------\\n        None\\n        '\n    if loss is None:\n        loss = np.inf\n    if budget not in self.configs.keys():\n        self.configs[budget] = []\n        self.losses[budget] = []\n    if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n        return\n    conf = ConfigSpace.Configuration(self.configspace, parameters)\n    self.configs[budget].append(conf.get_array())\n    self.losses[budget].append(loss)\n    if len(self.configs[budget]) <= self.min_points_in_model - 1:\n        logger.debug(\"Only %i run(s) for budget %f available, need more than %s             -> can't build model!\", len(self.configs[budget]), budget, self.min_points_in_model + 1)\n        return\n    if not update_model:\n        return\n    train_configs = np.array(self.configs[budget])\n    train_losses = np.array(self.losses[budget])\n    n_good = max(self.min_points_in_model, self.top_n_percent * train_configs.shape[0] // 100)\n    n_bad = max(self.min_points_in_model, (100 - self.top_n_percent) * train_configs.shape[0] // 100)\n    idx = np.argsort(train_losses)\n    train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n    train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good + n_bad]])\n    if train_data_good.shape[0] <= train_data_good.shape[1]:\n        return\n    if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n        return\n    bw_estimation = 'normal_reference'\n    bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n    good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n    bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n    good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n    self.kde_models[budget] = {'good': good_kde, 'bad': bad_kde}\n    logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n', budget, n_good, n_bad, np.min(train_losses))",
            "def new_result(self, loss, budget, parameters, update_model=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Function to register finished runs. Every time a run has finished, this function should be called\\n        to register it with the loss.\\n\\n        Parameters:\\n        -----------\\n        loss: float\\n            the loss of the parameters\\n        budget: float\\n            the budget of the parameters\\n        parameters: dict\\n            the parameters of this trial\\n        update_model: bool\\n            whether use this parameter to update BP model\\n\\n        Returns\\n        -------\\n        None\\n        '\n    if loss is None:\n        loss = np.inf\n    if budget not in self.configs.keys():\n        self.configs[budget] = []\n        self.losses[budget] = []\n    if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n        return\n    conf = ConfigSpace.Configuration(self.configspace, parameters)\n    self.configs[budget].append(conf.get_array())\n    self.losses[budget].append(loss)\n    if len(self.configs[budget]) <= self.min_points_in_model - 1:\n        logger.debug(\"Only %i run(s) for budget %f available, need more than %s             -> can't build model!\", len(self.configs[budget]), budget, self.min_points_in_model + 1)\n        return\n    if not update_model:\n        return\n    train_configs = np.array(self.configs[budget])\n    train_losses = np.array(self.losses[budget])\n    n_good = max(self.min_points_in_model, self.top_n_percent * train_configs.shape[0] // 100)\n    n_bad = max(self.min_points_in_model, (100 - self.top_n_percent) * train_configs.shape[0] // 100)\n    idx = np.argsort(train_losses)\n    train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n    train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good + n_bad]])\n    if train_data_good.shape[0] <= train_data_good.shape[1]:\n        return\n    if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n        return\n    bw_estimation = 'normal_reference'\n    bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n    good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n    bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n    good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n    self.kde_models[budget] = {'good': good_kde, 'bad': bad_kde}\n    logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n', budget, n_good, n_bad, np.min(train_losses))",
            "def new_result(self, loss, budget, parameters, update_model=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Function to register finished runs. Every time a run has finished, this function should be called\\n        to register it with the loss.\\n\\n        Parameters:\\n        -----------\\n        loss: float\\n            the loss of the parameters\\n        budget: float\\n            the budget of the parameters\\n        parameters: dict\\n            the parameters of this trial\\n        update_model: bool\\n            whether use this parameter to update BP model\\n\\n        Returns\\n        -------\\n        None\\n        '\n    if loss is None:\n        loss = np.inf\n    if budget not in self.configs.keys():\n        self.configs[budget] = []\n        self.losses[budget] = []\n    if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n        return\n    conf = ConfigSpace.Configuration(self.configspace, parameters)\n    self.configs[budget].append(conf.get_array())\n    self.losses[budget].append(loss)\n    if len(self.configs[budget]) <= self.min_points_in_model - 1:\n        logger.debug(\"Only %i run(s) for budget %f available, need more than %s             -> can't build model!\", len(self.configs[budget]), budget, self.min_points_in_model + 1)\n        return\n    if not update_model:\n        return\n    train_configs = np.array(self.configs[budget])\n    train_losses = np.array(self.losses[budget])\n    n_good = max(self.min_points_in_model, self.top_n_percent * train_configs.shape[0] // 100)\n    n_bad = max(self.min_points_in_model, (100 - self.top_n_percent) * train_configs.shape[0] // 100)\n    idx = np.argsort(train_losses)\n    train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n    train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good + n_bad]])\n    if train_data_good.shape[0] <= train_data_good.shape[1]:\n        return\n    if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n        return\n    bw_estimation = 'normal_reference'\n    bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n    good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n    bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n    good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n    self.kde_models[budget] = {'good': good_kde, 'bad': bad_kde}\n    logger.debug('done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n', budget, n_good, n_bad, np.min(train_losses))"
        ]
    }
]