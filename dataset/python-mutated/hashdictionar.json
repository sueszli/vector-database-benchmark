[
    {
        "func_name": "__init__",
        "original": "def __init__(self, documents=None, id_range=32000, myhash=zlib.adler32, debug=True):\n    \"\"\"\n\n        Parameters\n        ----------\n        documents : iterable of iterable of str, optional\n            Iterable of documents. If given, used to collect additional corpus statistics.\n            :class:`~gensim.corpora.hashdictionary.HashDictionary` can work\n            without these statistics (optional parameter).\n        id_range : int, optional\n            Number of hash-values in table, used as `id = myhash(key) %% id_range`.\n        myhash : function, optional\n            Hash function, should support interface `myhash(str) -> int`, uses `zlib.adler32` by default.\n        debug : bool, optional\n            Store which tokens have mapped to a given id? **Will use a lot of RAM**.\n            If you find yourself running out of memory (or not sure that you really need raw tokens),\n            keep `debug=False`.\n\n        \"\"\"\n    self.myhash = myhash\n    self.id_range = id_range\n    self.debug = debug\n    self.token2id = {}\n    self.id2token = {}\n    self.dfs = {}\n    self.dfs_debug = {}\n    self.num_docs = 0\n    self.num_pos = 0\n    self.num_nnz = 0\n    self.allow_update = True\n    if documents is not None:\n        self.add_documents(documents)",
        "mutated": [
            "def __init__(self, documents=None, id_range=32000, myhash=zlib.adler32, debug=True):\n    if False:\n        i = 10\n    '\\n\\n        Parameters\\n        ----------\\n        documents : iterable of iterable of str, optional\\n            Iterable of documents. If given, used to collect additional corpus statistics.\\n            :class:`~gensim.corpora.hashdictionary.HashDictionary` can work\\n            without these statistics (optional parameter).\\n        id_range : int, optional\\n            Number of hash-values in table, used as `id = myhash(key) %% id_range`.\\n        myhash : function, optional\\n            Hash function, should support interface `myhash(str) -> int`, uses `zlib.adler32` by default.\\n        debug : bool, optional\\n            Store which tokens have mapped to a given id? **Will use a lot of RAM**.\\n            If you find yourself running out of memory (or not sure that you really need raw tokens),\\n            keep `debug=False`.\\n\\n        '\n    self.myhash = myhash\n    self.id_range = id_range\n    self.debug = debug\n    self.token2id = {}\n    self.id2token = {}\n    self.dfs = {}\n    self.dfs_debug = {}\n    self.num_docs = 0\n    self.num_pos = 0\n    self.num_nnz = 0\n    self.allow_update = True\n    if documents is not None:\n        self.add_documents(documents)",
            "def __init__(self, documents=None, id_range=32000, myhash=zlib.adler32, debug=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n        Parameters\\n        ----------\\n        documents : iterable of iterable of str, optional\\n            Iterable of documents. If given, used to collect additional corpus statistics.\\n            :class:`~gensim.corpora.hashdictionary.HashDictionary` can work\\n            without these statistics (optional parameter).\\n        id_range : int, optional\\n            Number of hash-values in table, used as `id = myhash(key) %% id_range`.\\n        myhash : function, optional\\n            Hash function, should support interface `myhash(str) -> int`, uses `zlib.adler32` by default.\\n        debug : bool, optional\\n            Store which tokens have mapped to a given id? **Will use a lot of RAM**.\\n            If you find yourself running out of memory (or not sure that you really need raw tokens),\\n            keep `debug=False`.\\n\\n        '\n    self.myhash = myhash\n    self.id_range = id_range\n    self.debug = debug\n    self.token2id = {}\n    self.id2token = {}\n    self.dfs = {}\n    self.dfs_debug = {}\n    self.num_docs = 0\n    self.num_pos = 0\n    self.num_nnz = 0\n    self.allow_update = True\n    if documents is not None:\n        self.add_documents(documents)",
            "def __init__(self, documents=None, id_range=32000, myhash=zlib.adler32, debug=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n        Parameters\\n        ----------\\n        documents : iterable of iterable of str, optional\\n            Iterable of documents. If given, used to collect additional corpus statistics.\\n            :class:`~gensim.corpora.hashdictionary.HashDictionary` can work\\n            without these statistics (optional parameter).\\n        id_range : int, optional\\n            Number of hash-values in table, used as `id = myhash(key) %% id_range`.\\n        myhash : function, optional\\n            Hash function, should support interface `myhash(str) -> int`, uses `zlib.adler32` by default.\\n        debug : bool, optional\\n            Store which tokens have mapped to a given id? **Will use a lot of RAM**.\\n            If you find yourself running out of memory (or not sure that you really need raw tokens),\\n            keep `debug=False`.\\n\\n        '\n    self.myhash = myhash\n    self.id_range = id_range\n    self.debug = debug\n    self.token2id = {}\n    self.id2token = {}\n    self.dfs = {}\n    self.dfs_debug = {}\n    self.num_docs = 0\n    self.num_pos = 0\n    self.num_nnz = 0\n    self.allow_update = True\n    if documents is not None:\n        self.add_documents(documents)",
            "def __init__(self, documents=None, id_range=32000, myhash=zlib.adler32, debug=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n        Parameters\\n        ----------\\n        documents : iterable of iterable of str, optional\\n            Iterable of documents. If given, used to collect additional corpus statistics.\\n            :class:`~gensim.corpora.hashdictionary.HashDictionary` can work\\n            without these statistics (optional parameter).\\n        id_range : int, optional\\n            Number of hash-values in table, used as `id = myhash(key) %% id_range`.\\n        myhash : function, optional\\n            Hash function, should support interface `myhash(str) -> int`, uses `zlib.adler32` by default.\\n        debug : bool, optional\\n            Store which tokens have mapped to a given id? **Will use a lot of RAM**.\\n            If you find yourself running out of memory (or not sure that you really need raw tokens),\\n            keep `debug=False`.\\n\\n        '\n    self.myhash = myhash\n    self.id_range = id_range\n    self.debug = debug\n    self.token2id = {}\n    self.id2token = {}\n    self.dfs = {}\n    self.dfs_debug = {}\n    self.num_docs = 0\n    self.num_pos = 0\n    self.num_nnz = 0\n    self.allow_update = True\n    if documents is not None:\n        self.add_documents(documents)",
            "def __init__(self, documents=None, id_range=32000, myhash=zlib.adler32, debug=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n        Parameters\\n        ----------\\n        documents : iterable of iterable of str, optional\\n            Iterable of documents. If given, used to collect additional corpus statistics.\\n            :class:`~gensim.corpora.hashdictionary.HashDictionary` can work\\n            without these statistics (optional parameter).\\n        id_range : int, optional\\n            Number of hash-values in table, used as `id = myhash(key) %% id_range`.\\n        myhash : function, optional\\n            Hash function, should support interface `myhash(str) -> int`, uses `zlib.adler32` by default.\\n        debug : bool, optional\\n            Store which tokens have mapped to a given id? **Will use a lot of RAM**.\\n            If you find yourself running out of memory (or not sure that you really need raw tokens),\\n            keep `debug=False`.\\n\\n        '\n    self.myhash = myhash\n    self.id_range = id_range\n    self.debug = debug\n    self.token2id = {}\n    self.id2token = {}\n    self.dfs = {}\n    self.dfs_debug = {}\n    self.num_docs = 0\n    self.num_pos = 0\n    self.num_nnz = 0\n    self.allow_update = True\n    if documents is not None:\n        self.add_documents(documents)"
        ]
    },
    {
        "func_name": "__getitem__",
        "original": "def __getitem__(self, tokenid):\n    \"\"\"Get all words that have mapped to the given id so far, as a set.\n\n        Warnings\n        --------\n        Works only if you initialized your :class:`~gensim.corpora.hashdictionary.HashDictionary` object\n        with `debug=True`.\n\n        Parameters\n        ----------\n        tokenid : int\n            Token identifier (result of hashing).\n\n        Return\n        ------\n        set of str\n            Set of all words that have mapped to this id.\n\n        \"\"\"\n    return self.id2token.get(tokenid, set())",
        "mutated": [
            "def __getitem__(self, tokenid):\n    if False:\n        i = 10\n    'Get all words that have mapped to the given id so far, as a set.\\n\\n        Warnings\\n        --------\\n        Works only if you initialized your :class:`~gensim.corpora.hashdictionary.HashDictionary` object\\n        with `debug=True`.\\n\\n        Parameters\\n        ----------\\n        tokenid : int\\n            Token identifier (result of hashing).\\n\\n        Return\\n        ------\\n        set of str\\n            Set of all words that have mapped to this id.\\n\\n        '\n    return self.id2token.get(tokenid, set())",
            "def __getitem__(self, tokenid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get all words that have mapped to the given id so far, as a set.\\n\\n        Warnings\\n        --------\\n        Works only if you initialized your :class:`~gensim.corpora.hashdictionary.HashDictionary` object\\n        with `debug=True`.\\n\\n        Parameters\\n        ----------\\n        tokenid : int\\n            Token identifier (result of hashing).\\n\\n        Return\\n        ------\\n        set of str\\n            Set of all words that have mapped to this id.\\n\\n        '\n    return self.id2token.get(tokenid, set())",
            "def __getitem__(self, tokenid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get all words that have mapped to the given id so far, as a set.\\n\\n        Warnings\\n        --------\\n        Works only if you initialized your :class:`~gensim.corpora.hashdictionary.HashDictionary` object\\n        with `debug=True`.\\n\\n        Parameters\\n        ----------\\n        tokenid : int\\n            Token identifier (result of hashing).\\n\\n        Return\\n        ------\\n        set of str\\n            Set of all words that have mapped to this id.\\n\\n        '\n    return self.id2token.get(tokenid, set())",
            "def __getitem__(self, tokenid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get all words that have mapped to the given id so far, as a set.\\n\\n        Warnings\\n        --------\\n        Works only if you initialized your :class:`~gensim.corpora.hashdictionary.HashDictionary` object\\n        with `debug=True`.\\n\\n        Parameters\\n        ----------\\n        tokenid : int\\n            Token identifier (result of hashing).\\n\\n        Return\\n        ------\\n        set of str\\n            Set of all words that have mapped to this id.\\n\\n        '\n    return self.id2token.get(tokenid, set())",
            "def __getitem__(self, tokenid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get all words that have mapped to the given id so far, as a set.\\n\\n        Warnings\\n        --------\\n        Works only if you initialized your :class:`~gensim.corpora.hashdictionary.HashDictionary` object\\n        with `debug=True`.\\n\\n        Parameters\\n        ----------\\n        tokenid : int\\n            Token identifier (result of hashing).\\n\\n        Return\\n        ------\\n        set of str\\n            Set of all words that have mapped to this id.\\n\\n        '\n    return self.id2token.get(tokenid, set())"
        ]
    },
    {
        "func_name": "restricted_hash",
        "original": "def restricted_hash(self, token):\n    \"\"\"Calculate id of the given token.\n        Also keep track of what words were mapped to what ids, if `debug=True` was set in the constructor.\n\n        Parameters\n        ----------\n        token : str\n            Input token.\n\n        Return\n        ------\n        int\n            Hash value of `token`.\n\n        \"\"\"\n    h = self.myhash(utils.to_utf8(token)) % self.id_range\n    if self.debug:\n        self.token2id[token] = h\n        self.id2token.setdefault(h, set()).add(token)\n    return h",
        "mutated": [
            "def restricted_hash(self, token):\n    if False:\n        i = 10\n    'Calculate id of the given token.\\n        Also keep track of what words were mapped to what ids, if `debug=True` was set in the constructor.\\n\\n        Parameters\\n        ----------\\n        token : str\\n            Input token.\\n\\n        Return\\n        ------\\n        int\\n            Hash value of `token`.\\n\\n        '\n    h = self.myhash(utils.to_utf8(token)) % self.id_range\n    if self.debug:\n        self.token2id[token] = h\n        self.id2token.setdefault(h, set()).add(token)\n    return h",
            "def restricted_hash(self, token):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate id of the given token.\\n        Also keep track of what words were mapped to what ids, if `debug=True` was set in the constructor.\\n\\n        Parameters\\n        ----------\\n        token : str\\n            Input token.\\n\\n        Return\\n        ------\\n        int\\n            Hash value of `token`.\\n\\n        '\n    h = self.myhash(utils.to_utf8(token)) % self.id_range\n    if self.debug:\n        self.token2id[token] = h\n        self.id2token.setdefault(h, set()).add(token)\n    return h",
            "def restricted_hash(self, token):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate id of the given token.\\n        Also keep track of what words were mapped to what ids, if `debug=True` was set in the constructor.\\n\\n        Parameters\\n        ----------\\n        token : str\\n            Input token.\\n\\n        Return\\n        ------\\n        int\\n            Hash value of `token`.\\n\\n        '\n    h = self.myhash(utils.to_utf8(token)) % self.id_range\n    if self.debug:\n        self.token2id[token] = h\n        self.id2token.setdefault(h, set()).add(token)\n    return h",
            "def restricted_hash(self, token):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate id of the given token.\\n        Also keep track of what words were mapped to what ids, if `debug=True` was set in the constructor.\\n\\n        Parameters\\n        ----------\\n        token : str\\n            Input token.\\n\\n        Return\\n        ------\\n        int\\n            Hash value of `token`.\\n\\n        '\n    h = self.myhash(utils.to_utf8(token)) % self.id_range\n    if self.debug:\n        self.token2id[token] = h\n        self.id2token.setdefault(h, set()).add(token)\n    return h",
            "def restricted_hash(self, token):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate id of the given token.\\n        Also keep track of what words were mapped to what ids, if `debug=True` was set in the constructor.\\n\\n        Parameters\\n        ----------\\n        token : str\\n            Input token.\\n\\n        Return\\n        ------\\n        int\\n            Hash value of `token`.\\n\\n        '\n    h = self.myhash(utils.to_utf8(token)) % self.id_range\n    if self.debug:\n        self.token2id[token] = h\n        self.id2token.setdefault(h, set()).add(token)\n    return h"
        ]
    },
    {
        "func_name": "__len__",
        "original": "def __len__(self):\n    \"\"\"Get the number of distinct ids = the entire dictionary size.\"\"\"\n    return self.id_range",
        "mutated": [
            "def __len__(self):\n    if False:\n        i = 10\n    'Get the number of distinct ids = the entire dictionary size.'\n    return self.id_range",
            "def __len__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the number of distinct ids = the entire dictionary size.'\n    return self.id_range",
            "def __len__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the number of distinct ids = the entire dictionary size.'\n    return self.id_range",
            "def __len__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the number of distinct ids = the entire dictionary size.'\n    return self.id_range",
            "def __len__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the number of distinct ids = the entire dictionary size.'\n    return self.id_range"
        ]
    },
    {
        "func_name": "keys",
        "original": "def keys(self):\n    \"\"\"Get a list of all token ids.\"\"\"\n    return range(len(self))",
        "mutated": [
            "def keys(self):\n    if False:\n        i = 10\n    'Get a list of all token ids.'\n    return range(len(self))",
            "def keys(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get a list of all token ids.'\n    return range(len(self))",
            "def keys(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get a list of all token ids.'\n    return range(len(self))",
            "def keys(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get a list of all token ids.'\n    return range(len(self))",
            "def keys(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get a list of all token ids.'\n    return range(len(self))"
        ]
    },
    {
        "func_name": "__str__",
        "original": "def __str__(self):\n    return 'HashDictionary(%i id range)' % len(self)",
        "mutated": [
            "def __str__(self):\n    if False:\n        i = 10\n    return 'HashDictionary(%i id range)' % len(self)",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 'HashDictionary(%i id range)' % len(self)",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 'HashDictionary(%i id range)' % len(self)",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 'HashDictionary(%i id range)' % len(self)",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 'HashDictionary(%i id range)' % len(self)"
        ]
    },
    {
        "func_name": "from_documents",
        "original": "@staticmethod\ndef from_documents(*args, **kwargs):\n    return HashDictionary(*args, **kwargs)",
        "mutated": [
            "@staticmethod\ndef from_documents(*args, **kwargs):\n    if False:\n        i = 10\n    return HashDictionary(*args, **kwargs)",
            "@staticmethod\ndef from_documents(*args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return HashDictionary(*args, **kwargs)",
            "@staticmethod\ndef from_documents(*args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return HashDictionary(*args, **kwargs)",
            "@staticmethod\ndef from_documents(*args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return HashDictionary(*args, **kwargs)",
            "@staticmethod\ndef from_documents(*args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return HashDictionary(*args, **kwargs)"
        ]
    },
    {
        "func_name": "add_documents",
        "original": "def add_documents(self, documents):\n    \"\"\"Collect corpus statistics from a corpus.\n\n        Warnings\n        --------\n        Useful only if `debug=True`, to build the reverse `id=>set(words)` mapping.\n\n        Notes\n        -----\n        This is only a convenience wrapper for calling `doc2bow` on each document with `allow_update=True`.\n\n        Parameters\n        ----------\n        documents : iterable of list of str\n            Collection of documents.\n\n        Examples\n        --------\n        .. sourcecode:: pycon\n\n            >>> from gensim.corpora import HashDictionary\n            >>>\n            >>> dct = HashDictionary(debug=True)  # needs no training corpus!\n            >>>\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\n            >>> \"sparta\" in dct.token2id\n            False\n            >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\n            >>> \"sparta\" in dct.token2id\n            True\n\n        \"\"\"\n    for (docno, document) in enumerate(documents):\n        if docno % 10000 == 0:\n            logger.info('adding document #%i to %s', docno, self)\n        self.doc2bow(document, allow_update=True)\n    logger.info('built %s from %i documents (total %i corpus positions)', self, self.num_docs, self.num_pos)",
        "mutated": [
            "def add_documents(self, documents):\n    if False:\n        i = 10\n    'Collect corpus statistics from a corpus.\\n\\n        Warnings\\n        --------\\n        Useful only if `debug=True`, to build the reverse `id=>set(words)` mapping.\\n\\n        Notes\\n        -----\\n        This is only a convenience wrapper for calling `doc2bow` on each document with `allow_update=True`.\\n\\n        Parameters\\n        ----------\\n        documents : iterable of list of str\\n            Collection of documents.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary(debug=True)  # needs no training corpus!\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> \"sparta\" in dct.token2id\\n            False\\n            >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\\n            >>> \"sparta\" in dct.token2id\\n            True\\n\\n        '\n    for (docno, document) in enumerate(documents):\n        if docno % 10000 == 0:\n            logger.info('adding document #%i to %s', docno, self)\n        self.doc2bow(document, allow_update=True)\n    logger.info('built %s from %i documents (total %i corpus positions)', self, self.num_docs, self.num_pos)",
            "def add_documents(self, documents):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Collect corpus statistics from a corpus.\\n\\n        Warnings\\n        --------\\n        Useful only if `debug=True`, to build the reverse `id=>set(words)` mapping.\\n\\n        Notes\\n        -----\\n        This is only a convenience wrapper for calling `doc2bow` on each document with `allow_update=True`.\\n\\n        Parameters\\n        ----------\\n        documents : iterable of list of str\\n            Collection of documents.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary(debug=True)  # needs no training corpus!\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> \"sparta\" in dct.token2id\\n            False\\n            >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\\n            >>> \"sparta\" in dct.token2id\\n            True\\n\\n        '\n    for (docno, document) in enumerate(documents):\n        if docno % 10000 == 0:\n            logger.info('adding document #%i to %s', docno, self)\n        self.doc2bow(document, allow_update=True)\n    logger.info('built %s from %i documents (total %i corpus positions)', self, self.num_docs, self.num_pos)",
            "def add_documents(self, documents):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Collect corpus statistics from a corpus.\\n\\n        Warnings\\n        --------\\n        Useful only if `debug=True`, to build the reverse `id=>set(words)` mapping.\\n\\n        Notes\\n        -----\\n        This is only a convenience wrapper for calling `doc2bow` on each document with `allow_update=True`.\\n\\n        Parameters\\n        ----------\\n        documents : iterable of list of str\\n            Collection of documents.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary(debug=True)  # needs no training corpus!\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> \"sparta\" in dct.token2id\\n            False\\n            >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\\n            >>> \"sparta\" in dct.token2id\\n            True\\n\\n        '\n    for (docno, document) in enumerate(documents):\n        if docno % 10000 == 0:\n            logger.info('adding document #%i to %s', docno, self)\n        self.doc2bow(document, allow_update=True)\n    logger.info('built %s from %i documents (total %i corpus positions)', self, self.num_docs, self.num_pos)",
            "def add_documents(self, documents):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Collect corpus statistics from a corpus.\\n\\n        Warnings\\n        --------\\n        Useful only if `debug=True`, to build the reverse `id=>set(words)` mapping.\\n\\n        Notes\\n        -----\\n        This is only a convenience wrapper for calling `doc2bow` on each document with `allow_update=True`.\\n\\n        Parameters\\n        ----------\\n        documents : iterable of list of str\\n            Collection of documents.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary(debug=True)  # needs no training corpus!\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> \"sparta\" in dct.token2id\\n            False\\n            >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\\n            >>> \"sparta\" in dct.token2id\\n            True\\n\\n        '\n    for (docno, document) in enumerate(documents):\n        if docno % 10000 == 0:\n            logger.info('adding document #%i to %s', docno, self)\n        self.doc2bow(document, allow_update=True)\n    logger.info('built %s from %i documents (total %i corpus positions)', self, self.num_docs, self.num_pos)",
            "def add_documents(self, documents):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Collect corpus statistics from a corpus.\\n\\n        Warnings\\n        --------\\n        Useful only if `debug=True`, to build the reverse `id=>set(words)` mapping.\\n\\n        Notes\\n        -----\\n        This is only a convenience wrapper for calling `doc2bow` on each document with `allow_update=True`.\\n\\n        Parameters\\n        ----------\\n        documents : iterable of list of str\\n            Collection of documents.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary(debug=True)  # needs no training corpus!\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> \"sparta\" in dct.token2id\\n            False\\n            >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\\n            >>> \"sparta\" in dct.token2id\\n            True\\n\\n        '\n    for (docno, document) in enumerate(documents):\n        if docno % 10000 == 0:\n            logger.info('adding document #%i to %s', docno, self)\n        self.doc2bow(document, allow_update=True)\n    logger.info('built %s from %i documents (total %i corpus positions)', self, self.num_docs, self.num_pos)"
        ]
    },
    {
        "func_name": "doc2bow",
        "original": "def doc2bow(self, document, allow_update=False, return_missing=False):\n    \"\"\"Convert a sequence of words `document` into the bag-of-words format of `[(word_id, word_count)]`\n        (e.g. `[(1, 4), (150, 1), (2005, 2)]`).\n\n        Notes\n        -----\n        Each word is assumed to be a **tokenized and normalized** string. No further preprocessing\n        is done on the words in `document`: you have to apply tokenization, stemming etc before calling this method.\n\n        If `allow_update` or `self.allow_update` is set, then also update the dictionary in the process: update overall\n        corpus statistics and document frequencies. For each id appearing in this document, increase its document\n        frequency (`self.dfs`) by one.\n\n        Parameters\n        ----------\n        document : sequence of str\n            A sequence of word tokens = **tokenized and normalized** strings.\n        allow_update : bool, optional\n            Update corpus statistics and if `debug=True`, also the reverse id=>word mapping?\n        return_missing : bool, optional\n            Not used. Only here for compatibility with the Dictionary class.\n\n        Return\n        ------\n        list of (int, int)\n            Document in Bag-of-words (BoW) format.\n\n        Examples\n        --------\n        .. sourcecode:: pycon\n\n            >>> from gensim.corpora import HashDictionary\n            >>>\n            >>> dct = HashDictionary()\n            >>> dct.doc2bow([\"this\", \"is\", \"m\u00e1ma\"])\n            [(1721, 1), (5280, 1), (22493, 1)]\n\n        \"\"\"\n    result = {}\n    missing = {}\n    document = sorted(document)\n    for (word_norm, group) in itertools.groupby(document):\n        frequency = len(list(group))\n        tokenid = self.restricted_hash(word_norm)\n        result[tokenid] = result.get(tokenid, 0) + frequency\n        if self.debug:\n            self.dfs_debug[word_norm] = self.dfs_debug.get(word_norm, 0) + 1\n    if allow_update or self.allow_update:\n        self.num_docs += 1\n        self.num_pos += len(document)\n        self.num_nnz += len(result)\n        if self.debug:\n            for tokenid in result.keys():\n                self.dfs[tokenid] = self.dfs.get(tokenid, 0) + 1\n    result = sorted(result.items())\n    if return_missing:\n        return (result, missing)\n    else:\n        return result",
        "mutated": [
            "def doc2bow(self, document, allow_update=False, return_missing=False):\n    if False:\n        i = 10\n    'Convert a sequence of words `document` into the bag-of-words format of `[(word_id, word_count)]`\\n        (e.g. `[(1, 4), (150, 1), (2005, 2)]`).\\n\\n        Notes\\n        -----\\n        Each word is assumed to be a **tokenized and normalized** string. No further preprocessing\\n        is done on the words in `document`: you have to apply tokenization, stemming etc before calling this method.\\n\\n        If `allow_update` or `self.allow_update` is set, then also update the dictionary in the process: update overall\\n        corpus statistics and document frequencies. For each id appearing in this document, increase its document\\n        frequency (`self.dfs`) by one.\\n\\n        Parameters\\n        ----------\\n        document : sequence of str\\n            A sequence of word tokens = **tokenized and normalized** strings.\\n        allow_update : bool, optional\\n            Update corpus statistics and if `debug=True`, also the reverse id=>word mapping?\\n        return_missing : bool, optional\\n            Not used. Only here for compatibility with the Dictionary class.\\n\\n        Return\\n        ------\\n        list of (int, int)\\n            Document in Bag-of-words (BoW) format.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary()\\n            >>> dct.doc2bow([\"this\", \"is\", \"m\u00e1ma\"])\\n            [(1721, 1), (5280, 1), (22493, 1)]\\n\\n        '\n    result = {}\n    missing = {}\n    document = sorted(document)\n    for (word_norm, group) in itertools.groupby(document):\n        frequency = len(list(group))\n        tokenid = self.restricted_hash(word_norm)\n        result[tokenid] = result.get(tokenid, 0) + frequency\n        if self.debug:\n            self.dfs_debug[word_norm] = self.dfs_debug.get(word_norm, 0) + 1\n    if allow_update or self.allow_update:\n        self.num_docs += 1\n        self.num_pos += len(document)\n        self.num_nnz += len(result)\n        if self.debug:\n            for tokenid in result.keys():\n                self.dfs[tokenid] = self.dfs.get(tokenid, 0) + 1\n    result = sorted(result.items())\n    if return_missing:\n        return (result, missing)\n    else:\n        return result",
            "def doc2bow(self, document, allow_update=False, return_missing=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert a sequence of words `document` into the bag-of-words format of `[(word_id, word_count)]`\\n        (e.g. `[(1, 4), (150, 1), (2005, 2)]`).\\n\\n        Notes\\n        -----\\n        Each word is assumed to be a **tokenized and normalized** string. No further preprocessing\\n        is done on the words in `document`: you have to apply tokenization, stemming etc before calling this method.\\n\\n        If `allow_update` or `self.allow_update` is set, then also update the dictionary in the process: update overall\\n        corpus statistics and document frequencies. For each id appearing in this document, increase its document\\n        frequency (`self.dfs`) by one.\\n\\n        Parameters\\n        ----------\\n        document : sequence of str\\n            A sequence of word tokens = **tokenized and normalized** strings.\\n        allow_update : bool, optional\\n            Update corpus statistics and if `debug=True`, also the reverse id=>word mapping?\\n        return_missing : bool, optional\\n            Not used. Only here for compatibility with the Dictionary class.\\n\\n        Return\\n        ------\\n        list of (int, int)\\n            Document in Bag-of-words (BoW) format.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary()\\n            >>> dct.doc2bow([\"this\", \"is\", \"m\u00e1ma\"])\\n            [(1721, 1), (5280, 1), (22493, 1)]\\n\\n        '\n    result = {}\n    missing = {}\n    document = sorted(document)\n    for (word_norm, group) in itertools.groupby(document):\n        frequency = len(list(group))\n        tokenid = self.restricted_hash(word_norm)\n        result[tokenid] = result.get(tokenid, 0) + frequency\n        if self.debug:\n            self.dfs_debug[word_norm] = self.dfs_debug.get(word_norm, 0) + 1\n    if allow_update or self.allow_update:\n        self.num_docs += 1\n        self.num_pos += len(document)\n        self.num_nnz += len(result)\n        if self.debug:\n            for tokenid in result.keys():\n                self.dfs[tokenid] = self.dfs.get(tokenid, 0) + 1\n    result = sorted(result.items())\n    if return_missing:\n        return (result, missing)\n    else:\n        return result",
            "def doc2bow(self, document, allow_update=False, return_missing=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert a sequence of words `document` into the bag-of-words format of `[(word_id, word_count)]`\\n        (e.g. `[(1, 4), (150, 1), (2005, 2)]`).\\n\\n        Notes\\n        -----\\n        Each word is assumed to be a **tokenized and normalized** string. No further preprocessing\\n        is done on the words in `document`: you have to apply tokenization, stemming etc before calling this method.\\n\\n        If `allow_update` or `self.allow_update` is set, then also update the dictionary in the process: update overall\\n        corpus statistics and document frequencies. For each id appearing in this document, increase its document\\n        frequency (`self.dfs`) by one.\\n\\n        Parameters\\n        ----------\\n        document : sequence of str\\n            A sequence of word tokens = **tokenized and normalized** strings.\\n        allow_update : bool, optional\\n            Update corpus statistics and if `debug=True`, also the reverse id=>word mapping?\\n        return_missing : bool, optional\\n            Not used. Only here for compatibility with the Dictionary class.\\n\\n        Return\\n        ------\\n        list of (int, int)\\n            Document in Bag-of-words (BoW) format.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary()\\n            >>> dct.doc2bow([\"this\", \"is\", \"m\u00e1ma\"])\\n            [(1721, 1), (5280, 1), (22493, 1)]\\n\\n        '\n    result = {}\n    missing = {}\n    document = sorted(document)\n    for (word_norm, group) in itertools.groupby(document):\n        frequency = len(list(group))\n        tokenid = self.restricted_hash(word_norm)\n        result[tokenid] = result.get(tokenid, 0) + frequency\n        if self.debug:\n            self.dfs_debug[word_norm] = self.dfs_debug.get(word_norm, 0) + 1\n    if allow_update or self.allow_update:\n        self.num_docs += 1\n        self.num_pos += len(document)\n        self.num_nnz += len(result)\n        if self.debug:\n            for tokenid in result.keys():\n                self.dfs[tokenid] = self.dfs.get(tokenid, 0) + 1\n    result = sorted(result.items())\n    if return_missing:\n        return (result, missing)\n    else:\n        return result",
            "def doc2bow(self, document, allow_update=False, return_missing=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert a sequence of words `document` into the bag-of-words format of `[(word_id, word_count)]`\\n        (e.g. `[(1, 4), (150, 1), (2005, 2)]`).\\n\\n        Notes\\n        -----\\n        Each word is assumed to be a **tokenized and normalized** string. No further preprocessing\\n        is done on the words in `document`: you have to apply tokenization, stemming etc before calling this method.\\n\\n        If `allow_update` or `self.allow_update` is set, then also update the dictionary in the process: update overall\\n        corpus statistics and document frequencies. For each id appearing in this document, increase its document\\n        frequency (`self.dfs`) by one.\\n\\n        Parameters\\n        ----------\\n        document : sequence of str\\n            A sequence of word tokens = **tokenized and normalized** strings.\\n        allow_update : bool, optional\\n            Update corpus statistics and if `debug=True`, also the reverse id=>word mapping?\\n        return_missing : bool, optional\\n            Not used. Only here for compatibility with the Dictionary class.\\n\\n        Return\\n        ------\\n        list of (int, int)\\n            Document in Bag-of-words (BoW) format.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary()\\n            >>> dct.doc2bow([\"this\", \"is\", \"m\u00e1ma\"])\\n            [(1721, 1), (5280, 1), (22493, 1)]\\n\\n        '\n    result = {}\n    missing = {}\n    document = sorted(document)\n    for (word_norm, group) in itertools.groupby(document):\n        frequency = len(list(group))\n        tokenid = self.restricted_hash(word_norm)\n        result[tokenid] = result.get(tokenid, 0) + frequency\n        if self.debug:\n            self.dfs_debug[word_norm] = self.dfs_debug.get(word_norm, 0) + 1\n    if allow_update or self.allow_update:\n        self.num_docs += 1\n        self.num_pos += len(document)\n        self.num_nnz += len(result)\n        if self.debug:\n            for tokenid in result.keys():\n                self.dfs[tokenid] = self.dfs.get(tokenid, 0) + 1\n    result = sorted(result.items())\n    if return_missing:\n        return (result, missing)\n    else:\n        return result",
            "def doc2bow(self, document, allow_update=False, return_missing=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert a sequence of words `document` into the bag-of-words format of `[(word_id, word_count)]`\\n        (e.g. `[(1, 4), (150, 1), (2005, 2)]`).\\n\\n        Notes\\n        -----\\n        Each word is assumed to be a **tokenized and normalized** string. No further preprocessing\\n        is done on the words in `document`: you have to apply tokenization, stemming etc before calling this method.\\n\\n        If `allow_update` or `self.allow_update` is set, then also update the dictionary in the process: update overall\\n        corpus statistics and document frequencies. For each id appearing in this document, increase its document\\n        frequency (`self.dfs`) by one.\\n\\n        Parameters\\n        ----------\\n        document : sequence of str\\n            A sequence of word tokens = **tokenized and normalized** strings.\\n        allow_update : bool, optional\\n            Update corpus statistics and if `debug=True`, also the reverse id=>word mapping?\\n        return_missing : bool, optional\\n            Not used. Only here for compatibility with the Dictionary class.\\n\\n        Return\\n        ------\\n        list of (int, int)\\n            Document in Bag-of-words (BoW) format.\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>>\\n            >>> dct = HashDictionary()\\n            >>> dct.doc2bow([\"this\", \"is\", \"m\u00e1ma\"])\\n            [(1721, 1), (5280, 1), (22493, 1)]\\n\\n        '\n    result = {}\n    missing = {}\n    document = sorted(document)\n    for (word_norm, group) in itertools.groupby(document):\n        frequency = len(list(group))\n        tokenid = self.restricted_hash(word_norm)\n        result[tokenid] = result.get(tokenid, 0) + frequency\n        if self.debug:\n            self.dfs_debug[word_norm] = self.dfs_debug.get(word_norm, 0) + 1\n    if allow_update or self.allow_update:\n        self.num_docs += 1\n        self.num_pos += len(document)\n        self.num_nnz += len(result)\n        if self.debug:\n            for tokenid in result.keys():\n                self.dfs[tokenid] = self.dfs.get(tokenid, 0) + 1\n    result = sorted(result.items())\n    if return_missing:\n        return (result, missing)\n    else:\n        return result"
        ]
    },
    {
        "func_name": "filter_extremes",
        "original": "def filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000):\n    \"\"\"Filter tokens in the debug dictionary by their frequency.\n\n        Since :class:`~gensim.corpora.hashdictionary.HashDictionary` id range is fixed and doesn't depend on the number\n        of tokens seen, this doesn't really \"remove\" anything. It only clears some\n        internal corpus statistics, for easier debugging and a smaller RAM footprint.\n\n        Warnings\n        --------\n        Only makes sense when `debug=True`.\n\n        Parameters\n        ----------\n        no_below : int, optional\n            Keep tokens which are contained in at least `no_below` documents.\n        no_above : float, optional\n            Keep tokens which are contained in no more than `no_above` documents\n            (fraction of total corpus size, not an absolute number).\n        keep_n : int, optional\n            Keep only the first `keep_n` most frequent tokens.\n\n        Notes\n        -----\n        For tokens that appear in:\n\n        #. Less than `no_below` documents (absolute number) or \n\n        #. More than `no_above` documents (fraction of total corpus size, **not absolute number**).\n        #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `None`).\n\n        \"\"\"\n    no_above_abs = int(no_above * self.num_docs)\n    ok = [item for item in self.dfs_debug.items() if no_below <= item[1] <= no_above_abs]\n    ok = frozenset((word for (word, freq) in sorted(ok, key=lambda x: -x[1])[:keep_n]))\n    self.dfs_debug = {word: freq for (word, freq) in self.dfs_debug.items() if word in ok}\n    self.token2id = {token: tokenid for (token, tokenid) in self.token2id.items() if token in self.dfs_debug}\n    self.id2token = {tokenid: {token for token in tokens if token in self.dfs_debug} for (tokenid, tokens) in self.id2token.items()}\n    self.dfs = {tokenid: freq for (tokenid, freq) in self.dfs.items() if self.id2token.get(tokenid, False)}\n    logger.info('kept statistics for which were in no less than %i and no more than %i (=%.1f%%) documents', no_below, no_above_abs, 100.0 * no_above)",
        "mutated": [
            "def filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000):\n    if False:\n        i = 10\n    'Filter tokens in the debug dictionary by their frequency.\\n\\n        Since :class:`~gensim.corpora.hashdictionary.HashDictionary` id range is fixed and doesn\\'t depend on the number\\n        of tokens seen, this doesn\\'t really \"remove\" anything. It only clears some\\n        internal corpus statistics, for easier debugging and a smaller RAM footprint.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`.\\n\\n        Parameters\\n        ----------\\n        no_below : int, optional\\n            Keep tokens which are contained in at least `no_below` documents.\\n        no_above : float, optional\\n            Keep tokens which are contained in no more than `no_above` documents\\n            (fraction of total corpus size, not an absolute number).\\n        keep_n : int, optional\\n            Keep only the first `keep_n` most frequent tokens.\\n\\n        Notes\\n        -----\\n        For tokens that appear in:\\n\\n        #. Less than `no_below` documents (absolute number) or \\n\\n        #. More than `no_above` documents (fraction of total corpus size, **not absolute number**).\\n        #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `None`).\\n\\n        '\n    no_above_abs = int(no_above * self.num_docs)\n    ok = [item for item in self.dfs_debug.items() if no_below <= item[1] <= no_above_abs]\n    ok = frozenset((word for (word, freq) in sorted(ok, key=lambda x: -x[1])[:keep_n]))\n    self.dfs_debug = {word: freq for (word, freq) in self.dfs_debug.items() if word in ok}\n    self.token2id = {token: tokenid for (token, tokenid) in self.token2id.items() if token in self.dfs_debug}\n    self.id2token = {tokenid: {token for token in tokens if token in self.dfs_debug} for (tokenid, tokens) in self.id2token.items()}\n    self.dfs = {tokenid: freq for (tokenid, freq) in self.dfs.items() if self.id2token.get(tokenid, False)}\n    logger.info('kept statistics for which were in no less than %i and no more than %i (=%.1f%%) documents', no_below, no_above_abs, 100.0 * no_above)",
            "def filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Filter tokens in the debug dictionary by their frequency.\\n\\n        Since :class:`~gensim.corpora.hashdictionary.HashDictionary` id range is fixed and doesn\\'t depend on the number\\n        of tokens seen, this doesn\\'t really \"remove\" anything. It only clears some\\n        internal corpus statistics, for easier debugging and a smaller RAM footprint.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`.\\n\\n        Parameters\\n        ----------\\n        no_below : int, optional\\n            Keep tokens which are contained in at least `no_below` documents.\\n        no_above : float, optional\\n            Keep tokens which are contained in no more than `no_above` documents\\n            (fraction of total corpus size, not an absolute number).\\n        keep_n : int, optional\\n            Keep only the first `keep_n` most frequent tokens.\\n\\n        Notes\\n        -----\\n        For tokens that appear in:\\n\\n        #. Less than `no_below` documents (absolute number) or \\n\\n        #. More than `no_above` documents (fraction of total corpus size, **not absolute number**).\\n        #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `None`).\\n\\n        '\n    no_above_abs = int(no_above * self.num_docs)\n    ok = [item for item in self.dfs_debug.items() if no_below <= item[1] <= no_above_abs]\n    ok = frozenset((word for (word, freq) in sorted(ok, key=lambda x: -x[1])[:keep_n]))\n    self.dfs_debug = {word: freq for (word, freq) in self.dfs_debug.items() if word in ok}\n    self.token2id = {token: tokenid for (token, tokenid) in self.token2id.items() if token in self.dfs_debug}\n    self.id2token = {tokenid: {token for token in tokens if token in self.dfs_debug} for (tokenid, tokens) in self.id2token.items()}\n    self.dfs = {tokenid: freq for (tokenid, freq) in self.dfs.items() if self.id2token.get(tokenid, False)}\n    logger.info('kept statistics for which were in no less than %i and no more than %i (=%.1f%%) documents', no_below, no_above_abs, 100.0 * no_above)",
            "def filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Filter tokens in the debug dictionary by their frequency.\\n\\n        Since :class:`~gensim.corpora.hashdictionary.HashDictionary` id range is fixed and doesn\\'t depend on the number\\n        of tokens seen, this doesn\\'t really \"remove\" anything. It only clears some\\n        internal corpus statistics, for easier debugging and a smaller RAM footprint.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`.\\n\\n        Parameters\\n        ----------\\n        no_below : int, optional\\n            Keep tokens which are contained in at least `no_below` documents.\\n        no_above : float, optional\\n            Keep tokens which are contained in no more than `no_above` documents\\n            (fraction of total corpus size, not an absolute number).\\n        keep_n : int, optional\\n            Keep only the first `keep_n` most frequent tokens.\\n\\n        Notes\\n        -----\\n        For tokens that appear in:\\n\\n        #. Less than `no_below` documents (absolute number) or \\n\\n        #. More than `no_above` documents (fraction of total corpus size, **not absolute number**).\\n        #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `None`).\\n\\n        '\n    no_above_abs = int(no_above * self.num_docs)\n    ok = [item for item in self.dfs_debug.items() if no_below <= item[1] <= no_above_abs]\n    ok = frozenset((word for (word, freq) in sorted(ok, key=lambda x: -x[1])[:keep_n]))\n    self.dfs_debug = {word: freq for (word, freq) in self.dfs_debug.items() if word in ok}\n    self.token2id = {token: tokenid for (token, tokenid) in self.token2id.items() if token in self.dfs_debug}\n    self.id2token = {tokenid: {token for token in tokens if token in self.dfs_debug} for (tokenid, tokens) in self.id2token.items()}\n    self.dfs = {tokenid: freq for (tokenid, freq) in self.dfs.items() if self.id2token.get(tokenid, False)}\n    logger.info('kept statistics for which were in no less than %i and no more than %i (=%.1f%%) documents', no_below, no_above_abs, 100.0 * no_above)",
            "def filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Filter tokens in the debug dictionary by their frequency.\\n\\n        Since :class:`~gensim.corpora.hashdictionary.HashDictionary` id range is fixed and doesn\\'t depend on the number\\n        of tokens seen, this doesn\\'t really \"remove\" anything. It only clears some\\n        internal corpus statistics, for easier debugging and a smaller RAM footprint.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`.\\n\\n        Parameters\\n        ----------\\n        no_below : int, optional\\n            Keep tokens which are contained in at least `no_below` documents.\\n        no_above : float, optional\\n            Keep tokens which are contained in no more than `no_above` documents\\n            (fraction of total corpus size, not an absolute number).\\n        keep_n : int, optional\\n            Keep only the first `keep_n` most frequent tokens.\\n\\n        Notes\\n        -----\\n        For tokens that appear in:\\n\\n        #. Less than `no_below` documents (absolute number) or \\n\\n        #. More than `no_above` documents (fraction of total corpus size, **not absolute number**).\\n        #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `None`).\\n\\n        '\n    no_above_abs = int(no_above * self.num_docs)\n    ok = [item for item in self.dfs_debug.items() if no_below <= item[1] <= no_above_abs]\n    ok = frozenset((word for (word, freq) in sorted(ok, key=lambda x: -x[1])[:keep_n]))\n    self.dfs_debug = {word: freq for (word, freq) in self.dfs_debug.items() if word in ok}\n    self.token2id = {token: tokenid for (token, tokenid) in self.token2id.items() if token in self.dfs_debug}\n    self.id2token = {tokenid: {token for token in tokens if token in self.dfs_debug} for (tokenid, tokens) in self.id2token.items()}\n    self.dfs = {tokenid: freq for (tokenid, freq) in self.dfs.items() if self.id2token.get(tokenid, False)}\n    logger.info('kept statistics for which were in no less than %i and no more than %i (=%.1f%%) documents', no_below, no_above_abs, 100.0 * no_above)",
            "def filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Filter tokens in the debug dictionary by their frequency.\\n\\n        Since :class:`~gensim.corpora.hashdictionary.HashDictionary` id range is fixed and doesn\\'t depend on the number\\n        of tokens seen, this doesn\\'t really \"remove\" anything. It only clears some\\n        internal corpus statistics, for easier debugging and a smaller RAM footprint.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`.\\n\\n        Parameters\\n        ----------\\n        no_below : int, optional\\n            Keep tokens which are contained in at least `no_below` documents.\\n        no_above : float, optional\\n            Keep tokens which are contained in no more than `no_above` documents\\n            (fraction of total corpus size, not an absolute number).\\n        keep_n : int, optional\\n            Keep only the first `keep_n` most frequent tokens.\\n\\n        Notes\\n        -----\\n        For tokens that appear in:\\n\\n        #. Less than `no_below` documents (absolute number) or \\n\\n        #. More than `no_above` documents (fraction of total corpus size, **not absolute number**).\\n        #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `None`).\\n\\n        '\n    no_above_abs = int(no_above * self.num_docs)\n    ok = [item for item in self.dfs_debug.items() if no_below <= item[1] <= no_above_abs]\n    ok = frozenset((word for (word, freq) in sorted(ok, key=lambda x: -x[1])[:keep_n]))\n    self.dfs_debug = {word: freq for (word, freq) in self.dfs_debug.items() if word in ok}\n    self.token2id = {token: tokenid for (token, tokenid) in self.token2id.items() if token in self.dfs_debug}\n    self.id2token = {tokenid: {token for token in tokens if token in self.dfs_debug} for (tokenid, tokens) in self.id2token.items()}\n    self.dfs = {tokenid: freq for (tokenid, freq) in self.dfs.items() if self.id2token.get(tokenid, False)}\n    logger.info('kept statistics for which were in no less than %i and no more than %i (=%.1f%%) documents', no_below, no_above_abs, 100.0 * no_above)"
        ]
    },
    {
        "func_name": "save_as_text",
        "original": "def save_as_text(self, fname):\n    \"\"\"Save the debug token=>id mapping to a text file.\n\n        Warnings\n        --------\n        Only makes sense when `debug=True`, for debugging.\n\n        Parameters\n        ----------\n        fname : str\n            Path to output file.\n\n        Notes\n        -----\n        The format is:\n        `id[TAB]document frequency of this id[TAB]tab-separated set of words in UTF8 that map to this id[NEWLINE]`.\n\n\n        Examples\n        --------\n        .. sourcecode:: pycon\n\n            >>> from gensim.corpora import HashDictionary\n            >>> from gensim.test.utils import get_tmpfile\n            >>>\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\n            >>> data = HashDictionary(corpus)\n            >>> data.save_as_text(get_tmpfile(\"dictionary_in_text_format\"))\n\n        \"\"\"\n    logger.info('saving %s mapping to %s' % (self, fname))\n    with utils.open(fname, 'wb') as fout:\n        for tokenid in self.keys():\n            words = sorted(self[tokenid])\n            if words:\n                words_df = [(word, self.dfs_debug.get(word, 0)) for word in words]\n                words_df = ['%s(%i)' % item for item in sorted(words_df, key=lambda x: -x[1])]\n                words_df = '\\t'.join(words_df)\n                fout.write(utils.to_utf8('%i\\t%i\\t%s\\n' % (tokenid, self.dfs.get(tokenid, 0), words_df)))",
        "mutated": [
            "def save_as_text(self, fname):\n    if False:\n        i = 10\n    'Save the debug token=>id mapping to a text file.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`, for debugging.\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            Path to output file.\\n\\n        Notes\\n        -----\\n        The format is:\\n        `id[TAB]document frequency of this id[TAB]tab-separated set of words in UTF8 that map to this id[NEWLINE]`.\\n\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>> from gensim.test.utils import get_tmpfile\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> data = HashDictionary(corpus)\\n            >>> data.save_as_text(get_tmpfile(\"dictionary_in_text_format\"))\\n\\n        '\n    logger.info('saving %s mapping to %s' % (self, fname))\n    with utils.open(fname, 'wb') as fout:\n        for tokenid in self.keys():\n            words = sorted(self[tokenid])\n            if words:\n                words_df = [(word, self.dfs_debug.get(word, 0)) for word in words]\n                words_df = ['%s(%i)' % item for item in sorted(words_df, key=lambda x: -x[1])]\n                words_df = '\\t'.join(words_df)\n                fout.write(utils.to_utf8('%i\\t%i\\t%s\\n' % (tokenid, self.dfs.get(tokenid, 0), words_df)))",
            "def save_as_text(self, fname):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Save the debug token=>id mapping to a text file.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`, for debugging.\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            Path to output file.\\n\\n        Notes\\n        -----\\n        The format is:\\n        `id[TAB]document frequency of this id[TAB]tab-separated set of words in UTF8 that map to this id[NEWLINE]`.\\n\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>> from gensim.test.utils import get_tmpfile\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> data = HashDictionary(corpus)\\n            >>> data.save_as_text(get_tmpfile(\"dictionary_in_text_format\"))\\n\\n        '\n    logger.info('saving %s mapping to %s' % (self, fname))\n    with utils.open(fname, 'wb') as fout:\n        for tokenid in self.keys():\n            words = sorted(self[tokenid])\n            if words:\n                words_df = [(word, self.dfs_debug.get(word, 0)) for word in words]\n                words_df = ['%s(%i)' % item for item in sorted(words_df, key=lambda x: -x[1])]\n                words_df = '\\t'.join(words_df)\n                fout.write(utils.to_utf8('%i\\t%i\\t%s\\n' % (tokenid, self.dfs.get(tokenid, 0), words_df)))",
            "def save_as_text(self, fname):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Save the debug token=>id mapping to a text file.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`, for debugging.\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            Path to output file.\\n\\n        Notes\\n        -----\\n        The format is:\\n        `id[TAB]document frequency of this id[TAB]tab-separated set of words in UTF8 that map to this id[NEWLINE]`.\\n\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>> from gensim.test.utils import get_tmpfile\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> data = HashDictionary(corpus)\\n            >>> data.save_as_text(get_tmpfile(\"dictionary_in_text_format\"))\\n\\n        '\n    logger.info('saving %s mapping to %s' % (self, fname))\n    with utils.open(fname, 'wb') as fout:\n        for tokenid in self.keys():\n            words = sorted(self[tokenid])\n            if words:\n                words_df = [(word, self.dfs_debug.get(word, 0)) for word in words]\n                words_df = ['%s(%i)' % item for item in sorted(words_df, key=lambda x: -x[1])]\n                words_df = '\\t'.join(words_df)\n                fout.write(utils.to_utf8('%i\\t%i\\t%s\\n' % (tokenid, self.dfs.get(tokenid, 0), words_df)))",
            "def save_as_text(self, fname):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Save the debug token=>id mapping to a text file.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`, for debugging.\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            Path to output file.\\n\\n        Notes\\n        -----\\n        The format is:\\n        `id[TAB]document frequency of this id[TAB]tab-separated set of words in UTF8 that map to this id[NEWLINE]`.\\n\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>> from gensim.test.utils import get_tmpfile\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> data = HashDictionary(corpus)\\n            >>> data.save_as_text(get_tmpfile(\"dictionary_in_text_format\"))\\n\\n        '\n    logger.info('saving %s mapping to %s' % (self, fname))\n    with utils.open(fname, 'wb') as fout:\n        for tokenid in self.keys():\n            words = sorted(self[tokenid])\n            if words:\n                words_df = [(word, self.dfs_debug.get(word, 0)) for word in words]\n                words_df = ['%s(%i)' % item for item in sorted(words_df, key=lambda x: -x[1])]\n                words_df = '\\t'.join(words_df)\n                fout.write(utils.to_utf8('%i\\t%i\\t%s\\n' % (tokenid, self.dfs.get(tokenid, 0), words_df)))",
            "def save_as_text(self, fname):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Save the debug token=>id mapping to a text file.\\n\\n        Warnings\\n        --------\\n        Only makes sense when `debug=True`, for debugging.\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            Path to output file.\\n\\n        Notes\\n        -----\\n        The format is:\\n        `id[TAB]document frequency of this id[TAB]tab-separated set of words in UTF8 that map to this id[NEWLINE]`.\\n\\n\\n        Examples\\n        --------\\n        .. sourcecode:: pycon\\n\\n            >>> from gensim.corpora import HashDictionary\\n            >>> from gensim.test.utils import get_tmpfile\\n            >>>\\n            >>> corpus = [[\"m\u00e1ma\", \"mele\", \"maso\"], [\"ema\", \"m\u00e1\", \"m\u00e1ma\"]]\\n            >>> data = HashDictionary(corpus)\\n            >>> data.save_as_text(get_tmpfile(\"dictionary_in_text_format\"))\\n\\n        '\n    logger.info('saving %s mapping to %s' % (self, fname))\n    with utils.open(fname, 'wb') as fout:\n        for tokenid in self.keys():\n            words = sorted(self[tokenid])\n            if words:\n                words_df = [(word, self.dfs_debug.get(word, 0)) for word in words]\n                words_df = ['%s(%i)' % item for item in sorted(words_df, key=lambda x: -x[1])]\n                words_df = '\\t'.join(words_df)\n                fout.write(utils.to_utf8('%i\\t%i\\t%s\\n' % (tokenid, self.dfs.get(tokenid, 0), words_df)))"
        ]
    }
]