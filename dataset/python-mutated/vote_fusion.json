[
    {
        "func_name": "__init__",
        "original": "def __init__(self, num_classes=10, max_imvote_per_pixel=3):\n    super(VoteFusion, self).__init__()\n    self.num_classes = num_classes\n    self.max_imvote_per_pixel = max_imvote_per_pixel",
        "mutated": [
            "def __init__(self, num_classes=10, max_imvote_per_pixel=3):\n    if False:\n        i = 10\n    super(VoteFusion, self).__init__()\n    self.num_classes = num_classes\n    self.max_imvote_per_pixel = max_imvote_per_pixel",
            "def __init__(self, num_classes=10, max_imvote_per_pixel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(VoteFusion, self).__init__()\n    self.num_classes = num_classes\n    self.max_imvote_per_pixel = max_imvote_per_pixel",
            "def __init__(self, num_classes=10, max_imvote_per_pixel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(VoteFusion, self).__init__()\n    self.num_classes = num_classes\n    self.max_imvote_per_pixel = max_imvote_per_pixel",
            "def __init__(self, num_classes=10, max_imvote_per_pixel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(VoteFusion, self).__init__()\n    self.num_classes = num_classes\n    self.max_imvote_per_pixel = max_imvote_per_pixel",
            "def __init__(self, num_classes=10, max_imvote_per_pixel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(VoteFusion, self).__init__()\n    self.num_classes = num_classes\n    self.max_imvote_per_pixel = max_imvote_per_pixel"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas):\n    \"\"\"Forward function.\n\n        Args:\n            imgs (list[torch.Tensor]): Image features.\n            bboxes_2d_rescaled (list[torch.Tensor]): 2D bboxes.\n            seeds_3d_depth (torch.Tensor): 3D seeds.\n            img_metas (list[dict]): Meta information of images.\n\n        Returns:\n            torch.Tensor: Concatenated cues of each point.\n            torch.Tensor: Validity mask of each feature.\n        \"\"\"\n    img_features = []\n    masks = []\n    for (i, data) in enumerate(zip(imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas)):\n        (img, bbox_2d_rescaled, seed_3d_depth, img_meta) = data\n        bbox_num = bbox_2d_rescaled.shape[0]\n        seed_num = seed_3d_depth.shape[0]\n        img_shape = img_meta['img_shape']\n        (img_h, img_w, _) = img_shape\n        xyz_depth = apply_3d_transformation(seed_3d_depth, 'DEPTH', img_meta, reverse=True)\n        depth2img = xyz_depth.new_tensor(img_meta['depth2img'])\n        uvz_origin = points_cam2img(xyz_depth, depth2img, True)\n        z_cam = uvz_origin[..., 2]\n        uv_origin = (uvz_origin[..., :2] - 1).round()\n        uv_rescaled = coord_2d_transform(img_meta, uv_origin, True)\n        bbox_2d_origin = bbox_2d_transform(img_meta, bbox_2d_rescaled, False)\n        if bbox_num == 0:\n            imvote_num = seed_num * self.max_imvote_per_pixel\n            two_cues = torch.zeros((15, imvote_num), device=seed_3d_depth.device)\n            mask_zero = torch.zeros(imvote_num - seed_num, device=seed_3d_depth.device).bool()\n            mask_one = torch.ones(seed_num, device=seed_3d_depth.device).bool()\n            mask = torch.cat([mask_one, mask_zero], dim=0)\n        else:\n            bbox_expanded = bbox_2d_origin.view(1, bbox_num, -1).expand(seed_num, -1, -1)\n            seed_2d_expanded = uv_origin.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            (seed_2d_expanded_x, seed_2d_expanded_y) = seed_2d_expanded.split(1, dim=-1)\n            (bbox_expanded_l, bbox_expanded_t, bbox_expanded_r, bbox_expanded_b, bbox_expanded_conf, bbox_expanded_cls) = bbox_expanded.split(1, dim=-1)\n            bbox_expanded_midx = (bbox_expanded_l + bbox_expanded_r) / 2\n            bbox_expanded_midy = (bbox_expanded_t + bbox_expanded_b) / 2\n            seed_2d_in_bbox_x = (seed_2d_expanded_x > bbox_expanded_l) * (seed_2d_expanded_x < bbox_expanded_r)\n            seed_2d_in_bbox_y = (seed_2d_expanded_y > bbox_expanded_t) * (seed_2d_expanded_y < bbox_expanded_b)\n            seed_2d_in_bbox = seed_2d_in_bbox_x * seed_2d_in_bbox_y\n            sem_cue = torch.zeros_like(bbox_expanded_conf).expand(-1, -1, self.num_classes)\n            sem_cue = sem_cue.scatter(-1, bbox_expanded_cls.long(), bbox_expanded_conf)\n            delta_u = bbox_expanded_midx - seed_2d_expanded_x\n            delta_v = bbox_expanded_midy - seed_2d_expanded_y\n            seed_3d_expanded = seed_3d_depth.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            z_cam = z_cam.view(seed_num, 1, 1).expand(-1, bbox_num, -1)\n            imvote = torch.cat([delta_u, delta_v, torch.zeros_like(delta_v)], dim=-1).view(-1, 3)\n            imvote = imvote * z_cam.reshape(-1, 1)\n            imvote = imvote @ torch.inverse(depth2img.t())\n            imvote = apply_3d_transformation(imvote, 'DEPTH', img_meta, reverse=False)\n            seed_3d_expanded = seed_3d_expanded.reshape(imvote.shape)\n            ray_angle = seed_3d_expanded + imvote\n            ray_angle /= torch.sqrt(torch.sum(ray_angle ** 2, -1) + EPS).unsqueeze(-1)\n            xz = ray_angle[:, [0, 2]] / (ray_angle[:, [1]] + EPS) * seed_3d_expanded[:, [1]] - seed_3d_expanded[:, [0, 2]]\n            geo_cue = torch.cat([xz, ray_angle], dim=-1).view(seed_num, -1, 5)\n            two_cues = torch.cat([geo_cue, sem_cue], dim=-1)\n            two_cues = two_cues * seed_2d_in_bbox.float()\n            feature_size = two_cues.shape[-1]\n            if bbox_num < self.max_imvote_per_pixel:\n                append_num = self.max_imvote_per_pixel - bbox_num\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=seed_2d_in_bbox.device).bool()\n                seed_2d_in_bbox = torch.cat([seed_2d_in_bbox, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, feature_size), device=two_cues.device)\n                two_cues = torch.cat([two_cues, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=two_cues.device)\n                bbox_expanded_conf = torch.cat([bbox_expanded_conf, append_zeros], dim=1)\n            pair_score = seed_2d_in_bbox.float() + bbox_expanded_conf\n            (mask, indices) = pair_score.topk(self.max_imvote_per_pixel, dim=1, largest=True, sorted=True)\n            indices_img = indices.expand(-1, -1, feature_size)\n            two_cues = two_cues.gather(dim=1, index=indices_img)\n            two_cues = two_cues.transpose(1, 0)\n            two_cues = two_cues.reshape(-1, feature_size).transpose(1, 0).contiguous()\n            mask = mask.floor().int()\n            mask = mask.transpose(1, 0).reshape(-1).bool()\n        img = img[:, :img_shape[0], :img_shape[1]]\n        img_flatten = img.reshape(3, -1).float()\n        img_flatten /= 255.0\n        uv_rescaled[:, 0] = torch.clamp(uv_rescaled[:, 0].round(), 0, img_shape[1] - 1)\n        uv_rescaled[:, 1] = torch.clamp(uv_rescaled[:, 1].round(), 0, img_shape[0] - 1)\n        uv_flatten = uv_rescaled[:, 1].round() * img_shape[1] + uv_rescaled[:, 0].round()\n        uv_expanded = uv_flatten.unsqueeze(0).expand(3, -1).long()\n        txt_cue = torch.gather(img_flatten, dim=-1, index=uv_expanded)\n        txt_cue = txt_cue.unsqueeze(1).expand(-1, self.max_imvote_per_pixel, -1).reshape(3, -1)\n        img_feature = torch.cat([two_cues, txt_cue], dim=0)\n        img_features.append(img_feature)\n        masks.append(mask)\n    return (torch.stack(img_features, 0), torch.stack(masks, 0))",
        "mutated": [
            "def forward(self, imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas):\n    if False:\n        i = 10\n    'Forward function.\\n\\n        Args:\\n            imgs (list[torch.Tensor]): Image features.\\n            bboxes_2d_rescaled (list[torch.Tensor]): 2D bboxes.\\n            seeds_3d_depth (torch.Tensor): 3D seeds.\\n            img_metas (list[dict]): Meta information of images.\\n\\n        Returns:\\n            torch.Tensor: Concatenated cues of each point.\\n            torch.Tensor: Validity mask of each feature.\\n        '\n    img_features = []\n    masks = []\n    for (i, data) in enumerate(zip(imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas)):\n        (img, bbox_2d_rescaled, seed_3d_depth, img_meta) = data\n        bbox_num = bbox_2d_rescaled.shape[0]\n        seed_num = seed_3d_depth.shape[0]\n        img_shape = img_meta['img_shape']\n        (img_h, img_w, _) = img_shape\n        xyz_depth = apply_3d_transformation(seed_3d_depth, 'DEPTH', img_meta, reverse=True)\n        depth2img = xyz_depth.new_tensor(img_meta['depth2img'])\n        uvz_origin = points_cam2img(xyz_depth, depth2img, True)\n        z_cam = uvz_origin[..., 2]\n        uv_origin = (uvz_origin[..., :2] - 1).round()\n        uv_rescaled = coord_2d_transform(img_meta, uv_origin, True)\n        bbox_2d_origin = bbox_2d_transform(img_meta, bbox_2d_rescaled, False)\n        if bbox_num == 0:\n            imvote_num = seed_num * self.max_imvote_per_pixel\n            two_cues = torch.zeros((15, imvote_num), device=seed_3d_depth.device)\n            mask_zero = torch.zeros(imvote_num - seed_num, device=seed_3d_depth.device).bool()\n            mask_one = torch.ones(seed_num, device=seed_3d_depth.device).bool()\n            mask = torch.cat([mask_one, mask_zero], dim=0)\n        else:\n            bbox_expanded = bbox_2d_origin.view(1, bbox_num, -1).expand(seed_num, -1, -1)\n            seed_2d_expanded = uv_origin.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            (seed_2d_expanded_x, seed_2d_expanded_y) = seed_2d_expanded.split(1, dim=-1)\n            (bbox_expanded_l, bbox_expanded_t, bbox_expanded_r, bbox_expanded_b, bbox_expanded_conf, bbox_expanded_cls) = bbox_expanded.split(1, dim=-1)\n            bbox_expanded_midx = (bbox_expanded_l + bbox_expanded_r) / 2\n            bbox_expanded_midy = (bbox_expanded_t + bbox_expanded_b) / 2\n            seed_2d_in_bbox_x = (seed_2d_expanded_x > bbox_expanded_l) * (seed_2d_expanded_x < bbox_expanded_r)\n            seed_2d_in_bbox_y = (seed_2d_expanded_y > bbox_expanded_t) * (seed_2d_expanded_y < bbox_expanded_b)\n            seed_2d_in_bbox = seed_2d_in_bbox_x * seed_2d_in_bbox_y\n            sem_cue = torch.zeros_like(bbox_expanded_conf).expand(-1, -1, self.num_classes)\n            sem_cue = sem_cue.scatter(-1, bbox_expanded_cls.long(), bbox_expanded_conf)\n            delta_u = bbox_expanded_midx - seed_2d_expanded_x\n            delta_v = bbox_expanded_midy - seed_2d_expanded_y\n            seed_3d_expanded = seed_3d_depth.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            z_cam = z_cam.view(seed_num, 1, 1).expand(-1, bbox_num, -1)\n            imvote = torch.cat([delta_u, delta_v, torch.zeros_like(delta_v)], dim=-1).view(-1, 3)\n            imvote = imvote * z_cam.reshape(-1, 1)\n            imvote = imvote @ torch.inverse(depth2img.t())\n            imvote = apply_3d_transformation(imvote, 'DEPTH', img_meta, reverse=False)\n            seed_3d_expanded = seed_3d_expanded.reshape(imvote.shape)\n            ray_angle = seed_3d_expanded + imvote\n            ray_angle /= torch.sqrt(torch.sum(ray_angle ** 2, -1) + EPS).unsqueeze(-1)\n            xz = ray_angle[:, [0, 2]] / (ray_angle[:, [1]] + EPS) * seed_3d_expanded[:, [1]] - seed_3d_expanded[:, [0, 2]]\n            geo_cue = torch.cat([xz, ray_angle], dim=-1).view(seed_num, -1, 5)\n            two_cues = torch.cat([geo_cue, sem_cue], dim=-1)\n            two_cues = two_cues * seed_2d_in_bbox.float()\n            feature_size = two_cues.shape[-1]\n            if bbox_num < self.max_imvote_per_pixel:\n                append_num = self.max_imvote_per_pixel - bbox_num\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=seed_2d_in_bbox.device).bool()\n                seed_2d_in_bbox = torch.cat([seed_2d_in_bbox, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, feature_size), device=two_cues.device)\n                two_cues = torch.cat([two_cues, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=two_cues.device)\n                bbox_expanded_conf = torch.cat([bbox_expanded_conf, append_zeros], dim=1)\n            pair_score = seed_2d_in_bbox.float() + bbox_expanded_conf\n            (mask, indices) = pair_score.topk(self.max_imvote_per_pixel, dim=1, largest=True, sorted=True)\n            indices_img = indices.expand(-1, -1, feature_size)\n            two_cues = two_cues.gather(dim=1, index=indices_img)\n            two_cues = two_cues.transpose(1, 0)\n            two_cues = two_cues.reshape(-1, feature_size).transpose(1, 0).contiguous()\n            mask = mask.floor().int()\n            mask = mask.transpose(1, 0).reshape(-1).bool()\n        img = img[:, :img_shape[0], :img_shape[1]]\n        img_flatten = img.reshape(3, -1).float()\n        img_flatten /= 255.0\n        uv_rescaled[:, 0] = torch.clamp(uv_rescaled[:, 0].round(), 0, img_shape[1] - 1)\n        uv_rescaled[:, 1] = torch.clamp(uv_rescaled[:, 1].round(), 0, img_shape[0] - 1)\n        uv_flatten = uv_rescaled[:, 1].round() * img_shape[1] + uv_rescaled[:, 0].round()\n        uv_expanded = uv_flatten.unsqueeze(0).expand(3, -1).long()\n        txt_cue = torch.gather(img_flatten, dim=-1, index=uv_expanded)\n        txt_cue = txt_cue.unsqueeze(1).expand(-1, self.max_imvote_per_pixel, -1).reshape(3, -1)\n        img_feature = torch.cat([two_cues, txt_cue], dim=0)\n        img_features.append(img_feature)\n        masks.append(mask)\n    return (torch.stack(img_features, 0), torch.stack(masks, 0))",
            "def forward(self, imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward function.\\n\\n        Args:\\n            imgs (list[torch.Tensor]): Image features.\\n            bboxes_2d_rescaled (list[torch.Tensor]): 2D bboxes.\\n            seeds_3d_depth (torch.Tensor): 3D seeds.\\n            img_metas (list[dict]): Meta information of images.\\n\\n        Returns:\\n            torch.Tensor: Concatenated cues of each point.\\n            torch.Tensor: Validity mask of each feature.\\n        '\n    img_features = []\n    masks = []\n    for (i, data) in enumerate(zip(imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas)):\n        (img, bbox_2d_rescaled, seed_3d_depth, img_meta) = data\n        bbox_num = bbox_2d_rescaled.shape[0]\n        seed_num = seed_3d_depth.shape[0]\n        img_shape = img_meta['img_shape']\n        (img_h, img_w, _) = img_shape\n        xyz_depth = apply_3d_transformation(seed_3d_depth, 'DEPTH', img_meta, reverse=True)\n        depth2img = xyz_depth.new_tensor(img_meta['depth2img'])\n        uvz_origin = points_cam2img(xyz_depth, depth2img, True)\n        z_cam = uvz_origin[..., 2]\n        uv_origin = (uvz_origin[..., :2] - 1).round()\n        uv_rescaled = coord_2d_transform(img_meta, uv_origin, True)\n        bbox_2d_origin = bbox_2d_transform(img_meta, bbox_2d_rescaled, False)\n        if bbox_num == 0:\n            imvote_num = seed_num * self.max_imvote_per_pixel\n            two_cues = torch.zeros((15, imvote_num), device=seed_3d_depth.device)\n            mask_zero = torch.zeros(imvote_num - seed_num, device=seed_3d_depth.device).bool()\n            mask_one = torch.ones(seed_num, device=seed_3d_depth.device).bool()\n            mask = torch.cat([mask_one, mask_zero], dim=0)\n        else:\n            bbox_expanded = bbox_2d_origin.view(1, bbox_num, -1).expand(seed_num, -1, -1)\n            seed_2d_expanded = uv_origin.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            (seed_2d_expanded_x, seed_2d_expanded_y) = seed_2d_expanded.split(1, dim=-1)\n            (bbox_expanded_l, bbox_expanded_t, bbox_expanded_r, bbox_expanded_b, bbox_expanded_conf, bbox_expanded_cls) = bbox_expanded.split(1, dim=-1)\n            bbox_expanded_midx = (bbox_expanded_l + bbox_expanded_r) / 2\n            bbox_expanded_midy = (bbox_expanded_t + bbox_expanded_b) / 2\n            seed_2d_in_bbox_x = (seed_2d_expanded_x > bbox_expanded_l) * (seed_2d_expanded_x < bbox_expanded_r)\n            seed_2d_in_bbox_y = (seed_2d_expanded_y > bbox_expanded_t) * (seed_2d_expanded_y < bbox_expanded_b)\n            seed_2d_in_bbox = seed_2d_in_bbox_x * seed_2d_in_bbox_y\n            sem_cue = torch.zeros_like(bbox_expanded_conf).expand(-1, -1, self.num_classes)\n            sem_cue = sem_cue.scatter(-1, bbox_expanded_cls.long(), bbox_expanded_conf)\n            delta_u = bbox_expanded_midx - seed_2d_expanded_x\n            delta_v = bbox_expanded_midy - seed_2d_expanded_y\n            seed_3d_expanded = seed_3d_depth.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            z_cam = z_cam.view(seed_num, 1, 1).expand(-1, bbox_num, -1)\n            imvote = torch.cat([delta_u, delta_v, torch.zeros_like(delta_v)], dim=-1).view(-1, 3)\n            imvote = imvote * z_cam.reshape(-1, 1)\n            imvote = imvote @ torch.inverse(depth2img.t())\n            imvote = apply_3d_transformation(imvote, 'DEPTH', img_meta, reverse=False)\n            seed_3d_expanded = seed_3d_expanded.reshape(imvote.shape)\n            ray_angle = seed_3d_expanded + imvote\n            ray_angle /= torch.sqrt(torch.sum(ray_angle ** 2, -1) + EPS).unsqueeze(-1)\n            xz = ray_angle[:, [0, 2]] / (ray_angle[:, [1]] + EPS) * seed_3d_expanded[:, [1]] - seed_3d_expanded[:, [0, 2]]\n            geo_cue = torch.cat([xz, ray_angle], dim=-1).view(seed_num, -1, 5)\n            two_cues = torch.cat([geo_cue, sem_cue], dim=-1)\n            two_cues = two_cues * seed_2d_in_bbox.float()\n            feature_size = two_cues.shape[-1]\n            if bbox_num < self.max_imvote_per_pixel:\n                append_num = self.max_imvote_per_pixel - bbox_num\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=seed_2d_in_bbox.device).bool()\n                seed_2d_in_bbox = torch.cat([seed_2d_in_bbox, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, feature_size), device=two_cues.device)\n                two_cues = torch.cat([two_cues, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=two_cues.device)\n                bbox_expanded_conf = torch.cat([bbox_expanded_conf, append_zeros], dim=1)\n            pair_score = seed_2d_in_bbox.float() + bbox_expanded_conf\n            (mask, indices) = pair_score.topk(self.max_imvote_per_pixel, dim=1, largest=True, sorted=True)\n            indices_img = indices.expand(-1, -1, feature_size)\n            two_cues = two_cues.gather(dim=1, index=indices_img)\n            two_cues = two_cues.transpose(1, 0)\n            two_cues = two_cues.reshape(-1, feature_size).transpose(1, 0).contiguous()\n            mask = mask.floor().int()\n            mask = mask.transpose(1, 0).reshape(-1).bool()\n        img = img[:, :img_shape[0], :img_shape[1]]\n        img_flatten = img.reshape(3, -1).float()\n        img_flatten /= 255.0\n        uv_rescaled[:, 0] = torch.clamp(uv_rescaled[:, 0].round(), 0, img_shape[1] - 1)\n        uv_rescaled[:, 1] = torch.clamp(uv_rescaled[:, 1].round(), 0, img_shape[0] - 1)\n        uv_flatten = uv_rescaled[:, 1].round() * img_shape[1] + uv_rescaled[:, 0].round()\n        uv_expanded = uv_flatten.unsqueeze(0).expand(3, -1).long()\n        txt_cue = torch.gather(img_flatten, dim=-1, index=uv_expanded)\n        txt_cue = txt_cue.unsqueeze(1).expand(-1, self.max_imvote_per_pixel, -1).reshape(3, -1)\n        img_feature = torch.cat([two_cues, txt_cue], dim=0)\n        img_features.append(img_feature)\n        masks.append(mask)\n    return (torch.stack(img_features, 0), torch.stack(masks, 0))",
            "def forward(self, imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward function.\\n\\n        Args:\\n            imgs (list[torch.Tensor]): Image features.\\n            bboxes_2d_rescaled (list[torch.Tensor]): 2D bboxes.\\n            seeds_3d_depth (torch.Tensor): 3D seeds.\\n            img_metas (list[dict]): Meta information of images.\\n\\n        Returns:\\n            torch.Tensor: Concatenated cues of each point.\\n            torch.Tensor: Validity mask of each feature.\\n        '\n    img_features = []\n    masks = []\n    for (i, data) in enumerate(zip(imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas)):\n        (img, bbox_2d_rescaled, seed_3d_depth, img_meta) = data\n        bbox_num = bbox_2d_rescaled.shape[0]\n        seed_num = seed_3d_depth.shape[0]\n        img_shape = img_meta['img_shape']\n        (img_h, img_w, _) = img_shape\n        xyz_depth = apply_3d_transformation(seed_3d_depth, 'DEPTH', img_meta, reverse=True)\n        depth2img = xyz_depth.new_tensor(img_meta['depth2img'])\n        uvz_origin = points_cam2img(xyz_depth, depth2img, True)\n        z_cam = uvz_origin[..., 2]\n        uv_origin = (uvz_origin[..., :2] - 1).round()\n        uv_rescaled = coord_2d_transform(img_meta, uv_origin, True)\n        bbox_2d_origin = bbox_2d_transform(img_meta, bbox_2d_rescaled, False)\n        if bbox_num == 0:\n            imvote_num = seed_num * self.max_imvote_per_pixel\n            two_cues = torch.zeros((15, imvote_num), device=seed_3d_depth.device)\n            mask_zero = torch.zeros(imvote_num - seed_num, device=seed_3d_depth.device).bool()\n            mask_one = torch.ones(seed_num, device=seed_3d_depth.device).bool()\n            mask = torch.cat([mask_one, mask_zero], dim=0)\n        else:\n            bbox_expanded = bbox_2d_origin.view(1, bbox_num, -1).expand(seed_num, -1, -1)\n            seed_2d_expanded = uv_origin.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            (seed_2d_expanded_x, seed_2d_expanded_y) = seed_2d_expanded.split(1, dim=-1)\n            (bbox_expanded_l, bbox_expanded_t, bbox_expanded_r, bbox_expanded_b, bbox_expanded_conf, bbox_expanded_cls) = bbox_expanded.split(1, dim=-1)\n            bbox_expanded_midx = (bbox_expanded_l + bbox_expanded_r) / 2\n            bbox_expanded_midy = (bbox_expanded_t + bbox_expanded_b) / 2\n            seed_2d_in_bbox_x = (seed_2d_expanded_x > bbox_expanded_l) * (seed_2d_expanded_x < bbox_expanded_r)\n            seed_2d_in_bbox_y = (seed_2d_expanded_y > bbox_expanded_t) * (seed_2d_expanded_y < bbox_expanded_b)\n            seed_2d_in_bbox = seed_2d_in_bbox_x * seed_2d_in_bbox_y\n            sem_cue = torch.zeros_like(bbox_expanded_conf).expand(-1, -1, self.num_classes)\n            sem_cue = sem_cue.scatter(-1, bbox_expanded_cls.long(), bbox_expanded_conf)\n            delta_u = bbox_expanded_midx - seed_2d_expanded_x\n            delta_v = bbox_expanded_midy - seed_2d_expanded_y\n            seed_3d_expanded = seed_3d_depth.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            z_cam = z_cam.view(seed_num, 1, 1).expand(-1, bbox_num, -1)\n            imvote = torch.cat([delta_u, delta_v, torch.zeros_like(delta_v)], dim=-1).view(-1, 3)\n            imvote = imvote * z_cam.reshape(-1, 1)\n            imvote = imvote @ torch.inverse(depth2img.t())\n            imvote = apply_3d_transformation(imvote, 'DEPTH', img_meta, reverse=False)\n            seed_3d_expanded = seed_3d_expanded.reshape(imvote.shape)\n            ray_angle = seed_3d_expanded + imvote\n            ray_angle /= torch.sqrt(torch.sum(ray_angle ** 2, -1) + EPS).unsqueeze(-1)\n            xz = ray_angle[:, [0, 2]] / (ray_angle[:, [1]] + EPS) * seed_3d_expanded[:, [1]] - seed_3d_expanded[:, [0, 2]]\n            geo_cue = torch.cat([xz, ray_angle], dim=-1).view(seed_num, -1, 5)\n            two_cues = torch.cat([geo_cue, sem_cue], dim=-1)\n            two_cues = two_cues * seed_2d_in_bbox.float()\n            feature_size = two_cues.shape[-1]\n            if bbox_num < self.max_imvote_per_pixel:\n                append_num = self.max_imvote_per_pixel - bbox_num\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=seed_2d_in_bbox.device).bool()\n                seed_2d_in_bbox = torch.cat([seed_2d_in_bbox, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, feature_size), device=two_cues.device)\n                two_cues = torch.cat([two_cues, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=two_cues.device)\n                bbox_expanded_conf = torch.cat([bbox_expanded_conf, append_zeros], dim=1)\n            pair_score = seed_2d_in_bbox.float() + bbox_expanded_conf\n            (mask, indices) = pair_score.topk(self.max_imvote_per_pixel, dim=1, largest=True, sorted=True)\n            indices_img = indices.expand(-1, -1, feature_size)\n            two_cues = two_cues.gather(dim=1, index=indices_img)\n            two_cues = two_cues.transpose(1, 0)\n            two_cues = two_cues.reshape(-1, feature_size).transpose(1, 0).contiguous()\n            mask = mask.floor().int()\n            mask = mask.transpose(1, 0).reshape(-1).bool()\n        img = img[:, :img_shape[0], :img_shape[1]]\n        img_flatten = img.reshape(3, -1).float()\n        img_flatten /= 255.0\n        uv_rescaled[:, 0] = torch.clamp(uv_rescaled[:, 0].round(), 0, img_shape[1] - 1)\n        uv_rescaled[:, 1] = torch.clamp(uv_rescaled[:, 1].round(), 0, img_shape[0] - 1)\n        uv_flatten = uv_rescaled[:, 1].round() * img_shape[1] + uv_rescaled[:, 0].round()\n        uv_expanded = uv_flatten.unsqueeze(0).expand(3, -1).long()\n        txt_cue = torch.gather(img_flatten, dim=-1, index=uv_expanded)\n        txt_cue = txt_cue.unsqueeze(1).expand(-1, self.max_imvote_per_pixel, -1).reshape(3, -1)\n        img_feature = torch.cat([two_cues, txt_cue], dim=0)\n        img_features.append(img_feature)\n        masks.append(mask)\n    return (torch.stack(img_features, 0), torch.stack(masks, 0))",
            "def forward(self, imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward function.\\n\\n        Args:\\n            imgs (list[torch.Tensor]): Image features.\\n            bboxes_2d_rescaled (list[torch.Tensor]): 2D bboxes.\\n            seeds_3d_depth (torch.Tensor): 3D seeds.\\n            img_metas (list[dict]): Meta information of images.\\n\\n        Returns:\\n            torch.Tensor: Concatenated cues of each point.\\n            torch.Tensor: Validity mask of each feature.\\n        '\n    img_features = []\n    masks = []\n    for (i, data) in enumerate(zip(imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas)):\n        (img, bbox_2d_rescaled, seed_3d_depth, img_meta) = data\n        bbox_num = bbox_2d_rescaled.shape[0]\n        seed_num = seed_3d_depth.shape[0]\n        img_shape = img_meta['img_shape']\n        (img_h, img_w, _) = img_shape\n        xyz_depth = apply_3d_transformation(seed_3d_depth, 'DEPTH', img_meta, reverse=True)\n        depth2img = xyz_depth.new_tensor(img_meta['depth2img'])\n        uvz_origin = points_cam2img(xyz_depth, depth2img, True)\n        z_cam = uvz_origin[..., 2]\n        uv_origin = (uvz_origin[..., :2] - 1).round()\n        uv_rescaled = coord_2d_transform(img_meta, uv_origin, True)\n        bbox_2d_origin = bbox_2d_transform(img_meta, bbox_2d_rescaled, False)\n        if bbox_num == 0:\n            imvote_num = seed_num * self.max_imvote_per_pixel\n            two_cues = torch.zeros((15, imvote_num), device=seed_3d_depth.device)\n            mask_zero = torch.zeros(imvote_num - seed_num, device=seed_3d_depth.device).bool()\n            mask_one = torch.ones(seed_num, device=seed_3d_depth.device).bool()\n            mask = torch.cat([mask_one, mask_zero], dim=0)\n        else:\n            bbox_expanded = bbox_2d_origin.view(1, bbox_num, -1).expand(seed_num, -1, -1)\n            seed_2d_expanded = uv_origin.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            (seed_2d_expanded_x, seed_2d_expanded_y) = seed_2d_expanded.split(1, dim=-1)\n            (bbox_expanded_l, bbox_expanded_t, bbox_expanded_r, bbox_expanded_b, bbox_expanded_conf, bbox_expanded_cls) = bbox_expanded.split(1, dim=-1)\n            bbox_expanded_midx = (bbox_expanded_l + bbox_expanded_r) / 2\n            bbox_expanded_midy = (bbox_expanded_t + bbox_expanded_b) / 2\n            seed_2d_in_bbox_x = (seed_2d_expanded_x > bbox_expanded_l) * (seed_2d_expanded_x < bbox_expanded_r)\n            seed_2d_in_bbox_y = (seed_2d_expanded_y > bbox_expanded_t) * (seed_2d_expanded_y < bbox_expanded_b)\n            seed_2d_in_bbox = seed_2d_in_bbox_x * seed_2d_in_bbox_y\n            sem_cue = torch.zeros_like(bbox_expanded_conf).expand(-1, -1, self.num_classes)\n            sem_cue = sem_cue.scatter(-1, bbox_expanded_cls.long(), bbox_expanded_conf)\n            delta_u = bbox_expanded_midx - seed_2d_expanded_x\n            delta_v = bbox_expanded_midy - seed_2d_expanded_y\n            seed_3d_expanded = seed_3d_depth.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            z_cam = z_cam.view(seed_num, 1, 1).expand(-1, bbox_num, -1)\n            imvote = torch.cat([delta_u, delta_v, torch.zeros_like(delta_v)], dim=-1).view(-1, 3)\n            imvote = imvote * z_cam.reshape(-1, 1)\n            imvote = imvote @ torch.inverse(depth2img.t())\n            imvote = apply_3d_transformation(imvote, 'DEPTH', img_meta, reverse=False)\n            seed_3d_expanded = seed_3d_expanded.reshape(imvote.shape)\n            ray_angle = seed_3d_expanded + imvote\n            ray_angle /= torch.sqrt(torch.sum(ray_angle ** 2, -1) + EPS).unsqueeze(-1)\n            xz = ray_angle[:, [0, 2]] / (ray_angle[:, [1]] + EPS) * seed_3d_expanded[:, [1]] - seed_3d_expanded[:, [0, 2]]\n            geo_cue = torch.cat([xz, ray_angle], dim=-1).view(seed_num, -1, 5)\n            two_cues = torch.cat([geo_cue, sem_cue], dim=-1)\n            two_cues = two_cues * seed_2d_in_bbox.float()\n            feature_size = two_cues.shape[-1]\n            if bbox_num < self.max_imvote_per_pixel:\n                append_num = self.max_imvote_per_pixel - bbox_num\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=seed_2d_in_bbox.device).bool()\n                seed_2d_in_bbox = torch.cat([seed_2d_in_bbox, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, feature_size), device=two_cues.device)\n                two_cues = torch.cat([two_cues, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=two_cues.device)\n                bbox_expanded_conf = torch.cat([bbox_expanded_conf, append_zeros], dim=1)\n            pair_score = seed_2d_in_bbox.float() + bbox_expanded_conf\n            (mask, indices) = pair_score.topk(self.max_imvote_per_pixel, dim=1, largest=True, sorted=True)\n            indices_img = indices.expand(-1, -1, feature_size)\n            two_cues = two_cues.gather(dim=1, index=indices_img)\n            two_cues = two_cues.transpose(1, 0)\n            two_cues = two_cues.reshape(-1, feature_size).transpose(1, 0).contiguous()\n            mask = mask.floor().int()\n            mask = mask.transpose(1, 0).reshape(-1).bool()\n        img = img[:, :img_shape[0], :img_shape[1]]\n        img_flatten = img.reshape(3, -1).float()\n        img_flatten /= 255.0\n        uv_rescaled[:, 0] = torch.clamp(uv_rescaled[:, 0].round(), 0, img_shape[1] - 1)\n        uv_rescaled[:, 1] = torch.clamp(uv_rescaled[:, 1].round(), 0, img_shape[0] - 1)\n        uv_flatten = uv_rescaled[:, 1].round() * img_shape[1] + uv_rescaled[:, 0].round()\n        uv_expanded = uv_flatten.unsqueeze(0).expand(3, -1).long()\n        txt_cue = torch.gather(img_flatten, dim=-1, index=uv_expanded)\n        txt_cue = txt_cue.unsqueeze(1).expand(-1, self.max_imvote_per_pixel, -1).reshape(3, -1)\n        img_feature = torch.cat([two_cues, txt_cue], dim=0)\n        img_features.append(img_feature)\n        masks.append(mask)\n    return (torch.stack(img_features, 0), torch.stack(masks, 0))",
            "def forward(self, imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward function.\\n\\n        Args:\\n            imgs (list[torch.Tensor]): Image features.\\n            bboxes_2d_rescaled (list[torch.Tensor]): 2D bboxes.\\n            seeds_3d_depth (torch.Tensor): 3D seeds.\\n            img_metas (list[dict]): Meta information of images.\\n\\n        Returns:\\n            torch.Tensor: Concatenated cues of each point.\\n            torch.Tensor: Validity mask of each feature.\\n        '\n    img_features = []\n    masks = []\n    for (i, data) in enumerate(zip(imgs, bboxes_2d_rescaled, seeds_3d_depth, img_metas)):\n        (img, bbox_2d_rescaled, seed_3d_depth, img_meta) = data\n        bbox_num = bbox_2d_rescaled.shape[0]\n        seed_num = seed_3d_depth.shape[0]\n        img_shape = img_meta['img_shape']\n        (img_h, img_w, _) = img_shape\n        xyz_depth = apply_3d_transformation(seed_3d_depth, 'DEPTH', img_meta, reverse=True)\n        depth2img = xyz_depth.new_tensor(img_meta['depth2img'])\n        uvz_origin = points_cam2img(xyz_depth, depth2img, True)\n        z_cam = uvz_origin[..., 2]\n        uv_origin = (uvz_origin[..., :2] - 1).round()\n        uv_rescaled = coord_2d_transform(img_meta, uv_origin, True)\n        bbox_2d_origin = bbox_2d_transform(img_meta, bbox_2d_rescaled, False)\n        if bbox_num == 0:\n            imvote_num = seed_num * self.max_imvote_per_pixel\n            two_cues = torch.zeros((15, imvote_num), device=seed_3d_depth.device)\n            mask_zero = torch.zeros(imvote_num - seed_num, device=seed_3d_depth.device).bool()\n            mask_one = torch.ones(seed_num, device=seed_3d_depth.device).bool()\n            mask = torch.cat([mask_one, mask_zero], dim=0)\n        else:\n            bbox_expanded = bbox_2d_origin.view(1, bbox_num, -1).expand(seed_num, -1, -1)\n            seed_2d_expanded = uv_origin.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            (seed_2d_expanded_x, seed_2d_expanded_y) = seed_2d_expanded.split(1, dim=-1)\n            (bbox_expanded_l, bbox_expanded_t, bbox_expanded_r, bbox_expanded_b, bbox_expanded_conf, bbox_expanded_cls) = bbox_expanded.split(1, dim=-1)\n            bbox_expanded_midx = (bbox_expanded_l + bbox_expanded_r) / 2\n            bbox_expanded_midy = (bbox_expanded_t + bbox_expanded_b) / 2\n            seed_2d_in_bbox_x = (seed_2d_expanded_x > bbox_expanded_l) * (seed_2d_expanded_x < bbox_expanded_r)\n            seed_2d_in_bbox_y = (seed_2d_expanded_y > bbox_expanded_t) * (seed_2d_expanded_y < bbox_expanded_b)\n            seed_2d_in_bbox = seed_2d_in_bbox_x * seed_2d_in_bbox_y\n            sem_cue = torch.zeros_like(bbox_expanded_conf).expand(-1, -1, self.num_classes)\n            sem_cue = sem_cue.scatter(-1, bbox_expanded_cls.long(), bbox_expanded_conf)\n            delta_u = bbox_expanded_midx - seed_2d_expanded_x\n            delta_v = bbox_expanded_midy - seed_2d_expanded_y\n            seed_3d_expanded = seed_3d_depth.view(seed_num, 1, -1).expand(-1, bbox_num, -1)\n            z_cam = z_cam.view(seed_num, 1, 1).expand(-1, bbox_num, -1)\n            imvote = torch.cat([delta_u, delta_v, torch.zeros_like(delta_v)], dim=-1).view(-1, 3)\n            imvote = imvote * z_cam.reshape(-1, 1)\n            imvote = imvote @ torch.inverse(depth2img.t())\n            imvote = apply_3d_transformation(imvote, 'DEPTH', img_meta, reverse=False)\n            seed_3d_expanded = seed_3d_expanded.reshape(imvote.shape)\n            ray_angle = seed_3d_expanded + imvote\n            ray_angle /= torch.sqrt(torch.sum(ray_angle ** 2, -1) + EPS).unsqueeze(-1)\n            xz = ray_angle[:, [0, 2]] / (ray_angle[:, [1]] + EPS) * seed_3d_expanded[:, [1]] - seed_3d_expanded[:, [0, 2]]\n            geo_cue = torch.cat([xz, ray_angle], dim=-1).view(seed_num, -1, 5)\n            two_cues = torch.cat([geo_cue, sem_cue], dim=-1)\n            two_cues = two_cues * seed_2d_in_bbox.float()\n            feature_size = two_cues.shape[-1]\n            if bbox_num < self.max_imvote_per_pixel:\n                append_num = self.max_imvote_per_pixel - bbox_num\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=seed_2d_in_bbox.device).bool()\n                seed_2d_in_bbox = torch.cat([seed_2d_in_bbox, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, feature_size), device=two_cues.device)\n                two_cues = torch.cat([two_cues, append_zeros], dim=1)\n                append_zeros = torch.zeros((seed_num, append_num, 1), device=two_cues.device)\n                bbox_expanded_conf = torch.cat([bbox_expanded_conf, append_zeros], dim=1)\n            pair_score = seed_2d_in_bbox.float() + bbox_expanded_conf\n            (mask, indices) = pair_score.topk(self.max_imvote_per_pixel, dim=1, largest=True, sorted=True)\n            indices_img = indices.expand(-1, -1, feature_size)\n            two_cues = two_cues.gather(dim=1, index=indices_img)\n            two_cues = two_cues.transpose(1, 0)\n            two_cues = two_cues.reshape(-1, feature_size).transpose(1, 0).contiguous()\n            mask = mask.floor().int()\n            mask = mask.transpose(1, 0).reshape(-1).bool()\n        img = img[:, :img_shape[0], :img_shape[1]]\n        img_flatten = img.reshape(3, -1).float()\n        img_flatten /= 255.0\n        uv_rescaled[:, 0] = torch.clamp(uv_rescaled[:, 0].round(), 0, img_shape[1] - 1)\n        uv_rescaled[:, 1] = torch.clamp(uv_rescaled[:, 1].round(), 0, img_shape[0] - 1)\n        uv_flatten = uv_rescaled[:, 1].round() * img_shape[1] + uv_rescaled[:, 0].round()\n        uv_expanded = uv_flatten.unsqueeze(0).expand(3, -1).long()\n        txt_cue = torch.gather(img_flatten, dim=-1, index=uv_expanded)\n        txt_cue = txt_cue.unsqueeze(1).expand(-1, self.max_imvote_per_pixel, -1).reshape(3, -1)\n        img_feature = torch.cat([two_cues, txt_cue], dim=0)\n        img_features.append(img_feature)\n        masks.append(mask)\n    return (torch.stack(img_features, 0), torch.stack(masks, 0))"
        ]
    }
]