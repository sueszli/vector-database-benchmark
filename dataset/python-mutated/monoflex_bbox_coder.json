[
    {
        "func_name": "__init__",
        "original": "def __init__(self, depth_mode, base_depth, depth_range, combine_depth, uncertainty_range, base_dims, dims_mode, multibin, num_dir_bins, bin_centers, bin_margin, code_size, eps=0.001):\n    super(MonoFlexCoder, self).__init__()\n    self.depth_mode = depth_mode\n    self.base_depth = base_depth\n    self.depth_range = depth_range\n    self.combine_depth = combine_depth\n    self.uncertainty_range = uncertainty_range\n    self.base_dims = base_dims\n    self.dims_mode = dims_mode\n    self.multibin = multibin\n    self.num_dir_bins = num_dir_bins\n    self.bin_centers = bin_centers\n    self.bin_margin = bin_margin\n    self.bbox_code_size = code_size\n    self.eps = eps",
        "mutated": [
            "def __init__(self, depth_mode, base_depth, depth_range, combine_depth, uncertainty_range, base_dims, dims_mode, multibin, num_dir_bins, bin_centers, bin_margin, code_size, eps=0.001):\n    if False:\n        i = 10\n    super(MonoFlexCoder, self).__init__()\n    self.depth_mode = depth_mode\n    self.base_depth = base_depth\n    self.depth_range = depth_range\n    self.combine_depth = combine_depth\n    self.uncertainty_range = uncertainty_range\n    self.base_dims = base_dims\n    self.dims_mode = dims_mode\n    self.multibin = multibin\n    self.num_dir_bins = num_dir_bins\n    self.bin_centers = bin_centers\n    self.bin_margin = bin_margin\n    self.bbox_code_size = code_size\n    self.eps = eps",
            "def __init__(self, depth_mode, base_depth, depth_range, combine_depth, uncertainty_range, base_dims, dims_mode, multibin, num_dir_bins, bin_centers, bin_margin, code_size, eps=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(MonoFlexCoder, self).__init__()\n    self.depth_mode = depth_mode\n    self.base_depth = base_depth\n    self.depth_range = depth_range\n    self.combine_depth = combine_depth\n    self.uncertainty_range = uncertainty_range\n    self.base_dims = base_dims\n    self.dims_mode = dims_mode\n    self.multibin = multibin\n    self.num_dir_bins = num_dir_bins\n    self.bin_centers = bin_centers\n    self.bin_margin = bin_margin\n    self.bbox_code_size = code_size\n    self.eps = eps",
            "def __init__(self, depth_mode, base_depth, depth_range, combine_depth, uncertainty_range, base_dims, dims_mode, multibin, num_dir_bins, bin_centers, bin_margin, code_size, eps=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(MonoFlexCoder, self).__init__()\n    self.depth_mode = depth_mode\n    self.base_depth = base_depth\n    self.depth_range = depth_range\n    self.combine_depth = combine_depth\n    self.uncertainty_range = uncertainty_range\n    self.base_dims = base_dims\n    self.dims_mode = dims_mode\n    self.multibin = multibin\n    self.num_dir_bins = num_dir_bins\n    self.bin_centers = bin_centers\n    self.bin_margin = bin_margin\n    self.bbox_code_size = code_size\n    self.eps = eps",
            "def __init__(self, depth_mode, base_depth, depth_range, combine_depth, uncertainty_range, base_dims, dims_mode, multibin, num_dir_bins, bin_centers, bin_margin, code_size, eps=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(MonoFlexCoder, self).__init__()\n    self.depth_mode = depth_mode\n    self.base_depth = base_depth\n    self.depth_range = depth_range\n    self.combine_depth = combine_depth\n    self.uncertainty_range = uncertainty_range\n    self.base_dims = base_dims\n    self.dims_mode = dims_mode\n    self.multibin = multibin\n    self.num_dir_bins = num_dir_bins\n    self.bin_centers = bin_centers\n    self.bin_margin = bin_margin\n    self.bbox_code_size = code_size\n    self.eps = eps",
            "def __init__(self, depth_mode, base_depth, depth_range, combine_depth, uncertainty_range, base_dims, dims_mode, multibin, num_dir_bins, bin_centers, bin_margin, code_size, eps=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(MonoFlexCoder, self).__init__()\n    self.depth_mode = depth_mode\n    self.base_depth = base_depth\n    self.depth_range = depth_range\n    self.combine_depth = combine_depth\n    self.uncertainty_range = uncertainty_range\n    self.base_dims = base_dims\n    self.dims_mode = dims_mode\n    self.multibin = multibin\n    self.num_dir_bins = num_dir_bins\n    self.bin_centers = bin_centers\n    self.bin_margin = bin_margin\n    self.bbox_code_size = code_size\n    self.eps = eps"
        ]
    },
    {
        "func_name": "encode",
        "original": "def encode(self, gt_bboxes_3d):\n    \"\"\"Encode ground truth to prediction targets.\n\n        Args:\n            gt_bboxes_3d (`BaseInstance3DBoxes`): Ground truth 3D bboxes.\n                shape: (N, 7).\n\n        Returns:\n            torch.Tensor: Targets of orientations.\n        \"\"\"\n    local_yaw = gt_bboxes_3d.local_yaw\n    encode_local_yaw = local_yaw.new_zeros([local_yaw.shape[0], self.num_dir_bins * 2])\n    bin_size = 2 * np.pi / self.num_dir_bins\n    margin_size = bin_size * self.bin_margin\n    bin_centers = local_yaw.new_tensor(self.bin_centers)\n    range_size = bin_size / 2 + margin_size\n    offsets = local_yaw.unsqueeze(1) - bin_centers.unsqueeze(0)\n    offsets[offsets > np.pi] = offsets[offsets > np.pi] - 2 * np.pi\n    offsets[offsets < -np.pi] = offsets[offsets < -np.pi] + 2 * np.pi\n    for i in range(self.num_dir_bins):\n        offset = offsets[:, i]\n        inds = abs(offset) < range_size\n        encode_local_yaw[inds, i] = 1\n        encode_local_yaw[inds, i + self.num_dir_bins] = offset[inds]\n    orientation_target = encode_local_yaw\n    return orientation_target",
        "mutated": [
            "def encode(self, gt_bboxes_3d):\n    if False:\n        i = 10\n    'Encode ground truth to prediction targets.\\n\\n        Args:\\n            gt_bboxes_3d (`BaseInstance3DBoxes`): Ground truth 3D bboxes.\\n                shape: (N, 7).\\n\\n        Returns:\\n            torch.Tensor: Targets of orientations.\\n        '\n    local_yaw = gt_bboxes_3d.local_yaw\n    encode_local_yaw = local_yaw.new_zeros([local_yaw.shape[0], self.num_dir_bins * 2])\n    bin_size = 2 * np.pi / self.num_dir_bins\n    margin_size = bin_size * self.bin_margin\n    bin_centers = local_yaw.new_tensor(self.bin_centers)\n    range_size = bin_size / 2 + margin_size\n    offsets = local_yaw.unsqueeze(1) - bin_centers.unsqueeze(0)\n    offsets[offsets > np.pi] = offsets[offsets > np.pi] - 2 * np.pi\n    offsets[offsets < -np.pi] = offsets[offsets < -np.pi] + 2 * np.pi\n    for i in range(self.num_dir_bins):\n        offset = offsets[:, i]\n        inds = abs(offset) < range_size\n        encode_local_yaw[inds, i] = 1\n        encode_local_yaw[inds, i + self.num_dir_bins] = offset[inds]\n    orientation_target = encode_local_yaw\n    return orientation_target",
            "def encode(self, gt_bboxes_3d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Encode ground truth to prediction targets.\\n\\n        Args:\\n            gt_bboxes_3d (`BaseInstance3DBoxes`): Ground truth 3D bboxes.\\n                shape: (N, 7).\\n\\n        Returns:\\n            torch.Tensor: Targets of orientations.\\n        '\n    local_yaw = gt_bboxes_3d.local_yaw\n    encode_local_yaw = local_yaw.new_zeros([local_yaw.shape[0], self.num_dir_bins * 2])\n    bin_size = 2 * np.pi / self.num_dir_bins\n    margin_size = bin_size * self.bin_margin\n    bin_centers = local_yaw.new_tensor(self.bin_centers)\n    range_size = bin_size / 2 + margin_size\n    offsets = local_yaw.unsqueeze(1) - bin_centers.unsqueeze(0)\n    offsets[offsets > np.pi] = offsets[offsets > np.pi] - 2 * np.pi\n    offsets[offsets < -np.pi] = offsets[offsets < -np.pi] + 2 * np.pi\n    for i in range(self.num_dir_bins):\n        offset = offsets[:, i]\n        inds = abs(offset) < range_size\n        encode_local_yaw[inds, i] = 1\n        encode_local_yaw[inds, i + self.num_dir_bins] = offset[inds]\n    orientation_target = encode_local_yaw\n    return orientation_target",
            "def encode(self, gt_bboxes_3d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Encode ground truth to prediction targets.\\n\\n        Args:\\n            gt_bboxes_3d (`BaseInstance3DBoxes`): Ground truth 3D bboxes.\\n                shape: (N, 7).\\n\\n        Returns:\\n            torch.Tensor: Targets of orientations.\\n        '\n    local_yaw = gt_bboxes_3d.local_yaw\n    encode_local_yaw = local_yaw.new_zeros([local_yaw.shape[0], self.num_dir_bins * 2])\n    bin_size = 2 * np.pi / self.num_dir_bins\n    margin_size = bin_size * self.bin_margin\n    bin_centers = local_yaw.new_tensor(self.bin_centers)\n    range_size = bin_size / 2 + margin_size\n    offsets = local_yaw.unsqueeze(1) - bin_centers.unsqueeze(0)\n    offsets[offsets > np.pi] = offsets[offsets > np.pi] - 2 * np.pi\n    offsets[offsets < -np.pi] = offsets[offsets < -np.pi] + 2 * np.pi\n    for i in range(self.num_dir_bins):\n        offset = offsets[:, i]\n        inds = abs(offset) < range_size\n        encode_local_yaw[inds, i] = 1\n        encode_local_yaw[inds, i + self.num_dir_bins] = offset[inds]\n    orientation_target = encode_local_yaw\n    return orientation_target",
            "def encode(self, gt_bboxes_3d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Encode ground truth to prediction targets.\\n\\n        Args:\\n            gt_bboxes_3d (`BaseInstance3DBoxes`): Ground truth 3D bboxes.\\n                shape: (N, 7).\\n\\n        Returns:\\n            torch.Tensor: Targets of orientations.\\n        '\n    local_yaw = gt_bboxes_3d.local_yaw\n    encode_local_yaw = local_yaw.new_zeros([local_yaw.shape[0], self.num_dir_bins * 2])\n    bin_size = 2 * np.pi / self.num_dir_bins\n    margin_size = bin_size * self.bin_margin\n    bin_centers = local_yaw.new_tensor(self.bin_centers)\n    range_size = bin_size / 2 + margin_size\n    offsets = local_yaw.unsqueeze(1) - bin_centers.unsqueeze(0)\n    offsets[offsets > np.pi] = offsets[offsets > np.pi] - 2 * np.pi\n    offsets[offsets < -np.pi] = offsets[offsets < -np.pi] + 2 * np.pi\n    for i in range(self.num_dir_bins):\n        offset = offsets[:, i]\n        inds = abs(offset) < range_size\n        encode_local_yaw[inds, i] = 1\n        encode_local_yaw[inds, i + self.num_dir_bins] = offset[inds]\n    orientation_target = encode_local_yaw\n    return orientation_target",
            "def encode(self, gt_bboxes_3d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Encode ground truth to prediction targets.\\n\\n        Args:\\n            gt_bboxes_3d (`BaseInstance3DBoxes`): Ground truth 3D bboxes.\\n                shape: (N, 7).\\n\\n        Returns:\\n            torch.Tensor: Targets of orientations.\\n        '\n    local_yaw = gt_bboxes_3d.local_yaw\n    encode_local_yaw = local_yaw.new_zeros([local_yaw.shape[0], self.num_dir_bins * 2])\n    bin_size = 2 * np.pi / self.num_dir_bins\n    margin_size = bin_size * self.bin_margin\n    bin_centers = local_yaw.new_tensor(self.bin_centers)\n    range_size = bin_size / 2 + margin_size\n    offsets = local_yaw.unsqueeze(1) - bin_centers.unsqueeze(0)\n    offsets[offsets > np.pi] = offsets[offsets > np.pi] - 2 * np.pi\n    offsets[offsets < -np.pi] = offsets[offsets < -np.pi] + 2 * np.pi\n    for i in range(self.num_dir_bins):\n        offset = offsets[:, i]\n        inds = abs(offset) < range_size\n        encode_local_yaw[inds, i] = 1\n        encode_local_yaw[inds, i + self.num_dir_bins] = offset[inds]\n    orientation_target = encode_local_yaw\n    return orientation_target"
        ]
    },
    {
        "func_name": "decode",
        "original": "def decode(self, bbox, base_centers2d, labels, downsample_ratio, cam2imgs):\n    \"\"\"Decode bounding box regression into 3D predictions.\n\n        Args:\n            bbox (Tensor): Raw bounding box predictions for each\n                predict center2d point.\n                shape: (N, C)\n            base_centers2d (torch.Tensor): Base centers2d for 3D bboxes.\n                shape: (N, 2).\n            labels (Tensor): Batch predict class label for each predict\n                center2d point.\n                shape: (N, )\n            downsample_ratio (int): The stride of feature map.\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\n\n        Return:\n            dict: The 3D prediction dict decoded from regression map.\n            the dict has components below:\n                - bboxes2d (torch.Tensor): Decoded [x1, y1, x2, y2] format\n                    2D bboxes.\n                - dimensions (torch.Tensor): Decoded dimensions for each\n                    object.\n                - offsets2d (torch.Tenosr): Offsets between base centers2d\n                    and real centers2d.\n                - direct_depth (torch.Tensor): Decoded directly regressed\n                    depth.\n                - keypoints2d (torch.Tensor): Keypoints of each projected\n                    3D box on image.\n                - keypoints_depth (torch.Tensor): Decoded depth from keypoints.\n                - combined_depth (torch.Tensor): Combined depth using direct\n                    depth and keypoints depth with depth uncertainty.\n                - orientations (torch.Tensor): Multibin format orientations\n                    (local yaw) for each objects.\n        \"\"\"\n    pred_bboxes2d = bbox[:, 0:4]\n    pred_bboxes2d = self.decode_bboxes2d(pred_bboxes2d, base_centers2d)\n    pred_offsets2d = bbox[:, 4:6]\n    pred_dimensions_offsets3d = bbox[:, 29:32]\n    pred_orientations = torch.cat((bbox[:, 32:40], bbox[:, 40:48]), dim=1)\n    pred_keypoints_depth_uncertainty = bbox[:, 26:29]\n    pred_direct_depth_uncertainty = bbox[:, 49:50].squeeze(-1)\n    pred_keypoints2d = bbox[:, 6:26].reshape(-1, 10, 2)\n    pred_direct_depth_offsets = bbox[:, 48:49].squeeze(-1)\n    pred_dimensions = self.decode_dims(labels, pred_dimensions_offsets3d)\n    pred_direct_depth = self.decode_direct_depth(pred_direct_depth_offsets)\n    pred_keypoints_depth = self.keypoints2depth(pred_keypoints2d, pred_dimensions, cam2imgs, downsample_ratio)\n    pred_direct_depth_uncertainty = torch.clamp(pred_direct_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    pred_keypoints_depth_uncertainty = torch.clamp(pred_keypoints_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    if self.combine_depth:\n        pred_depth_uncertainty = torch.cat((pred_direct_depth_uncertainty.unsqueeze(-1), pred_keypoints_depth_uncertainty), dim=1).exp()\n        pred_depth = torch.cat((pred_direct_depth.unsqueeze(-1), pred_keypoints_depth), dim=1)\n        pred_combined_depth = self.combine_depths(pred_depth, pred_depth_uncertainty)\n    else:\n        pred_combined_depth = None\n    preds = dict(bboxes2d=pred_bboxes2d, dimensions=pred_dimensions, offsets2d=pred_offsets2d, keypoints2d=pred_keypoints2d, orientations=pred_orientations, direct_depth=pred_direct_depth, keypoints_depth=pred_keypoints_depth, combined_depth=pred_combined_depth, direct_depth_uncertainty=pred_direct_depth_uncertainty, keypoints_depth_uncertainty=pred_keypoints_depth_uncertainty)\n    return preds",
        "mutated": [
            "def decode(self, bbox, base_centers2d, labels, downsample_ratio, cam2imgs):\n    if False:\n        i = 10\n    \"Decode bounding box regression into 3D predictions.\\n\\n        Args:\\n            bbox (Tensor): Raw bounding box predictions for each\\n                predict center2d point.\\n                shape: (N, C)\\n            base_centers2d (torch.Tensor): Base centers2d for 3D bboxes.\\n                shape: (N, 2).\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (N, )\\n            downsample_ratio (int): The stride of feature map.\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n\\n        Return:\\n            dict: The 3D prediction dict decoded from regression map.\\n            the dict has components below:\\n                - bboxes2d (torch.Tensor): Decoded [x1, y1, x2, y2] format\\n                    2D bboxes.\\n                - dimensions (torch.Tensor): Decoded dimensions for each\\n                    object.\\n                - offsets2d (torch.Tenosr): Offsets between base centers2d\\n                    and real centers2d.\\n                - direct_depth (torch.Tensor): Decoded directly regressed\\n                    depth.\\n                - keypoints2d (torch.Tensor): Keypoints of each projected\\n                    3D box on image.\\n                - keypoints_depth (torch.Tensor): Decoded depth from keypoints.\\n                - combined_depth (torch.Tensor): Combined depth using direct\\n                    depth and keypoints depth with depth uncertainty.\\n                - orientations (torch.Tensor): Multibin format orientations\\n                    (local yaw) for each objects.\\n        \"\n    pred_bboxes2d = bbox[:, 0:4]\n    pred_bboxes2d = self.decode_bboxes2d(pred_bboxes2d, base_centers2d)\n    pred_offsets2d = bbox[:, 4:6]\n    pred_dimensions_offsets3d = bbox[:, 29:32]\n    pred_orientations = torch.cat((bbox[:, 32:40], bbox[:, 40:48]), dim=1)\n    pred_keypoints_depth_uncertainty = bbox[:, 26:29]\n    pred_direct_depth_uncertainty = bbox[:, 49:50].squeeze(-1)\n    pred_keypoints2d = bbox[:, 6:26].reshape(-1, 10, 2)\n    pred_direct_depth_offsets = bbox[:, 48:49].squeeze(-1)\n    pred_dimensions = self.decode_dims(labels, pred_dimensions_offsets3d)\n    pred_direct_depth = self.decode_direct_depth(pred_direct_depth_offsets)\n    pred_keypoints_depth = self.keypoints2depth(pred_keypoints2d, pred_dimensions, cam2imgs, downsample_ratio)\n    pred_direct_depth_uncertainty = torch.clamp(pred_direct_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    pred_keypoints_depth_uncertainty = torch.clamp(pred_keypoints_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    if self.combine_depth:\n        pred_depth_uncertainty = torch.cat((pred_direct_depth_uncertainty.unsqueeze(-1), pred_keypoints_depth_uncertainty), dim=1).exp()\n        pred_depth = torch.cat((pred_direct_depth.unsqueeze(-1), pred_keypoints_depth), dim=1)\n        pred_combined_depth = self.combine_depths(pred_depth, pred_depth_uncertainty)\n    else:\n        pred_combined_depth = None\n    preds = dict(bboxes2d=pred_bboxes2d, dimensions=pred_dimensions, offsets2d=pred_offsets2d, keypoints2d=pred_keypoints2d, orientations=pred_orientations, direct_depth=pred_direct_depth, keypoints_depth=pred_keypoints_depth, combined_depth=pred_combined_depth, direct_depth_uncertainty=pred_direct_depth_uncertainty, keypoints_depth_uncertainty=pred_keypoints_depth_uncertainty)\n    return preds",
            "def decode(self, bbox, base_centers2d, labels, downsample_ratio, cam2imgs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Decode bounding box regression into 3D predictions.\\n\\n        Args:\\n            bbox (Tensor): Raw bounding box predictions for each\\n                predict center2d point.\\n                shape: (N, C)\\n            base_centers2d (torch.Tensor): Base centers2d for 3D bboxes.\\n                shape: (N, 2).\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (N, )\\n            downsample_ratio (int): The stride of feature map.\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n\\n        Return:\\n            dict: The 3D prediction dict decoded from regression map.\\n            the dict has components below:\\n                - bboxes2d (torch.Tensor): Decoded [x1, y1, x2, y2] format\\n                    2D bboxes.\\n                - dimensions (torch.Tensor): Decoded dimensions for each\\n                    object.\\n                - offsets2d (torch.Tenosr): Offsets between base centers2d\\n                    and real centers2d.\\n                - direct_depth (torch.Tensor): Decoded directly regressed\\n                    depth.\\n                - keypoints2d (torch.Tensor): Keypoints of each projected\\n                    3D box on image.\\n                - keypoints_depth (torch.Tensor): Decoded depth from keypoints.\\n                - combined_depth (torch.Tensor): Combined depth using direct\\n                    depth and keypoints depth with depth uncertainty.\\n                - orientations (torch.Tensor): Multibin format orientations\\n                    (local yaw) for each objects.\\n        \"\n    pred_bboxes2d = bbox[:, 0:4]\n    pred_bboxes2d = self.decode_bboxes2d(pred_bboxes2d, base_centers2d)\n    pred_offsets2d = bbox[:, 4:6]\n    pred_dimensions_offsets3d = bbox[:, 29:32]\n    pred_orientations = torch.cat((bbox[:, 32:40], bbox[:, 40:48]), dim=1)\n    pred_keypoints_depth_uncertainty = bbox[:, 26:29]\n    pred_direct_depth_uncertainty = bbox[:, 49:50].squeeze(-1)\n    pred_keypoints2d = bbox[:, 6:26].reshape(-1, 10, 2)\n    pred_direct_depth_offsets = bbox[:, 48:49].squeeze(-1)\n    pred_dimensions = self.decode_dims(labels, pred_dimensions_offsets3d)\n    pred_direct_depth = self.decode_direct_depth(pred_direct_depth_offsets)\n    pred_keypoints_depth = self.keypoints2depth(pred_keypoints2d, pred_dimensions, cam2imgs, downsample_ratio)\n    pred_direct_depth_uncertainty = torch.clamp(pred_direct_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    pred_keypoints_depth_uncertainty = torch.clamp(pred_keypoints_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    if self.combine_depth:\n        pred_depth_uncertainty = torch.cat((pred_direct_depth_uncertainty.unsqueeze(-1), pred_keypoints_depth_uncertainty), dim=1).exp()\n        pred_depth = torch.cat((pred_direct_depth.unsqueeze(-1), pred_keypoints_depth), dim=1)\n        pred_combined_depth = self.combine_depths(pred_depth, pred_depth_uncertainty)\n    else:\n        pred_combined_depth = None\n    preds = dict(bboxes2d=pred_bboxes2d, dimensions=pred_dimensions, offsets2d=pred_offsets2d, keypoints2d=pred_keypoints2d, orientations=pred_orientations, direct_depth=pred_direct_depth, keypoints_depth=pred_keypoints_depth, combined_depth=pred_combined_depth, direct_depth_uncertainty=pred_direct_depth_uncertainty, keypoints_depth_uncertainty=pred_keypoints_depth_uncertainty)\n    return preds",
            "def decode(self, bbox, base_centers2d, labels, downsample_ratio, cam2imgs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Decode bounding box regression into 3D predictions.\\n\\n        Args:\\n            bbox (Tensor): Raw bounding box predictions for each\\n                predict center2d point.\\n                shape: (N, C)\\n            base_centers2d (torch.Tensor): Base centers2d for 3D bboxes.\\n                shape: (N, 2).\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (N, )\\n            downsample_ratio (int): The stride of feature map.\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n\\n        Return:\\n            dict: The 3D prediction dict decoded from regression map.\\n            the dict has components below:\\n                - bboxes2d (torch.Tensor): Decoded [x1, y1, x2, y2] format\\n                    2D bboxes.\\n                - dimensions (torch.Tensor): Decoded dimensions for each\\n                    object.\\n                - offsets2d (torch.Tenosr): Offsets between base centers2d\\n                    and real centers2d.\\n                - direct_depth (torch.Tensor): Decoded directly regressed\\n                    depth.\\n                - keypoints2d (torch.Tensor): Keypoints of each projected\\n                    3D box on image.\\n                - keypoints_depth (torch.Tensor): Decoded depth from keypoints.\\n                - combined_depth (torch.Tensor): Combined depth using direct\\n                    depth and keypoints depth with depth uncertainty.\\n                - orientations (torch.Tensor): Multibin format orientations\\n                    (local yaw) for each objects.\\n        \"\n    pred_bboxes2d = bbox[:, 0:4]\n    pred_bboxes2d = self.decode_bboxes2d(pred_bboxes2d, base_centers2d)\n    pred_offsets2d = bbox[:, 4:6]\n    pred_dimensions_offsets3d = bbox[:, 29:32]\n    pred_orientations = torch.cat((bbox[:, 32:40], bbox[:, 40:48]), dim=1)\n    pred_keypoints_depth_uncertainty = bbox[:, 26:29]\n    pred_direct_depth_uncertainty = bbox[:, 49:50].squeeze(-1)\n    pred_keypoints2d = bbox[:, 6:26].reshape(-1, 10, 2)\n    pred_direct_depth_offsets = bbox[:, 48:49].squeeze(-1)\n    pred_dimensions = self.decode_dims(labels, pred_dimensions_offsets3d)\n    pred_direct_depth = self.decode_direct_depth(pred_direct_depth_offsets)\n    pred_keypoints_depth = self.keypoints2depth(pred_keypoints2d, pred_dimensions, cam2imgs, downsample_ratio)\n    pred_direct_depth_uncertainty = torch.clamp(pred_direct_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    pred_keypoints_depth_uncertainty = torch.clamp(pred_keypoints_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    if self.combine_depth:\n        pred_depth_uncertainty = torch.cat((pred_direct_depth_uncertainty.unsqueeze(-1), pred_keypoints_depth_uncertainty), dim=1).exp()\n        pred_depth = torch.cat((pred_direct_depth.unsqueeze(-1), pred_keypoints_depth), dim=1)\n        pred_combined_depth = self.combine_depths(pred_depth, pred_depth_uncertainty)\n    else:\n        pred_combined_depth = None\n    preds = dict(bboxes2d=pred_bboxes2d, dimensions=pred_dimensions, offsets2d=pred_offsets2d, keypoints2d=pred_keypoints2d, orientations=pred_orientations, direct_depth=pred_direct_depth, keypoints_depth=pred_keypoints_depth, combined_depth=pred_combined_depth, direct_depth_uncertainty=pred_direct_depth_uncertainty, keypoints_depth_uncertainty=pred_keypoints_depth_uncertainty)\n    return preds",
            "def decode(self, bbox, base_centers2d, labels, downsample_ratio, cam2imgs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Decode bounding box regression into 3D predictions.\\n\\n        Args:\\n            bbox (Tensor): Raw bounding box predictions for each\\n                predict center2d point.\\n                shape: (N, C)\\n            base_centers2d (torch.Tensor): Base centers2d for 3D bboxes.\\n                shape: (N, 2).\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (N, )\\n            downsample_ratio (int): The stride of feature map.\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n\\n        Return:\\n            dict: The 3D prediction dict decoded from regression map.\\n            the dict has components below:\\n                - bboxes2d (torch.Tensor): Decoded [x1, y1, x2, y2] format\\n                    2D bboxes.\\n                - dimensions (torch.Tensor): Decoded dimensions for each\\n                    object.\\n                - offsets2d (torch.Tenosr): Offsets between base centers2d\\n                    and real centers2d.\\n                - direct_depth (torch.Tensor): Decoded directly regressed\\n                    depth.\\n                - keypoints2d (torch.Tensor): Keypoints of each projected\\n                    3D box on image.\\n                - keypoints_depth (torch.Tensor): Decoded depth from keypoints.\\n                - combined_depth (torch.Tensor): Combined depth using direct\\n                    depth and keypoints depth with depth uncertainty.\\n                - orientations (torch.Tensor): Multibin format orientations\\n                    (local yaw) for each objects.\\n        \"\n    pred_bboxes2d = bbox[:, 0:4]\n    pred_bboxes2d = self.decode_bboxes2d(pred_bboxes2d, base_centers2d)\n    pred_offsets2d = bbox[:, 4:6]\n    pred_dimensions_offsets3d = bbox[:, 29:32]\n    pred_orientations = torch.cat((bbox[:, 32:40], bbox[:, 40:48]), dim=1)\n    pred_keypoints_depth_uncertainty = bbox[:, 26:29]\n    pred_direct_depth_uncertainty = bbox[:, 49:50].squeeze(-1)\n    pred_keypoints2d = bbox[:, 6:26].reshape(-1, 10, 2)\n    pred_direct_depth_offsets = bbox[:, 48:49].squeeze(-1)\n    pred_dimensions = self.decode_dims(labels, pred_dimensions_offsets3d)\n    pred_direct_depth = self.decode_direct_depth(pred_direct_depth_offsets)\n    pred_keypoints_depth = self.keypoints2depth(pred_keypoints2d, pred_dimensions, cam2imgs, downsample_ratio)\n    pred_direct_depth_uncertainty = torch.clamp(pred_direct_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    pred_keypoints_depth_uncertainty = torch.clamp(pred_keypoints_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    if self.combine_depth:\n        pred_depth_uncertainty = torch.cat((pred_direct_depth_uncertainty.unsqueeze(-1), pred_keypoints_depth_uncertainty), dim=1).exp()\n        pred_depth = torch.cat((pred_direct_depth.unsqueeze(-1), pred_keypoints_depth), dim=1)\n        pred_combined_depth = self.combine_depths(pred_depth, pred_depth_uncertainty)\n    else:\n        pred_combined_depth = None\n    preds = dict(bboxes2d=pred_bboxes2d, dimensions=pred_dimensions, offsets2d=pred_offsets2d, keypoints2d=pred_keypoints2d, orientations=pred_orientations, direct_depth=pred_direct_depth, keypoints_depth=pred_keypoints_depth, combined_depth=pred_combined_depth, direct_depth_uncertainty=pred_direct_depth_uncertainty, keypoints_depth_uncertainty=pred_keypoints_depth_uncertainty)\n    return preds",
            "def decode(self, bbox, base_centers2d, labels, downsample_ratio, cam2imgs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Decode bounding box regression into 3D predictions.\\n\\n        Args:\\n            bbox (Tensor): Raw bounding box predictions for each\\n                predict center2d point.\\n                shape: (N, C)\\n            base_centers2d (torch.Tensor): Base centers2d for 3D bboxes.\\n                shape: (N, 2).\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (N, )\\n            downsample_ratio (int): The stride of feature map.\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n\\n        Return:\\n            dict: The 3D prediction dict decoded from regression map.\\n            the dict has components below:\\n                - bboxes2d (torch.Tensor): Decoded [x1, y1, x2, y2] format\\n                    2D bboxes.\\n                - dimensions (torch.Tensor): Decoded dimensions for each\\n                    object.\\n                - offsets2d (torch.Tenosr): Offsets between base centers2d\\n                    and real centers2d.\\n                - direct_depth (torch.Tensor): Decoded directly regressed\\n                    depth.\\n                - keypoints2d (torch.Tensor): Keypoints of each projected\\n                    3D box on image.\\n                - keypoints_depth (torch.Tensor): Decoded depth from keypoints.\\n                - combined_depth (torch.Tensor): Combined depth using direct\\n                    depth and keypoints depth with depth uncertainty.\\n                - orientations (torch.Tensor): Multibin format orientations\\n                    (local yaw) for each objects.\\n        \"\n    pred_bboxes2d = bbox[:, 0:4]\n    pred_bboxes2d = self.decode_bboxes2d(pred_bboxes2d, base_centers2d)\n    pred_offsets2d = bbox[:, 4:6]\n    pred_dimensions_offsets3d = bbox[:, 29:32]\n    pred_orientations = torch.cat((bbox[:, 32:40], bbox[:, 40:48]), dim=1)\n    pred_keypoints_depth_uncertainty = bbox[:, 26:29]\n    pred_direct_depth_uncertainty = bbox[:, 49:50].squeeze(-1)\n    pred_keypoints2d = bbox[:, 6:26].reshape(-1, 10, 2)\n    pred_direct_depth_offsets = bbox[:, 48:49].squeeze(-1)\n    pred_dimensions = self.decode_dims(labels, pred_dimensions_offsets3d)\n    pred_direct_depth = self.decode_direct_depth(pred_direct_depth_offsets)\n    pred_keypoints_depth = self.keypoints2depth(pred_keypoints2d, pred_dimensions, cam2imgs, downsample_ratio)\n    pred_direct_depth_uncertainty = torch.clamp(pred_direct_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    pred_keypoints_depth_uncertainty = torch.clamp(pred_keypoints_depth_uncertainty, self.uncertainty_range[0], self.uncertainty_range[1])\n    if self.combine_depth:\n        pred_depth_uncertainty = torch.cat((pred_direct_depth_uncertainty.unsqueeze(-1), pred_keypoints_depth_uncertainty), dim=1).exp()\n        pred_depth = torch.cat((pred_direct_depth.unsqueeze(-1), pred_keypoints_depth), dim=1)\n        pred_combined_depth = self.combine_depths(pred_depth, pred_depth_uncertainty)\n    else:\n        pred_combined_depth = None\n    preds = dict(bboxes2d=pred_bboxes2d, dimensions=pred_dimensions, offsets2d=pred_offsets2d, keypoints2d=pred_keypoints2d, orientations=pred_orientations, direct_depth=pred_direct_depth, keypoints_depth=pred_keypoints_depth, combined_depth=pred_combined_depth, direct_depth_uncertainty=pred_direct_depth_uncertainty, keypoints_depth_uncertainty=pred_keypoints_depth_uncertainty)\n    return preds"
        ]
    },
    {
        "func_name": "decode_direct_depth",
        "original": "def decode_direct_depth(self, depth_offsets):\n    \"\"\"Transform depth offset to directly regressed depth.\n\n        Args:\n            depth_offsets (torch.Tensor): Predicted depth offsets.\n                shape: (N, )\n\n        Return:\n            torch.Tensor: Directly regressed depth.\n                shape: (N, )\n        \"\"\"\n    if self.depth_mode == 'exp':\n        direct_depth = depth_offsets.exp()\n    elif self.depth_mode == 'linear':\n        base_depth = depth_offsets.new_tensor(self.base_depth)\n        direct_depth = depth_offsets * base_depth[1] + base_depth[0]\n    elif self.depth_mode == 'inv_sigmoid':\n        direct_depth = 1 / torch.sigmoid(depth_offsets) - 1\n    else:\n        raise ValueError\n    if self.depth_range is not None:\n        direct_depth = torch.clamp(direct_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return direct_depth",
        "mutated": [
            "def decode_direct_depth(self, depth_offsets):\n    if False:\n        i = 10\n    'Transform depth offset to directly regressed depth.\\n\\n        Args:\\n            depth_offsets (torch.Tensor): Predicted depth offsets.\\n                shape: (N, )\\n\\n        Return:\\n            torch.Tensor: Directly regressed depth.\\n                shape: (N, )\\n        '\n    if self.depth_mode == 'exp':\n        direct_depth = depth_offsets.exp()\n    elif self.depth_mode == 'linear':\n        base_depth = depth_offsets.new_tensor(self.base_depth)\n        direct_depth = depth_offsets * base_depth[1] + base_depth[0]\n    elif self.depth_mode == 'inv_sigmoid':\n        direct_depth = 1 / torch.sigmoid(depth_offsets) - 1\n    else:\n        raise ValueError\n    if self.depth_range is not None:\n        direct_depth = torch.clamp(direct_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return direct_depth",
            "def decode_direct_depth(self, depth_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transform depth offset to directly regressed depth.\\n\\n        Args:\\n            depth_offsets (torch.Tensor): Predicted depth offsets.\\n                shape: (N, )\\n\\n        Return:\\n            torch.Tensor: Directly regressed depth.\\n                shape: (N, )\\n        '\n    if self.depth_mode == 'exp':\n        direct_depth = depth_offsets.exp()\n    elif self.depth_mode == 'linear':\n        base_depth = depth_offsets.new_tensor(self.base_depth)\n        direct_depth = depth_offsets * base_depth[1] + base_depth[0]\n    elif self.depth_mode == 'inv_sigmoid':\n        direct_depth = 1 / torch.sigmoid(depth_offsets) - 1\n    else:\n        raise ValueError\n    if self.depth_range is not None:\n        direct_depth = torch.clamp(direct_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return direct_depth",
            "def decode_direct_depth(self, depth_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transform depth offset to directly regressed depth.\\n\\n        Args:\\n            depth_offsets (torch.Tensor): Predicted depth offsets.\\n                shape: (N, )\\n\\n        Return:\\n            torch.Tensor: Directly regressed depth.\\n                shape: (N, )\\n        '\n    if self.depth_mode == 'exp':\n        direct_depth = depth_offsets.exp()\n    elif self.depth_mode == 'linear':\n        base_depth = depth_offsets.new_tensor(self.base_depth)\n        direct_depth = depth_offsets * base_depth[1] + base_depth[0]\n    elif self.depth_mode == 'inv_sigmoid':\n        direct_depth = 1 / torch.sigmoid(depth_offsets) - 1\n    else:\n        raise ValueError\n    if self.depth_range is not None:\n        direct_depth = torch.clamp(direct_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return direct_depth",
            "def decode_direct_depth(self, depth_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transform depth offset to directly regressed depth.\\n\\n        Args:\\n            depth_offsets (torch.Tensor): Predicted depth offsets.\\n                shape: (N, )\\n\\n        Return:\\n            torch.Tensor: Directly regressed depth.\\n                shape: (N, )\\n        '\n    if self.depth_mode == 'exp':\n        direct_depth = depth_offsets.exp()\n    elif self.depth_mode == 'linear':\n        base_depth = depth_offsets.new_tensor(self.base_depth)\n        direct_depth = depth_offsets * base_depth[1] + base_depth[0]\n    elif self.depth_mode == 'inv_sigmoid':\n        direct_depth = 1 / torch.sigmoid(depth_offsets) - 1\n    else:\n        raise ValueError\n    if self.depth_range is not None:\n        direct_depth = torch.clamp(direct_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return direct_depth",
            "def decode_direct_depth(self, depth_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transform depth offset to directly regressed depth.\\n\\n        Args:\\n            depth_offsets (torch.Tensor): Predicted depth offsets.\\n                shape: (N, )\\n\\n        Return:\\n            torch.Tensor: Directly regressed depth.\\n                shape: (N, )\\n        '\n    if self.depth_mode == 'exp':\n        direct_depth = depth_offsets.exp()\n    elif self.depth_mode == 'linear':\n        base_depth = depth_offsets.new_tensor(self.base_depth)\n        direct_depth = depth_offsets * base_depth[1] + base_depth[0]\n    elif self.depth_mode == 'inv_sigmoid':\n        direct_depth = 1 / torch.sigmoid(depth_offsets) - 1\n    else:\n        raise ValueError\n    if self.depth_range is not None:\n        direct_depth = torch.clamp(direct_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return direct_depth"
        ]
    },
    {
        "func_name": "decode_location",
        "original": "def decode_location(self, base_centers2d, offsets2d, depths, cam2imgs, downsample_ratio, pad_mode='default'):\n    \"\"\"Retrieve object location.\n\n        Args:\n            base_centers2d (torch.Tensor): predicted base centers2d.\n                shape: (N, 2)\n            offsets2d (torch.Tensor): The offsets between real centers2d\n                and base centers2d.\n                shape: (N , 2)\n            depths (torch.Tensor): Depths of objects.\n                shape: (N, )\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\n            downsample_ratio (int): The stride of feature map.\n            pad_mode (str, optional): Padding mode used in\n                training data augmentation.\n\n        Return:\n            tuple(torch.Tensor): Centers of 3D boxes.\n                shape: (N, 3)\n        \"\"\"\n    N = cam2imgs.shape[0]\n    cam2imgs_inv = cam2imgs.inverse()\n    if pad_mode == 'default':\n        centers2d_img = (base_centers2d + offsets2d) * downsample_ratio\n    else:\n        raise NotImplementedError\n    centers2d_img = torch.cat((centers2d_img, depths.unsqueeze(-1)), dim=1)\n    centers2d_extend = torch.cat((centers2d_img, centers2d_img.new_ones(N, 1)), dim=1).unsqueeze(-1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_extend).squeeze(-1)\n    return locations[:, :3]",
        "mutated": [
            "def decode_location(self, base_centers2d, offsets2d, depths, cam2imgs, downsample_ratio, pad_mode='default'):\n    if False:\n        i = 10\n    \"Retrieve object location.\\n\\n        Args:\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n            offsets2d (torch.Tensor): The offsets between real centers2d\\n                and base centers2d.\\n                shape: (N , 2)\\n            depths (torch.Tensor): Depths of objects.\\n                shape: (N, )\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int): The stride of feature map.\\n            pad_mode (str, optional): Padding mode used in\\n                training data augmentation.\\n\\n        Return:\\n            tuple(torch.Tensor): Centers of 3D boxes.\\n                shape: (N, 3)\\n        \"\n    N = cam2imgs.shape[0]\n    cam2imgs_inv = cam2imgs.inverse()\n    if pad_mode == 'default':\n        centers2d_img = (base_centers2d + offsets2d) * downsample_ratio\n    else:\n        raise NotImplementedError\n    centers2d_img = torch.cat((centers2d_img, depths.unsqueeze(-1)), dim=1)\n    centers2d_extend = torch.cat((centers2d_img, centers2d_img.new_ones(N, 1)), dim=1).unsqueeze(-1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_extend).squeeze(-1)\n    return locations[:, :3]",
            "def decode_location(self, base_centers2d, offsets2d, depths, cam2imgs, downsample_ratio, pad_mode='default'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Retrieve object location.\\n\\n        Args:\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n            offsets2d (torch.Tensor): The offsets between real centers2d\\n                and base centers2d.\\n                shape: (N , 2)\\n            depths (torch.Tensor): Depths of objects.\\n                shape: (N, )\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int): The stride of feature map.\\n            pad_mode (str, optional): Padding mode used in\\n                training data augmentation.\\n\\n        Return:\\n            tuple(torch.Tensor): Centers of 3D boxes.\\n                shape: (N, 3)\\n        \"\n    N = cam2imgs.shape[0]\n    cam2imgs_inv = cam2imgs.inverse()\n    if pad_mode == 'default':\n        centers2d_img = (base_centers2d + offsets2d) * downsample_ratio\n    else:\n        raise NotImplementedError\n    centers2d_img = torch.cat((centers2d_img, depths.unsqueeze(-1)), dim=1)\n    centers2d_extend = torch.cat((centers2d_img, centers2d_img.new_ones(N, 1)), dim=1).unsqueeze(-1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_extend).squeeze(-1)\n    return locations[:, :3]",
            "def decode_location(self, base_centers2d, offsets2d, depths, cam2imgs, downsample_ratio, pad_mode='default'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Retrieve object location.\\n\\n        Args:\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n            offsets2d (torch.Tensor): The offsets between real centers2d\\n                and base centers2d.\\n                shape: (N , 2)\\n            depths (torch.Tensor): Depths of objects.\\n                shape: (N, )\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int): The stride of feature map.\\n            pad_mode (str, optional): Padding mode used in\\n                training data augmentation.\\n\\n        Return:\\n            tuple(torch.Tensor): Centers of 3D boxes.\\n                shape: (N, 3)\\n        \"\n    N = cam2imgs.shape[0]\n    cam2imgs_inv = cam2imgs.inverse()\n    if pad_mode == 'default':\n        centers2d_img = (base_centers2d + offsets2d) * downsample_ratio\n    else:\n        raise NotImplementedError\n    centers2d_img = torch.cat((centers2d_img, depths.unsqueeze(-1)), dim=1)\n    centers2d_extend = torch.cat((centers2d_img, centers2d_img.new_ones(N, 1)), dim=1).unsqueeze(-1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_extend).squeeze(-1)\n    return locations[:, :3]",
            "def decode_location(self, base_centers2d, offsets2d, depths, cam2imgs, downsample_ratio, pad_mode='default'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Retrieve object location.\\n\\n        Args:\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n            offsets2d (torch.Tensor): The offsets between real centers2d\\n                and base centers2d.\\n                shape: (N , 2)\\n            depths (torch.Tensor): Depths of objects.\\n                shape: (N, )\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int): The stride of feature map.\\n            pad_mode (str, optional): Padding mode used in\\n                training data augmentation.\\n\\n        Return:\\n            tuple(torch.Tensor): Centers of 3D boxes.\\n                shape: (N, 3)\\n        \"\n    N = cam2imgs.shape[0]\n    cam2imgs_inv = cam2imgs.inverse()\n    if pad_mode == 'default':\n        centers2d_img = (base_centers2d + offsets2d) * downsample_ratio\n    else:\n        raise NotImplementedError\n    centers2d_img = torch.cat((centers2d_img, depths.unsqueeze(-1)), dim=1)\n    centers2d_extend = torch.cat((centers2d_img, centers2d_img.new_ones(N, 1)), dim=1).unsqueeze(-1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_extend).squeeze(-1)\n    return locations[:, :3]",
            "def decode_location(self, base_centers2d, offsets2d, depths, cam2imgs, downsample_ratio, pad_mode='default'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Retrieve object location.\\n\\n        Args:\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n            offsets2d (torch.Tensor): The offsets between real centers2d\\n                and base centers2d.\\n                shape: (N , 2)\\n            depths (torch.Tensor): Depths of objects.\\n                shape: (N, )\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int): The stride of feature map.\\n            pad_mode (str, optional): Padding mode used in\\n                training data augmentation.\\n\\n        Return:\\n            tuple(torch.Tensor): Centers of 3D boxes.\\n                shape: (N, 3)\\n        \"\n    N = cam2imgs.shape[0]\n    cam2imgs_inv = cam2imgs.inverse()\n    if pad_mode == 'default':\n        centers2d_img = (base_centers2d + offsets2d) * downsample_ratio\n    else:\n        raise NotImplementedError\n    centers2d_img = torch.cat((centers2d_img, depths.unsqueeze(-1)), dim=1)\n    centers2d_extend = torch.cat((centers2d_img, centers2d_img.new_ones(N, 1)), dim=1).unsqueeze(-1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_extend).squeeze(-1)\n    return locations[:, :3]"
        ]
    },
    {
        "func_name": "keypoints2depth",
        "original": "def keypoints2depth(self, keypoints2d, dimensions, cam2imgs, downsample_ratio=4, group0_index=[(7, 3), (0, 4)], group1_index=[(2, 6), (1, 5)]):\n    \"\"\"Decode depth form three groups of keypoints and geometry projection\n        model. 2D keypoints inlucding 8 coreners and top/bottom centers will be\n        divided into three groups which will be used to calculate three depths\n        of object.\n\n        .. code-block:: none\n\n                Group center keypoints:\n\n                             + --------------- +\n                            /|   top center   /|\n                           / |      .        / |\n                          /  |      |       /  |\n                         + ---------|----- +   +\n                         |  /       |      |  /\n                         | /        .      | /\n                         |/ bottom center  |/\n                         + --------------- +\n\n                Group 0 keypoints:\n\n                             0\n                             + -------------- +\n                            /|               /|\n                           / |              / |\n                          /  |            5/  |\n                         + -------------- +   +\n                         |  /3            |  /\n                         | /              | /\n                         |/               |/\n                         + -------------- + 6\n\n                Group 1 keypoints:\n\n                                               4\n                             + -------------- +\n                            /|               /|\n                           / |              / |\n                          /  |             /  |\n                       1 + -------------- +   + 7\n                         |  /             |  /\n                         | /              | /\n                         |/               |/\n                       2 + -------------- +\n\n\n        Args:\n            keypoints2d (torch.Tensor): Keypoints of objects.\n                8 vertices + top/bottom center.\n                shape: (N, 10, 2)\n            dimensions (torch.Tensor): Dimensions of objetcts.\n                shape: (N, 3)\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\n            downsample_ratio (int, opitonal): The stride of feature map.\n                Defaults: 4.\n            group0_index(list[tuple[int]], optional): Keypoints group 0\n                of index to calculate the depth.\n                Defaults: [0, 3, 4, 7].\n            group1_index(list[tuple[int]], optional): Keypoints group 1\n                of index to calculate the depth.\n                Defaults: [1, 2, 5, 6]\n\n        Return:\n            tuple(torch.Tensor): Depth computed from three groups of\n                keypoints (top/bottom, group0, group1)\n                shape: (N, 3)\n        \"\"\"\n    pred_height_3d = dimensions[:, 1].clone()\n    f_u = cam2imgs[:, 0, 0]\n    center_height = keypoints2d[:, -2, 1] - keypoints2d[:, -1, 1]\n    corner_group0_height = keypoints2d[:, group0_index[0], 1] - keypoints2d[:, group0_index[1], 1]\n    corner_group1_height = keypoints2d[:, group1_index[0], 1] - keypoints2d[:, group1_index[1], 1]\n    center_depth = f_u * pred_height_3d / (F.relu(center_height) * downsample_ratio + self.eps)\n    corner_group0_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group0_height) * downsample_ratio + self.eps)\n    corner_group1_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group1_height) * downsample_ratio + self.eps)\n    corner_group0_depth = corner_group0_depth.mean(dim=1)\n    corner_group1_depth = corner_group1_depth.mean(dim=1)\n    keypoints_depth = torch.stack((center_depth, corner_group0_depth, corner_group1_depth), dim=1)\n    keypoints_depth = torch.clamp(keypoints_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return keypoints_depth",
        "mutated": [
            "def keypoints2depth(self, keypoints2d, dimensions, cam2imgs, downsample_ratio=4, group0_index=[(7, 3), (0, 4)], group1_index=[(2, 6), (1, 5)]):\n    if False:\n        i = 10\n    \"Decode depth form three groups of keypoints and geometry projection\\n        model. 2D keypoints inlucding 8 coreners and top/bottom centers will be\\n        divided into three groups which will be used to calculate three depths\\n        of object.\\n\\n        .. code-block:: none\\n\\n                Group center keypoints:\\n\\n                             + --------------- +\\n                            /|   top center   /|\\n                           / |      .        / |\\n                          /  |      |       /  |\\n                         + ---------|----- +   +\\n                         |  /       |      |  /\\n                         | /        .      | /\\n                         |/ bottom center  |/\\n                         + --------------- +\\n\\n                Group 0 keypoints:\\n\\n                             0\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |            5/  |\\n                         + -------------- +   +\\n                         |  /3            |  /\\n                         | /              | /\\n                         |/               |/\\n                         + -------------- + 6\\n\\n                Group 1 keypoints:\\n\\n                                               4\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |             /  |\\n                       1 + -------------- +   + 7\\n                         |  /             |  /\\n                         | /              | /\\n                         |/               |/\\n                       2 + -------------- +\\n\\n\\n        Args:\\n            keypoints2d (torch.Tensor): Keypoints of objects.\\n                8 vertices + top/bottom center.\\n                shape: (N, 10, 2)\\n            dimensions (torch.Tensor): Dimensions of objetcts.\\n                shape: (N, 3)\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int, opitonal): The stride of feature map.\\n                Defaults: 4.\\n            group0_index(list[tuple[int]], optional): Keypoints group 0\\n                of index to calculate the depth.\\n                Defaults: [0, 3, 4, 7].\\n            group1_index(list[tuple[int]], optional): Keypoints group 1\\n                of index to calculate the depth.\\n                Defaults: [1, 2, 5, 6]\\n\\n        Return:\\n            tuple(torch.Tensor): Depth computed from three groups of\\n                keypoints (top/bottom, group0, group1)\\n                shape: (N, 3)\\n        \"\n    pred_height_3d = dimensions[:, 1].clone()\n    f_u = cam2imgs[:, 0, 0]\n    center_height = keypoints2d[:, -2, 1] - keypoints2d[:, -1, 1]\n    corner_group0_height = keypoints2d[:, group0_index[0], 1] - keypoints2d[:, group0_index[1], 1]\n    corner_group1_height = keypoints2d[:, group1_index[0], 1] - keypoints2d[:, group1_index[1], 1]\n    center_depth = f_u * pred_height_3d / (F.relu(center_height) * downsample_ratio + self.eps)\n    corner_group0_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group0_height) * downsample_ratio + self.eps)\n    corner_group1_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group1_height) * downsample_ratio + self.eps)\n    corner_group0_depth = corner_group0_depth.mean(dim=1)\n    corner_group1_depth = corner_group1_depth.mean(dim=1)\n    keypoints_depth = torch.stack((center_depth, corner_group0_depth, corner_group1_depth), dim=1)\n    keypoints_depth = torch.clamp(keypoints_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return keypoints_depth",
            "def keypoints2depth(self, keypoints2d, dimensions, cam2imgs, downsample_ratio=4, group0_index=[(7, 3), (0, 4)], group1_index=[(2, 6), (1, 5)]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Decode depth form three groups of keypoints and geometry projection\\n        model. 2D keypoints inlucding 8 coreners and top/bottom centers will be\\n        divided into three groups which will be used to calculate three depths\\n        of object.\\n\\n        .. code-block:: none\\n\\n                Group center keypoints:\\n\\n                             + --------------- +\\n                            /|   top center   /|\\n                           / |      .        / |\\n                          /  |      |       /  |\\n                         + ---------|----- +   +\\n                         |  /       |      |  /\\n                         | /        .      | /\\n                         |/ bottom center  |/\\n                         + --------------- +\\n\\n                Group 0 keypoints:\\n\\n                             0\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |            5/  |\\n                         + -------------- +   +\\n                         |  /3            |  /\\n                         | /              | /\\n                         |/               |/\\n                         + -------------- + 6\\n\\n                Group 1 keypoints:\\n\\n                                               4\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |             /  |\\n                       1 + -------------- +   + 7\\n                         |  /             |  /\\n                         | /              | /\\n                         |/               |/\\n                       2 + -------------- +\\n\\n\\n        Args:\\n            keypoints2d (torch.Tensor): Keypoints of objects.\\n                8 vertices + top/bottom center.\\n                shape: (N, 10, 2)\\n            dimensions (torch.Tensor): Dimensions of objetcts.\\n                shape: (N, 3)\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int, opitonal): The stride of feature map.\\n                Defaults: 4.\\n            group0_index(list[tuple[int]], optional): Keypoints group 0\\n                of index to calculate the depth.\\n                Defaults: [0, 3, 4, 7].\\n            group1_index(list[tuple[int]], optional): Keypoints group 1\\n                of index to calculate the depth.\\n                Defaults: [1, 2, 5, 6]\\n\\n        Return:\\n            tuple(torch.Tensor): Depth computed from three groups of\\n                keypoints (top/bottom, group0, group1)\\n                shape: (N, 3)\\n        \"\n    pred_height_3d = dimensions[:, 1].clone()\n    f_u = cam2imgs[:, 0, 0]\n    center_height = keypoints2d[:, -2, 1] - keypoints2d[:, -1, 1]\n    corner_group0_height = keypoints2d[:, group0_index[0], 1] - keypoints2d[:, group0_index[1], 1]\n    corner_group1_height = keypoints2d[:, group1_index[0], 1] - keypoints2d[:, group1_index[1], 1]\n    center_depth = f_u * pred_height_3d / (F.relu(center_height) * downsample_ratio + self.eps)\n    corner_group0_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group0_height) * downsample_ratio + self.eps)\n    corner_group1_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group1_height) * downsample_ratio + self.eps)\n    corner_group0_depth = corner_group0_depth.mean(dim=1)\n    corner_group1_depth = corner_group1_depth.mean(dim=1)\n    keypoints_depth = torch.stack((center_depth, corner_group0_depth, corner_group1_depth), dim=1)\n    keypoints_depth = torch.clamp(keypoints_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return keypoints_depth",
            "def keypoints2depth(self, keypoints2d, dimensions, cam2imgs, downsample_ratio=4, group0_index=[(7, 3), (0, 4)], group1_index=[(2, 6), (1, 5)]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Decode depth form three groups of keypoints and geometry projection\\n        model. 2D keypoints inlucding 8 coreners and top/bottom centers will be\\n        divided into three groups which will be used to calculate three depths\\n        of object.\\n\\n        .. code-block:: none\\n\\n                Group center keypoints:\\n\\n                             + --------------- +\\n                            /|   top center   /|\\n                           / |      .        / |\\n                          /  |      |       /  |\\n                         + ---------|----- +   +\\n                         |  /       |      |  /\\n                         | /        .      | /\\n                         |/ bottom center  |/\\n                         + --------------- +\\n\\n                Group 0 keypoints:\\n\\n                             0\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |            5/  |\\n                         + -------------- +   +\\n                         |  /3            |  /\\n                         | /              | /\\n                         |/               |/\\n                         + -------------- + 6\\n\\n                Group 1 keypoints:\\n\\n                                               4\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |             /  |\\n                       1 + -------------- +   + 7\\n                         |  /             |  /\\n                         | /              | /\\n                         |/               |/\\n                       2 + -------------- +\\n\\n\\n        Args:\\n            keypoints2d (torch.Tensor): Keypoints of objects.\\n                8 vertices + top/bottom center.\\n                shape: (N, 10, 2)\\n            dimensions (torch.Tensor): Dimensions of objetcts.\\n                shape: (N, 3)\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int, opitonal): The stride of feature map.\\n                Defaults: 4.\\n            group0_index(list[tuple[int]], optional): Keypoints group 0\\n                of index to calculate the depth.\\n                Defaults: [0, 3, 4, 7].\\n            group1_index(list[tuple[int]], optional): Keypoints group 1\\n                of index to calculate the depth.\\n                Defaults: [1, 2, 5, 6]\\n\\n        Return:\\n            tuple(torch.Tensor): Depth computed from three groups of\\n                keypoints (top/bottom, group0, group1)\\n                shape: (N, 3)\\n        \"\n    pred_height_3d = dimensions[:, 1].clone()\n    f_u = cam2imgs[:, 0, 0]\n    center_height = keypoints2d[:, -2, 1] - keypoints2d[:, -1, 1]\n    corner_group0_height = keypoints2d[:, group0_index[0], 1] - keypoints2d[:, group0_index[1], 1]\n    corner_group1_height = keypoints2d[:, group1_index[0], 1] - keypoints2d[:, group1_index[1], 1]\n    center_depth = f_u * pred_height_3d / (F.relu(center_height) * downsample_ratio + self.eps)\n    corner_group0_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group0_height) * downsample_ratio + self.eps)\n    corner_group1_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group1_height) * downsample_ratio + self.eps)\n    corner_group0_depth = corner_group0_depth.mean(dim=1)\n    corner_group1_depth = corner_group1_depth.mean(dim=1)\n    keypoints_depth = torch.stack((center_depth, corner_group0_depth, corner_group1_depth), dim=1)\n    keypoints_depth = torch.clamp(keypoints_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return keypoints_depth",
            "def keypoints2depth(self, keypoints2d, dimensions, cam2imgs, downsample_ratio=4, group0_index=[(7, 3), (0, 4)], group1_index=[(2, 6), (1, 5)]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Decode depth form three groups of keypoints and geometry projection\\n        model. 2D keypoints inlucding 8 coreners and top/bottom centers will be\\n        divided into three groups which will be used to calculate three depths\\n        of object.\\n\\n        .. code-block:: none\\n\\n                Group center keypoints:\\n\\n                             + --------------- +\\n                            /|   top center   /|\\n                           / |      .        / |\\n                          /  |      |       /  |\\n                         + ---------|----- +   +\\n                         |  /       |      |  /\\n                         | /        .      | /\\n                         |/ bottom center  |/\\n                         + --------------- +\\n\\n                Group 0 keypoints:\\n\\n                             0\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |            5/  |\\n                         + -------------- +   +\\n                         |  /3            |  /\\n                         | /              | /\\n                         |/               |/\\n                         + -------------- + 6\\n\\n                Group 1 keypoints:\\n\\n                                               4\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |             /  |\\n                       1 + -------------- +   + 7\\n                         |  /             |  /\\n                         | /              | /\\n                         |/               |/\\n                       2 + -------------- +\\n\\n\\n        Args:\\n            keypoints2d (torch.Tensor): Keypoints of objects.\\n                8 vertices + top/bottom center.\\n                shape: (N, 10, 2)\\n            dimensions (torch.Tensor): Dimensions of objetcts.\\n                shape: (N, 3)\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int, opitonal): The stride of feature map.\\n                Defaults: 4.\\n            group0_index(list[tuple[int]], optional): Keypoints group 0\\n                of index to calculate the depth.\\n                Defaults: [0, 3, 4, 7].\\n            group1_index(list[tuple[int]], optional): Keypoints group 1\\n                of index to calculate the depth.\\n                Defaults: [1, 2, 5, 6]\\n\\n        Return:\\n            tuple(torch.Tensor): Depth computed from three groups of\\n                keypoints (top/bottom, group0, group1)\\n                shape: (N, 3)\\n        \"\n    pred_height_3d = dimensions[:, 1].clone()\n    f_u = cam2imgs[:, 0, 0]\n    center_height = keypoints2d[:, -2, 1] - keypoints2d[:, -1, 1]\n    corner_group0_height = keypoints2d[:, group0_index[0], 1] - keypoints2d[:, group0_index[1], 1]\n    corner_group1_height = keypoints2d[:, group1_index[0], 1] - keypoints2d[:, group1_index[1], 1]\n    center_depth = f_u * pred_height_3d / (F.relu(center_height) * downsample_ratio + self.eps)\n    corner_group0_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group0_height) * downsample_ratio + self.eps)\n    corner_group1_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group1_height) * downsample_ratio + self.eps)\n    corner_group0_depth = corner_group0_depth.mean(dim=1)\n    corner_group1_depth = corner_group1_depth.mean(dim=1)\n    keypoints_depth = torch.stack((center_depth, corner_group0_depth, corner_group1_depth), dim=1)\n    keypoints_depth = torch.clamp(keypoints_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return keypoints_depth",
            "def keypoints2depth(self, keypoints2d, dimensions, cam2imgs, downsample_ratio=4, group0_index=[(7, 3), (0, 4)], group1_index=[(2, 6), (1, 5)]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Decode depth form three groups of keypoints and geometry projection\\n        model. 2D keypoints inlucding 8 coreners and top/bottom centers will be\\n        divided into three groups which will be used to calculate three depths\\n        of object.\\n\\n        .. code-block:: none\\n\\n                Group center keypoints:\\n\\n                             + --------------- +\\n                            /|   top center   /|\\n                           / |      .        / |\\n                          /  |      |       /  |\\n                         + ---------|----- +   +\\n                         |  /       |      |  /\\n                         | /        .      | /\\n                         |/ bottom center  |/\\n                         + --------------- +\\n\\n                Group 0 keypoints:\\n\\n                             0\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |            5/  |\\n                         + -------------- +   +\\n                         |  /3            |  /\\n                         | /              | /\\n                         |/               |/\\n                         + -------------- + 6\\n\\n                Group 1 keypoints:\\n\\n                                               4\\n                             + -------------- +\\n                            /|               /|\\n                           / |              / |\\n                          /  |             /  |\\n                       1 + -------------- +   + 7\\n                         |  /             |  /\\n                         | /              | /\\n                         |/               |/\\n                       2 + -------------- +\\n\\n\\n        Args:\\n            keypoints2d (torch.Tensor): Keypoints of objects.\\n                8 vertices + top/bottom center.\\n                shape: (N, 10, 2)\\n            dimensions (torch.Tensor): Dimensions of objetcts.\\n                shape: (N, 3)\\n            cam2imgs (torch.Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (N, 4, 4)  nuscenes (N, 3, 3)\\n            downsample_ratio (int, opitonal): The stride of feature map.\\n                Defaults: 4.\\n            group0_index(list[tuple[int]], optional): Keypoints group 0\\n                of index to calculate the depth.\\n                Defaults: [0, 3, 4, 7].\\n            group1_index(list[tuple[int]], optional): Keypoints group 1\\n                of index to calculate the depth.\\n                Defaults: [1, 2, 5, 6]\\n\\n        Return:\\n            tuple(torch.Tensor): Depth computed from three groups of\\n                keypoints (top/bottom, group0, group1)\\n                shape: (N, 3)\\n        \"\n    pred_height_3d = dimensions[:, 1].clone()\n    f_u = cam2imgs[:, 0, 0]\n    center_height = keypoints2d[:, -2, 1] - keypoints2d[:, -1, 1]\n    corner_group0_height = keypoints2d[:, group0_index[0], 1] - keypoints2d[:, group0_index[1], 1]\n    corner_group1_height = keypoints2d[:, group1_index[0], 1] - keypoints2d[:, group1_index[1], 1]\n    center_depth = f_u * pred_height_3d / (F.relu(center_height) * downsample_ratio + self.eps)\n    corner_group0_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group0_height) * downsample_ratio + self.eps)\n    corner_group1_depth = (f_u * pred_height_3d).unsqueeze(-1) / (F.relu(corner_group1_height) * downsample_ratio + self.eps)\n    corner_group0_depth = corner_group0_depth.mean(dim=1)\n    corner_group1_depth = corner_group1_depth.mean(dim=1)\n    keypoints_depth = torch.stack((center_depth, corner_group0_depth, corner_group1_depth), dim=1)\n    keypoints_depth = torch.clamp(keypoints_depth, min=self.depth_range[0], max=self.depth_range[1])\n    return keypoints_depth"
        ]
    },
    {
        "func_name": "decode_dims",
        "original": "def decode_dims(self, labels, dims_offset):\n    \"\"\"Retrieve object dimensions.\n\n        Args:\n            labels (torch.Tensor): Each points' category id.\n                shape: (N, K)\n            dims_offset (torch.Tensor): Dimension offsets.\n                shape: (N, 3)\n\n        Returns:\n            torch.Tensor: Shape (N, 3)\n        \"\"\"\n    if self.dims_mode == 'exp':\n        dims_offset = dims_offset.exp()\n    elif self.dims_mode == 'linear':\n        labels = labels.long()\n        base_dims = dims_offset.new_tensor(self.base_dims)\n        dims_mean = base_dims[:, :3]\n        dims_std = base_dims[:, 3:6]\n        cls_dimension_mean = dims_mean[labels, :]\n        cls_dimension_std = dims_std[labels, :]\n        dimensions = dims_offset * cls_dimension_mean + cls_dimension_std\n    else:\n        raise ValueError\n    return dimensions",
        "mutated": [
            "def decode_dims(self, labels, dims_offset):\n    if False:\n        i = 10\n    \"Retrieve object dimensions.\\n\\n        Args:\\n            labels (torch.Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (torch.Tensor): Dimension offsets.\\n                shape: (N, 3)\\n\\n        Returns:\\n            torch.Tensor: Shape (N, 3)\\n        \"\n    if self.dims_mode == 'exp':\n        dims_offset = dims_offset.exp()\n    elif self.dims_mode == 'linear':\n        labels = labels.long()\n        base_dims = dims_offset.new_tensor(self.base_dims)\n        dims_mean = base_dims[:, :3]\n        dims_std = base_dims[:, 3:6]\n        cls_dimension_mean = dims_mean[labels, :]\n        cls_dimension_std = dims_std[labels, :]\n        dimensions = dims_offset * cls_dimension_mean + cls_dimension_std\n    else:\n        raise ValueError\n    return dimensions",
            "def decode_dims(self, labels, dims_offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Retrieve object dimensions.\\n\\n        Args:\\n            labels (torch.Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (torch.Tensor): Dimension offsets.\\n                shape: (N, 3)\\n\\n        Returns:\\n            torch.Tensor: Shape (N, 3)\\n        \"\n    if self.dims_mode == 'exp':\n        dims_offset = dims_offset.exp()\n    elif self.dims_mode == 'linear':\n        labels = labels.long()\n        base_dims = dims_offset.new_tensor(self.base_dims)\n        dims_mean = base_dims[:, :3]\n        dims_std = base_dims[:, 3:6]\n        cls_dimension_mean = dims_mean[labels, :]\n        cls_dimension_std = dims_std[labels, :]\n        dimensions = dims_offset * cls_dimension_mean + cls_dimension_std\n    else:\n        raise ValueError\n    return dimensions",
            "def decode_dims(self, labels, dims_offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Retrieve object dimensions.\\n\\n        Args:\\n            labels (torch.Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (torch.Tensor): Dimension offsets.\\n                shape: (N, 3)\\n\\n        Returns:\\n            torch.Tensor: Shape (N, 3)\\n        \"\n    if self.dims_mode == 'exp':\n        dims_offset = dims_offset.exp()\n    elif self.dims_mode == 'linear':\n        labels = labels.long()\n        base_dims = dims_offset.new_tensor(self.base_dims)\n        dims_mean = base_dims[:, :3]\n        dims_std = base_dims[:, 3:6]\n        cls_dimension_mean = dims_mean[labels, :]\n        cls_dimension_std = dims_std[labels, :]\n        dimensions = dims_offset * cls_dimension_mean + cls_dimension_std\n    else:\n        raise ValueError\n    return dimensions",
            "def decode_dims(self, labels, dims_offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Retrieve object dimensions.\\n\\n        Args:\\n            labels (torch.Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (torch.Tensor): Dimension offsets.\\n                shape: (N, 3)\\n\\n        Returns:\\n            torch.Tensor: Shape (N, 3)\\n        \"\n    if self.dims_mode == 'exp':\n        dims_offset = dims_offset.exp()\n    elif self.dims_mode == 'linear':\n        labels = labels.long()\n        base_dims = dims_offset.new_tensor(self.base_dims)\n        dims_mean = base_dims[:, :3]\n        dims_std = base_dims[:, 3:6]\n        cls_dimension_mean = dims_mean[labels, :]\n        cls_dimension_std = dims_std[labels, :]\n        dimensions = dims_offset * cls_dimension_mean + cls_dimension_std\n    else:\n        raise ValueError\n    return dimensions",
            "def decode_dims(self, labels, dims_offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Retrieve object dimensions.\\n\\n        Args:\\n            labels (torch.Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (torch.Tensor): Dimension offsets.\\n                shape: (N, 3)\\n\\n        Returns:\\n            torch.Tensor: Shape (N, 3)\\n        \"\n    if self.dims_mode == 'exp':\n        dims_offset = dims_offset.exp()\n    elif self.dims_mode == 'linear':\n        labels = labels.long()\n        base_dims = dims_offset.new_tensor(self.base_dims)\n        dims_mean = base_dims[:, :3]\n        dims_std = base_dims[:, 3:6]\n        cls_dimension_mean = dims_mean[labels, :]\n        cls_dimension_std = dims_std[labels, :]\n        dimensions = dims_offset * cls_dimension_mean + cls_dimension_std\n    else:\n        raise ValueError\n    return dimensions"
        ]
    },
    {
        "func_name": "decode_orientation",
        "original": "def decode_orientation(self, ori_vector, locations):\n    \"\"\"Retrieve object orientation.\n\n        Args:\n            ori_vector (torch.Tensor): Local orientation vector\n                in [axis_cls, head_cls, sin, cos] format.\n                shape: (N, num_dir_bins * 4)\n            locations (torch.Tensor): Object location.\n                shape: (N, 3)\n\n        Returns:\n            tuple[torch.Tensor]: yaws and local yaws of 3d bboxes.\n        \"\"\"\n    if self.multibin:\n        pred_bin_cls = ori_vector[:, :self.num_dir_bins * 2].view(-1, self.num_dir_bins, 2)\n        pred_bin_cls = pred_bin_cls.softmax(dim=2)[..., 1]\n        orientations = ori_vector.new_zeros(ori_vector.shape[0])\n        for i in range(self.num_dir_bins):\n            mask_i = pred_bin_cls.argmax(dim=1) == i\n            start_bin = self.num_dir_bins * 2 + i * 2\n            end_bin = start_bin + 2\n            pred_bin_offset = ori_vector[mask_i, start_bin:end_bin]\n            orientations[mask_i] = pred_bin_offset[:, 0].atan2(pred_bin_offset[:, 1]) + self.bin_centers[i]\n    else:\n        axis_cls = ori_vector[:, :2].softmax(dim=1)\n        axis_cls = axis_cls[:, 0] < axis_cls[:, 1]\n        head_cls = ori_vector[:, 2:4].softmax(dim=1)\n        head_cls = head_cls[:, 0] < head_cls[:, 1]\n        orientations = self.bin_centers[axis_cls + head_cls * 2]\n        sin_cos_offset = F.normalize(ori_vector[:, 4:])\n        orientations += sin_cos_offset[:, 0].atan(sin_cos_offset[:, 1])\n    locations = locations.view(-1, 3)\n    rays = locations[:, 0].atan2(locations[:, 2])\n    local_yaws = orientations\n    yaws = local_yaws + rays\n    larger_idx = (yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        yaws[small_idx] += 2 * np.pi\n    larger_idx = (local_yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (local_yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        local_yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        local_yaws[small_idx] += 2 * np.pi\n    return (yaws, local_yaws)",
        "mutated": [
            "def decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n    'Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (torch.Tensor): Local orientation vector\\n                in [axis_cls, head_cls, sin, cos] format.\\n                shape: (N, num_dir_bins * 4)\\n            locations (torch.Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Returns:\\n            tuple[torch.Tensor]: yaws and local yaws of 3d bboxes.\\n        '\n    if self.multibin:\n        pred_bin_cls = ori_vector[:, :self.num_dir_bins * 2].view(-1, self.num_dir_bins, 2)\n        pred_bin_cls = pred_bin_cls.softmax(dim=2)[..., 1]\n        orientations = ori_vector.new_zeros(ori_vector.shape[0])\n        for i in range(self.num_dir_bins):\n            mask_i = pred_bin_cls.argmax(dim=1) == i\n            start_bin = self.num_dir_bins * 2 + i * 2\n            end_bin = start_bin + 2\n            pred_bin_offset = ori_vector[mask_i, start_bin:end_bin]\n            orientations[mask_i] = pred_bin_offset[:, 0].atan2(pred_bin_offset[:, 1]) + self.bin_centers[i]\n    else:\n        axis_cls = ori_vector[:, :2].softmax(dim=1)\n        axis_cls = axis_cls[:, 0] < axis_cls[:, 1]\n        head_cls = ori_vector[:, 2:4].softmax(dim=1)\n        head_cls = head_cls[:, 0] < head_cls[:, 1]\n        orientations = self.bin_centers[axis_cls + head_cls * 2]\n        sin_cos_offset = F.normalize(ori_vector[:, 4:])\n        orientations += sin_cos_offset[:, 0].atan(sin_cos_offset[:, 1])\n    locations = locations.view(-1, 3)\n    rays = locations[:, 0].atan2(locations[:, 2])\n    local_yaws = orientations\n    yaws = local_yaws + rays\n    larger_idx = (yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        yaws[small_idx] += 2 * np.pi\n    larger_idx = (local_yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (local_yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        local_yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        local_yaws[small_idx] += 2 * np.pi\n    return (yaws, local_yaws)",
            "def decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (torch.Tensor): Local orientation vector\\n                in [axis_cls, head_cls, sin, cos] format.\\n                shape: (N, num_dir_bins * 4)\\n            locations (torch.Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Returns:\\n            tuple[torch.Tensor]: yaws and local yaws of 3d bboxes.\\n        '\n    if self.multibin:\n        pred_bin_cls = ori_vector[:, :self.num_dir_bins * 2].view(-1, self.num_dir_bins, 2)\n        pred_bin_cls = pred_bin_cls.softmax(dim=2)[..., 1]\n        orientations = ori_vector.new_zeros(ori_vector.shape[0])\n        for i in range(self.num_dir_bins):\n            mask_i = pred_bin_cls.argmax(dim=1) == i\n            start_bin = self.num_dir_bins * 2 + i * 2\n            end_bin = start_bin + 2\n            pred_bin_offset = ori_vector[mask_i, start_bin:end_bin]\n            orientations[mask_i] = pred_bin_offset[:, 0].atan2(pred_bin_offset[:, 1]) + self.bin_centers[i]\n    else:\n        axis_cls = ori_vector[:, :2].softmax(dim=1)\n        axis_cls = axis_cls[:, 0] < axis_cls[:, 1]\n        head_cls = ori_vector[:, 2:4].softmax(dim=1)\n        head_cls = head_cls[:, 0] < head_cls[:, 1]\n        orientations = self.bin_centers[axis_cls + head_cls * 2]\n        sin_cos_offset = F.normalize(ori_vector[:, 4:])\n        orientations += sin_cos_offset[:, 0].atan(sin_cos_offset[:, 1])\n    locations = locations.view(-1, 3)\n    rays = locations[:, 0].atan2(locations[:, 2])\n    local_yaws = orientations\n    yaws = local_yaws + rays\n    larger_idx = (yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        yaws[small_idx] += 2 * np.pi\n    larger_idx = (local_yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (local_yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        local_yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        local_yaws[small_idx] += 2 * np.pi\n    return (yaws, local_yaws)",
            "def decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (torch.Tensor): Local orientation vector\\n                in [axis_cls, head_cls, sin, cos] format.\\n                shape: (N, num_dir_bins * 4)\\n            locations (torch.Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Returns:\\n            tuple[torch.Tensor]: yaws and local yaws of 3d bboxes.\\n        '\n    if self.multibin:\n        pred_bin_cls = ori_vector[:, :self.num_dir_bins * 2].view(-1, self.num_dir_bins, 2)\n        pred_bin_cls = pred_bin_cls.softmax(dim=2)[..., 1]\n        orientations = ori_vector.new_zeros(ori_vector.shape[0])\n        for i in range(self.num_dir_bins):\n            mask_i = pred_bin_cls.argmax(dim=1) == i\n            start_bin = self.num_dir_bins * 2 + i * 2\n            end_bin = start_bin + 2\n            pred_bin_offset = ori_vector[mask_i, start_bin:end_bin]\n            orientations[mask_i] = pred_bin_offset[:, 0].atan2(pred_bin_offset[:, 1]) + self.bin_centers[i]\n    else:\n        axis_cls = ori_vector[:, :2].softmax(dim=1)\n        axis_cls = axis_cls[:, 0] < axis_cls[:, 1]\n        head_cls = ori_vector[:, 2:4].softmax(dim=1)\n        head_cls = head_cls[:, 0] < head_cls[:, 1]\n        orientations = self.bin_centers[axis_cls + head_cls * 2]\n        sin_cos_offset = F.normalize(ori_vector[:, 4:])\n        orientations += sin_cos_offset[:, 0].atan(sin_cos_offset[:, 1])\n    locations = locations.view(-1, 3)\n    rays = locations[:, 0].atan2(locations[:, 2])\n    local_yaws = orientations\n    yaws = local_yaws + rays\n    larger_idx = (yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        yaws[small_idx] += 2 * np.pi\n    larger_idx = (local_yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (local_yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        local_yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        local_yaws[small_idx] += 2 * np.pi\n    return (yaws, local_yaws)",
            "def decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (torch.Tensor): Local orientation vector\\n                in [axis_cls, head_cls, sin, cos] format.\\n                shape: (N, num_dir_bins * 4)\\n            locations (torch.Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Returns:\\n            tuple[torch.Tensor]: yaws and local yaws of 3d bboxes.\\n        '\n    if self.multibin:\n        pred_bin_cls = ori_vector[:, :self.num_dir_bins * 2].view(-1, self.num_dir_bins, 2)\n        pred_bin_cls = pred_bin_cls.softmax(dim=2)[..., 1]\n        orientations = ori_vector.new_zeros(ori_vector.shape[0])\n        for i in range(self.num_dir_bins):\n            mask_i = pred_bin_cls.argmax(dim=1) == i\n            start_bin = self.num_dir_bins * 2 + i * 2\n            end_bin = start_bin + 2\n            pred_bin_offset = ori_vector[mask_i, start_bin:end_bin]\n            orientations[mask_i] = pred_bin_offset[:, 0].atan2(pred_bin_offset[:, 1]) + self.bin_centers[i]\n    else:\n        axis_cls = ori_vector[:, :2].softmax(dim=1)\n        axis_cls = axis_cls[:, 0] < axis_cls[:, 1]\n        head_cls = ori_vector[:, 2:4].softmax(dim=1)\n        head_cls = head_cls[:, 0] < head_cls[:, 1]\n        orientations = self.bin_centers[axis_cls + head_cls * 2]\n        sin_cos_offset = F.normalize(ori_vector[:, 4:])\n        orientations += sin_cos_offset[:, 0].atan(sin_cos_offset[:, 1])\n    locations = locations.view(-1, 3)\n    rays = locations[:, 0].atan2(locations[:, 2])\n    local_yaws = orientations\n    yaws = local_yaws + rays\n    larger_idx = (yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        yaws[small_idx] += 2 * np.pi\n    larger_idx = (local_yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (local_yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        local_yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        local_yaws[small_idx] += 2 * np.pi\n    return (yaws, local_yaws)",
            "def decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (torch.Tensor): Local orientation vector\\n                in [axis_cls, head_cls, sin, cos] format.\\n                shape: (N, num_dir_bins * 4)\\n            locations (torch.Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Returns:\\n            tuple[torch.Tensor]: yaws and local yaws of 3d bboxes.\\n        '\n    if self.multibin:\n        pred_bin_cls = ori_vector[:, :self.num_dir_bins * 2].view(-1, self.num_dir_bins, 2)\n        pred_bin_cls = pred_bin_cls.softmax(dim=2)[..., 1]\n        orientations = ori_vector.new_zeros(ori_vector.shape[0])\n        for i in range(self.num_dir_bins):\n            mask_i = pred_bin_cls.argmax(dim=1) == i\n            start_bin = self.num_dir_bins * 2 + i * 2\n            end_bin = start_bin + 2\n            pred_bin_offset = ori_vector[mask_i, start_bin:end_bin]\n            orientations[mask_i] = pred_bin_offset[:, 0].atan2(pred_bin_offset[:, 1]) + self.bin_centers[i]\n    else:\n        axis_cls = ori_vector[:, :2].softmax(dim=1)\n        axis_cls = axis_cls[:, 0] < axis_cls[:, 1]\n        head_cls = ori_vector[:, 2:4].softmax(dim=1)\n        head_cls = head_cls[:, 0] < head_cls[:, 1]\n        orientations = self.bin_centers[axis_cls + head_cls * 2]\n        sin_cos_offset = F.normalize(ori_vector[:, 4:])\n        orientations += sin_cos_offset[:, 0].atan(sin_cos_offset[:, 1])\n    locations = locations.view(-1, 3)\n    rays = locations[:, 0].atan2(locations[:, 2])\n    local_yaws = orientations\n    yaws = local_yaws + rays\n    larger_idx = (yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        yaws[small_idx] += 2 * np.pi\n    larger_idx = (local_yaws > np.pi).nonzero(as_tuple=False)\n    small_idx = (local_yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_idx) != 0:\n        local_yaws[larger_idx] -= 2 * np.pi\n    if len(small_idx) != 0:\n        local_yaws[small_idx] += 2 * np.pi\n    return (yaws, local_yaws)"
        ]
    },
    {
        "func_name": "decode_bboxes2d",
        "original": "def decode_bboxes2d(self, reg_bboxes2d, base_centers2d):\n    \"\"\"Retrieve [x1, y1, x2, y2] format 2D bboxes.\n\n        Args:\n            reg_bboxes2d (torch.Tensor): Predicted FCOS style\n                2D bboxes.\n                shape: (N, 4)\n            base_centers2d (torch.Tensor): predicted base centers2d.\n                shape: (N, 2)\n\n        Returns:\n            torch.Tenosr: [x1, y1, x2, y2] format 2D bboxes.\n        \"\"\"\n    centers_x = base_centers2d[:, 0]\n    centers_y = base_centers2d[:, 1]\n    xs_min = centers_x - reg_bboxes2d[..., 0]\n    ys_min = centers_y - reg_bboxes2d[..., 1]\n    xs_max = centers_x + reg_bboxes2d[..., 2]\n    ys_max = centers_y + reg_bboxes2d[..., 3]\n    bboxes2d = torch.stack([xs_min, ys_min, xs_max, ys_max], dim=-1)\n    return bboxes2d",
        "mutated": [
            "def decode_bboxes2d(self, reg_bboxes2d, base_centers2d):\n    if False:\n        i = 10\n    'Retrieve [x1, y1, x2, y2] format 2D bboxes.\\n\\n        Args:\\n            reg_bboxes2d (torch.Tensor): Predicted FCOS style\\n                2D bboxes.\\n                shape: (N, 4)\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n\\n        Returns:\\n            torch.Tenosr: [x1, y1, x2, y2] format 2D bboxes.\\n        '\n    centers_x = base_centers2d[:, 0]\n    centers_y = base_centers2d[:, 1]\n    xs_min = centers_x - reg_bboxes2d[..., 0]\n    ys_min = centers_y - reg_bboxes2d[..., 1]\n    xs_max = centers_x + reg_bboxes2d[..., 2]\n    ys_max = centers_y + reg_bboxes2d[..., 3]\n    bboxes2d = torch.stack([xs_min, ys_min, xs_max, ys_max], dim=-1)\n    return bboxes2d",
            "def decode_bboxes2d(self, reg_bboxes2d, base_centers2d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Retrieve [x1, y1, x2, y2] format 2D bboxes.\\n\\n        Args:\\n            reg_bboxes2d (torch.Tensor): Predicted FCOS style\\n                2D bboxes.\\n                shape: (N, 4)\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n\\n        Returns:\\n            torch.Tenosr: [x1, y1, x2, y2] format 2D bboxes.\\n        '\n    centers_x = base_centers2d[:, 0]\n    centers_y = base_centers2d[:, 1]\n    xs_min = centers_x - reg_bboxes2d[..., 0]\n    ys_min = centers_y - reg_bboxes2d[..., 1]\n    xs_max = centers_x + reg_bboxes2d[..., 2]\n    ys_max = centers_y + reg_bboxes2d[..., 3]\n    bboxes2d = torch.stack([xs_min, ys_min, xs_max, ys_max], dim=-1)\n    return bboxes2d",
            "def decode_bboxes2d(self, reg_bboxes2d, base_centers2d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Retrieve [x1, y1, x2, y2] format 2D bboxes.\\n\\n        Args:\\n            reg_bboxes2d (torch.Tensor): Predicted FCOS style\\n                2D bboxes.\\n                shape: (N, 4)\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n\\n        Returns:\\n            torch.Tenosr: [x1, y1, x2, y2] format 2D bboxes.\\n        '\n    centers_x = base_centers2d[:, 0]\n    centers_y = base_centers2d[:, 1]\n    xs_min = centers_x - reg_bboxes2d[..., 0]\n    ys_min = centers_y - reg_bboxes2d[..., 1]\n    xs_max = centers_x + reg_bboxes2d[..., 2]\n    ys_max = centers_y + reg_bboxes2d[..., 3]\n    bboxes2d = torch.stack([xs_min, ys_min, xs_max, ys_max], dim=-1)\n    return bboxes2d",
            "def decode_bboxes2d(self, reg_bboxes2d, base_centers2d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Retrieve [x1, y1, x2, y2] format 2D bboxes.\\n\\n        Args:\\n            reg_bboxes2d (torch.Tensor): Predicted FCOS style\\n                2D bboxes.\\n                shape: (N, 4)\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n\\n        Returns:\\n            torch.Tenosr: [x1, y1, x2, y2] format 2D bboxes.\\n        '\n    centers_x = base_centers2d[:, 0]\n    centers_y = base_centers2d[:, 1]\n    xs_min = centers_x - reg_bboxes2d[..., 0]\n    ys_min = centers_y - reg_bboxes2d[..., 1]\n    xs_max = centers_x + reg_bboxes2d[..., 2]\n    ys_max = centers_y + reg_bboxes2d[..., 3]\n    bboxes2d = torch.stack([xs_min, ys_min, xs_max, ys_max], dim=-1)\n    return bboxes2d",
            "def decode_bboxes2d(self, reg_bboxes2d, base_centers2d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Retrieve [x1, y1, x2, y2] format 2D bboxes.\\n\\n        Args:\\n            reg_bboxes2d (torch.Tensor): Predicted FCOS style\\n                2D bboxes.\\n                shape: (N, 4)\\n            base_centers2d (torch.Tensor): predicted base centers2d.\\n                shape: (N, 2)\\n\\n        Returns:\\n            torch.Tenosr: [x1, y1, x2, y2] format 2D bboxes.\\n        '\n    centers_x = base_centers2d[:, 0]\n    centers_y = base_centers2d[:, 1]\n    xs_min = centers_x - reg_bboxes2d[..., 0]\n    ys_min = centers_y - reg_bboxes2d[..., 1]\n    xs_max = centers_x + reg_bboxes2d[..., 2]\n    ys_max = centers_y + reg_bboxes2d[..., 3]\n    bboxes2d = torch.stack([xs_min, ys_min, xs_max, ys_max], dim=-1)\n    return bboxes2d"
        ]
    },
    {
        "func_name": "combine_depths",
        "original": "def combine_depths(self, depth, depth_uncertainty):\n    \"\"\"Combine all the prediced depths with depth uncertainty.\n\n        Args:\n            depth (torch.Tensor): Predicted depths of each object.\n                2D bboxes.\n                shape: (N, 4)\n            depth_uncertainty (torch.Tensor): Depth uncertainty for\n                each depth of each object.\n                shape: (N, 4)\n\n        Returns:\n            torch.Tenosr: combined depth.\n        \"\"\"\n    uncertainty_weights = 1 / depth_uncertainty\n    uncertainty_weights = uncertainty_weights / uncertainty_weights.sum(dim=1, keepdim=True)\n    combined_depth = torch.sum(depth * uncertainty_weights, dim=1)\n    return combined_depth",
        "mutated": [
            "def combine_depths(self, depth, depth_uncertainty):\n    if False:\n        i = 10\n    'Combine all the prediced depths with depth uncertainty.\\n\\n        Args:\\n            depth (torch.Tensor): Predicted depths of each object.\\n                2D bboxes.\\n                shape: (N, 4)\\n            depth_uncertainty (torch.Tensor): Depth uncertainty for\\n                each depth of each object.\\n                shape: (N, 4)\\n\\n        Returns:\\n            torch.Tenosr: combined depth.\\n        '\n    uncertainty_weights = 1 / depth_uncertainty\n    uncertainty_weights = uncertainty_weights / uncertainty_weights.sum(dim=1, keepdim=True)\n    combined_depth = torch.sum(depth * uncertainty_weights, dim=1)\n    return combined_depth",
            "def combine_depths(self, depth, depth_uncertainty):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Combine all the prediced depths with depth uncertainty.\\n\\n        Args:\\n            depth (torch.Tensor): Predicted depths of each object.\\n                2D bboxes.\\n                shape: (N, 4)\\n            depth_uncertainty (torch.Tensor): Depth uncertainty for\\n                each depth of each object.\\n                shape: (N, 4)\\n\\n        Returns:\\n            torch.Tenosr: combined depth.\\n        '\n    uncertainty_weights = 1 / depth_uncertainty\n    uncertainty_weights = uncertainty_weights / uncertainty_weights.sum(dim=1, keepdim=True)\n    combined_depth = torch.sum(depth * uncertainty_weights, dim=1)\n    return combined_depth",
            "def combine_depths(self, depth, depth_uncertainty):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Combine all the prediced depths with depth uncertainty.\\n\\n        Args:\\n            depth (torch.Tensor): Predicted depths of each object.\\n                2D bboxes.\\n                shape: (N, 4)\\n            depth_uncertainty (torch.Tensor): Depth uncertainty for\\n                each depth of each object.\\n                shape: (N, 4)\\n\\n        Returns:\\n            torch.Tenosr: combined depth.\\n        '\n    uncertainty_weights = 1 / depth_uncertainty\n    uncertainty_weights = uncertainty_weights / uncertainty_weights.sum(dim=1, keepdim=True)\n    combined_depth = torch.sum(depth * uncertainty_weights, dim=1)\n    return combined_depth",
            "def combine_depths(self, depth, depth_uncertainty):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Combine all the prediced depths with depth uncertainty.\\n\\n        Args:\\n            depth (torch.Tensor): Predicted depths of each object.\\n                2D bboxes.\\n                shape: (N, 4)\\n            depth_uncertainty (torch.Tensor): Depth uncertainty for\\n                each depth of each object.\\n                shape: (N, 4)\\n\\n        Returns:\\n            torch.Tenosr: combined depth.\\n        '\n    uncertainty_weights = 1 / depth_uncertainty\n    uncertainty_weights = uncertainty_weights / uncertainty_weights.sum(dim=1, keepdim=True)\n    combined_depth = torch.sum(depth * uncertainty_weights, dim=1)\n    return combined_depth",
            "def combine_depths(self, depth, depth_uncertainty):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Combine all the prediced depths with depth uncertainty.\\n\\n        Args:\\n            depth (torch.Tensor): Predicted depths of each object.\\n                2D bboxes.\\n                shape: (N, 4)\\n            depth_uncertainty (torch.Tensor): Depth uncertainty for\\n                each depth of each object.\\n                shape: (N, 4)\\n\\n        Returns:\\n            torch.Tenosr: combined depth.\\n        '\n    uncertainty_weights = 1 / depth_uncertainty\n    uncertainty_weights = uncertainty_weights / uncertainty_weights.sum(dim=1, keepdim=True)\n    combined_depth = torch.sum(depth * uncertainty_weights, dim=1)\n    return combined_depth"
        ]
    }
]