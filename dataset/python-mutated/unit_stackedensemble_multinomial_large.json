[
    {
        "func_name": "stackedensemble_multinomial_test",
        "original": "def stackedensemble_multinomial_test():\n    \"\"\"This test check the following (for multinomial regression):\n    1) That H2OStackedEnsembleEstimator executes w/o errors on a 6-model manually constructed ensemble.\n    2) That .predict() works on a stack.\n    3) That .model_performance() works on a stack.\n    4) That test performance is better on ensemble vs the base learners.\n    5) That the validation_frame arg on H2OStackedEnsembleEstimator works correctly.\n    \"\"\"\n    df = h2o.import_file(path=pyunit_utils.locate('bigdata/laptop/mnist/test.csv.gz'))\n    y = 'C785'\n    x = list(range(784))\n    df[y] = df[y].asfactor()\n    train = df[0:5000, :]\n    test = df[5000:10000, :]\n    nfolds = 2\n    my_gbm = H2OGradientBoostingEstimator(distribution='multinomial', nfolds=nfolds, ntrees=10, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_gbm.train(x=x, y=y, training_frame=train)\n    perf_gbm_train = my_gbm.model_performance()\n    perf_gbm_test = my_gbm.model_performance(test_data=test)\n    print('GBM training performance: ')\n    print(perf_gbm_train)\n    print('GBM test performance: ')\n    print(perf_gbm_test)\n    my_rf = H2ORandomForestEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_rf.train(x=x, y=y, training_frame=train)\n    perf_rf_train = my_rf.model_performance()\n    perf_rf_test = my_rf.model_performance(test_data=test)\n    print('RF training performance: ')\n    print(perf_rf_train)\n    print('RF test performance: ')\n    print(perf_rf_test)\n    my_xgb = H2OXGBoostEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_xgb.train(x=x, y=y, training_frame=train)\n    perf_xgb_train = my_xgb.model_performance()\n    perf_xgb_test = my_xgb.model_performance(test_data=test)\n    print('XGB training performance: ')\n    print(perf_xgb_train)\n    print('XGB test performance: ')\n    print(perf_xgb_test)\n    my_nb = H2ONaiveBayesEstimator(nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_nb.train(x=x, y=y, training_frame=train)\n    perf_nb_train = my_nb.model_performance()\n    perf_nb_test = my_nb.model_performance(test_data=test)\n    print('NB training performance: ')\n    print(perf_nb_train)\n    print('NB test performance: ')\n    print(perf_nb_test)\n    my_dnn = H2ODeepLearningEstimator(hidden=[10, 10], nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_dnn.train(x=x, y=y, training_frame=train)\n    perf_dnn_train = my_dnn.model_performance()\n    perf_dnn_test = my_dnn.model_performance(test_data=test)\n    print('DNN training performance: ')\n    print(perf_dnn_train)\n    print('DNN test performance: ')\n    print(perf_dnn_test)\n    my_glm = H2OGeneralizedLinearEstimator(family='multinomial', nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_glm.train(x=x, y=y, training_frame=train)\n    perf_glm_train = my_glm.model_performance()\n    perf_glm_test = my_glm.model_performance(test_data=test)\n    print('GLM training performance: ')\n    print(perf_glm_train)\n    print('GLM test performance: ')\n    print(perf_glm_test)\n    stack = H2OStackedEnsembleEstimator(base_models=[my_gbm.model_id, my_rf.model_id, my_xgb.model_id, my_nb.model_id, my_dnn.model_id, my_glm.model_id])\n    stack.train(x=x, y=y, training_frame=train, validation_frame=test)\n    assert isinstance(stack, h2o.estimators.stackedensemble.H2OStackedEnsembleEstimator)\n    assert stack.type == 'classifier'\n    pred = stack.predict(test_data=test)\n    print(pred)\n    assert pred.nrow == test.nrow, 'expected ' + str(pred.nrow) + ' to be equal to ' + str(test.nrow)\n    assert pred.ncol == 11, 'expected ' + str(pred.ncol) + ' to be equal to 1 but it was equal to ' + str(pred.ncol)\n    perf_stack_train = stack.model_performance()\n    assert isinstance(perf_stack_train, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_valid = stack.model_performance(valid=True)\n    assert isinstance(perf_stack_valid, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_test = stack.model_performance(test_data=test)\n    assert isinstance(perf_stack_test, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    baselearner_best_mean_per_class_error_test = min(perf_gbm_test.mean_per_class_error(), perf_rf_test.mean_per_class_error(), perf_xgb_test.mean_per_class_error(), perf_nb_test.mean_per_class_error(), perf_dnn_test.mean_per_class_error(), perf_glm_test.mean_per_class_error())\n    stack_mean_per_class_error_test = perf_stack_test.mean_per_class_error()\n    print('Best Base-learner Test Mean Per Class Error:  {0}'.format(baselearner_best_mean_per_class_error_test))\n    print('Ensemble Test Mean Per Class Error:  {0}'.format(stack_mean_per_class_error_test))\n    assert stack_mean_per_class_error_test <= baselearner_best_mean_per_class_error_test, +\"expected stack_mean_per_class_error_test would be less than  baselearner_best_mean_per_class_error_test, found it wasn't  baselearner_best_mean_per_class_error_test = \" + str(baselearner_best_mean_per_class_error_test) + ',stack_mean_per_class_error_test = ' + str(stack_mean_per_class_error_test)\n    perf_stack_validation_frame = stack.model_performance(valid=True)\n    assert stack_mean_per_class_error_test == perf_stack_validation_frame.mean_per_class_error(), \"expected stack_mean_per_class_error_test to be the same as perf_stack_validation_frame.mean_per_class_error() found it wasn'tperf_stack_validation_frame.mean_per_class_error() = \" + str(perf_stack_validation_frame.mean_per_class_error()) + 'stack_mean_per_class_error_test was ' + str(stack_mean_per_class_error_test)",
        "mutated": [
            "def stackedensemble_multinomial_test():\n    if False:\n        i = 10\n    'This test check the following (for multinomial regression):\\n    1) That H2OStackedEnsembleEstimator executes w/o errors on a 6-model manually constructed ensemble.\\n    2) That .predict() works on a stack.\\n    3) That .model_performance() works on a stack.\\n    4) That test performance is better on ensemble vs the base learners.\\n    5) That the validation_frame arg on H2OStackedEnsembleEstimator works correctly.\\n    '\n    df = h2o.import_file(path=pyunit_utils.locate('bigdata/laptop/mnist/test.csv.gz'))\n    y = 'C785'\n    x = list(range(784))\n    df[y] = df[y].asfactor()\n    train = df[0:5000, :]\n    test = df[5000:10000, :]\n    nfolds = 2\n    my_gbm = H2OGradientBoostingEstimator(distribution='multinomial', nfolds=nfolds, ntrees=10, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_gbm.train(x=x, y=y, training_frame=train)\n    perf_gbm_train = my_gbm.model_performance()\n    perf_gbm_test = my_gbm.model_performance(test_data=test)\n    print('GBM training performance: ')\n    print(perf_gbm_train)\n    print('GBM test performance: ')\n    print(perf_gbm_test)\n    my_rf = H2ORandomForestEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_rf.train(x=x, y=y, training_frame=train)\n    perf_rf_train = my_rf.model_performance()\n    perf_rf_test = my_rf.model_performance(test_data=test)\n    print('RF training performance: ')\n    print(perf_rf_train)\n    print('RF test performance: ')\n    print(perf_rf_test)\n    my_xgb = H2OXGBoostEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_xgb.train(x=x, y=y, training_frame=train)\n    perf_xgb_train = my_xgb.model_performance()\n    perf_xgb_test = my_xgb.model_performance(test_data=test)\n    print('XGB training performance: ')\n    print(perf_xgb_train)\n    print('XGB test performance: ')\n    print(perf_xgb_test)\n    my_nb = H2ONaiveBayesEstimator(nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_nb.train(x=x, y=y, training_frame=train)\n    perf_nb_train = my_nb.model_performance()\n    perf_nb_test = my_nb.model_performance(test_data=test)\n    print('NB training performance: ')\n    print(perf_nb_train)\n    print('NB test performance: ')\n    print(perf_nb_test)\n    my_dnn = H2ODeepLearningEstimator(hidden=[10, 10], nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_dnn.train(x=x, y=y, training_frame=train)\n    perf_dnn_train = my_dnn.model_performance()\n    perf_dnn_test = my_dnn.model_performance(test_data=test)\n    print('DNN training performance: ')\n    print(perf_dnn_train)\n    print('DNN test performance: ')\n    print(perf_dnn_test)\n    my_glm = H2OGeneralizedLinearEstimator(family='multinomial', nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_glm.train(x=x, y=y, training_frame=train)\n    perf_glm_train = my_glm.model_performance()\n    perf_glm_test = my_glm.model_performance(test_data=test)\n    print('GLM training performance: ')\n    print(perf_glm_train)\n    print('GLM test performance: ')\n    print(perf_glm_test)\n    stack = H2OStackedEnsembleEstimator(base_models=[my_gbm.model_id, my_rf.model_id, my_xgb.model_id, my_nb.model_id, my_dnn.model_id, my_glm.model_id])\n    stack.train(x=x, y=y, training_frame=train, validation_frame=test)\n    assert isinstance(stack, h2o.estimators.stackedensemble.H2OStackedEnsembleEstimator)\n    assert stack.type == 'classifier'\n    pred = stack.predict(test_data=test)\n    print(pred)\n    assert pred.nrow == test.nrow, 'expected ' + str(pred.nrow) + ' to be equal to ' + str(test.nrow)\n    assert pred.ncol == 11, 'expected ' + str(pred.ncol) + ' to be equal to 1 but it was equal to ' + str(pred.ncol)\n    perf_stack_train = stack.model_performance()\n    assert isinstance(perf_stack_train, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_valid = stack.model_performance(valid=True)\n    assert isinstance(perf_stack_valid, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_test = stack.model_performance(test_data=test)\n    assert isinstance(perf_stack_test, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    baselearner_best_mean_per_class_error_test = min(perf_gbm_test.mean_per_class_error(), perf_rf_test.mean_per_class_error(), perf_xgb_test.mean_per_class_error(), perf_nb_test.mean_per_class_error(), perf_dnn_test.mean_per_class_error(), perf_glm_test.mean_per_class_error())\n    stack_mean_per_class_error_test = perf_stack_test.mean_per_class_error()\n    print('Best Base-learner Test Mean Per Class Error:  {0}'.format(baselearner_best_mean_per_class_error_test))\n    print('Ensemble Test Mean Per Class Error:  {0}'.format(stack_mean_per_class_error_test))\n    assert stack_mean_per_class_error_test <= baselearner_best_mean_per_class_error_test, +\"expected stack_mean_per_class_error_test would be less than  baselearner_best_mean_per_class_error_test, found it wasn't  baselearner_best_mean_per_class_error_test = \" + str(baselearner_best_mean_per_class_error_test) + ',stack_mean_per_class_error_test = ' + str(stack_mean_per_class_error_test)\n    perf_stack_validation_frame = stack.model_performance(valid=True)\n    assert stack_mean_per_class_error_test == perf_stack_validation_frame.mean_per_class_error(), \"expected stack_mean_per_class_error_test to be the same as perf_stack_validation_frame.mean_per_class_error() found it wasn'tperf_stack_validation_frame.mean_per_class_error() = \" + str(perf_stack_validation_frame.mean_per_class_error()) + 'stack_mean_per_class_error_test was ' + str(stack_mean_per_class_error_test)",
            "def stackedensemble_multinomial_test():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'This test check the following (for multinomial regression):\\n    1) That H2OStackedEnsembleEstimator executes w/o errors on a 6-model manually constructed ensemble.\\n    2) That .predict() works on a stack.\\n    3) That .model_performance() works on a stack.\\n    4) That test performance is better on ensemble vs the base learners.\\n    5) That the validation_frame arg on H2OStackedEnsembleEstimator works correctly.\\n    '\n    df = h2o.import_file(path=pyunit_utils.locate('bigdata/laptop/mnist/test.csv.gz'))\n    y = 'C785'\n    x = list(range(784))\n    df[y] = df[y].asfactor()\n    train = df[0:5000, :]\n    test = df[5000:10000, :]\n    nfolds = 2\n    my_gbm = H2OGradientBoostingEstimator(distribution='multinomial', nfolds=nfolds, ntrees=10, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_gbm.train(x=x, y=y, training_frame=train)\n    perf_gbm_train = my_gbm.model_performance()\n    perf_gbm_test = my_gbm.model_performance(test_data=test)\n    print('GBM training performance: ')\n    print(perf_gbm_train)\n    print('GBM test performance: ')\n    print(perf_gbm_test)\n    my_rf = H2ORandomForestEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_rf.train(x=x, y=y, training_frame=train)\n    perf_rf_train = my_rf.model_performance()\n    perf_rf_test = my_rf.model_performance(test_data=test)\n    print('RF training performance: ')\n    print(perf_rf_train)\n    print('RF test performance: ')\n    print(perf_rf_test)\n    my_xgb = H2OXGBoostEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_xgb.train(x=x, y=y, training_frame=train)\n    perf_xgb_train = my_xgb.model_performance()\n    perf_xgb_test = my_xgb.model_performance(test_data=test)\n    print('XGB training performance: ')\n    print(perf_xgb_train)\n    print('XGB test performance: ')\n    print(perf_xgb_test)\n    my_nb = H2ONaiveBayesEstimator(nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_nb.train(x=x, y=y, training_frame=train)\n    perf_nb_train = my_nb.model_performance()\n    perf_nb_test = my_nb.model_performance(test_data=test)\n    print('NB training performance: ')\n    print(perf_nb_train)\n    print('NB test performance: ')\n    print(perf_nb_test)\n    my_dnn = H2ODeepLearningEstimator(hidden=[10, 10], nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_dnn.train(x=x, y=y, training_frame=train)\n    perf_dnn_train = my_dnn.model_performance()\n    perf_dnn_test = my_dnn.model_performance(test_data=test)\n    print('DNN training performance: ')\n    print(perf_dnn_train)\n    print('DNN test performance: ')\n    print(perf_dnn_test)\n    my_glm = H2OGeneralizedLinearEstimator(family='multinomial', nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_glm.train(x=x, y=y, training_frame=train)\n    perf_glm_train = my_glm.model_performance()\n    perf_glm_test = my_glm.model_performance(test_data=test)\n    print('GLM training performance: ')\n    print(perf_glm_train)\n    print('GLM test performance: ')\n    print(perf_glm_test)\n    stack = H2OStackedEnsembleEstimator(base_models=[my_gbm.model_id, my_rf.model_id, my_xgb.model_id, my_nb.model_id, my_dnn.model_id, my_glm.model_id])\n    stack.train(x=x, y=y, training_frame=train, validation_frame=test)\n    assert isinstance(stack, h2o.estimators.stackedensemble.H2OStackedEnsembleEstimator)\n    assert stack.type == 'classifier'\n    pred = stack.predict(test_data=test)\n    print(pred)\n    assert pred.nrow == test.nrow, 'expected ' + str(pred.nrow) + ' to be equal to ' + str(test.nrow)\n    assert pred.ncol == 11, 'expected ' + str(pred.ncol) + ' to be equal to 1 but it was equal to ' + str(pred.ncol)\n    perf_stack_train = stack.model_performance()\n    assert isinstance(perf_stack_train, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_valid = stack.model_performance(valid=True)\n    assert isinstance(perf_stack_valid, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_test = stack.model_performance(test_data=test)\n    assert isinstance(perf_stack_test, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    baselearner_best_mean_per_class_error_test = min(perf_gbm_test.mean_per_class_error(), perf_rf_test.mean_per_class_error(), perf_xgb_test.mean_per_class_error(), perf_nb_test.mean_per_class_error(), perf_dnn_test.mean_per_class_error(), perf_glm_test.mean_per_class_error())\n    stack_mean_per_class_error_test = perf_stack_test.mean_per_class_error()\n    print('Best Base-learner Test Mean Per Class Error:  {0}'.format(baselearner_best_mean_per_class_error_test))\n    print('Ensemble Test Mean Per Class Error:  {0}'.format(stack_mean_per_class_error_test))\n    assert stack_mean_per_class_error_test <= baselearner_best_mean_per_class_error_test, +\"expected stack_mean_per_class_error_test would be less than  baselearner_best_mean_per_class_error_test, found it wasn't  baselearner_best_mean_per_class_error_test = \" + str(baselearner_best_mean_per_class_error_test) + ',stack_mean_per_class_error_test = ' + str(stack_mean_per_class_error_test)\n    perf_stack_validation_frame = stack.model_performance(valid=True)\n    assert stack_mean_per_class_error_test == perf_stack_validation_frame.mean_per_class_error(), \"expected stack_mean_per_class_error_test to be the same as perf_stack_validation_frame.mean_per_class_error() found it wasn'tperf_stack_validation_frame.mean_per_class_error() = \" + str(perf_stack_validation_frame.mean_per_class_error()) + 'stack_mean_per_class_error_test was ' + str(stack_mean_per_class_error_test)",
            "def stackedensemble_multinomial_test():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'This test check the following (for multinomial regression):\\n    1) That H2OStackedEnsembleEstimator executes w/o errors on a 6-model manually constructed ensemble.\\n    2) That .predict() works on a stack.\\n    3) That .model_performance() works on a stack.\\n    4) That test performance is better on ensemble vs the base learners.\\n    5) That the validation_frame arg on H2OStackedEnsembleEstimator works correctly.\\n    '\n    df = h2o.import_file(path=pyunit_utils.locate('bigdata/laptop/mnist/test.csv.gz'))\n    y = 'C785'\n    x = list(range(784))\n    df[y] = df[y].asfactor()\n    train = df[0:5000, :]\n    test = df[5000:10000, :]\n    nfolds = 2\n    my_gbm = H2OGradientBoostingEstimator(distribution='multinomial', nfolds=nfolds, ntrees=10, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_gbm.train(x=x, y=y, training_frame=train)\n    perf_gbm_train = my_gbm.model_performance()\n    perf_gbm_test = my_gbm.model_performance(test_data=test)\n    print('GBM training performance: ')\n    print(perf_gbm_train)\n    print('GBM test performance: ')\n    print(perf_gbm_test)\n    my_rf = H2ORandomForestEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_rf.train(x=x, y=y, training_frame=train)\n    perf_rf_train = my_rf.model_performance()\n    perf_rf_test = my_rf.model_performance(test_data=test)\n    print('RF training performance: ')\n    print(perf_rf_train)\n    print('RF test performance: ')\n    print(perf_rf_test)\n    my_xgb = H2OXGBoostEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_xgb.train(x=x, y=y, training_frame=train)\n    perf_xgb_train = my_xgb.model_performance()\n    perf_xgb_test = my_xgb.model_performance(test_data=test)\n    print('XGB training performance: ')\n    print(perf_xgb_train)\n    print('XGB test performance: ')\n    print(perf_xgb_test)\n    my_nb = H2ONaiveBayesEstimator(nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_nb.train(x=x, y=y, training_frame=train)\n    perf_nb_train = my_nb.model_performance()\n    perf_nb_test = my_nb.model_performance(test_data=test)\n    print('NB training performance: ')\n    print(perf_nb_train)\n    print('NB test performance: ')\n    print(perf_nb_test)\n    my_dnn = H2ODeepLearningEstimator(hidden=[10, 10], nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_dnn.train(x=x, y=y, training_frame=train)\n    perf_dnn_train = my_dnn.model_performance()\n    perf_dnn_test = my_dnn.model_performance(test_data=test)\n    print('DNN training performance: ')\n    print(perf_dnn_train)\n    print('DNN test performance: ')\n    print(perf_dnn_test)\n    my_glm = H2OGeneralizedLinearEstimator(family='multinomial', nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_glm.train(x=x, y=y, training_frame=train)\n    perf_glm_train = my_glm.model_performance()\n    perf_glm_test = my_glm.model_performance(test_data=test)\n    print('GLM training performance: ')\n    print(perf_glm_train)\n    print('GLM test performance: ')\n    print(perf_glm_test)\n    stack = H2OStackedEnsembleEstimator(base_models=[my_gbm.model_id, my_rf.model_id, my_xgb.model_id, my_nb.model_id, my_dnn.model_id, my_glm.model_id])\n    stack.train(x=x, y=y, training_frame=train, validation_frame=test)\n    assert isinstance(stack, h2o.estimators.stackedensemble.H2OStackedEnsembleEstimator)\n    assert stack.type == 'classifier'\n    pred = stack.predict(test_data=test)\n    print(pred)\n    assert pred.nrow == test.nrow, 'expected ' + str(pred.nrow) + ' to be equal to ' + str(test.nrow)\n    assert pred.ncol == 11, 'expected ' + str(pred.ncol) + ' to be equal to 1 but it was equal to ' + str(pred.ncol)\n    perf_stack_train = stack.model_performance()\n    assert isinstance(perf_stack_train, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_valid = stack.model_performance(valid=True)\n    assert isinstance(perf_stack_valid, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_test = stack.model_performance(test_data=test)\n    assert isinstance(perf_stack_test, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    baselearner_best_mean_per_class_error_test = min(perf_gbm_test.mean_per_class_error(), perf_rf_test.mean_per_class_error(), perf_xgb_test.mean_per_class_error(), perf_nb_test.mean_per_class_error(), perf_dnn_test.mean_per_class_error(), perf_glm_test.mean_per_class_error())\n    stack_mean_per_class_error_test = perf_stack_test.mean_per_class_error()\n    print('Best Base-learner Test Mean Per Class Error:  {0}'.format(baselearner_best_mean_per_class_error_test))\n    print('Ensemble Test Mean Per Class Error:  {0}'.format(stack_mean_per_class_error_test))\n    assert stack_mean_per_class_error_test <= baselearner_best_mean_per_class_error_test, +\"expected stack_mean_per_class_error_test would be less than  baselearner_best_mean_per_class_error_test, found it wasn't  baselearner_best_mean_per_class_error_test = \" + str(baselearner_best_mean_per_class_error_test) + ',stack_mean_per_class_error_test = ' + str(stack_mean_per_class_error_test)\n    perf_stack_validation_frame = stack.model_performance(valid=True)\n    assert stack_mean_per_class_error_test == perf_stack_validation_frame.mean_per_class_error(), \"expected stack_mean_per_class_error_test to be the same as perf_stack_validation_frame.mean_per_class_error() found it wasn'tperf_stack_validation_frame.mean_per_class_error() = \" + str(perf_stack_validation_frame.mean_per_class_error()) + 'stack_mean_per_class_error_test was ' + str(stack_mean_per_class_error_test)",
            "def stackedensemble_multinomial_test():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'This test check the following (for multinomial regression):\\n    1) That H2OStackedEnsembleEstimator executes w/o errors on a 6-model manually constructed ensemble.\\n    2) That .predict() works on a stack.\\n    3) That .model_performance() works on a stack.\\n    4) That test performance is better on ensemble vs the base learners.\\n    5) That the validation_frame arg on H2OStackedEnsembleEstimator works correctly.\\n    '\n    df = h2o.import_file(path=pyunit_utils.locate('bigdata/laptop/mnist/test.csv.gz'))\n    y = 'C785'\n    x = list(range(784))\n    df[y] = df[y].asfactor()\n    train = df[0:5000, :]\n    test = df[5000:10000, :]\n    nfolds = 2\n    my_gbm = H2OGradientBoostingEstimator(distribution='multinomial', nfolds=nfolds, ntrees=10, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_gbm.train(x=x, y=y, training_frame=train)\n    perf_gbm_train = my_gbm.model_performance()\n    perf_gbm_test = my_gbm.model_performance(test_data=test)\n    print('GBM training performance: ')\n    print(perf_gbm_train)\n    print('GBM test performance: ')\n    print(perf_gbm_test)\n    my_rf = H2ORandomForestEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_rf.train(x=x, y=y, training_frame=train)\n    perf_rf_train = my_rf.model_performance()\n    perf_rf_test = my_rf.model_performance(test_data=test)\n    print('RF training performance: ')\n    print(perf_rf_train)\n    print('RF test performance: ')\n    print(perf_rf_test)\n    my_xgb = H2OXGBoostEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_xgb.train(x=x, y=y, training_frame=train)\n    perf_xgb_train = my_xgb.model_performance()\n    perf_xgb_test = my_xgb.model_performance(test_data=test)\n    print('XGB training performance: ')\n    print(perf_xgb_train)\n    print('XGB test performance: ')\n    print(perf_xgb_test)\n    my_nb = H2ONaiveBayesEstimator(nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_nb.train(x=x, y=y, training_frame=train)\n    perf_nb_train = my_nb.model_performance()\n    perf_nb_test = my_nb.model_performance(test_data=test)\n    print('NB training performance: ')\n    print(perf_nb_train)\n    print('NB test performance: ')\n    print(perf_nb_test)\n    my_dnn = H2ODeepLearningEstimator(hidden=[10, 10], nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_dnn.train(x=x, y=y, training_frame=train)\n    perf_dnn_train = my_dnn.model_performance()\n    perf_dnn_test = my_dnn.model_performance(test_data=test)\n    print('DNN training performance: ')\n    print(perf_dnn_train)\n    print('DNN test performance: ')\n    print(perf_dnn_test)\n    my_glm = H2OGeneralizedLinearEstimator(family='multinomial', nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_glm.train(x=x, y=y, training_frame=train)\n    perf_glm_train = my_glm.model_performance()\n    perf_glm_test = my_glm.model_performance(test_data=test)\n    print('GLM training performance: ')\n    print(perf_glm_train)\n    print('GLM test performance: ')\n    print(perf_glm_test)\n    stack = H2OStackedEnsembleEstimator(base_models=[my_gbm.model_id, my_rf.model_id, my_xgb.model_id, my_nb.model_id, my_dnn.model_id, my_glm.model_id])\n    stack.train(x=x, y=y, training_frame=train, validation_frame=test)\n    assert isinstance(stack, h2o.estimators.stackedensemble.H2OStackedEnsembleEstimator)\n    assert stack.type == 'classifier'\n    pred = stack.predict(test_data=test)\n    print(pred)\n    assert pred.nrow == test.nrow, 'expected ' + str(pred.nrow) + ' to be equal to ' + str(test.nrow)\n    assert pred.ncol == 11, 'expected ' + str(pred.ncol) + ' to be equal to 1 but it was equal to ' + str(pred.ncol)\n    perf_stack_train = stack.model_performance()\n    assert isinstance(perf_stack_train, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_valid = stack.model_performance(valid=True)\n    assert isinstance(perf_stack_valid, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_test = stack.model_performance(test_data=test)\n    assert isinstance(perf_stack_test, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    baselearner_best_mean_per_class_error_test = min(perf_gbm_test.mean_per_class_error(), perf_rf_test.mean_per_class_error(), perf_xgb_test.mean_per_class_error(), perf_nb_test.mean_per_class_error(), perf_dnn_test.mean_per_class_error(), perf_glm_test.mean_per_class_error())\n    stack_mean_per_class_error_test = perf_stack_test.mean_per_class_error()\n    print('Best Base-learner Test Mean Per Class Error:  {0}'.format(baselearner_best_mean_per_class_error_test))\n    print('Ensemble Test Mean Per Class Error:  {0}'.format(stack_mean_per_class_error_test))\n    assert stack_mean_per_class_error_test <= baselearner_best_mean_per_class_error_test, +\"expected stack_mean_per_class_error_test would be less than  baselearner_best_mean_per_class_error_test, found it wasn't  baselearner_best_mean_per_class_error_test = \" + str(baselearner_best_mean_per_class_error_test) + ',stack_mean_per_class_error_test = ' + str(stack_mean_per_class_error_test)\n    perf_stack_validation_frame = stack.model_performance(valid=True)\n    assert stack_mean_per_class_error_test == perf_stack_validation_frame.mean_per_class_error(), \"expected stack_mean_per_class_error_test to be the same as perf_stack_validation_frame.mean_per_class_error() found it wasn'tperf_stack_validation_frame.mean_per_class_error() = \" + str(perf_stack_validation_frame.mean_per_class_error()) + 'stack_mean_per_class_error_test was ' + str(stack_mean_per_class_error_test)",
            "def stackedensemble_multinomial_test():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'This test check the following (for multinomial regression):\\n    1) That H2OStackedEnsembleEstimator executes w/o errors on a 6-model manually constructed ensemble.\\n    2) That .predict() works on a stack.\\n    3) That .model_performance() works on a stack.\\n    4) That test performance is better on ensemble vs the base learners.\\n    5) That the validation_frame arg on H2OStackedEnsembleEstimator works correctly.\\n    '\n    df = h2o.import_file(path=pyunit_utils.locate('bigdata/laptop/mnist/test.csv.gz'))\n    y = 'C785'\n    x = list(range(784))\n    df[y] = df[y].asfactor()\n    train = df[0:5000, :]\n    test = df[5000:10000, :]\n    nfolds = 2\n    my_gbm = H2OGradientBoostingEstimator(distribution='multinomial', nfolds=nfolds, ntrees=10, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_gbm.train(x=x, y=y, training_frame=train)\n    perf_gbm_train = my_gbm.model_performance()\n    perf_gbm_test = my_gbm.model_performance(test_data=test)\n    print('GBM training performance: ')\n    print(perf_gbm_train)\n    print('GBM test performance: ')\n    print(perf_gbm_test)\n    my_rf = H2ORandomForestEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_rf.train(x=x, y=y, training_frame=train)\n    perf_rf_train = my_rf.model_performance()\n    perf_rf_test = my_rf.model_performance(test_data=test)\n    print('RF training performance: ')\n    print(perf_rf_train)\n    print('RF test performance: ')\n    print(perf_rf_test)\n    my_xgb = H2OXGBoostEstimator(ntrees=10, nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_xgb.train(x=x, y=y, training_frame=train)\n    perf_xgb_train = my_xgb.model_performance()\n    perf_xgb_test = my_xgb.model_performance(test_data=test)\n    print('XGB training performance: ')\n    print(perf_xgb_train)\n    print('XGB test performance: ')\n    print(perf_xgb_test)\n    my_nb = H2ONaiveBayesEstimator(nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_nb.train(x=x, y=y, training_frame=train)\n    perf_nb_train = my_nb.model_performance()\n    perf_nb_test = my_nb.model_performance(test_data=test)\n    print('NB training performance: ')\n    print(perf_nb_train)\n    print('NB test performance: ')\n    print(perf_nb_test)\n    my_dnn = H2ODeepLearningEstimator(hidden=[10, 10], nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_dnn.train(x=x, y=y, training_frame=train)\n    perf_dnn_train = my_dnn.model_performance()\n    perf_dnn_test = my_dnn.model_performance(test_data=test)\n    print('DNN training performance: ')\n    print(perf_dnn_train)\n    print('DNN test performance: ')\n    print(perf_dnn_test)\n    my_glm = H2OGeneralizedLinearEstimator(family='multinomial', nfolds=nfolds, fold_assignment='Modulo', keep_cross_validation_predictions=True, seed=1)\n    my_glm.train(x=x, y=y, training_frame=train)\n    perf_glm_train = my_glm.model_performance()\n    perf_glm_test = my_glm.model_performance(test_data=test)\n    print('GLM training performance: ')\n    print(perf_glm_train)\n    print('GLM test performance: ')\n    print(perf_glm_test)\n    stack = H2OStackedEnsembleEstimator(base_models=[my_gbm.model_id, my_rf.model_id, my_xgb.model_id, my_nb.model_id, my_dnn.model_id, my_glm.model_id])\n    stack.train(x=x, y=y, training_frame=train, validation_frame=test)\n    assert isinstance(stack, h2o.estimators.stackedensemble.H2OStackedEnsembleEstimator)\n    assert stack.type == 'classifier'\n    pred = stack.predict(test_data=test)\n    print(pred)\n    assert pred.nrow == test.nrow, 'expected ' + str(pred.nrow) + ' to be equal to ' + str(test.nrow)\n    assert pred.ncol == 11, 'expected ' + str(pred.ncol) + ' to be equal to 1 but it was equal to ' + str(pred.ncol)\n    perf_stack_train = stack.model_performance()\n    assert isinstance(perf_stack_train, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_valid = stack.model_performance(valid=True)\n    assert isinstance(perf_stack_valid, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    perf_stack_test = stack.model_performance(test_data=test)\n    assert isinstance(perf_stack_test, h2o.model.metrics.multinomial.H2OMultinomialModelMetrics)\n    baselearner_best_mean_per_class_error_test = min(perf_gbm_test.mean_per_class_error(), perf_rf_test.mean_per_class_error(), perf_xgb_test.mean_per_class_error(), perf_nb_test.mean_per_class_error(), perf_dnn_test.mean_per_class_error(), perf_glm_test.mean_per_class_error())\n    stack_mean_per_class_error_test = perf_stack_test.mean_per_class_error()\n    print('Best Base-learner Test Mean Per Class Error:  {0}'.format(baselearner_best_mean_per_class_error_test))\n    print('Ensemble Test Mean Per Class Error:  {0}'.format(stack_mean_per_class_error_test))\n    assert stack_mean_per_class_error_test <= baselearner_best_mean_per_class_error_test, +\"expected stack_mean_per_class_error_test would be less than  baselearner_best_mean_per_class_error_test, found it wasn't  baselearner_best_mean_per_class_error_test = \" + str(baselearner_best_mean_per_class_error_test) + ',stack_mean_per_class_error_test = ' + str(stack_mean_per_class_error_test)\n    perf_stack_validation_frame = stack.model_performance(valid=True)\n    assert stack_mean_per_class_error_test == perf_stack_validation_frame.mean_per_class_error(), \"expected stack_mean_per_class_error_test to be the same as perf_stack_validation_frame.mean_per_class_error() found it wasn'tperf_stack_validation_frame.mean_per_class_error() = \" + str(perf_stack_validation_frame.mean_per_class_error()) + 'stack_mean_per_class_error_test was ' + str(stack_mean_per_class_error_test)"
        ]
    }
]