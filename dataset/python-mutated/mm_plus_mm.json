[
    {
        "func_name": "mm_configs",
        "original": "@functools.lru_cache(None)\ndef mm_configs():\n    import triton\n    mm_triton_configs = [{'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 3, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 16, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 128}, 'num_stages': 1, 'num_warps': 8, 'cond': torch.version.hip is None}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 16}, 'num_stages': 1, 'num_warps': 2, 'cond': True}]\n    if torch.version.hip:\n        filtered_configs = [triton.Config(c['config'], num_stages=1, num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    else:\n        filtered_configs = [triton.Config(c['config'], num_stages=c['num_stages'], num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    return filtered_configs",
        "mutated": [
            "@functools.lru_cache(None)\ndef mm_configs():\n    if False:\n        i = 10\n    import triton\n    mm_triton_configs = [{'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 3, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 16, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 128}, 'num_stages': 1, 'num_warps': 8, 'cond': torch.version.hip is None}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 16}, 'num_stages': 1, 'num_warps': 2, 'cond': True}]\n    if torch.version.hip:\n        filtered_configs = [triton.Config(c['config'], num_stages=1, num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    else:\n        filtered_configs = [triton.Config(c['config'], num_stages=c['num_stages'], num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    return filtered_configs",
            "@functools.lru_cache(None)\ndef mm_configs():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    import triton\n    mm_triton_configs = [{'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 3, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 16, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 128}, 'num_stages': 1, 'num_warps': 8, 'cond': torch.version.hip is None}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 16}, 'num_stages': 1, 'num_warps': 2, 'cond': True}]\n    if torch.version.hip:\n        filtered_configs = [triton.Config(c['config'], num_stages=1, num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    else:\n        filtered_configs = [triton.Config(c['config'], num_stages=c['num_stages'], num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    return filtered_configs",
            "@functools.lru_cache(None)\ndef mm_configs():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    import triton\n    mm_triton_configs = [{'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 3, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 16, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 128}, 'num_stages': 1, 'num_warps': 8, 'cond': torch.version.hip is None}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 16}, 'num_stages': 1, 'num_warps': 2, 'cond': True}]\n    if torch.version.hip:\n        filtered_configs = [triton.Config(c['config'], num_stages=1, num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    else:\n        filtered_configs = [triton.Config(c['config'], num_stages=c['num_stages'], num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    return filtered_configs",
            "@functools.lru_cache(None)\ndef mm_configs():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    import triton\n    mm_triton_configs = [{'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 3, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 16, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 128}, 'num_stages': 1, 'num_warps': 8, 'cond': torch.version.hip is None}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 16}, 'num_stages': 1, 'num_warps': 2, 'cond': True}]\n    if torch.version.hip:\n        filtered_configs = [triton.Config(c['config'], num_stages=1, num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    else:\n        filtered_configs = [triton.Config(c['config'], num_stages=c['num_stages'], num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    return filtered_configs",
            "@functools.lru_cache(None)\ndef mm_configs():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    import triton\n    mm_triton_configs = [{'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 3, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 16, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32}, 'num_stages': 4, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}, 'num_stages': 1, 'num_warps': 8, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 128}, 'num_stages': 1, 'num_warps': 8, 'cond': torch.version.hip is None}, {'config': {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16}, 'num_stages': 2, 'num_warps': 4, 'cond': True}, {'config': {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 16}, 'num_stages': 1, 'num_warps': 2, 'cond': True}]\n    if torch.version.hip:\n        filtered_configs = [triton.Config(c['config'], num_stages=1, num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    else:\n        filtered_configs = [triton.Config(c['config'], num_stages=c['num_stages'], num_warps=c['num_warps']) for c in mm_triton_configs if c['cond']]\n    return filtered_configs"
        ]
    },
    {
        "func_name": "tuned_mm_plus_mm",
        "original": "def tuned_mm_plus_mm(mat1, mat2, mat3, mat4, *, layout=None):\n    \"\"\"\n    Computes mm(mat1, mat2) + mm(mat3, mat4)\n    \"\"\"\n    (m1, n1, k1, layout1, mat1, mat2) = mm_args(mat1, mat2, layout=layout)\n    (m2, n2, _, layout2, mat3, mat4) = mm_args(mat3, mat4, layout=layout)\n    if m1 * n1 == 0 or m2 * n2 == 0 or (not V.graph.sizevars.statically_known_list_equals(mat1.get_size(), mat3.get_size())) or (not V.graph.sizevars.statically_known_list_equals(mat2.get_size(), mat4.get_size())):\n        if m1 == m2 and n1 == n2:\n            V.graph.sizevars.guard_equals(m1, m2)\n            V.graph.sizevars.guard_equals(n1, n2)\n            return lowerings[aten.addmm](lowerings[aten.mm](mat3, mat4), mat1, mat2)\n        return lowerings[aten.add](lowerings[aten.mm](mat1, mat2), lowerings[aten.mm](mat3, mat4))\n    assert layout1 == layout2\n    choices = [aten_mm_plus_mm.bind((mat1, mat2, mat3, mat4), layout1)] if use_aten_gemm_kernels() else []\n    if use_triton_template(layout1):\n        for config in mm_configs():\n            if config.kwargs['BLOCK_K'] < k1:\n                mm_plus_mm_template.maybe_append_choice(choices, input_nodes=(mat1, mat2, mat3, mat4), layout=layout1, **mm_options(config, k1, layout1))\n    return autotune_select_algorithm('mm_plus_mm', choices, [mat1, mat2, mat3, mat4], layout1)",
        "mutated": [
            "def tuned_mm_plus_mm(mat1, mat2, mat3, mat4, *, layout=None):\n    if False:\n        i = 10\n    '\\n    Computes mm(mat1, mat2) + mm(mat3, mat4)\\n    '\n    (m1, n1, k1, layout1, mat1, mat2) = mm_args(mat1, mat2, layout=layout)\n    (m2, n2, _, layout2, mat3, mat4) = mm_args(mat3, mat4, layout=layout)\n    if m1 * n1 == 0 or m2 * n2 == 0 or (not V.graph.sizevars.statically_known_list_equals(mat1.get_size(), mat3.get_size())) or (not V.graph.sizevars.statically_known_list_equals(mat2.get_size(), mat4.get_size())):\n        if m1 == m2 and n1 == n2:\n            V.graph.sizevars.guard_equals(m1, m2)\n            V.graph.sizevars.guard_equals(n1, n2)\n            return lowerings[aten.addmm](lowerings[aten.mm](mat3, mat4), mat1, mat2)\n        return lowerings[aten.add](lowerings[aten.mm](mat1, mat2), lowerings[aten.mm](mat3, mat4))\n    assert layout1 == layout2\n    choices = [aten_mm_plus_mm.bind((mat1, mat2, mat3, mat4), layout1)] if use_aten_gemm_kernels() else []\n    if use_triton_template(layout1):\n        for config in mm_configs():\n            if config.kwargs['BLOCK_K'] < k1:\n                mm_plus_mm_template.maybe_append_choice(choices, input_nodes=(mat1, mat2, mat3, mat4), layout=layout1, **mm_options(config, k1, layout1))\n    return autotune_select_algorithm('mm_plus_mm', choices, [mat1, mat2, mat3, mat4], layout1)",
            "def tuned_mm_plus_mm(mat1, mat2, mat3, mat4, *, layout=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Computes mm(mat1, mat2) + mm(mat3, mat4)\\n    '\n    (m1, n1, k1, layout1, mat1, mat2) = mm_args(mat1, mat2, layout=layout)\n    (m2, n2, _, layout2, mat3, mat4) = mm_args(mat3, mat4, layout=layout)\n    if m1 * n1 == 0 or m2 * n2 == 0 or (not V.graph.sizevars.statically_known_list_equals(mat1.get_size(), mat3.get_size())) or (not V.graph.sizevars.statically_known_list_equals(mat2.get_size(), mat4.get_size())):\n        if m1 == m2 and n1 == n2:\n            V.graph.sizevars.guard_equals(m1, m2)\n            V.graph.sizevars.guard_equals(n1, n2)\n            return lowerings[aten.addmm](lowerings[aten.mm](mat3, mat4), mat1, mat2)\n        return lowerings[aten.add](lowerings[aten.mm](mat1, mat2), lowerings[aten.mm](mat3, mat4))\n    assert layout1 == layout2\n    choices = [aten_mm_plus_mm.bind((mat1, mat2, mat3, mat4), layout1)] if use_aten_gemm_kernels() else []\n    if use_triton_template(layout1):\n        for config in mm_configs():\n            if config.kwargs['BLOCK_K'] < k1:\n                mm_plus_mm_template.maybe_append_choice(choices, input_nodes=(mat1, mat2, mat3, mat4), layout=layout1, **mm_options(config, k1, layout1))\n    return autotune_select_algorithm('mm_plus_mm', choices, [mat1, mat2, mat3, mat4], layout1)",
            "def tuned_mm_plus_mm(mat1, mat2, mat3, mat4, *, layout=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Computes mm(mat1, mat2) + mm(mat3, mat4)\\n    '\n    (m1, n1, k1, layout1, mat1, mat2) = mm_args(mat1, mat2, layout=layout)\n    (m2, n2, _, layout2, mat3, mat4) = mm_args(mat3, mat4, layout=layout)\n    if m1 * n1 == 0 or m2 * n2 == 0 or (not V.graph.sizevars.statically_known_list_equals(mat1.get_size(), mat3.get_size())) or (not V.graph.sizevars.statically_known_list_equals(mat2.get_size(), mat4.get_size())):\n        if m1 == m2 and n1 == n2:\n            V.graph.sizevars.guard_equals(m1, m2)\n            V.graph.sizevars.guard_equals(n1, n2)\n            return lowerings[aten.addmm](lowerings[aten.mm](mat3, mat4), mat1, mat2)\n        return lowerings[aten.add](lowerings[aten.mm](mat1, mat2), lowerings[aten.mm](mat3, mat4))\n    assert layout1 == layout2\n    choices = [aten_mm_plus_mm.bind((mat1, mat2, mat3, mat4), layout1)] if use_aten_gemm_kernels() else []\n    if use_triton_template(layout1):\n        for config in mm_configs():\n            if config.kwargs['BLOCK_K'] < k1:\n                mm_plus_mm_template.maybe_append_choice(choices, input_nodes=(mat1, mat2, mat3, mat4), layout=layout1, **mm_options(config, k1, layout1))\n    return autotune_select_algorithm('mm_plus_mm', choices, [mat1, mat2, mat3, mat4], layout1)",
            "def tuned_mm_plus_mm(mat1, mat2, mat3, mat4, *, layout=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Computes mm(mat1, mat2) + mm(mat3, mat4)\\n    '\n    (m1, n1, k1, layout1, mat1, mat2) = mm_args(mat1, mat2, layout=layout)\n    (m2, n2, _, layout2, mat3, mat4) = mm_args(mat3, mat4, layout=layout)\n    if m1 * n1 == 0 or m2 * n2 == 0 or (not V.graph.sizevars.statically_known_list_equals(mat1.get_size(), mat3.get_size())) or (not V.graph.sizevars.statically_known_list_equals(mat2.get_size(), mat4.get_size())):\n        if m1 == m2 and n1 == n2:\n            V.graph.sizevars.guard_equals(m1, m2)\n            V.graph.sizevars.guard_equals(n1, n2)\n            return lowerings[aten.addmm](lowerings[aten.mm](mat3, mat4), mat1, mat2)\n        return lowerings[aten.add](lowerings[aten.mm](mat1, mat2), lowerings[aten.mm](mat3, mat4))\n    assert layout1 == layout2\n    choices = [aten_mm_plus_mm.bind((mat1, mat2, mat3, mat4), layout1)] if use_aten_gemm_kernels() else []\n    if use_triton_template(layout1):\n        for config in mm_configs():\n            if config.kwargs['BLOCK_K'] < k1:\n                mm_plus_mm_template.maybe_append_choice(choices, input_nodes=(mat1, mat2, mat3, mat4), layout=layout1, **mm_options(config, k1, layout1))\n    return autotune_select_algorithm('mm_plus_mm', choices, [mat1, mat2, mat3, mat4], layout1)",
            "def tuned_mm_plus_mm(mat1, mat2, mat3, mat4, *, layout=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Computes mm(mat1, mat2) + mm(mat3, mat4)\\n    '\n    (m1, n1, k1, layout1, mat1, mat2) = mm_args(mat1, mat2, layout=layout)\n    (m2, n2, _, layout2, mat3, mat4) = mm_args(mat3, mat4, layout=layout)\n    if m1 * n1 == 0 or m2 * n2 == 0 or (not V.graph.sizevars.statically_known_list_equals(mat1.get_size(), mat3.get_size())) or (not V.graph.sizevars.statically_known_list_equals(mat2.get_size(), mat4.get_size())):\n        if m1 == m2 and n1 == n2:\n            V.graph.sizevars.guard_equals(m1, m2)\n            V.graph.sizevars.guard_equals(n1, n2)\n            return lowerings[aten.addmm](lowerings[aten.mm](mat3, mat4), mat1, mat2)\n        return lowerings[aten.add](lowerings[aten.mm](mat1, mat2), lowerings[aten.mm](mat3, mat4))\n    assert layout1 == layout2\n    choices = [aten_mm_plus_mm.bind((mat1, mat2, mat3, mat4), layout1)] if use_aten_gemm_kernels() else []\n    if use_triton_template(layout1):\n        for config in mm_configs():\n            if config.kwargs['BLOCK_K'] < k1:\n                mm_plus_mm_template.maybe_append_choice(choices, input_nodes=(mat1, mat2, mat3, mat4), layout=layout1, **mm_options(config, k1, layout1))\n    return autotune_select_algorithm('mm_plus_mm', choices, [mat1, mat2, mat3, mat4], layout1)"
        ]
    }
]