[
    {
        "func_name": "paired_ttest_resampled",
        "original": "def paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None):\n    \"\"\"\n    Implements the resampled paired t test procedure\n    to compare the performance of two models\n    (also called k-hold-out paired t test).\n\n    Parameters\n    ----------\n    estimator1 : scikit-learn classifier or regressor\n\n    estimator2 : scikit-learn classifier or regressor\n\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n        Training vectors, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape = [n_samples]\n        Target values.\n\n    num_rounds : int (default: 30)\n        Number of resampling iterations\n        (i.e., train/test splits)\n\n    test_size : float or int (default: 0.3)\n        If float, should be between 0.0 and 1.0 and\n        represent the proportion of the dataset to use\n        as a test set.\n        If int, represents the absolute number of test exsamples.\n\n    scoring : str, callable, or None (default: None)\n        If None (default), uses 'accuracy' for sklearn classifiers\n        and 'r2' for sklearn regressors.\n        If str, uses a sklearn scoring metric string identifier, for example\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\n        'median_absolute_error', 'r2'} for regressors.\n        If a callable object or function is provided, it has to be conform with\n        sklearn's signature ``scorer(estimator, X, y)``; see\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n        for more information.\n\n    random_seed : int or None (default: None)\n        Random seed for creating the test/train splits.\n\n    Returns\n    ----------\n    t : float\n        The t-statistic\n\n    pvalue : float\n        Two-tailed p-value.\n        If the chosen significance level is larger\n        than the p-value, we reject the null hypothesis\n        and accept that there are significant differences\n        in the two compared models.\n\n    Examples\n    -----------\n    For usage examples, please see\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/\n\n    \"\"\"\n    if not isinstance(test_size, int) and (not isinstance(test_size, float)):\n        raise ValueError('train_size must be of type int or float. Got %s.' % type(test_size))\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for i in range(num_rounds):\n        randint = rng.randint(low=0, high=32767)\n        (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=test_size, random_state=randint)\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(num_rounds)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (num_rounds - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), num_rounds - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
        "mutated": [
            "def paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None):\n    if False:\n        i = 10\n    \"\\n    Implements the resampled paired t test procedure\\n    to compare the performance of two models\\n    (also called k-hold-out paired t test).\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    num_rounds : int (default: 30)\\n        Number of resampling iterations\\n        (i.e., train/test splits)\\n\\n    test_size : float or int (default: 0.3)\\n        If float, should be between 0.0 and 1.0 and\\n        represent the proportion of the dataset to use\\n        as a test set.\\n        If int, represents the absolute number of test exsamples.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/\\n\\n    \"\n    if not isinstance(test_size, int) and (not isinstance(test_size, float)):\n        raise ValueError('train_size must be of type int or float. Got %s.' % type(test_size))\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for i in range(num_rounds):\n        randint = rng.randint(low=0, high=32767)\n        (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=test_size, random_state=randint)\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(num_rounds)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (num_rounds - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), num_rounds - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Implements the resampled paired t test procedure\\n    to compare the performance of two models\\n    (also called k-hold-out paired t test).\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    num_rounds : int (default: 30)\\n        Number of resampling iterations\\n        (i.e., train/test splits)\\n\\n    test_size : float or int (default: 0.3)\\n        If float, should be between 0.0 and 1.0 and\\n        represent the proportion of the dataset to use\\n        as a test set.\\n        If int, represents the absolute number of test exsamples.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/\\n\\n    \"\n    if not isinstance(test_size, int) and (not isinstance(test_size, float)):\n        raise ValueError('train_size must be of type int or float. Got %s.' % type(test_size))\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for i in range(num_rounds):\n        randint = rng.randint(low=0, high=32767)\n        (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=test_size, random_state=randint)\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(num_rounds)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (num_rounds - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), num_rounds - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Implements the resampled paired t test procedure\\n    to compare the performance of two models\\n    (also called k-hold-out paired t test).\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    num_rounds : int (default: 30)\\n        Number of resampling iterations\\n        (i.e., train/test splits)\\n\\n    test_size : float or int (default: 0.3)\\n        If float, should be between 0.0 and 1.0 and\\n        represent the proportion of the dataset to use\\n        as a test set.\\n        If int, represents the absolute number of test exsamples.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/\\n\\n    \"\n    if not isinstance(test_size, int) and (not isinstance(test_size, float)):\n        raise ValueError('train_size must be of type int or float. Got %s.' % type(test_size))\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for i in range(num_rounds):\n        randint = rng.randint(low=0, high=32767)\n        (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=test_size, random_state=randint)\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(num_rounds)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (num_rounds - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), num_rounds - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Implements the resampled paired t test procedure\\n    to compare the performance of two models\\n    (also called k-hold-out paired t test).\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    num_rounds : int (default: 30)\\n        Number of resampling iterations\\n        (i.e., train/test splits)\\n\\n    test_size : float or int (default: 0.3)\\n        If float, should be between 0.0 and 1.0 and\\n        represent the proportion of the dataset to use\\n        as a test set.\\n        If int, represents the absolute number of test exsamples.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/\\n\\n    \"\n    if not isinstance(test_size, int) and (not isinstance(test_size, float)):\n        raise ValueError('train_size must be of type int or float. Got %s.' % type(test_size))\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for i in range(num_rounds):\n        randint = rng.randint(low=0, high=32767)\n        (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=test_size, random_state=randint)\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(num_rounds)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (num_rounds - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), num_rounds - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Implements the resampled paired t test procedure\\n    to compare the performance of two models\\n    (also called k-hold-out paired t test).\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    num_rounds : int (default: 30)\\n        Number of resampling iterations\\n        (i.e., train/test splits)\\n\\n    test_size : float or int (default: 0.3)\\n        If float, should be between 0.0 and 1.0 and\\n        represent the proportion of the dataset to use\\n        as a test set.\\n        If int, represents the absolute number of test exsamples.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/\\n\\n    \"\n    if not isinstance(test_size, int) and (not isinstance(test_size, float)):\n        raise ValueError('train_size must be of type int or float. Got %s.' % type(test_size))\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for i in range(num_rounds):\n        randint = rng.randint(low=0, high=32767)\n        (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=test_size, random_state=randint)\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(num_rounds)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (num_rounds - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), num_rounds - 1) * 2.0\n    return (float(t_stat), float(pvalue))"
        ]
    },
    {
        "func_name": "paired_ttest_kfold_cv",
        "original": "def paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None):\n    \"\"\"\n    Implements the k-fold paired t test procedure\n    to compare the performance of two models.\n\n    Parameters\n    ----------\n    estimator1 : scikit-learn classifier or regressor\n\n    estimator2 : scikit-learn classifier or regressor\n\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n        Training vectors, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape = [n_samples]\n        Target values.\n\n    cv : int (default: 10)\n        Number of splits and iteration for the\n        cross-validation procedure\n\n    scoring : str, callable, or None (default: None)\n        If None (default), uses 'accuracy' for sklearn classifiers\n        and 'r2' for sklearn regressors.\n        If str, uses a sklearn scoring metric string identifier, for example\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\n        'median_absolute_error', 'r2'} for regressors.\n        If a callable object or function is provided, it has to be conform with\n        sklearn's signature ``scorer(estimator, X, y)``; see\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n        for more information.\n\n    shuffle : bool (default: True)\n        Whether to shuffle the dataset for generating\n        the k-fold splits.\n\n    random_seed : int or None (default: None)\n        Random seed for shuffling the dataset\n        for generating the k-fold splits.\n        Ignored if shuffle=False.\n\n    Returns\n    ----------\n    t : float\n        The t-statistic\n\n    pvalue : float\n        Two-tailed p-value.\n        If the chosen significance level is larger\n        than the p-value, we reject the null hypothesis\n        and accept that there are significant differences\n        in the two compared models.\n\n    Examples\n    -----------\n    For usage examples, please see\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/\n\n    \"\"\"\n    if not shuffle:\n        kf = KFold(n_splits=cv, shuffle=shuffle)\n    else:\n        kf = KFold(n_splits=cv, random_state=random_seed, shuffle=shuffle)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for (train_index, test_index) in kf.split(X):\n        (X_train, X_test) = (X[train_index], X[test_index])\n        (y_train, y_test) = (y[train_index], y[test_index])\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(cv)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (cv - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), cv - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
        "mutated": [
            "def paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None):\n    if False:\n        i = 10\n    \"\\n    Implements the k-fold paired t test procedure\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    cv : int (default: 10)\\n        Number of splits and iteration for the\\n        cross-validation procedure\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    shuffle : bool (default: True)\\n        Whether to shuffle the dataset for generating\\n        the k-fold splits.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for shuffling the dataset\\n        for generating the k-fold splits.\\n        Ignored if shuffle=False.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/\\n\\n    \"\n    if not shuffle:\n        kf = KFold(n_splits=cv, shuffle=shuffle)\n    else:\n        kf = KFold(n_splits=cv, random_state=random_seed, shuffle=shuffle)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for (train_index, test_index) in kf.split(X):\n        (X_train, X_test) = (X[train_index], X[test_index])\n        (y_train, y_test) = (y[train_index], y[test_index])\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(cv)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (cv - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), cv - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Implements the k-fold paired t test procedure\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    cv : int (default: 10)\\n        Number of splits and iteration for the\\n        cross-validation procedure\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    shuffle : bool (default: True)\\n        Whether to shuffle the dataset for generating\\n        the k-fold splits.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for shuffling the dataset\\n        for generating the k-fold splits.\\n        Ignored if shuffle=False.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/\\n\\n    \"\n    if not shuffle:\n        kf = KFold(n_splits=cv, shuffle=shuffle)\n    else:\n        kf = KFold(n_splits=cv, random_state=random_seed, shuffle=shuffle)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for (train_index, test_index) in kf.split(X):\n        (X_train, X_test) = (X[train_index], X[test_index])\n        (y_train, y_test) = (y[train_index], y[test_index])\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(cv)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (cv - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), cv - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Implements the k-fold paired t test procedure\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    cv : int (default: 10)\\n        Number of splits and iteration for the\\n        cross-validation procedure\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    shuffle : bool (default: True)\\n        Whether to shuffle the dataset for generating\\n        the k-fold splits.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for shuffling the dataset\\n        for generating the k-fold splits.\\n        Ignored if shuffle=False.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/\\n\\n    \"\n    if not shuffle:\n        kf = KFold(n_splits=cv, shuffle=shuffle)\n    else:\n        kf = KFold(n_splits=cv, random_state=random_seed, shuffle=shuffle)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for (train_index, test_index) in kf.split(X):\n        (X_train, X_test) = (X[train_index], X[test_index])\n        (y_train, y_test) = (y[train_index], y[test_index])\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(cv)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (cv - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), cv - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Implements the k-fold paired t test procedure\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    cv : int (default: 10)\\n        Number of splits and iteration for the\\n        cross-validation procedure\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    shuffle : bool (default: True)\\n        Whether to shuffle the dataset for generating\\n        the k-fold splits.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for shuffling the dataset\\n        for generating the k-fold splits.\\n        Ignored if shuffle=False.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/\\n\\n    \"\n    if not shuffle:\n        kf = KFold(n_splits=cv, shuffle=shuffle)\n    else:\n        kf = KFold(n_splits=cv, random_state=random_seed, shuffle=shuffle)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for (train_index, test_index) in kf.split(X):\n        (X_train, X_test) = (X[train_index], X[test_index])\n        (y_train, y_test) = (y[train_index], y[test_index])\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(cv)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (cv - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), cv - 1) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Implements the k-fold paired t test procedure\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    cv : int (default: 10)\\n        Number of splits and iteration for the\\n        cross-validation procedure\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    shuffle : bool (default: True)\\n        Whether to shuffle the dataset for generating\\n        the k-fold splits.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for shuffling the dataset\\n        for generating the k-fold splits.\\n        Ignored if shuffle=False.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/\\n\\n    \"\n    if not shuffle:\n        kf = KFold(n_splits=cv, shuffle=shuffle)\n    else:\n        kf = KFold(n_splits=cv, random_state=random_seed, shuffle=shuffle)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    score_diff = []\n    for (train_index, test_index) in kf.split(X):\n        (X_train, X_test) = (X[train_index], X[test_index])\n        (y_train, y_test) = (y[train_index], y[test_index])\n        estimator1.fit(X_train, y_train)\n        estimator2.fit(X_train, y_train)\n        est1_score = scorer(estimator1, X_test, y_test)\n        est2_score = scorer(estimator2, X_test, y_test)\n        score_diff.append(est1_score - est2_score)\n    avg_diff = np.mean(score_diff)\n    numerator = avg_diff * np.sqrt(cv)\n    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (cv - 1))\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), cv - 1) * 2.0\n    return (float(t_stat), float(pvalue))"
        ]
    },
    {
        "func_name": "score_diff",
        "original": "def score_diff(X_1, X_2, y_1, y_2):\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
        "mutated": [
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff"
        ]
    },
    {
        "func_name": "paired_ttest_5x2cv",
        "original": "def paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    \"\"\"\n    Implements the 5x2cv paired t test proposed\n    by Dieterrich (1998)\n    to compare the performance of two models.\n\n    Parameters\n    ----------\n    estimator1 : scikit-learn classifier or regressor\n\n    estimator2 : scikit-learn classifier or regressor\n\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n        Training vectors, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape = [n_samples]\n        Target values.\n\n    scoring : str, callable, or None (default: None)\n        If None (default), uses 'accuracy' for sklearn classifiers\n        and 'r2' for sklearn regressors.\n        If str, uses a sklearn scoring metric string identifier, for example\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\n        'median_absolute_error', 'r2'} for regressors.\n        If a callable object or function is provided, it has to be conform with\n        sklearn's signature ``scorer(estimator, X, y)``; see\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n        for more information.\n\n    random_seed : int or None (default: None)\n        Random seed for creating the test/train splits.\n\n    Returns\n    ----------\n    t : float\n        The t-statistic\n\n    pvalue : float\n        Two-tailed p-value.\n        If the chosen significance level is larger\n        than the p-value, we reject the null hypothesis\n        and accept that there are significant differences\n        in the two compared models.\n\n    Examples\n    -----------\n    For usage examples, please see\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/\n\n    \"\"\"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variance_sum = 0.0\n    first_diff = None\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        variance_sum += score_var\n        if first_diff is None:\n            first_diff = score_diff_1\n    numerator = first_diff\n    denominator = np.sqrt(1 / 5.0 * variance_sum)\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), 5) * 2.0\n    return (float(t_stat), float(pvalue))",
        "mutated": [
            "def paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n    \"\\n    Implements the 5x2cv paired t test proposed\\n    by Dieterrich (1998)\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variance_sum = 0.0\n    first_diff = None\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        variance_sum += score_var\n        if first_diff is None:\n            first_diff = score_diff_1\n    numerator = first_diff\n    denominator = np.sqrt(1 / 5.0 * variance_sum)\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), 5) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Implements the 5x2cv paired t test proposed\\n    by Dieterrich (1998)\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variance_sum = 0.0\n    first_diff = None\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        variance_sum += score_var\n        if first_diff is None:\n            first_diff = score_diff_1\n    numerator = first_diff\n    denominator = np.sqrt(1 / 5.0 * variance_sum)\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), 5) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Implements the 5x2cv paired t test proposed\\n    by Dieterrich (1998)\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variance_sum = 0.0\n    first_diff = None\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        variance_sum += score_var\n        if first_diff is None:\n            first_diff = score_diff_1\n    numerator = first_diff\n    denominator = np.sqrt(1 / 5.0 * variance_sum)\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), 5) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Implements the 5x2cv paired t test proposed\\n    by Dieterrich (1998)\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variance_sum = 0.0\n    first_diff = None\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        variance_sum += score_var\n        if first_diff is None:\n            first_diff = score_diff_1\n    numerator = first_diff\n    denominator = np.sqrt(1 / 5.0 * variance_sum)\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), 5) * 2.0\n    return (float(t_stat), float(pvalue))",
            "def paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Implements the 5x2cv paired t test proposed\\n    by Dieterrich (1998)\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    t : float\\n        The t-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variance_sum = 0.0\n    first_diff = None\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        variance_sum += score_var\n        if first_diff is None:\n            first_diff = score_diff_1\n    numerator = first_diff\n    denominator = np.sqrt(1 / 5.0 * variance_sum)\n    t_stat = numerator / denominator\n    pvalue = stats.t.sf(np.abs(t_stat), 5) * 2.0\n    return (float(t_stat), float(pvalue))"
        ]
    }
]