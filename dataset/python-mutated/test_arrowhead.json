[
    {
        "func_name": "test_utilities",
        "original": "@pytest.mark.parametrize('head_size', [0, 2, 5])\ndef test_utilities(head_size):\n    size = 5\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n    mask = torch.ones(size, size)\n    mask[head_size:, head_size:] = 0.0\n    mask.view(-1)[::size + 1][head_size:] = 1.0\n    arrowhead_full = mask * cov\n    expected = torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1))\n    assert_close(expected.triu(), expected)\n    assert_close(expected.matmul(expected.t()), arrowhead_full)\n    arrowhead = SymmArrowhead(cov[:head_size], cov.diag()[head_size:])\n    actual = sqrt(arrowhead)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    expected = expected.inverse()\n    actual = triu_inverse(actual)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    v = torch.randn(size)\n    assert_close(triu_matvecmul(actual, v), expected.matmul(v))\n    assert_close(triu_matvecmul(actual, v, transpose=True), expected.t().matmul(v))\n    actual = triu_gram(actual)\n    expected = arrowhead_full.inverse() if head_size > 0 else arrowhead_full.diag().reciprocal()\n    assert_close(actual, expected)",
        "mutated": [
            "@pytest.mark.parametrize('head_size', [0, 2, 5])\ndef test_utilities(head_size):\n    if False:\n        i = 10\n    size = 5\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n    mask = torch.ones(size, size)\n    mask[head_size:, head_size:] = 0.0\n    mask.view(-1)[::size + 1][head_size:] = 1.0\n    arrowhead_full = mask * cov\n    expected = torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1))\n    assert_close(expected.triu(), expected)\n    assert_close(expected.matmul(expected.t()), arrowhead_full)\n    arrowhead = SymmArrowhead(cov[:head_size], cov.diag()[head_size:])\n    actual = sqrt(arrowhead)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    expected = expected.inverse()\n    actual = triu_inverse(actual)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    v = torch.randn(size)\n    assert_close(triu_matvecmul(actual, v), expected.matmul(v))\n    assert_close(triu_matvecmul(actual, v, transpose=True), expected.t().matmul(v))\n    actual = triu_gram(actual)\n    expected = arrowhead_full.inverse() if head_size > 0 else arrowhead_full.diag().reciprocal()\n    assert_close(actual, expected)",
            "@pytest.mark.parametrize('head_size', [0, 2, 5])\ndef test_utilities(head_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    size = 5\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n    mask = torch.ones(size, size)\n    mask[head_size:, head_size:] = 0.0\n    mask.view(-1)[::size + 1][head_size:] = 1.0\n    arrowhead_full = mask * cov\n    expected = torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1))\n    assert_close(expected.triu(), expected)\n    assert_close(expected.matmul(expected.t()), arrowhead_full)\n    arrowhead = SymmArrowhead(cov[:head_size], cov.diag()[head_size:])\n    actual = sqrt(arrowhead)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    expected = expected.inverse()\n    actual = triu_inverse(actual)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    v = torch.randn(size)\n    assert_close(triu_matvecmul(actual, v), expected.matmul(v))\n    assert_close(triu_matvecmul(actual, v, transpose=True), expected.t().matmul(v))\n    actual = triu_gram(actual)\n    expected = arrowhead_full.inverse() if head_size > 0 else arrowhead_full.diag().reciprocal()\n    assert_close(actual, expected)",
            "@pytest.mark.parametrize('head_size', [0, 2, 5])\ndef test_utilities(head_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    size = 5\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n    mask = torch.ones(size, size)\n    mask[head_size:, head_size:] = 0.0\n    mask.view(-1)[::size + 1][head_size:] = 1.0\n    arrowhead_full = mask * cov\n    expected = torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1))\n    assert_close(expected.triu(), expected)\n    assert_close(expected.matmul(expected.t()), arrowhead_full)\n    arrowhead = SymmArrowhead(cov[:head_size], cov.diag()[head_size:])\n    actual = sqrt(arrowhead)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    expected = expected.inverse()\n    actual = triu_inverse(actual)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    v = torch.randn(size)\n    assert_close(triu_matvecmul(actual, v), expected.matmul(v))\n    assert_close(triu_matvecmul(actual, v, transpose=True), expected.t().matmul(v))\n    actual = triu_gram(actual)\n    expected = arrowhead_full.inverse() if head_size > 0 else arrowhead_full.diag().reciprocal()\n    assert_close(actual, expected)",
            "@pytest.mark.parametrize('head_size', [0, 2, 5])\ndef test_utilities(head_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    size = 5\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n    mask = torch.ones(size, size)\n    mask[head_size:, head_size:] = 0.0\n    mask.view(-1)[::size + 1][head_size:] = 1.0\n    arrowhead_full = mask * cov\n    expected = torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1))\n    assert_close(expected.triu(), expected)\n    assert_close(expected.matmul(expected.t()), arrowhead_full)\n    arrowhead = SymmArrowhead(cov[:head_size], cov.diag()[head_size:])\n    actual = sqrt(arrowhead)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    expected = expected.inverse()\n    actual = triu_inverse(actual)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    v = torch.randn(size)\n    assert_close(triu_matvecmul(actual, v), expected.matmul(v))\n    assert_close(triu_matvecmul(actual, v, transpose=True), expected.t().matmul(v))\n    actual = triu_gram(actual)\n    expected = arrowhead_full.inverse() if head_size > 0 else arrowhead_full.diag().reciprocal()\n    assert_close(actual, expected)",
            "@pytest.mark.parametrize('head_size', [0, 2, 5])\ndef test_utilities(head_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    size = 5\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n    mask = torch.ones(size, size)\n    mask[head_size:, head_size:] = 0.0\n    mask.view(-1)[::size + 1][head_size:] = 1.0\n    arrowhead_full = mask * cov\n    expected = torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1))\n    assert_close(expected.triu(), expected)\n    assert_close(expected.matmul(expected.t()), arrowhead_full)\n    arrowhead = SymmArrowhead(cov[:head_size], cov.diag()[head_size:])\n    actual = sqrt(arrowhead)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    expected = expected.inverse()\n    actual = triu_inverse(actual)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n    v = torch.randn(size)\n    assert_close(triu_matvecmul(actual, v), expected.matmul(v))\n    assert_close(triu_matvecmul(actual, v, transpose=True), expected.t().matmul(v))\n    actual = triu_gram(actual)\n    expected = arrowhead_full.inverse() if head_size > 0 else arrowhead_full.diag().reciprocal()\n    assert_close(actual, expected)"
        ]
    }
]