[
    {
        "func_name": "_transitions",
        "original": "def _transitions(state, policies):\n    \"\"\"Returns a list of (action, prob) pairs from the specified state.\"\"\"\n    if state.is_chance_node():\n        return state.chance_outcomes()\n    else:\n        pl = state.current_player()\n        return list(policies[pl].action_probabilities(state).items())",
        "mutated": [
            "def _transitions(state, policies):\n    if False:\n        i = 10\n    'Returns a list of (action, prob) pairs from the specified state.'\n    if state.is_chance_node():\n        return state.chance_outcomes()\n    else:\n        pl = state.current_player()\n        return list(policies[pl].action_probabilities(state).items())",
            "def _transitions(state, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a list of (action, prob) pairs from the specified state.'\n    if state.is_chance_node():\n        return state.chance_outcomes()\n    else:\n        pl = state.current_player()\n        return list(policies[pl].action_probabilities(state).items())",
            "def _transitions(state, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a list of (action, prob) pairs from the specified state.'\n    if state.is_chance_node():\n        return state.chance_outcomes()\n    else:\n        pl = state.current_player()\n        return list(policies[pl].action_probabilities(state).items())",
            "def _transitions(state, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a list of (action, prob) pairs from the specified state.'\n    if state.is_chance_node():\n        return state.chance_outcomes()\n    else:\n        pl = state.current_player()\n        return list(policies[pl].action_probabilities(state).items())",
            "def _transitions(state, policies):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a list of (action, prob) pairs from the specified state.'\n    if state.is_chance_node():\n        return state.chance_outcomes()\n    else:\n        pl = state.current_player()\n        return list(policies[pl].action_probabilities(state).items())"
        ]
    },
    {
        "func_name": "_tuples_from_policy",
        "original": "def _tuples_from_policy(policy_vector):\n    return [(action, probability) for (action, probability) in enumerate(policy_vector)]",
        "mutated": [
            "def _tuples_from_policy(policy_vector):\n    if False:\n        i = 10\n    return [(action, probability) for (action, probability) in enumerate(policy_vector)]",
            "def _tuples_from_policy(policy_vector):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return [(action, probability) for (action, probability) in enumerate(policy_vector)]",
            "def _tuples_from_policy(policy_vector):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return [(action, probability) for (action, probability) in enumerate(policy_vector)]",
            "def _tuples_from_policy(policy_vector):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return [(action, probability) for (action, probability) in enumerate(policy_vector)]",
            "def _tuples_from_policy(policy_vector):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return [(action, probability) for (action, probability) in enumerate(policy_vector)]"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, game):\n    if game.num_players() != 2:\n        raise ValueError('Only supports 2-player games.')\n    self.game = game\n    self._num_players = game.num_players()\n    self._num_actions = game.num_distinct_actions()\n    self._action_value_calculator = action_value.TreeWalkCalculator(game)\n    self._best_responder = {0: None, 1: None}\n    self._all_states = None",
        "mutated": [
            "def __init__(self, game):\n    if False:\n        i = 10\n    if game.num_players() != 2:\n        raise ValueError('Only supports 2-player games.')\n    self.game = game\n    self._num_players = game.num_players()\n    self._num_actions = game.num_distinct_actions()\n    self._action_value_calculator = action_value.TreeWalkCalculator(game)\n    self._best_responder = {0: None, 1: None}\n    self._all_states = None",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if game.num_players() != 2:\n        raise ValueError('Only supports 2-player games.')\n    self.game = game\n    self._num_players = game.num_players()\n    self._num_actions = game.num_distinct_actions()\n    self._action_value_calculator = action_value.TreeWalkCalculator(game)\n    self._best_responder = {0: None, 1: None}\n    self._all_states = None",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if game.num_players() != 2:\n        raise ValueError('Only supports 2-player games.')\n    self.game = game\n    self._num_players = game.num_players()\n    self._num_actions = game.num_distinct_actions()\n    self._action_value_calculator = action_value.TreeWalkCalculator(game)\n    self._best_responder = {0: None, 1: None}\n    self._all_states = None",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if game.num_players() != 2:\n        raise ValueError('Only supports 2-player games.')\n    self.game = game\n    self._num_players = game.num_players()\n    self._num_actions = game.num_distinct_actions()\n    self._action_value_calculator = action_value.TreeWalkCalculator(game)\n    self._best_responder = {0: None, 1: None}\n    self._all_states = None",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if game.num_players() != 2:\n        raise ValueError('Only supports 2-player games.')\n    self.game = game\n    self._num_players = game.num_players()\n    self._num_actions = game.num_distinct_actions()\n    self._action_value_calculator = action_value.TreeWalkCalculator(game)\n    self._best_responder = {0: None, 1: None}\n    self._all_states = None"
        ]
    },
    {
        "func_name": "best_response_policy",
        "original": "def best_response_policy(state):\n    infostate = state.information_state_string(opponent)\n    action = best_response_actions[infostate]\n    return [(action, 1.0)]",
        "mutated": [
            "def best_response_policy(state):\n    if False:\n        i = 10\n    infostate = state.information_state_string(opponent)\n    action = best_response_actions[infostate]\n    return [(action, 1.0)]",
            "def best_response_policy(state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    infostate = state.information_state_string(opponent)\n    action = best_response_actions[infostate]\n    return [(action, 1.0)]",
            "def best_response_policy(state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    infostate = state.information_state_string(opponent)\n    action = best_response_actions[infostate]\n    return [(action, 1.0)]",
            "def best_response_policy(state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    infostate = state.information_state_string(opponent)\n    action = best_response_actions[infostate]\n    return [(action, 1.0)]",
            "def best_response_policy(state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    infostate = state.information_state_string(opponent)\n    action = best_response_actions[infostate]\n    return [(action, 1.0)]"
        ]
    },
    {
        "func_name": "__call__",
        "original": "def __call__(self, player, player_policy, info_states):\n    \"\"\"Computes action values per state for the player.\n\n    Args:\n      player: The id of the player (0 <= player < game.num_players()). This\n        player will play `player_policy`, while the opponent will play a best\n        response.\n      player_policy: A `policy.Policy` object.\n      info_states: A list of info state strings.\n\n    Returns:\n      A `_CalculatorReturn` nametuple. See its docstring for the documentation.\n    \"\"\"\n    self.player = player\n    opponent = 1 - player\n\n    def best_response_policy(state):\n        infostate = state.information_state_string(opponent)\n        action = best_response_actions[infostate]\n        return [(action, 1.0)]\n    if isinstance(player_policy, policy.TabularPolicy):\n        tabular_policy = {key: _tuples_from_policy(player_policy.policy_for_key(key)) for key in player_policy.state_lookup}\n    else:\n        if self._all_states is None:\n            self._all_states = get_all_states.get_all_states(self.game, depth_limit=-1, include_terminals=False, include_chance_states=False)\n            self._state_to_information_state = {state: self._all_states[state].information_state_string() for state in self._all_states}\n        tabular_policy = policy_utils.policy_to_dict(player_policy, self.game, self._all_states, self._state_to_information_state)\n    if self._best_responder[player] is None:\n        self._best_responder[player] = pyspiel.TabularBestResponse(self.game, opponent, tabular_policy)\n    else:\n        self._best_responder[player].set_policy(tabular_policy)\n    best_response_value = self._best_responder[player].value_from_state(self.game.new_initial_state())\n    best_response_actions = self._best_responder[player].get_best_response_actions()\n    self._action_value_calculator.compute_all_states_action_values({player: player_policy, opponent: policy.tabular_policy_from_callable(self.game, best_response_policy, [opponent])})\n    obj = self._action_value_calculator._get_tabular_statistics(((player, s) for s in info_states))\n    return _CalculatorReturn(exploitability=best_response_value, values_vs_br=obj.action_values, counterfactual_reach_probs_vs_br=obj.counterfactual_reach_probs, player_reach_probs_vs_br=obj.player_reach_probs)",
        "mutated": [
            "def __call__(self, player, player_policy, info_states):\n    if False:\n        i = 10\n    'Computes action values per state for the player.\\n\\n    Args:\\n      player: The id of the player (0 <= player < game.num_players()). This\\n        player will play `player_policy`, while the opponent will play a best\\n        response.\\n      player_policy: A `policy.Policy` object.\\n      info_states: A list of info state strings.\\n\\n    Returns:\\n      A `_CalculatorReturn` nametuple. See its docstring for the documentation.\\n    '\n    self.player = player\n    opponent = 1 - player\n\n    def best_response_policy(state):\n        infostate = state.information_state_string(opponent)\n        action = best_response_actions[infostate]\n        return [(action, 1.0)]\n    if isinstance(player_policy, policy.TabularPolicy):\n        tabular_policy = {key: _tuples_from_policy(player_policy.policy_for_key(key)) for key in player_policy.state_lookup}\n    else:\n        if self._all_states is None:\n            self._all_states = get_all_states.get_all_states(self.game, depth_limit=-1, include_terminals=False, include_chance_states=False)\n            self._state_to_information_state = {state: self._all_states[state].information_state_string() for state in self._all_states}\n        tabular_policy = policy_utils.policy_to_dict(player_policy, self.game, self._all_states, self._state_to_information_state)\n    if self._best_responder[player] is None:\n        self._best_responder[player] = pyspiel.TabularBestResponse(self.game, opponent, tabular_policy)\n    else:\n        self._best_responder[player].set_policy(tabular_policy)\n    best_response_value = self._best_responder[player].value_from_state(self.game.new_initial_state())\n    best_response_actions = self._best_responder[player].get_best_response_actions()\n    self._action_value_calculator.compute_all_states_action_values({player: player_policy, opponent: policy.tabular_policy_from_callable(self.game, best_response_policy, [opponent])})\n    obj = self._action_value_calculator._get_tabular_statistics(((player, s) for s in info_states))\n    return _CalculatorReturn(exploitability=best_response_value, values_vs_br=obj.action_values, counterfactual_reach_probs_vs_br=obj.counterfactual_reach_probs, player_reach_probs_vs_br=obj.player_reach_probs)",
            "def __call__(self, player, player_policy, info_states):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes action values per state for the player.\\n\\n    Args:\\n      player: The id of the player (0 <= player < game.num_players()). This\\n        player will play `player_policy`, while the opponent will play a best\\n        response.\\n      player_policy: A `policy.Policy` object.\\n      info_states: A list of info state strings.\\n\\n    Returns:\\n      A `_CalculatorReturn` nametuple. See its docstring for the documentation.\\n    '\n    self.player = player\n    opponent = 1 - player\n\n    def best_response_policy(state):\n        infostate = state.information_state_string(opponent)\n        action = best_response_actions[infostate]\n        return [(action, 1.0)]\n    if isinstance(player_policy, policy.TabularPolicy):\n        tabular_policy = {key: _tuples_from_policy(player_policy.policy_for_key(key)) for key in player_policy.state_lookup}\n    else:\n        if self._all_states is None:\n            self._all_states = get_all_states.get_all_states(self.game, depth_limit=-1, include_terminals=False, include_chance_states=False)\n            self._state_to_information_state = {state: self._all_states[state].information_state_string() for state in self._all_states}\n        tabular_policy = policy_utils.policy_to_dict(player_policy, self.game, self._all_states, self._state_to_information_state)\n    if self._best_responder[player] is None:\n        self._best_responder[player] = pyspiel.TabularBestResponse(self.game, opponent, tabular_policy)\n    else:\n        self._best_responder[player].set_policy(tabular_policy)\n    best_response_value = self._best_responder[player].value_from_state(self.game.new_initial_state())\n    best_response_actions = self._best_responder[player].get_best_response_actions()\n    self._action_value_calculator.compute_all_states_action_values({player: player_policy, opponent: policy.tabular_policy_from_callable(self.game, best_response_policy, [opponent])})\n    obj = self._action_value_calculator._get_tabular_statistics(((player, s) for s in info_states))\n    return _CalculatorReturn(exploitability=best_response_value, values_vs_br=obj.action_values, counterfactual_reach_probs_vs_br=obj.counterfactual_reach_probs, player_reach_probs_vs_br=obj.player_reach_probs)",
            "def __call__(self, player, player_policy, info_states):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes action values per state for the player.\\n\\n    Args:\\n      player: The id of the player (0 <= player < game.num_players()). This\\n        player will play `player_policy`, while the opponent will play a best\\n        response.\\n      player_policy: A `policy.Policy` object.\\n      info_states: A list of info state strings.\\n\\n    Returns:\\n      A `_CalculatorReturn` nametuple. See its docstring for the documentation.\\n    '\n    self.player = player\n    opponent = 1 - player\n\n    def best_response_policy(state):\n        infostate = state.information_state_string(opponent)\n        action = best_response_actions[infostate]\n        return [(action, 1.0)]\n    if isinstance(player_policy, policy.TabularPolicy):\n        tabular_policy = {key: _tuples_from_policy(player_policy.policy_for_key(key)) for key in player_policy.state_lookup}\n    else:\n        if self._all_states is None:\n            self._all_states = get_all_states.get_all_states(self.game, depth_limit=-1, include_terminals=False, include_chance_states=False)\n            self._state_to_information_state = {state: self._all_states[state].information_state_string() for state in self._all_states}\n        tabular_policy = policy_utils.policy_to_dict(player_policy, self.game, self._all_states, self._state_to_information_state)\n    if self._best_responder[player] is None:\n        self._best_responder[player] = pyspiel.TabularBestResponse(self.game, opponent, tabular_policy)\n    else:\n        self._best_responder[player].set_policy(tabular_policy)\n    best_response_value = self._best_responder[player].value_from_state(self.game.new_initial_state())\n    best_response_actions = self._best_responder[player].get_best_response_actions()\n    self._action_value_calculator.compute_all_states_action_values({player: player_policy, opponent: policy.tabular_policy_from_callable(self.game, best_response_policy, [opponent])})\n    obj = self._action_value_calculator._get_tabular_statistics(((player, s) for s in info_states))\n    return _CalculatorReturn(exploitability=best_response_value, values_vs_br=obj.action_values, counterfactual_reach_probs_vs_br=obj.counterfactual_reach_probs, player_reach_probs_vs_br=obj.player_reach_probs)",
            "def __call__(self, player, player_policy, info_states):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes action values per state for the player.\\n\\n    Args:\\n      player: The id of the player (0 <= player < game.num_players()). This\\n        player will play `player_policy`, while the opponent will play a best\\n        response.\\n      player_policy: A `policy.Policy` object.\\n      info_states: A list of info state strings.\\n\\n    Returns:\\n      A `_CalculatorReturn` nametuple. See its docstring for the documentation.\\n    '\n    self.player = player\n    opponent = 1 - player\n\n    def best_response_policy(state):\n        infostate = state.information_state_string(opponent)\n        action = best_response_actions[infostate]\n        return [(action, 1.0)]\n    if isinstance(player_policy, policy.TabularPolicy):\n        tabular_policy = {key: _tuples_from_policy(player_policy.policy_for_key(key)) for key in player_policy.state_lookup}\n    else:\n        if self._all_states is None:\n            self._all_states = get_all_states.get_all_states(self.game, depth_limit=-1, include_terminals=False, include_chance_states=False)\n            self._state_to_information_state = {state: self._all_states[state].information_state_string() for state in self._all_states}\n        tabular_policy = policy_utils.policy_to_dict(player_policy, self.game, self._all_states, self._state_to_information_state)\n    if self._best_responder[player] is None:\n        self._best_responder[player] = pyspiel.TabularBestResponse(self.game, opponent, tabular_policy)\n    else:\n        self._best_responder[player].set_policy(tabular_policy)\n    best_response_value = self._best_responder[player].value_from_state(self.game.new_initial_state())\n    best_response_actions = self._best_responder[player].get_best_response_actions()\n    self._action_value_calculator.compute_all_states_action_values({player: player_policy, opponent: policy.tabular_policy_from_callable(self.game, best_response_policy, [opponent])})\n    obj = self._action_value_calculator._get_tabular_statistics(((player, s) for s in info_states))\n    return _CalculatorReturn(exploitability=best_response_value, values_vs_br=obj.action_values, counterfactual_reach_probs_vs_br=obj.counterfactual_reach_probs, player_reach_probs_vs_br=obj.player_reach_probs)",
            "def __call__(self, player, player_policy, info_states):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes action values per state for the player.\\n\\n    Args:\\n      player: The id of the player (0 <= player < game.num_players()). This\\n        player will play `player_policy`, while the opponent will play a best\\n        response.\\n      player_policy: A `policy.Policy` object.\\n      info_states: A list of info state strings.\\n\\n    Returns:\\n      A `_CalculatorReturn` nametuple. See its docstring for the documentation.\\n    '\n    self.player = player\n    opponent = 1 - player\n\n    def best_response_policy(state):\n        infostate = state.information_state_string(opponent)\n        action = best_response_actions[infostate]\n        return [(action, 1.0)]\n    if isinstance(player_policy, policy.TabularPolicy):\n        tabular_policy = {key: _tuples_from_policy(player_policy.policy_for_key(key)) for key in player_policy.state_lookup}\n    else:\n        if self._all_states is None:\n            self._all_states = get_all_states.get_all_states(self.game, depth_limit=-1, include_terminals=False, include_chance_states=False)\n            self._state_to_information_state = {state: self._all_states[state].information_state_string() for state in self._all_states}\n        tabular_policy = policy_utils.policy_to_dict(player_policy, self.game, self._all_states, self._state_to_information_state)\n    if self._best_responder[player] is None:\n        self._best_responder[player] = pyspiel.TabularBestResponse(self.game, opponent, tabular_policy)\n    else:\n        self._best_responder[player].set_policy(tabular_policy)\n    best_response_value = self._best_responder[player].value_from_state(self.game.new_initial_state())\n    best_response_actions = self._best_responder[player].get_best_response_actions()\n    self._action_value_calculator.compute_all_states_action_values({player: player_policy, opponent: policy.tabular_policy_from_callable(self.game, best_response_policy, [opponent])})\n    obj = self._action_value_calculator._get_tabular_statistics(((player, s) for s in info_states))\n    return _CalculatorReturn(exploitability=best_response_value, values_vs_br=obj.action_values, counterfactual_reach_probs_vs_br=obj.counterfactual_reach_probs, player_reach_probs_vs_br=obj.player_reach_probs)"
        ]
    }
]