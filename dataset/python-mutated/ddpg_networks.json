[
    {
        "func_name": "critic_net",
        "original": "@gin.configurable('ddpg_critic_net')\ndef critic_net(states, actions, for_critic_loss=False, num_reward_dims=1, states_hidden_layers=(400,), actions_hidden_layers=None, joint_hidden_layers=(300,), weight_decay=0.0001, normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    \"\"\"Creates a critic that returns q values for the given states and actions.\n\n  Args:\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\n      representing a batch of states.\n    actions: (castable to tf.float32) a [batch_size, num_action_dims] tensor\n      representing a batch of actions.\n    num_reward_dims: Number of reward dimensions.\n    states_hidden_layers: tuple of hidden layers units for states.\n    actions_hidden_layers: tuple of hidden layers units for actions.\n    joint_hidden_layers: tuple of hidden layers units after joining states\n      and actions using tf.concat().\n    weight_decay: Weight decay for l2 weights regularizer.\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\n  Returns:\n    A tf.float32 [batch_size] tensor of q values, or a tf.float32\n      [batch_size, num_reward_dims] tensor of vector q values if\n      num_reward_dims > 1.\n  \"\"\"\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_regularizer=slim.l2_regularizer(weight_decay), weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        orig_states = tf.to_float(states)\n        states = tf.concat([tf.to_float(states), tf.to_float(actions)], -1)\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        actions = tf.to_float(actions)\n        if states_hidden_layers:\n            states = slim.stack(states, slim.fully_connected, states_hidden_layers, scope='states')\n        if actions_hidden_layers:\n            actions = slim.stack(actions, slim.fully_connected, actions_hidden_layers, scope='actions')\n        joint = tf.concat([states, actions], 1)\n        if joint_hidden_layers:\n            joint = slim.stack(joint, slim.fully_connected, joint_hidden_layers, scope='joint')\n        with slim.arg_scope([slim.fully_connected], weights_regularizer=None, weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            value = slim.fully_connected(joint, num_reward_dims, activation_fn=None, normalizer_fn=None, scope='q_value')\n        if num_reward_dims == 1:\n            value = tf.reshape(value, [-1])\n        if not for_critic_loss and num_reward_dims > 1:\n            value = tf.reduce_sum(value * tf.abs(orig_states[:, -num_reward_dims:]), -1)\n    return value",
        "mutated": [
            "@gin.configurable('ddpg_critic_net')\ndef critic_net(states, actions, for_critic_loss=False, num_reward_dims=1, states_hidden_layers=(400,), actions_hidden_layers=None, joint_hidden_layers=(300,), weight_decay=0.0001, normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n    'Creates a critic that returns q values for the given states and actions.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    actions: (castable to tf.float32) a [batch_size, num_action_dims] tensor\\n      representing a batch of actions.\\n    num_reward_dims: Number of reward dimensions.\\n    states_hidden_layers: tuple of hidden layers units for states.\\n    actions_hidden_layers: tuple of hidden layers units for actions.\\n    joint_hidden_layers: tuple of hidden layers units after joining states\\n      and actions using tf.concat().\\n    weight_decay: Weight decay for l2 weights regularizer.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size] tensor of q values, or a tf.float32\\n      [batch_size, num_reward_dims] tensor of vector q values if\\n      num_reward_dims > 1.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_regularizer=slim.l2_regularizer(weight_decay), weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        orig_states = tf.to_float(states)\n        states = tf.concat([tf.to_float(states), tf.to_float(actions)], -1)\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        actions = tf.to_float(actions)\n        if states_hidden_layers:\n            states = slim.stack(states, slim.fully_connected, states_hidden_layers, scope='states')\n        if actions_hidden_layers:\n            actions = slim.stack(actions, slim.fully_connected, actions_hidden_layers, scope='actions')\n        joint = tf.concat([states, actions], 1)\n        if joint_hidden_layers:\n            joint = slim.stack(joint, slim.fully_connected, joint_hidden_layers, scope='joint')\n        with slim.arg_scope([slim.fully_connected], weights_regularizer=None, weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            value = slim.fully_connected(joint, num_reward_dims, activation_fn=None, normalizer_fn=None, scope='q_value')\n        if num_reward_dims == 1:\n            value = tf.reshape(value, [-1])\n        if not for_critic_loss and num_reward_dims > 1:\n            value = tf.reduce_sum(value * tf.abs(orig_states[:, -num_reward_dims:]), -1)\n    return value",
            "@gin.configurable('ddpg_critic_net')\ndef critic_net(states, actions, for_critic_loss=False, num_reward_dims=1, states_hidden_layers=(400,), actions_hidden_layers=None, joint_hidden_layers=(300,), weight_decay=0.0001, normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a critic that returns q values for the given states and actions.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    actions: (castable to tf.float32) a [batch_size, num_action_dims] tensor\\n      representing a batch of actions.\\n    num_reward_dims: Number of reward dimensions.\\n    states_hidden_layers: tuple of hidden layers units for states.\\n    actions_hidden_layers: tuple of hidden layers units for actions.\\n    joint_hidden_layers: tuple of hidden layers units after joining states\\n      and actions using tf.concat().\\n    weight_decay: Weight decay for l2 weights regularizer.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size] tensor of q values, or a tf.float32\\n      [batch_size, num_reward_dims] tensor of vector q values if\\n      num_reward_dims > 1.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_regularizer=slim.l2_regularizer(weight_decay), weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        orig_states = tf.to_float(states)\n        states = tf.concat([tf.to_float(states), tf.to_float(actions)], -1)\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        actions = tf.to_float(actions)\n        if states_hidden_layers:\n            states = slim.stack(states, slim.fully_connected, states_hidden_layers, scope='states')\n        if actions_hidden_layers:\n            actions = slim.stack(actions, slim.fully_connected, actions_hidden_layers, scope='actions')\n        joint = tf.concat([states, actions], 1)\n        if joint_hidden_layers:\n            joint = slim.stack(joint, slim.fully_connected, joint_hidden_layers, scope='joint')\n        with slim.arg_scope([slim.fully_connected], weights_regularizer=None, weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            value = slim.fully_connected(joint, num_reward_dims, activation_fn=None, normalizer_fn=None, scope='q_value')\n        if num_reward_dims == 1:\n            value = tf.reshape(value, [-1])\n        if not for_critic_loss and num_reward_dims > 1:\n            value = tf.reduce_sum(value * tf.abs(orig_states[:, -num_reward_dims:]), -1)\n    return value",
            "@gin.configurable('ddpg_critic_net')\ndef critic_net(states, actions, for_critic_loss=False, num_reward_dims=1, states_hidden_layers=(400,), actions_hidden_layers=None, joint_hidden_layers=(300,), weight_decay=0.0001, normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a critic that returns q values for the given states and actions.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    actions: (castable to tf.float32) a [batch_size, num_action_dims] tensor\\n      representing a batch of actions.\\n    num_reward_dims: Number of reward dimensions.\\n    states_hidden_layers: tuple of hidden layers units for states.\\n    actions_hidden_layers: tuple of hidden layers units for actions.\\n    joint_hidden_layers: tuple of hidden layers units after joining states\\n      and actions using tf.concat().\\n    weight_decay: Weight decay for l2 weights regularizer.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size] tensor of q values, or a tf.float32\\n      [batch_size, num_reward_dims] tensor of vector q values if\\n      num_reward_dims > 1.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_regularizer=slim.l2_regularizer(weight_decay), weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        orig_states = tf.to_float(states)\n        states = tf.concat([tf.to_float(states), tf.to_float(actions)], -1)\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        actions = tf.to_float(actions)\n        if states_hidden_layers:\n            states = slim.stack(states, slim.fully_connected, states_hidden_layers, scope='states')\n        if actions_hidden_layers:\n            actions = slim.stack(actions, slim.fully_connected, actions_hidden_layers, scope='actions')\n        joint = tf.concat([states, actions], 1)\n        if joint_hidden_layers:\n            joint = slim.stack(joint, slim.fully_connected, joint_hidden_layers, scope='joint')\n        with slim.arg_scope([slim.fully_connected], weights_regularizer=None, weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            value = slim.fully_connected(joint, num_reward_dims, activation_fn=None, normalizer_fn=None, scope='q_value')\n        if num_reward_dims == 1:\n            value = tf.reshape(value, [-1])\n        if not for_critic_loss and num_reward_dims > 1:\n            value = tf.reduce_sum(value * tf.abs(orig_states[:, -num_reward_dims:]), -1)\n    return value",
            "@gin.configurable('ddpg_critic_net')\ndef critic_net(states, actions, for_critic_loss=False, num_reward_dims=1, states_hidden_layers=(400,), actions_hidden_layers=None, joint_hidden_layers=(300,), weight_decay=0.0001, normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a critic that returns q values for the given states and actions.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    actions: (castable to tf.float32) a [batch_size, num_action_dims] tensor\\n      representing a batch of actions.\\n    num_reward_dims: Number of reward dimensions.\\n    states_hidden_layers: tuple of hidden layers units for states.\\n    actions_hidden_layers: tuple of hidden layers units for actions.\\n    joint_hidden_layers: tuple of hidden layers units after joining states\\n      and actions using tf.concat().\\n    weight_decay: Weight decay for l2 weights regularizer.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size] tensor of q values, or a tf.float32\\n      [batch_size, num_reward_dims] tensor of vector q values if\\n      num_reward_dims > 1.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_regularizer=slim.l2_regularizer(weight_decay), weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        orig_states = tf.to_float(states)\n        states = tf.concat([tf.to_float(states), tf.to_float(actions)], -1)\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        actions = tf.to_float(actions)\n        if states_hidden_layers:\n            states = slim.stack(states, slim.fully_connected, states_hidden_layers, scope='states')\n        if actions_hidden_layers:\n            actions = slim.stack(actions, slim.fully_connected, actions_hidden_layers, scope='actions')\n        joint = tf.concat([states, actions], 1)\n        if joint_hidden_layers:\n            joint = slim.stack(joint, slim.fully_connected, joint_hidden_layers, scope='joint')\n        with slim.arg_scope([slim.fully_connected], weights_regularizer=None, weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            value = slim.fully_connected(joint, num_reward_dims, activation_fn=None, normalizer_fn=None, scope='q_value')\n        if num_reward_dims == 1:\n            value = tf.reshape(value, [-1])\n        if not for_critic_loss and num_reward_dims > 1:\n            value = tf.reduce_sum(value * tf.abs(orig_states[:, -num_reward_dims:]), -1)\n    return value",
            "@gin.configurable('ddpg_critic_net')\ndef critic_net(states, actions, for_critic_loss=False, num_reward_dims=1, states_hidden_layers=(400,), actions_hidden_layers=None, joint_hidden_layers=(300,), weight_decay=0.0001, normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a critic that returns q values for the given states and actions.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    actions: (castable to tf.float32) a [batch_size, num_action_dims] tensor\\n      representing a batch of actions.\\n    num_reward_dims: Number of reward dimensions.\\n    states_hidden_layers: tuple of hidden layers units for states.\\n    actions_hidden_layers: tuple of hidden layers units for actions.\\n    joint_hidden_layers: tuple of hidden layers units after joining states\\n      and actions using tf.concat().\\n    weight_decay: Weight decay for l2 weights regularizer.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size] tensor of q values, or a tf.float32\\n      [batch_size, num_reward_dims] tensor of vector q values if\\n      num_reward_dims > 1.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_regularizer=slim.l2_regularizer(weight_decay), weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        orig_states = tf.to_float(states)\n        states = tf.concat([tf.to_float(states), tf.to_float(actions)], -1)\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        actions = tf.to_float(actions)\n        if states_hidden_layers:\n            states = slim.stack(states, slim.fully_connected, states_hidden_layers, scope='states')\n        if actions_hidden_layers:\n            actions = slim.stack(actions, slim.fully_connected, actions_hidden_layers, scope='actions')\n        joint = tf.concat([states, actions], 1)\n        if joint_hidden_layers:\n            joint = slim.stack(joint, slim.fully_connected, joint_hidden_layers, scope='joint')\n        with slim.arg_scope([slim.fully_connected], weights_regularizer=None, weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            value = slim.fully_connected(joint, num_reward_dims, activation_fn=None, normalizer_fn=None, scope='q_value')\n        if num_reward_dims == 1:\n            value = tf.reshape(value, [-1])\n        if not for_critic_loss and num_reward_dims > 1:\n            value = tf.reduce_sum(value * tf.abs(orig_states[:, -num_reward_dims:]), -1)\n    return value"
        ]
    },
    {
        "func_name": "actor_net",
        "original": "@gin.configurable('ddpg_actor_net')\ndef actor_net(states, action_spec, hidden_layers=(400, 300), normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    \"\"\"Creates an actor that returns actions for the given states.\n\n  Args:\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\n      representing a batch of states.\n    action_spec: (BoundedTensorSpec) A tensor spec indicating the shape\n      and range of actions.\n    hidden_layers: tuple of hidden layers units.\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\n  Returns:\n    A tf.float32 [batch_size, num_action_dims] tensor of actions.\n  \"\"\"\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        states = tf.to_float(states)\n        orig_states = states\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        if hidden_layers:\n            states = slim.stack(states, slim.fully_connected, hidden_layers, scope='states')\n        with slim.arg_scope([slim.fully_connected], weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            actions = slim.fully_connected(states, action_spec.shape.num_elements(), scope='actions', normalizer_fn=None, activation_fn=tf.nn.tanh)\n            action_means = (action_spec.maximum + action_spec.minimum) / 2.0\n            action_magnitudes = (action_spec.maximum - action_spec.minimum) / 2.0\n            actions = action_means + action_magnitudes * actions\n    return actions",
        "mutated": [
            "@gin.configurable('ddpg_actor_net')\ndef actor_net(states, action_spec, hidden_layers=(400, 300), normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n    'Creates an actor that returns actions for the given states.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    action_spec: (BoundedTensorSpec) A tensor spec indicating the shape\\n      and range of actions.\\n    hidden_layers: tuple of hidden layers units.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size, num_action_dims] tensor of actions.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        states = tf.to_float(states)\n        orig_states = states\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        if hidden_layers:\n            states = slim.stack(states, slim.fully_connected, hidden_layers, scope='states')\n        with slim.arg_scope([slim.fully_connected], weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            actions = slim.fully_connected(states, action_spec.shape.num_elements(), scope='actions', normalizer_fn=None, activation_fn=tf.nn.tanh)\n            action_means = (action_spec.maximum + action_spec.minimum) / 2.0\n            action_magnitudes = (action_spec.maximum - action_spec.minimum) / 2.0\n            actions = action_means + action_magnitudes * actions\n    return actions",
            "@gin.configurable('ddpg_actor_net')\ndef actor_net(states, action_spec, hidden_layers=(400, 300), normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates an actor that returns actions for the given states.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    action_spec: (BoundedTensorSpec) A tensor spec indicating the shape\\n      and range of actions.\\n    hidden_layers: tuple of hidden layers units.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size, num_action_dims] tensor of actions.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        states = tf.to_float(states)\n        orig_states = states\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        if hidden_layers:\n            states = slim.stack(states, slim.fully_connected, hidden_layers, scope='states')\n        with slim.arg_scope([slim.fully_connected], weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            actions = slim.fully_connected(states, action_spec.shape.num_elements(), scope='actions', normalizer_fn=None, activation_fn=tf.nn.tanh)\n            action_means = (action_spec.maximum + action_spec.minimum) / 2.0\n            action_magnitudes = (action_spec.maximum - action_spec.minimum) / 2.0\n            actions = action_means + action_magnitudes * actions\n    return actions",
            "@gin.configurable('ddpg_actor_net')\ndef actor_net(states, action_spec, hidden_layers=(400, 300), normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates an actor that returns actions for the given states.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    action_spec: (BoundedTensorSpec) A tensor spec indicating the shape\\n      and range of actions.\\n    hidden_layers: tuple of hidden layers units.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size, num_action_dims] tensor of actions.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        states = tf.to_float(states)\n        orig_states = states\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        if hidden_layers:\n            states = slim.stack(states, slim.fully_connected, hidden_layers, scope='states')\n        with slim.arg_scope([slim.fully_connected], weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            actions = slim.fully_connected(states, action_spec.shape.num_elements(), scope='actions', normalizer_fn=None, activation_fn=tf.nn.tanh)\n            action_means = (action_spec.maximum + action_spec.minimum) / 2.0\n            action_magnitudes = (action_spec.maximum - action_spec.minimum) / 2.0\n            actions = action_means + action_magnitudes * actions\n    return actions",
            "@gin.configurable('ddpg_actor_net')\ndef actor_net(states, action_spec, hidden_layers=(400, 300), normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates an actor that returns actions for the given states.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    action_spec: (BoundedTensorSpec) A tensor spec indicating the shape\\n      and range of actions.\\n    hidden_layers: tuple of hidden layers units.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size, num_action_dims] tensor of actions.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        states = tf.to_float(states)\n        orig_states = states\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        if hidden_layers:\n            states = slim.stack(states, slim.fully_connected, hidden_layers, scope='states')\n        with slim.arg_scope([slim.fully_connected], weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            actions = slim.fully_connected(states, action_spec.shape.num_elements(), scope='actions', normalizer_fn=None, activation_fn=tf.nn.tanh)\n            action_means = (action_spec.maximum + action_spec.minimum) / 2.0\n            action_magnitudes = (action_spec.maximum - action_spec.minimum) / 2.0\n            actions = action_means + action_magnitudes * actions\n    return actions",
            "@gin.configurable('ddpg_actor_net')\ndef actor_net(states, action_spec, hidden_layers=(400, 300), normalizer_fn=None, activation_fn=tf.nn.relu, zero_obs=False, images=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates an actor that returns actions for the given states.\\n\\n  Args:\\n    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor\\n      representing a batch of states.\\n    action_spec: (BoundedTensorSpec) A tensor spec indicating the shape\\n      and range of actions.\\n    hidden_layers: tuple of hidden layers units.\\n    normalizer_fn: Normalizer function, i.e. slim.layer_norm,\\n    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...\\n  Returns:\\n    A tf.float32 [batch_size, num_action_dims] tensor of actions.\\n  '\n    with slim.arg_scope([slim.fully_connected], activation_fn=activation_fn, normalizer_fn=normalizer_fn, weights_initializer=slim.variance_scaling_initializer(factor=1.0 / 3.0, mode='FAN_IN', uniform=True)):\n        states = tf.to_float(states)\n        orig_states = states\n        if images or zero_obs:\n            states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))\n        if hidden_layers:\n            states = slim.stack(states, slim.fully_connected, hidden_layers, scope='states')\n        with slim.arg_scope([slim.fully_connected], weights_initializer=tf.random_uniform_initializer(minval=-0.003, maxval=0.003)):\n            actions = slim.fully_connected(states, action_spec.shape.num_elements(), scope='actions', normalizer_fn=None, activation_fn=tf.nn.tanh)\n            action_means = (action_spec.maximum + action_spec.minimum) / 2.0\n            action_magnitudes = (action_spec.maximum - action_spec.minimum) / 2.0\n            actions = action_means + action_magnitudes * actions\n    return actions"
        ]
    }
]