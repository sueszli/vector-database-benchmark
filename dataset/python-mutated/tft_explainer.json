[
    {
        "func_name": "__init__",
        "original": "def __init__(self, model: TFTModel, background_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    \"\"\"\n        Explainer class for the `TFTModel`.\n\n        **Definitions**\n\n        - A background series is a `TimeSeries` that is used as a default for generating the explainability result\n          (if no `foreground` is passed to :func:`explain() <TFTExplainer.explain>`).\n        - A foreground series is a `TimeSeries` that can be passed to :func:`explain() <TFTExplainer.explain>` to use\n          instead of the background for generating the explainability result.\n\n        Parameters\n        ----------\n        model\n            The fitted `TFTModel` to be explained.\n        background_series\n            Optionally, a series or list of series to use as a default target series for the explanations.\n            Optional if `model` was trained on a single target series. By default, it is the `series` used at fitting\n            time.\n            Mandatory if `model` was trained on multiple (sequence of) target series.\n        background_past_covariates\n            Optionally, a past covariates series or list of series to use as a default past covariates series\n            for the explanations. The same requirements apply as for `background_series` .\n        background_future_covariates\n            Optionally, a future covariates series or list of series to use as a default future covariates series\n            for the explanations. The same requirements apply as for `background_series`.\n\n        Examples\n        --------\n        >>> from darts.datasets import AirPassengersDataset\n        >>> from darts.explainability.tft_explainer import TFTExplainer\n        >>> from darts.models import TFTModel\n        >>> series = AirPassengersDataset().load()\n        >>> model = TFTModel(\n        >>>     input_chunk_length=12,\n        >>>     output_chunk_length=6,\n        >>>     add_encoders={\"cyclic\": {\"future\": [\"hour\"]}}\n        >>> )\n        >>> model.fit(series)\n        >>> # create the explainer and generate explanations\n        >>> explainer = TFTExplainer(model)\n        >>> results = explainer.explain()\n        >>> # plot the results\n        >>> explainer.plot_attention(results, plot_type=\"all\")\n        >>> explainer.plot_variable_selection(results)\n        \"\"\"\n    super().__init__(model, background_series=background_series, background_past_covariates=background_past_covariates, background_future_covariates=background_future_covariates, requires_background=True, requires_covariates_encoding=False, check_component_names=False, test_stationarity=False)\n    if model.add_relative_index:\n        if self.future_covariates_components is not None:\n            self.future_covariates_components.append('add_relative_index')\n        else:\n            self.future_covariates_components = ['add_relative_index']",
        "mutated": [
            "def __init__(self, model: TFTModel, background_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n    '\\n        Explainer class for the `TFTModel`.\\n\\n        **Definitions**\\n\\n        - A background series is a `TimeSeries` that is used as a default for generating the explainability result\\n          (if no `foreground` is passed to :func:`explain() <TFTExplainer.explain>`).\\n        - A foreground series is a `TimeSeries` that can be passed to :func:`explain() <TFTExplainer.explain>` to use\\n          instead of the background for generating the explainability result.\\n\\n        Parameters\\n        ----------\\n        model\\n            The fitted `TFTModel` to be explained.\\n        background_series\\n            Optionally, a series or list of series to use as a default target series for the explanations.\\n            Optional if `model` was trained on a single target series. By default, it is the `series` used at fitting\\n            time.\\n            Mandatory if `model` was trained on multiple (sequence of) target series.\\n        background_past_covariates\\n            Optionally, a past covariates series or list of series to use as a default past covariates series\\n            for the explanations. The same requirements apply as for `background_series` .\\n        background_future_covariates\\n            Optionally, a future covariates series or list of series to use as a default future covariates series\\n            for the explanations. The same requirements apply as for `background_series`.\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.explainability.tft_explainer import TFTExplainer\\n        >>> from darts.models import TFTModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=12,\\n        >>>     output_chunk_length=6,\\n        >>>     add_encoders={\"cyclic\": {\"future\": [\"hour\"]}}\\n        >>> )\\n        >>> model.fit(series)\\n        >>> # create the explainer and generate explanations\\n        >>> explainer = TFTExplainer(model)\\n        >>> results = explainer.explain()\\n        >>> # plot the results\\n        >>> explainer.plot_attention(results, plot_type=\"all\")\\n        >>> explainer.plot_variable_selection(results)\\n        '\n    super().__init__(model, background_series=background_series, background_past_covariates=background_past_covariates, background_future_covariates=background_future_covariates, requires_background=True, requires_covariates_encoding=False, check_component_names=False, test_stationarity=False)\n    if model.add_relative_index:\n        if self.future_covariates_components is not None:\n            self.future_covariates_components.append('add_relative_index')\n        else:\n            self.future_covariates_components = ['add_relative_index']",
            "def __init__(self, model: TFTModel, background_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Explainer class for the `TFTModel`.\\n\\n        **Definitions**\\n\\n        - A background series is a `TimeSeries` that is used as a default for generating the explainability result\\n          (if no `foreground` is passed to :func:`explain() <TFTExplainer.explain>`).\\n        - A foreground series is a `TimeSeries` that can be passed to :func:`explain() <TFTExplainer.explain>` to use\\n          instead of the background for generating the explainability result.\\n\\n        Parameters\\n        ----------\\n        model\\n            The fitted `TFTModel` to be explained.\\n        background_series\\n            Optionally, a series or list of series to use as a default target series for the explanations.\\n            Optional if `model` was trained on a single target series. By default, it is the `series` used at fitting\\n            time.\\n            Mandatory if `model` was trained on multiple (sequence of) target series.\\n        background_past_covariates\\n            Optionally, a past covariates series or list of series to use as a default past covariates series\\n            for the explanations. The same requirements apply as for `background_series` .\\n        background_future_covariates\\n            Optionally, a future covariates series or list of series to use as a default future covariates series\\n            for the explanations. The same requirements apply as for `background_series`.\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.explainability.tft_explainer import TFTExplainer\\n        >>> from darts.models import TFTModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=12,\\n        >>>     output_chunk_length=6,\\n        >>>     add_encoders={\"cyclic\": {\"future\": [\"hour\"]}}\\n        >>> )\\n        >>> model.fit(series)\\n        >>> # create the explainer and generate explanations\\n        >>> explainer = TFTExplainer(model)\\n        >>> results = explainer.explain()\\n        >>> # plot the results\\n        >>> explainer.plot_attention(results, plot_type=\"all\")\\n        >>> explainer.plot_variable_selection(results)\\n        '\n    super().__init__(model, background_series=background_series, background_past_covariates=background_past_covariates, background_future_covariates=background_future_covariates, requires_background=True, requires_covariates_encoding=False, check_component_names=False, test_stationarity=False)\n    if model.add_relative_index:\n        if self.future_covariates_components is not None:\n            self.future_covariates_components.append('add_relative_index')\n        else:\n            self.future_covariates_components = ['add_relative_index']",
            "def __init__(self, model: TFTModel, background_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Explainer class for the `TFTModel`.\\n\\n        **Definitions**\\n\\n        - A background series is a `TimeSeries` that is used as a default for generating the explainability result\\n          (if no `foreground` is passed to :func:`explain() <TFTExplainer.explain>`).\\n        - A foreground series is a `TimeSeries` that can be passed to :func:`explain() <TFTExplainer.explain>` to use\\n          instead of the background for generating the explainability result.\\n\\n        Parameters\\n        ----------\\n        model\\n            The fitted `TFTModel` to be explained.\\n        background_series\\n            Optionally, a series or list of series to use as a default target series for the explanations.\\n            Optional if `model` was trained on a single target series. By default, it is the `series` used at fitting\\n            time.\\n            Mandatory if `model` was trained on multiple (sequence of) target series.\\n        background_past_covariates\\n            Optionally, a past covariates series or list of series to use as a default past covariates series\\n            for the explanations. The same requirements apply as for `background_series` .\\n        background_future_covariates\\n            Optionally, a future covariates series or list of series to use as a default future covariates series\\n            for the explanations. The same requirements apply as for `background_series`.\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.explainability.tft_explainer import TFTExplainer\\n        >>> from darts.models import TFTModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=12,\\n        >>>     output_chunk_length=6,\\n        >>>     add_encoders={\"cyclic\": {\"future\": [\"hour\"]}}\\n        >>> )\\n        >>> model.fit(series)\\n        >>> # create the explainer and generate explanations\\n        >>> explainer = TFTExplainer(model)\\n        >>> results = explainer.explain()\\n        >>> # plot the results\\n        >>> explainer.plot_attention(results, plot_type=\"all\")\\n        >>> explainer.plot_variable_selection(results)\\n        '\n    super().__init__(model, background_series=background_series, background_past_covariates=background_past_covariates, background_future_covariates=background_future_covariates, requires_background=True, requires_covariates_encoding=False, check_component_names=False, test_stationarity=False)\n    if model.add_relative_index:\n        if self.future_covariates_components is not None:\n            self.future_covariates_components.append('add_relative_index')\n        else:\n            self.future_covariates_components = ['add_relative_index']",
            "def __init__(self, model: TFTModel, background_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Explainer class for the `TFTModel`.\\n\\n        **Definitions**\\n\\n        - A background series is a `TimeSeries` that is used as a default for generating the explainability result\\n          (if no `foreground` is passed to :func:`explain() <TFTExplainer.explain>`).\\n        - A foreground series is a `TimeSeries` that can be passed to :func:`explain() <TFTExplainer.explain>` to use\\n          instead of the background for generating the explainability result.\\n\\n        Parameters\\n        ----------\\n        model\\n            The fitted `TFTModel` to be explained.\\n        background_series\\n            Optionally, a series or list of series to use as a default target series for the explanations.\\n            Optional if `model` was trained on a single target series. By default, it is the `series` used at fitting\\n            time.\\n            Mandatory if `model` was trained on multiple (sequence of) target series.\\n        background_past_covariates\\n            Optionally, a past covariates series or list of series to use as a default past covariates series\\n            for the explanations. The same requirements apply as for `background_series` .\\n        background_future_covariates\\n            Optionally, a future covariates series or list of series to use as a default future covariates series\\n            for the explanations. The same requirements apply as for `background_series`.\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.explainability.tft_explainer import TFTExplainer\\n        >>> from darts.models import TFTModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=12,\\n        >>>     output_chunk_length=6,\\n        >>>     add_encoders={\"cyclic\": {\"future\": [\"hour\"]}}\\n        >>> )\\n        >>> model.fit(series)\\n        >>> # create the explainer and generate explanations\\n        >>> explainer = TFTExplainer(model)\\n        >>> results = explainer.explain()\\n        >>> # plot the results\\n        >>> explainer.plot_attention(results, plot_type=\"all\")\\n        >>> explainer.plot_variable_selection(results)\\n        '\n    super().__init__(model, background_series=background_series, background_past_covariates=background_past_covariates, background_future_covariates=background_future_covariates, requires_background=True, requires_covariates_encoding=False, check_component_names=False, test_stationarity=False)\n    if model.add_relative_index:\n        if self.future_covariates_components is not None:\n            self.future_covariates_components.append('add_relative_index')\n        else:\n            self.future_covariates_components = ['add_relative_index']",
            "def __init__(self, model: TFTModel, background_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, background_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Explainer class for the `TFTModel`.\\n\\n        **Definitions**\\n\\n        - A background series is a `TimeSeries` that is used as a default for generating the explainability result\\n          (if no `foreground` is passed to :func:`explain() <TFTExplainer.explain>`).\\n        - A foreground series is a `TimeSeries` that can be passed to :func:`explain() <TFTExplainer.explain>` to use\\n          instead of the background for generating the explainability result.\\n\\n        Parameters\\n        ----------\\n        model\\n            The fitted `TFTModel` to be explained.\\n        background_series\\n            Optionally, a series or list of series to use as a default target series for the explanations.\\n            Optional if `model` was trained on a single target series. By default, it is the `series` used at fitting\\n            time.\\n            Mandatory if `model` was trained on multiple (sequence of) target series.\\n        background_past_covariates\\n            Optionally, a past covariates series or list of series to use as a default past covariates series\\n            for the explanations. The same requirements apply as for `background_series` .\\n        background_future_covariates\\n            Optionally, a future covariates series or list of series to use as a default future covariates series\\n            for the explanations. The same requirements apply as for `background_series`.\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import AirPassengersDataset\\n        >>> from darts.explainability.tft_explainer import TFTExplainer\\n        >>> from darts.models import TFTModel\\n        >>> series = AirPassengersDataset().load()\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=12,\\n        >>>     output_chunk_length=6,\\n        >>>     add_encoders={\"cyclic\": {\"future\": [\"hour\"]}}\\n        >>> )\\n        >>> model.fit(series)\\n        >>> # create the explainer and generate explanations\\n        >>> explainer = TFTExplainer(model)\\n        >>> results = explainer.explain()\\n        >>> # plot the results\\n        >>> explainer.plot_attention(results, plot_type=\"all\")\\n        >>> explainer.plot_variable_selection(results)\\n        '\n    super().__init__(model, background_series=background_series, background_past_covariates=background_past_covariates, background_future_covariates=background_future_covariates, requires_background=True, requires_covariates_encoding=False, check_component_names=False, test_stationarity=False)\n    if model.add_relative_index:\n        if self.future_covariates_components is not None:\n            self.future_covariates_components.append('add_relative_index')\n        else:\n            self.future_covariates_components = ['add_relative_index']"
        ]
    },
    {
        "func_name": "explain",
        "original": "def explain(self, foreground_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, horizons: Optional[Sequence[int]]=None, target_components: Optional[Sequence[str]]=None) -> TFTExplainabilityResult:\n    \"\"\"Returns the :class:`TFTExplainabilityResult\n        <darts.explainability.explainability_result.TFTExplainabilityResult>` result for all series in\n        `foreground_series`. If `foreground_series` is `None`, will use the `background` input\n        from `TFTExplainer` creation (either the `background` passed to creation, or the series stored in the\n        `TFTModel` in case it was only trained on a single series).\n        For each series, the results contain the attention heads, encoder variable importances, decoder variable\n        importances, and static covariates importances.\n\n        Parameters\n        ----------\n        foreground_series\n            Optionally, one or a sequence of target `TimeSeries` to be explained. Can be multivariate.\n            If not provided, the background `TimeSeries` will be explained instead.\n        foreground_past_covariates\n            Optionally, one or a sequence of past covariates `TimeSeries` if required by the forecasting model.\n        foreground_future_covariates\n            Optionally, one or a sequence of future covariates `TimeSeries` if required by the forecasting model.\n        horizons\n            This parameter is not used by the `TFTExplainer`.\n        target_components\n            This parameter is not used by the `TFTExplainer`.\n\n        Returns\n        -------\n        TFTExplainabilityResult\n            The explainability result containing the attention heads, encoder variable importances, decoder variable\n            importances, and static covariates importances.\n\n        Examples\n        --------\n        >>> explainer = TFTExplainer(model)  # requires `background` if model was trained on multiple series\n\n        Optionally, give a foreground input to generate the explanation on a new input.\n        Otherwise, leave it empty to compute the explanation on the background from `TFTExplainer` creation\n\n        >>> explain_results = explainer.explain(\n        >>>     foreground_series=foreground_series,\n        >>>     foreground_past_covariates=foreground_past_covariates,\n        >>>     foreground_future_covariates=foreground_future_covariates,\n        >>> )\n        >>> attn = explain_results.get_attention()\n        >>> importances = explain_results.get_feature_importances()\n        \"\"\"\n    if target_components is not None or horizons is not None:\n        logger.warning('`horizons`, and `target_components` are not supported by `TFTExplainer` and will be ignored.')\n    super().explain(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (foreground_series, foreground_past_covariates, foreground_future_covariates, _, _, _, _) = self._process_foreground(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (horizons, _) = self._process_horizons_and_targets(None, None)\n    preds = self.model.predict(n=self.n, series=foreground_series, past_covariates=foreground_past_covariates, future_covariates=foreground_future_covariates)\n    attention_heads = self.model.model._attn_out_weights.detach().cpu().numpy().sum(axis=-2)\n    encoder_importance = self._encoder_importance\n    decoder_importance = self._decoder_importance\n    static_covariates_importance = self._static_covariates_importance\n    horizon_idx = [h - 1 for h in horizons]\n    results = []\n    icl = self.model.input_chunk_length\n    for (idx, (series, pred_series)) in enumerate(zip(foreground_series, preds)):\n        times = series.time_index[-icl:].union(pred_series.time_index)\n        attention = TimeSeries.from_times_and_values(values=np.take(attention_heads[idx], horizon_idx, axis=0).T, times=times, columns=[f'horizon {str(i)}' for i in horizons])\n        results.append({'attention': attention, 'encoder_importance': encoder_importance.iloc[idx:idx + 1], 'decoder_importance': decoder_importance.iloc[idx:idx + 1], 'static_covariates_importance': static_covariates_importance.iloc[idx:idx + 1]})\n    return TFTExplainabilityResult(explanations=results[0] if len(results) == 1 else results)",
        "mutated": [
            "def explain(self, foreground_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, horizons: Optional[Sequence[int]]=None, target_components: Optional[Sequence[str]]=None) -> TFTExplainabilityResult:\n    if False:\n        i = 10\n    'Returns the :class:`TFTExplainabilityResult\\n        <darts.explainability.explainability_result.TFTExplainabilityResult>` result for all series in\\n        `foreground_series`. If `foreground_series` is `None`, will use the `background` input\\n        from `TFTExplainer` creation (either the `background` passed to creation, or the series stored in the\\n        `TFTModel` in case it was only trained on a single series).\\n        For each series, the results contain the attention heads, encoder variable importances, decoder variable\\n        importances, and static covariates importances.\\n\\n        Parameters\\n        ----------\\n        foreground_series\\n            Optionally, one or a sequence of target `TimeSeries` to be explained. Can be multivariate.\\n            If not provided, the background `TimeSeries` will be explained instead.\\n        foreground_past_covariates\\n            Optionally, one or a sequence of past covariates `TimeSeries` if required by the forecasting model.\\n        foreground_future_covariates\\n            Optionally, one or a sequence of future covariates `TimeSeries` if required by the forecasting model.\\n        horizons\\n            This parameter is not used by the `TFTExplainer`.\\n        target_components\\n            This parameter is not used by the `TFTExplainer`.\\n\\n        Returns\\n        -------\\n        TFTExplainabilityResult\\n            The explainability result containing the attention heads, encoder variable importances, decoder variable\\n            importances, and static covariates importances.\\n\\n        Examples\\n        --------\\n        >>> explainer = TFTExplainer(model)  # requires `background` if model was trained on multiple series\\n\\n        Optionally, give a foreground input to generate the explanation on a new input.\\n        Otherwise, leave it empty to compute the explanation on the background from `TFTExplainer` creation\\n\\n        >>> explain_results = explainer.explain(\\n        >>>     foreground_series=foreground_series,\\n        >>>     foreground_past_covariates=foreground_past_covariates,\\n        >>>     foreground_future_covariates=foreground_future_covariates,\\n        >>> )\\n        >>> attn = explain_results.get_attention()\\n        >>> importances = explain_results.get_feature_importances()\\n        '\n    if target_components is not None or horizons is not None:\n        logger.warning('`horizons`, and `target_components` are not supported by `TFTExplainer` and will be ignored.')\n    super().explain(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (foreground_series, foreground_past_covariates, foreground_future_covariates, _, _, _, _) = self._process_foreground(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (horizons, _) = self._process_horizons_and_targets(None, None)\n    preds = self.model.predict(n=self.n, series=foreground_series, past_covariates=foreground_past_covariates, future_covariates=foreground_future_covariates)\n    attention_heads = self.model.model._attn_out_weights.detach().cpu().numpy().sum(axis=-2)\n    encoder_importance = self._encoder_importance\n    decoder_importance = self._decoder_importance\n    static_covariates_importance = self._static_covariates_importance\n    horizon_idx = [h - 1 for h in horizons]\n    results = []\n    icl = self.model.input_chunk_length\n    for (idx, (series, pred_series)) in enumerate(zip(foreground_series, preds)):\n        times = series.time_index[-icl:].union(pred_series.time_index)\n        attention = TimeSeries.from_times_and_values(values=np.take(attention_heads[idx], horizon_idx, axis=0).T, times=times, columns=[f'horizon {str(i)}' for i in horizons])\n        results.append({'attention': attention, 'encoder_importance': encoder_importance.iloc[idx:idx + 1], 'decoder_importance': decoder_importance.iloc[idx:idx + 1], 'static_covariates_importance': static_covariates_importance.iloc[idx:idx + 1]})\n    return TFTExplainabilityResult(explanations=results[0] if len(results) == 1 else results)",
            "def explain(self, foreground_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, horizons: Optional[Sequence[int]]=None, target_components: Optional[Sequence[str]]=None) -> TFTExplainabilityResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the :class:`TFTExplainabilityResult\\n        <darts.explainability.explainability_result.TFTExplainabilityResult>` result for all series in\\n        `foreground_series`. If `foreground_series` is `None`, will use the `background` input\\n        from `TFTExplainer` creation (either the `background` passed to creation, or the series stored in the\\n        `TFTModel` in case it was only trained on a single series).\\n        For each series, the results contain the attention heads, encoder variable importances, decoder variable\\n        importances, and static covariates importances.\\n\\n        Parameters\\n        ----------\\n        foreground_series\\n            Optionally, one or a sequence of target `TimeSeries` to be explained. Can be multivariate.\\n            If not provided, the background `TimeSeries` will be explained instead.\\n        foreground_past_covariates\\n            Optionally, one or a sequence of past covariates `TimeSeries` if required by the forecasting model.\\n        foreground_future_covariates\\n            Optionally, one or a sequence of future covariates `TimeSeries` if required by the forecasting model.\\n        horizons\\n            This parameter is not used by the `TFTExplainer`.\\n        target_components\\n            This parameter is not used by the `TFTExplainer`.\\n\\n        Returns\\n        -------\\n        TFTExplainabilityResult\\n            The explainability result containing the attention heads, encoder variable importances, decoder variable\\n            importances, and static covariates importances.\\n\\n        Examples\\n        --------\\n        >>> explainer = TFTExplainer(model)  # requires `background` if model was trained on multiple series\\n\\n        Optionally, give a foreground input to generate the explanation on a new input.\\n        Otherwise, leave it empty to compute the explanation on the background from `TFTExplainer` creation\\n\\n        >>> explain_results = explainer.explain(\\n        >>>     foreground_series=foreground_series,\\n        >>>     foreground_past_covariates=foreground_past_covariates,\\n        >>>     foreground_future_covariates=foreground_future_covariates,\\n        >>> )\\n        >>> attn = explain_results.get_attention()\\n        >>> importances = explain_results.get_feature_importances()\\n        '\n    if target_components is not None or horizons is not None:\n        logger.warning('`horizons`, and `target_components` are not supported by `TFTExplainer` and will be ignored.')\n    super().explain(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (foreground_series, foreground_past_covariates, foreground_future_covariates, _, _, _, _) = self._process_foreground(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (horizons, _) = self._process_horizons_and_targets(None, None)\n    preds = self.model.predict(n=self.n, series=foreground_series, past_covariates=foreground_past_covariates, future_covariates=foreground_future_covariates)\n    attention_heads = self.model.model._attn_out_weights.detach().cpu().numpy().sum(axis=-2)\n    encoder_importance = self._encoder_importance\n    decoder_importance = self._decoder_importance\n    static_covariates_importance = self._static_covariates_importance\n    horizon_idx = [h - 1 for h in horizons]\n    results = []\n    icl = self.model.input_chunk_length\n    for (idx, (series, pred_series)) in enumerate(zip(foreground_series, preds)):\n        times = series.time_index[-icl:].union(pred_series.time_index)\n        attention = TimeSeries.from_times_and_values(values=np.take(attention_heads[idx], horizon_idx, axis=0).T, times=times, columns=[f'horizon {str(i)}' for i in horizons])\n        results.append({'attention': attention, 'encoder_importance': encoder_importance.iloc[idx:idx + 1], 'decoder_importance': decoder_importance.iloc[idx:idx + 1], 'static_covariates_importance': static_covariates_importance.iloc[idx:idx + 1]})\n    return TFTExplainabilityResult(explanations=results[0] if len(results) == 1 else results)",
            "def explain(self, foreground_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, horizons: Optional[Sequence[int]]=None, target_components: Optional[Sequence[str]]=None) -> TFTExplainabilityResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the :class:`TFTExplainabilityResult\\n        <darts.explainability.explainability_result.TFTExplainabilityResult>` result for all series in\\n        `foreground_series`. If `foreground_series` is `None`, will use the `background` input\\n        from `TFTExplainer` creation (either the `background` passed to creation, or the series stored in the\\n        `TFTModel` in case it was only trained on a single series).\\n        For each series, the results contain the attention heads, encoder variable importances, decoder variable\\n        importances, and static covariates importances.\\n\\n        Parameters\\n        ----------\\n        foreground_series\\n            Optionally, one or a sequence of target `TimeSeries` to be explained. Can be multivariate.\\n            If not provided, the background `TimeSeries` will be explained instead.\\n        foreground_past_covariates\\n            Optionally, one or a sequence of past covariates `TimeSeries` if required by the forecasting model.\\n        foreground_future_covariates\\n            Optionally, one or a sequence of future covariates `TimeSeries` if required by the forecasting model.\\n        horizons\\n            This parameter is not used by the `TFTExplainer`.\\n        target_components\\n            This parameter is not used by the `TFTExplainer`.\\n\\n        Returns\\n        -------\\n        TFTExplainabilityResult\\n            The explainability result containing the attention heads, encoder variable importances, decoder variable\\n            importances, and static covariates importances.\\n\\n        Examples\\n        --------\\n        >>> explainer = TFTExplainer(model)  # requires `background` if model was trained on multiple series\\n\\n        Optionally, give a foreground input to generate the explanation on a new input.\\n        Otherwise, leave it empty to compute the explanation on the background from `TFTExplainer` creation\\n\\n        >>> explain_results = explainer.explain(\\n        >>>     foreground_series=foreground_series,\\n        >>>     foreground_past_covariates=foreground_past_covariates,\\n        >>>     foreground_future_covariates=foreground_future_covariates,\\n        >>> )\\n        >>> attn = explain_results.get_attention()\\n        >>> importances = explain_results.get_feature_importances()\\n        '\n    if target_components is not None or horizons is not None:\n        logger.warning('`horizons`, and `target_components` are not supported by `TFTExplainer` and will be ignored.')\n    super().explain(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (foreground_series, foreground_past_covariates, foreground_future_covariates, _, _, _, _) = self._process_foreground(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (horizons, _) = self._process_horizons_and_targets(None, None)\n    preds = self.model.predict(n=self.n, series=foreground_series, past_covariates=foreground_past_covariates, future_covariates=foreground_future_covariates)\n    attention_heads = self.model.model._attn_out_weights.detach().cpu().numpy().sum(axis=-2)\n    encoder_importance = self._encoder_importance\n    decoder_importance = self._decoder_importance\n    static_covariates_importance = self._static_covariates_importance\n    horizon_idx = [h - 1 for h in horizons]\n    results = []\n    icl = self.model.input_chunk_length\n    for (idx, (series, pred_series)) in enumerate(zip(foreground_series, preds)):\n        times = series.time_index[-icl:].union(pred_series.time_index)\n        attention = TimeSeries.from_times_and_values(values=np.take(attention_heads[idx], horizon_idx, axis=0).T, times=times, columns=[f'horizon {str(i)}' for i in horizons])\n        results.append({'attention': attention, 'encoder_importance': encoder_importance.iloc[idx:idx + 1], 'decoder_importance': decoder_importance.iloc[idx:idx + 1], 'static_covariates_importance': static_covariates_importance.iloc[idx:idx + 1]})\n    return TFTExplainabilityResult(explanations=results[0] if len(results) == 1 else results)",
            "def explain(self, foreground_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, horizons: Optional[Sequence[int]]=None, target_components: Optional[Sequence[str]]=None) -> TFTExplainabilityResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the :class:`TFTExplainabilityResult\\n        <darts.explainability.explainability_result.TFTExplainabilityResult>` result for all series in\\n        `foreground_series`. If `foreground_series` is `None`, will use the `background` input\\n        from `TFTExplainer` creation (either the `background` passed to creation, or the series stored in the\\n        `TFTModel` in case it was only trained on a single series).\\n        For each series, the results contain the attention heads, encoder variable importances, decoder variable\\n        importances, and static covariates importances.\\n\\n        Parameters\\n        ----------\\n        foreground_series\\n            Optionally, one or a sequence of target `TimeSeries` to be explained. Can be multivariate.\\n            If not provided, the background `TimeSeries` will be explained instead.\\n        foreground_past_covariates\\n            Optionally, one or a sequence of past covariates `TimeSeries` if required by the forecasting model.\\n        foreground_future_covariates\\n            Optionally, one or a sequence of future covariates `TimeSeries` if required by the forecasting model.\\n        horizons\\n            This parameter is not used by the `TFTExplainer`.\\n        target_components\\n            This parameter is not used by the `TFTExplainer`.\\n\\n        Returns\\n        -------\\n        TFTExplainabilityResult\\n            The explainability result containing the attention heads, encoder variable importances, decoder variable\\n            importances, and static covariates importances.\\n\\n        Examples\\n        --------\\n        >>> explainer = TFTExplainer(model)  # requires `background` if model was trained on multiple series\\n\\n        Optionally, give a foreground input to generate the explanation on a new input.\\n        Otherwise, leave it empty to compute the explanation on the background from `TFTExplainer` creation\\n\\n        >>> explain_results = explainer.explain(\\n        >>>     foreground_series=foreground_series,\\n        >>>     foreground_past_covariates=foreground_past_covariates,\\n        >>>     foreground_future_covariates=foreground_future_covariates,\\n        >>> )\\n        >>> attn = explain_results.get_attention()\\n        >>> importances = explain_results.get_feature_importances()\\n        '\n    if target_components is not None or horizons is not None:\n        logger.warning('`horizons`, and `target_components` are not supported by `TFTExplainer` and will be ignored.')\n    super().explain(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (foreground_series, foreground_past_covariates, foreground_future_covariates, _, _, _, _) = self._process_foreground(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (horizons, _) = self._process_horizons_and_targets(None, None)\n    preds = self.model.predict(n=self.n, series=foreground_series, past_covariates=foreground_past_covariates, future_covariates=foreground_future_covariates)\n    attention_heads = self.model.model._attn_out_weights.detach().cpu().numpy().sum(axis=-2)\n    encoder_importance = self._encoder_importance\n    decoder_importance = self._decoder_importance\n    static_covariates_importance = self._static_covariates_importance\n    horizon_idx = [h - 1 for h in horizons]\n    results = []\n    icl = self.model.input_chunk_length\n    for (idx, (series, pred_series)) in enumerate(zip(foreground_series, preds)):\n        times = series.time_index[-icl:].union(pred_series.time_index)\n        attention = TimeSeries.from_times_and_values(values=np.take(attention_heads[idx], horizon_idx, axis=0).T, times=times, columns=[f'horizon {str(i)}' for i in horizons])\n        results.append({'attention': attention, 'encoder_importance': encoder_importance.iloc[idx:idx + 1], 'decoder_importance': decoder_importance.iloc[idx:idx + 1], 'static_covariates_importance': static_covariates_importance.iloc[idx:idx + 1]})\n    return TFTExplainabilityResult(explanations=results[0] if len(results) == 1 else results)",
            "def explain(self, foreground_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, foreground_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]]=None, horizons: Optional[Sequence[int]]=None, target_components: Optional[Sequence[str]]=None) -> TFTExplainabilityResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the :class:`TFTExplainabilityResult\\n        <darts.explainability.explainability_result.TFTExplainabilityResult>` result for all series in\\n        `foreground_series`. If `foreground_series` is `None`, will use the `background` input\\n        from `TFTExplainer` creation (either the `background` passed to creation, or the series stored in the\\n        `TFTModel` in case it was only trained on a single series).\\n        For each series, the results contain the attention heads, encoder variable importances, decoder variable\\n        importances, and static covariates importances.\\n\\n        Parameters\\n        ----------\\n        foreground_series\\n            Optionally, one or a sequence of target `TimeSeries` to be explained. Can be multivariate.\\n            If not provided, the background `TimeSeries` will be explained instead.\\n        foreground_past_covariates\\n            Optionally, one or a sequence of past covariates `TimeSeries` if required by the forecasting model.\\n        foreground_future_covariates\\n            Optionally, one or a sequence of future covariates `TimeSeries` if required by the forecasting model.\\n        horizons\\n            This parameter is not used by the `TFTExplainer`.\\n        target_components\\n            This parameter is not used by the `TFTExplainer`.\\n\\n        Returns\\n        -------\\n        TFTExplainabilityResult\\n            The explainability result containing the attention heads, encoder variable importances, decoder variable\\n            importances, and static covariates importances.\\n\\n        Examples\\n        --------\\n        >>> explainer = TFTExplainer(model)  # requires `background` if model was trained on multiple series\\n\\n        Optionally, give a foreground input to generate the explanation on a new input.\\n        Otherwise, leave it empty to compute the explanation on the background from `TFTExplainer` creation\\n\\n        >>> explain_results = explainer.explain(\\n        >>>     foreground_series=foreground_series,\\n        >>>     foreground_past_covariates=foreground_past_covariates,\\n        >>>     foreground_future_covariates=foreground_future_covariates,\\n        >>> )\\n        >>> attn = explain_results.get_attention()\\n        >>> importances = explain_results.get_feature_importances()\\n        '\n    if target_components is not None or horizons is not None:\n        logger.warning('`horizons`, and `target_components` are not supported by `TFTExplainer` and will be ignored.')\n    super().explain(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (foreground_series, foreground_past_covariates, foreground_future_covariates, _, _, _, _) = self._process_foreground(foreground_series, foreground_past_covariates, foreground_future_covariates)\n    (horizons, _) = self._process_horizons_and_targets(None, None)\n    preds = self.model.predict(n=self.n, series=foreground_series, past_covariates=foreground_past_covariates, future_covariates=foreground_future_covariates)\n    attention_heads = self.model.model._attn_out_weights.detach().cpu().numpy().sum(axis=-2)\n    encoder_importance = self._encoder_importance\n    decoder_importance = self._decoder_importance\n    static_covariates_importance = self._static_covariates_importance\n    horizon_idx = [h - 1 for h in horizons]\n    results = []\n    icl = self.model.input_chunk_length\n    for (idx, (series, pred_series)) in enumerate(zip(foreground_series, preds)):\n        times = series.time_index[-icl:].union(pred_series.time_index)\n        attention = TimeSeries.from_times_and_values(values=np.take(attention_heads[idx], horizon_idx, axis=0).T, times=times, columns=[f'horizon {str(i)}' for i in horizons])\n        results.append({'attention': attention, 'encoder_importance': encoder_importance.iloc[idx:idx + 1], 'decoder_importance': decoder_importance.iloc[idx:idx + 1], 'static_covariates_importance': static_covariates_importance.iloc[idx:idx + 1]})\n    return TFTExplainabilityResult(explanations=results[0] if len(results) == 1 else results)"
        ]
    },
    {
        "func_name": "plot_variable_selection",
        "original": "def plot_variable_selection(self, expl_result: TFTExplainabilityResult, fig_size=None, max_nr_series: int=5):\n    \"\"\"Plots the variable selection / feature importances of the `TFTModel` based on the input.\n        The figure includes three subplots:\n\n        - encoder importances: contains the past target, past covariates, and historic future covariates importance\n          on the encoder (input chunk)\n        - decoder importances: contains the future covariates importance on the decoder (output chunk)\n        - static covariates importances: contains the numeric and / or categorical static covariates importance\n\n        Parameters\n        ----------\n        expl_result\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\n        fig_size\n            The size of the figure to be plotted.\n        max_nr_series\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\n        \"\"\"\n    encoder_importance = expl_result.get_encoder_importance()\n    decoder_importance = expl_result.get_decoder_importance()\n    static_covariates_importance = expl_result.get_static_covariates_importance()\n    if not isinstance(encoder_importance, list):\n        encoder_importance = [encoder_importance]\n        decoder_importance = [decoder_importance]\n        static_covariates_importance = [static_covariates_importance]\n    uses_static_covariates = not static_covariates_importance[0].empty\n    for (idx, (enc_imp, dec_imp, stc_imp)) in enumerate(zip(encoder_importance, decoder_importance, static_covariates_importance)):\n        (fig, axes) = plt.subplots(nrows=3 if uses_static_covariates else 2, sharex=True, figsize=fig_size)\n        self._plot_cov_selection(enc_imp, title='Encoder variable importance', ax=axes[0])\n        axes[0].set_xlabel('')\n        self._plot_cov_selection(dec_imp, title='Decoder variable importance', ax=axes[1])\n        if uses_static_covariates:\n            axes[1].set_xlabel('')\n            self._plot_cov_selection(stc_imp, title='Static variable importance', ax=axes[2])\n        fig.tight_layout()\n        plt.show()\n        if idx + 1 == max_nr_series:\n            break",
        "mutated": [
            "def plot_variable_selection(self, expl_result: TFTExplainabilityResult, fig_size=None, max_nr_series: int=5):\n    if False:\n        i = 10\n    'Plots the variable selection / feature importances of the `TFTModel` based on the input.\\n        The figure includes three subplots:\\n\\n        - encoder importances: contains the past target, past covariates, and historic future covariates importance\\n          on the encoder (input chunk)\\n        - decoder importances: contains the future covariates importance on the decoder (output chunk)\\n        - static covariates importances: contains the numeric and / or categorical static covariates importance\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        fig_size\\n            The size of the figure to be plotted.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        '\n    encoder_importance = expl_result.get_encoder_importance()\n    decoder_importance = expl_result.get_decoder_importance()\n    static_covariates_importance = expl_result.get_static_covariates_importance()\n    if not isinstance(encoder_importance, list):\n        encoder_importance = [encoder_importance]\n        decoder_importance = [decoder_importance]\n        static_covariates_importance = [static_covariates_importance]\n    uses_static_covariates = not static_covariates_importance[0].empty\n    for (idx, (enc_imp, dec_imp, stc_imp)) in enumerate(zip(encoder_importance, decoder_importance, static_covariates_importance)):\n        (fig, axes) = plt.subplots(nrows=3 if uses_static_covariates else 2, sharex=True, figsize=fig_size)\n        self._plot_cov_selection(enc_imp, title='Encoder variable importance', ax=axes[0])\n        axes[0].set_xlabel('')\n        self._plot_cov_selection(dec_imp, title='Decoder variable importance', ax=axes[1])\n        if uses_static_covariates:\n            axes[1].set_xlabel('')\n            self._plot_cov_selection(stc_imp, title='Static variable importance', ax=axes[2])\n        fig.tight_layout()\n        plt.show()\n        if idx + 1 == max_nr_series:\n            break",
            "def plot_variable_selection(self, expl_result: TFTExplainabilityResult, fig_size=None, max_nr_series: int=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Plots the variable selection / feature importances of the `TFTModel` based on the input.\\n        The figure includes three subplots:\\n\\n        - encoder importances: contains the past target, past covariates, and historic future covariates importance\\n          on the encoder (input chunk)\\n        - decoder importances: contains the future covariates importance on the decoder (output chunk)\\n        - static covariates importances: contains the numeric and / or categorical static covariates importance\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        fig_size\\n            The size of the figure to be plotted.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        '\n    encoder_importance = expl_result.get_encoder_importance()\n    decoder_importance = expl_result.get_decoder_importance()\n    static_covariates_importance = expl_result.get_static_covariates_importance()\n    if not isinstance(encoder_importance, list):\n        encoder_importance = [encoder_importance]\n        decoder_importance = [decoder_importance]\n        static_covariates_importance = [static_covariates_importance]\n    uses_static_covariates = not static_covariates_importance[0].empty\n    for (idx, (enc_imp, dec_imp, stc_imp)) in enumerate(zip(encoder_importance, decoder_importance, static_covariates_importance)):\n        (fig, axes) = plt.subplots(nrows=3 if uses_static_covariates else 2, sharex=True, figsize=fig_size)\n        self._plot_cov_selection(enc_imp, title='Encoder variable importance', ax=axes[0])\n        axes[0].set_xlabel('')\n        self._plot_cov_selection(dec_imp, title='Decoder variable importance', ax=axes[1])\n        if uses_static_covariates:\n            axes[1].set_xlabel('')\n            self._plot_cov_selection(stc_imp, title='Static variable importance', ax=axes[2])\n        fig.tight_layout()\n        plt.show()\n        if idx + 1 == max_nr_series:\n            break",
            "def plot_variable_selection(self, expl_result: TFTExplainabilityResult, fig_size=None, max_nr_series: int=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Plots the variable selection / feature importances of the `TFTModel` based on the input.\\n        The figure includes three subplots:\\n\\n        - encoder importances: contains the past target, past covariates, and historic future covariates importance\\n          on the encoder (input chunk)\\n        - decoder importances: contains the future covariates importance on the decoder (output chunk)\\n        - static covariates importances: contains the numeric and / or categorical static covariates importance\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        fig_size\\n            The size of the figure to be plotted.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        '\n    encoder_importance = expl_result.get_encoder_importance()\n    decoder_importance = expl_result.get_decoder_importance()\n    static_covariates_importance = expl_result.get_static_covariates_importance()\n    if not isinstance(encoder_importance, list):\n        encoder_importance = [encoder_importance]\n        decoder_importance = [decoder_importance]\n        static_covariates_importance = [static_covariates_importance]\n    uses_static_covariates = not static_covariates_importance[0].empty\n    for (idx, (enc_imp, dec_imp, stc_imp)) in enumerate(zip(encoder_importance, decoder_importance, static_covariates_importance)):\n        (fig, axes) = plt.subplots(nrows=3 if uses_static_covariates else 2, sharex=True, figsize=fig_size)\n        self._plot_cov_selection(enc_imp, title='Encoder variable importance', ax=axes[0])\n        axes[0].set_xlabel('')\n        self._plot_cov_selection(dec_imp, title='Decoder variable importance', ax=axes[1])\n        if uses_static_covariates:\n            axes[1].set_xlabel('')\n            self._plot_cov_selection(stc_imp, title='Static variable importance', ax=axes[2])\n        fig.tight_layout()\n        plt.show()\n        if idx + 1 == max_nr_series:\n            break",
            "def plot_variable_selection(self, expl_result: TFTExplainabilityResult, fig_size=None, max_nr_series: int=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Plots the variable selection / feature importances of the `TFTModel` based on the input.\\n        The figure includes three subplots:\\n\\n        - encoder importances: contains the past target, past covariates, and historic future covariates importance\\n          on the encoder (input chunk)\\n        - decoder importances: contains the future covariates importance on the decoder (output chunk)\\n        - static covariates importances: contains the numeric and / or categorical static covariates importance\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        fig_size\\n            The size of the figure to be plotted.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        '\n    encoder_importance = expl_result.get_encoder_importance()\n    decoder_importance = expl_result.get_decoder_importance()\n    static_covariates_importance = expl_result.get_static_covariates_importance()\n    if not isinstance(encoder_importance, list):\n        encoder_importance = [encoder_importance]\n        decoder_importance = [decoder_importance]\n        static_covariates_importance = [static_covariates_importance]\n    uses_static_covariates = not static_covariates_importance[0].empty\n    for (idx, (enc_imp, dec_imp, stc_imp)) in enumerate(zip(encoder_importance, decoder_importance, static_covariates_importance)):\n        (fig, axes) = plt.subplots(nrows=3 if uses_static_covariates else 2, sharex=True, figsize=fig_size)\n        self._plot_cov_selection(enc_imp, title='Encoder variable importance', ax=axes[0])\n        axes[0].set_xlabel('')\n        self._plot_cov_selection(dec_imp, title='Decoder variable importance', ax=axes[1])\n        if uses_static_covariates:\n            axes[1].set_xlabel('')\n            self._plot_cov_selection(stc_imp, title='Static variable importance', ax=axes[2])\n        fig.tight_layout()\n        plt.show()\n        if idx + 1 == max_nr_series:\n            break",
            "def plot_variable_selection(self, expl_result: TFTExplainabilityResult, fig_size=None, max_nr_series: int=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Plots the variable selection / feature importances of the `TFTModel` based on the input.\\n        The figure includes three subplots:\\n\\n        - encoder importances: contains the past target, past covariates, and historic future covariates importance\\n          on the encoder (input chunk)\\n        - decoder importances: contains the future covariates importance on the decoder (output chunk)\\n        - static covariates importances: contains the numeric and / or categorical static covariates importance\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        fig_size\\n            The size of the figure to be plotted.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        '\n    encoder_importance = expl_result.get_encoder_importance()\n    decoder_importance = expl_result.get_decoder_importance()\n    static_covariates_importance = expl_result.get_static_covariates_importance()\n    if not isinstance(encoder_importance, list):\n        encoder_importance = [encoder_importance]\n        decoder_importance = [decoder_importance]\n        static_covariates_importance = [static_covariates_importance]\n    uses_static_covariates = not static_covariates_importance[0].empty\n    for (idx, (enc_imp, dec_imp, stc_imp)) in enumerate(zip(encoder_importance, decoder_importance, static_covariates_importance)):\n        (fig, axes) = plt.subplots(nrows=3 if uses_static_covariates else 2, sharex=True, figsize=fig_size)\n        self._plot_cov_selection(enc_imp, title='Encoder variable importance', ax=axes[0])\n        axes[0].set_xlabel('')\n        self._plot_cov_selection(dec_imp, title='Decoder variable importance', ax=axes[1])\n        if uses_static_covariates:\n            axes[1].set_xlabel('')\n            self._plot_cov_selection(stc_imp, title='Static variable importance', ax=axes[2])\n        fig.tight_layout()\n        plt.show()\n        if idx + 1 == max_nr_series:\n            break"
        ]
    },
    {
        "func_name": "plot_attention",
        "original": "def plot_attention(self, expl_result: TFTExplainabilityResult, plot_type: Optional[Literal['all', 'time', 'heatmap']]='all', show_index_as: Literal['relative', 'time']='relative', ax: Optional[matplotlib.axes.Axes]=None, max_nr_series: int=5, show_plot: bool=True) -> matplotlib.axes.Axes:\n    \"\"\"Plots the attention heads of the `TFTModel`.\n\n        Parameters\n        ----------\n        expl_result\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\n        plot_type\n            The type of attention head plot. One of (\"all\", \"time\", \"heatmap\").\n            If \"all\", will plot the attention per horizon (given the horizons in the `TFTExplainabilityResult`).\n            The maximum horizon corresponds to the `output_chunk_length` of the trained `TFTModel`.\n            If \"time\", will plot the mean attention over all horizons.\n            If \"heatmap\", will plot the attention per horizon on a heat map. The horizons are shown on the y-axis,\n            and times / relative indices on the x-axis.\n        show_index_as\n            The type of index to be shown. One of (\"relative\", \"time\").\n            If \"relative\", will plot the x-axis from `(-input_chunk_length, output_chunk_length - 1)`. `0` corresponds\n            to the first prediction point.\n            If \"time\", will plot the x-axis with the actual time index (or range index) of the corresponding\n            `TFTExplainabilityResult`.\n        ax\n            Optionally, an axis to plot on. Only effective on a single `expl_result`.\n        max_nr_series\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\n        show_plot\n            Whether to show the plot.\n        \"\"\"\n    single_series = False\n    attentions = expl_result.get_explanation(component='attention')\n    if isinstance(attentions, TimeSeries):\n        attentions = [attentions]\n        single_series = True\n    for (idx, attention) in enumerate(attentions):\n        if ax is None or not single_series:\n            (fig, ax) = plt.subplots()\n        if show_index_as == 'relative':\n            x_ticks = generate_index(start=-self.model.input_chunk_length, end=self.n - 1)\n            attention = TimeSeries.from_times_and_values(times=generate_index(start=-self.model.input_chunk_length, end=self.n - 1), values=attention.values(copy=False), columns=attention.components)\n            x_label = 'Index relative to first prediction point'\n        elif show_index_as == 'time':\n            x_ticks = attention.time_index\n            x_label = 'Time index'\n        else:\n            (x_label, x_ticks) = (None, None)\n            raise_log(ValueError(\"`show_index_as` must either be 'relative', or 'time'.\"))\n        prediction_start_color = 'red'\n        if plot_type == 'all':\n            ax_title = 'Attention per Horizon'\n            y_label = 'Attention'\n            attention.plot(max_nr_components=-1, ax=ax)\n        elif plot_type == 'time':\n            ax_title = 'Mean Attention'\n            y_label = 'Attention'\n            attention.mean(1).plot(label='Mean Attention Head', ax=ax)\n        elif plot_type == 'heatmap':\n            ax_title = 'Attention Heat Map'\n            y_label = 'Horizon'\n            (x, y) = np.meshgrid(x_ticks, np.arange(1, self.n + 1))\n            c = ax.pcolormesh(x, y, attention.values().transpose(), cmap='hot')\n            ax.axis([x.min(), x.max(), y.max(), y.min()])\n            prediction_start_color = 'lightblue'\n            fig.colorbar(c, ax=ax, orientation='horizontal')\n        else:\n            raise raise_log(ValueError(\"`plot_type` must be either 'all', 'time' or 'heatmap'\"), logger=logger)\n        (y_min, y_max) = ax.get_ylim()\n        ax.vlines(x=x_ticks[-self.n], ymin=y_min, ymax=y_max, label='prediction start', ls='dashed', lw=2, colors=prediction_start_color)\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n        title_suffix = '' if single_series else f': series index {idx}'\n        ax.set_title(ax_title + title_suffix)\n        ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n        if show_plot:\n            plt.show()\n        if idx + 1 == max_nr_series:\n            break\n    return ax",
        "mutated": [
            "def plot_attention(self, expl_result: TFTExplainabilityResult, plot_type: Optional[Literal['all', 'time', 'heatmap']]='all', show_index_as: Literal['relative', 'time']='relative', ax: Optional[matplotlib.axes.Axes]=None, max_nr_series: int=5, show_plot: bool=True) -> matplotlib.axes.Axes:\n    if False:\n        i = 10\n    'Plots the attention heads of the `TFTModel`.\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        plot_type\\n            The type of attention head plot. One of (\"all\", \"time\", \"heatmap\").\\n            If \"all\", will plot the attention per horizon (given the horizons in the `TFTExplainabilityResult`).\\n            The maximum horizon corresponds to the `output_chunk_length` of the trained `TFTModel`.\\n            If \"time\", will plot the mean attention over all horizons.\\n            If \"heatmap\", will plot the attention per horizon on a heat map. The horizons are shown on the y-axis,\\n            and times / relative indices on the x-axis.\\n        show_index_as\\n            The type of index to be shown. One of (\"relative\", \"time\").\\n            If \"relative\", will plot the x-axis from `(-input_chunk_length, output_chunk_length - 1)`. `0` corresponds\\n            to the first prediction point.\\n            If \"time\", will plot the x-axis with the actual time index (or range index) of the corresponding\\n            `TFTExplainabilityResult`.\\n        ax\\n            Optionally, an axis to plot on. Only effective on a single `expl_result`.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        show_plot\\n            Whether to show the plot.\\n        '\n    single_series = False\n    attentions = expl_result.get_explanation(component='attention')\n    if isinstance(attentions, TimeSeries):\n        attentions = [attentions]\n        single_series = True\n    for (idx, attention) in enumerate(attentions):\n        if ax is None or not single_series:\n            (fig, ax) = plt.subplots()\n        if show_index_as == 'relative':\n            x_ticks = generate_index(start=-self.model.input_chunk_length, end=self.n - 1)\n            attention = TimeSeries.from_times_and_values(times=generate_index(start=-self.model.input_chunk_length, end=self.n - 1), values=attention.values(copy=False), columns=attention.components)\n            x_label = 'Index relative to first prediction point'\n        elif show_index_as == 'time':\n            x_ticks = attention.time_index\n            x_label = 'Time index'\n        else:\n            (x_label, x_ticks) = (None, None)\n            raise_log(ValueError(\"`show_index_as` must either be 'relative', or 'time'.\"))\n        prediction_start_color = 'red'\n        if plot_type == 'all':\n            ax_title = 'Attention per Horizon'\n            y_label = 'Attention'\n            attention.plot(max_nr_components=-1, ax=ax)\n        elif plot_type == 'time':\n            ax_title = 'Mean Attention'\n            y_label = 'Attention'\n            attention.mean(1).plot(label='Mean Attention Head', ax=ax)\n        elif plot_type == 'heatmap':\n            ax_title = 'Attention Heat Map'\n            y_label = 'Horizon'\n            (x, y) = np.meshgrid(x_ticks, np.arange(1, self.n + 1))\n            c = ax.pcolormesh(x, y, attention.values().transpose(), cmap='hot')\n            ax.axis([x.min(), x.max(), y.max(), y.min()])\n            prediction_start_color = 'lightblue'\n            fig.colorbar(c, ax=ax, orientation='horizontal')\n        else:\n            raise raise_log(ValueError(\"`plot_type` must be either 'all', 'time' or 'heatmap'\"), logger=logger)\n        (y_min, y_max) = ax.get_ylim()\n        ax.vlines(x=x_ticks[-self.n], ymin=y_min, ymax=y_max, label='prediction start', ls='dashed', lw=2, colors=prediction_start_color)\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n        title_suffix = '' if single_series else f': series index {idx}'\n        ax.set_title(ax_title + title_suffix)\n        ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n        if show_plot:\n            plt.show()\n        if idx + 1 == max_nr_series:\n            break\n    return ax",
            "def plot_attention(self, expl_result: TFTExplainabilityResult, plot_type: Optional[Literal['all', 'time', 'heatmap']]='all', show_index_as: Literal['relative', 'time']='relative', ax: Optional[matplotlib.axes.Axes]=None, max_nr_series: int=5, show_plot: bool=True) -> matplotlib.axes.Axes:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Plots the attention heads of the `TFTModel`.\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        plot_type\\n            The type of attention head plot. One of (\"all\", \"time\", \"heatmap\").\\n            If \"all\", will plot the attention per horizon (given the horizons in the `TFTExplainabilityResult`).\\n            The maximum horizon corresponds to the `output_chunk_length` of the trained `TFTModel`.\\n            If \"time\", will plot the mean attention over all horizons.\\n            If \"heatmap\", will plot the attention per horizon on a heat map. The horizons are shown on the y-axis,\\n            and times / relative indices on the x-axis.\\n        show_index_as\\n            The type of index to be shown. One of (\"relative\", \"time\").\\n            If \"relative\", will plot the x-axis from `(-input_chunk_length, output_chunk_length - 1)`. `0` corresponds\\n            to the first prediction point.\\n            If \"time\", will plot the x-axis with the actual time index (or range index) of the corresponding\\n            `TFTExplainabilityResult`.\\n        ax\\n            Optionally, an axis to plot on. Only effective on a single `expl_result`.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        show_plot\\n            Whether to show the plot.\\n        '\n    single_series = False\n    attentions = expl_result.get_explanation(component='attention')\n    if isinstance(attentions, TimeSeries):\n        attentions = [attentions]\n        single_series = True\n    for (idx, attention) in enumerate(attentions):\n        if ax is None or not single_series:\n            (fig, ax) = plt.subplots()\n        if show_index_as == 'relative':\n            x_ticks = generate_index(start=-self.model.input_chunk_length, end=self.n - 1)\n            attention = TimeSeries.from_times_and_values(times=generate_index(start=-self.model.input_chunk_length, end=self.n - 1), values=attention.values(copy=False), columns=attention.components)\n            x_label = 'Index relative to first prediction point'\n        elif show_index_as == 'time':\n            x_ticks = attention.time_index\n            x_label = 'Time index'\n        else:\n            (x_label, x_ticks) = (None, None)\n            raise_log(ValueError(\"`show_index_as` must either be 'relative', or 'time'.\"))\n        prediction_start_color = 'red'\n        if plot_type == 'all':\n            ax_title = 'Attention per Horizon'\n            y_label = 'Attention'\n            attention.plot(max_nr_components=-1, ax=ax)\n        elif plot_type == 'time':\n            ax_title = 'Mean Attention'\n            y_label = 'Attention'\n            attention.mean(1).plot(label='Mean Attention Head', ax=ax)\n        elif plot_type == 'heatmap':\n            ax_title = 'Attention Heat Map'\n            y_label = 'Horizon'\n            (x, y) = np.meshgrid(x_ticks, np.arange(1, self.n + 1))\n            c = ax.pcolormesh(x, y, attention.values().transpose(), cmap='hot')\n            ax.axis([x.min(), x.max(), y.max(), y.min()])\n            prediction_start_color = 'lightblue'\n            fig.colorbar(c, ax=ax, orientation='horizontal')\n        else:\n            raise raise_log(ValueError(\"`plot_type` must be either 'all', 'time' or 'heatmap'\"), logger=logger)\n        (y_min, y_max) = ax.get_ylim()\n        ax.vlines(x=x_ticks[-self.n], ymin=y_min, ymax=y_max, label='prediction start', ls='dashed', lw=2, colors=prediction_start_color)\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n        title_suffix = '' if single_series else f': series index {idx}'\n        ax.set_title(ax_title + title_suffix)\n        ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n        if show_plot:\n            plt.show()\n        if idx + 1 == max_nr_series:\n            break\n    return ax",
            "def plot_attention(self, expl_result: TFTExplainabilityResult, plot_type: Optional[Literal['all', 'time', 'heatmap']]='all', show_index_as: Literal['relative', 'time']='relative', ax: Optional[matplotlib.axes.Axes]=None, max_nr_series: int=5, show_plot: bool=True) -> matplotlib.axes.Axes:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Plots the attention heads of the `TFTModel`.\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        plot_type\\n            The type of attention head plot. One of (\"all\", \"time\", \"heatmap\").\\n            If \"all\", will plot the attention per horizon (given the horizons in the `TFTExplainabilityResult`).\\n            The maximum horizon corresponds to the `output_chunk_length` of the trained `TFTModel`.\\n            If \"time\", will plot the mean attention over all horizons.\\n            If \"heatmap\", will plot the attention per horizon on a heat map. The horizons are shown on the y-axis,\\n            and times / relative indices on the x-axis.\\n        show_index_as\\n            The type of index to be shown. One of (\"relative\", \"time\").\\n            If \"relative\", will plot the x-axis from `(-input_chunk_length, output_chunk_length - 1)`. `0` corresponds\\n            to the first prediction point.\\n            If \"time\", will plot the x-axis with the actual time index (or range index) of the corresponding\\n            `TFTExplainabilityResult`.\\n        ax\\n            Optionally, an axis to plot on. Only effective on a single `expl_result`.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        show_plot\\n            Whether to show the plot.\\n        '\n    single_series = False\n    attentions = expl_result.get_explanation(component='attention')\n    if isinstance(attentions, TimeSeries):\n        attentions = [attentions]\n        single_series = True\n    for (idx, attention) in enumerate(attentions):\n        if ax is None or not single_series:\n            (fig, ax) = plt.subplots()\n        if show_index_as == 'relative':\n            x_ticks = generate_index(start=-self.model.input_chunk_length, end=self.n - 1)\n            attention = TimeSeries.from_times_and_values(times=generate_index(start=-self.model.input_chunk_length, end=self.n - 1), values=attention.values(copy=False), columns=attention.components)\n            x_label = 'Index relative to first prediction point'\n        elif show_index_as == 'time':\n            x_ticks = attention.time_index\n            x_label = 'Time index'\n        else:\n            (x_label, x_ticks) = (None, None)\n            raise_log(ValueError(\"`show_index_as` must either be 'relative', or 'time'.\"))\n        prediction_start_color = 'red'\n        if plot_type == 'all':\n            ax_title = 'Attention per Horizon'\n            y_label = 'Attention'\n            attention.plot(max_nr_components=-1, ax=ax)\n        elif plot_type == 'time':\n            ax_title = 'Mean Attention'\n            y_label = 'Attention'\n            attention.mean(1).plot(label='Mean Attention Head', ax=ax)\n        elif plot_type == 'heatmap':\n            ax_title = 'Attention Heat Map'\n            y_label = 'Horizon'\n            (x, y) = np.meshgrid(x_ticks, np.arange(1, self.n + 1))\n            c = ax.pcolormesh(x, y, attention.values().transpose(), cmap='hot')\n            ax.axis([x.min(), x.max(), y.max(), y.min()])\n            prediction_start_color = 'lightblue'\n            fig.colorbar(c, ax=ax, orientation='horizontal')\n        else:\n            raise raise_log(ValueError(\"`plot_type` must be either 'all', 'time' or 'heatmap'\"), logger=logger)\n        (y_min, y_max) = ax.get_ylim()\n        ax.vlines(x=x_ticks[-self.n], ymin=y_min, ymax=y_max, label='prediction start', ls='dashed', lw=2, colors=prediction_start_color)\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n        title_suffix = '' if single_series else f': series index {idx}'\n        ax.set_title(ax_title + title_suffix)\n        ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n        if show_plot:\n            plt.show()\n        if idx + 1 == max_nr_series:\n            break\n    return ax",
            "def plot_attention(self, expl_result: TFTExplainabilityResult, plot_type: Optional[Literal['all', 'time', 'heatmap']]='all', show_index_as: Literal['relative', 'time']='relative', ax: Optional[matplotlib.axes.Axes]=None, max_nr_series: int=5, show_plot: bool=True) -> matplotlib.axes.Axes:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Plots the attention heads of the `TFTModel`.\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        plot_type\\n            The type of attention head plot. One of (\"all\", \"time\", \"heatmap\").\\n            If \"all\", will plot the attention per horizon (given the horizons in the `TFTExplainabilityResult`).\\n            The maximum horizon corresponds to the `output_chunk_length` of the trained `TFTModel`.\\n            If \"time\", will plot the mean attention over all horizons.\\n            If \"heatmap\", will plot the attention per horizon on a heat map. The horizons are shown on the y-axis,\\n            and times / relative indices on the x-axis.\\n        show_index_as\\n            The type of index to be shown. One of (\"relative\", \"time\").\\n            If \"relative\", will plot the x-axis from `(-input_chunk_length, output_chunk_length - 1)`. `0` corresponds\\n            to the first prediction point.\\n            If \"time\", will plot the x-axis with the actual time index (or range index) of the corresponding\\n            `TFTExplainabilityResult`.\\n        ax\\n            Optionally, an axis to plot on. Only effective on a single `expl_result`.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        show_plot\\n            Whether to show the plot.\\n        '\n    single_series = False\n    attentions = expl_result.get_explanation(component='attention')\n    if isinstance(attentions, TimeSeries):\n        attentions = [attentions]\n        single_series = True\n    for (idx, attention) in enumerate(attentions):\n        if ax is None or not single_series:\n            (fig, ax) = plt.subplots()\n        if show_index_as == 'relative':\n            x_ticks = generate_index(start=-self.model.input_chunk_length, end=self.n - 1)\n            attention = TimeSeries.from_times_and_values(times=generate_index(start=-self.model.input_chunk_length, end=self.n - 1), values=attention.values(copy=False), columns=attention.components)\n            x_label = 'Index relative to first prediction point'\n        elif show_index_as == 'time':\n            x_ticks = attention.time_index\n            x_label = 'Time index'\n        else:\n            (x_label, x_ticks) = (None, None)\n            raise_log(ValueError(\"`show_index_as` must either be 'relative', or 'time'.\"))\n        prediction_start_color = 'red'\n        if plot_type == 'all':\n            ax_title = 'Attention per Horizon'\n            y_label = 'Attention'\n            attention.plot(max_nr_components=-1, ax=ax)\n        elif plot_type == 'time':\n            ax_title = 'Mean Attention'\n            y_label = 'Attention'\n            attention.mean(1).plot(label='Mean Attention Head', ax=ax)\n        elif plot_type == 'heatmap':\n            ax_title = 'Attention Heat Map'\n            y_label = 'Horizon'\n            (x, y) = np.meshgrid(x_ticks, np.arange(1, self.n + 1))\n            c = ax.pcolormesh(x, y, attention.values().transpose(), cmap='hot')\n            ax.axis([x.min(), x.max(), y.max(), y.min()])\n            prediction_start_color = 'lightblue'\n            fig.colorbar(c, ax=ax, orientation='horizontal')\n        else:\n            raise raise_log(ValueError(\"`plot_type` must be either 'all', 'time' or 'heatmap'\"), logger=logger)\n        (y_min, y_max) = ax.get_ylim()\n        ax.vlines(x=x_ticks[-self.n], ymin=y_min, ymax=y_max, label='prediction start', ls='dashed', lw=2, colors=prediction_start_color)\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n        title_suffix = '' if single_series else f': series index {idx}'\n        ax.set_title(ax_title + title_suffix)\n        ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n        if show_plot:\n            plt.show()\n        if idx + 1 == max_nr_series:\n            break\n    return ax",
            "def plot_attention(self, expl_result: TFTExplainabilityResult, plot_type: Optional[Literal['all', 'time', 'heatmap']]='all', show_index_as: Literal['relative', 'time']='relative', ax: Optional[matplotlib.axes.Axes]=None, max_nr_series: int=5, show_plot: bool=True) -> matplotlib.axes.Axes:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Plots the attention heads of the `TFTModel`.\\n\\n        Parameters\\n        ----------\\n        expl_result\\n            A `TFTExplainabilityResult` object. Corresponds to the output of :func:`explain() <TFTExplainer.explain>`.\\n        plot_type\\n            The type of attention head plot. One of (\"all\", \"time\", \"heatmap\").\\n            If \"all\", will plot the attention per horizon (given the horizons in the `TFTExplainabilityResult`).\\n            The maximum horizon corresponds to the `output_chunk_length` of the trained `TFTModel`.\\n            If \"time\", will plot the mean attention over all horizons.\\n            If \"heatmap\", will plot the attention per horizon on a heat map. The horizons are shown on the y-axis,\\n            and times / relative indices on the x-axis.\\n        show_index_as\\n            The type of index to be shown. One of (\"relative\", \"time\").\\n            If \"relative\", will plot the x-axis from `(-input_chunk_length, output_chunk_length - 1)`. `0` corresponds\\n            to the first prediction point.\\n            If \"time\", will plot the x-axis with the actual time index (or range index) of the corresponding\\n            `TFTExplainabilityResult`.\\n        ax\\n            Optionally, an axis to plot on. Only effective on a single `expl_result`.\\n        max_nr_series\\n            The maximum number of plots to show in case `expl_result` was computed on multiple series.\\n        show_plot\\n            Whether to show the plot.\\n        '\n    single_series = False\n    attentions = expl_result.get_explanation(component='attention')\n    if isinstance(attentions, TimeSeries):\n        attentions = [attentions]\n        single_series = True\n    for (idx, attention) in enumerate(attentions):\n        if ax is None or not single_series:\n            (fig, ax) = plt.subplots()\n        if show_index_as == 'relative':\n            x_ticks = generate_index(start=-self.model.input_chunk_length, end=self.n - 1)\n            attention = TimeSeries.from_times_and_values(times=generate_index(start=-self.model.input_chunk_length, end=self.n - 1), values=attention.values(copy=False), columns=attention.components)\n            x_label = 'Index relative to first prediction point'\n        elif show_index_as == 'time':\n            x_ticks = attention.time_index\n            x_label = 'Time index'\n        else:\n            (x_label, x_ticks) = (None, None)\n            raise_log(ValueError(\"`show_index_as` must either be 'relative', or 'time'.\"))\n        prediction_start_color = 'red'\n        if plot_type == 'all':\n            ax_title = 'Attention per Horizon'\n            y_label = 'Attention'\n            attention.plot(max_nr_components=-1, ax=ax)\n        elif plot_type == 'time':\n            ax_title = 'Mean Attention'\n            y_label = 'Attention'\n            attention.mean(1).plot(label='Mean Attention Head', ax=ax)\n        elif plot_type == 'heatmap':\n            ax_title = 'Attention Heat Map'\n            y_label = 'Horizon'\n            (x, y) = np.meshgrid(x_ticks, np.arange(1, self.n + 1))\n            c = ax.pcolormesh(x, y, attention.values().transpose(), cmap='hot')\n            ax.axis([x.min(), x.max(), y.max(), y.min()])\n            prediction_start_color = 'lightblue'\n            fig.colorbar(c, ax=ax, orientation='horizontal')\n        else:\n            raise raise_log(ValueError(\"`plot_type` must be either 'all', 'time' or 'heatmap'\"), logger=logger)\n        (y_min, y_max) = ax.get_ylim()\n        ax.vlines(x=x_ticks[-self.n], ymin=y_min, ymax=y_max, label='prediction start', ls='dashed', lw=2, colors=prediction_start_color)\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n        title_suffix = '' if single_series else f': series index {idx}'\n        ax.set_title(ax_title + title_suffix)\n        ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n        if show_plot:\n            plt.show()\n        if idx + 1 == max_nr_series:\n            break\n    return ax"
        ]
    },
    {
        "func_name": "_encoder_importance",
        "original": "@property\ndef _encoder_importance(self) -> pd.DataFrame:\n    \"\"\"Returns the encoder variable importance of the TFT model.\n\n        The encoder_weights are calculated for the past inputs of the model.\n        The encoder_importance contains the weights of the encoder variable selection network.\n        The encoder variable selection network is used to select the most important static and time dependent\n        covariates. It provides insights which variable are most significant for the prediction problem.\n        See section 4.2 of the paper for more details.\n\n        Returns\n        -------\n        pd.DataFrame\n            The encoder variable importance.\n        \"\"\"\n    return self._get_importance(weight=self.model.model._encoder_sparse_weights, names=self.model.model.encoder_variables)",
        "mutated": [
            "@property\ndef _encoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n    'Returns the encoder variable importance of the TFT model.\\n\\n        The encoder_weights are calculated for the past inputs of the model.\\n        The encoder_importance contains the weights of the encoder variable selection network.\\n        The encoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The encoder variable importance.\\n        '\n    return self._get_importance(weight=self.model.model._encoder_sparse_weights, names=self.model.model.encoder_variables)",
            "@property\ndef _encoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the encoder variable importance of the TFT model.\\n\\n        The encoder_weights are calculated for the past inputs of the model.\\n        The encoder_importance contains the weights of the encoder variable selection network.\\n        The encoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The encoder variable importance.\\n        '\n    return self._get_importance(weight=self.model.model._encoder_sparse_weights, names=self.model.model.encoder_variables)",
            "@property\ndef _encoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the encoder variable importance of the TFT model.\\n\\n        The encoder_weights are calculated for the past inputs of the model.\\n        The encoder_importance contains the weights of the encoder variable selection network.\\n        The encoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The encoder variable importance.\\n        '\n    return self._get_importance(weight=self.model.model._encoder_sparse_weights, names=self.model.model.encoder_variables)",
            "@property\ndef _encoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the encoder variable importance of the TFT model.\\n\\n        The encoder_weights are calculated for the past inputs of the model.\\n        The encoder_importance contains the weights of the encoder variable selection network.\\n        The encoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The encoder variable importance.\\n        '\n    return self._get_importance(weight=self.model.model._encoder_sparse_weights, names=self.model.model.encoder_variables)",
            "@property\ndef _encoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the encoder variable importance of the TFT model.\\n\\n        The encoder_weights are calculated for the past inputs of the model.\\n        The encoder_importance contains the weights of the encoder variable selection network.\\n        The encoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The encoder variable importance.\\n        '\n    return self._get_importance(weight=self.model.model._encoder_sparse_weights, names=self.model.model.encoder_variables)"
        ]
    },
    {
        "func_name": "_decoder_importance",
        "original": "@property\ndef _decoder_importance(self) -> pd.DataFrame:\n    \"\"\"Returns the decoder variable importance of the TFT model.\n\n        The decoder_weights are calculated for the known future inputs of the model.\n        The decoder_importance contains the weights of the decoder variable selection network.\n        The decoder variable selection network is used to select the most important static and time dependent\n        covariates. It provides insights which variable are most significant for the prediction problem.\n        See section 4.2 of the paper for more details.\n\n        Returns\n        -------\n        pd.DataFrame\n            The importance of the decoder variables.\n        \"\"\"\n    return self._get_importance(weight=self.model.model._decoder_sparse_weights, names=self.model.model.decoder_variables)",
        "mutated": [
            "@property\ndef _decoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n    'Returns the decoder variable importance of the TFT model.\\n\\n        The decoder_weights are calculated for the known future inputs of the model.\\n        The decoder_importance contains the weights of the decoder variable selection network.\\n        The decoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the decoder variables.\\n        '\n    return self._get_importance(weight=self.model.model._decoder_sparse_weights, names=self.model.model.decoder_variables)",
            "@property\ndef _decoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the decoder variable importance of the TFT model.\\n\\n        The decoder_weights are calculated for the known future inputs of the model.\\n        The decoder_importance contains the weights of the decoder variable selection network.\\n        The decoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the decoder variables.\\n        '\n    return self._get_importance(weight=self.model.model._decoder_sparse_weights, names=self.model.model.decoder_variables)",
            "@property\ndef _decoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the decoder variable importance of the TFT model.\\n\\n        The decoder_weights are calculated for the known future inputs of the model.\\n        The decoder_importance contains the weights of the decoder variable selection network.\\n        The decoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the decoder variables.\\n        '\n    return self._get_importance(weight=self.model.model._decoder_sparse_weights, names=self.model.model.decoder_variables)",
            "@property\ndef _decoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the decoder variable importance of the TFT model.\\n\\n        The decoder_weights are calculated for the known future inputs of the model.\\n        The decoder_importance contains the weights of the decoder variable selection network.\\n        The decoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the decoder variables.\\n        '\n    return self._get_importance(weight=self.model.model._decoder_sparse_weights, names=self.model.model.decoder_variables)",
            "@property\ndef _decoder_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the decoder variable importance of the TFT model.\\n\\n        The decoder_weights are calculated for the known future inputs of the model.\\n        The decoder_importance contains the weights of the decoder variable selection network.\\n        The decoder variable selection network is used to select the most important static and time dependent\\n        covariates. It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the decoder variables.\\n        '\n    return self._get_importance(weight=self.model.model._decoder_sparse_weights, names=self.model.model.decoder_variables)"
        ]
    },
    {
        "func_name": "_static_covariates_importance",
        "original": "@property\ndef _static_covariates_importance(self) -> pd.DataFrame:\n    \"\"\"Returns the static covariates importance of the TFT model.\n\n        The static covariate importances are calculated for the static inputs of the model (numeric and / or\n        categorical). The static variable selection network is used to select the most important static covariates.\n        It provides insights which variable are most significant for the prediction problem.\n        See section 4.2, and 4.3 of the paper for more details.\n\n        Returns\n        -------\n        pd.DataFrame\n            The static covariates importance.\n        \"\"\"\n    return self._get_importance(weight=self.model.model._static_covariate_var, names=self.model.model.static_variables)",
        "mutated": [
            "@property\ndef _static_covariates_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n    'Returns the static covariates importance of the TFT model.\\n\\n        The static covariate importances are calculated for the static inputs of the model (numeric and / or\\n        categorical). The static variable selection network is used to select the most important static covariates.\\n        It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2, and 4.3 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The static covariates importance.\\n        '\n    return self._get_importance(weight=self.model.model._static_covariate_var, names=self.model.model.static_variables)",
            "@property\ndef _static_covariates_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the static covariates importance of the TFT model.\\n\\n        The static covariate importances are calculated for the static inputs of the model (numeric and / or\\n        categorical). The static variable selection network is used to select the most important static covariates.\\n        It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2, and 4.3 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The static covariates importance.\\n        '\n    return self._get_importance(weight=self.model.model._static_covariate_var, names=self.model.model.static_variables)",
            "@property\ndef _static_covariates_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the static covariates importance of the TFT model.\\n\\n        The static covariate importances are calculated for the static inputs of the model (numeric and / or\\n        categorical). The static variable selection network is used to select the most important static covariates.\\n        It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2, and 4.3 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The static covariates importance.\\n        '\n    return self._get_importance(weight=self.model.model._static_covariate_var, names=self.model.model.static_variables)",
            "@property\ndef _static_covariates_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the static covariates importance of the TFT model.\\n\\n        The static covariate importances are calculated for the static inputs of the model (numeric and / or\\n        categorical). The static variable selection network is used to select the most important static covariates.\\n        It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2, and 4.3 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The static covariates importance.\\n        '\n    return self._get_importance(weight=self.model.model._static_covariate_var, names=self.model.model.static_variables)",
            "@property\ndef _static_covariates_importance(self) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the static covariates importance of the TFT model.\\n\\n        The static covariate importances are calculated for the static inputs of the model (numeric and / or\\n        categorical). The static variable selection network is used to select the most important static covariates.\\n        It provides insights which variable are most significant for the prediction problem.\\n        See section 4.2, and 4.3 of the paper for more details.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The static covariates importance.\\n        '\n    return self._get_importance(weight=self.model.model._static_covariate_var, names=self.model.model.static_variables)"
        ]
    },
    {
        "func_name": "_get_importance",
        "original": "def _get_importance(self, weight: Tensor, names: List[str], n_decimals=3) -> pd.DataFrame:\n    \"\"\"Returns the encoder or decoder variable of the TFT model.\n\n        Parameters\n        ----------\n        weights\n            The weights of the encoder or decoder of the trained TFT model.\n        names\n            The encoder or decoder names saved in the TFT model class.\n        n_decimals\n            The number of decimals to round the importance to.\n\n        Returns\n        -------\n        pd.DataFrame\n            The importance of the variables.\n        \"\"\"\n    if weight is None:\n        return pd.DataFrame()\n    if weight.ndim == 3:\n        weight = weight.unsqueeze(1)\n    weights_percentage = weight.detach().cpu().numpy().mean(axis=1).squeeze(axis=1).round(n_decimals) * 100\n    name_mapping = self._name_mapping\n    importance = pd.DataFrame(weights_percentage, columns=[name_mapping[name] for name in names])\n    return importance.transpose().sort_values(0, ascending=True).transpose()",
        "mutated": [
            "def _get_importance(self, weight: Tensor, names: List[str], n_decimals=3) -> pd.DataFrame:\n    if False:\n        i = 10\n    'Returns the encoder or decoder variable of the TFT model.\\n\\n        Parameters\\n        ----------\\n        weights\\n            The weights of the encoder or decoder of the trained TFT model.\\n        names\\n            The encoder or decoder names saved in the TFT model class.\\n        n_decimals\\n            The number of decimals to round the importance to.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the variables.\\n        '\n    if weight is None:\n        return pd.DataFrame()\n    if weight.ndim == 3:\n        weight = weight.unsqueeze(1)\n    weights_percentage = weight.detach().cpu().numpy().mean(axis=1).squeeze(axis=1).round(n_decimals) * 100\n    name_mapping = self._name_mapping\n    importance = pd.DataFrame(weights_percentage, columns=[name_mapping[name] for name in names])\n    return importance.transpose().sort_values(0, ascending=True).transpose()",
            "def _get_importance(self, weight: Tensor, names: List[str], n_decimals=3) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the encoder or decoder variable of the TFT model.\\n\\n        Parameters\\n        ----------\\n        weights\\n            The weights of the encoder or decoder of the trained TFT model.\\n        names\\n            The encoder or decoder names saved in the TFT model class.\\n        n_decimals\\n            The number of decimals to round the importance to.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the variables.\\n        '\n    if weight is None:\n        return pd.DataFrame()\n    if weight.ndim == 3:\n        weight = weight.unsqueeze(1)\n    weights_percentage = weight.detach().cpu().numpy().mean(axis=1).squeeze(axis=1).round(n_decimals) * 100\n    name_mapping = self._name_mapping\n    importance = pd.DataFrame(weights_percentage, columns=[name_mapping[name] for name in names])\n    return importance.transpose().sort_values(0, ascending=True).transpose()",
            "def _get_importance(self, weight: Tensor, names: List[str], n_decimals=3) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the encoder or decoder variable of the TFT model.\\n\\n        Parameters\\n        ----------\\n        weights\\n            The weights of the encoder or decoder of the trained TFT model.\\n        names\\n            The encoder or decoder names saved in the TFT model class.\\n        n_decimals\\n            The number of decimals to round the importance to.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the variables.\\n        '\n    if weight is None:\n        return pd.DataFrame()\n    if weight.ndim == 3:\n        weight = weight.unsqueeze(1)\n    weights_percentage = weight.detach().cpu().numpy().mean(axis=1).squeeze(axis=1).round(n_decimals) * 100\n    name_mapping = self._name_mapping\n    importance = pd.DataFrame(weights_percentage, columns=[name_mapping[name] for name in names])\n    return importance.transpose().sort_values(0, ascending=True).transpose()",
            "def _get_importance(self, weight: Tensor, names: List[str], n_decimals=3) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the encoder or decoder variable of the TFT model.\\n\\n        Parameters\\n        ----------\\n        weights\\n            The weights of the encoder or decoder of the trained TFT model.\\n        names\\n            The encoder or decoder names saved in the TFT model class.\\n        n_decimals\\n            The number of decimals to round the importance to.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the variables.\\n        '\n    if weight is None:\n        return pd.DataFrame()\n    if weight.ndim == 3:\n        weight = weight.unsqueeze(1)\n    weights_percentage = weight.detach().cpu().numpy().mean(axis=1).squeeze(axis=1).round(n_decimals) * 100\n    name_mapping = self._name_mapping\n    importance = pd.DataFrame(weights_percentage, columns=[name_mapping[name] for name in names])\n    return importance.transpose().sort_values(0, ascending=True).transpose()",
            "def _get_importance(self, weight: Tensor, names: List[str], n_decimals=3) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the encoder or decoder variable of the TFT model.\\n\\n        Parameters\\n        ----------\\n        weights\\n            The weights of the encoder or decoder of the trained TFT model.\\n        names\\n            The encoder or decoder names saved in the TFT model class.\\n        n_decimals\\n            The number of decimals to round the importance to.\\n\\n        Returns\\n        -------\\n        pd.DataFrame\\n            The importance of the variables.\\n        '\n    if weight is None:\n        return pd.DataFrame()\n    if weight.ndim == 3:\n        weight = weight.unsqueeze(1)\n    weights_percentage = weight.detach().cpu().numpy().mean(axis=1).squeeze(axis=1).round(n_decimals) * 100\n    name_mapping = self._name_mapping\n    importance = pd.DataFrame(weights_percentage, columns=[name_mapping[name] for name in names])\n    return importance.transpose().sort_values(0, ascending=True).transpose()"
        ]
    },
    {
        "func_name": "map_cols",
        "original": "def map_cols(comps, name, suffix):\n    comps = comps if comps is not None else []\n    return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}",
        "mutated": [
            "def map_cols(comps, name, suffix):\n    if False:\n        i = 10\n    comps = comps if comps is not None else []\n    return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}",
            "def map_cols(comps, name, suffix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    comps = comps if comps is not None else []\n    return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}",
            "def map_cols(comps, name, suffix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    comps = comps if comps is not None else []\n    return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}",
            "def map_cols(comps, name, suffix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    comps = comps if comps is not None else []\n    return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}",
            "def map_cols(comps, name, suffix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    comps = comps if comps is not None else []\n    return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}"
        ]
    },
    {
        "func_name": "_name_mapping",
        "original": "@property\ndef _name_mapping(self) -> Dict[str, str]:\n    \"\"\"Returns the feature name mapping of the TFT model.\n\n        Returns\n        -------\n        Dict[str, str]\n            The feature name mapping. For example\n            {\n                'target_0': 'ice cream',\n                'past_covariate_0': 'heater',\n                'past_covariate_1': 'year',\n                'past_covariate_2': 'month',\n                'future_covariate_0': 'darts_enc_fc_cyc_month_sin',\n                'future_covariate_1': 'darts_enc_fc_cyc_month_cos',\n             }\n        \"\"\"\n\n    def map_cols(comps, name, suffix):\n        comps = comps if comps is not None else []\n        return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}\n    return {**map_cols(self.target_components, 'target', 'target'), **map_cols(self.static_covariates_components, 'static_covariate', 'statcov'), **map_cols(self.past_covariates_components, 'past_covariate', 'pastcov'), **map_cols(self.future_covariates_components, 'future_covariate', 'futcov')}",
        "mutated": [
            "@property\ndef _name_mapping(self) -> Dict[str, str]:\n    if False:\n        i = 10\n    \"Returns the feature name mapping of the TFT model.\\n\\n        Returns\\n        -------\\n        Dict[str, str]\\n            The feature name mapping. For example\\n            {\\n                'target_0': 'ice cream',\\n                'past_covariate_0': 'heater',\\n                'past_covariate_1': 'year',\\n                'past_covariate_2': 'month',\\n                'future_covariate_0': 'darts_enc_fc_cyc_month_sin',\\n                'future_covariate_1': 'darts_enc_fc_cyc_month_cos',\\n             }\\n        \"\n\n    def map_cols(comps, name, suffix):\n        comps = comps if comps is not None else []\n        return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}\n    return {**map_cols(self.target_components, 'target', 'target'), **map_cols(self.static_covariates_components, 'static_covariate', 'statcov'), **map_cols(self.past_covariates_components, 'past_covariate', 'pastcov'), **map_cols(self.future_covariates_components, 'future_covariate', 'futcov')}",
            "@property\ndef _name_mapping(self) -> Dict[str, str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Returns the feature name mapping of the TFT model.\\n\\n        Returns\\n        -------\\n        Dict[str, str]\\n            The feature name mapping. For example\\n            {\\n                'target_0': 'ice cream',\\n                'past_covariate_0': 'heater',\\n                'past_covariate_1': 'year',\\n                'past_covariate_2': 'month',\\n                'future_covariate_0': 'darts_enc_fc_cyc_month_sin',\\n                'future_covariate_1': 'darts_enc_fc_cyc_month_cos',\\n             }\\n        \"\n\n    def map_cols(comps, name, suffix):\n        comps = comps if comps is not None else []\n        return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}\n    return {**map_cols(self.target_components, 'target', 'target'), **map_cols(self.static_covariates_components, 'static_covariate', 'statcov'), **map_cols(self.past_covariates_components, 'past_covariate', 'pastcov'), **map_cols(self.future_covariates_components, 'future_covariate', 'futcov')}",
            "@property\ndef _name_mapping(self) -> Dict[str, str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Returns the feature name mapping of the TFT model.\\n\\n        Returns\\n        -------\\n        Dict[str, str]\\n            The feature name mapping. For example\\n            {\\n                'target_0': 'ice cream',\\n                'past_covariate_0': 'heater',\\n                'past_covariate_1': 'year',\\n                'past_covariate_2': 'month',\\n                'future_covariate_0': 'darts_enc_fc_cyc_month_sin',\\n                'future_covariate_1': 'darts_enc_fc_cyc_month_cos',\\n             }\\n        \"\n\n    def map_cols(comps, name, suffix):\n        comps = comps if comps is not None else []\n        return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}\n    return {**map_cols(self.target_components, 'target', 'target'), **map_cols(self.static_covariates_components, 'static_covariate', 'statcov'), **map_cols(self.past_covariates_components, 'past_covariate', 'pastcov'), **map_cols(self.future_covariates_components, 'future_covariate', 'futcov')}",
            "@property\ndef _name_mapping(self) -> Dict[str, str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Returns the feature name mapping of the TFT model.\\n\\n        Returns\\n        -------\\n        Dict[str, str]\\n            The feature name mapping. For example\\n            {\\n                'target_0': 'ice cream',\\n                'past_covariate_0': 'heater',\\n                'past_covariate_1': 'year',\\n                'past_covariate_2': 'month',\\n                'future_covariate_0': 'darts_enc_fc_cyc_month_sin',\\n                'future_covariate_1': 'darts_enc_fc_cyc_month_cos',\\n             }\\n        \"\n\n    def map_cols(comps, name, suffix):\n        comps = comps if comps is not None else []\n        return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}\n    return {**map_cols(self.target_components, 'target', 'target'), **map_cols(self.static_covariates_components, 'static_covariate', 'statcov'), **map_cols(self.past_covariates_components, 'past_covariate', 'pastcov'), **map_cols(self.future_covariates_components, 'future_covariate', 'futcov')}",
            "@property\ndef _name_mapping(self) -> Dict[str, str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Returns the feature name mapping of the TFT model.\\n\\n        Returns\\n        -------\\n        Dict[str, str]\\n            The feature name mapping. For example\\n            {\\n                'target_0': 'ice cream',\\n                'past_covariate_0': 'heater',\\n                'past_covariate_1': 'year',\\n                'past_covariate_2': 'month',\\n                'future_covariate_0': 'darts_enc_fc_cyc_month_sin',\\n                'future_covariate_1': 'darts_enc_fc_cyc_month_cos',\\n             }\\n        \"\n\n    def map_cols(comps, name, suffix):\n        comps = comps if comps is not None else []\n        return {f'{name}_{i}': colname + f'_{suffix}' for (i, colname) in enumerate(comps)}\n    return {**map_cols(self.target_components, 'target', 'target'), **map_cols(self.static_covariates_components, 'static_covariate', 'statcov'), **map_cols(self.past_covariates_components, 'past_covariate', 'pastcov'), **map_cols(self.future_covariates_components, 'future_covariate', 'futcov')}"
        ]
    },
    {
        "func_name": "_plot_cov_selection",
        "original": "@staticmethod\ndef _plot_cov_selection(importance: pd.DataFrame, title: str='Variable importance', ax: Optional[matplotlib.axes.Axes]=None):\n    \"\"\"Plots the variable importance of the TFT model.\n\n        Parameters\n        ----------\n        importance\n            The encoder / decoder importance.\n        title\n            The title of the plot.\n        ax\n            Optionally, an axis to plot on. Otherwise, will create and plot on a new axis.\n        \"\"\"\n    if ax is None:\n        (_, ax) = plt.subplots()\n    ax.barh(importance.columns.tolist(), importance.values[0].tolist())\n    ax.set_title(title)\n    ax.set_ylabel('Variable', fontsize=12)\n    ax.set_xlabel('Variable importance in %')\n    return ax",
        "mutated": [
            "@staticmethod\ndef _plot_cov_selection(importance: pd.DataFrame, title: str='Variable importance', ax: Optional[matplotlib.axes.Axes]=None):\n    if False:\n        i = 10\n    'Plots the variable importance of the TFT model.\\n\\n        Parameters\\n        ----------\\n        importance\\n            The encoder / decoder importance.\\n        title\\n            The title of the plot.\\n        ax\\n            Optionally, an axis to plot on. Otherwise, will create and plot on a new axis.\\n        '\n    if ax is None:\n        (_, ax) = plt.subplots()\n    ax.barh(importance.columns.tolist(), importance.values[0].tolist())\n    ax.set_title(title)\n    ax.set_ylabel('Variable', fontsize=12)\n    ax.set_xlabel('Variable importance in %')\n    return ax",
            "@staticmethod\ndef _plot_cov_selection(importance: pd.DataFrame, title: str='Variable importance', ax: Optional[matplotlib.axes.Axes]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Plots the variable importance of the TFT model.\\n\\n        Parameters\\n        ----------\\n        importance\\n            The encoder / decoder importance.\\n        title\\n            The title of the plot.\\n        ax\\n            Optionally, an axis to plot on. Otherwise, will create and plot on a new axis.\\n        '\n    if ax is None:\n        (_, ax) = plt.subplots()\n    ax.barh(importance.columns.tolist(), importance.values[0].tolist())\n    ax.set_title(title)\n    ax.set_ylabel('Variable', fontsize=12)\n    ax.set_xlabel('Variable importance in %')\n    return ax",
            "@staticmethod\ndef _plot_cov_selection(importance: pd.DataFrame, title: str='Variable importance', ax: Optional[matplotlib.axes.Axes]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Plots the variable importance of the TFT model.\\n\\n        Parameters\\n        ----------\\n        importance\\n            The encoder / decoder importance.\\n        title\\n            The title of the plot.\\n        ax\\n            Optionally, an axis to plot on. Otherwise, will create and plot on a new axis.\\n        '\n    if ax is None:\n        (_, ax) = plt.subplots()\n    ax.barh(importance.columns.tolist(), importance.values[0].tolist())\n    ax.set_title(title)\n    ax.set_ylabel('Variable', fontsize=12)\n    ax.set_xlabel('Variable importance in %')\n    return ax",
            "@staticmethod\ndef _plot_cov_selection(importance: pd.DataFrame, title: str='Variable importance', ax: Optional[matplotlib.axes.Axes]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Plots the variable importance of the TFT model.\\n\\n        Parameters\\n        ----------\\n        importance\\n            The encoder / decoder importance.\\n        title\\n            The title of the plot.\\n        ax\\n            Optionally, an axis to plot on. Otherwise, will create and plot on a new axis.\\n        '\n    if ax is None:\n        (_, ax) = plt.subplots()\n    ax.barh(importance.columns.tolist(), importance.values[0].tolist())\n    ax.set_title(title)\n    ax.set_ylabel('Variable', fontsize=12)\n    ax.set_xlabel('Variable importance in %')\n    return ax",
            "@staticmethod\ndef _plot_cov_selection(importance: pd.DataFrame, title: str='Variable importance', ax: Optional[matplotlib.axes.Axes]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Plots the variable importance of the TFT model.\\n\\n        Parameters\\n        ----------\\n        importance\\n            The encoder / decoder importance.\\n        title\\n            The title of the plot.\\n        ax\\n            Optionally, an axis to plot on. Otherwise, will create and plot on a new axis.\\n        '\n    if ax is None:\n        (_, ax) = plt.subplots()\n    ax.barh(importance.columns.tolist(), importance.values[0].tolist())\n    ax.set_title(title)\n    ax.set_ylabel('Variable', fontsize=12)\n    ax.set_xlabel('Variable importance in %')\n    return ax"
        ]
    }
]