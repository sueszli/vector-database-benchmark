[
    {
        "func_name": "tb_cross_entropy",
        "original": "def tb_cross_entropy(logit, label, mask=None):\n    \"\"\"\n    Overview:\n        Compute the cross entropy loss for label and logit, with mask support\n    Arguments:\n        - logit (:obj:`torch.Tensor`): the logit tensor, of size [T, B, N] or [T, B, N, N2]\n        - label (:obj:`torch.Tensor`): the label tensor, of size [T, B] or [T, B, N2]\n        - mask (:obj:`torch.Tensor` or :obj:`None`): the mask tensor, of size [T, B] or [T, B, N2]\n    Returns:\n        - ce (:obj:`torch.Tensor`): the computed cross entropy, of size [T, B]\n    Examples:\n        >>> T, B, N, N2 = 4, 8, 5, 7\n        >>> logit = torch.randn(T, B, N, N2).softmax(-1).requires_grad_(True)\n        >>> action = logit.argmax(-1).detach()\n        >>> ce = tb_cross_entropy(logit, action)\n    \"\"\"\n    assert len(label.shape) >= 2\n    (T, B) = label.shape[:2]\n    if len(label.shape) > 2:\n        assert len(label.shape) == 3\n        (s, n) = logit.shape[-2:]\n        logit = logit.reshape(-1, n)\n        label = label.reshape(-1)\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.view(T * B, -1)\n        if mask is not None:\n            ce *= mask.reshape(-1, s)\n        ce = ce.sum(dim=1)\n        ce = ce.reshape(T, B)\n    else:\n        label = label.reshape(-1)\n        logit = logit.reshape(-1, logit.shape[-1])\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.reshape(T, B, -1)\n        ce = ce.mean(dim=2)\n    return ce",
        "mutated": [
            "def tb_cross_entropy(logit, label, mask=None):\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Compute the cross entropy loss for label and logit, with mask support\\n    Arguments:\\n        - logit (:obj:`torch.Tensor`): the logit tensor, of size [T, B, N] or [T, B, N, N2]\\n        - label (:obj:`torch.Tensor`): the label tensor, of size [T, B] or [T, B, N2]\\n        - mask (:obj:`torch.Tensor` or :obj:`None`): the mask tensor, of size [T, B] or [T, B, N2]\\n    Returns:\\n        - ce (:obj:`torch.Tensor`): the computed cross entropy, of size [T, B]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> logit = torch.randn(T, B, N, N2).softmax(-1).requires_grad_(True)\\n        >>> action = logit.argmax(-1).detach()\\n        >>> ce = tb_cross_entropy(logit, action)\\n    '\n    assert len(label.shape) >= 2\n    (T, B) = label.shape[:2]\n    if len(label.shape) > 2:\n        assert len(label.shape) == 3\n        (s, n) = logit.shape[-2:]\n        logit = logit.reshape(-1, n)\n        label = label.reshape(-1)\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.view(T * B, -1)\n        if mask is not None:\n            ce *= mask.reshape(-1, s)\n        ce = ce.sum(dim=1)\n        ce = ce.reshape(T, B)\n    else:\n        label = label.reshape(-1)\n        logit = logit.reshape(-1, logit.shape[-1])\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.reshape(T, B, -1)\n        ce = ce.mean(dim=2)\n    return ce",
            "def tb_cross_entropy(logit, label, mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Compute the cross entropy loss for label and logit, with mask support\\n    Arguments:\\n        - logit (:obj:`torch.Tensor`): the logit tensor, of size [T, B, N] or [T, B, N, N2]\\n        - label (:obj:`torch.Tensor`): the label tensor, of size [T, B] or [T, B, N2]\\n        - mask (:obj:`torch.Tensor` or :obj:`None`): the mask tensor, of size [T, B] or [T, B, N2]\\n    Returns:\\n        - ce (:obj:`torch.Tensor`): the computed cross entropy, of size [T, B]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> logit = torch.randn(T, B, N, N2).softmax(-1).requires_grad_(True)\\n        >>> action = logit.argmax(-1).detach()\\n        >>> ce = tb_cross_entropy(logit, action)\\n    '\n    assert len(label.shape) >= 2\n    (T, B) = label.shape[:2]\n    if len(label.shape) > 2:\n        assert len(label.shape) == 3\n        (s, n) = logit.shape[-2:]\n        logit = logit.reshape(-1, n)\n        label = label.reshape(-1)\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.view(T * B, -1)\n        if mask is not None:\n            ce *= mask.reshape(-1, s)\n        ce = ce.sum(dim=1)\n        ce = ce.reshape(T, B)\n    else:\n        label = label.reshape(-1)\n        logit = logit.reshape(-1, logit.shape[-1])\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.reshape(T, B, -1)\n        ce = ce.mean(dim=2)\n    return ce",
            "def tb_cross_entropy(logit, label, mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Compute the cross entropy loss for label and logit, with mask support\\n    Arguments:\\n        - logit (:obj:`torch.Tensor`): the logit tensor, of size [T, B, N] or [T, B, N, N2]\\n        - label (:obj:`torch.Tensor`): the label tensor, of size [T, B] or [T, B, N2]\\n        - mask (:obj:`torch.Tensor` or :obj:`None`): the mask tensor, of size [T, B] or [T, B, N2]\\n    Returns:\\n        - ce (:obj:`torch.Tensor`): the computed cross entropy, of size [T, B]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> logit = torch.randn(T, B, N, N2).softmax(-1).requires_grad_(True)\\n        >>> action = logit.argmax(-1).detach()\\n        >>> ce = tb_cross_entropy(logit, action)\\n    '\n    assert len(label.shape) >= 2\n    (T, B) = label.shape[:2]\n    if len(label.shape) > 2:\n        assert len(label.shape) == 3\n        (s, n) = logit.shape[-2:]\n        logit = logit.reshape(-1, n)\n        label = label.reshape(-1)\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.view(T * B, -1)\n        if mask is not None:\n            ce *= mask.reshape(-1, s)\n        ce = ce.sum(dim=1)\n        ce = ce.reshape(T, B)\n    else:\n        label = label.reshape(-1)\n        logit = logit.reshape(-1, logit.shape[-1])\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.reshape(T, B, -1)\n        ce = ce.mean(dim=2)\n    return ce",
            "def tb_cross_entropy(logit, label, mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Compute the cross entropy loss for label and logit, with mask support\\n    Arguments:\\n        - logit (:obj:`torch.Tensor`): the logit tensor, of size [T, B, N] or [T, B, N, N2]\\n        - label (:obj:`torch.Tensor`): the label tensor, of size [T, B] or [T, B, N2]\\n        - mask (:obj:`torch.Tensor` or :obj:`None`): the mask tensor, of size [T, B] or [T, B, N2]\\n    Returns:\\n        - ce (:obj:`torch.Tensor`): the computed cross entropy, of size [T, B]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> logit = torch.randn(T, B, N, N2).softmax(-1).requires_grad_(True)\\n        >>> action = logit.argmax(-1).detach()\\n        >>> ce = tb_cross_entropy(logit, action)\\n    '\n    assert len(label.shape) >= 2\n    (T, B) = label.shape[:2]\n    if len(label.shape) > 2:\n        assert len(label.shape) == 3\n        (s, n) = logit.shape[-2:]\n        logit = logit.reshape(-1, n)\n        label = label.reshape(-1)\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.view(T * B, -1)\n        if mask is not None:\n            ce *= mask.reshape(-1, s)\n        ce = ce.sum(dim=1)\n        ce = ce.reshape(T, B)\n    else:\n        label = label.reshape(-1)\n        logit = logit.reshape(-1, logit.shape[-1])\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.reshape(T, B, -1)\n        ce = ce.mean(dim=2)\n    return ce",
            "def tb_cross_entropy(logit, label, mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Compute the cross entropy loss for label and logit, with mask support\\n    Arguments:\\n        - logit (:obj:`torch.Tensor`): the logit tensor, of size [T, B, N] or [T, B, N, N2]\\n        - label (:obj:`torch.Tensor`): the label tensor, of size [T, B] or [T, B, N2]\\n        - mask (:obj:`torch.Tensor` or :obj:`None`): the mask tensor, of size [T, B] or [T, B, N2]\\n    Returns:\\n        - ce (:obj:`torch.Tensor`): the computed cross entropy, of size [T, B]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> logit = torch.randn(T, B, N, N2).softmax(-1).requires_grad_(True)\\n        >>> action = logit.argmax(-1).detach()\\n        >>> ce = tb_cross_entropy(logit, action)\\n    '\n    assert len(label.shape) >= 2\n    (T, B) = label.shape[:2]\n    if len(label.shape) > 2:\n        assert len(label.shape) == 3\n        (s, n) = logit.shape[-2:]\n        logit = logit.reshape(-1, n)\n        label = label.reshape(-1)\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.view(T * B, -1)\n        if mask is not None:\n            ce *= mask.reshape(-1, s)\n        ce = ce.sum(dim=1)\n        ce = ce.reshape(T, B)\n    else:\n        label = label.reshape(-1)\n        logit = logit.reshape(-1, logit.shape[-1])\n        ce = -F.cross_entropy(logit, label, reduction='none')\n        ce = ce.reshape(T, B, -1)\n        ce = ce.mean(dim=2)\n    return ce"
        ]
    },
    {
        "func_name": "upgo_returns",
        "original": "def upgo_returns(rewards: torch.Tensor, bootstrap_values: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Computing UPGO return targets. Also notice there is no special handling for the terminal state.\n    Arguments:\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, \\\\\n            of size [T_traj, batchsize]\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\n            of size [T_traj+1, batchsize]\n    Returns:\n        - ret (:obj:`torch.Tensor`): Computed lambda return value for each state from 0 to T-1, \\\\\n            of size [T_traj, batchsize]\n    Examples:\n        >>> T, B, N, N2 = 4, 8, 5, 7\n        >>> rewards = torch.randn(T, B)\n        >>> bootstrap_values = torch.randn(T + 1, B).requires_grad_(True)\n        >>> returns = upgo_returns(rewards, bootstrap_values)\n    \"\"\"\n    lambdas = rewards + bootstrap_values[1:] >= bootstrap_values[:-1]\n    lambdas = torch.cat([lambdas[1:], torch.ones_like(lambdas[-1:])], dim=0)\n    return generalized_lambda_returns(bootstrap_values, rewards, 1.0, lambdas)",
        "mutated": [
            "def upgo_returns(rewards: torch.Tensor, bootstrap_values: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Computing UPGO return targets. Also notice there is no special handling for the terminal state.\\n    Arguments:\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value for each state from 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rewards = torch.randn(T, B)\\n        >>> bootstrap_values = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> returns = upgo_returns(rewards, bootstrap_values)\\n    '\n    lambdas = rewards + bootstrap_values[1:] >= bootstrap_values[:-1]\n    lambdas = torch.cat([lambdas[1:], torch.ones_like(lambdas[-1:])], dim=0)\n    return generalized_lambda_returns(bootstrap_values, rewards, 1.0, lambdas)",
            "def upgo_returns(rewards: torch.Tensor, bootstrap_values: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Computing UPGO return targets. Also notice there is no special handling for the terminal state.\\n    Arguments:\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value for each state from 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rewards = torch.randn(T, B)\\n        >>> bootstrap_values = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> returns = upgo_returns(rewards, bootstrap_values)\\n    '\n    lambdas = rewards + bootstrap_values[1:] >= bootstrap_values[:-1]\n    lambdas = torch.cat([lambdas[1:], torch.ones_like(lambdas[-1:])], dim=0)\n    return generalized_lambda_returns(bootstrap_values, rewards, 1.0, lambdas)",
            "def upgo_returns(rewards: torch.Tensor, bootstrap_values: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Computing UPGO return targets. Also notice there is no special handling for the terminal state.\\n    Arguments:\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value for each state from 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rewards = torch.randn(T, B)\\n        >>> bootstrap_values = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> returns = upgo_returns(rewards, bootstrap_values)\\n    '\n    lambdas = rewards + bootstrap_values[1:] >= bootstrap_values[:-1]\n    lambdas = torch.cat([lambdas[1:], torch.ones_like(lambdas[-1:])], dim=0)\n    return generalized_lambda_returns(bootstrap_values, rewards, 1.0, lambdas)",
            "def upgo_returns(rewards: torch.Tensor, bootstrap_values: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Computing UPGO return targets. Also notice there is no special handling for the terminal state.\\n    Arguments:\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value for each state from 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rewards = torch.randn(T, B)\\n        >>> bootstrap_values = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> returns = upgo_returns(rewards, bootstrap_values)\\n    '\n    lambdas = rewards + bootstrap_values[1:] >= bootstrap_values[:-1]\n    lambdas = torch.cat([lambdas[1:], torch.ones_like(lambdas[-1:])], dim=0)\n    return generalized_lambda_returns(bootstrap_values, rewards, 1.0, lambdas)",
            "def upgo_returns(rewards: torch.Tensor, bootstrap_values: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Computing UPGO return targets. Also notice there is no special handling for the terminal state.\\n    Arguments:\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - ret (:obj:`torch.Tensor`): Computed lambda return value for each state from 0 to T-1, \\\\\\n            of size [T_traj, batchsize]\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rewards = torch.randn(T, B)\\n        >>> bootstrap_values = torch.randn(T + 1, B).requires_grad_(True)\\n        >>> returns = upgo_returns(rewards, bootstrap_values)\\n    '\n    lambdas = rewards + bootstrap_values[1:] >= bootstrap_values[:-1]\n    lambdas = torch.cat([lambdas[1:], torch.ones_like(lambdas[-1:])], dim=0)\n    return generalized_lambda_returns(bootstrap_values, rewards, 1.0, lambdas)"
        ]
    },
    {
        "func_name": "upgo_loss",
        "original": "@hpc_wrapper(shape_fn=lambda args: args[0].shape, namedtuple_data=True, include_args=5, include_kwargs=['target_output', 'rhos', 'action', 'rewards', 'bootstrap_values'])\ndef upgo_loss(target_output: torch.Tensor, rhos: torch.Tensor, action: torch.Tensor, rewards: torch.Tensor, bootstrap_values: torch.Tensor, mask=None) -> torch.Tensor:\n    \"\"\"\n    Overview:\n        Computing UPGO loss given constant gamma and lambda. There is no special handling for terminal state value,\n        if the last state in trajectory is the terminal, just pass a 0 as bootstrap_terminal_value.\n    Arguments:\n        - target_output (:obj:`torch.Tensor`): the output computed by the target policy network, \\\\\n            of size [T_traj, batchsize, n_output]\n        - rhos (:obj:`torch.Tensor`): the importance sampling ratio, of size [T_traj, batchsize]\n        - action (:obj:`torch.Tensor`): the action taken, of size [T_traj, batchsize]\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, of size [T_traj, batchsize]\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\n            of size [T_traj+1, batchsize]\n    Returns:\n        - loss (:obj:`torch.Tensor`): Computed importance sampled UPGO loss, averaged over the samples, of size []\n    Examples:\n        >>> T, B, N, N2 = 4, 8, 5, 7\n        >>> rhos = torch.randn(T, B)\n        >>> loss = upgo_loss(logit, rhos, action, rewards, bootstrap_values)\n    \"\"\"\n    with torch.no_grad():\n        returns = upgo_returns(rewards, bootstrap_values)\n        advantages = rhos * (returns - bootstrap_values[:-1])\n    metric = tb_cross_entropy(target_output, action, mask)\n    assert metric.shape == action.shape[:2]\n    losses = advantages * metric\n    return -losses.mean()",
        "mutated": [
            "@hpc_wrapper(shape_fn=lambda args: args[0].shape, namedtuple_data=True, include_args=5, include_kwargs=['target_output', 'rhos', 'action', 'rewards', 'bootstrap_values'])\ndef upgo_loss(target_output: torch.Tensor, rhos: torch.Tensor, action: torch.Tensor, rewards: torch.Tensor, bootstrap_values: torch.Tensor, mask=None) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n    Overview:\\n        Computing UPGO loss given constant gamma and lambda. There is no special handling for terminal state value,\\n        if the last state in trajectory is the terminal, just pass a 0 as bootstrap_terminal_value.\\n    Arguments:\\n        - target_output (:obj:`torch.Tensor`): the output computed by the target policy network, \\\\\\n            of size [T_traj, batchsize, n_output]\\n        - rhos (:obj:`torch.Tensor`): the importance sampling ratio, of size [T_traj, batchsize]\\n        - action (:obj:`torch.Tensor`): the action taken, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed importance sampled UPGO loss, averaged over the samples, of size []\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rhos = torch.randn(T, B)\\n        >>> loss = upgo_loss(logit, rhos, action, rewards, bootstrap_values)\\n    '\n    with torch.no_grad():\n        returns = upgo_returns(rewards, bootstrap_values)\n        advantages = rhos * (returns - bootstrap_values[:-1])\n    metric = tb_cross_entropy(target_output, action, mask)\n    assert metric.shape == action.shape[:2]\n    losses = advantages * metric\n    return -losses.mean()",
            "@hpc_wrapper(shape_fn=lambda args: args[0].shape, namedtuple_data=True, include_args=5, include_kwargs=['target_output', 'rhos', 'action', 'rewards', 'bootstrap_values'])\ndef upgo_loss(target_output: torch.Tensor, rhos: torch.Tensor, action: torch.Tensor, rewards: torch.Tensor, bootstrap_values: torch.Tensor, mask=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Overview:\\n        Computing UPGO loss given constant gamma and lambda. There is no special handling for terminal state value,\\n        if the last state in trajectory is the terminal, just pass a 0 as bootstrap_terminal_value.\\n    Arguments:\\n        - target_output (:obj:`torch.Tensor`): the output computed by the target policy network, \\\\\\n            of size [T_traj, batchsize, n_output]\\n        - rhos (:obj:`torch.Tensor`): the importance sampling ratio, of size [T_traj, batchsize]\\n        - action (:obj:`torch.Tensor`): the action taken, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed importance sampled UPGO loss, averaged over the samples, of size []\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rhos = torch.randn(T, B)\\n        >>> loss = upgo_loss(logit, rhos, action, rewards, bootstrap_values)\\n    '\n    with torch.no_grad():\n        returns = upgo_returns(rewards, bootstrap_values)\n        advantages = rhos * (returns - bootstrap_values[:-1])\n    metric = tb_cross_entropy(target_output, action, mask)\n    assert metric.shape == action.shape[:2]\n    losses = advantages * metric\n    return -losses.mean()",
            "@hpc_wrapper(shape_fn=lambda args: args[0].shape, namedtuple_data=True, include_args=5, include_kwargs=['target_output', 'rhos', 'action', 'rewards', 'bootstrap_values'])\ndef upgo_loss(target_output: torch.Tensor, rhos: torch.Tensor, action: torch.Tensor, rewards: torch.Tensor, bootstrap_values: torch.Tensor, mask=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Overview:\\n        Computing UPGO loss given constant gamma and lambda. There is no special handling for terminal state value,\\n        if the last state in trajectory is the terminal, just pass a 0 as bootstrap_terminal_value.\\n    Arguments:\\n        - target_output (:obj:`torch.Tensor`): the output computed by the target policy network, \\\\\\n            of size [T_traj, batchsize, n_output]\\n        - rhos (:obj:`torch.Tensor`): the importance sampling ratio, of size [T_traj, batchsize]\\n        - action (:obj:`torch.Tensor`): the action taken, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed importance sampled UPGO loss, averaged over the samples, of size []\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rhos = torch.randn(T, B)\\n        >>> loss = upgo_loss(logit, rhos, action, rewards, bootstrap_values)\\n    '\n    with torch.no_grad():\n        returns = upgo_returns(rewards, bootstrap_values)\n        advantages = rhos * (returns - bootstrap_values[:-1])\n    metric = tb_cross_entropy(target_output, action, mask)\n    assert metric.shape == action.shape[:2]\n    losses = advantages * metric\n    return -losses.mean()",
            "@hpc_wrapper(shape_fn=lambda args: args[0].shape, namedtuple_data=True, include_args=5, include_kwargs=['target_output', 'rhos', 'action', 'rewards', 'bootstrap_values'])\ndef upgo_loss(target_output: torch.Tensor, rhos: torch.Tensor, action: torch.Tensor, rewards: torch.Tensor, bootstrap_values: torch.Tensor, mask=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Overview:\\n        Computing UPGO loss given constant gamma and lambda. There is no special handling for terminal state value,\\n        if the last state in trajectory is the terminal, just pass a 0 as bootstrap_terminal_value.\\n    Arguments:\\n        - target_output (:obj:`torch.Tensor`): the output computed by the target policy network, \\\\\\n            of size [T_traj, batchsize, n_output]\\n        - rhos (:obj:`torch.Tensor`): the importance sampling ratio, of size [T_traj, batchsize]\\n        - action (:obj:`torch.Tensor`): the action taken, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed importance sampled UPGO loss, averaged over the samples, of size []\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rhos = torch.randn(T, B)\\n        >>> loss = upgo_loss(logit, rhos, action, rewards, bootstrap_values)\\n    '\n    with torch.no_grad():\n        returns = upgo_returns(rewards, bootstrap_values)\n        advantages = rhos * (returns - bootstrap_values[:-1])\n    metric = tb_cross_entropy(target_output, action, mask)\n    assert metric.shape == action.shape[:2]\n    losses = advantages * metric\n    return -losses.mean()",
            "@hpc_wrapper(shape_fn=lambda args: args[0].shape, namedtuple_data=True, include_args=5, include_kwargs=['target_output', 'rhos', 'action', 'rewards', 'bootstrap_values'])\ndef upgo_loss(target_output: torch.Tensor, rhos: torch.Tensor, action: torch.Tensor, rewards: torch.Tensor, bootstrap_values: torch.Tensor, mask=None) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Overview:\\n        Computing UPGO loss given constant gamma and lambda. There is no special handling for terminal state value,\\n        if the last state in trajectory is the terminal, just pass a 0 as bootstrap_terminal_value.\\n    Arguments:\\n        - target_output (:obj:`torch.Tensor`): the output computed by the target policy network, \\\\\\n            of size [T_traj, batchsize, n_output]\\n        - rhos (:obj:`torch.Tensor`): the importance sampling ratio, of size [T_traj, batchsize]\\n        - action (:obj:`torch.Tensor`): the action taken, of size [T_traj, batchsize]\\n        - rewards (:obj:`torch.Tensor`): the returns from time step 0 to T-1, of size [T_traj, batchsize]\\n        - bootstrap_values (:obj:`torch.Tensor`): estimation of the state value at step 0 to T, \\\\\\n            of size [T_traj+1, batchsize]\\n    Returns:\\n        - loss (:obj:`torch.Tensor`): Computed importance sampled UPGO loss, averaged over the samples, of size []\\n    Examples:\\n        >>> T, B, N, N2 = 4, 8, 5, 7\\n        >>> rhos = torch.randn(T, B)\\n        >>> loss = upgo_loss(logit, rhos, action, rewards, bootstrap_values)\\n    '\n    with torch.no_grad():\n        returns = upgo_returns(rewards, bootstrap_values)\n        advantages = rhos * (returns - bootstrap_values[:-1])\n    metric = tb_cross_entropy(target_output, action, mask)\n    assert metric.shape == action.shape[:2]\n    losses = advantages * metric\n    return -losses.mean()"
        ]
    }
]