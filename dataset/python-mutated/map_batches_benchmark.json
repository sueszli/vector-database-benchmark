[
    {
        "func_name": "map_batches",
        "original": "def map_batches(input_ds: Dataset, batch_size: int, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy'], compute: Optional[Union[str, ComputeStrategy]]=None, num_calls: Optional[int]=1, is_eager_executed: Optional[bool]=False) -> Dataset:\n    assert isinstance(input_ds, MaterializedDataset)\n    ds = input_ds\n    for _ in range(num_calls):\n        ds = ds.map_batches(lambda x: x, batch_format=batch_format, batch_size=batch_size, compute=compute)\n        if is_eager_executed:\n            ds = ds.materialize()\n    return ds",
        "mutated": [
            "def map_batches(input_ds: Dataset, batch_size: int, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy'], compute: Optional[Union[str, ComputeStrategy]]=None, num_calls: Optional[int]=1, is_eager_executed: Optional[bool]=False) -> Dataset:\n    if False:\n        i = 10\n    assert isinstance(input_ds, MaterializedDataset)\n    ds = input_ds\n    for _ in range(num_calls):\n        ds = ds.map_batches(lambda x: x, batch_format=batch_format, batch_size=batch_size, compute=compute)\n        if is_eager_executed:\n            ds = ds.materialize()\n    return ds",
            "def map_batches(input_ds: Dataset, batch_size: int, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy'], compute: Optional[Union[str, ComputeStrategy]]=None, num_calls: Optional[int]=1, is_eager_executed: Optional[bool]=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert isinstance(input_ds, MaterializedDataset)\n    ds = input_ds\n    for _ in range(num_calls):\n        ds = ds.map_batches(lambda x: x, batch_format=batch_format, batch_size=batch_size, compute=compute)\n        if is_eager_executed:\n            ds = ds.materialize()\n    return ds",
            "def map_batches(input_ds: Dataset, batch_size: int, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy'], compute: Optional[Union[str, ComputeStrategy]]=None, num_calls: Optional[int]=1, is_eager_executed: Optional[bool]=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert isinstance(input_ds, MaterializedDataset)\n    ds = input_ds\n    for _ in range(num_calls):\n        ds = ds.map_batches(lambda x: x, batch_format=batch_format, batch_size=batch_size, compute=compute)\n        if is_eager_executed:\n            ds = ds.materialize()\n    return ds",
            "def map_batches(input_ds: Dataset, batch_size: int, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy'], compute: Optional[Union[str, ComputeStrategy]]=None, num_calls: Optional[int]=1, is_eager_executed: Optional[bool]=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert isinstance(input_ds, MaterializedDataset)\n    ds = input_ds\n    for _ in range(num_calls):\n        ds = ds.map_batches(lambda x: x, batch_format=batch_format, batch_size=batch_size, compute=compute)\n        if is_eager_executed:\n            ds = ds.materialize()\n    return ds",
            "def map_batches(input_ds: Dataset, batch_size: int, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy'], compute: Optional[Union[str, ComputeStrategy]]=None, num_calls: Optional[int]=1, is_eager_executed: Optional[bool]=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert isinstance(input_ds, MaterializedDataset)\n    ds = input_ds\n    for _ in range(num_calls):\n        ds = ds.map_batches(lambda x: x, batch_format=batch_format, batch_size=batch_size, compute=compute)\n        if is_eager_executed:\n            ds = ds.materialize()\n    return ds"
        ]
    },
    {
        "func_name": "map_batches_fn",
        "original": "def map_batches_fn(num_output_blocks, batch):\n    \"\"\"A map_batches function that generates num_output_blocks output blocks.\"\"\"\n    per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n    for _ in range(num_output_blocks):\n        yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}",
        "mutated": [
            "def map_batches_fn(num_output_blocks, batch):\n    if False:\n        i = 10\n    'A map_batches function that generates num_output_blocks output blocks.'\n    per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n    for _ in range(num_output_blocks):\n        yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}",
            "def map_batches_fn(num_output_blocks, batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'A map_batches function that generates num_output_blocks output blocks.'\n    per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n    for _ in range(num_output_blocks):\n        yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}",
            "def map_batches_fn(num_output_blocks, batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'A map_batches function that generates num_output_blocks output blocks.'\n    per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n    for _ in range(num_output_blocks):\n        yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}",
            "def map_batches_fn(num_output_blocks, batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'A map_batches function that generates num_output_blocks output blocks.'\n    per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n    for _ in range(num_output_blocks):\n        yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}",
            "def map_batches_fn(num_output_blocks, batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'A map_batches function that generates num_output_blocks output blocks.'\n    per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n    for _ in range(num_output_blocks):\n        yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}"
        ]
    },
    {
        "func_name": "test_fn",
        "original": "def test_fn():\n    nonlocal return_ds\n    ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n    ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n    ds = ds.materialize()\n    return_ds = ds\n    return ds",
        "mutated": [
            "def test_fn():\n    if False:\n        i = 10\n    nonlocal return_ds\n    ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n    ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n    ds = ds.materialize()\n    return_ds = ds\n    return ds",
            "def test_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nonlocal return_ds\n    ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n    ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n    ds = ds.materialize()\n    return_ds = ds\n    return ds",
            "def test_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nonlocal return_ds\n    ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n    ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n    ds = ds.materialize()\n    return_ds = ds\n    return ds",
            "def test_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nonlocal return_ds\n    ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n    ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n    ds = ds.materialize()\n    return_ds = ds\n    return ds",
            "def test_fn():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nonlocal return_ds\n    ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n    ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n    ds = ds.materialize()\n    return_ds = ds\n    return ds"
        ]
    },
    {
        "func_name": "run_map_batches_benchmark",
        "original": "def run_map_batches_benchmark(benchmark: Benchmark):\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018/01').materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [1024, 2048, 4096, None]\n    num_calls_list = [1, 2, 4]\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            num_calls = 2\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls)\n    for num_calls in num_calls_list:\n        for compute in [None, ActorPoolStrategy(size=1)]:\n            batch_size = 4096\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls)\n    for current_format in ['pyarrow', 'pandas']:\n        new_input_ds = input_ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'map-batches-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, map_batches, input_ds=new_input_ds, batch_format=new_format, batch_size=batch_size, num_calls=1)\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018').materialize()\n    for batch_format in batch_formats:\n        for compute in [None, ActorPoolStrategy(min_size=1, max_size=float('inf'))]:\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{compute_strategy}-multi-files'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=4096, compute=compute, num_calls=1)\n    num_output_blocks = [10, 50, 100]\n    input_size = 1024 * 1024\n    batch_size = 1024\n    ray.data.DataContext.get_current().target_max_block_size = 10 * 1024 * 1024\n    ray.data._internal.logical.optimizers.PHYSICAL_OPTIMIZER_RULES = []\n    parallelism = input_size // batch_size\n    input_ds = ray.data.range(input_size, parallelism=parallelism).materialize()\n\n    def map_batches_fn(num_output_blocks, batch):\n        \"\"\"A map_batches function that generates num_output_blocks output blocks.\"\"\"\n        per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n        for _ in range(num_output_blocks):\n            yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}\n    for num_blocks in num_output_blocks:\n        test_name = f'map-batches-multiple-output-blocks-{num_blocks}'\n        return_ds = None\n\n        def test_fn():\n            nonlocal return_ds\n            ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n            ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n            ds = ds.materialize()\n            return_ds = ds\n            return ds\n        benchmark.run_materialize_ds(test_name, test_fn)\n        total_size = sum((sum(batch['data_size']) for batch in return_ds.iter_batches(batch_size=batch_size)))\n        expected_total_size = input_size * num_blocks * ray.data.DataContext.get_current().target_max_block_size // batch_size\n        assert total_size == expected_total_size, (total_size, expected_total_size)",
        "mutated": [
            "def run_map_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018/01').materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [1024, 2048, 4096, None]\n    num_calls_list = [1, 2, 4]\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            num_calls = 2\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls)\n    for num_calls in num_calls_list:\n        for compute in [None, ActorPoolStrategy(size=1)]:\n            batch_size = 4096\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls)\n    for current_format in ['pyarrow', 'pandas']:\n        new_input_ds = input_ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'map-batches-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, map_batches, input_ds=new_input_ds, batch_format=new_format, batch_size=batch_size, num_calls=1)\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018').materialize()\n    for batch_format in batch_formats:\n        for compute in [None, ActorPoolStrategy(min_size=1, max_size=float('inf'))]:\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{compute_strategy}-multi-files'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=4096, compute=compute, num_calls=1)\n    num_output_blocks = [10, 50, 100]\n    input_size = 1024 * 1024\n    batch_size = 1024\n    ray.data.DataContext.get_current().target_max_block_size = 10 * 1024 * 1024\n    ray.data._internal.logical.optimizers.PHYSICAL_OPTIMIZER_RULES = []\n    parallelism = input_size // batch_size\n    input_ds = ray.data.range(input_size, parallelism=parallelism).materialize()\n\n    def map_batches_fn(num_output_blocks, batch):\n        \"\"\"A map_batches function that generates num_output_blocks output blocks.\"\"\"\n        per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n        for _ in range(num_output_blocks):\n            yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}\n    for num_blocks in num_output_blocks:\n        test_name = f'map-batches-multiple-output-blocks-{num_blocks}'\n        return_ds = None\n\n        def test_fn():\n            nonlocal return_ds\n            ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n            ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n            ds = ds.materialize()\n            return_ds = ds\n            return ds\n        benchmark.run_materialize_ds(test_name, test_fn)\n        total_size = sum((sum(batch['data_size']) for batch in return_ds.iter_batches(batch_size=batch_size)))\n        expected_total_size = input_size * num_blocks * ray.data.DataContext.get_current().target_max_block_size // batch_size\n        assert total_size == expected_total_size, (total_size, expected_total_size)",
            "def run_map_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018/01').materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [1024, 2048, 4096, None]\n    num_calls_list = [1, 2, 4]\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            num_calls = 2\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls)\n    for num_calls in num_calls_list:\n        for compute in [None, ActorPoolStrategy(size=1)]:\n            batch_size = 4096\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls)\n    for current_format in ['pyarrow', 'pandas']:\n        new_input_ds = input_ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'map-batches-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, map_batches, input_ds=new_input_ds, batch_format=new_format, batch_size=batch_size, num_calls=1)\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018').materialize()\n    for batch_format in batch_formats:\n        for compute in [None, ActorPoolStrategy(min_size=1, max_size=float('inf'))]:\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{compute_strategy}-multi-files'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=4096, compute=compute, num_calls=1)\n    num_output_blocks = [10, 50, 100]\n    input_size = 1024 * 1024\n    batch_size = 1024\n    ray.data.DataContext.get_current().target_max_block_size = 10 * 1024 * 1024\n    ray.data._internal.logical.optimizers.PHYSICAL_OPTIMIZER_RULES = []\n    parallelism = input_size // batch_size\n    input_ds = ray.data.range(input_size, parallelism=parallelism).materialize()\n\n    def map_batches_fn(num_output_blocks, batch):\n        \"\"\"A map_batches function that generates num_output_blocks output blocks.\"\"\"\n        per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n        for _ in range(num_output_blocks):\n            yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}\n    for num_blocks in num_output_blocks:\n        test_name = f'map-batches-multiple-output-blocks-{num_blocks}'\n        return_ds = None\n\n        def test_fn():\n            nonlocal return_ds\n            ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n            ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n            ds = ds.materialize()\n            return_ds = ds\n            return ds\n        benchmark.run_materialize_ds(test_name, test_fn)\n        total_size = sum((sum(batch['data_size']) for batch in return_ds.iter_batches(batch_size=batch_size)))\n        expected_total_size = input_size * num_blocks * ray.data.DataContext.get_current().target_max_block_size // batch_size\n        assert total_size == expected_total_size, (total_size, expected_total_size)",
            "def run_map_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018/01').materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [1024, 2048, 4096, None]\n    num_calls_list = [1, 2, 4]\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            num_calls = 2\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls)\n    for num_calls in num_calls_list:\n        for compute in [None, ActorPoolStrategy(size=1)]:\n            batch_size = 4096\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls)\n    for current_format in ['pyarrow', 'pandas']:\n        new_input_ds = input_ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'map-batches-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, map_batches, input_ds=new_input_ds, batch_format=new_format, batch_size=batch_size, num_calls=1)\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018').materialize()\n    for batch_format in batch_formats:\n        for compute in [None, ActorPoolStrategy(min_size=1, max_size=float('inf'))]:\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{compute_strategy}-multi-files'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=4096, compute=compute, num_calls=1)\n    num_output_blocks = [10, 50, 100]\n    input_size = 1024 * 1024\n    batch_size = 1024\n    ray.data.DataContext.get_current().target_max_block_size = 10 * 1024 * 1024\n    ray.data._internal.logical.optimizers.PHYSICAL_OPTIMIZER_RULES = []\n    parallelism = input_size // batch_size\n    input_ds = ray.data.range(input_size, parallelism=parallelism).materialize()\n\n    def map_batches_fn(num_output_blocks, batch):\n        \"\"\"A map_batches function that generates num_output_blocks output blocks.\"\"\"\n        per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n        for _ in range(num_output_blocks):\n            yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}\n    for num_blocks in num_output_blocks:\n        test_name = f'map-batches-multiple-output-blocks-{num_blocks}'\n        return_ds = None\n\n        def test_fn():\n            nonlocal return_ds\n            ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n            ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n            ds = ds.materialize()\n            return_ds = ds\n            return ds\n        benchmark.run_materialize_ds(test_name, test_fn)\n        total_size = sum((sum(batch['data_size']) for batch in return_ds.iter_batches(batch_size=batch_size)))\n        expected_total_size = input_size * num_blocks * ray.data.DataContext.get_current().target_max_block_size // batch_size\n        assert total_size == expected_total_size, (total_size, expected_total_size)",
            "def run_map_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018/01').materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [1024, 2048, 4096, None]\n    num_calls_list = [1, 2, 4]\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            num_calls = 2\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls)\n    for num_calls in num_calls_list:\n        for compute in [None, ActorPoolStrategy(size=1)]:\n            batch_size = 4096\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls)\n    for current_format in ['pyarrow', 'pandas']:\n        new_input_ds = input_ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'map-batches-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, map_batches, input_ds=new_input_ds, batch_format=new_format, batch_size=batch_size, num_calls=1)\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018').materialize()\n    for batch_format in batch_formats:\n        for compute in [None, ActorPoolStrategy(min_size=1, max_size=float('inf'))]:\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{compute_strategy}-multi-files'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=4096, compute=compute, num_calls=1)\n    num_output_blocks = [10, 50, 100]\n    input_size = 1024 * 1024\n    batch_size = 1024\n    ray.data.DataContext.get_current().target_max_block_size = 10 * 1024 * 1024\n    ray.data._internal.logical.optimizers.PHYSICAL_OPTIMIZER_RULES = []\n    parallelism = input_size // batch_size\n    input_ds = ray.data.range(input_size, parallelism=parallelism).materialize()\n\n    def map_batches_fn(num_output_blocks, batch):\n        \"\"\"A map_batches function that generates num_output_blocks output blocks.\"\"\"\n        per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n        for _ in range(num_output_blocks):\n            yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}\n    for num_blocks in num_output_blocks:\n        test_name = f'map-batches-multiple-output-blocks-{num_blocks}'\n        return_ds = None\n\n        def test_fn():\n            nonlocal return_ds\n            ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n            ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n            ds = ds.materialize()\n            return_ds = ds\n            return ds\n        benchmark.run_materialize_ds(test_name, test_fn)\n        total_size = sum((sum(batch['data_size']) for batch in return_ds.iter_batches(batch_size=batch_size)))\n        expected_total_size = input_size * num_blocks * ray.data.DataContext.get_current().target_max_block_size // batch_size\n        assert total_size == expected_total_size, (total_size, expected_total_size)",
            "def run_map_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018/01').materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [1024, 2048, 4096, None]\n    num_calls_list = [1, 2, 4]\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            num_calls = 2\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, num_calls=num_calls)\n    for num_calls in num_calls_list:\n        for compute in [None, ActorPoolStrategy(size=1)]:\n            batch_size = 4096\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-eager'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls, is_eager_executed=True)\n            test_name = f'map-batches-{batch_format}-{batch_size}-{num_calls}-{compute_strategy}-lazy'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=batch_size, compute=compute, num_calls=num_calls)\n    for current_format in ['pyarrow', 'pandas']:\n        new_input_ds = input_ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'map-batches-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, map_batches, input_ds=new_input_ds, batch_format=new_format, batch_size=batch_size, num_calls=1)\n    input_ds = ray.data.read_parquet('s3://air-example-data/ursa-labs-taxi-data/by_year/2018').materialize()\n    for batch_format in batch_formats:\n        for compute in [None, ActorPoolStrategy(min_size=1, max_size=float('inf'))]:\n            if compute is None:\n                compute_strategy = 'tasks'\n            else:\n                compute_strategy = 'actors'\n            test_name = f'map-batches-{batch_format}-{compute_strategy}-multi-files'\n            benchmark.run_materialize_ds(test_name, map_batches, input_ds=input_ds, batch_format=batch_format, batch_size=4096, compute=compute, num_calls=1)\n    num_output_blocks = [10, 50, 100]\n    input_size = 1024 * 1024\n    batch_size = 1024\n    ray.data.DataContext.get_current().target_max_block_size = 10 * 1024 * 1024\n    ray.data._internal.logical.optimizers.PHYSICAL_OPTIMIZER_RULES = []\n    parallelism = input_size // batch_size\n    input_ds = ray.data.range(input_size, parallelism=parallelism).materialize()\n\n    def map_batches_fn(num_output_blocks, batch):\n        \"\"\"A map_batches function that generates num_output_blocks output blocks.\"\"\"\n        per_row_output_size = ray.data.DataContext.get_current().target_max_block_size // len(batch['id'])\n        for _ in range(num_output_blocks):\n            yield {'data': [np.zeros(shape=(per_row_output_size,), dtype=np.int8) for _ in range(len(batch['id']))]}\n    for num_blocks in num_output_blocks:\n        test_name = f'map-batches-multiple-output-blocks-{num_blocks}'\n        return_ds = None\n\n        def test_fn():\n            nonlocal return_ds\n            ds = input_ds.map_batches(lambda batch: map_batches_fn(num_blocks, batch), batch_size=batch_size, batch_format='numpy')\n            ds = ds.map_batches(lambda batch: {'data_size': [sum((x.nbytes for x in batch['data']))]}, batch_size=batch_size, batch_format='numpy')\n            ds = ds.materialize()\n            return_ds = ds\n            return ds\n        benchmark.run_materialize_ds(test_name, test_fn)\n        total_size = sum((sum(batch['data_size']) for batch in return_ds.iter_batches(batch_size=batch_size)))\n        expected_total_size = input_size * num_blocks * ray.data.DataContext.get_current().target_max_block_size // batch_size\n        assert total_size == expected_total_size, (total_size, expected_total_size)"
        ]
    }
]