"""
This is an example dag for managing twitter data.
"""
from __future__ import annotations
import os
from datetime import date, datetime, timedelta
from airflow import DAG
from airflow.decorators import task
from airflow.operators.bash import BashOperator
from airflow.providers.apache.hive.operators.hive import HiveOperator
ENV_ID = os.environ.get('SYSTEM_TESTS_ENV_ID')
DAG_ID = 'example_twitter_dag'

@task
def fetch_tweets():
    if False:
        for i in range(10):
            print('nop')
    '\n    This task should call Twitter API and retrieve tweets from yesterday from and to for the four twitter\n    users (Twitter_A,..,Twitter_D) There should be eight csv output files generated by this task and naming\n    convention is direction(from or to)_twitterHandle_date.csv\n    '

@task
def clean_tweets():
    if False:
        for i in range(10):
            print('nop')
    '\n    This is a placeholder to clean the eight files. In this step you can get rid of or cherry pick columns\n    and different parts of the text.\n    '

@task
def analyze_tweets():
    if False:
        return 10
    '\n    This is a placeholder to analyze the twitter data. Could simply be a sentiment analysis through algorithms\n    like bag of words or something more complicated. You can also take a look at Web Services to do such\n    tasks.\n    '

@task
def transfer_to_db():
    if False:
        print('Hello World!')
    '\n    This is a placeholder to extract summary from Hive data and store it to MySQL.\n    '
with DAG(dag_id=DAG_ID, default_args={'owner': 'Ekhtiar', 'retries': 1}, schedule='@daily', start_date=datetime(2021, 1, 1), tags=['example'], catchup=False) as dag:
    fetch = fetch_tweets()
    clean = clean_tweets()
    analyze = analyze_tweets()
    hive_to_mysql = transfer_to_db()
    fetch >> clean >> analyze
    from_channels = ['fromTwitter_A', 'fromTwitter_B', 'fromTwitter_C', 'fromTwitter_D']
    to_channels = ['toTwitter_A', 'toTwitter_B', 'toTwitter_C', 'toTwitter_D']
    yesterday = date.today() - timedelta(days=1)
    dt = yesterday.strftime('%Y-%m-%d')
    local_dir = '/tmp/'
    hdfs_dir = ' /tmp/'
    for channel in to_channels:
        file_name = f'to_{channel}_{dt}.csv'
        load_to_hdfs = BashOperator(task_id=f'put_{channel}_to_hdfs', bash_command=f'HADOOP_USER_NAME=hdfs hadoop fs -put -f {local_dir}{file_name}{hdfs_dir}{channel}/')
        load_to_hive = HiveOperator(task_id=f'load_{channel}_to_hive', hql=f"LOAD DATA INPATH '{hdfs_dir}{channel}/{file_name}'INTO TABLE {channel}PARTITION(dt='{dt}')")
        analyze >> load_to_hdfs >> load_to_hive >> hive_to_mysql
    for channel in from_channels:
        file_name = f'from_{channel}_{dt}.csv'
        load_to_hdfs = BashOperator(task_id=f'put_{channel}_to_hdfs', bash_command=f'HADOOP_USER_NAME=hdfs hadoop fs -put -f {local_dir}{file_name}{hdfs_dir}{channel}/')
        load_to_hive = HiveOperator(task_id=f'load_{channel}_to_hive', hql=f"LOAD DATA INPATH '{hdfs_dir}{channel}/{file_name}' INTO TABLE {channel} PARTITION(dt='{dt}')")
        analyze >> load_to_hdfs >> load_to_hive >> hive_to_mysql
    from tests.system.utils.watcher import watcher
    list(dag.tasks) >> watcher()
from tests.system.utils import get_test_run
test_run = get_test_run(dag)