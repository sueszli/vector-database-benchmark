[
    {
        "func_name": "batch_sparse_to_dense_ref",
        "original": "def batch_sparse_to_dense_ref(L, I, V, S=None):\n    if S is None:\n        ret = np.zeros((batch_size, dense_last_dim))\n    else:\n        ret = np.zeros(S.shape)\n    ret.fill(default_value)\n    batch = 0\n    v_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[batch][I[v_idx]] = V[v_idx]\n            v_idx += 1\n        batch += 1\n    return [ret]",
        "mutated": [
            "def batch_sparse_to_dense_ref(L, I, V, S=None):\n    if False:\n        i = 10\n    if S is None:\n        ret = np.zeros((batch_size, dense_last_dim))\n    else:\n        ret = np.zeros(S.shape)\n    ret.fill(default_value)\n    batch = 0\n    v_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[batch][I[v_idx]] = V[v_idx]\n            v_idx += 1\n        batch += 1\n    return [ret]",
            "def batch_sparse_to_dense_ref(L, I, V, S=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if S is None:\n        ret = np.zeros((batch_size, dense_last_dim))\n    else:\n        ret = np.zeros(S.shape)\n    ret.fill(default_value)\n    batch = 0\n    v_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[batch][I[v_idx]] = V[v_idx]\n            v_idx += 1\n        batch += 1\n    return [ret]",
            "def batch_sparse_to_dense_ref(L, I, V, S=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if S is None:\n        ret = np.zeros((batch_size, dense_last_dim))\n    else:\n        ret = np.zeros(S.shape)\n    ret.fill(default_value)\n    batch = 0\n    v_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[batch][I[v_idx]] = V[v_idx]\n            v_idx += 1\n        batch += 1\n    return [ret]",
            "def batch_sparse_to_dense_ref(L, I, V, S=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if S is None:\n        ret = np.zeros((batch_size, dense_last_dim))\n    else:\n        ret = np.zeros(S.shape)\n    ret.fill(default_value)\n    batch = 0\n    v_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[batch][I[v_idx]] = V[v_idx]\n            v_idx += 1\n        batch += 1\n    return [ret]",
            "def batch_sparse_to_dense_ref(L, I, V, S=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if S is None:\n        ret = np.zeros((batch_size, dense_last_dim))\n    else:\n        ret = np.zeros(S.shape)\n    ret.fill(default_value)\n    batch = 0\n    v_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[batch][I[v_idx]] = V[v_idx]\n            v_idx += 1\n        batch += 1\n    return [ret]"
        ]
    },
    {
        "func_name": "test_batch_sparse_to_dense",
        "original": "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), default_value=st.floats(min_value=2.0, max_value=3.0), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_sparse_to_dense(self, batch_size, dense_last_dim, default_value, gc, dc):\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    num_data = L.sum()\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    V = np.random.rand(num_data).astype(np.float32)\n    op = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V'], ['O'], dense_last_dim=dense_last_dim, default_value=default_value)\n    S = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op2 = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V', 'S'], ['O'], default_value=default_value)\n\n    def batch_sparse_to_dense_ref(L, I, V, S=None):\n        if S is None:\n            ret = np.zeros((batch_size, dense_last_dim))\n        else:\n            ret = np.zeros(S.shape)\n        ret.fill(default_value)\n        batch = 0\n        v_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[batch][I[v_idx]] = V[v_idx]\n                v_idx += 1\n            batch += 1\n        return [ret]\n    self.assertDeviceChecks(dc, op, [L, I, V], [0])\n    self.assertReferenceChecks(gc, op, [L, I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L, I, V], 2, [0])\n    self.assertDeviceChecks(dc, op2, [L, I, V, S], [0])\n    self.assertReferenceChecks(gc, op2, [L, I, V, S], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op2, [L, I, V, S], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, V], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, V], 2, [0])",
        "mutated": [
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), default_value=st.floats(min_value=2.0, max_value=3.0), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_sparse_to_dense(self, batch_size, dense_last_dim, default_value, gc, dc):\n    if False:\n        i = 10\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    num_data = L.sum()\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    V = np.random.rand(num_data).astype(np.float32)\n    op = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V'], ['O'], dense_last_dim=dense_last_dim, default_value=default_value)\n    S = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op2 = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V', 'S'], ['O'], default_value=default_value)\n\n    def batch_sparse_to_dense_ref(L, I, V, S=None):\n        if S is None:\n            ret = np.zeros((batch_size, dense_last_dim))\n        else:\n            ret = np.zeros(S.shape)\n        ret.fill(default_value)\n        batch = 0\n        v_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[batch][I[v_idx]] = V[v_idx]\n                v_idx += 1\n            batch += 1\n        return [ret]\n    self.assertDeviceChecks(dc, op, [L, I, V], [0])\n    self.assertReferenceChecks(gc, op, [L, I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L, I, V], 2, [0])\n    self.assertDeviceChecks(dc, op2, [L, I, V, S], [0])\n    self.assertReferenceChecks(gc, op2, [L, I, V, S], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op2, [L, I, V, S], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, V], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, V], 2, [0])",
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), default_value=st.floats(min_value=2.0, max_value=3.0), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_sparse_to_dense(self, batch_size, dense_last_dim, default_value, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    num_data = L.sum()\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    V = np.random.rand(num_data).astype(np.float32)\n    op = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V'], ['O'], dense_last_dim=dense_last_dim, default_value=default_value)\n    S = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op2 = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V', 'S'], ['O'], default_value=default_value)\n\n    def batch_sparse_to_dense_ref(L, I, V, S=None):\n        if S is None:\n            ret = np.zeros((batch_size, dense_last_dim))\n        else:\n            ret = np.zeros(S.shape)\n        ret.fill(default_value)\n        batch = 0\n        v_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[batch][I[v_idx]] = V[v_idx]\n                v_idx += 1\n            batch += 1\n        return [ret]\n    self.assertDeviceChecks(dc, op, [L, I, V], [0])\n    self.assertReferenceChecks(gc, op, [L, I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L, I, V], 2, [0])\n    self.assertDeviceChecks(dc, op2, [L, I, V, S], [0])\n    self.assertReferenceChecks(gc, op2, [L, I, V, S], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op2, [L, I, V, S], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, V], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, V], 2, [0])",
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), default_value=st.floats(min_value=2.0, max_value=3.0), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_sparse_to_dense(self, batch_size, dense_last_dim, default_value, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    num_data = L.sum()\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    V = np.random.rand(num_data).astype(np.float32)\n    op = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V'], ['O'], dense_last_dim=dense_last_dim, default_value=default_value)\n    S = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op2 = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V', 'S'], ['O'], default_value=default_value)\n\n    def batch_sparse_to_dense_ref(L, I, V, S=None):\n        if S is None:\n            ret = np.zeros((batch_size, dense_last_dim))\n        else:\n            ret = np.zeros(S.shape)\n        ret.fill(default_value)\n        batch = 0\n        v_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[batch][I[v_idx]] = V[v_idx]\n                v_idx += 1\n            batch += 1\n        return [ret]\n    self.assertDeviceChecks(dc, op, [L, I, V], [0])\n    self.assertReferenceChecks(gc, op, [L, I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L, I, V], 2, [0])\n    self.assertDeviceChecks(dc, op2, [L, I, V, S], [0])\n    self.assertReferenceChecks(gc, op2, [L, I, V, S], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op2, [L, I, V, S], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, V], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, V], 2, [0])",
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), default_value=st.floats(min_value=2.0, max_value=3.0), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_sparse_to_dense(self, batch_size, dense_last_dim, default_value, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    num_data = L.sum()\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    V = np.random.rand(num_data).astype(np.float32)\n    op = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V'], ['O'], dense_last_dim=dense_last_dim, default_value=default_value)\n    S = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op2 = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V', 'S'], ['O'], default_value=default_value)\n\n    def batch_sparse_to_dense_ref(L, I, V, S=None):\n        if S is None:\n            ret = np.zeros((batch_size, dense_last_dim))\n        else:\n            ret = np.zeros(S.shape)\n        ret.fill(default_value)\n        batch = 0\n        v_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[batch][I[v_idx]] = V[v_idx]\n                v_idx += 1\n            batch += 1\n        return [ret]\n    self.assertDeviceChecks(dc, op, [L, I, V], [0])\n    self.assertReferenceChecks(gc, op, [L, I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L, I, V], 2, [0])\n    self.assertDeviceChecks(dc, op2, [L, I, V, S], [0])\n    self.assertReferenceChecks(gc, op2, [L, I, V, S], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op2, [L, I, V, S], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, V], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, V], 2, [0])",
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), default_value=st.floats(min_value=2.0, max_value=3.0), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_sparse_to_dense(self, batch_size, dense_last_dim, default_value, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    num_data = L.sum()\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    V = np.random.rand(num_data).astype(np.float32)\n    op = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V'], ['O'], dense_last_dim=dense_last_dim, default_value=default_value)\n    S = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op2 = core.CreateOperator('BatchSparseToDense', ['L', 'I', 'V', 'S'], ['O'], default_value=default_value)\n\n    def batch_sparse_to_dense_ref(L, I, V, S=None):\n        if S is None:\n            ret = np.zeros((batch_size, dense_last_dim))\n        else:\n            ret = np.zeros(S.shape)\n        ret.fill(default_value)\n        batch = 0\n        v_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[batch][I[v_idx]] = V[v_idx]\n                v_idx += 1\n            batch += 1\n        return [ret]\n    self.assertDeviceChecks(dc, op, [L, I, V], [0])\n    self.assertReferenceChecks(gc, op, [L, I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L, I, V], 2, [0])\n    self.assertDeviceChecks(dc, op2, [L, I, V, S], [0])\n    self.assertReferenceChecks(gc, op2, [L, I, V, S], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op2, [L, I, V, S], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, V], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, V], batch_sparse_to_dense_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, V], 2, [0])"
        ]
    },
    {
        "func_name": "batch_dense_to_sparse_ref",
        "original": "def batch_dense_to_sparse_ref(L, I, D):\n    ret = np.zeros(I.shape)\n    batch = 0\n    i_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[i_idx] = D[batch][I[i_idx]]\n            i_idx += 1\n        batch += 1\n    return [ret]",
        "mutated": [
            "def batch_dense_to_sparse_ref(L, I, D):\n    if False:\n        i = 10\n    ret = np.zeros(I.shape)\n    batch = 0\n    i_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[i_idx] = D[batch][I[i_idx]]\n            i_idx += 1\n        batch += 1\n    return [ret]",
            "def batch_dense_to_sparse_ref(L, I, D):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ret = np.zeros(I.shape)\n    batch = 0\n    i_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[i_idx] = D[batch][I[i_idx]]\n            i_idx += 1\n        batch += 1\n    return [ret]",
            "def batch_dense_to_sparse_ref(L, I, D):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ret = np.zeros(I.shape)\n    batch = 0\n    i_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[i_idx] = D[batch][I[i_idx]]\n            i_idx += 1\n        batch += 1\n    return [ret]",
            "def batch_dense_to_sparse_ref(L, I, D):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ret = np.zeros(I.shape)\n    batch = 0\n    i_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[i_idx] = D[batch][I[i_idx]]\n            i_idx += 1\n        batch += 1\n    return [ret]",
            "def batch_dense_to_sparse_ref(L, I, D):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ret = np.zeros(I.shape)\n    batch = 0\n    i_idx = 0\n    for length in L:\n        for _ in range(length):\n            ret[i_idx] = D[batch][I[i_idx]]\n            i_idx += 1\n        batch += 1\n    return [ret]"
        ]
    },
    {
        "func_name": "test_batch_dense_to_sparse",
        "original": "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_dense_to_sparse(self, batch_size, dense_last_dim, gc, dc):\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    D = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op = core.CreateOperator('BatchDenseToSparse', ['L', 'I', 'D'], ['V'])\n\n    def batch_dense_to_sparse_ref(L, I, D):\n        ret = np.zeros(I.shape)\n        batch = 0\n        i_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[i_idx] = D[batch][I[i_idx]]\n                i_idx += 1\n            batch += 1\n        return [ret]\n    print(L, I, D)\n    self.assertDeviceChecks(dc, op, [L, I, D], [0])\n    self.assertReferenceChecks(gc, op, [L, I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L, I, D], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, D], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, D], 2, [0])",
        "mutated": [
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_dense_to_sparse(self, batch_size, dense_last_dim, gc, dc):\n    if False:\n        i = 10\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    D = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op = core.CreateOperator('BatchDenseToSparse', ['L', 'I', 'D'], ['V'])\n\n    def batch_dense_to_sparse_ref(L, I, D):\n        ret = np.zeros(I.shape)\n        batch = 0\n        i_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[i_idx] = D[batch][I[i_idx]]\n                i_idx += 1\n            batch += 1\n        return [ret]\n    print(L, I, D)\n    self.assertDeviceChecks(dc, op, [L, I, D], [0])\n    self.assertReferenceChecks(gc, op, [L, I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L, I, D], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, D], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, D], 2, [0])",
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_dense_to_sparse(self, batch_size, dense_last_dim, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    D = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op = core.CreateOperator('BatchDenseToSparse', ['L', 'I', 'D'], ['V'])\n\n    def batch_dense_to_sparse_ref(L, I, D):\n        ret = np.zeros(I.shape)\n        batch = 0\n        i_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[i_idx] = D[batch][I[i_idx]]\n                i_idx += 1\n            batch += 1\n        return [ret]\n    print(L, I, D)\n    self.assertDeviceChecks(dc, op, [L, I, D], [0])\n    self.assertReferenceChecks(gc, op, [L, I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L, I, D], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, D], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, D], 2, [0])",
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_dense_to_sparse(self, batch_size, dense_last_dim, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    D = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op = core.CreateOperator('BatchDenseToSparse', ['L', 'I', 'D'], ['V'])\n\n    def batch_dense_to_sparse_ref(L, I, D):\n        ret = np.zeros(I.shape)\n        batch = 0\n        i_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[i_idx] = D[batch][I[i_idx]]\n                i_idx += 1\n            batch += 1\n        return [ret]\n    print(L, I, D)\n    self.assertDeviceChecks(dc, op, [L, I, D], [0])\n    self.assertReferenceChecks(gc, op, [L, I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L, I, D], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, D], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, D], 2, [0])",
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_dense_to_sparse(self, batch_size, dense_last_dim, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    D = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op = core.CreateOperator('BatchDenseToSparse', ['L', 'I', 'D'], ['V'])\n\n    def batch_dense_to_sparse_ref(L, I, D):\n        ret = np.zeros(I.shape)\n        batch = 0\n        i_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[i_idx] = D[batch][I[i_idx]]\n                i_idx += 1\n            batch += 1\n        return [ret]\n    print(L, I, D)\n    self.assertDeviceChecks(dc, op, [L, I, D], [0])\n    self.assertReferenceChecks(gc, op, [L, I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L, I, D], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, D], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, D], 2, [0])",
            "@given(batch_size=st.integers(5, 10), dense_last_dim=st.integers(5, 10), **hu.gcs)\n@settings(deadline=None)\ndef test_batch_dense_to_sparse(self, batch_size, dense_last_dim, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    L = np.random.randint(1, dense_last_dim + 1, size=batch_size)\n    I = np.array([]).astype(np.int32)\n    for l in L:\n        I_l = np.random.choice(dense_last_dim, l, replace=False)\n        I = np.concatenate((I, I_l))\n    D = np.random.rand(batch_size, dense_last_dim).astype(np.float32)\n    op = core.CreateOperator('BatchDenseToSparse', ['L', 'I', 'D'], ['V'])\n\n    def batch_dense_to_sparse_ref(L, I, D):\n        ret = np.zeros(I.shape)\n        batch = 0\n        i_idx = 0\n        for length in L:\n            for _ in range(length):\n                ret[i_idx] = D[batch][I[i_idx]]\n                i_idx += 1\n            batch += 1\n        return [ret]\n    print(L, I, D)\n    self.assertDeviceChecks(dc, op, [L, I, D], [0])\n    self.assertReferenceChecks(gc, op, [L, I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L, I, D], 2, [0])\n    self.assertDeviceChecks(dc, op, [L.astype(np.int32), I, D], [0])\n    self.assertReferenceChecks(gc, op, [L.astype(np.int32), I, D], batch_dense_to_sparse_ref)\n    self.assertGradientChecks(gc, op, [L.astype(np.int32), I, D], 2, [0])"
        ]
    }
]