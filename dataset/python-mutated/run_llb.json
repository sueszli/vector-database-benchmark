[
    {
        "func_name": "parse_args",
        "original": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', required=True)\n    parser.add_argument('--model_args', default='')\n    parser.add_argument('--pretrained', required=True, type=str)\n    parser.add_argument('--tasks', required=True, nargs='+', type=str)\n    parser.add_argument('--precision', required=True, nargs='+', type=str)\n    parser.add_argument('--provide_description', action='store_true')\n    parser.add_argument('--num_fewshot', type=int, default=0)\n    parser.add_argument('--batch_size', type=str, default=None)\n    parser.add_argument('--max_batch_size', type=int, default=None, help='Maximal batch size to try with --batch_size auto')\n    parser.add_argument('--device', type=str, default=None)\n    parser.add_argument('--output_path', default=None)\n    parser.add_argument('--limit', type=float, default=None, help='Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.')\n    parser.add_argument('--data_sampling', type=float, default=None)\n    parser.add_argument('--no_cache', action='store_true')\n    parser.add_argument('--decontamination_ngrams_path', default=None)\n    parser.add_argument('--description_dict_path', default=None)\n    parser.add_argument('--check_integrity', action='store_true')\n    parser.add_argument('--write_out', action='store_true', default=False)\n    parser.add_argument('--output_base_path', type=str, default=None)\n    return parser.parse_args()",
        "mutated": [
            "def parse_args():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', required=True)\n    parser.add_argument('--model_args', default='')\n    parser.add_argument('--pretrained', required=True, type=str)\n    parser.add_argument('--tasks', required=True, nargs='+', type=str)\n    parser.add_argument('--precision', required=True, nargs='+', type=str)\n    parser.add_argument('--provide_description', action='store_true')\n    parser.add_argument('--num_fewshot', type=int, default=0)\n    parser.add_argument('--batch_size', type=str, default=None)\n    parser.add_argument('--max_batch_size', type=int, default=None, help='Maximal batch size to try with --batch_size auto')\n    parser.add_argument('--device', type=str, default=None)\n    parser.add_argument('--output_path', default=None)\n    parser.add_argument('--limit', type=float, default=None, help='Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.')\n    parser.add_argument('--data_sampling', type=float, default=None)\n    parser.add_argument('--no_cache', action='store_true')\n    parser.add_argument('--decontamination_ngrams_path', default=None)\n    parser.add_argument('--description_dict_path', default=None)\n    parser.add_argument('--check_integrity', action='store_true')\n    parser.add_argument('--write_out', action='store_true', default=False)\n    parser.add_argument('--output_base_path', type=str, default=None)\n    return parser.parse_args()",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', required=True)\n    parser.add_argument('--model_args', default='')\n    parser.add_argument('--pretrained', required=True, type=str)\n    parser.add_argument('--tasks', required=True, nargs='+', type=str)\n    parser.add_argument('--precision', required=True, nargs='+', type=str)\n    parser.add_argument('--provide_description', action='store_true')\n    parser.add_argument('--num_fewshot', type=int, default=0)\n    parser.add_argument('--batch_size', type=str, default=None)\n    parser.add_argument('--max_batch_size', type=int, default=None, help='Maximal batch size to try with --batch_size auto')\n    parser.add_argument('--device', type=str, default=None)\n    parser.add_argument('--output_path', default=None)\n    parser.add_argument('--limit', type=float, default=None, help='Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.')\n    parser.add_argument('--data_sampling', type=float, default=None)\n    parser.add_argument('--no_cache', action='store_true')\n    parser.add_argument('--decontamination_ngrams_path', default=None)\n    parser.add_argument('--description_dict_path', default=None)\n    parser.add_argument('--check_integrity', action='store_true')\n    parser.add_argument('--write_out', action='store_true', default=False)\n    parser.add_argument('--output_base_path', type=str, default=None)\n    return parser.parse_args()",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', required=True)\n    parser.add_argument('--model_args', default='')\n    parser.add_argument('--pretrained', required=True, type=str)\n    parser.add_argument('--tasks', required=True, nargs='+', type=str)\n    parser.add_argument('--precision', required=True, nargs='+', type=str)\n    parser.add_argument('--provide_description', action='store_true')\n    parser.add_argument('--num_fewshot', type=int, default=0)\n    parser.add_argument('--batch_size', type=str, default=None)\n    parser.add_argument('--max_batch_size', type=int, default=None, help='Maximal batch size to try with --batch_size auto')\n    parser.add_argument('--device', type=str, default=None)\n    parser.add_argument('--output_path', default=None)\n    parser.add_argument('--limit', type=float, default=None, help='Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.')\n    parser.add_argument('--data_sampling', type=float, default=None)\n    parser.add_argument('--no_cache', action='store_true')\n    parser.add_argument('--decontamination_ngrams_path', default=None)\n    parser.add_argument('--description_dict_path', default=None)\n    parser.add_argument('--check_integrity', action='store_true')\n    parser.add_argument('--write_out', action='store_true', default=False)\n    parser.add_argument('--output_base_path', type=str, default=None)\n    return parser.parse_args()",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', required=True)\n    parser.add_argument('--model_args', default='')\n    parser.add_argument('--pretrained', required=True, type=str)\n    parser.add_argument('--tasks', required=True, nargs='+', type=str)\n    parser.add_argument('--precision', required=True, nargs='+', type=str)\n    parser.add_argument('--provide_description', action='store_true')\n    parser.add_argument('--num_fewshot', type=int, default=0)\n    parser.add_argument('--batch_size', type=str, default=None)\n    parser.add_argument('--max_batch_size', type=int, default=None, help='Maximal batch size to try with --batch_size auto')\n    parser.add_argument('--device', type=str, default=None)\n    parser.add_argument('--output_path', default=None)\n    parser.add_argument('--limit', type=float, default=None, help='Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.')\n    parser.add_argument('--data_sampling', type=float, default=None)\n    parser.add_argument('--no_cache', action='store_true')\n    parser.add_argument('--decontamination_ngrams_path', default=None)\n    parser.add_argument('--description_dict_path', default=None)\n    parser.add_argument('--check_integrity', action='store_true')\n    parser.add_argument('--write_out', action='store_true', default=False)\n    parser.add_argument('--output_base_path', type=str, default=None)\n    return parser.parse_args()",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', required=True)\n    parser.add_argument('--model_args', default='')\n    parser.add_argument('--pretrained', required=True, type=str)\n    parser.add_argument('--tasks', required=True, nargs='+', type=str)\n    parser.add_argument('--precision', required=True, nargs='+', type=str)\n    parser.add_argument('--provide_description', action='store_true')\n    parser.add_argument('--num_fewshot', type=int, default=0)\n    parser.add_argument('--batch_size', type=str, default=None)\n    parser.add_argument('--max_batch_size', type=int, default=None, help='Maximal batch size to try with --batch_size auto')\n    parser.add_argument('--device', type=str, default=None)\n    parser.add_argument('--output_path', default=None)\n    parser.add_argument('--limit', type=float, default=None, help='Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.')\n    parser.add_argument('--data_sampling', type=float, default=None)\n    parser.add_argument('--no_cache', action='store_true')\n    parser.add_argument('--decontamination_ngrams_path', default=None)\n    parser.add_argument('--description_dict_path', default=None)\n    parser.add_argument('--check_integrity', action='store_true')\n    parser.add_argument('--write_out', action='store_true', default=False)\n    parser.add_argument('--output_base_path', type=str, default=None)\n    return parser.parse_args()"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    args = parse_args()\n    assert not args.provide_description\n    if args.limit:\n        print('WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.')\n    print(f'Selected Tasks: {args.tasks}')\n    description_dict = {}\n    if args.description_dict_path:\n        with open(args.description_dict_path, 'r') as f:\n            description_dict = json.load(f)\n    success = []\n    fail = []\n    for prec in args.precision:\n        prec_arg = parse_precision(prec, args.model)\n        model_args = f'pretrained={args.pretrained},{prec_arg}'\n        if len(args.model_args) > 0:\n            model_args += args.model_args\n        for task in args.tasks:\n            task_names = task_map.get(task, task).split(',')\n            num_fewshot = task_to_n_few_shots.get(task, args.num_fewshot)\n            try:\n                results = evaluator.simple_evaluate(model=args.model, model_args=model_args, tasks=task_names, num_fewshot=num_fewshot, batch_size=args.batch_size, max_batch_size=args.max_batch_size, device=args.device, no_cache=args.no_cache, limit=args.limit, description_dict=description_dict, decontamination_ngrams_path=args.decontamination_ngrams_path, check_integrity=args.check_integrity, write_out=args.write_out, output_base_path=args.output_base_path)\n                if len(results['results']) > 1:\n                    average = {}\n                    for (_, subtask) in results['results'].items():\n                        for (metric, value) in subtask.items():\n                            average[metric] = average.get(metric, []) + [value]\n                    for (k, v) in average.items():\n                        average[k] = sum(average[k]) / len(average[k]) if not k.endswith('_stderr') else 0\n                    results['results'][f'avg_{task}'] = average\n                    results['versions'][f'avg_{task}'] = 1\n                dumped = json.dumps(results, indent=2)\n                print(dumped)\n                if args.output_path:\n                    dirname = os.path.dirname(args.output_path)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(args.output_path, 'w') as f:\n                        f.write(dumped)\n                success.append(results)\n            except Exception as e:\n                fail.append(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n                print(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n    print('Here are results of all successful tasks:')\n    for results in success:\n        print(results['config'])\n        print(evaluator.make_table(results))\n    if len(fail) > 0:\n        raise RuntimeError('\\n'.join(fail))",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    args = parse_args()\n    assert not args.provide_description\n    if args.limit:\n        print('WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.')\n    print(f'Selected Tasks: {args.tasks}')\n    description_dict = {}\n    if args.description_dict_path:\n        with open(args.description_dict_path, 'r') as f:\n            description_dict = json.load(f)\n    success = []\n    fail = []\n    for prec in args.precision:\n        prec_arg = parse_precision(prec, args.model)\n        model_args = f'pretrained={args.pretrained},{prec_arg}'\n        if len(args.model_args) > 0:\n            model_args += args.model_args\n        for task in args.tasks:\n            task_names = task_map.get(task, task).split(',')\n            num_fewshot = task_to_n_few_shots.get(task, args.num_fewshot)\n            try:\n                results = evaluator.simple_evaluate(model=args.model, model_args=model_args, tasks=task_names, num_fewshot=num_fewshot, batch_size=args.batch_size, max_batch_size=args.max_batch_size, device=args.device, no_cache=args.no_cache, limit=args.limit, description_dict=description_dict, decontamination_ngrams_path=args.decontamination_ngrams_path, check_integrity=args.check_integrity, write_out=args.write_out, output_base_path=args.output_base_path)\n                if len(results['results']) > 1:\n                    average = {}\n                    for (_, subtask) in results['results'].items():\n                        for (metric, value) in subtask.items():\n                            average[metric] = average.get(metric, []) + [value]\n                    for (k, v) in average.items():\n                        average[k] = sum(average[k]) / len(average[k]) if not k.endswith('_stderr') else 0\n                    results['results'][f'avg_{task}'] = average\n                    results['versions'][f'avg_{task}'] = 1\n                dumped = json.dumps(results, indent=2)\n                print(dumped)\n                if args.output_path:\n                    dirname = os.path.dirname(args.output_path)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(args.output_path, 'w') as f:\n                        f.write(dumped)\n                success.append(results)\n            except Exception as e:\n                fail.append(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n                print(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n    print('Here are results of all successful tasks:')\n    for results in success:\n        print(results['config'])\n        print(evaluator.make_table(results))\n    if len(fail) > 0:\n        raise RuntimeError('\\n'.join(fail))",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    args = parse_args()\n    assert not args.provide_description\n    if args.limit:\n        print('WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.')\n    print(f'Selected Tasks: {args.tasks}')\n    description_dict = {}\n    if args.description_dict_path:\n        with open(args.description_dict_path, 'r') as f:\n            description_dict = json.load(f)\n    success = []\n    fail = []\n    for prec in args.precision:\n        prec_arg = parse_precision(prec, args.model)\n        model_args = f'pretrained={args.pretrained},{prec_arg}'\n        if len(args.model_args) > 0:\n            model_args += args.model_args\n        for task in args.tasks:\n            task_names = task_map.get(task, task).split(',')\n            num_fewshot = task_to_n_few_shots.get(task, args.num_fewshot)\n            try:\n                results = evaluator.simple_evaluate(model=args.model, model_args=model_args, tasks=task_names, num_fewshot=num_fewshot, batch_size=args.batch_size, max_batch_size=args.max_batch_size, device=args.device, no_cache=args.no_cache, limit=args.limit, description_dict=description_dict, decontamination_ngrams_path=args.decontamination_ngrams_path, check_integrity=args.check_integrity, write_out=args.write_out, output_base_path=args.output_base_path)\n                if len(results['results']) > 1:\n                    average = {}\n                    for (_, subtask) in results['results'].items():\n                        for (metric, value) in subtask.items():\n                            average[metric] = average.get(metric, []) + [value]\n                    for (k, v) in average.items():\n                        average[k] = sum(average[k]) / len(average[k]) if not k.endswith('_stderr') else 0\n                    results['results'][f'avg_{task}'] = average\n                    results['versions'][f'avg_{task}'] = 1\n                dumped = json.dumps(results, indent=2)\n                print(dumped)\n                if args.output_path:\n                    dirname = os.path.dirname(args.output_path)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(args.output_path, 'w') as f:\n                        f.write(dumped)\n                success.append(results)\n            except Exception as e:\n                fail.append(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n                print(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n    print('Here are results of all successful tasks:')\n    for results in success:\n        print(results['config'])\n        print(evaluator.make_table(results))\n    if len(fail) > 0:\n        raise RuntimeError('\\n'.join(fail))",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    args = parse_args()\n    assert not args.provide_description\n    if args.limit:\n        print('WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.')\n    print(f'Selected Tasks: {args.tasks}')\n    description_dict = {}\n    if args.description_dict_path:\n        with open(args.description_dict_path, 'r') as f:\n            description_dict = json.load(f)\n    success = []\n    fail = []\n    for prec in args.precision:\n        prec_arg = parse_precision(prec, args.model)\n        model_args = f'pretrained={args.pretrained},{prec_arg}'\n        if len(args.model_args) > 0:\n            model_args += args.model_args\n        for task in args.tasks:\n            task_names = task_map.get(task, task).split(',')\n            num_fewshot = task_to_n_few_shots.get(task, args.num_fewshot)\n            try:\n                results = evaluator.simple_evaluate(model=args.model, model_args=model_args, tasks=task_names, num_fewshot=num_fewshot, batch_size=args.batch_size, max_batch_size=args.max_batch_size, device=args.device, no_cache=args.no_cache, limit=args.limit, description_dict=description_dict, decontamination_ngrams_path=args.decontamination_ngrams_path, check_integrity=args.check_integrity, write_out=args.write_out, output_base_path=args.output_base_path)\n                if len(results['results']) > 1:\n                    average = {}\n                    for (_, subtask) in results['results'].items():\n                        for (metric, value) in subtask.items():\n                            average[metric] = average.get(metric, []) + [value]\n                    for (k, v) in average.items():\n                        average[k] = sum(average[k]) / len(average[k]) if not k.endswith('_stderr') else 0\n                    results['results'][f'avg_{task}'] = average\n                    results['versions'][f'avg_{task}'] = 1\n                dumped = json.dumps(results, indent=2)\n                print(dumped)\n                if args.output_path:\n                    dirname = os.path.dirname(args.output_path)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(args.output_path, 'w') as f:\n                        f.write(dumped)\n                success.append(results)\n            except Exception as e:\n                fail.append(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n                print(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n    print('Here are results of all successful tasks:')\n    for results in success:\n        print(results['config'])\n        print(evaluator.make_table(results))\n    if len(fail) > 0:\n        raise RuntimeError('\\n'.join(fail))",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    args = parse_args()\n    assert not args.provide_description\n    if args.limit:\n        print('WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.')\n    print(f'Selected Tasks: {args.tasks}')\n    description_dict = {}\n    if args.description_dict_path:\n        with open(args.description_dict_path, 'r') as f:\n            description_dict = json.load(f)\n    success = []\n    fail = []\n    for prec in args.precision:\n        prec_arg = parse_precision(prec, args.model)\n        model_args = f'pretrained={args.pretrained},{prec_arg}'\n        if len(args.model_args) > 0:\n            model_args += args.model_args\n        for task in args.tasks:\n            task_names = task_map.get(task, task).split(',')\n            num_fewshot = task_to_n_few_shots.get(task, args.num_fewshot)\n            try:\n                results = evaluator.simple_evaluate(model=args.model, model_args=model_args, tasks=task_names, num_fewshot=num_fewshot, batch_size=args.batch_size, max_batch_size=args.max_batch_size, device=args.device, no_cache=args.no_cache, limit=args.limit, description_dict=description_dict, decontamination_ngrams_path=args.decontamination_ngrams_path, check_integrity=args.check_integrity, write_out=args.write_out, output_base_path=args.output_base_path)\n                if len(results['results']) > 1:\n                    average = {}\n                    for (_, subtask) in results['results'].items():\n                        for (metric, value) in subtask.items():\n                            average[metric] = average.get(metric, []) + [value]\n                    for (k, v) in average.items():\n                        average[k] = sum(average[k]) / len(average[k]) if not k.endswith('_stderr') else 0\n                    results['results'][f'avg_{task}'] = average\n                    results['versions'][f'avg_{task}'] = 1\n                dumped = json.dumps(results, indent=2)\n                print(dumped)\n                if args.output_path:\n                    dirname = os.path.dirname(args.output_path)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(args.output_path, 'w') as f:\n                        f.write(dumped)\n                success.append(results)\n            except Exception as e:\n                fail.append(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n                print(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n    print('Here are results of all successful tasks:')\n    for results in success:\n        print(results['config'])\n        print(evaluator.make_table(results))\n    if len(fail) > 0:\n        raise RuntimeError('\\n'.join(fail))",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    args = parse_args()\n    assert not args.provide_description\n    if args.limit:\n        print('WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.')\n    print(f'Selected Tasks: {args.tasks}')\n    description_dict = {}\n    if args.description_dict_path:\n        with open(args.description_dict_path, 'r') as f:\n            description_dict = json.load(f)\n    success = []\n    fail = []\n    for prec in args.precision:\n        prec_arg = parse_precision(prec, args.model)\n        model_args = f'pretrained={args.pretrained},{prec_arg}'\n        if len(args.model_args) > 0:\n            model_args += args.model_args\n        for task in args.tasks:\n            task_names = task_map.get(task, task).split(',')\n            num_fewshot = task_to_n_few_shots.get(task, args.num_fewshot)\n            try:\n                results = evaluator.simple_evaluate(model=args.model, model_args=model_args, tasks=task_names, num_fewshot=num_fewshot, batch_size=args.batch_size, max_batch_size=args.max_batch_size, device=args.device, no_cache=args.no_cache, limit=args.limit, description_dict=description_dict, decontamination_ngrams_path=args.decontamination_ngrams_path, check_integrity=args.check_integrity, write_out=args.write_out, output_base_path=args.output_base_path)\n                if len(results['results']) > 1:\n                    average = {}\n                    for (_, subtask) in results['results'].items():\n                        for (metric, value) in subtask.items():\n                            average[metric] = average.get(metric, []) + [value]\n                    for (k, v) in average.items():\n                        average[k] = sum(average[k]) / len(average[k]) if not k.endswith('_stderr') else 0\n                    results['results'][f'avg_{task}'] = average\n                    results['versions'][f'avg_{task}'] = 1\n                dumped = json.dumps(results, indent=2)\n                print(dumped)\n                if args.output_path:\n                    dirname = os.path.dirname(args.output_path)\n                    if dirname:\n                        os.makedirs(dirname, exist_ok=True)\n                    with open(args.output_path, 'w') as f:\n                        f.write(dumped)\n                success.append(results)\n            except Exception as e:\n                fail.append(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n                print(f'Job config of task={task}, precision={prec} failed. Error Message: {str(e)}')\n    print('Here are results of all successful tasks:')\n    for results in success:\n        print(results['config'])\n        print(evaluator.make_table(results))\n    if len(fail) > 0:\n        raise RuntimeError('\\n'.join(fail))"
        ]
    }
]