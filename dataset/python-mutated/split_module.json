[
    {
        "func_name": "__init__",
        "original": "def __init__(self, name: str):\n    self.name: str = name\n    self.submod_name = f'submod_{name}'\n    self.node_names: List[str] = []\n    self.inputs: Dict[str, None] = {}\n    self.outputs: Dict[str, None] = {}\n    self.dependencies: Dict[str, None] = {}\n    self.dependents: Dict[str, None] = {}\n    self.graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    self.environment: Dict[Node, Node] = {}\n    self.targets: Dict[str, Any] = {}",
        "mutated": [
            "def __init__(self, name: str):\n    if False:\n        i = 10\n    self.name: str = name\n    self.submod_name = f'submod_{name}'\n    self.node_names: List[str] = []\n    self.inputs: Dict[str, None] = {}\n    self.outputs: Dict[str, None] = {}\n    self.dependencies: Dict[str, None] = {}\n    self.dependents: Dict[str, None] = {}\n    self.graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    self.environment: Dict[Node, Node] = {}\n    self.targets: Dict[str, Any] = {}",
            "def __init__(self, name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.name: str = name\n    self.submod_name = f'submod_{name}'\n    self.node_names: List[str] = []\n    self.inputs: Dict[str, None] = {}\n    self.outputs: Dict[str, None] = {}\n    self.dependencies: Dict[str, None] = {}\n    self.dependents: Dict[str, None] = {}\n    self.graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    self.environment: Dict[Node, Node] = {}\n    self.targets: Dict[str, Any] = {}",
            "def __init__(self, name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.name: str = name\n    self.submod_name = f'submod_{name}'\n    self.node_names: List[str] = []\n    self.inputs: Dict[str, None] = {}\n    self.outputs: Dict[str, None] = {}\n    self.dependencies: Dict[str, None] = {}\n    self.dependents: Dict[str, None] = {}\n    self.graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    self.environment: Dict[Node, Node] = {}\n    self.targets: Dict[str, Any] = {}",
            "def __init__(self, name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.name: str = name\n    self.submod_name = f'submod_{name}'\n    self.node_names: List[str] = []\n    self.inputs: Dict[str, None] = {}\n    self.outputs: Dict[str, None] = {}\n    self.dependencies: Dict[str, None] = {}\n    self.dependents: Dict[str, None] = {}\n    self.graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    self.environment: Dict[Node, Node] = {}\n    self.targets: Dict[str, Any] = {}",
            "def __init__(self, name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.name: str = name\n    self.submod_name = f'submod_{name}'\n    self.node_names: List[str] = []\n    self.inputs: Dict[str, None] = {}\n    self.outputs: Dict[str, None] = {}\n    self.dependencies: Dict[str, None] = {}\n    self.dependents: Dict[str, None] = {}\n    self.graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    self.environment: Dict[Node, Node] = {}\n    self.targets: Dict[str, Any] = {}"
        ]
    },
    {
        "func_name": "__repr__",
        "original": "def __repr__(self) -> str:\n    return f'name: {self.name},\\n nodes: {self.node_names},\\n inputs: {self.inputs},\\n outputs: {self.outputs},\\n partitions depended on: {self.dependencies},\\n partition dependents: {self.dependents}'",
        "mutated": [
            "def __repr__(self) -> str:\n    if False:\n        i = 10\n    return f'name: {self.name},\\n nodes: {self.node_names},\\n inputs: {self.inputs},\\n outputs: {self.outputs},\\n partitions depended on: {self.dependencies},\\n partition dependents: {self.dependents}'",
            "def __repr__(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return f'name: {self.name},\\n nodes: {self.node_names},\\n inputs: {self.inputs},\\n outputs: {self.outputs},\\n partitions depended on: {self.dependencies},\\n partition dependents: {self.dependents}'",
            "def __repr__(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return f'name: {self.name},\\n nodes: {self.node_names},\\n inputs: {self.inputs},\\n outputs: {self.outputs},\\n partitions depended on: {self.dependencies},\\n partition dependents: {self.dependents}'",
            "def __repr__(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return f'name: {self.name},\\n nodes: {self.node_names},\\n inputs: {self.inputs},\\n outputs: {self.outputs},\\n partitions depended on: {self.dependencies},\\n partition dependents: {self.dependents}'",
            "def __repr__(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return f'name: {self.name},\\n nodes: {self.node_names},\\n inputs: {self.inputs},\\n outputs: {self.outputs},\\n partitions depended on: {self.dependencies},\\n partition dependents: {self.dependents}'"
        ]
    },
    {
        "func_name": "construct_graph",
        "original": "def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n    if node.op == 'placeholder':\n        default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n        base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n        base_mod_env[node.name].meta = node.meta.copy()\n    elif node.op == 'get_attr':\n        base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n        base_mod_env[node.name].meta = node.meta.copy()\n        attr_val = m\n        for atom in node.target.split('.'):\n            if not hasattr(attr_val, atom):\n                raise AttributeError(f'Node target {node.target} not found!')\n            attr_val = getattr(attr_val, atom)\n        base_mod_attrs[node.target] = attr_val\n    return (base_mod_env, base_mod_attrs)",
        "mutated": [
            "def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n    if False:\n        i = 10\n    if node.op == 'placeholder':\n        default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n        base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n        base_mod_env[node.name].meta = node.meta.copy()\n    elif node.op == 'get_attr':\n        base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n        base_mod_env[node.name].meta = node.meta.copy()\n        attr_val = m\n        for atom in node.target.split('.'):\n            if not hasattr(attr_val, atom):\n                raise AttributeError(f'Node target {node.target} not found!')\n            attr_val = getattr(attr_val, atom)\n        base_mod_attrs[node.target] = attr_val\n    return (base_mod_env, base_mod_attrs)",
            "def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if node.op == 'placeholder':\n        default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n        base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n        base_mod_env[node.name].meta = node.meta.copy()\n    elif node.op == 'get_attr':\n        base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n        base_mod_env[node.name].meta = node.meta.copy()\n        attr_val = m\n        for atom in node.target.split('.'):\n            if not hasattr(attr_val, atom):\n                raise AttributeError(f'Node target {node.target} not found!')\n            attr_val = getattr(attr_val, atom)\n        base_mod_attrs[node.target] = attr_val\n    return (base_mod_env, base_mod_attrs)",
            "def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if node.op == 'placeholder':\n        default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n        base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n        base_mod_env[node.name].meta = node.meta.copy()\n    elif node.op == 'get_attr':\n        base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n        base_mod_env[node.name].meta = node.meta.copy()\n        attr_val = m\n        for atom in node.target.split('.'):\n            if not hasattr(attr_val, atom):\n                raise AttributeError(f'Node target {node.target} not found!')\n            attr_val = getattr(attr_val, atom)\n        base_mod_attrs[node.target] = attr_val\n    return (base_mod_env, base_mod_attrs)",
            "def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if node.op == 'placeholder':\n        default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n        base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n        base_mod_env[node.name].meta = node.meta.copy()\n    elif node.op == 'get_attr':\n        base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n        base_mod_env[node.name].meta = node.meta.copy()\n        attr_val = m\n        for atom in node.target.split('.'):\n            if not hasattr(attr_val, atom):\n                raise AttributeError(f'Node target {node.target} not found!')\n            attr_val = getattr(attr_val, atom)\n        base_mod_attrs[node.target] = attr_val\n    return (base_mod_env, base_mod_attrs)",
            "def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if node.op == 'placeholder':\n        default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n        base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n        base_mod_env[node.name].meta = node.meta.copy()\n    elif node.op == 'get_attr':\n        base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n        base_mod_env[node.name].meta = node.meta.copy()\n        attr_val = m\n        for atom in node.target.split('.'):\n            if not hasattr(attr_val, atom):\n                raise AttributeError(f'Node target {node.target} not found!')\n            attr_val = getattr(attr_val, atom)\n        base_mod_attrs[node.target] = attr_val\n    return (base_mod_env, base_mod_attrs)"
        ]
    },
    {
        "func_name": "record_cross_partition_use",
        "original": "def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n    defined = getattr(def_node, '_fx_partition', None)\n    used = getattr(use_node, '_fx_partition', None)\n    if defined != used:\n        if defined is not None:\n            def_partition = partitions[defined]\n            def_partition.outputs.setdefault(def_node.name)\n            if used is not None:\n                def_partition.dependents.setdefault(used)\n        if used is not None:\n            use_partition = partitions[used]\n            use_partition.inputs.setdefault(def_node.name)\n            if defined is not None:\n                use_partition.dependencies.setdefault(defined)",
        "mutated": [
            "def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n    if False:\n        i = 10\n    defined = getattr(def_node, '_fx_partition', None)\n    used = getattr(use_node, '_fx_partition', None)\n    if defined != used:\n        if defined is not None:\n            def_partition = partitions[defined]\n            def_partition.outputs.setdefault(def_node.name)\n            if used is not None:\n                def_partition.dependents.setdefault(used)\n        if used is not None:\n            use_partition = partitions[used]\n            use_partition.inputs.setdefault(def_node.name)\n            if defined is not None:\n                use_partition.dependencies.setdefault(defined)",
            "def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    defined = getattr(def_node, '_fx_partition', None)\n    used = getattr(use_node, '_fx_partition', None)\n    if defined != used:\n        if defined is not None:\n            def_partition = partitions[defined]\n            def_partition.outputs.setdefault(def_node.name)\n            if used is not None:\n                def_partition.dependents.setdefault(used)\n        if used is not None:\n            use_partition = partitions[used]\n            use_partition.inputs.setdefault(def_node.name)\n            if defined is not None:\n                use_partition.dependencies.setdefault(defined)",
            "def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    defined = getattr(def_node, '_fx_partition', None)\n    used = getattr(use_node, '_fx_partition', None)\n    if defined != used:\n        if defined is not None:\n            def_partition = partitions[defined]\n            def_partition.outputs.setdefault(def_node.name)\n            if used is not None:\n                def_partition.dependents.setdefault(used)\n        if used is not None:\n            use_partition = partitions[used]\n            use_partition.inputs.setdefault(def_node.name)\n            if defined is not None:\n                use_partition.dependencies.setdefault(defined)",
            "def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    defined = getattr(def_node, '_fx_partition', None)\n    used = getattr(use_node, '_fx_partition', None)\n    if defined != used:\n        if defined is not None:\n            def_partition = partitions[defined]\n            def_partition.outputs.setdefault(def_node.name)\n            if used is not None:\n                def_partition.dependents.setdefault(used)\n        if used is not None:\n            use_partition = partitions[used]\n            use_partition.inputs.setdefault(def_node.name)\n            if defined is not None:\n                use_partition.dependencies.setdefault(defined)",
            "def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    defined = getattr(def_node, '_fx_partition', None)\n    used = getattr(use_node, '_fx_partition', None)\n    if defined != used:\n        if defined is not None:\n            def_partition = partitions[defined]\n            def_partition.outputs.setdefault(def_node.name)\n            if used is not None:\n                def_partition.dependents.setdefault(used)\n        if used is not None:\n            use_partition = partitions[used]\n            use_partition.inputs.setdefault(def_node.name)\n            if defined is not None:\n                use_partition.dependencies.setdefault(defined)"
        ]
    },
    {
        "func_name": "instantiate_node_partition_mapping",
        "original": "def instantiate_node_partition_mapping(node):\n    partition_name = str(split_callback(node))\n    partition = partitions.get(partition_name)\n    if partition is None:\n        partitions[partition_name] = partition = Partition(partition_name)\n    partition.node_names.append(node.name)\n    node._fx_partition = partition_name",
        "mutated": [
            "def instantiate_node_partition_mapping(node):\n    if False:\n        i = 10\n    partition_name = str(split_callback(node))\n    partition = partitions.get(partition_name)\n    if partition is None:\n        partitions[partition_name] = partition = Partition(partition_name)\n    partition.node_names.append(node.name)\n    node._fx_partition = partition_name",
            "def instantiate_node_partition_mapping(node):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    partition_name = str(split_callback(node))\n    partition = partitions.get(partition_name)\n    if partition is None:\n        partitions[partition_name] = partition = Partition(partition_name)\n    partition.node_names.append(node.name)\n    node._fx_partition = partition_name",
            "def instantiate_node_partition_mapping(node):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    partition_name = str(split_callback(node))\n    partition = partitions.get(partition_name)\n    if partition is None:\n        partitions[partition_name] = partition = Partition(partition_name)\n    partition.node_names.append(node.name)\n    node._fx_partition = partition_name",
            "def instantiate_node_partition_mapping(node):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    partition_name = str(split_callback(node))\n    partition = partitions.get(partition_name)\n    if partition is None:\n        partitions[partition_name] = partition = Partition(partition_name)\n    partition.node_names.append(node.name)\n    node._fx_partition = partition_name",
            "def instantiate_node_partition_mapping(node):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    partition_name = str(split_callback(node))\n    partition = partitions.get(partition_name)\n    if partition is None:\n        partitions[partition_name] = partition = Partition(partition_name)\n    partition.node_names.append(node.name)\n    node._fx_partition = partition_name"
        ]
    },
    {
        "func_name": "split_module",
        "original": "@compatibility(is_backward_compatible=True)\ndef split_module(m: GraphModule, root_m: torch.nn.Module, split_callback: Callable[[Node], int], qualname_map: Optional[Dict[str, str]]=None, keep_original_order: Optional[bool]=False):\n    \"\"\"\n    Creates subgraphs out of main graph\n\n    Args:\n        m (GraphModule): Graph module to split\n        root_m (torch.nn.Module): root nn module. Not currently used. Included\n            because the root nn module is usually transformed via\n            torch.fx._symbolic_trace.symbolic_trace (see example below)\n        split_callback (Callable[[Node], int]): Callable function\n            that maps a given Node instance to a numeric partition identifier.\n            split_module will use this function as the policy for which operations\n            appear in which partitions in the output Module.\n        qualname_map: Optional[Dict[str, str]]: optional output parameter that returns a\n            mapping from new target names in the module after split to old target\n            names in the original module.\n        keep_original_order: Optional[bool]: keep the original order of the GraphModule\n            or use the Topological order of the new constructed GraphModule\n\n\n    Returns:\n        GraphModule: the module after split.\n\n    Example:\n\n        This is a sample setup:\n\n            import torch\n            from torch.fx.symbolic_trace import symbolic_trace\n            from torch.fx.graph_module import GraphModule\n            from torch.fx.node import Node\n            from torch.fx.passes.split_module import split_module\n\n            class MyModule(torch.nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.param = torch.nn.Parameter(torch.rand(3, 4))\n                    self.linear = torch.nn.Linear(4, 5)\n\n                def forward(self, x, y):\n                    z = self.linear(x + self.param).clamp(min=0.0, max=1.0)\n                    w = self.linear(y).clamp(min=0.0, max=1.0)\n                    return z + w\n\n            # symbolically trace model\n            my_module = MyModule()\n            my_module_traced = symbolic_trace(my_module)\n\n            # random mod partitioning\n            partition_counter = 0\n            NPARTITIONS = 3\n\n            def mod_partition(node: Node):\n                global partition_counter\n                partition = partition_counter % NPARTITIONS\n                partition_counter = (partition_counter + 1) % NPARTITIONS\n                return partition\n\n            # split module in module with submodules\n            module_with_submodules = split_module(\n                my_module_traced, my_module, mod_partition\n            )\n\n        Output looks like this. Original graph is broken into partitions\n\n            > print(module_with_submodules)\n            GraphModule(\n                (submod_0): GraphModule(\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\n                )\n                (submod_1): GraphModule(\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\n                )\n                (submod_2): GraphModule()\n            )\n\n            def forward(self, x, y):\n                param = self.param\n                submod_0 = self.submod_0(x, param, y);  x = param = y = None\n                getitem = submod_0[0]\n                getitem_1 = submod_0[1];  submod_0 = None\n                submod_1 = self.submod_1(getitem, getitem_1);  getitem = getitem_1 = None\n                getitem_2 = submod_1[0]\n                getitem_3 = submod_1[1];  submod_1 = None\n                submod_2 = self.submod_2(getitem_2, getitem_3);  getitem_2 = getitem_3 = None\n                return submod_2\n\n        Output of split module is the same as output of input traced module.\n        This is an example within a test setting:\n\n            > orig_out = my_module_traced(x, y)\n            > submodules_out = module_with_submodules(x, y)\n            > self.assertEqual(orig_out, submodules_out)\n            True\n    \"\"\"\n\n    def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n        if node.op == 'placeholder':\n            default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n            base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n            base_mod_env[node.name].meta = node.meta.copy()\n        elif node.op == 'get_attr':\n            base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n            base_mod_env[node.name].meta = node.meta.copy()\n            attr_val = m\n            for atom in node.target.split('.'):\n                if not hasattr(attr_val, atom):\n                    raise AttributeError(f'Node target {node.target} not found!')\n                attr_val = getattr(attr_val, atom)\n            base_mod_attrs[node.target] = attr_val\n        return (base_mod_env, base_mod_attrs)\n    partitions: Dict[str, Partition] = {}\n    orig_nodes: Dict[str, Node] = {}\n\n    def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n        defined = getattr(def_node, '_fx_partition', None)\n        used = getattr(use_node, '_fx_partition', None)\n        if defined != used:\n            if defined is not None:\n                def_partition = partitions[defined]\n                def_partition.outputs.setdefault(def_node.name)\n                if used is not None:\n                    def_partition.dependents.setdefault(used)\n            if used is not None:\n                use_partition = partitions[used]\n                use_partition.inputs.setdefault(def_node.name)\n                if defined is not None:\n                    use_partition.dependencies.setdefault(defined)\n\n    def instantiate_node_partition_mapping(node):\n        partition_name = str(split_callback(node))\n        partition = partitions.get(partition_name)\n        if partition is None:\n            partitions[partition_name] = partition = Partition(partition_name)\n        partition.node_names.append(node.name)\n        node._fx_partition = partition_name\n    for node in m.graph.nodes:\n        orig_nodes[node.name] = node\n        if node.op in ['placeholder', 'get_attr']:\n            continue\n        if node.op == 'output':\n            torch.fx.graph.map_arg(node.args[0], lambda n: record_cross_partition_use(n, None))\n            continue\n        instantiate_node_partition_mapping(node)\n        torch.fx.graph.map_arg(node.args, lambda def_node: record_cross_partition_use(def_node, node))\n        torch.fx.graph.map_arg(node.kwargs, lambda def_node: record_cross_partition_use(def_node, node))\n    original_partition_order = list(partitions.keys())\n    root_partitions: List[str] = []\n    for (partition_name, partition) in partitions.items():\n        if not len(partition.dependencies):\n            root_partitions.append(partition_name)\n    sorted_partitions: List[str] = []\n    while root_partitions:\n        root_partition = root_partitions.pop()\n        sorted_partitions.append(root_partition)\n        for dependent in partitions[root_partition].dependents:\n            partitions[dependent].dependencies.pop(root_partition)\n            if not partitions[dependent].dependencies:\n                root_partitions.append(dependent)\n    if len(sorted_partitions) != len(partitions):\n        raise RuntimeError('cycle exists between partitions!')\n    for partition_name in sorted_partitions:\n        partition = partitions[partition_name]\n        for inp in partition.inputs:\n            placeholder = partition.graph.placeholder(inp, type_expr=orig_nodes[inp].type)\n            placeholder.meta = orig_nodes[inp].meta.copy()\n            partition.environment[orig_nodes[inp]] = placeholder\n    for node in m.graph.nodes:\n        if hasattr(node, '_fx_partition'):\n            partition = partitions[node._fx_partition]\n            environment = partition.environment\n            gathered_args = torch.fx.graph.map_arg(node.args, lambda n: environment[n])\n            gathered_kwargs = torch.fx.graph.map_arg(node.kwargs, lambda n: environment[n])\n            if node.op not in ['call_module', 'get_attr']:\n                target = node.target\n            else:\n                target_atoms = node.target.split('.')\n                target_attr = m\n                for atom in target_atoms:\n                    if not hasattr(target_attr, atom):\n                        raise AttributeError(f'Operator target {node.target} not found!')\n                    target_attr = getattr(target_attr, atom)\n                target = '_'.join(target_atoms)\n                partition.targets[target] = target_attr\n                if qualname_map is not None:\n                    qualname = f'{partition.submod_name}.{target}'\n                    qualname_map[qualname] = node.target\n            assert isinstance(gathered_args, tuple)\n            assert isinstance(gathered_kwargs, dict)\n            new_node = partition.graph.create_node(op=node.op, target=target, args=gathered_args, kwargs=gathered_kwargs, type_expr=node.type)\n            new_node.meta = node.meta.copy()\n            partition.environment[node] = new_node\n    orig_mod_env: Dict[str, Node] = {}\n    base_mod_env: Dict[str, Node] = {}\n    base_mod_graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule] = {}\n    if not keep_original_order:\n        for node in m.graph.nodes:\n            (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n    else:\n        for node in m.graph.nodes:\n            orig_mod_env[node.name] = node\n    construct_order_partitions = sorted_partitions if not keep_original_order else original_partition_order\n    already_constructed_attr_nodes = set()\n    for partition_name in construct_order_partitions:\n        partition = partitions[partition_name]\n        output_vals = tuple((partition.environment[orig_nodes[name]] for name in partition.outputs))\n        num_output_vals = len(output_vals)\n        if num_output_vals == 1:\n            partition.graph.output(output_vals[0])\n        elif num_output_vals > 1:\n            partition.graph.output(output_vals)\n        if keep_original_order:\n            orig_mod_attr_nodes: List[Node] = [orig_mod_env[key] for key in partition.inputs]\n            for node in orig_mod_attr_nodes:\n                if node in already_constructed_attr_nodes:\n                    continue\n                (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n                already_constructed_attr_nodes.add(node)\n        base_mod_attrs[partition.submod_name] = torch.fx.graph_module.GraphModule(partition.targets, partition.graph)\n        output_val = base_mod_graph.call_module(partition.submod_name, tuple((base_mod_env[name] for name in partition.inputs)))\n        num_outputs = len(partition.outputs)\n        if num_outputs > 1:\n            output_val_proxy = torch.fx.proxy.Proxy(output_val)\n            for (i, output_name) in enumerate(partition.outputs):\n                base_mod_env[output_name] = output_val_proxy[i].node\n        elif num_outputs == 1:\n            base_mod_env[next(iter(partition.outputs))] = output_val\n    for node in m.graph.nodes:\n        if node.op == 'output':\n            base_mod_graph.output(torch.fx.graph.map_arg(node.args[0], lambda n: base_mod_env[n.name]))\n    return torch.fx.graph_module.GraphModule(base_mod_attrs, base_mod_graph)",
        "mutated": [
            "@compatibility(is_backward_compatible=True)\ndef split_module(m: GraphModule, root_m: torch.nn.Module, split_callback: Callable[[Node], int], qualname_map: Optional[Dict[str, str]]=None, keep_original_order: Optional[bool]=False):\n    if False:\n        i = 10\n    '\\n    Creates subgraphs out of main graph\\n\\n    Args:\\n        m (GraphModule): Graph module to split\\n        root_m (torch.nn.Module): root nn module. Not currently used. Included\\n            because the root nn module is usually transformed via\\n            torch.fx._symbolic_trace.symbolic_trace (see example below)\\n        split_callback (Callable[[Node], int]): Callable function\\n            that maps a given Node instance to a numeric partition identifier.\\n            split_module will use this function as the policy for which operations\\n            appear in which partitions in the output Module.\\n        qualname_map: Optional[Dict[str, str]]: optional output parameter that returns a\\n            mapping from new target names in the module after split to old target\\n            names in the original module.\\n        keep_original_order: Optional[bool]: keep the original order of the GraphModule\\n            or use the Topological order of the new constructed GraphModule\\n\\n\\n    Returns:\\n        GraphModule: the module after split.\\n\\n    Example:\\n\\n        This is a sample setup:\\n\\n            import torch\\n            from torch.fx.symbolic_trace import symbolic_trace\\n            from torch.fx.graph_module import GraphModule\\n            from torch.fx.node import Node\\n            from torch.fx.passes.split_module import split_module\\n\\n            class MyModule(torch.nn.Module):\\n                def __init__(self):\\n                    super().__init__()\\n                    self.param = torch.nn.Parameter(torch.rand(3, 4))\\n                    self.linear = torch.nn.Linear(4, 5)\\n\\n                def forward(self, x, y):\\n                    z = self.linear(x + self.param).clamp(min=0.0, max=1.0)\\n                    w = self.linear(y).clamp(min=0.0, max=1.0)\\n                    return z + w\\n\\n            # symbolically trace model\\n            my_module = MyModule()\\n            my_module_traced = symbolic_trace(my_module)\\n\\n            # random mod partitioning\\n            partition_counter = 0\\n            NPARTITIONS = 3\\n\\n            def mod_partition(node: Node):\\n                global partition_counter\\n                partition = partition_counter % NPARTITIONS\\n                partition_counter = (partition_counter + 1) % NPARTITIONS\\n                return partition\\n\\n            # split module in module with submodules\\n            module_with_submodules = split_module(\\n                my_module_traced, my_module, mod_partition\\n            )\\n\\n        Output looks like this. Original graph is broken into partitions\\n\\n            > print(module_with_submodules)\\n            GraphModule(\\n                (submod_0): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_1): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_2): GraphModule()\\n            )\\n\\n            def forward(self, x, y):\\n                param = self.param\\n                submod_0 = self.submod_0(x, param, y);  x = param = y = None\\n                getitem = submod_0[0]\\n                getitem_1 = submod_0[1];  submod_0 = None\\n                submod_1 = self.submod_1(getitem, getitem_1);  getitem = getitem_1 = None\\n                getitem_2 = submod_1[0]\\n                getitem_3 = submod_1[1];  submod_1 = None\\n                submod_2 = self.submod_2(getitem_2, getitem_3);  getitem_2 = getitem_3 = None\\n                return submod_2\\n\\n        Output of split module is the same as output of input traced module.\\n        This is an example within a test setting:\\n\\n            > orig_out = my_module_traced(x, y)\\n            > submodules_out = module_with_submodules(x, y)\\n            > self.assertEqual(orig_out, submodules_out)\\n            True\\n    '\n\n    def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n        if node.op == 'placeholder':\n            default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n            base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n            base_mod_env[node.name].meta = node.meta.copy()\n        elif node.op == 'get_attr':\n            base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n            base_mod_env[node.name].meta = node.meta.copy()\n            attr_val = m\n            for atom in node.target.split('.'):\n                if not hasattr(attr_val, atom):\n                    raise AttributeError(f'Node target {node.target} not found!')\n                attr_val = getattr(attr_val, atom)\n            base_mod_attrs[node.target] = attr_val\n        return (base_mod_env, base_mod_attrs)\n    partitions: Dict[str, Partition] = {}\n    orig_nodes: Dict[str, Node] = {}\n\n    def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n        defined = getattr(def_node, '_fx_partition', None)\n        used = getattr(use_node, '_fx_partition', None)\n        if defined != used:\n            if defined is not None:\n                def_partition = partitions[defined]\n                def_partition.outputs.setdefault(def_node.name)\n                if used is not None:\n                    def_partition.dependents.setdefault(used)\n            if used is not None:\n                use_partition = partitions[used]\n                use_partition.inputs.setdefault(def_node.name)\n                if defined is not None:\n                    use_partition.dependencies.setdefault(defined)\n\n    def instantiate_node_partition_mapping(node):\n        partition_name = str(split_callback(node))\n        partition = partitions.get(partition_name)\n        if partition is None:\n            partitions[partition_name] = partition = Partition(partition_name)\n        partition.node_names.append(node.name)\n        node._fx_partition = partition_name\n    for node in m.graph.nodes:\n        orig_nodes[node.name] = node\n        if node.op in ['placeholder', 'get_attr']:\n            continue\n        if node.op == 'output':\n            torch.fx.graph.map_arg(node.args[0], lambda n: record_cross_partition_use(n, None))\n            continue\n        instantiate_node_partition_mapping(node)\n        torch.fx.graph.map_arg(node.args, lambda def_node: record_cross_partition_use(def_node, node))\n        torch.fx.graph.map_arg(node.kwargs, lambda def_node: record_cross_partition_use(def_node, node))\n    original_partition_order = list(partitions.keys())\n    root_partitions: List[str] = []\n    for (partition_name, partition) in partitions.items():\n        if not len(partition.dependencies):\n            root_partitions.append(partition_name)\n    sorted_partitions: List[str] = []\n    while root_partitions:\n        root_partition = root_partitions.pop()\n        sorted_partitions.append(root_partition)\n        for dependent in partitions[root_partition].dependents:\n            partitions[dependent].dependencies.pop(root_partition)\n            if not partitions[dependent].dependencies:\n                root_partitions.append(dependent)\n    if len(sorted_partitions) != len(partitions):\n        raise RuntimeError('cycle exists between partitions!')\n    for partition_name in sorted_partitions:\n        partition = partitions[partition_name]\n        for inp in partition.inputs:\n            placeholder = partition.graph.placeholder(inp, type_expr=orig_nodes[inp].type)\n            placeholder.meta = orig_nodes[inp].meta.copy()\n            partition.environment[orig_nodes[inp]] = placeholder\n    for node in m.graph.nodes:\n        if hasattr(node, '_fx_partition'):\n            partition = partitions[node._fx_partition]\n            environment = partition.environment\n            gathered_args = torch.fx.graph.map_arg(node.args, lambda n: environment[n])\n            gathered_kwargs = torch.fx.graph.map_arg(node.kwargs, lambda n: environment[n])\n            if node.op not in ['call_module', 'get_attr']:\n                target = node.target\n            else:\n                target_atoms = node.target.split('.')\n                target_attr = m\n                for atom in target_atoms:\n                    if not hasattr(target_attr, atom):\n                        raise AttributeError(f'Operator target {node.target} not found!')\n                    target_attr = getattr(target_attr, atom)\n                target = '_'.join(target_atoms)\n                partition.targets[target] = target_attr\n                if qualname_map is not None:\n                    qualname = f'{partition.submod_name}.{target}'\n                    qualname_map[qualname] = node.target\n            assert isinstance(gathered_args, tuple)\n            assert isinstance(gathered_kwargs, dict)\n            new_node = partition.graph.create_node(op=node.op, target=target, args=gathered_args, kwargs=gathered_kwargs, type_expr=node.type)\n            new_node.meta = node.meta.copy()\n            partition.environment[node] = new_node\n    orig_mod_env: Dict[str, Node] = {}\n    base_mod_env: Dict[str, Node] = {}\n    base_mod_graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule] = {}\n    if not keep_original_order:\n        for node in m.graph.nodes:\n            (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n    else:\n        for node in m.graph.nodes:\n            orig_mod_env[node.name] = node\n    construct_order_partitions = sorted_partitions if not keep_original_order else original_partition_order\n    already_constructed_attr_nodes = set()\n    for partition_name in construct_order_partitions:\n        partition = partitions[partition_name]\n        output_vals = tuple((partition.environment[orig_nodes[name]] for name in partition.outputs))\n        num_output_vals = len(output_vals)\n        if num_output_vals == 1:\n            partition.graph.output(output_vals[0])\n        elif num_output_vals > 1:\n            partition.graph.output(output_vals)\n        if keep_original_order:\n            orig_mod_attr_nodes: List[Node] = [orig_mod_env[key] for key in partition.inputs]\n            for node in orig_mod_attr_nodes:\n                if node in already_constructed_attr_nodes:\n                    continue\n                (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n                already_constructed_attr_nodes.add(node)\n        base_mod_attrs[partition.submod_name] = torch.fx.graph_module.GraphModule(partition.targets, partition.graph)\n        output_val = base_mod_graph.call_module(partition.submod_name, tuple((base_mod_env[name] for name in partition.inputs)))\n        num_outputs = len(partition.outputs)\n        if num_outputs > 1:\n            output_val_proxy = torch.fx.proxy.Proxy(output_val)\n            for (i, output_name) in enumerate(partition.outputs):\n                base_mod_env[output_name] = output_val_proxy[i].node\n        elif num_outputs == 1:\n            base_mod_env[next(iter(partition.outputs))] = output_val\n    for node in m.graph.nodes:\n        if node.op == 'output':\n            base_mod_graph.output(torch.fx.graph.map_arg(node.args[0], lambda n: base_mod_env[n.name]))\n    return torch.fx.graph_module.GraphModule(base_mod_attrs, base_mod_graph)",
            "@compatibility(is_backward_compatible=True)\ndef split_module(m: GraphModule, root_m: torch.nn.Module, split_callback: Callable[[Node], int], qualname_map: Optional[Dict[str, str]]=None, keep_original_order: Optional[bool]=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Creates subgraphs out of main graph\\n\\n    Args:\\n        m (GraphModule): Graph module to split\\n        root_m (torch.nn.Module): root nn module. Not currently used. Included\\n            because the root nn module is usually transformed via\\n            torch.fx._symbolic_trace.symbolic_trace (see example below)\\n        split_callback (Callable[[Node], int]): Callable function\\n            that maps a given Node instance to a numeric partition identifier.\\n            split_module will use this function as the policy for which operations\\n            appear in which partitions in the output Module.\\n        qualname_map: Optional[Dict[str, str]]: optional output parameter that returns a\\n            mapping from new target names in the module after split to old target\\n            names in the original module.\\n        keep_original_order: Optional[bool]: keep the original order of the GraphModule\\n            or use the Topological order of the new constructed GraphModule\\n\\n\\n    Returns:\\n        GraphModule: the module after split.\\n\\n    Example:\\n\\n        This is a sample setup:\\n\\n            import torch\\n            from torch.fx.symbolic_trace import symbolic_trace\\n            from torch.fx.graph_module import GraphModule\\n            from torch.fx.node import Node\\n            from torch.fx.passes.split_module import split_module\\n\\n            class MyModule(torch.nn.Module):\\n                def __init__(self):\\n                    super().__init__()\\n                    self.param = torch.nn.Parameter(torch.rand(3, 4))\\n                    self.linear = torch.nn.Linear(4, 5)\\n\\n                def forward(self, x, y):\\n                    z = self.linear(x + self.param).clamp(min=0.0, max=1.0)\\n                    w = self.linear(y).clamp(min=0.0, max=1.0)\\n                    return z + w\\n\\n            # symbolically trace model\\n            my_module = MyModule()\\n            my_module_traced = symbolic_trace(my_module)\\n\\n            # random mod partitioning\\n            partition_counter = 0\\n            NPARTITIONS = 3\\n\\n            def mod_partition(node: Node):\\n                global partition_counter\\n                partition = partition_counter % NPARTITIONS\\n                partition_counter = (partition_counter + 1) % NPARTITIONS\\n                return partition\\n\\n            # split module in module with submodules\\n            module_with_submodules = split_module(\\n                my_module_traced, my_module, mod_partition\\n            )\\n\\n        Output looks like this. Original graph is broken into partitions\\n\\n            > print(module_with_submodules)\\n            GraphModule(\\n                (submod_0): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_1): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_2): GraphModule()\\n            )\\n\\n            def forward(self, x, y):\\n                param = self.param\\n                submod_0 = self.submod_0(x, param, y);  x = param = y = None\\n                getitem = submod_0[0]\\n                getitem_1 = submod_0[1];  submod_0 = None\\n                submod_1 = self.submod_1(getitem, getitem_1);  getitem = getitem_1 = None\\n                getitem_2 = submod_1[0]\\n                getitem_3 = submod_1[1];  submod_1 = None\\n                submod_2 = self.submod_2(getitem_2, getitem_3);  getitem_2 = getitem_3 = None\\n                return submod_2\\n\\n        Output of split module is the same as output of input traced module.\\n        This is an example within a test setting:\\n\\n            > orig_out = my_module_traced(x, y)\\n            > submodules_out = module_with_submodules(x, y)\\n            > self.assertEqual(orig_out, submodules_out)\\n            True\\n    '\n\n    def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n        if node.op == 'placeholder':\n            default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n            base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n            base_mod_env[node.name].meta = node.meta.copy()\n        elif node.op == 'get_attr':\n            base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n            base_mod_env[node.name].meta = node.meta.copy()\n            attr_val = m\n            for atom in node.target.split('.'):\n                if not hasattr(attr_val, atom):\n                    raise AttributeError(f'Node target {node.target} not found!')\n                attr_val = getattr(attr_val, atom)\n            base_mod_attrs[node.target] = attr_val\n        return (base_mod_env, base_mod_attrs)\n    partitions: Dict[str, Partition] = {}\n    orig_nodes: Dict[str, Node] = {}\n\n    def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n        defined = getattr(def_node, '_fx_partition', None)\n        used = getattr(use_node, '_fx_partition', None)\n        if defined != used:\n            if defined is not None:\n                def_partition = partitions[defined]\n                def_partition.outputs.setdefault(def_node.name)\n                if used is not None:\n                    def_partition.dependents.setdefault(used)\n            if used is not None:\n                use_partition = partitions[used]\n                use_partition.inputs.setdefault(def_node.name)\n                if defined is not None:\n                    use_partition.dependencies.setdefault(defined)\n\n    def instantiate_node_partition_mapping(node):\n        partition_name = str(split_callback(node))\n        partition = partitions.get(partition_name)\n        if partition is None:\n            partitions[partition_name] = partition = Partition(partition_name)\n        partition.node_names.append(node.name)\n        node._fx_partition = partition_name\n    for node in m.graph.nodes:\n        orig_nodes[node.name] = node\n        if node.op in ['placeholder', 'get_attr']:\n            continue\n        if node.op == 'output':\n            torch.fx.graph.map_arg(node.args[0], lambda n: record_cross_partition_use(n, None))\n            continue\n        instantiate_node_partition_mapping(node)\n        torch.fx.graph.map_arg(node.args, lambda def_node: record_cross_partition_use(def_node, node))\n        torch.fx.graph.map_arg(node.kwargs, lambda def_node: record_cross_partition_use(def_node, node))\n    original_partition_order = list(partitions.keys())\n    root_partitions: List[str] = []\n    for (partition_name, partition) in partitions.items():\n        if not len(partition.dependencies):\n            root_partitions.append(partition_name)\n    sorted_partitions: List[str] = []\n    while root_partitions:\n        root_partition = root_partitions.pop()\n        sorted_partitions.append(root_partition)\n        for dependent in partitions[root_partition].dependents:\n            partitions[dependent].dependencies.pop(root_partition)\n            if not partitions[dependent].dependencies:\n                root_partitions.append(dependent)\n    if len(sorted_partitions) != len(partitions):\n        raise RuntimeError('cycle exists between partitions!')\n    for partition_name in sorted_partitions:\n        partition = partitions[partition_name]\n        for inp in partition.inputs:\n            placeholder = partition.graph.placeholder(inp, type_expr=orig_nodes[inp].type)\n            placeholder.meta = orig_nodes[inp].meta.copy()\n            partition.environment[orig_nodes[inp]] = placeholder\n    for node in m.graph.nodes:\n        if hasattr(node, '_fx_partition'):\n            partition = partitions[node._fx_partition]\n            environment = partition.environment\n            gathered_args = torch.fx.graph.map_arg(node.args, lambda n: environment[n])\n            gathered_kwargs = torch.fx.graph.map_arg(node.kwargs, lambda n: environment[n])\n            if node.op not in ['call_module', 'get_attr']:\n                target = node.target\n            else:\n                target_atoms = node.target.split('.')\n                target_attr = m\n                for atom in target_atoms:\n                    if not hasattr(target_attr, atom):\n                        raise AttributeError(f'Operator target {node.target} not found!')\n                    target_attr = getattr(target_attr, atom)\n                target = '_'.join(target_atoms)\n                partition.targets[target] = target_attr\n                if qualname_map is not None:\n                    qualname = f'{partition.submod_name}.{target}'\n                    qualname_map[qualname] = node.target\n            assert isinstance(gathered_args, tuple)\n            assert isinstance(gathered_kwargs, dict)\n            new_node = partition.graph.create_node(op=node.op, target=target, args=gathered_args, kwargs=gathered_kwargs, type_expr=node.type)\n            new_node.meta = node.meta.copy()\n            partition.environment[node] = new_node\n    orig_mod_env: Dict[str, Node] = {}\n    base_mod_env: Dict[str, Node] = {}\n    base_mod_graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule] = {}\n    if not keep_original_order:\n        for node in m.graph.nodes:\n            (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n    else:\n        for node in m.graph.nodes:\n            orig_mod_env[node.name] = node\n    construct_order_partitions = sorted_partitions if not keep_original_order else original_partition_order\n    already_constructed_attr_nodes = set()\n    for partition_name in construct_order_partitions:\n        partition = partitions[partition_name]\n        output_vals = tuple((partition.environment[orig_nodes[name]] for name in partition.outputs))\n        num_output_vals = len(output_vals)\n        if num_output_vals == 1:\n            partition.graph.output(output_vals[0])\n        elif num_output_vals > 1:\n            partition.graph.output(output_vals)\n        if keep_original_order:\n            orig_mod_attr_nodes: List[Node] = [orig_mod_env[key] for key in partition.inputs]\n            for node in orig_mod_attr_nodes:\n                if node in already_constructed_attr_nodes:\n                    continue\n                (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n                already_constructed_attr_nodes.add(node)\n        base_mod_attrs[partition.submod_name] = torch.fx.graph_module.GraphModule(partition.targets, partition.graph)\n        output_val = base_mod_graph.call_module(partition.submod_name, tuple((base_mod_env[name] for name in partition.inputs)))\n        num_outputs = len(partition.outputs)\n        if num_outputs > 1:\n            output_val_proxy = torch.fx.proxy.Proxy(output_val)\n            for (i, output_name) in enumerate(partition.outputs):\n                base_mod_env[output_name] = output_val_proxy[i].node\n        elif num_outputs == 1:\n            base_mod_env[next(iter(partition.outputs))] = output_val\n    for node in m.graph.nodes:\n        if node.op == 'output':\n            base_mod_graph.output(torch.fx.graph.map_arg(node.args[0], lambda n: base_mod_env[n.name]))\n    return torch.fx.graph_module.GraphModule(base_mod_attrs, base_mod_graph)",
            "@compatibility(is_backward_compatible=True)\ndef split_module(m: GraphModule, root_m: torch.nn.Module, split_callback: Callable[[Node], int], qualname_map: Optional[Dict[str, str]]=None, keep_original_order: Optional[bool]=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Creates subgraphs out of main graph\\n\\n    Args:\\n        m (GraphModule): Graph module to split\\n        root_m (torch.nn.Module): root nn module. Not currently used. Included\\n            because the root nn module is usually transformed via\\n            torch.fx._symbolic_trace.symbolic_trace (see example below)\\n        split_callback (Callable[[Node], int]): Callable function\\n            that maps a given Node instance to a numeric partition identifier.\\n            split_module will use this function as the policy for which operations\\n            appear in which partitions in the output Module.\\n        qualname_map: Optional[Dict[str, str]]: optional output parameter that returns a\\n            mapping from new target names in the module after split to old target\\n            names in the original module.\\n        keep_original_order: Optional[bool]: keep the original order of the GraphModule\\n            or use the Topological order of the new constructed GraphModule\\n\\n\\n    Returns:\\n        GraphModule: the module after split.\\n\\n    Example:\\n\\n        This is a sample setup:\\n\\n            import torch\\n            from torch.fx.symbolic_trace import symbolic_trace\\n            from torch.fx.graph_module import GraphModule\\n            from torch.fx.node import Node\\n            from torch.fx.passes.split_module import split_module\\n\\n            class MyModule(torch.nn.Module):\\n                def __init__(self):\\n                    super().__init__()\\n                    self.param = torch.nn.Parameter(torch.rand(3, 4))\\n                    self.linear = torch.nn.Linear(4, 5)\\n\\n                def forward(self, x, y):\\n                    z = self.linear(x + self.param).clamp(min=0.0, max=1.0)\\n                    w = self.linear(y).clamp(min=0.0, max=1.0)\\n                    return z + w\\n\\n            # symbolically trace model\\n            my_module = MyModule()\\n            my_module_traced = symbolic_trace(my_module)\\n\\n            # random mod partitioning\\n            partition_counter = 0\\n            NPARTITIONS = 3\\n\\n            def mod_partition(node: Node):\\n                global partition_counter\\n                partition = partition_counter % NPARTITIONS\\n                partition_counter = (partition_counter + 1) % NPARTITIONS\\n                return partition\\n\\n            # split module in module with submodules\\n            module_with_submodules = split_module(\\n                my_module_traced, my_module, mod_partition\\n            )\\n\\n        Output looks like this. Original graph is broken into partitions\\n\\n            > print(module_with_submodules)\\n            GraphModule(\\n                (submod_0): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_1): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_2): GraphModule()\\n            )\\n\\n            def forward(self, x, y):\\n                param = self.param\\n                submod_0 = self.submod_0(x, param, y);  x = param = y = None\\n                getitem = submod_0[0]\\n                getitem_1 = submod_0[1];  submod_0 = None\\n                submod_1 = self.submod_1(getitem, getitem_1);  getitem = getitem_1 = None\\n                getitem_2 = submod_1[0]\\n                getitem_3 = submod_1[1];  submod_1 = None\\n                submod_2 = self.submod_2(getitem_2, getitem_3);  getitem_2 = getitem_3 = None\\n                return submod_2\\n\\n        Output of split module is the same as output of input traced module.\\n        This is an example within a test setting:\\n\\n            > orig_out = my_module_traced(x, y)\\n            > submodules_out = module_with_submodules(x, y)\\n            > self.assertEqual(orig_out, submodules_out)\\n            True\\n    '\n\n    def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n        if node.op == 'placeholder':\n            default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n            base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n            base_mod_env[node.name].meta = node.meta.copy()\n        elif node.op == 'get_attr':\n            base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n            base_mod_env[node.name].meta = node.meta.copy()\n            attr_val = m\n            for atom in node.target.split('.'):\n                if not hasattr(attr_val, atom):\n                    raise AttributeError(f'Node target {node.target} not found!')\n                attr_val = getattr(attr_val, atom)\n            base_mod_attrs[node.target] = attr_val\n        return (base_mod_env, base_mod_attrs)\n    partitions: Dict[str, Partition] = {}\n    orig_nodes: Dict[str, Node] = {}\n\n    def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n        defined = getattr(def_node, '_fx_partition', None)\n        used = getattr(use_node, '_fx_partition', None)\n        if defined != used:\n            if defined is not None:\n                def_partition = partitions[defined]\n                def_partition.outputs.setdefault(def_node.name)\n                if used is not None:\n                    def_partition.dependents.setdefault(used)\n            if used is not None:\n                use_partition = partitions[used]\n                use_partition.inputs.setdefault(def_node.name)\n                if defined is not None:\n                    use_partition.dependencies.setdefault(defined)\n\n    def instantiate_node_partition_mapping(node):\n        partition_name = str(split_callback(node))\n        partition = partitions.get(partition_name)\n        if partition is None:\n            partitions[partition_name] = partition = Partition(partition_name)\n        partition.node_names.append(node.name)\n        node._fx_partition = partition_name\n    for node in m.graph.nodes:\n        orig_nodes[node.name] = node\n        if node.op in ['placeholder', 'get_attr']:\n            continue\n        if node.op == 'output':\n            torch.fx.graph.map_arg(node.args[0], lambda n: record_cross_partition_use(n, None))\n            continue\n        instantiate_node_partition_mapping(node)\n        torch.fx.graph.map_arg(node.args, lambda def_node: record_cross_partition_use(def_node, node))\n        torch.fx.graph.map_arg(node.kwargs, lambda def_node: record_cross_partition_use(def_node, node))\n    original_partition_order = list(partitions.keys())\n    root_partitions: List[str] = []\n    for (partition_name, partition) in partitions.items():\n        if not len(partition.dependencies):\n            root_partitions.append(partition_name)\n    sorted_partitions: List[str] = []\n    while root_partitions:\n        root_partition = root_partitions.pop()\n        sorted_partitions.append(root_partition)\n        for dependent in partitions[root_partition].dependents:\n            partitions[dependent].dependencies.pop(root_partition)\n            if not partitions[dependent].dependencies:\n                root_partitions.append(dependent)\n    if len(sorted_partitions) != len(partitions):\n        raise RuntimeError('cycle exists between partitions!')\n    for partition_name in sorted_partitions:\n        partition = partitions[partition_name]\n        for inp in partition.inputs:\n            placeholder = partition.graph.placeholder(inp, type_expr=orig_nodes[inp].type)\n            placeholder.meta = orig_nodes[inp].meta.copy()\n            partition.environment[orig_nodes[inp]] = placeholder\n    for node in m.graph.nodes:\n        if hasattr(node, '_fx_partition'):\n            partition = partitions[node._fx_partition]\n            environment = partition.environment\n            gathered_args = torch.fx.graph.map_arg(node.args, lambda n: environment[n])\n            gathered_kwargs = torch.fx.graph.map_arg(node.kwargs, lambda n: environment[n])\n            if node.op not in ['call_module', 'get_attr']:\n                target = node.target\n            else:\n                target_atoms = node.target.split('.')\n                target_attr = m\n                for atom in target_atoms:\n                    if not hasattr(target_attr, atom):\n                        raise AttributeError(f'Operator target {node.target} not found!')\n                    target_attr = getattr(target_attr, atom)\n                target = '_'.join(target_atoms)\n                partition.targets[target] = target_attr\n                if qualname_map is not None:\n                    qualname = f'{partition.submod_name}.{target}'\n                    qualname_map[qualname] = node.target\n            assert isinstance(gathered_args, tuple)\n            assert isinstance(gathered_kwargs, dict)\n            new_node = partition.graph.create_node(op=node.op, target=target, args=gathered_args, kwargs=gathered_kwargs, type_expr=node.type)\n            new_node.meta = node.meta.copy()\n            partition.environment[node] = new_node\n    orig_mod_env: Dict[str, Node] = {}\n    base_mod_env: Dict[str, Node] = {}\n    base_mod_graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule] = {}\n    if not keep_original_order:\n        for node in m.graph.nodes:\n            (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n    else:\n        for node in m.graph.nodes:\n            orig_mod_env[node.name] = node\n    construct_order_partitions = sorted_partitions if not keep_original_order else original_partition_order\n    already_constructed_attr_nodes = set()\n    for partition_name in construct_order_partitions:\n        partition = partitions[partition_name]\n        output_vals = tuple((partition.environment[orig_nodes[name]] for name in partition.outputs))\n        num_output_vals = len(output_vals)\n        if num_output_vals == 1:\n            partition.graph.output(output_vals[0])\n        elif num_output_vals > 1:\n            partition.graph.output(output_vals)\n        if keep_original_order:\n            orig_mod_attr_nodes: List[Node] = [orig_mod_env[key] for key in partition.inputs]\n            for node in orig_mod_attr_nodes:\n                if node in already_constructed_attr_nodes:\n                    continue\n                (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n                already_constructed_attr_nodes.add(node)\n        base_mod_attrs[partition.submod_name] = torch.fx.graph_module.GraphModule(partition.targets, partition.graph)\n        output_val = base_mod_graph.call_module(partition.submod_name, tuple((base_mod_env[name] for name in partition.inputs)))\n        num_outputs = len(partition.outputs)\n        if num_outputs > 1:\n            output_val_proxy = torch.fx.proxy.Proxy(output_val)\n            for (i, output_name) in enumerate(partition.outputs):\n                base_mod_env[output_name] = output_val_proxy[i].node\n        elif num_outputs == 1:\n            base_mod_env[next(iter(partition.outputs))] = output_val\n    for node in m.graph.nodes:\n        if node.op == 'output':\n            base_mod_graph.output(torch.fx.graph.map_arg(node.args[0], lambda n: base_mod_env[n.name]))\n    return torch.fx.graph_module.GraphModule(base_mod_attrs, base_mod_graph)",
            "@compatibility(is_backward_compatible=True)\ndef split_module(m: GraphModule, root_m: torch.nn.Module, split_callback: Callable[[Node], int], qualname_map: Optional[Dict[str, str]]=None, keep_original_order: Optional[bool]=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Creates subgraphs out of main graph\\n\\n    Args:\\n        m (GraphModule): Graph module to split\\n        root_m (torch.nn.Module): root nn module. Not currently used. Included\\n            because the root nn module is usually transformed via\\n            torch.fx._symbolic_trace.symbolic_trace (see example below)\\n        split_callback (Callable[[Node], int]): Callable function\\n            that maps a given Node instance to a numeric partition identifier.\\n            split_module will use this function as the policy for which operations\\n            appear in which partitions in the output Module.\\n        qualname_map: Optional[Dict[str, str]]: optional output parameter that returns a\\n            mapping from new target names in the module after split to old target\\n            names in the original module.\\n        keep_original_order: Optional[bool]: keep the original order of the GraphModule\\n            or use the Topological order of the new constructed GraphModule\\n\\n\\n    Returns:\\n        GraphModule: the module after split.\\n\\n    Example:\\n\\n        This is a sample setup:\\n\\n            import torch\\n            from torch.fx.symbolic_trace import symbolic_trace\\n            from torch.fx.graph_module import GraphModule\\n            from torch.fx.node import Node\\n            from torch.fx.passes.split_module import split_module\\n\\n            class MyModule(torch.nn.Module):\\n                def __init__(self):\\n                    super().__init__()\\n                    self.param = torch.nn.Parameter(torch.rand(3, 4))\\n                    self.linear = torch.nn.Linear(4, 5)\\n\\n                def forward(self, x, y):\\n                    z = self.linear(x + self.param).clamp(min=0.0, max=1.0)\\n                    w = self.linear(y).clamp(min=0.0, max=1.0)\\n                    return z + w\\n\\n            # symbolically trace model\\n            my_module = MyModule()\\n            my_module_traced = symbolic_trace(my_module)\\n\\n            # random mod partitioning\\n            partition_counter = 0\\n            NPARTITIONS = 3\\n\\n            def mod_partition(node: Node):\\n                global partition_counter\\n                partition = partition_counter % NPARTITIONS\\n                partition_counter = (partition_counter + 1) % NPARTITIONS\\n                return partition\\n\\n            # split module in module with submodules\\n            module_with_submodules = split_module(\\n                my_module_traced, my_module, mod_partition\\n            )\\n\\n        Output looks like this. Original graph is broken into partitions\\n\\n            > print(module_with_submodules)\\n            GraphModule(\\n                (submod_0): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_1): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_2): GraphModule()\\n            )\\n\\n            def forward(self, x, y):\\n                param = self.param\\n                submod_0 = self.submod_0(x, param, y);  x = param = y = None\\n                getitem = submod_0[0]\\n                getitem_1 = submod_0[1];  submod_0 = None\\n                submod_1 = self.submod_1(getitem, getitem_1);  getitem = getitem_1 = None\\n                getitem_2 = submod_1[0]\\n                getitem_3 = submod_1[1];  submod_1 = None\\n                submod_2 = self.submod_2(getitem_2, getitem_3);  getitem_2 = getitem_3 = None\\n                return submod_2\\n\\n        Output of split module is the same as output of input traced module.\\n        This is an example within a test setting:\\n\\n            > orig_out = my_module_traced(x, y)\\n            > submodules_out = module_with_submodules(x, y)\\n            > self.assertEqual(orig_out, submodules_out)\\n            True\\n    '\n\n    def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n        if node.op == 'placeholder':\n            default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n            base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n            base_mod_env[node.name].meta = node.meta.copy()\n        elif node.op == 'get_attr':\n            base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n            base_mod_env[node.name].meta = node.meta.copy()\n            attr_val = m\n            for atom in node.target.split('.'):\n                if not hasattr(attr_val, atom):\n                    raise AttributeError(f'Node target {node.target} not found!')\n                attr_val = getattr(attr_val, atom)\n            base_mod_attrs[node.target] = attr_val\n        return (base_mod_env, base_mod_attrs)\n    partitions: Dict[str, Partition] = {}\n    orig_nodes: Dict[str, Node] = {}\n\n    def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n        defined = getattr(def_node, '_fx_partition', None)\n        used = getattr(use_node, '_fx_partition', None)\n        if defined != used:\n            if defined is not None:\n                def_partition = partitions[defined]\n                def_partition.outputs.setdefault(def_node.name)\n                if used is not None:\n                    def_partition.dependents.setdefault(used)\n            if used is not None:\n                use_partition = partitions[used]\n                use_partition.inputs.setdefault(def_node.name)\n                if defined is not None:\n                    use_partition.dependencies.setdefault(defined)\n\n    def instantiate_node_partition_mapping(node):\n        partition_name = str(split_callback(node))\n        partition = partitions.get(partition_name)\n        if partition is None:\n            partitions[partition_name] = partition = Partition(partition_name)\n        partition.node_names.append(node.name)\n        node._fx_partition = partition_name\n    for node in m.graph.nodes:\n        orig_nodes[node.name] = node\n        if node.op in ['placeholder', 'get_attr']:\n            continue\n        if node.op == 'output':\n            torch.fx.graph.map_arg(node.args[0], lambda n: record_cross_partition_use(n, None))\n            continue\n        instantiate_node_partition_mapping(node)\n        torch.fx.graph.map_arg(node.args, lambda def_node: record_cross_partition_use(def_node, node))\n        torch.fx.graph.map_arg(node.kwargs, lambda def_node: record_cross_partition_use(def_node, node))\n    original_partition_order = list(partitions.keys())\n    root_partitions: List[str] = []\n    for (partition_name, partition) in partitions.items():\n        if not len(partition.dependencies):\n            root_partitions.append(partition_name)\n    sorted_partitions: List[str] = []\n    while root_partitions:\n        root_partition = root_partitions.pop()\n        sorted_partitions.append(root_partition)\n        for dependent in partitions[root_partition].dependents:\n            partitions[dependent].dependencies.pop(root_partition)\n            if not partitions[dependent].dependencies:\n                root_partitions.append(dependent)\n    if len(sorted_partitions) != len(partitions):\n        raise RuntimeError('cycle exists between partitions!')\n    for partition_name in sorted_partitions:\n        partition = partitions[partition_name]\n        for inp in partition.inputs:\n            placeholder = partition.graph.placeholder(inp, type_expr=orig_nodes[inp].type)\n            placeholder.meta = orig_nodes[inp].meta.copy()\n            partition.environment[orig_nodes[inp]] = placeholder\n    for node in m.graph.nodes:\n        if hasattr(node, '_fx_partition'):\n            partition = partitions[node._fx_partition]\n            environment = partition.environment\n            gathered_args = torch.fx.graph.map_arg(node.args, lambda n: environment[n])\n            gathered_kwargs = torch.fx.graph.map_arg(node.kwargs, lambda n: environment[n])\n            if node.op not in ['call_module', 'get_attr']:\n                target = node.target\n            else:\n                target_atoms = node.target.split('.')\n                target_attr = m\n                for atom in target_atoms:\n                    if not hasattr(target_attr, atom):\n                        raise AttributeError(f'Operator target {node.target} not found!')\n                    target_attr = getattr(target_attr, atom)\n                target = '_'.join(target_atoms)\n                partition.targets[target] = target_attr\n                if qualname_map is not None:\n                    qualname = f'{partition.submod_name}.{target}'\n                    qualname_map[qualname] = node.target\n            assert isinstance(gathered_args, tuple)\n            assert isinstance(gathered_kwargs, dict)\n            new_node = partition.graph.create_node(op=node.op, target=target, args=gathered_args, kwargs=gathered_kwargs, type_expr=node.type)\n            new_node.meta = node.meta.copy()\n            partition.environment[node] = new_node\n    orig_mod_env: Dict[str, Node] = {}\n    base_mod_env: Dict[str, Node] = {}\n    base_mod_graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule] = {}\n    if not keep_original_order:\n        for node in m.graph.nodes:\n            (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n    else:\n        for node in m.graph.nodes:\n            orig_mod_env[node.name] = node\n    construct_order_partitions = sorted_partitions if not keep_original_order else original_partition_order\n    already_constructed_attr_nodes = set()\n    for partition_name in construct_order_partitions:\n        partition = partitions[partition_name]\n        output_vals = tuple((partition.environment[orig_nodes[name]] for name in partition.outputs))\n        num_output_vals = len(output_vals)\n        if num_output_vals == 1:\n            partition.graph.output(output_vals[0])\n        elif num_output_vals > 1:\n            partition.graph.output(output_vals)\n        if keep_original_order:\n            orig_mod_attr_nodes: List[Node] = [orig_mod_env[key] for key in partition.inputs]\n            for node in orig_mod_attr_nodes:\n                if node in already_constructed_attr_nodes:\n                    continue\n                (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n                already_constructed_attr_nodes.add(node)\n        base_mod_attrs[partition.submod_name] = torch.fx.graph_module.GraphModule(partition.targets, partition.graph)\n        output_val = base_mod_graph.call_module(partition.submod_name, tuple((base_mod_env[name] for name in partition.inputs)))\n        num_outputs = len(partition.outputs)\n        if num_outputs > 1:\n            output_val_proxy = torch.fx.proxy.Proxy(output_val)\n            for (i, output_name) in enumerate(partition.outputs):\n                base_mod_env[output_name] = output_val_proxy[i].node\n        elif num_outputs == 1:\n            base_mod_env[next(iter(partition.outputs))] = output_val\n    for node in m.graph.nodes:\n        if node.op == 'output':\n            base_mod_graph.output(torch.fx.graph.map_arg(node.args[0], lambda n: base_mod_env[n.name]))\n    return torch.fx.graph_module.GraphModule(base_mod_attrs, base_mod_graph)",
            "@compatibility(is_backward_compatible=True)\ndef split_module(m: GraphModule, root_m: torch.nn.Module, split_callback: Callable[[Node], int], qualname_map: Optional[Dict[str, str]]=None, keep_original_order: Optional[bool]=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Creates subgraphs out of main graph\\n\\n    Args:\\n        m (GraphModule): Graph module to split\\n        root_m (torch.nn.Module): root nn module. Not currently used. Included\\n            because the root nn module is usually transformed via\\n            torch.fx._symbolic_trace.symbolic_trace (see example below)\\n        split_callback (Callable[[Node], int]): Callable function\\n            that maps a given Node instance to a numeric partition identifier.\\n            split_module will use this function as the policy for which operations\\n            appear in which partitions in the output Module.\\n        qualname_map: Optional[Dict[str, str]]: optional output parameter that returns a\\n            mapping from new target names in the module after split to old target\\n            names in the original module.\\n        keep_original_order: Optional[bool]: keep the original order of the GraphModule\\n            or use the Topological order of the new constructed GraphModule\\n\\n\\n    Returns:\\n        GraphModule: the module after split.\\n\\n    Example:\\n\\n        This is a sample setup:\\n\\n            import torch\\n            from torch.fx.symbolic_trace import symbolic_trace\\n            from torch.fx.graph_module import GraphModule\\n            from torch.fx.node import Node\\n            from torch.fx.passes.split_module import split_module\\n\\n            class MyModule(torch.nn.Module):\\n                def __init__(self):\\n                    super().__init__()\\n                    self.param = torch.nn.Parameter(torch.rand(3, 4))\\n                    self.linear = torch.nn.Linear(4, 5)\\n\\n                def forward(self, x, y):\\n                    z = self.linear(x + self.param).clamp(min=0.0, max=1.0)\\n                    w = self.linear(y).clamp(min=0.0, max=1.0)\\n                    return z + w\\n\\n            # symbolically trace model\\n            my_module = MyModule()\\n            my_module_traced = symbolic_trace(my_module)\\n\\n            # random mod partitioning\\n            partition_counter = 0\\n            NPARTITIONS = 3\\n\\n            def mod_partition(node: Node):\\n                global partition_counter\\n                partition = partition_counter % NPARTITIONS\\n                partition_counter = (partition_counter + 1) % NPARTITIONS\\n                return partition\\n\\n            # split module in module with submodules\\n            module_with_submodules = split_module(\\n                my_module_traced, my_module, mod_partition\\n            )\\n\\n        Output looks like this. Original graph is broken into partitions\\n\\n            > print(module_with_submodules)\\n            GraphModule(\\n                (submod_0): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_1): GraphModule(\\n                    (linear): Linear(in_features=4, out_features=5, bias=True)\\n                )\\n                (submod_2): GraphModule()\\n            )\\n\\n            def forward(self, x, y):\\n                param = self.param\\n                submod_0 = self.submod_0(x, param, y);  x = param = y = None\\n                getitem = submod_0[0]\\n                getitem_1 = submod_0[1];  submod_0 = None\\n                submod_1 = self.submod_1(getitem, getitem_1);  getitem = getitem_1 = None\\n                getitem_2 = submod_1[0]\\n                getitem_3 = submod_1[1];  submod_1 = None\\n                submod_2 = self.submod_2(getitem_2, getitem_3);  getitem_2 = getitem_3 = None\\n                return submod_2\\n\\n        Output of split module is the same as output of input traced module.\\n        This is an example within a test setting:\\n\\n            > orig_out = my_module_traced(x, y)\\n            > submodules_out = module_with_submodules(x, y)\\n            > self.assertEqual(orig_out, submodules_out)\\n            True\\n    '\n\n    def construct_graph(node: Node, base_mod_env: Dict[str, Node], base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule]):\n        if node.op == 'placeholder':\n            default_value = node.args[0] if len(node.args) > 0 else inspect.Signature.empty\n            base_mod_env[node.name] = base_mod_graph.placeholder(node.target, type_expr=node.type, default_value=default_value)\n            base_mod_env[node.name].meta = node.meta.copy()\n        elif node.op == 'get_attr':\n            base_mod_env[node.name] = base_mod_graph.get_attr(node.target)\n            base_mod_env[node.name].meta = node.meta.copy()\n            attr_val = m\n            for atom in node.target.split('.'):\n                if not hasattr(attr_val, atom):\n                    raise AttributeError(f'Node target {node.target} not found!')\n                attr_val = getattr(attr_val, atom)\n            base_mod_attrs[node.target] = attr_val\n        return (base_mod_env, base_mod_attrs)\n    partitions: Dict[str, Partition] = {}\n    orig_nodes: Dict[str, Node] = {}\n\n    def record_cross_partition_use(def_node: Node, use_node: Optional[Node]):\n        defined = getattr(def_node, '_fx_partition', None)\n        used = getattr(use_node, '_fx_partition', None)\n        if defined != used:\n            if defined is not None:\n                def_partition = partitions[defined]\n                def_partition.outputs.setdefault(def_node.name)\n                if used is not None:\n                    def_partition.dependents.setdefault(used)\n            if used is not None:\n                use_partition = partitions[used]\n                use_partition.inputs.setdefault(def_node.name)\n                if defined is not None:\n                    use_partition.dependencies.setdefault(defined)\n\n    def instantiate_node_partition_mapping(node):\n        partition_name = str(split_callback(node))\n        partition = partitions.get(partition_name)\n        if partition is None:\n            partitions[partition_name] = partition = Partition(partition_name)\n        partition.node_names.append(node.name)\n        node._fx_partition = partition_name\n    for node in m.graph.nodes:\n        orig_nodes[node.name] = node\n        if node.op in ['placeholder', 'get_attr']:\n            continue\n        if node.op == 'output':\n            torch.fx.graph.map_arg(node.args[0], lambda n: record_cross_partition_use(n, None))\n            continue\n        instantiate_node_partition_mapping(node)\n        torch.fx.graph.map_arg(node.args, lambda def_node: record_cross_partition_use(def_node, node))\n        torch.fx.graph.map_arg(node.kwargs, lambda def_node: record_cross_partition_use(def_node, node))\n    original_partition_order = list(partitions.keys())\n    root_partitions: List[str] = []\n    for (partition_name, partition) in partitions.items():\n        if not len(partition.dependencies):\n            root_partitions.append(partition_name)\n    sorted_partitions: List[str] = []\n    while root_partitions:\n        root_partition = root_partitions.pop()\n        sorted_partitions.append(root_partition)\n        for dependent in partitions[root_partition].dependents:\n            partitions[dependent].dependencies.pop(root_partition)\n            if not partitions[dependent].dependencies:\n                root_partitions.append(dependent)\n    if len(sorted_partitions) != len(partitions):\n        raise RuntimeError('cycle exists between partitions!')\n    for partition_name in sorted_partitions:\n        partition = partitions[partition_name]\n        for inp in partition.inputs:\n            placeholder = partition.graph.placeholder(inp, type_expr=orig_nodes[inp].type)\n            placeholder.meta = orig_nodes[inp].meta.copy()\n            partition.environment[orig_nodes[inp]] = placeholder\n    for node in m.graph.nodes:\n        if hasattr(node, '_fx_partition'):\n            partition = partitions[node._fx_partition]\n            environment = partition.environment\n            gathered_args = torch.fx.graph.map_arg(node.args, lambda n: environment[n])\n            gathered_kwargs = torch.fx.graph.map_arg(node.kwargs, lambda n: environment[n])\n            if node.op not in ['call_module', 'get_attr']:\n                target = node.target\n            else:\n                target_atoms = node.target.split('.')\n                target_attr = m\n                for atom in target_atoms:\n                    if not hasattr(target_attr, atom):\n                        raise AttributeError(f'Operator target {node.target} not found!')\n                    target_attr = getattr(target_attr, atom)\n                target = '_'.join(target_atoms)\n                partition.targets[target] = target_attr\n                if qualname_map is not None:\n                    qualname = f'{partition.submod_name}.{target}'\n                    qualname_map[qualname] = node.target\n            assert isinstance(gathered_args, tuple)\n            assert isinstance(gathered_kwargs, dict)\n            new_node = partition.graph.create_node(op=node.op, target=target, args=gathered_args, kwargs=gathered_kwargs, type_expr=node.type)\n            new_node.meta = node.meta.copy()\n            partition.environment[node] = new_node\n    orig_mod_env: Dict[str, Node] = {}\n    base_mod_env: Dict[str, Node] = {}\n    base_mod_graph: torch.fx.graph.Graph = torch.fx.graph.Graph()\n    base_mod_attrs: Dict[str, torch.fx.graph_module.GraphModule] = {}\n    if not keep_original_order:\n        for node in m.graph.nodes:\n            (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n    else:\n        for node in m.graph.nodes:\n            orig_mod_env[node.name] = node\n    construct_order_partitions = sorted_partitions if not keep_original_order else original_partition_order\n    already_constructed_attr_nodes = set()\n    for partition_name in construct_order_partitions:\n        partition = partitions[partition_name]\n        output_vals = tuple((partition.environment[orig_nodes[name]] for name in partition.outputs))\n        num_output_vals = len(output_vals)\n        if num_output_vals == 1:\n            partition.graph.output(output_vals[0])\n        elif num_output_vals > 1:\n            partition.graph.output(output_vals)\n        if keep_original_order:\n            orig_mod_attr_nodes: List[Node] = [orig_mod_env[key] for key in partition.inputs]\n            for node in orig_mod_attr_nodes:\n                if node in already_constructed_attr_nodes:\n                    continue\n                (base_mod_env, base_mod_attrs) = construct_graph(node, base_mod_env, base_mod_attrs)\n                already_constructed_attr_nodes.add(node)\n        base_mod_attrs[partition.submod_name] = torch.fx.graph_module.GraphModule(partition.targets, partition.graph)\n        output_val = base_mod_graph.call_module(partition.submod_name, tuple((base_mod_env[name] for name in partition.inputs)))\n        num_outputs = len(partition.outputs)\n        if num_outputs > 1:\n            output_val_proxy = torch.fx.proxy.Proxy(output_val)\n            for (i, output_name) in enumerate(partition.outputs):\n                base_mod_env[output_name] = output_val_proxy[i].node\n        elif num_outputs == 1:\n            base_mod_env[next(iter(partition.outputs))] = output_val\n    for node in m.graph.nodes:\n        if node.op == 'output':\n            base_mod_graph.output(torch.fx.graph.map_arg(node.args[0], lambda n: base_mod_env[n.name]))\n    return torch.fx.graph_module.GraphModule(base_mod_attrs, base_mod_graph)"
        ]
    }
]