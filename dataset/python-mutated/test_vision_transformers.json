[
    {
        "func_name": "fix_get_mnist_data",
        "original": "@pytest.fixture()\ndef fix_get_mnist_data():\n    \"\"\"\n    Get the first 128 samples of the mnist test set with channels first format\n\n    :return: First 128 sample/label pairs of the MNIST test dataset.\n    \"\"\"\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('mnist')\n    x_test = np.squeeze(x_test).astype(np.float32)\n    x_test = np.expand_dims(x_test, axis=1)\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    return (x_test, y_test)",
        "mutated": [
            "@pytest.fixture()\ndef fix_get_mnist_data():\n    if False:\n        i = 10\n    '\\n    Get the first 128 samples of the mnist test set with channels first format\\n\\n    :return: First 128 sample/label pairs of the MNIST test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('mnist')\n    x_test = np.squeeze(x_test).astype(np.float32)\n    x_test = np.expand_dims(x_test, axis=1)\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    return (x_test, y_test)",
            "@pytest.fixture()\ndef fix_get_mnist_data():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Get the first 128 samples of the mnist test set with channels first format\\n\\n    :return: First 128 sample/label pairs of the MNIST test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('mnist')\n    x_test = np.squeeze(x_test).astype(np.float32)\n    x_test = np.expand_dims(x_test, axis=1)\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    return (x_test, y_test)",
            "@pytest.fixture()\ndef fix_get_mnist_data():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Get the first 128 samples of the mnist test set with channels first format\\n\\n    :return: First 128 sample/label pairs of the MNIST test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('mnist')\n    x_test = np.squeeze(x_test).astype(np.float32)\n    x_test = np.expand_dims(x_test, axis=1)\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    return (x_test, y_test)",
            "@pytest.fixture()\ndef fix_get_mnist_data():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Get the first 128 samples of the mnist test set with channels first format\\n\\n    :return: First 128 sample/label pairs of the MNIST test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('mnist')\n    x_test = np.squeeze(x_test).astype(np.float32)\n    x_test = np.expand_dims(x_test, axis=1)\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    return (x_test, y_test)",
            "@pytest.fixture()\ndef fix_get_mnist_data():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Get the first 128 samples of the mnist test set with channels first format\\n\\n    :return: First 128 sample/label pairs of the MNIST test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('mnist')\n    x_test = np.squeeze(x_test).astype(np.float32)\n    x_test = np.expand_dims(x_test, axis=1)\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    return (x_test, y_test)"
        ]
    },
    {
        "func_name": "fix_get_cifar10_data",
        "original": "@pytest.fixture()\ndef fix_get_cifar10_data():\n    \"\"\"\n    Get the first 128 samples of the cifar10 test set\n\n    :return: First 128 sample/label pairs of the cifar10 test dataset.\n    \"\"\"\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('cifar10')\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    x_test = np.transpose(x_test, (0, 3, 1, 2))\n    return (x_test.astype(np.float32), y_test)",
        "mutated": [
            "@pytest.fixture()\ndef fix_get_cifar10_data():\n    if False:\n        i = 10\n    '\\n    Get the first 128 samples of the cifar10 test set\\n\\n    :return: First 128 sample/label pairs of the cifar10 test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('cifar10')\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    x_test = np.transpose(x_test, (0, 3, 1, 2))\n    return (x_test.astype(np.float32), y_test)",
            "@pytest.fixture()\ndef fix_get_cifar10_data():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Get the first 128 samples of the cifar10 test set\\n\\n    :return: First 128 sample/label pairs of the cifar10 test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('cifar10')\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    x_test = np.transpose(x_test, (0, 3, 1, 2))\n    return (x_test.astype(np.float32), y_test)",
            "@pytest.fixture()\ndef fix_get_cifar10_data():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Get the first 128 samples of the cifar10 test set\\n\\n    :return: First 128 sample/label pairs of the cifar10 test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('cifar10')\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    x_test = np.transpose(x_test, (0, 3, 1, 2))\n    return (x_test.astype(np.float32), y_test)",
            "@pytest.fixture()\ndef fix_get_cifar10_data():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Get the first 128 samples of the cifar10 test set\\n\\n    :return: First 128 sample/label pairs of the cifar10 test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('cifar10')\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    x_test = np.transpose(x_test, (0, 3, 1, 2))\n    return (x_test.astype(np.float32), y_test)",
            "@pytest.fixture()\ndef fix_get_cifar10_data():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Get the first 128 samples of the cifar10 test set\\n\\n    :return: First 128 sample/label pairs of the cifar10 test dataset.\\n    '\n    nb_test = 128\n    ((_, _), (x_test, y_test), _, _) = load_dataset('cifar10')\n    y_test = np.argmax(y_test, axis=1)\n    (x_test, y_test) = (x_test[:nb_test], y_test[:nb_test])\n    x_test = np.transpose(x_test, (0, 3, 1, 2))\n    return (x_test.astype(np.float32), y_test)"
        ]
    },
    {
        "func_name": "test_ablation",
        "original": "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    \"\"\"\n    Check that the ablation is being performed correctly\n    \"\"\"\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 0:10]) == 0\n        assert torch.sum(ablated[:, :, :, 10:14]) > 0\n        assert torch.sum(ablated[:, :, :, 14:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30:]) > 0\n        assert torch.sum(ablated[:, :, :, 2:30]) == 0\n        assert torch.sum(ablated[:, :, :, :2]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, :10 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, 10 * 7:14 * 7]) > 0\n        assert torch.sum(ablated[:, :, :, 14 * 7:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30 * 7:]) > 0\n        assert torch.sum(ablated[:, :, :, 2 * 7:30 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, :2 * 7]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
        "mutated": [
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 0:10]) == 0\n        assert torch.sum(ablated[:, :, :, 10:14]) > 0\n        assert torch.sum(ablated[:, :, :, 14:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30:]) > 0\n        assert torch.sum(ablated[:, :, :, 2:30]) == 0\n        assert torch.sum(ablated[:, :, :, :2]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, :10 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, 10 * 7:14 * 7]) > 0\n        assert torch.sum(ablated[:, :, :, 14 * 7:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30 * 7:]) > 0\n        assert torch.sum(ablated[:, :, :, 2 * 7:30 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, :2 * 7]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 0:10]) == 0\n        assert torch.sum(ablated[:, :, :, 10:14]) > 0\n        assert torch.sum(ablated[:, :, :, 14:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30:]) > 0\n        assert torch.sum(ablated[:, :, :, 2:30]) == 0\n        assert torch.sum(ablated[:, :, :, :2]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, :10 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, 10 * 7:14 * 7]) > 0\n        assert torch.sum(ablated[:, :, :, 14 * 7:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30 * 7:]) > 0\n        assert torch.sum(ablated[:, :, :, 2 * 7:30 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, :2 * 7]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 0:10]) == 0\n        assert torch.sum(ablated[:, :, :, 10:14]) > 0\n        assert torch.sum(ablated[:, :, :, 14:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30:]) > 0\n        assert torch.sum(ablated[:, :, :, 2:30]) == 0\n        assert torch.sum(ablated[:, :, :, :2]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, :10 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, 10 * 7:14 * 7]) > 0\n        assert torch.sum(ablated[:, :, :, 14 * 7:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30 * 7:]) > 0\n        assert torch.sum(ablated[:, :, :, 2 * 7:30 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, :2 * 7]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 0:10]) == 0\n        assert torch.sum(ablated[:, :, :, 10:14]) > 0\n        assert torch.sum(ablated[:, :, :, 14:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30:]) > 0\n        assert torch.sum(ablated[:, :, :, 2:30]) == 0\n        assert torch.sum(ablated[:, :, :, :2]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, :10 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, 10 * 7:14 * 7]) > 0\n        assert torch.sum(ablated[:, :, :, 14 * 7:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30 * 7:]) > 0\n        assert torch.sum(ablated[:, :, :, 2 * 7:30 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, :2 * 7]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 0:10]) == 0\n        assert torch.sum(ablated[:, :, :, 10:14]) > 0\n        assert torch.sum(ablated[:, :, :, 14:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30:]) > 0\n        assert torch.sum(ablated[:, :, :, 2:30]) == 0\n        assert torch.sum(ablated[:, :, :, :2]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, :10 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, 10 * 7:14 * 7]) > 0\n        assert torch.sum(ablated[:, :, :, 14 * 7:]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :, 30 * 7:]) > 0\n        assert torch.sum(ablated[:, :, :, 2 * 7:30 * 7]) == 0\n        assert torch.sum(ablated[:, :, :, :2 * 7]) > 0\n    except ARTTestException as e:\n        art_warning(e)"
        ]
    },
    {
        "func_name": "test_ablation_row",
        "original": "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation_row(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    \"\"\"\n    Check that the ablation is being performed correctly\n    \"\"\"\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 0:10, :]) == 0\n        assert torch.sum(ablated[:, :, 10:14, :]) > 0\n        assert torch.sum(ablated[:, :, 14:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30:, :]) > 0\n        assert torch.sum(ablated[:, :, 2:30, :]) == 0\n        assert torch.sum(ablated[:, :, :2, :]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :10 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, 10 * 7:14 * 7, :]) > 0\n        assert torch.sum(ablated[:, :, 14 * 7:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30 * 7:, :]) > 0\n        assert torch.sum(ablated[:, :, 2 * 7:30 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, :2 * 7, :]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
        "mutated": [
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation_row(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 0:10, :]) == 0\n        assert torch.sum(ablated[:, :, 10:14, :]) > 0\n        assert torch.sum(ablated[:, :, 14:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30:, :]) > 0\n        assert torch.sum(ablated[:, :, 2:30, :]) == 0\n        assert torch.sum(ablated[:, :, :2, :]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :10 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, 10 * 7:14 * 7, :]) > 0\n        assert torch.sum(ablated[:, :, 14 * 7:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30 * 7:, :]) > 0\n        assert torch.sum(ablated[:, :, 2 * 7:30 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, :2 * 7, :]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation_row(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 0:10, :]) == 0\n        assert torch.sum(ablated[:, :, 10:14, :]) > 0\n        assert torch.sum(ablated[:, :, 14:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30:, :]) > 0\n        assert torch.sum(ablated[:, :, 2:30, :]) == 0\n        assert torch.sum(ablated[:, :, :2, :]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :10 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, 10 * 7:14 * 7, :]) > 0\n        assert torch.sum(ablated[:, :, 14 * 7:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30 * 7:, :]) > 0\n        assert torch.sum(ablated[:, :, 2 * 7:30 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, :2 * 7, :]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation_row(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 0:10, :]) == 0\n        assert torch.sum(ablated[:, :, 10:14, :]) > 0\n        assert torch.sum(ablated[:, :, 14:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30:, :]) > 0\n        assert torch.sum(ablated[:, :, 2:30, :]) == 0\n        assert torch.sum(ablated[:, :, :2, :]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :10 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, 10 * 7:14 * 7, :]) > 0\n        assert torch.sum(ablated[:, :, 14 * 7:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30 * 7:, :]) > 0\n        assert torch.sum(ablated[:, :, 2 * 7:30 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, :2 * 7, :]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation_row(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 0:10, :]) == 0\n        assert torch.sum(ablated[:, :, 10:14, :]) > 0\n        assert torch.sum(ablated[:, :, 14:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30:, :]) > 0\n        assert torch.sum(ablated[:, :, 2:30, :]) == 0\n        assert torch.sum(ablated[:, :, :2, :]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :10 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, 10 * 7:14 * 7, :]) > 0\n        assert torch.sum(ablated[:, :, 14 * 7:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30 * 7:, :]) > 0\n        assert torch.sum(ablated[:, :, 2 * 7:30 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, :2 * 7, :]) > 0\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_ablation_row(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Check that the ablation is being performed correctly\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    try:\n        cifar_data = fix_get_cifar10_data[0]\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=False, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        cifar_data = torch.from_numpy(cifar_data).to(device)\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 0:10, :]) == 0\n        assert torch.sum(ablated[:, :, 10:14, :]) > 0\n        assert torch.sum(ablated[:, :, 14:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30:, :]) > 0\n        assert torch.sum(ablated[:, :, 2:30, :]) == 0\n        assert torch.sum(ablated[:, :, :2, :]) > 0\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', ablation_mode='row', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        ablated = col_ablator.forward(cifar_data, column_pos=10)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, :10 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, 10 * 7:14 * 7, :]) > 0\n        assert torch.sum(ablated[:, :, 14 * 7:, :]) == 0\n        ablated = col_ablator.forward(cifar_data, column_pos=30)\n        assert ablated.shape[1] == 4\n        assert torch.sum(ablated[:, :, 30 * 7:, :]) > 0\n        assert torch.sum(ablated[:, :, 2 * 7:30 * 7, :]) == 0\n        assert torch.sum(ablated[:, :, :2 * 7, :]) > 0\n    except ARTTestException as e:\n        art_warning(e)"
        ]
    },
    {
        "func_name": "test_pytorch_training",
        "original": "@pytest.mark.only_with_platform('pytorch')\ndef test_pytorch_training(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    \"\"\"\n    Check that the training loop for pytorch does not result in errors\n    \"\"\"\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=2, update_batchnorm=True, scheduler=scheduler)\n        preds = art_model.predict(cifar_data)\n        gt_preds = np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/cumulative_predictions.npy'))\n        np.array_equal(preds, gt_preds)\n    except ARTTestException as e:\n        art_warning(e)",
        "mutated": [
            "@pytest.mark.only_with_platform('pytorch')\ndef test_pytorch_training(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n    '\\n    Check that the training loop for pytorch does not result in errors\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=2, update_batchnorm=True, scheduler=scheduler)\n        preds = art_model.predict(cifar_data)\n        gt_preds = np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/cumulative_predictions.npy'))\n        np.array_equal(preds, gt_preds)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_pytorch_training(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Check that the training loop for pytorch does not result in errors\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=2, update_batchnorm=True, scheduler=scheduler)\n        preds = art_model.predict(cifar_data)\n        gt_preds = np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/cumulative_predictions.npy'))\n        np.array_equal(preds, gt_preds)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_pytorch_training(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Check that the training loop for pytorch does not result in errors\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=2, update_batchnorm=True, scheduler=scheduler)\n        preds = art_model.predict(cifar_data)\n        gt_preds = np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/cumulative_predictions.npy'))\n        np.array_equal(preds, gt_preds)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_pytorch_training(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Check that the training loop for pytorch does not result in errors\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=2, update_batchnorm=True, scheduler=scheduler)\n        preds = art_model.predict(cifar_data)\n        gt_preds = np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/cumulative_predictions.npy'))\n        np.array_equal(preds, gt_preds)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_pytorch_training(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Check that the training loop for pytorch does not result in errors\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=2, update_batchnorm=True, scheduler=scheduler)\n        preds = art_model.predict(cifar_data)\n        gt_preds = np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/cumulative_predictions.npy'))\n        np.array_equal(preds, gt_preds)\n    except ARTTestException as e:\n        art_warning(e)"
        ]
    },
    {
        "func_name": "test_certification_function",
        "original": "@pytest.mark.only_with_platform('pytorch')\ndef test_certification_function(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    \"\"\"\n    Check that based on a given set of synthetic class predictions the certification gives the expected results.\n    \"\"\"\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    import torch\n    try:\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, mode='ViT', to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        pred_counts = torch.from_numpy(np.asarray([[20, 5, 1], [10, 5, 1], [1, 16, 1]]))\n        (cert, cert_and_correct, top_predicted_class) = col_ablator.certify(pred_counts=pred_counts, size_to_certify=4, label=0)\n        assert torch.equal(cert, torch.tensor([True, False, True]))\n        assert torch.equal(cert_and_correct, torch.tensor([True, False, False]))\n    except ARTTestException as e:\n        art_warning(e)",
        "mutated": [
            "@pytest.mark.only_with_platform('pytorch')\ndef test_certification_function(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n    '\\n    Check that based on a given set of synthetic class predictions the certification gives the expected results.\\n    '\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    import torch\n    try:\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, mode='ViT', to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        pred_counts = torch.from_numpy(np.asarray([[20, 5, 1], [10, 5, 1], [1, 16, 1]]))\n        (cert, cert_and_correct, top_predicted_class) = col_ablator.certify(pred_counts=pred_counts, size_to_certify=4, label=0)\n        assert torch.equal(cert, torch.tensor([True, False, True]))\n        assert torch.equal(cert_and_correct, torch.tensor([True, False, False]))\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_certification_function(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Check that based on a given set of synthetic class predictions the certification gives the expected results.\\n    '\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    import torch\n    try:\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, mode='ViT', to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        pred_counts = torch.from_numpy(np.asarray([[20, 5, 1], [10, 5, 1], [1, 16, 1]]))\n        (cert, cert_and_correct, top_predicted_class) = col_ablator.certify(pred_counts=pred_counts, size_to_certify=4, label=0)\n        assert torch.equal(cert, torch.tensor([True, False, True]))\n        assert torch.equal(cert_and_correct, torch.tensor([True, False, False]))\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_certification_function(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Check that based on a given set of synthetic class predictions the certification gives the expected results.\\n    '\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    import torch\n    try:\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, mode='ViT', to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        pred_counts = torch.from_numpy(np.asarray([[20, 5, 1], [10, 5, 1], [1, 16, 1]]))\n        (cert, cert_and_correct, top_predicted_class) = col_ablator.certify(pred_counts=pred_counts, size_to_certify=4, label=0)\n        assert torch.equal(cert, torch.tensor([True, False, True]))\n        assert torch.equal(cert_and_correct, torch.tensor([True, False, False]))\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_certification_function(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Check that based on a given set of synthetic class predictions the certification gives the expected results.\\n    '\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    import torch\n    try:\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, mode='ViT', to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        pred_counts = torch.from_numpy(np.asarray([[20, 5, 1], [10, 5, 1], [1, 16, 1]]))\n        (cert, cert_and_correct, top_predicted_class) = col_ablator.certify(pred_counts=pred_counts, size_to_certify=4, label=0)\n        assert torch.equal(cert, torch.tensor([True, False, True]))\n        assert torch.equal(cert_and_correct, torch.tensor([True, False, False]))\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_certification_function(art_warning, fix_get_mnist_data, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Check that based on a given set of synthetic class predictions the certification gives the expected results.\\n    '\n    from art.estimators.certification.derandomized_smoothing.ablators.pytorch import ColumnAblatorPyTorch\n    import torch\n    try:\n        col_ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, mode='ViT', to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n        pred_counts = torch.from_numpy(np.asarray([[20, 5, 1], [10, 5, 1], [1, 16, 1]]))\n        (cert, cert_and_correct, top_predicted_class) = col_ablator.certify(pred_counts=pred_counts, size_to_certify=4, label=0)\n        assert torch.equal(cert, torch.tensor([True, False, True]))\n        assert torch.equal(cert_and_correct, torch.tensor([True, False, False]))\n    except ARTTestException as e:\n        art_warning(e)"
        ]
    },
    {
        "func_name": "test_end_to_end_equivalence",
        "original": "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_end_to_end_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    \"\"\"\n    Assert implementations matches original with a forward pass through the same model architecture.\n    There are some differences in architecture between the same model names in timm vs the original implementation.\n    We use vit_base_patch16_224 which matches.\n    \"\"\"\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.ablators import ColumnAblatorPyTorch, BlockAblatorPyTorch\n    cifar_data = fix_get_cifar10_data[0][:50]\n    torch.manual_seed(1234)\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_base_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        if ablation == 'column':\n            ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n            ablated = ablator.forward(cifar_data, column_pos=10)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_column.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n        elif ablation == 'block':\n            ablator = BlockAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224), mode='ViT')\n            ablated = ablator.forward(cifar_data, column_pos=10, row_pos=28)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_block.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n    except ARTTestException as e:\n        art_warning(e)",
        "mutated": [
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_end_to_end_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n    '\\n    Assert implementations matches original with a forward pass through the same model architecture.\\n    There are some differences in architecture between the same model names in timm vs the original implementation.\\n    We use vit_base_patch16_224 which matches.\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.ablators import ColumnAblatorPyTorch, BlockAblatorPyTorch\n    cifar_data = fix_get_cifar10_data[0][:50]\n    torch.manual_seed(1234)\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_base_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        if ablation == 'column':\n            ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n            ablated = ablator.forward(cifar_data, column_pos=10)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_column.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n        elif ablation == 'block':\n            ablator = BlockAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224), mode='ViT')\n            ablated = ablator.forward(cifar_data, column_pos=10, row_pos=28)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_block.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_end_to_end_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Assert implementations matches original with a forward pass through the same model architecture.\\n    There are some differences in architecture between the same model names in timm vs the original implementation.\\n    We use vit_base_patch16_224 which matches.\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.ablators import ColumnAblatorPyTorch, BlockAblatorPyTorch\n    cifar_data = fix_get_cifar10_data[0][:50]\n    torch.manual_seed(1234)\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_base_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        if ablation == 'column':\n            ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n            ablated = ablator.forward(cifar_data, column_pos=10)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_column.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n        elif ablation == 'block':\n            ablator = BlockAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224), mode='ViT')\n            ablated = ablator.forward(cifar_data, column_pos=10, row_pos=28)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_block.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_end_to_end_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Assert implementations matches original with a forward pass through the same model architecture.\\n    There are some differences in architecture between the same model names in timm vs the original implementation.\\n    We use vit_base_patch16_224 which matches.\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.ablators import ColumnAblatorPyTorch, BlockAblatorPyTorch\n    cifar_data = fix_get_cifar10_data[0][:50]\n    torch.manual_seed(1234)\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_base_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        if ablation == 'column':\n            ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n            ablated = ablator.forward(cifar_data, column_pos=10)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_column.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n        elif ablation == 'block':\n            ablator = BlockAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224), mode='ViT')\n            ablated = ablator.forward(cifar_data, column_pos=10, row_pos=28)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_block.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_end_to_end_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Assert implementations matches original with a forward pass through the same model architecture.\\n    There are some differences in architecture between the same model names in timm vs the original implementation.\\n    We use vit_base_patch16_224 which matches.\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.ablators import ColumnAblatorPyTorch, BlockAblatorPyTorch\n    cifar_data = fix_get_cifar10_data[0][:50]\n    torch.manual_seed(1234)\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_base_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        if ablation == 'column':\n            ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n            ablated = ablator.forward(cifar_data, column_pos=10)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_column.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n        elif ablation == 'block':\n            ablator = BlockAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224), mode='ViT')\n            ablated = ablator.forward(cifar_data, column_pos=10, row_pos=28)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_block.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_end_to_end_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Assert implementations matches original with a forward pass through the same model architecture.\\n    There are some differences in architecture between the same model names in timm vs the original implementation.\\n    We use vit_base_patch16_224 which matches.\\n    '\n    import torch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.ablators import ColumnAblatorPyTorch, BlockAblatorPyTorch\n    cifar_data = fix_get_cifar10_data[0][:50]\n    torch.manual_seed(1234)\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_base_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        if ablation == 'column':\n            ablator = ColumnAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, mode='ViT', original_shape=(3, 32, 32), output_shape=(3, 224, 224))\n            ablated = ablator.forward(cifar_data, column_pos=10)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_column.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n        elif ablation == 'block':\n            ablator = BlockAblatorPyTorch(ablation_size=4, channels_first=True, to_reshape=True, original_shape=(3, 32, 32), output_shape=(3, 224, 224), mode='ViT')\n            ablated = ablator.forward(cifar_data, column_pos=10, row_pos=28)\n            madry_preds = torch.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/madry_preds_block.pt'))\n            art_preds = art_model.model(ablated)\n            assert torch.allclose(madry_preds.to(device), art_preds, rtol=0.0001, atol=0.0001)\n    except ARTTestException as e:\n        art_warning(e)"
        ]
    },
    {
        "func_name": "test_certification_equivalence",
        "original": "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_certification_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    \"\"\"\n    With the forward pass equivalence asserted, we now confirm that the certification functions in the same\n    way by doing a full end to end prediction and certification test over the data.\n    \"\"\"\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_type=ablation, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        if torch.cuda.is_available():\n            num_to_fetch = 100\n        else:\n            num_to_fetch = 10\n        cifar_data = torch.from_numpy(fix_get_cifar10_data[0][:num_to_fetch]).to(device)\n        cifar_labels = torch.from_numpy(fix_get_cifar10_data[1][:num_to_fetch]).to(device)\n        (acc, cert_acc) = art_model.eval_and_certify(x=cifar_data.cpu().numpy(), y=cifar_labels.cpu().numpy(), batch_size=num_to_fetch, size_to_certify=4)\n        upsample = torch.nn.Upsample(scale_factor=224 / 32)\n        cifar_data = upsample(cifar_data)\n        acc_non_ablation = art_model.model(cifar_data)\n        acc_non_ablation = art_model.get_accuracy(acc_non_ablation, cifar_labels)\n        if torch.cuda.is_available():\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.29)\n                assert np.allclose(acc.cpu().numpy(), 0.57)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.16)\n                assert np.allclose(acc.cpu().numpy(), 0.24)\n            assert np.allclose(acc_non_ablation, 0.52)\n        else:\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.3)\n                assert np.allclose(acc.cpu().numpy(), 0.7)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.2)\n                assert np.allclose(acc.cpu().numpy(), 0.2)\n            assert np.allclose(acc_non_ablation, 0.6)\n    except ARTTestException as e:\n        art_warning(e)",
        "mutated": [
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_certification_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n    '\\n    With the forward pass equivalence asserted, we now confirm that the certification functions in the same\\n    way by doing a full end to end prediction and certification test over the data.\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_type=ablation, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        if torch.cuda.is_available():\n            num_to_fetch = 100\n        else:\n            num_to_fetch = 10\n        cifar_data = torch.from_numpy(fix_get_cifar10_data[0][:num_to_fetch]).to(device)\n        cifar_labels = torch.from_numpy(fix_get_cifar10_data[1][:num_to_fetch]).to(device)\n        (acc, cert_acc) = art_model.eval_and_certify(x=cifar_data.cpu().numpy(), y=cifar_labels.cpu().numpy(), batch_size=num_to_fetch, size_to_certify=4)\n        upsample = torch.nn.Upsample(scale_factor=224 / 32)\n        cifar_data = upsample(cifar_data)\n        acc_non_ablation = art_model.model(cifar_data)\n        acc_non_ablation = art_model.get_accuracy(acc_non_ablation, cifar_labels)\n        if torch.cuda.is_available():\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.29)\n                assert np.allclose(acc.cpu().numpy(), 0.57)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.16)\n                assert np.allclose(acc.cpu().numpy(), 0.24)\n            assert np.allclose(acc_non_ablation, 0.52)\n        else:\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.3)\n                assert np.allclose(acc.cpu().numpy(), 0.7)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.2)\n                assert np.allclose(acc.cpu().numpy(), 0.2)\n            assert np.allclose(acc_non_ablation, 0.6)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_certification_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    With the forward pass equivalence asserted, we now confirm that the certification functions in the same\\n    way by doing a full end to end prediction and certification test over the data.\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_type=ablation, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        if torch.cuda.is_available():\n            num_to_fetch = 100\n        else:\n            num_to_fetch = 10\n        cifar_data = torch.from_numpy(fix_get_cifar10_data[0][:num_to_fetch]).to(device)\n        cifar_labels = torch.from_numpy(fix_get_cifar10_data[1][:num_to_fetch]).to(device)\n        (acc, cert_acc) = art_model.eval_and_certify(x=cifar_data.cpu().numpy(), y=cifar_labels.cpu().numpy(), batch_size=num_to_fetch, size_to_certify=4)\n        upsample = torch.nn.Upsample(scale_factor=224 / 32)\n        cifar_data = upsample(cifar_data)\n        acc_non_ablation = art_model.model(cifar_data)\n        acc_non_ablation = art_model.get_accuracy(acc_non_ablation, cifar_labels)\n        if torch.cuda.is_available():\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.29)\n                assert np.allclose(acc.cpu().numpy(), 0.57)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.16)\n                assert np.allclose(acc.cpu().numpy(), 0.24)\n            assert np.allclose(acc_non_ablation, 0.52)\n        else:\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.3)\n                assert np.allclose(acc.cpu().numpy(), 0.7)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.2)\n                assert np.allclose(acc.cpu().numpy(), 0.2)\n            assert np.allclose(acc_non_ablation, 0.6)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_certification_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    With the forward pass equivalence asserted, we now confirm that the certification functions in the same\\n    way by doing a full end to end prediction and certification test over the data.\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_type=ablation, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        if torch.cuda.is_available():\n            num_to_fetch = 100\n        else:\n            num_to_fetch = 10\n        cifar_data = torch.from_numpy(fix_get_cifar10_data[0][:num_to_fetch]).to(device)\n        cifar_labels = torch.from_numpy(fix_get_cifar10_data[1][:num_to_fetch]).to(device)\n        (acc, cert_acc) = art_model.eval_and_certify(x=cifar_data.cpu().numpy(), y=cifar_labels.cpu().numpy(), batch_size=num_to_fetch, size_to_certify=4)\n        upsample = torch.nn.Upsample(scale_factor=224 / 32)\n        cifar_data = upsample(cifar_data)\n        acc_non_ablation = art_model.model(cifar_data)\n        acc_non_ablation = art_model.get_accuracy(acc_non_ablation, cifar_labels)\n        if torch.cuda.is_available():\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.29)\n                assert np.allclose(acc.cpu().numpy(), 0.57)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.16)\n                assert np.allclose(acc.cpu().numpy(), 0.24)\n            assert np.allclose(acc_non_ablation, 0.52)\n        else:\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.3)\n                assert np.allclose(acc.cpu().numpy(), 0.7)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.2)\n                assert np.allclose(acc.cpu().numpy(), 0.2)\n            assert np.allclose(acc_non_ablation, 0.6)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_certification_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    With the forward pass equivalence asserted, we now confirm that the certification functions in the same\\n    way by doing a full end to end prediction and certification test over the data.\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_type=ablation, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        if torch.cuda.is_available():\n            num_to_fetch = 100\n        else:\n            num_to_fetch = 10\n        cifar_data = torch.from_numpy(fix_get_cifar10_data[0][:num_to_fetch]).to(device)\n        cifar_labels = torch.from_numpy(fix_get_cifar10_data[1][:num_to_fetch]).to(device)\n        (acc, cert_acc) = art_model.eval_and_certify(x=cifar_data.cpu().numpy(), y=cifar_labels.cpu().numpy(), batch_size=num_to_fetch, size_to_certify=4)\n        upsample = torch.nn.Upsample(scale_factor=224 / 32)\n        cifar_data = upsample(cifar_data)\n        acc_non_ablation = art_model.model(cifar_data)\n        acc_non_ablation = art_model.get_accuracy(acc_non_ablation, cifar_labels)\n        if torch.cuda.is_available():\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.29)\n                assert np.allclose(acc.cpu().numpy(), 0.57)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.16)\n                assert np.allclose(acc.cpu().numpy(), 0.24)\n            assert np.allclose(acc_non_ablation, 0.52)\n        else:\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.3)\n                assert np.allclose(acc.cpu().numpy(), 0.7)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.2)\n                assert np.allclose(acc.cpu().numpy(), 0.2)\n            assert np.allclose(acc_non_ablation, 0.6)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\n@pytest.mark.parametrize('ablation', ['block', 'column'])\ndef test_certification_equivalence(art_warning, fix_get_mnist_data, fix_get_cifar10_data, ablation):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    With the forward pass equivalence asserted, we now confirm that the certification functions in the same\\n    way by doing a full end to end prediction and certification test over the data.\\n    '\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_type=ablation, ablation_size=4, load_pretrained=True, replace_last_layer=True, verbose=False)\n        head = {'weight': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_weight.npy'))).to(device), 'bias': torch.tensor(np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), '../../utils/resources/models/certification/smooth_vit/head_bias.npy'))).to(device)}\n        art_model.model.head.load_state_dict(head)\n        if torch.cuda.is_available():\n            num_to_fetch = 100\n        else:\n            num_to_fetch = 10\n        cifar_data = torch.from_numpy(fix_get_cifar10_data[0][:num_to_fetch]).to(device)\n        cifar_labels = torch.from_numpy(fix_get_cifar10_data[1][:num_to_fetch]).to(device)\n        (acc, cert_acc) = art_model.eval_and_certify(x=cifar_data.cpu().numpy(), y=cifar_labels.cpu().numpy(), batch_size=num_to_fetch, size_to_certify=4)\n        upsample = torch.nn.Upsample(scale_factor=224 / 32)\n        cifar_data = upsample(cifar_data)\n        acc_non_ablation = art_model.model(cifar_data)\n        acc_non_ablation = art_model.get_accuracy(acc_non_ablation, cifar_labels)\n        if torch.cuda.is_available():\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.29)\n                assert np.allclose(acc.cpu().numpy(), 0.57)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.16)\n                assert np.allclose(acc.cpu().numpy(), 0.24)\n            assert np.allclose(acc_non_ablation, 0.52)\n        else:\n            if ablation == 'column':\n                assert np.allclose(cert_acc.cpu().numpy(), 0.3)\n                assert np.allclose(acc.cpu().numpy(), 0.7)\n            else:\n                assert np.allclose(cert_acc.cpu().numpy(), 0.2)\n                assert np.allclose(acc.cpu().numpy(), 0.2)\n            assert np.allclose(acc_non_ablation, 0.6)\n    except ARTTestException as e:\n        art_warning(e)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    pass",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, patch_size=16):\n    super().__init__()\n    self.avg_pool = torch.nn.AvgPool2d(patch_size)",
        "mutated": [
            "def __init__(self, patch_size=16):\n    if False:\n        i = 10\n    super().__init__()\n    self.avg_pool = torch.nn.AvgPool2d(patch_size)",
            "def __init__(self, patch_size=16):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.avg_pool = torch.nn.AvgPool2d(patch_size)",
            "def __init__(self, patch_size=16):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.avg_pool = torch.nn.AvgPool2d(patch_size)",
            "def __init__(self, patch_size=16):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.avg_pool = torch.nn.AvgPool2d(patch_size)",
            "def __init__(self, patch_size=16):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.avg_pool = torch.nn.AvgPool2d(patch_size)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, ones_mask):\n    B = ones_mask.shape[0]\n    ones_mask = ones_mask[0].unsqueeze(0)\n    ones_mask = self.avg_pool(ones_mask)[0]\n    ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n    ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n    ones_mask = ones_mask.expand(B, -1)\n    return ones_mask",
        "mutated": [
            "def forward(self, ones_mask):\n    if False:\n        i = 10\n    B = ones_mask.shape[0]\n    ones_mask = ones_mask[0].unsqueeze(0)\n    ones_mask = self.avg_pool(ones_mask)[0]\n    ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n    ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n    ones_mask = ones_mask.expand(B, -1)\n    return ones_mask",
            "def forward(self, ones_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    B = ones_mask.shape[0]\n    ones_mask = ones_mask[0].unsqueeze(0)\n    ones_mask = self.avg_pool(ones_mask)[0]\n    ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n    ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n    ones_mask = ones_mask.expand(B, -1)\n    return ones_mask",
            "def forward(self, ones_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    B = ones_mask.shape[0]\n    ones_mask = ones_mask[0].unsqueeze(0)\n    ones_mask = self.avg_pool(ones_mask)[0]\n    ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n    ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n    ones_mask = ones_mask.expand(B, -1)\n    return ones_mask",
            "def forward(self, ones_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    B = ones_mask.shape[0]\n    ones_mask = ones_mask[0].unsqueeze(0)\n    ones_mask = self.avg_pool(ones_mask)[0]\n    ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n    ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n    ones_mask = ones_mask.expand(B, -1)\n    return ones_mask",
            "def forward(self, ones_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    B = ones_mask.shape[0]\n    ones_mask = ones_mask[0].unsqueeze(0)\n    ones_mask = self.avg_pool(ones_mask)[0]\n    ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n    ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n    ones_mask = ones_mask.expand(B, -1)\n    return ones_mask"
        ]
    },
    {
        "func_name": "token_dropper",
        "original": "@classmethod\ndef token_dropper(cls, x, mask):\n    \"\"\"\n                The implementation of dropping tokens has been done slightly differently in this tool.\n                Here we check that it is equivalent to the original implementation\n                \"\"\"\n\n    class MaskProcessor(torch.nn.Module):\n\n        def __init__(self, patch_size=16):\n            super().__init__()\n            self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n        def forward(self, ones_mask):\n            B = ones_mask.shape[0]\n            ones_mask = ones_mask[0].unsqueeze(0)\n            ones_mask = self.avg_pool(ones_mask)[0]\n            ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n            ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n            ones_mask = ones_mask.expand(B, -1)\n            return ones_mask\n    mask_processor = MaskProcessor()\n    patch_mask = mask_processor(mask)\n    if patch_mask is not None:\n        (B, N, C) = x.shape\n        if len(patch_mask.shape) == 1:\n            x = x[:, patch_mask]\n        else:\n            patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n            x = torch.gather(x, 1, patch_mask)\n    return x",
        "mutated": [
            "@classmethod\ndef token_dropper(cls, x, mask):\n    if False:\n        i = 10\n    '\\n                The implementation of dropping tokens has been done slightly differently in this tool.\\n                Here we check that it is equivalent to the original implementation\\n                '\n\n    class MaskProcessor(torch.nn.Module):\n\n        def __init__(self, patch_size=16):\n            super().__init__()\n            self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n        def forward(self, ones_mask):\n            B = ones_mask.shape[0]\n            ones_mask = ones_mask[0].unsqueeze(0)\n            ones_mask = self.avg_pool(ones_mask)[0]\n            ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n            ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n            ones_mask = ones_mask.expand(B, -1)\n            return ones_mask\n    mask_processor = MaskProcessor()\n    patch_mask = mask_processor(mask)\n    if patch_mask is not None:\n        (B, N, C) = x.shape\n        if len(patch_mask.shape) == 1:\n            x = x[:, patch_mask]\n        else:\n            patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n            x = torch.gather(x, 1, patch_mask)\n    return x",
            "@classmethod\ndef token_dropper(cls, x, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n                The implementation of dropping tokens has been done slightly differently in this tool.\\n                Here we check that it is equivalent to the original implementation\\n                '\n\n    class MaskProcessor(torch.nn.Module):\n\n        def __init__(self, patch_size=16):\n            super().__init__()\n            self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n        def forward(self, ones_mask):\n            B = ones_mask.shape[0]\n            ones_mask = ones_mask[0].unsqueeze(0)\n            ones_mask = self.avg_pool(ones_mask)[0]\n            ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n            ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n            ones_mask = ones_mask.expand(B, -1)\n            return ones_mask\n    mask_processor = MaskProcessor()\n    patch_mask = mask_processor(mask)\n    if patch_mask is not None:\n        (B, N, C) = x.shape\n        if len(patch_mask.shape) == 1:\n            x = x[:, patch_mask]\n        else:\n            patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n            x = torch.gather(x, 1, patch_mask)\n    return x",
            "@classmethod\ndef token_dropper(cls, x, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n                The implementation of dropping tokens has been done slightly differently in this tool.\\n                Here we check that it is equivalent to the original implementation\\n                '\n\n    class MaskProcessor(torch.nn.Module):\n\n        def __init__(self, patch_size=16):\n            super().__init__()\n            self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n        def forward(self, ones_mask):\n            B = ones_mask.shape[0]\n            ones_mask = ones_mask[0].unsqueeze(0)\n            ones_mask = self.avg_pool(ones_mask)[0]\n            ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n            ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n            ones_mask = ones_mask.expand(B, -1)\n            return ones_mask\n    mask_processor = MaskProcessor()\n    patch_mask = mask_processor(mask)\n    if patch_mask is not None:\n        (B, N, C) = x.shape\n        if len(patch_mask.shape) == 1:\n            x = x[:, patch_mask]\n        else:\n            patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n            x = torch.gather(x, 1, patch_mask)\n    return x",
            "@classmethod\ndef token_dropper(cls, x, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n                The implementation of dropping tokens has been done slightly differently in this tool.\\n                Here we check that it is equivalent to the original implementation\\n                '\n\n    class MaskProcessor(torch.nn.Module):\n\n        def __init__(self, patch_size=16):\n            super().__init__()\n            self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n        def forward(self, ones_mask):\n            B = ones_mask.shape[0]\n            ones_mask = ones_mask[0].unsqueeze(0)\n            ones_mask = self.avg_pool(ones_mask)[0]\n            ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n            ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n            ones_mask = ones_mask.expand(B, -1)\n            return ones_mask\n    mask_processor = MaskProcessor()\n    patch_mask = mask_processor(mask)\n    if patch_mask is not None:\n        (B, N, C) = x.shape\n        if len(patch_mask.shape) == 1:\n            x = x[:, patch_mask]\n        else:\n            patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n            x = torch.gather(x, 1, patch_mask)\n    return x",
            "@classmethod\ndef token_dropper(cls, x, mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n                The implementation of dropping tokens has been done slightly differently in this tool.\\n                Here we check that it is equivalent to the original implementation\\n                '\n\n    class MaskProcessor(torch.nn.Module):\n\n        def __init__(self, patch_size=16):\n            super().__init__()\n            self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n        def forward(self, ones_mask):\n            B = ones_mask.shape[0]\n            ones_mask = ones_mask[0].unsqueeze(0)\n            ones_mask = self.avg_pool(ones_mask)[0]\n            ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n            ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n            ones_mask = ones_mask.expand(B, -1)\n            return ones_mask\n    mask_processor = MaskProcessor()\n    patch_mask = mask_processor(mask)\n    if patch_mask is not None:\n        (B, N, C) = x.shape\n        if len(patch_mask.shape) == 1:\n            x = x[:, patch_mask]\n        else:\n            patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n            x = torch.gather(x, 1, patch_mask)\n    return x"
        ]
    },
    {
        "func_name": "embedder",
        "original": "@classmethod\ndef embedder(cls, x, pos_embed, cls_token):\n    \"\"\"\n                NB, original code used the pos embed from the divit rather than vit\n                (which we pull from our model) which we use here.\n\n                From timm vit:\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n\n                From timm dvit:\n                self.pos_embed = nn.Parameter(torch.zeros(1,\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\n                                                         self.embed_dim))\n\n                From repo:\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n                \"\"\"\n    x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n    return x + pos_embed",
        "mutated": [
            "@classmethod\ndef embedder(cls, x, pos_embed, cls_token):\n    if False:\n        i = 10\n    '\\n                NB, original code used the pos embed from the divit rather than vit\\n                (which we pull from our model) which we use here.\\n\\n                From timm vit:\\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\\n\\n                From timm dvit:\\n                self.pos_embed = nn.Parameter(torch.zeros(1,\\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\\n                                                         self.embed_dim))\\n\\n                From repo:\\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\\n                '\n    x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n    return x + pos_embed",
            "@classmethod\ndef embedder(cls, x, pos_embed, cls_token):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n                NB, original code used the pos embed from the divit rather than vit\\n                (which we pull from our model) which we use here.\\n\\n                From timm vit:\\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\\n\\n                From timm dvit:\\n                self.pos_embed = nn.Parameter(torch.zeros(1,\\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\\n                                                         self.embed_dim))\\n\\n                From repo:\\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\\n                '\n    x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n    return x + pos_embed",
            "@classmethod\ndef embedder(cls, x, pos_embed, cls_token):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n                NB, original code used the pos embed from the divit rather than vit\\n                (which we pull from our model) which we use here.\\n\\n                From timm vit:\\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\\n\\n                From timm dvit:\\n                self.pos_embed = nn.Parameter(torch.zeros(1,\\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\\n                                                         self.embed_dim))\\n\\n                From repo:\\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\\n                '\n    x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n    return x + pos_embed",
            "@classmethod\ndef embedder(cls, x, pos_embed, cls_token):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n                NB, original code used the pos embed from the divit rather than vit\\n                (which we pull from our model) which we use here.\\n\\n                From timm vit:\\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\\n\\n                From timm dvit:\\n                self.pos_embed = nn.Parameter(torch.zeros(1,\\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\\n                                                         self.embed_dim))\\n\\n                From repo:\\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\\n                '\n    x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n    return x + pos_embed",
            "@classmethod\ndef embedder(cls, x, pos_embed, cls_token):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n                NB, original code used the pos embed from the divit rather than vit\\n                (which we pull from our model) which we use here.\\n\\n                From timm vit:\\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\\n\\n                From timm dvit:\\n                self.pos_embed = nn.Parameter(torch.zeros(1,\\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\\n                                                         self.embed_dim))\\n\\n                From repo:\\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\\n                '\n    x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n    return x + pos_embed"
        ]
    },
    {
        "func_name": "forward_features",
        "original": "def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n            This is a copy of the function in ArtViT.forward_features\n            except we also perform an equivalence assertion compared to the implementation\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\n\n            The forward pass of the ViT.\n\n            :param x: Input data.\n            :return: The input processed by the ViT backbone\n            \"\"\"\n    import copy\n    ablated_input = False\n    if x.shape[1] == self.in_chans + 1:\n        ablated_input = True\n    if ablated_input:\n        (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n    x = self.patch_embed(x)\n    madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n    x = self._pos_embed(x)\n    assert torch.equal(madry_embed, x)\n    madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n    if self.to_drop_tokens and ablated_input:\n        ones = self.ablation_mask_embedder(ablation_mask)\n        to_drop = torch.sum(ones, dim=2)\n        indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n        x = self.drop_tokens(x, indexes)\n    assert torch.equal(madry_dropped, x)\n    x = self.norm_pre(x)\n    x = self.blocks(x)\n    return self.norm(x)",
        "mutated": [
            "def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n            This is a copy of the function in ArtViT.forward_features\\n            except we also perform an equivalence assertion compared to the implementation\\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\\n\\n            The forward pass of the ViT.\\n\\n            :param x: Input data.\\n            :return: The input processed by the ViT backbone\\n            '\n    import copy\n    ablated_input = False\n    if x.shape[1] == self.in_chans + 1:\n        ablated_input = True\n    if ablated_input:\n        (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n    x = self.patch_embed(x)\n    madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n    x = self._pos_embed(x)\n    assert torch.equal(madry_embed, x)\n    madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n    if self.to_drop_tokens and ablated_input:\n        ones = self.ablation_mask_embedder(ablation_mask)\n        to_drop = torch.sum(ones, dim=2)\n        indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n        x = self.drop_tokens(x, indexes)\n    assert torch.equal(madry_dropped, x)\n    x = self.norm_pre(x)\n    x = self.blocks(x)\n    return self.norm(x)",
            "def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n            This is a copy of the function in ArtViT.forward_features\\n            except we also perform an equivalence assertion compared to the implementation\\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\\n\\n            The forward pass of the ViT.\\n\\n            :param x: Input data.\\n            :return: The input processed by the ViT backbone\\n            '\n    import copy\n    ablated_input = False\n    if x.shape[1] == self.in_chans + 1:\n        ablated_input = True\n    if ablated_input:\n        (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n    x = self.patch_embed(x)\n    madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n    x = self._pos_embed(x)\n    assert torch.equal(madry_embed, x)\n    madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n    if self.to_drop_tokens and ablated_input:\n        ones = self.ablation_mask_embedder(ablation_mask)\n        to_drop = torch.sum(ones, dim=2)\n        indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n        x = self.drop_tokens(x, indexes)\n    assert torch.equal(madry_dropped, x)\n    x = self.norm_pre(x)\n    x = self.blocks(x)\n    return self.norm(x)",
            "def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n            This is a copy of the function in ArtViT.forward_features\\n            except we also perform an equivalence assertion compared to the implementation\\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\\n\\n            The forward pass of the ViT.\\n\\n            :param x: Input data.\\n            :return: The input processed by the ViT backbone\\n            '\n    import copy\n    ablated_input = False\n    if x.shape[1] == self.in_chans + 1:\n        ablated_input = True\n    if ablated_input:\n        (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n    x = self.patch_embed(x)\n    madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n    x = self._pos_embed(x)\n    assert torch.equal(madry_embed, x)\n    madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n    if self.to_drop_tokens and ablated_input:\n        ones = self.ablation_mask_embedder(ablation_mask)\n        to_drop = torch.sum(ones, dim=2)\n        indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n        x = self.drop_tokens(x, indexes)\n    assert torch.equal(madry_dropped, x)\n    x = self.norm_pre(x)\n    x = self.blocks(x)\n    return self.norm(x)",
            "def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n            This is a copy of the function in ArtViT.forward_features\\n            except we also perform an equivalence assertion compared to the implementation\\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\\n\\n            The forward pass of the ViT.\\n\\n            :param x: Input data.\\n            :return: The input processed by the ViT backbone\\n            '\n    import copy\n    ablated_input = False\n    if x.shape[1] == self.in_chans + 1:\n        ablated_input = True\n    if ablated_input:\n        (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n    x = self.patch_embed(x)\n    madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n    x = self._pos_embed(x)\n    assert torch.equal(madry_embed, x)\n    madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n    if self.to_drop_tokens and ablated_input:\n        ones = self.ablation_mask_embedder(ablation_mask)\n        to_drop = torch.sum(ones, dim=2)\n        indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n        x = self.drop_tokens(x, indexes)\n    assert torch.equal(madry_dropped, x)\n    x = self.norm_pre(x)\n    x = self.blocks(x)\n    return self.norm(x)",
            "def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n            This is a copy of the function in ArtViT.forward_features\\n            except we also perform an equivalence assertion compared to the implementation\\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\\n\\n            The forward pass of the ViT.\\n\\n            :param x: Input data.\\n            :return: The input processed by the ViT backbone\\n            '\n    import copy\n    ablated_input = False\n    if x.shape[1] == self.in_chans + 1:\n        ablated_input = True\n    if ablated_input:\n        (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n    x = self.patch_embed(x)\n    madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n    x = self._pos_embed(x)\n    assert torch.equal(madry_embed, x)\n    madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n    if self.to_drop_tokens and ablated_input:\n        ones = self.ablation_mask_embedder(ablation_mask)\n        to_drop = torch.sum(ones, dim=2)\n        indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n        x = self.drop_tokens(x, indexes)\n    assert torch.equal(madry_dropped, x)\n    x = self.norm_pre(x)\n    x = self.blocks(x)\n    return self.norm(x)"
        ]
    },
    {
        "func_name": "test_equivalence",
        "original": "@pytest.mark.only_with_platform('pytorch')\ndef test_equivalence(art_warning, fix_get_cifar10_data):\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.vision_transformers.pytorch import PyTorchVisionTransformer\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n\n        class MadrylabImplementations:\n            \"\"\"\n            Code adapted from the implementation in https://github.com/MadryLab/smoothed-vit\n            to check against our own functionality.\n\n            Original License:\n\n            MIT License\n\n            Copyright (c) 2021 Madry Lab\n\n            Permission is hereby granted, free of charge, to any person obtaining a copy\n            of this software and associated documentation files (the \"Software\"), to deal\n            in the Software without restriction, including without limitation the rights\n            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n            copies of the Software, and to permit persons to whom the Software is\n            furnished to do so, subject to the following conditions:\n\n            The above copyright notice and this permission notice shall be included in all\n            copies or substantial portions of the Software.\n\n            THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n            SOFTWARE.\n\n            \"\"\"\n\n            def __init__(self):\n                pass\n\n            @classmethod\n            def token_dropper(cls, x, mask):\n                \"\"\"\n                The implementation of dropping tokens has been done slightly differently in this tool.\n                Here we check that it is equivalent to the original implementation\n                \"\"\"\n\n                class MaskProcessor(torch.nn.Module):\n\n                    def __init__(self, patch_size=16):\n                        super().__init__()\n                        self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n                    def forward(self, ones_mask):\n                        B = ones_mask.shape[0]\n                        ones_mask = ones_mask[0].unsqueeze(0)\n                        ones_mask = self.avg_pool(ones_mask)[0]\n                        ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n                        ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n                        ones_mask = ones_mask.expand(B, -1)\n                        return ones_mask\n                mask_processor = MaskProcessor()\n                patch_mask = mask_processor(mask)\n                if patch_mask is not None:\n                    (B, N, C) = x.shape\n                    if len(patch_mask.shape) == 1:\n                        x = x[:, patch_mask]\n                    else:\n                        patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n                        x = torch.gather(x, 1, patch_mask)\n                return x\n\n            @classmethod\n            def embedder(cls, x, pos_embed, cls_token):\n                \"\"\"\n                NB, original code used the pos embed from the divit rather than vit\n                (which we pull from our model) which we use here.\n\n                From timm vit:\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n\n                From timm dvit:\n                self.pos_embed = nn.Parameter(torch.zeros(1,\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\n                                                         self.embed_dim))\n\n                From repo:\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n                \"\"\"\n                x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n                return x + pos_embed\n\n        def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n            \"\"\"\n            This is a copy of the function in ArtViT.forward_features\n            except we also perform an equivalence assertion compared to the implementation\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\n\n            The forward pass of the ViT.\n\n            :param x: Input data.\n            :return: The input processed by the ViT backbone\n            \"\"\"\n            import copy\n            ablated_input = False\n            if x.shape[1] == self.in_chans + 1:\n                ablated_input = True\n            if ablated_input:\n                (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n            x = self.patch_embed(x)\n            madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n            x = self._pos_embed(x)\n            assert torch.equal(madry_embed, x)\n            madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n            if self.to_drop_tokens and ablated_input:\n                ones = self.ablation_mask_embedder(ablation_mask)\n                to_drop = torch.sum(ones, dim=2)\n                indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n                x = self.drop_tokens(x, indexes)\n            assert torch.equal(madry_dropped, x)\n            x = self.norm_pre(x)\n            x = self.blocks(x)\n            return self.norm(x)\n        PyTorchVisionTransformer.forward_features = forward_features\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=False, replace_last_layer=True, verbose=False)\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=1, update_batchnorm=True, scheduler=scheduler, batch_size=128)\n    except ARTTestException as e:\n        art_warning(e)",
        "mutated": [
            "@pytest.mark.only_with_platform('pytorch')\ndef test_equivalence(art_warning, fix_get_cifar10_data):\n    if False:\n        i = 10\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.vision_transformers.pytorch import PyTorchVisionTransformer\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n\n        class MadrylabImplementations:\n            \"\"\"\n            Code adapted from the implementation in https://github.com/MadryLab/smoothed-vit\n            to check against our own functionality.\n\n            Original License:\n\n            MIT License\n\n            Copyright (c) 2021 Madry Lab\n\n            Permission is hereby granted, free of charge, to any person obtaining a copy\n            of this software and associated documentation files (the \"Software\"), to deal\n            in the Software without restriction, including without limitation the rights\n            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n            copies of the Software, and to permit persons to whom the Software is\n            furnished to do so, subject to the following conditions:\n\n            The above copyright notice and this permission notice shall be included in all\n            copies or substantial portions of the Software.\n\n            THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n            SOFTWARE.\n\n            \"\"\"\n\n            def __init__(self):\n                pass\n\n            @classmethod\n            def token_dropper(cls, x, mask):\n                \"\"\"\n                The implementation of dropping tokens has been done slightly differently in this tool.\n                Here we check that it is equivalent to the original implementation\n                \"\"\"\n\n                class MaskProcessor(torch.nn.Module):\n\n                    def __init__(self, patch_size=16):\n                        super().__init__()\n                        self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n                    def forward(self, ones_mask):\n                        B = ones_mask.shape[0]\n                        ones_mask = ones_mask[0].unsqueeze(0)\n                        ones_mask = self.avg_pool(ones_mask)[0]\n                        ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n                        ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n                        ones_mask = ones_mask.expand(B, -1)\n                        return ones_mask\n                mask_processor = MaskProcessor()\n                patch_mask = mask_processor(mask)\n                if patch_mask is not None:\n                    (B, N, C) = x.shape\n                    if len(patch_mask.shape) == 1:\n                        x = x[:, patch_mask]\n                    else:\n                        patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n                        x = torch.gather(x, 1, patch_mask)\n                return x\n\n            @classmethod\n            def embedder(cls, x, pos_embed, cls_token):\n                \"\"\"\n                NB, original code used the pos embed from the divit rather than vit\n                (which we pull from our model) which we use here.\n\n                From timm vit:\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n\n                From timm dvit:\n                self.pos_embed = nn.Parameter(torch.zeros(1,\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\n                                                         self.embed_dim))\n\n                From repo:\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n                \"\"\"\n                x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n                return x + pos_embed\n\n        def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n            \"\"\"\n            This is a copy of the function in ArtViT.forward_features\n            except we also perform an equivalence assertion compared to the implementation\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\n\n            The forward pass of the ViT.\n\n            :param x: Input data.\n            :return: The input processed by the ViT backbone\n            \"\"\"\n            import copy\n            ablated_input = False\n            if x.shape[1] == self.in_chans + 1:\n                ablated_input = True\n            if ablated_input:\n                (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n            x = self.patch_embed(x)\n            madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n            x = self._pos_embed(x)\n            assert torch.equal(madry_embed, x)\n            madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n            if self.to_drop_tokens and ablated_input:\n                ones = self.ablation_mask_embedder(ablation_mask)\n                to_drop = torch.sum(ones, dim=2)\n                indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n                x = self.drop_tokens(x, indexes)\n            assert torch.equal(madry_dropped, x)\n            x = self.norm_pre(x)\n            x = self.blocks(x)\n            return self.norm(x)\n        PyTorchVisionTransformer.forward_features = forward_features\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=False, replace_last_layer=True, verbose=False)\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=1, update_batchnorm=True, scheduler=scheduler, batch_size=128)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_equivalence(art_warning, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.vision_transformers.pytorch import PyTorchVisionTransformer\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n\n        class MadrylabImplementations:\n            \"\"\"\n            Code adapted from the implementation in https://github.com/MadryLab/smoothed-vit\n            to check against our own functionality.\n\n            Original License:\n\n            MIT License\n\n            Copyright (c) 2021 Madry Lab\n\n            Permission is hereby granted, free of charge, to any person obtaining a copy\n            of this software and associated documentation files (the \"Software\"), to deal\n            in the Software without restriction, including without limitation the rights\n            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n            copies of the Software, and to permit persons to whom the Software is\n            furnished to do so, subject to the following conditions:\n\n            The above copyright notice and this permission notice shall be included in all\n            copies or substantial portions of the Software.\n\n            THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n            SOFTWARE.\n\n            \"\"\"\n\n            def __init__(self):\n                pass\n\n            @classmethod\n            def token_dropper(cls, x, mask):\n                \"\"\"\n                The implementation of dropping tokens has been done slightly differently in this tool.\n                Here we check that it is equivalent to the original implementation\n                \"\"\"\n\n                class MaskProcessor(torch.nn.Module):\n\n                    def __init__(self, patch_size=16):\n                        super().__init__()\n                        self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n                    def forward(self, ones_mask):\n                        B = ones_mask.shape[0]\n                        ones_mask = ones_mask[0].unsqueeze(0)\n                        ones_mask = self.avg_pool(ones_mask)[0]\n                        ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n                        ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n                        ones_mask = ones_mask.expand(B, -1)\n                        return ones_mask\n                mask_processor = MaskProcessor()\n                patch_mask = mask_processor(mask)\n                if patch_mask is not None:\n                    (B, N, C) = x.shape\n                    if len(patch_mask.shape) == 1:\n                        x = x[:, patch_mask]\n                    else:\n                        patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n                        x = torch.gather(x, 1, patch_mask)\n                return x\n\n            @classmethod\n            def embedder(cls, x, pos_embed, cls_token):\n                \"\"\"\n                NB, original code used the pos embed from the divit rather than vit\n                (which we pull from our model) which we use here.\n\n                From timm vit:\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n\n                From timm dvit:\n                self.pos_embed = nn.Parameter(torch.zeros(1,\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\n                                                         self.embed_dim))\n\n                From repo:\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n                \"\"\"\n                x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n                return x + pos_embed\n\n        def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n            \"\"\"\n            This is a copy of the function in ArtViT.forward_features\n            except we also perform an equivalence assertion compared to the implementation\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\n\n            The forward pass of the ViT.\n\n            :param x: Input data.\n            :return: The input processed by the ViT backbone\n            \"\"\"\n            import copy\n            ablated_input = False\n            if x.shape[1] == self.in_chans + 1:\n                ablated_input = True\n            if ablated_input:\n                (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n            x = self.patch_embed(x)\n            madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n            x = self._pos_embed(x)\n            assert torch.equal(madry_embed, x)\n            madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n            if self.to_drop_tokens and ablated_input:\n                ones = self.ablation_mask_embedder(ablation_mask)\n                to_drop = torch.sum(ones, dim=2)\n                indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n                x = self.drop_tokens(x, indexes)\n            assert torch.equal(madry_dropped, x)\n            x = self.norm_pre(x)\n            x = self.blocks(x)\n            return self.norm(x)\n        PyTorchVisionTransformer.forward_features = forward_features\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=False, replace_last_layer=True, verbose=False)\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=1, update_batchnorm=True, scheduler=scheduler, batch_size=128)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_equivalence(art_warning, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.vision_transformers.pytorch import PyTorchVisionTransformer\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n\n        class MadrylabImplementations:\n            \"\"\"\n            Code adapted from the implementation in https://github.com/MadryLab/smoothed-vit\n            to check against our own functionality.\n\n            Original License:\n\n            MIT License\n\n            Copyright (c) 2021 Madry Lab\n\n            Permission is hereby granted, free of charge, to any person obtaining a copy\n            of this software and associated documentation files (the \"Software\"), to deal\n            in the Software without restriction, including without limitation the rights\n            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n            copies of the Software, and to permit persons to whom the Software is\n            furnished to do so, subject to the following conditions:\n\n            The above copyright notice and this permission notice shall be included in all\n            copies or substantial portions of the Software.\n\n            THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n            SOFTWARE.\n\n            \"\"\"\n\n            def __init__(self):\n                pass\n\n            @classmethod\n            def token_dropper(cls, x, mask):\n                \"\"\"\n                The implementation of dropping tokens has been done slightly differently in this tool.\n                Here we check that it is equivalent to the original implementation\n                \"\"\"\n\n                class MaskProcessor(torch.nn.Module):\n\n                    def __init__(self, patch_size=16):\n                        super().__init__()\n                        self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n                    def forward(self, ones_mask):\n                        B = ones_mask.shape[0]\n                        ones_mask = ones_mask[0].unsqueeze(0)\n                        ones_mask = self.avg_pool(ones_mask)[0]\n                        ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n                        ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n                        ones_mask = ones_mask.expand(B, -1)\n                        return ones_mask\n                mask_processor = MaskProcessor()\n                patch_mask = mask_processor(mask)\n                if patch_mask is not None:\n                    (B, N, C) = x.shape\n                    if len(patch_mask.shape) == 1:\n                        x = x[:, patch_mask]\n                    else:\n                        patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n                        x = torch.gather(x, 1, patch_mask)\n                return x\n\n            @classmethod\n            def embedder(cls, x, pos_embed, cls_token):\n                \"\"\"\n                NB, original code used the pos embed from the divit rather than vit\n                (which we pull from our model) which we use here.\n\n                From timm vit:\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n\n                From timm dvit:\n                self.pos_embed = nn.Parameter(torch.zeros(1,\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\n                                                         self.embed_dim))\n\n                From repo:\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n                \"\"\"\n                x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n                return x + pos_embed\n\n        def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n            \"\"\"\n            This is a copy of the function in ArtViT.forward_features\n            except we also perform an equivalence assertion compared to the implementation\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\n\n            The forward pass of the ViT.\n\n            :param x: Input data.\n            :return: The input processed by the ViT backbone\n            \"\"\"\n            import copy\n            ablated_input = False\n            if x.shape[1] == self.in_chans + 1:\n                ablated_input = True\n            if ablated_input:\n                (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n            x = self.patch_embed(x)\n            madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n            x = self._pos_embed(x)\n            assert torch.equal(madry_embed, x)\n            madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n            if self.to_drop_tokens and ablated_input:\n                ones = self.ablation_mask_embedder(ablation_mask)\n                to_drop = torch.sum(ones, dim=2)\n                indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n                x = self.drop_tokens(x, indexes)\n            assert torch.equal(madry_dropped, x)\n            x = self.norm_pre(x)\n            x = self.blocks(x)\n            return self.norm(x)\n        PyTorchVisionTransformer.forward_features = forward_features\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=False, replace_last_layer=True, verbose=False)\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=1, update_batchnorm=True, scheduler=scheduler, batch_size=128)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_equivalence(art_warning, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.vision_transformers.pytorch import PyTorchVisionTransformer\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n\n        class MadrylabImplementations:\n            \"\"\"\n            Code adapted from the implementation in https://github.com/MadryLab/smoothed-vit\n            to check against our own functionality.\n\n            Original License:\n\n            MIT License\n\n            Copyright (c) 2021 Madry Lab\n\n            Permission is hereby granted, free of charge, to any person obtaining a copy\n            of this software and associated documentation files (the \"Software\"), to deal\n            in the Software without restriction, including without limitation the rights\n            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n            copies of the Software, and to permit persons to whom the Software is\n            furnished to do so, subject to the following conditions:\n\n            The above copyright notice and this permission notice shall be included in all\n            copies or substantial portions of the Software.\n\n            THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n            SOFTWARE.\n\n            \"\"\"\n\n            def __init__(self):\n                pass\n\n            @classmethod\n            def token_dropper(cls, x, mask):\n                \"\"\"\n                The implementation of dropping tokens has been done slightly differently in this tool.\n                Here we check that it is equivalent to the original implementation\n                \"\"\"\n\n                class MaskProcessor(torch.nn.Module):\n\n                    def __init__(self, patch_size=16):\n                        super().__init__()\n                        self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n                    def forward(self, ones_mask):\n                        B = ones_mask.shape[0]\n                        ones_mask = ones_mask[0].unsqueeze(0)\n                        ones_mask = self.avg_pool(ones_mask)[0]\n                        ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n                        ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n                        ones_mask = ones_mask.expand(B, -1)\n                        return ones_mask\n                mask_processor = MaskProcessor()\n                patch_mask = mask_processor(mask)\n                if patch_mask is not None:\n                    (B, N, C) = x.shape\n                    if len(patch_mask.shape) == 1:\n                        x = x[:, patch_mask]\n                    else:\n                        patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n                        x = torch.gather(x, 1, patch_mask)\n                return x\n\n            @classmethod\n            def embedder(cls, x, pos_embed, cls_token):\n                \"\"\"\n                NB, original code used the pos embed from the divit rather than vit\n                (which we pull from our model) which we use here.\n\n                From timm vit:\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n\n                From timm dvit:\n                self.pos_embed = nn.Parameter(torch.zeros(1,\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\n                                                         self.embed_dim))\n\n                From repo:\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n                \"\"\"\n                x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n                return x + pos_embed\n\n        def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n            \"\"\"\n            This is a copy of the function in ArtViT.forward_features\n            except we also perform an equivalence assertion compared to the implementation\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\n\n            The forward pass of the ViT.\n\n            :param x: Input data.\n            :return: The input processed by the ViT backbone\n            \"\"\"\n            import copy\n            ablated_input = False\n            if x.shape[1] == self.in_chans + 1:\n                ablated_input = True\n            if ablated_input:\n                (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n            x = self.patch_embed(x)\n            madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n            x = self._pos_embed(x)\n            assert torch.equal(madry_embed, x)\n            madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n            if self.to_drop_tokens and ablated_input:\n                ones = self.ablation_mask_embedder(ablation_mask)\n                to_drop = torch.sum(ones, dim=2)\n                indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n                x = self.drop_tokens(x, indexes)\n            assert torch.equal(madry_dropped, x)\n            x = self.norm_pre(x)\n            x = self.blocks(x)\n            return self.norm(x)\n        PyTorchVisionTransformer.forward_features = forward_features\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=False, replace_last_layer=True, verbose=False)\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=1, update_batchnorm=True, scheduler=scheduler, batch_size=128)\n    except ARTTestException as e:\n        art_warning(e)",
            "@pytest.mark.only_with_platform('pytorch')\ndef test_equivalence(art_warning, fix_get_cifar10_data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    import torch\n    from art.estimators.certification.derandomized_smoothing import PyTorchDeRandomizedSmoothing\n    from art.estimators.certification.derandomized_smoothing.vision_transformers.pytorch import PyTorchVisionTransformer\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    try:\n\n        class MadrylabImplementations:\n            \"\"\"\n            Code adapted from the implementation in https://github.com/MadryLab/smoothed-vit\n            to check against our own functionality.\n\n            Original License:\n\n            MIT License\n\n            Copyright (c) 2021 Madry Lab\n\n            Permission is hereby granted, free of charge, to any person obtaining a copy\n            of this software and associated documentation files (the \"Software\"), to deal\n            in the Software without restriction, including without limitation the rights\n            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n            copies of the Software, and to permit persons to whom the Software is\n            furnished to do so, subject to the following conditions:\n\n            The above copyright notice and this permission notice shall be included in all\n            copies or substantial portions of the Software.\n\n            THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n            SOFTWARE.\n\n            \"\"\"\n\n            def __init__(self):\n                pass\n\n            @classmethod\n            def token_dropper(cls, x, mask):\n                \"\"\"\n                The implementation of dropping tokens has been done slightly differently in this tool.\n                Here we check that it is equivalent to the original implementation\n                \"\"\"\n\n                class MaskProcessor(torch.nn.Module):\n\n                    def __init__(self, patch_size=16):\n                        super().__init__()\n                        self.avg_pool = torch.nn.AvgPool2d(patch_size)\n\n                    def forward(self, ones_mask):\n                        B = ones_mask.shape[0]\n                        ones_mask = ones_mask[0].unsqueeze(0)\n                        ones_mask = self.avg_pool(ones_mask)[0]\n                        ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n                        ones_mask = torch.cat([torch.IntTensor(1).fill_(0).to(device), ones_mask]).unsqueeze(0)\n                        ones_mask = ones_mask.expand(B, -1)\n                        return ones_mask\n                mask_processor = MaskProcessor()\n                patch_mask = mask_processor(mask)\n                if patch_mask is not None:\n                    (B, N, C) = x.shape\n                    if len(patch_mask.shape) == 1:\n                        x = x[:, patch_mask]\n                    else:\n                        patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n                        x = torch.gather(x, 1, patch_mask)\n                return x\n\n            @classmethod\n            def embedder(cls, x, pos_embed, cls_token):\n                \"\"\"\n                NB, original code used the pos embed from the divit rather than vit\n                (which we pull from our model) which we use here.\n\n                From timm vit:\n                self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n\n                From timm dvit:\n                self.pos_embed = nn.Parameter(torch.zeros(1,\n                                                         self.patch_embed.num_patches + self.num_prefix_tokens,\n                                                         self.embed_dim))\n\n                From repo:\n                self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n                \"\"\"\n                x = torch.cat((cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n                return x + pos_embed\n\n        def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n            \"\"\"\n            This is a copy of the function in ArtViT.forward_features\n            except we also perform an equivalence assertion compared to the implementation\n            in https://github.com/MadryLab/smoothed-vit (see MadrylabImplementations class above)\n\n            The forward pass of the ViT.\n\n            :param x: Input data.\n            :return: The input processed by the ViT backbone\n            \"\"\"\n            import copy\n            ablated_input = False\n            if x.shape[1] == self.in_chans + 1:\n                ablated_input = True\n            if ablated_input:\n                (x, ablation_mask) = (x[:, :self.in_chans], x[:, self.in_chans:self.in_chans + 1])\n            x = self.patch_embed(x)\n            madry_embed = MadrylabImplementations.embedder(copy.copy(x), self.pos_embed, self.cls_token)\n            x = self._pos_embed(x)\n            assert torch.equal(madry_embed, x)\n            madry_dropped = MadrylabImplementations.token_dropper(copy.copy(x), ablation_mask)\n            if self.to_drop_tokens and ablated_input:\n                ones = self.ablation_mask_embedder(ablation_mask)\n                to_drop = torch.sum(ones, dim=2)\n                indexes = torch.gt(torch.where(to_drop > 1, 1, 0), 0)\n                x = self.drop_tokens(x, indexes)\n            assert torch.equal(madry_dropped, x)\n            x = self.norm_pre(x)\n            x = self.blocks(x)\n            return self.norm(x)\n        PyTorchVisionTransformer.forward_features = forward_features\n        art_model = PyTorchDeRandomizedSmoothing(model='vit_small_patch16_224', loss=torch.nn.CrossEntropyLoss(), optimizer=torch.optim.SGD, optimizer_params={'lr': 0.01}, input_shape=(3, 32, 32), nb_classes=10, ablation_size=4, load_pretrained=False, replace_last_layer=True, verbose=False)\n        cifar_data = fix_get_cifar10_data[0][:50]\n        cifar_labels = fix_get_cifar10_data[1][:50]\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(art_model.optimizer, milestones=[1], gamma=0.1)\n        art_model.fit(cifar_data, cifar_labels, nb_epochs=1, update_batchnorm=True, scheduler=scheduler, batch_size=128)\n    except ARTTestException as e:\n        art_warning(e)"
        ]
    }
]