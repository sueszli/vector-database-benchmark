[
    {
        "func_name": "main",
        "original": "def main():\n    ((x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_) = load_mnist(raw=True)\n    n_train = np.shape(x_raw)[0]\n    num_selection = 5000\n    random_selection_indices = np.random.choice(n_train, num_selection)\n    x_raw = x_raw[random_selection_indices]\n    y_raw = y_raw[random_selection_indices]\n    perc_poison = 0.33\n    (is_poison_train, x_poisoned_raw, y_poisoned_raw) = generate_backdoor(x_raw, y_raw, perc_poison)\n    (x_train, y_train) = preprocess(x_poisoned_raw, y_poisoned_raw)\n    x_train = np.expand_dims(x_train, axis=3)\n    (is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = generate_backdoor(x_raw_test, y_raw_test, perc_poison)\n    (x_test, y_test) = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n    x_test = np.expand_dims(x_test, axis=3)\n    n_train = np.shape(y_train)[0]\n    shuffled_indices = np.arange(n_train)\n    np.random.shuffle(shuffled_indices)\n    x_train = x_train[shuffled_indices]\n    y_train = y_train[shuffled_indices]\n    is_poison_train = is_poison_train[shuffled_indices]\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifier = KerasClassifier(model=model, clip_values=(min_, max_))\n    classifier.fit(x_train, y_train, nb_epochs=30, batch_size=128)\n    preds = np.argmax(classifier.predict(x_test), axis=1)\n    acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n    print('\\nTest accuracy: %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test], axis=1)) / y_test[is_poison_test].shape[0]\n    print('\\nPoisonous test set accuracy (i.e. effectiveness of poison): %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test == 0]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test == 0], axis=1)) / y_test[is_poison_test == 0].shape[0]\n    print('\\nClean test set accuracy: %.2f%%' % (acc * 100))\n    defence = ActivationDefence(classifier, x_train, y_train)\n    print('------------------- Results using size metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA')\n    is_clean = is_poison_train == 0\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for size-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    print('Visualize clusters')\n    sprites_by_class = defence.visualize_clusters(x_train, 'mnist_poison_demo')\n    n_class = 5\n    try:\n        import matplotlib.pyplot as plt\n        plt.imshow(sprites_by_class[n_class][0])\n        plt.title('Class ' + str(n_class) + ' cluster: 0')\n        plt.show()\n        plt.imshow(sprites_by_class[n_class][1])\n        plt.title('Class ' + str(n_class) + ' cluster: 1')\n        plt.show()\n    except ImportError:\n        print('matplotlib not installed. For this reason, cluster visualization was not displayed')\n    print('------------------- Results using distance metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA', cluster_analysis='distance')\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for distance-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    kwargs = {'nb_clusters': 2, 'nb_dims': 10, 'reduce': 'PCA'}\n    defence.cluster_activations(**kwargs)\n    kwargs = {'cluster_analysis': 'distance'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    kwargs = {'cluster_analysis': 'smaller'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    print('done :) ')",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    ((x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_) = load_mnist(raw=True)\n    n_train = np.shape(x_raw)[0]\n    num_selection = 5000\n    random_selection_indices = np.random.choice(n_train, num_selection)\n    x_raw = x_raw[random_selection_indices]\n    y_raw = y_raw[random_selection_indices]\n    perc_poison = 0.33\n    (is_poison_train, x_poisoned_raw, y_poisoned_raw) = generate_backdoor(x_raw, y_raw, perc_poison)\n    (x_train, y_train) = preprocess(x_poisoned_raw, y_poisoned_raw)\n    x_train = np.expand_dims(x_train, axis=3)\n    (is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = generate_backdoor(x_raw_test, y_raw_test, perc_poison)\n    (x_test, y_test) = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n    x_test = np.expand_dims(x_test, axis=3)\n    n_train = np.shape(y_train)[0]\n    shuffled_indices = np.arange(n_train)\n    np.random.shuffle(shuffled_indices)\n    x_train = x_train[shuffled_indices]\n    y_train = y_train[shuffled_indices]\n    is_poison_train = is_poison_train[shuffled_indices]\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifier = KerasClassifier(model=model, clip_values=(min_, max_))\n    classifier.fit(x_train, y_train, nb_epochs=30, batch_size=128)\n    preds = np.argmax(classifier.predict(x_test), axis=1)\n    acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n    print('\\nTest accuracy: %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test], axis=1)) / y_test[is_poison_test].shape[0]\n    print('\\nPoisonous test set accuracy (i.e. effectiveness of poison): %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test == 0]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test == 0], axis=1)) / y_test[is_poison_test == 0].shape[0]\n    print('\\nClean test set accuracy: %.2f%%' % (acc * 100))\n    defence = ActivationDefence(classifier, x_train, y_train)\n    print('------------------- Results using size metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA')\n    is_clean = is_poison_train == 0\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for size-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    print('Visualize clusters')\n    sprites_by_class = defence.visualize_clusters(x_train, 'mnist_poison_demo')\n    n_class = 5\n    try:\n        import matplotlib.pyplot as plt\n        plt.imshow(sprites_by_class[n_class][0])\n        plt.title('Class ' + str(n_class) + ' cluster: 0')\n        plt.show()\n        plt.imshow(sprites_by_class[n_class][1])\n        plt.title('Class ' + str(n_class) + ' cluster: 1')\n        plt.show()\n    except ImportError:\n        print('matplotlib not installed. For this reason, cluster visualization was not displayed')\n    print('------------------- Results using distance metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA', cluster_analysis='distance')\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for distance-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    kwargs = {'nb_clusters': 2, 'nb_dims': 10, 'reduce': 'PCA'}\n    defence.cluster_activations(**kwargs)\n    kwargs = {'cluster_analysis': 'distance'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    kwargs = {'cluster_analysis': 'smaller'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    print('done :) ')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ((x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_) = load_mnist(raw=True)\n    n_train = np.shape(x_raw)[0]\n    num_selection = 5000\n    random_selection_indices = np.random.choice(n_train, num_selection)\n    x_raw = x_raw[random_selection_indices]\n    y_raw = y_raw[random_selection_indices]\n    perc_poison = 0.33\n    (is_poison_train, x_poisoned_raw, y_poisoned_raw) = generate_backdoor(x_raw, y_raw, perc_poison)\n    (x_train, y_train) = preprocess(x_poisoned_raw, y_poisoned_raw)\n    x_train = np.expand_dims(x_train, axis=3)\n    (is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = generate_backdoor(x_raw_test, y_raw_test, perc_poison)\n    (x_test, y_test) = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n    x_test = np.expand_dims(x_test, axis=3)\n    n_train = np.shape(y_train)[0]\n    shuffled_indices = np.arange(n_train)\n    np.random.shuffle(shuffled_indices)\n    x_train = x_train[shuffled_indices]\n    y_train = y_train[shuffled_indices]\n    is_poison_train = is_poison_train[shuffled_indices]\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifier = KerasClassifier(model=model, clip_values=(min_, max_))\n    classifier.fit(x_train, y_train, nb_epochs=30, batch_size=128)\n    preds = np.argmax(classifier.predict(x_test), axis=1)\n    acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n    print('\\nTest accuracy: %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test], axis=1)) / y_test[is_poison_test].shape[0]\n    print('\\nPoisonous test set accuracy (i.e. effectiveness of poison): %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test == 0]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test == 0], axis=1)) / y_test[is_poison_test == 0].shape[0]\n    print('\\nClean test set accuracy: %.2f%%' % (acc * 100))\n    defence = ActivationDefence(classifier, x_train, y_train)\n    print('------------------- Results using size metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA')\n    is_clean = is_poison_train == 0\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for size-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    print('Visualize clusters')\n    sprites_by_class = defence.visualize_clusters(x_train, 'mnist_poison_demo')\n    n_class = 5\n    try:\n        import matplotlib.pyplot as plt\n        plt.imshow(sprites_by_class[n_class][0])\n        plt.title('Class ' + str(n_class) + ' cluster: 0')\n        plt.show()\n        plt.imshow(sprites_by_class[n_class][1])\n        plt.title('Class ' + str(n_class) + ' cluster: 1')\n        plt.show()\n    except ImportError:\n        print('matplotlib not installed. For this reason, cluster visualization was not displayed')\n    print('------------------- Results using distance metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA', cluster_analysis='distance')\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for distance-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    kwargs = {'nb_clusters': 2, 'nb_dims': 10, 'reduce': 'PCA'}\n    defence.cluster_activations(**kwargs)\n    kwargs = {'cluster_analysis': 'distance'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    kwargs = {'cluster_analysis': 'smaller'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    print('done :) ')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ((x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_) = load_mnist(raw=True)\n    n_train = np.shape(x_raw)[0]\n    num_selection = 5000\n    random_selection_indices = np.random.choice(n_train, num_selection)\n    x_raw = x_raw[random_selection_indices]\n    y_raw = y_raw[random_selection_indices]\n    perc_poison = 0.33\n    (is_poison_train, x_poisoned_raw, y_poisoned_raw) = generate_backdoor(x_raw, y_raw, perc_poison)\n    (x_train, y_train) = preprocess(x_poisoned_raw, y_poisoned_raw)\n    x_train = np.expand_dims(x_train, axis=3)\n    (is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = generate_backdoor(x_raw_test, y_raw_test, perc_poison)\n    (x_test, y_test) = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n    x_test = np.expand_dims(x_test, axis=3)\n    n_train = np.shape(y_train)[0]\n    shuffled_indices = np.arange(n_train)\n    np.random.shuffle(shuffled_indices)\n    x_train = x_train[shuffled_indices]\n    y_train = y_train[shuffled_indices]\n    is_poison_train = is_poison_train[shuffled_indices]\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifier = KerasClassifier(model=model, clip_values=(min_, max_))\n    classifier.fit(x_train, y_train, nb_epochs=30, batch_size=128)\n    preds = np.argmax(classifier.predict(x_test), axis=1)\n    acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n    print('\\nTest accuracy: %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test], axis=1)) / y_test[is_poison_test].shape[0]\n    print('\\nPoisonous test set accuracy (i.e. effectiveness of poison): %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test == 0]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test == 0], axis=1)) / y_test[is_poison_test == 0].shape[0]\n    print('\\nClean test set accuracy: %.2f%%' % (acc * 100))\n    defence = ActivationDefence(classifier, x_train, y_train)\n    print('------------------- Results using size metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA')\n    is_clean = is_poison_train == 0\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for size-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    print('Visualize clusters')\n    sprites_by_class = defence.visualize_clusters(x_train, 'mnist_poison_demo')\n    n_class = 5\n    try:\n        import matplotlib.pyplot as plt\n        plt.imshow(sprites_by_class[n_class][0])\n        plt.title('Class ' + str(n_class) + ' cluster: 0')\n        plt.show()\n        plt.imshow(sprites_by_class[n_class][1])\n        plt.title('Class ' + str(n_class) + ' cluster: 1')\n        plt.show()\n    except ImportError:\n        print('matplotlib not installed. For this reason, cluster visualization was not displayed')\n    print('------------------- Results using distance metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA', cluster_analysis='distance')\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for distance-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    kwargs = {'nb_clusters': 2, 'nb_dims': 10, 'reduce': 'PCA'}\n    defence.cluster_activations(**kwargs)\n    kwargs = {'cluster_analysis': 'distance'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    kwargs = {'cluster_analysis': 'smaller'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    print('done :) ')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ((x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_) = load_mnist(raw=True)\n    n_train = np.shape(x_raw)[0]\n    num_selection = 5000\n    random_selection_indices = np.random.choice(n_train, num_selection)\n    x_raw = x_raw[random_selection_indices]\n    y_raw = y_raw[random_selection_indices]\n    perc_poison = 0.33\n    (is_poison_train, x_poisoned_raw, y_poisoned_raw) = generate_backdoor(x_raw, y_raw, perc_poison)\n    (x_train, y_train) = preprocess(x_poisoned_raw, y_poisoned_raw)\n    x_train = np.expand_dims(x_train, axis=3)\n    (is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = generate_backdoor(x_raw_test, y_raw_test, perc_poison)\n    (x_test, y_test) = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n    x_test = np.expand_dims(x_test, axis=3)\n    n_train = np.shape(y_train)[0]\n    shuffled_indices = np.arange(n_train)\n    np.random.shuffle(shuffled_indices)\n    x_train = x_train[shuffled_indices]\n    y_train = y_train[shuffled_indices]\n    is_poison_train = is_poison_train[shuffled_indices]\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifier = KerasClassifier(model=model, clip_values=(min_, max_))\n    classifier.fit(x_train, y_train, nb_epochs=30, batch_size=128)\n    preds = np.argmax(classifier.predict(x_test), axis=1)\n    acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n    print('\\nTest accuracy: %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test], axis=1)) / y_test[is_poison_test].shape[0]\n    print('\\nPoisonous test set accuracy (i.e. effectiveness of poison): %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test == 0]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test == 0], axis=1)) / y_test[is_poison_test == 0].shape[0]\n    print('\\nClean test set accuracy: %.2f%%' % (acc * 100))\n    defence = ActivationDefence(classifier, x_train, y_train)\n    print('------------------- Results using size metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA')\n    is_clean = is_poison_train == 0\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for size-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    print('Visualize clusters')\n    sprites_by_class = defence.visualize_clusters(x_train, 'mnist_poison_demo')\n    n_class = 5\n    try:\n        import matplotlib.pyplot as plt\n        plt.imshow(sprites_by_class[n_class][0])\n        plt.title('Class ' + str(n_class) + ' cluster: 0')\n        plt.show()\n        plt.imshow(sprites_by_class[n_class][1])\n        plt.title('Class ' + str(n_class) + ' cluster: 1')\n        plt.show()\n    except ImportError:\n        print('matplotlib not installed. For this reason, cluster visualization was not displayed')\n    print('------------------- Results using distance metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA', cluster_analysis='distance')\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for distance-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    kwargs = {'nb_clusters': 2, 'nb_dims': 10, 'reduce': 'PCA'}\n    defence.cluster_activations(**kwargs)\n    kwargs = {'cluster_analysis': 'distance'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    kwargs = {'cluster_analysis': 'smaller'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    print('done :) ')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ((x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_) = load_mnist(raw=True)\n    n_train = np.shape(x_raw)[0]\n    num_selection = 5000\n    random_selection_indices = np.random.choice(n_train, num_selection)\n    x_raw = x_raw[random_selection_indices]\n    y_raw = y_raw[random_selection_indices]\n    perc_poison = 0.33\n    (is_poison_train, x_poisoned_raw, y_poisoned_raw) = generate_backdoor(x_raw, y_raw, perc_poison)\n    (x_train, y_train) = preprocess(x_poisoned_raw, y_poisoned_raw)\n    x_train = np.expand_dims(x_train, axis=3)\n    (is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = generate_backdoor(x_raw_test, y_raw_test, perc_poison)\n    (x_test, y_test) = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n    x_test = np.expand_dims(x_test, axis=3)\n    n_train = np.shape(y_train)[0]\n    shuffled_indices = np.arange(n_train)\n    np.random.shuffle(shuffled_indices)\n    x_train = x_train[shuffled_indices]\n    y_train = y_train[shuffled_indices]\n    is_poison_train = is_poison_train[shuffled_indices]\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifier = KerasClassifier(model=model, clip_values=(min_, max_))\n    classifier.fit(x_train, y_train, nb_epochs=30, batch_size=128)\n    preds = np.argmax(classifier.predict(x_test), axis=1)\n    acc = np.sum(preds == np.argmax(y_test, axis=1)) / y_test.shape[0]\n    print('\\nTest accuracy: %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test], axis=1)) / y_test[is_poison_test].shape[0]\n    print('\\nPoisonous test set accuracy (i.e. effectiveness of poison): %.2f%%' % (acc * 100))\n    preds = np.argmax(classifier.predict(x_test[is_poison_test == 0]), axis=1)\n    acc = np.sum(preds == np.argmax(y_test[is_poison_test == 0], axis=1)) / y_test[is_poison_test == 0].shape[0]\n    print('\\nClean test set accuracy: %.2f%%' % (acc * 100))\n    defence = ActivationDefence(classifier, x_train, y_train)\n    print('------------------- Results using size metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA')\n    is_clean = is_poison_train == 0\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for size-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    print('Visualize clusters')\n    sprites_by_class = defence.visualize_clusters(x_train, 'mnist_poison_demo')\n    n_class = 5\n    try:\n        import matplotlib.pyplot as plt\n        plt.imshow(sprites_by_class[n_class][0])\n        plt.title('Class ' + str(n_class) + ' cluster: 0')\n        plt.show()\n        plt.imshow(sprites_by_class[n_class][1])\n        plt.title('Class ' + str(n_class) + ' cluster: 1')\n        plt.show()\n    except ImportError:\n        print('matplotlib not installed. For this reason, cluster visualization was not displayed')\n    print('------------------- Results using distance metric -------------------')\n    print(defence.get_params())\n    defence.detect_poison(nb_clusters=2, nb_dims=10, reduce='PCA', cluster_analysis='distance')\n    confusion_matrix = defence.evaluate_defence(is_clean)\n    print('Evaluation defence results for distance-based metric: ')\n    jsonObject = json.loads(confusion_matrix)\n    for label in jsonObject:\n        print(label)\n        pprint.pprint(jsonObject[label])\n    kwargs = {'nb_clusters': 2, 'nb_dims': 10, 'reduce': 'PCA'}\n    defence.cluster_activations(**kwargs)\n    kwargs = {'cluster_analysis': 'distance'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    kwargs = {'cluster_analysis': 'smaller'}\n    defence.analyze_clusters(**kwargs)\n    defence.evaluate_defence(is_clean)\n    print('done :) ')"
        ]
    },
    {
        "func_name": "generate_backdoor",
        "original": "def generate_backdoor(x_clean, y_clean, percent_poison, backdoor_type='pattern', sources=np.arange(10), targets=(np.arange(10) + 1) % 10):\n    \"\"\"\n    Creates a backdoor in MNIST images by adding a pattern or pixel to the image and changing the label to a targeted\n    class. Default parameters poison each digit so that it gets classified to the next digit.\n\n    :param x_clean: Original raw data\n    :type x_clean: `np.ndarray`\n    :param y_clean: Original labels\n    :type y_clean:`np.ndarray`\n    :param percent_poison: After poisoning, the target class should contain this percentage of poison\n    :type percent_poison: `float`\n    :param backdoor_type: Backdoor type can be `pixel` or `pattern`.\n    :type backdoor_type: `str`\n    :param sources: Array that holds the source classes for each backdoor. Poison is\n    generating by taking images from the source class, adding the backdoor trigger, and labeling as the target class.\n    Poisonous images from sources[i] will be labeled as targets[i].\n    :type sources: `np.ndarray`\n    :param targets: This array holds the target classes for each backdoor. Poisonous images from sources[i] will be\n                    labeled as targets[i].\n    :type targets: `np.ndarray`\n    :return: Returns is_poison, which is a boolean array indicating which points are poisonous, x_poison, which\n    contains all of the data both legitimate and poisoned, and y_poison, which contains all of the labels\n    both legitimate and poisoned.\n    :rtype: `tuple`\n    \"\"\"\n    max_val = np.max(x_clean)\n    x_poison = np.copy(x_clean)\n    y_poison = np.copy(y_clean)\n    is_poison = np.zeros(np.shape(y_poison))\n    for (i, (src, tgt)) in enumerate(zip(sources, targets)):\n        n_points_in_tgt = np.size(np.where(y_clean == tgt))\n        num_poison = round(percent_poison * n_points_in_tgt / (1 - percent_poison))\n        src_imgs = x_clean[y_clean == src]\n        n_points_in_src = np.shape(src_imgs)[0]\n        indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n        imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n        if backdoor_type == 'pattern':\n            imgs_to_be_poisoned = add_pattern_bd(x=imgs_to_be_poisoned, pixel_value=max_val)\n        elif backdoor_type == 'pixel':\n            imgs_to_be_poisoned = add_single_bd(imgs_to_be_poisoned, pixel_value=max_val)\n        x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n        y_poison = np.append(y_poison, np.ones(num_poison) * tgt, axis=0)\n        is_poison = np.append(is_poison, np.ones(num_poison))\n    is_poison = is_poison != 0\n    return (is_poison, x_poison, y_poison)",
        "mutated": [
            "def generate_backdoor(x_clean, y_clean, percent_poison, backdoor_type='pattern', sources=np.arange(10), targets=(np.arange(10) + 1) % 10):\n    if False:\n        i = 10\n    '\\n    Creates a backdoor in MNIST images by adding a pattern or pixel to the image and changing the label to a targeted\\n    class. Default parameters poison each digit so that it gets classified to the next digit.\\n\\n    :param x_clean: Original raw data\\n    :type x_clean: `np.ndarray`\\n    :param y_clean: Original labels\\n    :type y_clean:`np.ndarray`\\n    :param percent_poison: After poisoning, the target class should contain this percentage of poison\\n    :type percent_poison: `float`\\n    :param backdoor_type: Backdoor type can be `pixel` or `pattern`.\\n    :type backdoor_type: `str`\\n    :param sources: Array that holds the source classes for each backdoor. Poison is\\n    generating by taking images from the source class, adding the backdoor trigger, and labeling as the target class.\\n    Poisonous images from sources[i] will be labeled as targets[i].\\n    :type sources: `np.ndarray`\\n    :param targets: This array holds the target classes for each backdoor. Poisonous images from sources[i] will be\\n                    labeled as targets[i].\\n    :type targets: `np.ndarray`\\n    :return: Returns is_poison, which is a boolean array indicating which points are poisonous, x_poison, which\\n    contains all of the data both legitimate and poisoned, and y_poison, which contains all of the labels\\n    both legitimate and poisoned.\\n    :rtype: `tuple`\\n    '\n    max_val = np.max(x_clean)\n    x_poison = np.copy(x_clean)\n    y_poison = np.copy(y_clean)\n    is_poison = np.zeros(np.shape(y_poison))\n    for (i, (src, tgt)) in enumerate(zip(sources, targets)):\n        n_points_in_tgt = np.size(np.where(y_clean == tgt))\n        num_poison = round(percent_poison * n_points_in_tgt / (1 - percent_poison))\n        src_imgs = x_clean[y_clean == src]\n        n_points_in_src = np.shape(src_imgs)[0]\n        indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n        imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n        if backdoor_type == 'pattern':\n            imgs_to_be_poisoned = add_pattern_bd(x=imgs_to_be_poisoned, pixel_value=max_val)\n        elif backdoor_type == 'pixel':\n            imgs_to_be_poisoned = add_single_bd(imgs_to_be_poisoned, pixel_value=max_val)\n        x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n        y_poison = np.append(y_poison, np.ones(num_poison) * tgt, axis=0)\n        is_poison = np.append(is_poison, np.ones(num_poison))\n    is_poison = is_poison != 0\n    return (is_poison, x_poison, y_poison)",
            "def generate_backdoor(x_clean, y_clean, percent_poison, backdoor_type='pattern', sources=np.arange(10), targets=(np.arange(10) + 1) % 10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Creates a backdoor in MNIST images by adding a pattern or pixel to the image and changing the label to a targeted\\n    class. Default parameters poison each digit so that it gets classified to the next digit.\\n\\n    :param x_clean: Original raw data\\n    :type x_clean: `np.ndarray`\\n    :param y_clean: Original labels\\n    :type y_clean:`np.ndarray`\\n    :param percent_poison: After poisoning, the target class should contain this percentage of poison\\n    :type percent_poison: `float`\\n    :param backdoor_type: Backdoor type can be `pixel` or `pattern`.\\n    :type backdoor_type: `str`\\n    :param sources: Array that holds the source classes for each backdoor. Poison is\\n    generating by taking images from the source class, adding the backdoor trigger, and labeling as the target class.\\n    Poisonous images from sources[i] will be labeled as targets[i].\\n    :type sources: `np.ndarray`\\n    :param targets: This array holds the target classes for each backdoor. Poisonous images from sources[i] will be\\n                    labeled as targets[i].\\n    :type targets: `np.ndarray`\\n    :return: Returns is_poison, which is a boolean array indicating which points are poisonous, x_poison, which\\n    contains all of the data both legitimate and poisoned, and y_poison, which contains all of the labels\\n    both legitimate and poisoned.\\n    :rtype: `tuple`\\n    '\n    max_val = np.max(x_clean)\n    x_poison = np.copy(x_clean)\n    y_poison = np.copy(y_clean)\n    is_poison = np.zeros(np.shape(y_poison))\n    for (i, (src, tgt)) in enumerate(zip(sources, targets)):\n        n_points_in_tgt = np.size(np.where(y_clean == tgt))\n        num_poison = round(percent_poison * n_points_in_tgt / (1 - percent_poison))\n        src_imgs = x_clean[y_clean == src]\n        n_points_in_src = np.shape(src_imgs)[0]\n        indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n        imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n        if backdoor_type == 'pattern':\n            imgs_to_be_poisoned = add_pattern_bd(x=imgs_to_be_poisoned, pixel_value=max_val)\n        elif backdoor_type == 'pixel':\n            imgs_to_be_poisoned = add_single_bd(imgs_to_be_poisoned, pixel_value=max_val)\n        x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n        y_poison = np.append(y_poison, np.ones(num_poison) * tgt, axis=0)\n        is_poison = np.append(is_poison, np.ones(num_poison))\n    is_poison = is_poison != 0\n    return (is_poison, x_poison, y_poison)",
            "def generate_backdoor(x_clean, y_clean, percent_poison, backdoor_type='pattern', sources=np.arange(10), targets=(np.arange(10) + 1) % 10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Creates a backdoor in MNIST images by adding a pattern or pixel to the image and changing the label to a targeted\\n    class. Default parameters poison each digit so that it gets classified to the next digit.\\n\\n    :param x_clean: Original raw data\\n    :type x_clean: `np.ndarray`\\n    :param y_clean: Original labels\\n    :type y_clean:`np.ndarray`\\n    :param percent_poison: After poisoning, the target class should contain this percentage of poison\\n    :type percent_poison: `float`\\n    :param backdoor_type: Backdoor type can be `pixel` or `pattern`.\\n    :type backdoor_type: `str`\\n    :param sources: Array that holds the source classes for each backdoor. Poison is\\n    generating by taking images from the source class, adding the backdoor trigger, and labeling as the target class.\\n    Poisonous images from sources[i] will be labeled as targets[i].\\n    :type sources: `np.ndarray`\\n    :param targets: This array holds the target classes for each backdoor. Poisonous images from sources[i] will be\\n                    labeled as targets[i].\\n    :type targets: `np.ndarray`\\n    :return: Returns is_poison, which is a boolean array indicating which points are poisonous, x_poison, which\\n    contains all of the data both legitimate and poisoned, and y_poison, which contains all of the labels\\n    both legitimate and poisoned.\\n    :rtype: `tuple`\\n    '\n    max_val = np.max(x_clean)\n    x_poison = np.copy(x_clean)\n    y_poison = np.copy(y_clean)\n    is_poison = np.zeros(np.shape(y_poison))\n    for (i, (src, tgt)) in enumerate(zip(sources, targets)):\n        n_points_in_tgt = np.size(np.where(y_clean == tgt))\n        num_poison = round(percent_poison * n_points_in_tgt / (1 - percent_poison))\n        src_imgs = x_clean[y_clean == src]\n        n_points_in_src = np.shape(src_imgs)[0]\n        indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n        imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n        if backdoor_type == 'pattern':\n            imgs_to_be_poisoned = add_pattern_bd(x=imgs_to_be_poisoned, pixel_value=max_val)\n        elif backdoor_type == 'pixel':\n            imgs_to_be_poisoned = add_single_bd(imgs_to_be_poisoned, pixel_value=max_val)\n        x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n        y_poison = np.append(y_poison, np.ones(num_poison) * tgt, axis=0)\n        is_poison = np.append(is_poison, np.ones(num_poison))\n    is_poison = is_poison != 0\n    return (is_poison, x_poison, y_poison)",
            "def generate_backdoor(x_clean, y_clean, percent_poison, backdoor_type='pattern', sources=np.arange(10), targets=(np.arange(10) + 1) % 10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Creates a backdoor in MNIST images by adding a pattern or pixel to the image and changing the label to a targeted\\n    class. Default parameters poison each digit so that it gets classified to the next digit.\\n\\n    :param x_clean: Original raw data\\n    :type x_clean: `np.ndarray`\\n    :param y_clean: Original labels\\n    :type y_clean:`np.ndarray`\\n    :param percent_poison: After poisoning, the target class should contain this percentage of poison\\n    :type percent_poison: `float`\\n    :param backdoor_type: Backdoor type can be `pixel` or `pattern`.\\n    :type backdoor_type: `str`\\n    :param sources: Array that holds the source classes for each backdoor. Poison is\\n    generating by taking images from the source class, adding the backdoor trigger, and labeling as the target class.\\n    Poisonous images from sources[i] will be labeled as targets[i].\\n    :type sources: `np.ndarray`\\n    :param targets: This array holds the target classes for each backdoor. Poisonous images from sources[i] will be\\n                    labeled as targets[i].\\n    :type targets: `np.ndarray`\\n    :return: Returns is_poison, which is a boolean array indicating which points are poisonous, x_poison, which\\n    contains all of the data both legitimate and poisoned, and y_poison, which contains all of the labels\\n    both legitimate and poisoned.\\n    :rtype: `tuple`\\n    '\n    max_val = np.max(x_clean)\n    x_poison = np.copy(x_clean)\n    y_poison = np.copy(y_clean)\n    is_poison = np.zeros(np.shape(y_poison))\n    for (i, (src, tgt)) in enumerate(zip(sources, targets)):\n        n_points_in_tgt = np.size(np.where(y_clean == tgt))\n        num_poison = round(percent_poison * n_points_in_tgt / (1 - percent_poison))\n        src_imgs = x_clean[y_clean == src]\n        n_points_in_src = np.shape(src_imgs)[0]\n        indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n        imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n        if backdoor_type == 'pattern':\n            imgs_to_be_poisoned = add_pattern_bd(x=imgs_to_be_poisoned, pixel_value=max_val)\n        elif backdoor_type == 'pixel':\n            imgs_to_be_poisoned = add_single_bd(imgs_to_be_poisoned, pixel_value=max_val)\n        x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n        y_poison = np.append(y_poison, np.ones(num_poison) * tgt, axis=0)\n        is_poison = np.append(is_poison, np.ones(num_poison))\n    is_poison = is_poison != 0\n    return (is_poison, x_poison, y_poison)",
            "def generate_backdoor(x_clean, y_clean, percent_poison, backdoor_type='pattern', sources=np.arange(10), targets=(np.arange(10) + 1) % 10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Creates a backdoor in MNIST images by adding a pattern or pixel to the image and changing the label to a targeted\\n    class. Default parameters poison each digit so that it gets classified to the next digit.\\n\\n    :param x_clean: Original raw data\\n    :type x_clean: `np.ndarray`\\n    :param y_clean: Original labels\\n    :type y_clean:`np.ndarray`\\n    :param percent_poison: After poisoning, the target class should contain this percentage of poison\\n    :type percent_poison: `float`\\n    :param backdoor_type: Backdoor type can be `pixel` or `pattern`.\\n    :type backdoor_type: `str`\\n    :param sources: Array that holds the source classes for each backdoor. Poison is\\n    generating by taking images from the source class, adding the backdoor trigger, and labeling as the target class.\\n    Poisonous images from sources[i] will be labeled as targets[i].\\n    :type sources: `np.ndarray`\\n    :param targets: This array holds the target classes for each backdoor. Poisonous images from sources[i] will be\\n                    labeled as targets[i].\\n    :type targets: `np.ndarray`\\n    :return: Returns is_poison, which is a boolean array indicating which points are poisonous, x_poison, which\\n    contains all of the data both legitimate and poisoned, and y_poison, which contains all of the labels\\n    both legitimate and poisoned.\\n    :rtype: `tuple`\\n    '\n    max_val = np.max(x_clean)\n    x_poison = np.copy(x_clean)\n    y_poison = np.copy(y_clean)\n    is_poison = np.zeros(np.shape(y_poison))\n    for (i, (src, tgt)) in enumerate(zip(sources, targets)):\n        n_points_in_tgt = np.size(np.where(y_clean == tgt))\n        num_poison = round(percent_poison * n_points_in_tgt / (1 - percent_poison))\n        src_imgs = x_clean[y_clean == src]\n        n_points_in_src = np.shape(src_imgs)[0]\n        indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n        imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n        if backdoor_type == 'pattern':\n            imgs_to_be_poisoned = add_pattern_bd(x=imgs_to_be_poisoned, pixel_value=max_val)\n        elif backdoor_type == 'pixel':\n            imgs_to_be_poisoned = add_single_bd(imgs_to_be_poisoned, pixel_value=max_val)\n        x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n        y_poison = np.append(y_poison, np.ones(num_poison) * tgt, axis=0)\n        is_poison = np.append(is_poison, np.ones(num_poison))\n    is_poison = is_poison != 0\n    return (is_poison, x_poison, y_poison)"
        ]
    }
]