[
    {
        "func_name": "__init__",
        "original": "def __init__(self, backdoor: PoisoningAttackBackdoor, proxy_classifier: 'CLASSIFIER_LOSS_GRADIENTS_TYPE', target: np.ndarray, pp_poison: float=0.33, norm: Union[int, float, str]=np.inf, eps: float=0.3, eps_step: float=0.1, max_iter: int=100, num_random_init: int=0) -> None:\n    \"\"\"\n        Creates a new Clean Label Backdoor poisoning attack\n\n        :param backdoor: the backdoor chosen for this attack\n        :param proxy_classifier: the classifier for this attack ideally it solves the same or similar classification\n                                 task as the original classifier\n        :param target: The target label to poison\n        :param pp_poison: The percentage of the data to poison. Note: Only data within the target label is poisoned\n        :param norm: The norm of the adversarial perturbation supporting \"inf\", np.inf, 1 or 2.\n        :param eps: Maximum perturbation that the attacker can introduce.\n        :param eps_step: Attack step size (input variation) at each iteration.\n        :param max_iter: The maximum number of iterations.\n        :param num_random_init: Number of random initialisations within the epsilon ball. For num_random_init=0 starting\n                                at the original input.\n        \"\"\"\n    super().__init__()\n    self.backdoor = backdoor\n    self.proxy_classifier = proxy_classifier\n    self.target = target\n    self.pp_poison = pp_poison\n    self.attack = ProjectedGradientDescent(proxy_classifier, norm=norm, eps=eps, eps_step=eps_step, max_iter=max_iter, targeted=False, num_random_init=num_random_init)\n    self._check_params()",
        "mutated": [
            "def __init__(self, backdoor: PoisoningAttackBackdoor, proxy_classifier: 'CLASSIFIER_LOSS_GRADIENTS_TYPE', target: np.ndarray, pp_poison: float=0.33, norm: Union[int, float, str]=np.inf, eps: float=0.3, eps_step: float=0.1, max_iter: int=100, num_random_init: int=0) -> None:\n    if False:\n        i = 10\n    '\\n        Creates a new Clean Label Backdoor poisoning attack\\n\\n        :param backdoor: the backdoor chosen for this attack\\n        :param proxy_classifier: the classifier for this attack ideally it solves the same or similar classification\\n                                 task as the original classifier\\n        :param target: The target label to poison\\n        :param pp_poison: The percentage of the data to poison. Note: Only data within the target label is poisoned\\n        :param norm: The norm of the adversarial perturbation supporting \"inf\", np.inf, 1 or 2.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param eps_step: Attack step size (input variation) at each iteration.\\n        :param max_iter: The maximum number of iterations.\\n        :param num_random_init: Number of random initialisations within the epsilon ball. For num_random_init=0 starting\\n                                at the original input.\\n        '\n    super().__init__()\n    self.backdoor = backdoor\n    self.proxy_classifier = proxy_classifier\n    self.target = target\n    self.pp_poison = pp_poison\n    self.attack = ProjectedGradientDescent(proxy_classifier, norm=norm, eps=eps, eps_step=eps_step, max_iter=max_iter, targeted=False, num_random_init=num_random_init)\n    self._check_params()",
            "def __init__(self, backdoor: PoisoningAttackBackdoor, proxy_classifier: 'CLASSIFIER_LOSS_GRADIENTS_TYPE', target: np.ndarray, pp_poison: float=0.33, norm: Union[int, float, str]=np.inf, eps: float=0.3, eps_step: float=0.1, max_iter: int=100, num_random_init: int=0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Creates a new Clean Label Backdoor poisoning attack\\n\\n        :param backdoor: the backdoor chosen for this attack\\n        :param proxy_classifier: the classifier for this attack ideally it solves the same or similar classification\\n                                 task as the original classifier\\n        :param target: The target label to poison\\n        :param pp_poison: The percentage of the data to poison. Note: Only data within the target label is poisoned\\n        :param norm: The norm of the adversarial perturbation supporting \"inf\", np.inf, 1 or 2.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param eps_step: Attack step size (input variation) at each iteration.\\n        :param max_iter: The maximum number of iterations.\\n        :param num_random_init: Number of random initialisations within the epsilon ball. For num_random_init=0 starting\\n                                at the original input.\\n        '\n    super().__init__()\n    self.backdoor = backdoor\n    self.proxy_classifier = proxy_classifier\n    self.target = target\n    self.pp_poison = pp_poison\n    self.attack = ProjectedGradientDescent(proxy_classifier, norm=norm, eps=eps, eps_step=eps_step, max_iter=max_iter, targeted=False, num_random_init=num_random_init)\n    self._check_params()",
            "def __init__(self, backdoor: PoisoningAttackBackdoor, proxy_classifier: 'CLASSIFIER_LOSS_GRADIENTS_TYPE', target: np.ndarray, pp_poison: float=0.33, norm: Union[int, float, str]=np.inf, eps: float=0.3, eps_step: float=0.1, max_iter: int=100, num_random_init: int=0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Creates a new Clean Label Backdoor poisoning attack\\n\\n        :param backdoor: the backdoor chosen for this attack\\n        :param proxy_classifier: the classifier for this attack ideally it solves the same or similar classification\\n                                 task as the original classifier\\n        :param target: The target label to poison\\n        :param pp_poison: The percentage of the data to poison. Note: Only data within the target label is poisoned\\n        :param norm: The norm of the adversarial perturbation supporting \"inf\", np.inf, 1 or 2.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param eps_step: Attack step size (input variation) at each iteration.\\n        :param max_iter: The maximum number of iterations.\\n        :param num_random_init: Number of random initialisations within the epsilon ball. For num_random_init=0 starting\\n                                at the original input.\\n        '\n    super().__init__()\n    self.backdoor = backdoor\n    self.proxy_classifier = proxy_classifier\n    self.target = target\n    self.pp_poison = pp_poison\n    self.attack = ProjectedGradientDescent(proxy_classifier, norm=norm, eps=eps, eps_step=eps_step, max_iter=max_iter, targeted=False, num_random_init=num_random_init)\n    self._check_params()",
            "def __init__(self, backdoor: PoisoningAttackBackdoor, proxy_classifier: 'CLASSIFIER_LOSS_GRADIENTS_TYPE', target: np.ndarray, pp_poison: float=0.33, norm: Union[int, float, str]=np.inf, eps: float=0.3, eps_step: float=0.1, max_iter: int=100, num_random_init: int=0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Creates a new Clean Label Backdoor poisoning attack\\n\\n        :param backdoor: the backdoor chosen for this attack\\n        :param proxy_classifier: the classifier for this attack ideally it solves the same or similar classification\\n                                 task as the original classifier\\n        :param target: The target label to poison\\n        :param pp_poison: The percentage of the data to poison. Note: Only data within the target label is poisoned\\n        :param norm: The norm of the adversarial perturbation supporting \"inf\", np.inf, 1 or 2.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param eps_step: Attack step size (input variation) at each iteration.\\n        :param max_iter: The maximum number of iterations.\\n        :param num_random_init: Number of random initialisations within the epsilon ball. For num_random_init=0 starting\\n                                at the original input.\\n        '\n    super().__init__()\n    self.backdoor = backdoor\n    self.proxy_classifier = proxy_classifier\n    self.target = target\n    self.pp_poison = pp_poison\n    self.attack = ProjectedGradientDescent(proxy_classifier, norm=norm, eps=eps, eps_step=eps_step, max_iter=max_iter, targeted=False, num_random_init=num_random_init)\n    self._check_params()",
            "def __init__(self, backdoor: PoisoningAttackBackdoor, proxy_classifier: 'CLASSIFIER_LOSS_GRADIENTS_TYPE', target: np.ndarray, pp_poison: float=0.33, norm: Union[int, float, str]=np.inf, eps: float=0.3, eps_step: float=0.1, max_iter: int=100, num_random_init: int=0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Creates a new Clean Label Backdoor poisoning attack\\n\\n        :param backdoor: the backdoor chosen for this attack\\n        :param proxy_classifier: the classifier for this attack ideally it solves the same or similar classification\\n                                 task as the original classifier\\n        :param target: The target label to poison\\n        :param pp_poison: The percentage of the data to poison. Note: Only data within the target label is poisoned\\n        :param norm: The norm of the adversarial perturbation supporting \"inf\", np.inf, 1 or 2.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param eps_step: Attack step size (input variation) at each iteration.\\n        :param max_iter: The maximum number of iterations.\\n        :param num_random_init: Number of random initialisations within the epsilon ball. For num_random_init=0 starting\\n                                at the original input.\\n        '\n    super().__init__()\n    self.backdoor = backdoor\n    self.proxy_classifier = proxy_classifier\n    self.target = target\n    self.pp_poison = pp_poison\n    self.attack = ProjectedGradientDescent(proxy_classifier, norm=norm, eps=eps, eps_step=eps_step, max_iter=max_iter, targeted=False, num_random_init=num_random_init)\n    self._check_params()"
        ]
    },
    {
        "func_name": "poison",
        "original": "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, broadcast: bool=True, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        Calls perturbation function on input x and returns the perturbed input and poison labels for the data.\n\n        :param x: An array with the points that initialize attack points.\n        :param y: The target labels for the attack.\n        :param broadcast: whether or not to broadcast single target label\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\n        \"\"\"\n    data = np.copy(x)\n    estimated_labels = self.proxy_classifier.predict(data) if y is None else np.copy(y)\n    all_indices = np.arange(len(data))\n    target_indices = all_indices[np.all(estimated_labels == self.target, axis=1)]\n    num_poison = int(self.pp_poison * len(target_indices))\n    selected_indices = np.random.choice(target_indices, num_poison)\n    perturbed_input = self.attack.generate(data[selected_indices])\n    no_change_detected = np.array([np.all(data[selected_indices][poison_idx] == perturbed_input[poison_idx]) for poison_idx in range(len(perturbed_input))])\n    if any(no_change_detected):\n        logger.warning('Perturbed input is the same as original data after PGD. Check params.')\n        idx_no_change = np.arange(len(no_change_detected))[no_change_detected]\n        logger.warning('%d indices without change: %s', len(idx_no_change), idx_no_change)\n    (poisoned_input, _) = self.backdoor.poison(perturbed_input, self.target, broadcast=broadcast)\n    data[selected_indices] = poisoned_input\n    return (data, estimated_labels)",
        "mutated": [
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, broadcast: bool=True, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n    '\\n        Calls perturbation function on input x and returns the perturbed input and poison labels for the data.\\n\\n        :param x: An array with the points that initialize attack points.\\n        :param y: The target labels for the attack.\\n        :param broadcast: whether or not to broadcast single target label\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    data = np.copy(x)\n    estimated_labels = self.proxy_classifier.predict(data) if y is None else np.copy(y)\n    all_indices = np.arange(len(data))\n    target_indices = all_indices[np.all(estimated_labels == self.target, axis=1)]\n    num_poison = int(self.pp_poison * len(target_indices))\n    selected_indices = np.random.choice(target_indices, num_poison)\n    perturbed_input = self.attack.generate(data[selected_indices])\n    no_change_detected = np.array([np.all(data[selected_indices][poison_idx] == perturbed_input[poison_idx]) for poison_idx in range(len(perturbed_input))])\n    if any(no_change_detected):\n        logger.warning('Perturbed input is the same as original data after PGD. Check params.')\n        idx_no_change = np.arange(len(no_change_detected))[no_change_detected]\n        logger.warning('%d indices without change: %s', len(idx_no_change), idx_no_change)\n    (poisoned_input, _) = self.backdoor.poison(perturbed_input, self.target, broadcast=broadcast)\n    data[selected_indices] = poisoned_input\n    return (data, estimated_labels)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, broadcast: bool=True, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Calls perturbation function on input x and returns the perturbed input and poison labels for the data.\\n\\n        :param x: An array with the points that initialize attack points.\\n        :param y: The target labels for the attack.\\n        :param broadcast: whether or not to broadcast single target label\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    data = np.copy(x)\n    estimated_labels = self.proxy_classifier.predict(data) if y is None else np.copy(y)\n    all_indices = np.arange(len(data))\n    target_indices = all_indices[np.all(estimated_labels == self.target, axis=1)]\n    num_poison = int(self.pp_poison * len(target_indices))\n    selected_indices = np.random.choice(target_indices, num_poison)\n    perturbed_input = self.attack.generate(data[selected_indices])\n    no_change_detected = np.array([np.all(data[selected_indices][poison_idx] == perturbed_input[poison_idx]) for poison_idx in range(len(perturbed_input))])\n    if any(no_change_detected):\n        logger.warning('Perturbed input is the same as original data after PGD. Check params.')\n        idx_no_change = np.arange(len(no_change_detected))[no_change_detected]\n        logger.warning('%d indices without change: %s', len(idx_no_change), idx_no_change)\n    (poisoned_input, _) = self.backdoor.poison(perturbed_input, self.target, broadcast=broadcast)\n    data[selected_indices] = poisoned_input\n    return (data, estimated_labels)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, broadcast: bool=True, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Calls perturbation function on input x and returns the perturbed input and poison labels for the data.\\n\\n        :param x: An array with the points that initialize attack points.\\n        :param y: The target labels for the attack.\\n        :param broadcast: whether or not to broadcast single target label\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    data = np.copy(x)\n    estimated_labels = self.proxy_classifier.predict(data) if y is None else np.copy(y)\n    all_indices = np.arange(len(data))\n    target_indices = all_indices[np.all(estimated_labels == self.target, axis=1)]\n    num_poison = int(self.pp_poison * len(target_indices))\n    selected_indices = np.random.choice(target_indices, num_poison)\n    perturbed_input = self.attack.generate(data[selected_indices])\n    no_change_detected = np.array([np.all(data[selected_indices][poison_idx] == perturbed_input[poison_idx]) for poison_idx in range(len(perturbed_input))])\n    if any(no_change_detected):\n        logger.warning('Perturbed input is the same as original data after PGD. Check params.')\n        idx_no_change = np.arange(len(no_change_detected))[no_change_detected]\n        logger.warning('%d indices without change: %s', len(idx_no_change), idx_no_change)\n    (poisoned_input, _) = self.backdoor.poison(perturbed_input, self.target, broadcast=broadcast)\n    data[selected_indices] = poisoned_input\n    return (data, estimated_labels)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, broadcast: bool=True, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Calls perturbation function on input x and returns the perturbed input and poison labels for the data.\\n\\n        :param x: An array with the points that initialize attack points.\\n        :param y: The target labels for the attack.\\n        :param broadcast: whether or not to broadcast single target label\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    data = np.copy(x)\n    estimated_labels = self.proxy_classifier.predict(data) if y is None else np.copy(y)\n    all_indices = np.arange(len(data))\n    target_indices = all_indices[np.all(estimated_labels == self.target, axis=1)]\n    num_poison = int(self.pp_poison * len(target_indices))\n    selected_indices = np.random.choice(target_indices, num_poison)\n    perturbed_input = self.attack.generate(data[selected_indices])\n    no_change_detected = np.array([np.all(data[selected_indices][poison_idx] == perturbed_input[poison_idx]) for poison_idx in range(len(perturbed_input))])\n    if any(no_change_detected):\n        logger.warning('Perturbed input is the same as original data after PGD. Check params.')\n        idx_no_change = np.arange(len(no_change_detected))[no_change_detected]\n        logger.warning('%d indices without change: %s', len(idx_no_change), idx_no_change)\n    (poisoned_input, _) = self.backdoor.poison(perturbed_input, self.target, broadcast=broadcast)\n    data[selected_indices] = poisoned_input\n    return (data, estimated_labels)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, broadcast: bool=True, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Calls perturbation function on input x and returns the perturbed input and poison labels for the data.\\n\\n        :param x: An array with the points that initialize attack points.\\n        :param y: The target labels for the attack.\\n        :param broadcast: whether or not to broadcast single target label\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    data = np.copy(x)\n    estimated_labels = self.proxy_classifier.predict(data) if y is None else np.copy(y)\n    all_indices = np.arange(len(data))\n    target_indices = all_indices[np.all(estimated_labels == self.target, axis=1)]\n    num_poison = int(self.pp_poison * len(target_indices))\n    selected_indices = np.random.choice(target_indices, num_poison)\n    perturbed_input = self.attack.generate(data[selected_indices])\n    no_change_detected = np.array([np.all(data[selected_indices][poison_idx] == perturbed_input[poison_idx]) for poison_idx in range(len(perturbed_input))])\n    if any(no_change_detected):\n        logger.warning('Perturbed input is the same as original data after PGD. Check params.')\n        idx_no_change = np.arange(len(no_change_detected))[no_change_detected]\n        logger.warning('%d indices without change: %s', len(idx_no_change), idx_no_change)\n    (poisoned_input, _) = self.backdoor.poison(perturbed_input, self.target, broadcast=broadcast)\n    data[selected_indices] = poisoned_input\n    return (data, estimated_labels)"
        ]
    },
    {
        "func_name": "_check_params",
        "original": "def _check_params(self) -> None:\n    if not isinstance(self.backdoor, PoisoningAttackBackdoor):\n        raise ValueError('Backdoor must be of type PoisoningAttackBackdoor')\n    if not isinstance(self.attack, ProjectedGradientDescent):\n        raise ValueError('There was an issue creating the PGD attack')\n    if not 0 < self.pp_poison < 1:\n        raise ValueError('pp_poison must be between 0 and 1')",
        "mutated": [
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n    if not isinstance(self.backdoor, PoisoningAttackBackdoor):\n        raise ValueError('Backdoor must be of type PoisoningAttackBackdoor')\n    if not isinstance(self.attack, ProjectedGradientDescent):\n        raise ValueError('There was an issue creating the PGD attack')\n    if not 0 < self.pp_poison < 1:\n        raise ValueError('pp_poison must be between 0 and 1')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not isinstance(self.backdoor, PoisoningAttackBackdoor):\n        raise ValueError('Backdoor must be of type PoisoningAttackBackdoor')\n    if not isinstance(self.attack, ProjectedGradientDescent):\n        raise ValueError('There was an issue creating the PGD attack')\n    if not 0 < self.pp_poison < 1:\n        raise ValueError('pp_poison must be between 0 and 1')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not isinstance(self.backdoor, PoisoningAttackBackdoor):\n        raise ValueError('Backdoor must be of type PoisoningAttackBackdoor')\n    if not isinstance(self.attack, ProjectedGradientDescent):\n        raise ValueError('There was an issue creating the PGD attack')\n    if not 0 < self.pp_poison < 1:\n        raise ValueError('pp_poison must be between 0 and 1')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not isinstance(self.backdoor, PoisoningAttackBackdoor):\n        raise ValueError('Backdoor must be of type PoisoningAttackBackdoor')\n    if not isinstance(self.attack, ProjectedGradientDescent):\n        raise ValueError('There was an issue creating the PGD attack')\n    if not 0 < self.pp_poison < 1:\n        raise ValueError('pp_poison must be between 0 and 1')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not isinstance(self.backdoor, PoisoningAttackBackdoor):\n        raise ValueError('Backdoor must be of type PoisoningAttackBackdoor')\n    if not isinstance(self.attack, ProjectedGradientDescent):\n        raise ValueError('There was an issue creating the PGD attack')\n    if not 0 < self.pp_poison < 1:\n        raise ValueError('pp_poison must be between 0 and 1')"
        ]
    }
]