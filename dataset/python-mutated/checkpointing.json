[
    {
        "func_name": "get_checkpoint_names",
        "original": "def get_checkpoint_names(checkpoints_path, path_load_tag, num_experts, tensor_rank=None, expp_rank=None):\n    \"\"\"Determine the directory name for this rank's checkpoint.\"\"\"\n    if tensor_rank is None:\n        tensor_rank = mpu.get_tensor_model_parallel_rank()\n    common_path = os.path.join(checkpoints_path, path_load_tag, f'mp_rank_{tensor_rank:02d}')\n    if num_experts[0] > 0:\n        model_name = os.path.join(common_path, 'model_rng.pt')\n        optim_name = os.path.join(checkpoints_path, path_load_tag, f'expp_rank_{expp_rank}_mp_rank_{tensor_rank:02d}_optim_states.pt')\n    else:\n        model_name = optim_name = os.path.join(common_path, 'model_optim_rng.pt')\n    return (model_name, optim_name)",
        "mutated": [
            "def get_checkpoint_names(checkpoints_path, path_load_tag, num_experts, tensor_rank=None, expp_rank=None):\n    if False:\n        i = 10\n    \"Determine the directory name for this rank's checkpoint.\"\n    if tensor_rank is None:\n        tensor_rank = mpu.get_tensor_model_parallel_rank()\n    common_path = os.path.join(checkpoints_path, path_load_tag, f'mp_rank_{tensor_rank:02d}')\n    if num_experts[0] > 0:\n        model_name = os.path.join(common_path, 'model_rng.pt')\n        optim_name = os.path.join(checkpoints_path, path_load_tag, f'expp_rank_{expp_rank}_mp_rank_{tensor_rank:02d}_optim_states.pt')\n    else:\n        model_name = optim_name = os.path.join(common_path, 'model_optim_rng.pt')\n    return (model_name, optim_name)",
            "def get_checkpoint_names(checkpoints_path, path_load_tag, num_experts, tensor_rank=None, expp_rank=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Determine the directory name for this rank's checkpoint.\"\n    if tensor_rank is None:\n        tensor_rank = mpu.get_tensor_model_parallel_rank()\n    common_path = os.path.join(checkpoints_path, path_load_tag, f'mp_rank_{tensor_rank:02d}')\n    if num_experts[0] > 0:\n        model_name = os.path.join(common_path, 'model_rng.pt')\n        optim_name = os.path.join(checkpoints_path, path_load_tag, f'expp_rank_{expp_rank}_mp_rank_{tensor_rank:02d}_optim_states.pt')\n    else:\n        model_name = optim_name = os.path.join(common_path, 'model_optim_rng.pt')\n    return (model_name, optim_name)",
            "def get_checkpoint_names(checkpoints_path, path_load_tag, num_experts, tensor_rank=None, expp_rank=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Determine the directory name for this rank's checkpoint.\"\n    if tensor_rank is None:\n        tensor_rank = mpu.get_tensor_model_parallel_rank()\n    common_path = os.path.join(checkpoints_path, path_load_tag, f'mp_rank_{tensor_rank:02d}')\n    if num_experts[0] > 0:\n        model_name = os.path.join(common_path, 'model_rng.pt')\n        optim_name = os.path.join(checkpoints_path, path_load_tag, f'expp_rank_{expp_rank}_mp_rank_{tensor_rank:02d}_optim_states.pt')\n    else:\n        model_name = optim_name = os.path.join(common_path, 'model_optim_rng.pt')\n    return (model_name, optim_name)",
            "def get_checkpoint_names(checkpoints_path, path_load_tag, num_experts, tensor_rank=None, expp_rank=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Determine the directory name for this rank's checkpoint.\"\n    if tensor_rank is None:\n        tensor_rank = mpu.get_tensor_model_parallel_rank()\n    common_path = os.path.join(checkpoints_path, path_load_tag, f'mp_rank_{tensor_rank:02d}')\n    if num_experts[0] > 0:\n        model_name = os.path.join(common_path, 'model_rng.pt')\n        optim_name = os.path.join(checkpoints_path, path_load_tag, f'expp_rank_{expp_rank}_mp_rank_{tensor_rank:02d}_optim_states.pt')\n    else:\n        model_name = optim_name = os.path.join(common_path, 'model_optim_rng.pt')\n    return (model_name, optim_name)",
            "def get_checkpoint_names(checkpoints_path, path_load_tag, num_experts, tensor_rank=None, expp_rank=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Determine the directory name for this rank's checkpoint.\"\n    if tensor_rank is None:\n        tensor_rank = mpu.get_tensor_model_parallel_rank()\n    common_path = os.path.join(checkpoints_path, path_load_tag, f'mp_rank_{tensor_rank:02d}')\n    if num_experts[0] > 0:\n        model_name = os.path.join(common_path, 'model_rng.pt')\n        optim_name = os.path.join(checkpoints_path, path_load_tag, f'expp_rank_{expp_rank}_mp_rank_{tensor_rank:02d}_optim_states.pt')\n    else:\n        model_name = optim_name = os.path.join(common_path, 'model_optim_rng.pt')\n    return (model_name, optim_name)"
        ]
    },
    {
        "func_name": "_get_expert_ckpt_name",
        "original": "def _get_expert_ckpt_name(checkpoints_path, layer_id, expert_id):\n    mp_rank = mpu.get_tensor_model_parallel_rank()\n    ckpt_name = os.path.join(os.path.join(checkpoints_path, 'model'), f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')\n    return ckpt_name",
        "mutated": [
            "def _get_expert_ckpt_name(checkpoints_path, layer_id, expert_id):\n    if False:\n        i = 10\n    mp_rank = mpu.get_tensor_model_parallel_rank()\n    ckpt_name = os.path.join(os.path.join(checkpoints_path, 'model'), f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')\n    return ckpt_name",
            "def _get_expert_ckpt_name(checkpoints_path, layer_id, expert_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mp_rank = mpu.get_tensor_model_parallel_rank()\n    ckpt_name = os.path.join(os.path.join(checkpoints_path, 'model'), f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')\n    return ckpt_name",
            "def _get_expert_ckpt_name(checkpoints_path, layer_id, expert_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mp_rank = mpu.get_tensor_model_parallel_rank()\n    ckpt_name = os.path.join(os.path.join(checkpoints_path, 'model'), f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')\n    return ckpt_name",
            "def _get_expert_ckpt_name(checkpoints_path, layer_id, expert_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mp_rank = mpu.get_tensor_model_parallel_rank()\n    ckpt_name = os.path.join(os.path.join(checkpoints_path, 'model'), f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')\n    return ckpt_name",
            "def _get_expert_ckpt_name(checkpoints_path, layer_id, expert_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mp_rank = mpu.get_tensor_model_parallel_rank()\n    ckpt_name = os.path.join(os.path.join(checkpoints_path, 'model'), f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')\n    return ckpt_name"
        ]
    },
    {
        "func_name": "_load_base_checkpoint",
        "original": "def _load_base_checkpoint(load_dir, path_load_tag=None, num_experts=None):\n    \"\"\" Load the base state_dict from the given directory\n\n    If rank0 is true, just loads rank 0 checkpoint, ignoring arguments.\n    \"\"\"\n    largest_group_name = mpu.get_max_expert_size_name()\n    expp_rank = mpu.get_expert_parallel_rank(largest_group_name)\n    checkpoint_names = get_checkpoint_names(load_dir, path_load_tag=path_load_tag, num_experts=num_experts, expp_rank=expp_rank)\n    (model_checkpoint_name, optim_checkpoint_name) = checkpoint_names\n    logger.info(f'Loading model checkpoint from {model_checkpoint_name}')\n    model_state_dict = torch.load(model_checkpoint_name, map_location='cpu')\n    return model_state_dict",
        "mutated": [
            "def _load_base_checkpoint(load_dir, path_load_tag=None, num_experts=None):\n    if False:\n        i = 10\n    ' Load the base state_dict from the given directory\\n\\n    If rank0 is true, just loads rank 0 checkpoint, ignoring arguments.\\n    '\n    largest_group_name = mpu.get_max_expert_size_name()\n    expp_rank = mpu.get_expert_parallel_rank(largest_group_name)\n    checkpoint_names = get_checkpoint_names(load_dir, path_load_tag=path_load_tag, num_experts=num_experts, expp_rank=expp_rank)\n    (model_checkpoint_name, optim_checkpoint_name) = checkpoint_names\n    logger.info(f'Loading model checkpoint from {model_checkpoint_name}')\n    model_state_dict = torch.load(model_checkpoint_name, map_location='cpu')\n    return model_state_dict",
            "def _load_base_checkpoint(load_dir, path_load_tag=None, num_experts=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Load the base state_dict from the given directory\\n\\n    If rank0 is true, just loads rank 0 checkpoint, ignoring arguments.\\n    '\n    largest_group_name = mpu.get_max_expert_size_name()\n    expp_rank = mpu.get_expert_parallel_rank(largest_group_name)\n    checkpoint_names = get_checkpoint_names(load_dir, path_load_tag=path_load_tag, num_experts=num_experts, expp_rank=expp_rank)\n    (model_checkpoint_name, optim_checkpoint_name) = checkpoint_names\n    logger.info(f'Loading model checkpoint from {model_checkpoint_name}')\n    model_state_dict = torch.load(model_checkpoint_name, map_location='cpu')\n    return model_state_dict",
            "def _load_base_checkpoint(load_dir, path_load_tag=None, num_experts=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Load the base state_dict from the given directory\\n\\n    If rank0 is true, just loads rank 0 checkpoint, ignoring arguments.\\n    '\n    largest_group_name = mpu.get_max_expert_size_name()\n    expp_rank = mpu.get_expert_parallel_rank(largest_group_name)\n    checkpoint_names = get_checkpoint_names(load_dir, path_load_tag=path_load_tag, num_experts=num_experts, expp_rank=expp_rank)\n    (model_checkpoint_name, optim_checkpoint_name) = checkpoint_names\n    logger.info(f'Loading model checkpoint from {model_checkpoint_name}')\n    model_state_dict = torch.load(model_checkpoint_name, map_location='cpu')\n    return model_state_dict",
            "def _load_base_checkpoint(load_dir, path_load_tag=None, num_experts=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Load the base state_dict from the given directory\\n\\n    If rank0 is true, just loads rank 0 checkpoint, ignoring arguments.\\n    '\n    largest_group_name = mpu.get_max_expert_size_name()\n    expp_rank = mpu.get_expert_parallel_rank(largest_group_name)\n    checkpoint_names = get_checkpoint_names(load_dir, path_load_tag=path_load_tag, num_experts=num_experts, expp_rank=expp_rank)\n    (model_checkpoint_name, optim_checkpoint_name) = checkpoint_names\n    logger.info(f'Loading model checkpoint from {model_checkpoint_name}')\n    model_state_dict = torch.load(model_checkpoint_name, map_location='cpu')\n    return model_state_dict",
            "def _load_base_checkpoint(load_dir, path_load_tag=None, num_experts=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Load the base state_dict from the given directory\\n\\n    If rank0 is true, just loads rank 0 checkpoint, ignoring arguments.\\n    '\n    largest_group_name = mpu.get_max_expert_size_name()\n    expp_rank = mpu.get_expert_parallel_rank(largest_group_name)\n    checkpoint_names = get_checkpoint_names(load_dir, path_load_tag=path_load_tag, num_experts=num_experts, expp_rank=expp_rank)\n    (model_checkpoint_name, optim_checkpoint_name) = checkpoint_names\n    logger.info(f'Loading model checkpoint from {model_checkpoint_name}')\n    model_state_dict = torch.load(model_checkpoint_name, map_location='cpu')\n    return model_state_dict"
        ]
    },
    {
        "func_name": "load_checkpoint",
        "original": "def load_checkpoint(model, load_dir, num_experts=None, strict=True, path_load_tag='model', load_ds_ckpts=True):\n    model = unwrap_model(model, (torchDDP, Float16Module))\n    model_state_dict = _load_base_checkpoint(load_dir, path_load_tag=path_load_tag, num_experts=num_experts)\n    assert model_state_dict is not None\n    if load_ds_ckpts:\n        load_moe_checkpoint(model, model_state_dict['module'], load_dir)\n    else:\n        load_moe_checkpoint(model, model_state_dict['model'], load_dir)\n    if load_ds_ckpts:\n        model.load_state_dict(model_state_dict['module'], strict=strict)\n    else:\n        model.load_state_dict(model_state_dict['model'], strict=strict)\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier()",
        "mutated": [
            "def load_checkpoint(model, load_dir, num_experts=None, strict=True, path_load_tag='model', load_ds_ckpts=True):\n    if False:\n        i = 10\n    model = unwrap_model(model, (torchDDP, Float16Module))\n    model_state_dict = _load_base_checkpoint(load_dir, path_load_tag=path_load_tag, num_experts=num_experts)\n    assert model_state_dict is not None\n    if load_ds_ckpts:\n        load_moe_checkpoint(model, model_state_dict['module'], load_dir)\n    else:\n        load_moe_checkpoint(model, model_state_dict['model'], load_dir)\n    if load_ds_ckpts:\n        model.load_state_dict(model_state_dict['module'], strict=strict)\n    else:\n        model.load_state_dict(model_state_dict['model'], strict=strict)\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier()",
            "def load_checkpoint(model, load_dir, num_experts=None, strict=True, path_load_tag='model', load_ds_ckpts=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model = unwrap_model(model, (torchDDP, Float16Module))\n    model_state_dict = _load_base_checkpoint(load_dir, path_load_tag=path_load_tag, num_experts=num_experts)\n    assert model_state_dict is not None\n    if load_ds_ckpts:\n        load_moe_checkpoint(model, model_state_dict['module'], load_dir)\n    else:\n        load_moe_checkpoint(model, model_state_dict['model'], load_dir)\n    if load_ds_ckpts:\n        model.load_state_dict(model_state_dict['module'], strict=strict)\n    else:\n        model.load_state_dict(model_state_dict['model'], strict=strict)\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier()",
            "def load_checkpoint(model, load_dir, num_experts=None, strict=True, path_load_tag='model', load_ds_ckpts=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model = unwrap_model(model, (torchDDP, Float16Module))\n    model_state_dict = _load_base_checkpoint(load_dir, path_load_tag=path_load_tag, num_experts=num_experts)\n    assert model_state_dict is not None\n    if load_ds_ckpts:\n        load_moe_checkpoint(model, model_state_dict['module'], load_dir)\n    else:\n        load_moe_checkpoint(model, model_state_dict['model'], load_dir)\n    if load_ds_ckpts:\n        model.load_state_dict(model_state_dict['module'], strict=strict)\n    else:\n        model.load_state_dict(model_state_dict['model'], strict=strict)\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier()",
            "def load_checkpoint(model, load_dir, num_experts=None, strict=True, path_load_tag='model', load_ds_ckpts=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model = unwrap_model(model, (torchDDP, Float16Module))\n    model_state_dict = _load_base_checkpoint(load_dir, path_load_tag=path_load_tag, num_experts=num_experts)\n    assert model_state_dict is not None\n    if load_ds_ckpts:\n        load_moe_checkpoint(model, model_state_dict['module'], load_dir)\n    else:\n        load_moe_checkpoint(model, model_state_dict['model'], load_dir)\n    if load_ds_ckpts:\n        model.load_state_dict(model_state_dict['module'], strict=strict)\n    else:\n        model.load_state_dict(model_state_dict['model'], strict=strict)\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier()",
            "def load_checkpoint(model, load_dir, num_experts=None, strict=True, path_load_tag='model', load_ds_ckpts=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model = unwrap_model(model, (torchDDP, Float16Module))\n    model_state_dict = _load_base_checkpoint(load_dir, path_load_tag=path_load_tag, num_experts=num_experts)\n    assert model_state_dict is not None\n    if load_ds_ckpts:\n        load_moe_checkpoint(model, model_state_dict['module'], load_dir)\n    else:\n        load_moe_checkpoint(model, model_state_dict['model'], load_dir)\n    if load_ds_ckpts:\n        model.load_state_dict(model_state_dict['module'], strict=strict)\n    else:\n        model.load_state_dict(model_state_dict['model'], strict=strict)\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier()"
        ]
    },
    {
        "func_name": "load_moe_checkpoint",
        "original": "def load_moe_checkpoint(model, state_dict, load_dir):\n    moe_layer_id = 0\n    for (n_module, module) in model.named_modules():\n        if isinstance(module, MoE):\n            group_name = module.expert_group_name\n            num_local_experts = module.num_local_experts\n            expp_rank = mpu.get_expert_parallel_rank(group_name)\n            for local_expert_id in range(num_local_experts):\n                global_expert_id = expp_rank * num_local_experts + local_expert_id\n                moe_load_path = _get_expert_ckpt_name(load_dir, moe_layer_id, global_expert_id)\n                logger.info(f'Loading expert states from {moe_load_path}')\n                expert_state_dict = torch.load(moe_load_path, map_location=torch.device('cpu'))\n                moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                for key in list(expert_state_dict.keys()):\n                    local_key = key.replace(f'{moe_str_prefix}{global_expert_id}', f'{moe_str_prefix}{local_expert_id}')\n                    expert_state_dict[local_key] = expert_state_dict.pop(key)\n                state_dict.update(expert_state_dict)\n            moe_layer_id += 1",
        "mutated": [
            "def load_moe_checkpoint(model, state_dict, load_dir):\n    if False:\n        i = 10\n    moe_layer_id = 0\n    for (n_module, module) in model.named_modules():\n        if isinstance(module, MoE):\n            group_name = module.expert_group_name\n            num_local_experts = module.num_local_experts\n            expp_rank = mpu.get_expert_parallel_rank(group_name)\n            for local_expert_id in range(num_local_experts):\n                global_expert_id = expp_rank * num_local_experts + local_expert_id\n                moe_load_path = _get_expert_ckpt_name(load_dir, moe_layer_id, global_expert_id)\n                logger.info(f'Loading expert states from {moe_load_path}')\n                expert_state_dict = torch.load(moe_load_path, map_location=torch.device('cpu'))\n                moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                for key in list(expert_state_dict.keys()):\n                    local_key = key.replace(f'{moe_str_prefix}{global_expert_id}', f'{moe_str_prefix}{local_expert_id}')\n                    expert_state_dict[local_key] = expert_state_dict.pop(key)\n                state_dict.update(expert_state_dict)\n            moe_layer_id += 1",
            "def load_moe_checkpoint(model, state_dict, load_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    moe_layer_id = 0\n    for (n_module, module) in model.named_modules():\n        if isinstance(module, MoE):\n            group_name = module.expert_group_name\n            num_local_experts = module.num_local_experts\n            expp_rank = mpu.get_expert_parallel_rank(group_name)\n            for local_expert_id in range(num_local_experts):\n                global_expert_id = expp_rank * num_local_experts + local_expert_id\n                moe_load_path = _get_expert_ckpt_name(load_dir, moe_layer_id, global_expert_id)\n                logger.info(f'Loading expert states from {moe_load_path}')\n                expert_state_dict = torch.load(moe_load_path, map_location=torch.device('cpu'))\n                moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                for key in list(expert_state_dict.keys()):\n                    local_key = key.replace(f'{moe_str_prefix}{global_expert_id}', f'{moe_str_prefix}{local_expert_id}')\n                    expert_state_dict[local_key] = expert_state_dict.pop(key)\n                state_dict.update(expert_state_dict)\n            moe_layer_id += 1",
            "def load_moe_checkpoint(model, state_dict, load_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    moe_layer_id = 0\n    for (n_module, module) in model.named_modules():\n        if isinstance(module, MoE):\n            group_name = module.expert_group_name\n            num_local_experts = module.num_local_experts\n            expp_rank = mpu.get_expert_parallel_rank(group_name)\n            for local_expert_id in range(num_local_experts):\n                global_expert_id = expp_rank * num_local_experts + local_expert_id\n                moe_load_path = _get_expert_ckpt_name(load_dir, moe_layer_id, global_expert_id)\n                logger.info(f'Loading expert states from {moe_load_path}')\n                expert_state_dict = torch.load(moe_load_path, map_location=torch.device('cpu'))\n                moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                for key in list(expert_state_dict.keys()):\n                    local_key = key.replace(f'{moe_str_prefix}{global_expert_id}', f'{moe_str_prefix}{local_expert_id}')\n                    expert_state_dict[local_key] = expert_state_dict.pop(key)\n                state_dict.update(expert_state_dict)\n            moe_layer_id += 1",
            "def load_moe_checkpoint(model, state_dict, load_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    moe_layer_id = 0\n    for (n_module, module) in model.named_modules():\n        if isinstance(module, MoE):\n            group_name = module.expert_group_name\n            num_local_experts = module.num_local_experts\n            expp_rank = mpu.get_expert_parallel_rank(group_name)\n            for local_expert_id in range(num_local_experts):\n                global_expert_id = expp_rank * num_local_experts + local_expert_id\n                moe_load_path = _get_expert_ckpt_name(load_dir, moe_layer_id, global_expert_id)\n                logger.info(f'Loading expert states from {moe_load_path}')\n                expert_state_dict = torch.load(moe_load_path, map_location=torch.device('cpu'))\n                moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                for key in list(expert_state_dict.keys()):\n                    local_key = key.replace(f'{moe_str_prefix}{global_expert_id}', f'{moe_str_prefix}{local_expert_id}')\n                    expert_state_dict[local_key] = expert_state_dict.pop(key)\n                state_dict.update(expert_state_dict)\n            moe_layer_id += 1",
            "def load_moe_checkpoint(model, state_dict, load_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    moe_layer_id = 0\n    for (n_module, module) in model.named_modules():\n        if isinstance(module, MoE):\n            group_name = module.expert_group_name\n            num_local_experts = module.num_local_experts\n            expp_rank = mpu.get_expert_parallel_rank(group_name)\n            for local_expert_id in range(num_local_experts):\n                global_expert_id = expp_rank * num_local_experts + local_expert_id\n                moe_load_path = _get_expert_ckpt_name(load_dir, moe_layer_id, global_expert_id)\n                logger.info(f'Loading expert states from {moe_load_path}')\n                expert_state_dict = torch.load(moe_load_path, map_location=torch.device('cpu'))\n                moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                for key in list(expert_state_dict.keys()):\n                    local_key = key.replace(f'{moe_str_prefix}{global_expert_id}', f'{moe_str_prefix}{local_expert_id}')\n                    expert_state_dict[local_key] = expert_state_dict.pop(key)\n                state_dict.update(expert_state_dict)\n            moe_layer_id += 1"
        ]
    }
]