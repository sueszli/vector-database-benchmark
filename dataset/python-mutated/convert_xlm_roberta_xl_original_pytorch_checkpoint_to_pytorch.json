[
    {
        "func_name": "convert_xlm_roberta_xl_checkpoint_to_pytorch",
        "original": "def convert_xlm_roberta_xl_checkpoint_to_pytorch(roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    \"\"\"\n    Copy/paste/tweak roberta's weights to our BERT structure.\n    \"\"\"\n    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)\n    roberta.eval()\n    roberta_sent_encoder = roberta.model.encoder.sentence_encoder\n    config = XLMRobertaConfig(vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings, hidden_size=roberta.cfg.model.encoder_embed_dim, num_hidden_layers=roberta.cfg.model.encoder_layers, num_attention_heads=roberta.cfg.model.encoder_attention_heads, intermediate_size=roberta.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)\n    if classification_head:\n        config.num_labels = roberta.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our RoBERTa config:', config)\n    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.encoder.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight\n    model.roberta.encoder.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias\n    for i in range(config.num_hidden_layers):\n        layer: BertLayer = model.roberta.encoder.layer[i]\n        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]\n        attention: RobertaAttention = layer.attention\n        attention.self_attn_layer_norm.weight = roberta_layer.self_attn_layer_norm.weight\n        attention.self_attn_layer_norm.bias = roberta_layer.self_attn_layer_norm.bias\n        self_attn: BertSelfAttention = layer.attention.self\n        assert roberta_layer.self_attn.k_proj.weight.data.shape == roberta_layer.self_attn.q_proj.weight.data.shape == roberta_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias\n        self_output: BertSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight\n        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias\n        layer.LayerNorm.weight = roberta_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = roberta_layer.final_layer_norm.bias\n        intermediate: BertIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape\n        intermediate.dense.weight = roberta_layer.fc1.weight\n        intermediate.dense.bias = roberta_layer.fc1.bias\n        bert_output: BertOutput = layer.output\n        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape\n        bert_output.dense.weight = roberta_layer.fc2.weight\n        bert_output.dense.bias = roberta_layer.fc2.bias\n    if classification_head:\n        model.classifier.dense.weight = roberta.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = roberta.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = roberta.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = roberta.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias\n    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = roberta.model.classification_heads['mnli'](roberta.extract_features(input_ids))\n    else:\n        their_output = roberta.model(input_ids)[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "def convert_xlm_roberta_xl_checkpoint_to_pytorch(roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak roberta's weights to our BERT structure.\\n    \"\n    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)\n    roberta.eval()\n    roberta_sent_encoder = roberta.model.encoder.sentence_encoder\n    config = XLMRobertaConfig(vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings, hidden_size=roberta.cfg.model.encoder_embed_dim, num_hidden_layers=roberta.cfg.model.encoder_layers, num_attention_heads=roberta.cfg.model.encoder_attention_heads, intermediate_size=roberta.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)\n    if classification_head:\n        config.num_labels = roberta.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our RoBERTa config:', config)\n    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.encoder.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight\n    model.roberta.encoder.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias\n    for i in range(config.num_hidden_layers):\n        layer: BertLayer = model.roberta.encoder.layer[i]\n        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]\n        attention: RobertaAttention = layer.attention\n        attention.self_attn_layer_norm.weight = roberta_layer.self_attn_layer_norm.weight\n        attention.self_attn_layer_norm.bias = roberta_layer.self_attn_layer_norm.bias\n        self_attn: BertSelfAttention = layer.attention.self\n        assert roberta_layer.self_attn.k_proj.weight.data.shape == roberta_layer.self_attn.q_proj.weight.data.shape == roberta_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias\n        self_output: BertSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight\n        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias\n        layer.LayerNorm.weight = roberta_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = roberta_layer.final_layer_norm.bias\n        intermediate: BertIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape\n        intermediate.dense.weight = roberta_layer.fc1.weight\n        intermediate.dense.bias = roberta_layer.fc1.bias\n        bert_output: BertOutput = layer.output\n        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape\n        bert_output.dense.weight = roberta_layer.fc2.weight\n        bert_output.dense.bias = roberta_layer.fc2.bias\n    if classification_head:\n        model.classifier.dense.weight = roberta.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = roberta.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = roberta.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = roberta.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias\n    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = roberta.model.classification_heads['mnli'](roberta.extract_features(input_ids))\n    else:\n        their_output = roberta.model(input_ids)[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "def convert_xlm_roberta_xl_checkpoint_to_pytorch(roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak roberta's weights to our BERT structure.\\n    \"\n    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)\n    roberta.eval()\n    roberta_sent_encoder = roberta.model.encoder.sentence_encoder\n    config = XLMRobertaConfig(vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings, hidden_size=roberta.cfg.model.encoder_embed_dim, num_hidden_layers=roberta.cfg.model.encoder_layers, num_attention_heads=roberta.cfg.model.encoder_attention_heads, intermediate_size=roberta.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)\n    if classification_head:\n        config.num_labels = roberta.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our RoBERTa config:', config)\n    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.encoder.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight\n    model.roberta.encoder.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias\n    for i in range(config.num_hidden_layers):\n        layer: BertLayer = model.roberta.encoder.layer[i]\n        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]\n        attention: RobertaAttention = layer.attention\n        attention.self_attn_layer_norm.weight = roberta_layer.self_attn_layer_norm.weight\n        attention.self_attn_layer_norm.bias = roberta_layer.self_attn_layer_norm.bias\n        self_attn: BertSelfAttention = layer.attention.self\n        assert roberta_layer.self_attn.k_proj.weight.data.shape == roberta_layer.self_attn.q_proj.weight.data.shape == roberta_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias\n        self_output: BertSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight\n        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias\n        layer.LayerNorm.weight = roberta_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = roberta_layer.final_layer_norm.bias\n        intermediate: BertIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape\n        intermediate.dense.weight = roberta_layer.fc1.weight\n        intermediate.dense.bias = roberta_layer.fc1.bias\n        bert_output: BertOutput = layer.output\n        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape\n        bert_output.dense.weight = roberta_layer.fc2.weight\n        bert_output.dense.bias = roberta_layer.fc2.bias\n    if classification_head:\n        model.classifier.dense.weight = roberta.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = roberta.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = roberta.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = roberta.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias\n    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = roberta.model.classification_heads['mnli'](roberta.extract_features(input_ids))\n    else:\n        their_output = roberta.model(input_ids)[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "def convert_xlm_roberta_xl_checkpoint_to_pytorch(roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak roberta's weights to our BERT structure.\\n    \"\n    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)\n    roberta.eval()\n    roberta_sent_encoder = roberta.model.encoder.sentence_encoder\n    config = XLMRobertaConfig(vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings, hidden_size=roberta.cfg.model.encoder_embed_dim, num_hidden_layers=roberta.cfg.model.encoder_layers, num_attention_heads=roberta.cfg.model.encoder_attention_heads, intermediate_size=roberta.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)\n    if classification_head:\n        config.num_labels = roberta.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our RoBERTa config:', config)\n    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.encoder.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight\n    model.roberta.encoder.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias\n    for i in range(config.num_hidden_layers):\n        layer: BertLayer = model.roberta.encoder.layer[i]\n        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]\n        attention: RobertaAttention = layer.attention\n        attention.self_attn_layer_norm.weight = roberta_layer.self_attn_layer_norm.weight\n        attention.self_attn_layer_norm.bias = roberta_layer.self_attn_layer_norm.bias\n        self_attn: BertSelfAttention = layer.attention.self\n        assert roberta_layer.self_attn.k_proj.weight.data.shape == roberta_layer.self_attn.q_proj.weight.data.shape == roberta_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias\n        self_output: BertSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight\n        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias\n        layer.LayerNorm.weight = roberta_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = roberta_layer.final_layer_norm.bias\n        intermediate: BertIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape\n        intermediate.dense.weight = roberta_layer.fc1.weight\n        intermediate.dense.bias = roberta_layer.fc1.bias\n        bert_output: BertOutput = layer.output\n        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape\n        bert_output.dense.weight = roberta_layer.fc2.weight\n        bert_output.dense.bias = roberta_layer.fc2.bias\n    if classification_head:\n        model.classifier.dense.weight = roberta.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = roberta.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = roberta.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = roberta.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias\n    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = roberta.model.classification_heads['mnli'](roberta.extract_features(input_ids))\n    else:\n        their_output = roberta.model(input_ids)[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "def convert_xlm_roberta_xl_checkpoint_to_pytorch(roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak roberta's weights to our BERT structure.\\n    \"\n    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)\n    roberta.eval()\n    roberta_sent_encoder = roberta.model.encoder.sentence_encoder\n    config = XLMRobertaConfig(vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings, hidden_size=roberta.cfg.model.encoder_embed_dim, num_hidden_layers=roberta.cfg.model.encoder_layers, num_attention_heads=roberta.cfg.model.encoder_attention_heads, intermediate_size=roberta.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)\n    if classification_head:\n        config.num_labels = roberta.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our RoBERTa config:', config)\n    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.encoder.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight\n    model.roberta.encoder.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias\n    for i in range(config.num_hidden_layers):\n        layer: BertLayer = model.roberta.encoder.layer[i]\n        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]\n        attention: RobertaAttention = layer.attention\n        attention.self_attn_layer_norm.weight = roberta_layer.self_attn_layer_norm.weight\n        attention.self_attn_layer_norm.bias = roberta_layer.self_attn_layer_norm.bias\n        self_attn: BertSelfAttention = layer.attention.self\n        assert roberta_layer.self_attn.k_proj.weight.data.shape == roberta_layer.self_attn.q_proj.weight.data.shape == roberta_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias\n        self_output: BertSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight\n        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias\n        layer.LayerNorm.weight = roberta_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = roberta_layer.final_layer_norm.bias\n        intermediate: BertIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape\n        intermediate.dense.weight = roberta_layer.fc1.weight\n        intermediate.dense.bias = roberta_layer.fc1.bias\n        bert_output: BertOutput = layer.output\n        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape\n        bert_output.dense.weight = roberta_layer.fc2.weight\n        bert_output.dense.bias = roberta_layer.fc2.bias\n    if classification_head:\n        model.classifier.dense.weight = roberta.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = roberta.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = roberta.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = roberta.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias\n    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = roberta.model.classification_heads['mnli'](roberta.extract_features(input_ids))\n    else:\n        their_output = roberta.model(input_ids)[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "def convert_xlm_roberta_xl_checkpoint_to_pytorch(roberta_checkpoint_path: str, pytorch_dump_folder_path: str, classification_head: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak roberta's weights to our BERT structure.\\n    \"\n    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)\n    roberta.eval()\n    roberta_sent_encoder = roberta.model.encoder.sentence_encoder\n    config = XLMRobertaConfig(vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings, hidden_size=roberta.cfg.model.encoder_embed_dim, num_hidden_layers=roberta.cfg.model.encoder_layers, num_attention_heads=roberta.cfg.model.encoder_attention_heads, intermediate_size=roberta.cfg.model.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)\n    if classification_head:\n        config.num_labels = roberta.model.classification_heads['mnli'].out_proj.weight.shape[0]\n    print('Our RoBERTa config:', config)\n    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)\n    model.eval()\n    model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight\n    model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight\n    model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)\n    model.roberta.encoder.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight\n    model.roberta.encoder.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias\n    for i in range(config.num_hidden_layers):\n        layer: BertLayer = model.roberta.encoder.layer[i]\n        roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]\n        attention: RobertaAttention = layer.attention\n        attention.self_attn_layer_norm.weight = roberta_layer.self_attn_layer_norm.weight\n        attention.self_attn_layer_norm.bias = roberta_layer.self_attn_layer_norm.bias\n        self_attn: BertSelfAttention = layer.attention.self\n        assert roberta_layer.self_attn.k_proj.weight.data.shape == roberta_layer.self_attn.q_proj.weight.data.shape == roberta_layer.self_attn.v_proj.weight.data.shape == torch.Size((config.hidden_size, config.hidden_size))\n        self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight\n        self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias\n        self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight\n        self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias\n        self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight\n        self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias\n        self_output: BertSelfOutput = layer.attention.output\n        assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape\n        self_output.dense.weight = roberta_layer.self_attn.out_proj.weight\n        self_output.dense.bias = roberta_layer.self_attn.out_proj.bias\n        layer.LayerNorm.weight = roberta_layer.final_layer_norm.weight\n        layer.LayerNorm.bias = roberta_layer.final_layer_norm.bias\n        intermediate: BertIntermediate = layer.intermediate\n        assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape\n        intermediate.dense.weight = roberta_layer.fc1.weight\n        intermediate.dense.bias = roberta_layer.fc1.bias\n        bert_output: BertOutput = layer.output\n        assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape\n        bert_output.dense.weight = roberta_layer.fc2.weight\n        bert_output.dense.bias = roberta_layer.fc2.bias\n    if classification_head:\n        model.classifier.dense.weight = roberta.model.classification_heads['mnli'].dense.weight\n        model.classifier.dense.bias = roberta.model.classification_heads['mnli'].dense.bias\n        model.classifier.out_proj.weight = roberta.model.classification_heads['mnli'].out_proj.weight\n        model.classifier.out_proj.bias = roberta.model.classification_heads['mnli'].out_proj.bias\n    else:\n        model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight\n        model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias\n        model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight\n        model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias\n        model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight\n        model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias\n    input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)\n    our_output = model(input_ids)[0]\n    if classification_head:\n        their_output = roberta.model.classification_heads['mnli'](roberta.extract_features(input_ids))\n    else:\n        their_output = roberta.model(input_ids)[0]\n    print(our_output.shape, their_output.shape)\n    max_absolute_diff = torch.max(torch.abs(our_output - their_output)).item()\n    print(f'max_absolute_diff = {max_absolute_diff}')\n    success = torch.allclose(our_output, their_output, atol=0.001)\n    print('Do both models output the same tensors?', '\ud83d\udd25' if success else '\ud83d\udca9')\n    if not success:\n        raise Exception('Something went wRoNg')\n    pathlib.Path(pytorch_dump_folder_path).mkdir(parents=True, exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)"
        ]
    }
]