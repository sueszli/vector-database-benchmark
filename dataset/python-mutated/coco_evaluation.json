[
    {
        "func_name": "_cocoeval_summarize",
        "original": "def _cocoeval_summarize(cocoeval, ap=1, iouThr=None, catIdx=None, areaRng='all', maxDets=100, catName='', nameStrLen=None):\n    p = cocoeval.params\n    if catName:\n        iStr = ' {:<18} {} {:<{nameStrLen}} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n        nameStr = catName\n    else:\n        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n    titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n    typeStr = '(AP)' if ap == 1 else '(AR)'\n    iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else '{:0.2f}'.format(iouThr)\n    aind = [i for (i, aRng) in enumerate(p.areaRngLbl) if aRng == areaRng]\n    mind = [i for (i, mDet) in enumerate(p.maxDets) if mDet == maxDets]\n    if ap == 1:\n        s = cocoeval.eval['precision']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, :, catIdx, aind, mind]\n        else:\n            s = s[:, :, :, aind, mind]\n    else:\n        s = cocoeval.eval['recall']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, catIdx, aind, mind]\n        else:\n            s = s[:, :, aind, mind]\n    if len(s[s > -1]) == 0:\n        mean_s = -1\n    else:\n        mean_s = np.mean(s[s > -1])\n    if catName:\n        print(iStr.format(titleStr, typeStr, nameStr, iouStr, areaRng, maxDets, mean_s, nameStrLen=nameStrLen))\n    else:\n        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n    return mean_s",
        "mutated": [
            "def _cocoeval_summarize(cocoeval, ap=1, iouThr=None, catIdx=None, areaRng='all', maxDets=100, catName='', nameStrLen=None):\n    if False:\n        i = 10\n    p = cocoeval.params\n    if catName:\n        iStr = ' {:<18} {} {:<{nameStrLen}} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n        nameStr = catName\n    else:\n        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n    titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n    typeStr = '(AP)' if ap == 1 else '(AR)'\n    iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else '{:0.2f}'.format(iouThr)\n    aind = [i for (i, aRng) in enumerate(p.areaRngLbl) if aRng == areaRng]\n    mind = [i for (i, mDet) in enumerate(p.maxDets) if mDet == maxDets]\n    if ap == 1:\n        s = cocoeval.eval['precision']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, :, catIdx, aind, mind]\n        else:\n            s = s[:, :, :, aind, mind]\n    else:\n        s = cocoeval.eval['recall']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, catIdx, aind, mind]\n        else:\n            s = s[:, :, aind, mind]\n    if len(s[s > -1]) == 0:\n        mean_s = -1\n    else:\n        mean_s = np.mean(s[s > -1])\n    if catName:\n        print(iStr.format(titleStr, typeStr, nameStr, iouStr, areaRng, maxDets, mean_s, nameStrLen=nameStrLen))\n    else:\n        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n    return mean_s",
            "def _cocoeval_summarize(cocoeval, ap=1, iouThr=None, catIdx=None, areaRng='all', maxDets=100, catName='', nameStrLen=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    p = cocoeval.params\n    if catName:\n        iStr = ' {:<18} {} {:<{nameStrLen}} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n        nameStr = catName\n    else:\n        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n    titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n    typeStr = '(AP)' if ap == 1 else '(AR)'\n    iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else '{:0.2f}'.format(iouThr)\n    aind = [i for (i, aRng) in enumerate(p.areaRngLbl) if aRng == areaRng]\n    mind = [i for (i, mDet) in enumerate(p.maxDets) if mDet == maxDets]\n    if ap == 1:\n        s = cocoeval.eval['precision']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, :, catIdx, aind, mind]\n        else:\n            s = s[:, :, :, aind, mind]\n    else:\n        s = cocoeval.eval['recall']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, catIdx, aind, mind]\n        else:\n            s = s[:, :, aind, mind]\n    if len(s[s > -1]) == 0:\n        mean_s = -1\n    else:\n        mean_s = np.mean(s[s > -1])\n    if catName:\n        print(iStr.format(titleStr, typeStr, nameStr, iouStr, areaRng, maxDets, mean_s, nameStrLen=nameStrLen))\n    else:\n        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n    return mean_s",
            "def _cocoeval_summarize(cocoeval, ap=1, iouThr=None, catIdx=None, areaRng='all', maxDets=100, catName='', nameStrLen=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    p = cocoeval.params\n    if catName:\n        iStr = ' {:<18} {} {:<{nameStrLen}} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n        nameStr = catName\n    else:\n        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n    titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n    typeStr = '(AP)' if ap == 1 else '(AR)'\n    iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else '{:0.2f}'.format(iouThr)\n    aind = [i for (i, aRng) in enumerate(p.areaRngLbl) if aRng == areaRng]\n    mind = [i for (i, mDet) in enumerate(p.maxDets) if mDet == maxDets]\n    if ap == 1:\n        s = cocoeval.eval['precision']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, :, catIdx, aind, mind]\n        else:\n            s = s[:, :, :, aind, mind]\n    else:\n        s = cocoeval.eval['recall']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, catIdx, aind, mind]\n        else:\n            s = s[:, :, aind, mind]\n    if len(s[s > -1]) == 0:\n        mean_s = -1\n    else:\n        mean_s = np.mean(s[s > -1])\n    if catName:\n        print(iStr.format(titleStr, typeStr, nameStr, iouStr, areaRng, maxDets, mean_s, nameStrLen=nameStrLen))\n    else:\n        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n    return mean_s",
            "def _cocoeval_summarize(cocoeval, ap=1, iouThr=None, catIdx=None, areaRng='all', maxDets=100, catName='', nameStrLen=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    p = cocoeval.params\n    if catName:\n        iStr = ' {:<18} {} {:<{nameStrLen}} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n        nameStr = catName\n    else:\n        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n    titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n    typeStr = '(AP)' if ap == 1 else '(AR)'\n    iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else '{:0.2f}'.format(iouThr)\n    aind = [i for (i, aRng) in enumerate(p.areaRngLbl) if aRng == areaRng]\n    mind = [i for (i, mDet) in enumerate(p.maxDets) if mDet == maxDets]\n    if ap == 1:\n        s = cocoeval.eval['precision']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, :, catIdx, aind, mind]\n        else:\n            s = s[:, :, :, aind, mind]\n    else:\n        s = cocoeval.eval['recall']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, catIdx, aind, mind]\n        else:\n            s = s[:, :, aind, mind]\n    if len(s[s > -1]) == 0:\n        mean_s = -1\n    else:\n        mean_s = np.mean(s[s > -1])\n    if catName:\n        print(iStr.format(titleStr, typeStr, nameStr, iouStr, areaRng, maxDets, mean_s, nameStrLen=nameStrLen))\n    else:\n        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n    return mean_s",
            "def _cocoeval_summarize(cocoeval, ap=1, iouThr=None, catIdx=None, areaRng='all', maxDets=100, catName='', nameStrLen=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    p = cocoeval.params\n    if catName:\n        iStr = ' {:<18} {} {:<{nameStrLen}} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n        nameStr = catName\n    else:\n        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n    titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n    typeStr = '(AP)' if ap == 1 else '(AR)'\n    iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else '{:0.2f}'.format(iouThr)\n    aind = [i for (i, aRng) in enumerate(p.areaRngLbl) if aRng == areaRng]\n    mind = [i for (i, mDet) in enumerate(p.maxDets) if mDet == maxDets]\n    if ap == 1:\n        s = cocoeval.eval['precision']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, :, catIdx, aind, mind]\n        else:\n            s = s[:, :, :, aind, mind]\n    else:\n        s = cocoeval.eval['recall']\n        if iouThr is not None:\n            t = np.where(iouThr == p.iouThrs)[0]\n            s = s[t]\n        if catIdx is not None:\n            s = s[:, catIdx, aind, mind]\n        else:\n            s = s[:, :, aind, mind]\n    if len(s[s > -1]) == 0:\n        mean_s = -1\n    else:\n        mean_s = np.mean(s[s > -1])\n    if catName:\n        print(iStr.format(titleStr, typeStr, nameStr, iouStr, areaRng, maxDets, mean_s, nameStrLen=nameStrLen))\n    else:\n        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n    return mean_s"
        ]
    },
    {
        "func_name": "evaluate_core",
        "original": "def evaluate_core(dataset_path, result_path, metric: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs=None, metric_items=None, out_dir: str=None, areas: List[int]=[1024, 9216, 10000000000], COCO=None, COCOeval=None):\n    \"\"\"Evaluation in COCO protocol.\n    Args:\n        dataset_path (str): COCO dataset json path.\n        result_path (str): COCO result json path.\n        metric (str | list[str]): Metrics to be evaluated. Options are\n            'bbox', 'segm', 'proposal'.\n        classwise (bool): Whether to evaluating the AP for each class.\n        max_detections (int): Maximum number of detections to consider for AP\n            calculation.\n            Default: 500\n        iou_thrs (List[float], optional): IoU threshold used for\n            evaluating recalls/mAPs. If set to a list, the average of all\n            IoUs will also be computed. If not specified, [0.50, 0.55,\n            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\n            Default: None.\n        metric_items (list[str] | str, optional): Metric items that will\n            be returned. If not specified, ``['AR@10', 'AR@100',\n            'AR@500', 'AR_s@500', 'AR_m@500', 'AR_l@500' ]`` will be\n            used when ``metric=='proposal'``, ``['mAP', 'mAP50', 'mAP75',\n            'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']``\n            will be used when ``metric=='bbox' or metric=='segm'``.\n        out_dir (str): Directory to save evaluation result json.\n        areas (List[int]): area regions for coco evaluation calculations\n    Returns:\n        dict:\n            eval_results (dict[str, float]): COCO style evaluation metric.\n            export_path (str): Path for the exported eval result json.\n\n    \"\"\"\n    metrics = metric if isinstance(metric, list) else [metric]\n    allowed_metrics = ['bbox', 'segm']\n    for metric in metrics:\n        if metric not in allowed_metrics:\n            raise KeyError(f'metric {metric} is not supported')\n    if iou_thrs is None:\n        iou_thrs = np.linspace(0.5, 0.95, int(np.round((0.95 - 0.5) / 0.05)) + 1, endpoint=True)\n    if metric_items is not None:\n        if not isinstance(metric_items, list):\n            metric_items = [metric_items]\n    if areas is not None:\n        if len(areas) != 3:\n            raise ValueError('3 integers should be specified as areas, representing 3 area regions')\n    eval_results = OrderedDict()\n    cocoGt = COCO(dataset_path)\n    cat_ids = list(cocoGt.cats.keys())\n    for metric in metrics:\n        msg = f'Evaluating {metric}...'\n        msg = '\\n' + msg\n        print(msg)\n        iou_type = metric\n        with open(result_path) as json_file:\n            results = json.load(json_file)\n        try:\n            cocoDt = cocoGt.loadRes(results)\n        except IndexError:\n            print('The testing results of the whole dataset is empty.')\n            break\n        cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n        if areas is not None:\n            cocoEval.params.areaRng = [[0 ** 2, areas[2]], [0 ** 2, areas[0]], [areas[0], areas[1]], [areas[1], areas[2]]]\n        cocoEval.params.catIds = cat_ids\n        cocoEval.params.maxDets = [max_detections]\n        cocoEval.params.iouThrs = [iou_thrs] if not isinstance(iou_thrs, list) and (not isinstance(iou_thrs, np.ndarray)) else iou_thrs\n        coco_metric_names = {'mAP': 0, 'mAP75': 1, 'mAP50': 2, 'mAP_s': 3, 'mAP_m': 4, 'mAP_l': 5, 'mAP50_s': 6, 'mAP50_m': 7, 'mAP50_l': 8, 'AR_s': 9, 'AR_m': 10, 'AR_l': 11}\n        if metric_items is not None:\n            for metric_item in metric_items:\n                if metric_item not in coco_metric_names:\n                    raise KeyError(f'metric item {metric_item} is not supported')\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        mAP = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='all', maxDets=max_detections)\n        mAP50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='all', maxDets=max_detections)\n        mAP75 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.75, areaRng='all', maxDets=max_detections)\n        mAP50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='small', maxDets=max_detections)\n        mAP50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='medium', maxDets=max_detections)\n        mAP50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='large', maxDets=max_detections)\n        mAP_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='small', maxDets=max_detections)\n        mAP_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='medium', maxDets=max_detections)\n        mAP_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='large', maxDets=max_detections)\n        AR_s = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='small', maxDets=max_detections)\n        AR_m = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='medium', maxDets=max_detections)\n        AR_l = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='large', maxDets=max_detections)\n        cocoEval.stats = np.append([mAP, mAP75, mAP50, mAP_s, mAP_m, mAP_l, mAP50_s, mAP50_m, mAP50_l, AR_s, AR_m, AR_l], 0)\n        if classwise:\n            precisions = cocoEval.eval['precision']\n            if len(cat_ids) != precisions.shape[2]:\n                raise ValueError(f'The number of categories {len(cat_ids)} is not equal to the number of precisions {precisions.shape[2]}')\n            max_cat_name_len = 0\n            for (idx, catId) in enumerate(cat_ids):\n                nm = cocoGt.loadCats(catId)[0]\n                cat_name_len = len(nm['name'])\n                max_cat_name_len = cat_name_len if cat_name_len > max_cat_name_len else max_cat_name_len\n            results_per_category = []\n            for (idx, catId) in enumerate(cat_ids):\n                image_ids = cocoGt.getImgIds(catIds=[catId])\n                if len(image_ids) == 0:\n                    continue\n                nm = cocoGt.loadCats(catId)[0]\n                ap = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_s = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_m = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_l = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP\", f'{float(ap):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_s\", f'{float(ap_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_m\", f'{float(ap_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_l\", f'{float(ap_l):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50\", f'{float(ap50):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_s\", f'{float(ap50_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_m\", f'{float(ap50_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_l\", f'{float(ap50_l):0.3f}'))\n            num_columns = min(6, len(results_per_category) * 2)\n            results_flatten = list(itertools.chain(*results_per_category))\n            headers = ['category', 'AP'] * (num_columns // 2)\n            results_2d = itertools.zip_longest(*[results_flatten[i::num_columns] for i in range(num_columns)])\n            table_data = [headers]\n            table_data += [result for result in results_2d]\n            table = AsciiTable(table_data)\n            print('\\n' + table.table)\n        if metric_items is None:\n            metric_items = ['mAP', 'mAP50', 'mAP75', 'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']\n        for metric_item in metric_items:\n            key = f'{metric}_{metric_item}'\n            val = float(f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}')\n            eval_results[key] = val\n        ap = cocoEval.stats\n        eval_results[f'{metric}_mAP_copypaste'] = f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} {ap[4]:.3f} {ap[5]:.3f} {ap[6]:.3f} {ap[7]:.3f} {ap[8]:.3f}'\n        if classwise:\n            eval_results['results_per_category'] = {key: value for (key, value) in results_per_category}\n    if not out_dir:\n        out_dir = Path(result_path).parent\n    Path(out_dir).mkdir(parents=True, exist_ok=True)\n    export_path = str(Path(out_dir) / 'eval.json')\n    with open(export_path, 'w', encoding='utf-8') as outfile:\n        json.dump(eval_results, outfile, indent=4, separators=(',', ':'))\n    print(f'COCO evaluation results are successfully exported to {export_path}')\n    return {'eval_results': eval_results, 'export_path': export_path}",
        "mutated": [
            "def evaluate_core(dataset_path, result_path, metric: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs=None, metric_items=None, out_dir: str=None, areas: List[int]=[1024, 9216, 10000000000], COCO=None, COCOeval=None):\n    if False:\n        i = 10\n    \"Evaluation in COCO protocol.\\n    Args:\\n        dataset_path (str): COCO dataset json path.\\n        result_path (str): COCO result json path.\\n        metric (str | list[str]): Metrics to be evaluated. Options are\\n            'bbox', 'segm', 'proposal'.\\n        classwise (bool): Whether to evaluating the AP for each class.\\n        max_detections (int): Maximum number of detections to consider for AP\\n            calculation.\\n            Default: 500\\n        iou_thrs (List[float], optional): IoU threshold used for\\n            evaluating recalls/mAPs. If set to a list, the average of all\\n            IoUs will also be computed. If not specified, [0.50, 0.55,\\n            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\\n            Default: None.\\n        metric_items (list[str] | str, optional): Metric items that will\\n            be returned. If not specified, ``['AR@10', 'AR@100',\\n            'AR@500', 'AR_s@500', 'AR_m@500', 'AR_l@500' ]`` will be\\n            used when ``metric=='proposal'``, ``['mAP', 'mAP50', 'mAP75',\\n            'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']``\\n            will be used when ``metric=='bbox' or metric=='segm'``.\\n        out_dir (str): Directory to save evaluation result json.\\n        areas (List[int]): area regions for coco evaluation calculations\\n    Returns:\\n        dict:\\n            eval_results (dict[str, float]): COCO style evaluation metric.\\n            export_path (str): Path for the exported eval result json.\\n\\n    \"\n    metrics = metric if isinstance(metric, list) else [metric]\n    allowed_metrics = ['bbox', 'segm']\n    for metric in metrics:\n        if metric not in allowed_metrics:\n            raise KeyError(f'metric {metric} is not supported')\n    if iou_thrs is None:\n        iou_thrs = np.linspace(0.5, 0.95, int(np.round((0.95 - 0.5) / 0.05)) + 1, endpoint=True)\n    if metric_items is not None:\n        if not isinstance(metric_items, list):\n            metric_items = [metric_items]\n    if areas is not None:\n        if len(areas) != 3:\n            raise ValueError('3 integers should be specified as areas, representing 3 area regions')\n    eval_results = OrderedDict()\n    cocoGt = COCO(dataset_path)\n    cat_ids = list(cocoGt.cats.keys())\n    for metric in metrics:\n        msg = f'Evaluating {metric}...'\n        msg = '\\n' + msg\n        print(msg)\n        iou_type = metric\n        with open(result_path) as json_file:\n            results = json.load(json_file)\n        try:\n            cocoDt = cocoGt.loadRes(results)\n        except IndexError:\n            print('The testing results of the whole dataset is empty.')\n            break\n        cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n        if areas is not None:\n            cocoEval.params.areaRng = [[0 ** 2, areas[2]], [0 ** 2, areas[0]], [areas[0], areas[1]], [areas[1], areas[2]]]\n        cocoEval.params.catIds = cat_ids\n        cocoEval.params.maxDets = [max_detections]\n        cocoEval.params.iouThrs = [iou_thrs] if not isinstance(iou_thrs, list) and (not isinstance(iou_thrs, np.ndarray)) else iou_thrs\n        coco_metric_names = {'mAP': 0, 'mAP75': 1, 'mAP50': 2, 'mAP_s': 3, 'mAP_m': 4, 'mAP_l': 5, 'mAP50_s': 6, 'mAP50_m': 7, 'mAP50_l': 8, 'AR_s': 9, 'AR_m': 10, 'AR_l': 11}\n        if metric_items is not None:\n            for metric_item in metric_items:\n                if metric_item not in coco_metric_names:\n                    raise KeyError(f'metric item {metric_item} is not supported')\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        mAP = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='all', maxDets=max_detections)\n        mAP50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='all', maxDets=max_detections)\n        mAP75 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.75, areaRng='all', maxDets=max_detections)\n        mAP50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='small', maxDets=max_detections)\n        mAP50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='medium', maxDets=max_detections)\n        mAP50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='large', maxDets=max_detections)\n        mAP_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='small', maxDets=max_detections)\n        mAP_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='medium', maxDets=max_detections)\n        mAP_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='large', maxDets=max_detections)\n        AR_s = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='small', maxDets=max_detections)\n        AR_m = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='medium', maxDets=max_detections)\n        AR_l = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='large', maxDets=max_detections)\n        cocoEval.stats = np.append([mAP, mAP75, mAP50, mAP_s, mAP_m, mAP_l, mAP50_s, mAP50_m, mAP50_l, AR_s, AR_m, AR_l], 0)\n        if classwise:\n            precisions = cocoEval.eval['precision']\n            if len(cat_ids) != precisions.shape[2]:\n                raise ValueError(f'The number of categories {len(cat_ids)} is not equal to the number of precisions {precisions.shape[2]}')\n            max_cat_name_len = 0\n            for (idx, catId) in enumerate(cat_ids):\n                nm = cocoGt.loadCats(catId)[0]\n                cat_name_len = len(nm['name'])\n                max_cat_name_len = cat_name_len if cat_name_len > max_cat_name_len else max_cat_name_len\n            results_per_category = []\n            for (idx, catId) in enumerate(cat_ids):\n                image_ids = cocoGt.getImgIds(catIds=[catId])\n                if len(image_ids) == 0:\n                    continue\n                nm = cocoGt.loadCats(catId)[0]\n                ap = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_s = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_m = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_l = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP\", f'{float(ap):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_s\", f'{float(ap_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_m\", f'{float(ap_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_l\", f'{float(ap_l):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50\", f'{float(ap50):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_s\", f'{float(ap50_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_m\", f'{float(ap50_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_l\", f'{float(ap50_l):0.3f}'))\n            num_columns = min(6, len(results_per_category) * 2)\n            results_flatten = list(itertools.chain(*results_per_category))\n            headers = ['category', 'AP'] * (num_columns // 2)\n            results_2d = itertools.zip_longest(*[results_flatten[i::num_columns] for i in range(num_columns)])\n            table_data = [headers]\n            table_data += [result for result in results_2d]\n            table = AsciiTable(table_data)\n            print('\\n' + table.table)\n        if metric_items is None:\n            metric_items = ['mAP', 'mAP50', 'mAP75', 'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']\n        for metric_item in metric_items:\n            key = f'{metric}_{metric_item}'\n            val = float(f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}')\n            eval_results[key] = val\n        ap = cocoEval.stats\n        eval_results[f'{metric}_mAP_copypaste'] = f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} {ap[4]:.3f} {ap[5]:.3f} {ap[6]:.3f} {ap[7]:.3f} {ap[8]:.3f}'\n        if classwise:\n            eval_results['results_per_category'] = {key: value for (key, value) in results_per_category}\n    if not out_dir:\n        out_dir = Path(result_path).parent\n    Path(out_dir).mkdir(parents=True, exist_ok=True)\n    export_path = str(Path(out_dir) / 'eval.json')\n    with open(export_path, 'w', encoding='utf-8') as outfile:\n        json.dump(eval_results, outfile, indent=4, separators=(',', ':'))\n    print(f'COCO evaluation results are successfully exported to {export_path}')\n    return {'eval_results': eval_results, 'export_path': export_path}",
            "def evaluate_core(dataset_path, result_path, metric: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs=None, metric_items=None, out_dir: str=None, areas: List[int]=[1024, 9216, 10000000000], COCO=None, COCOeval=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Evaluation in COCO protocol.\\n    Args:\\n        dataset_path (str): COCO dataset json path.\\n        result_path (str): COCO result json path.\\n        metric (str | list[str]): Metrics to be evaluated. Options are\\n            'bbox', 'segm', 'proposal'.\\n        classwise (bool): Whether to evaluating the AP for each class.\\n        max_detections (int): Maximum number of detections to consider for AP\\n            calculation.\\n            Default: 500\\n        iou_thrs (List[float], optional): IoU threshold used for\\n            evaluating recalls/mAPs. If set to a list, the average of all\\n            IoUs will also be computed. If not specified, [0.50, 0.55,\\n            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\\n            Default: None.\\n        metric_items (list[str] | str, optional): Metric items that will\\n            be returned. If not specified, ``['AR@10', 'AR@100',\\n            'AR@500', 'AR_s@500', 'AR_m@500', 'AR_l@500' ]`` will be\\n            used when ``metric=='proposal'``, ``['mAP', 'mAP50', 'mAP75',\\n            'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']``\\n            will be used when ``metric=='bbox' or metric=='segm'``.\\n        out_dir (str): Directory to save evaluation result json.\\n        areas (List[int]): area regions for coco evaluation calculations\\n    Returns:\\n        dict:\\n            eval_results (dict[str, float]): COCO style evaluation metric.\\n            export_path (str): Path for the exported eval result json.\\n\\n    \"\n    metrics = metric if isinstance(metric, list) else [metric]\n    allowed_metrics = ['bbox', 'segm']\n    for metric in metrics:\n        if metric not in allowed_metrics:\n            raise KeyError(f'metric {metric} is not supported')\n    if iou_thrs is None:\n        iou_thrs = np.linspace(0.5, 0.95, int(np.round((0.95 - 0.5) / 0.05)) + 1, endpoint=True)\n    if metric_items is not None:\n        if not isinstance(metric_items, list):\n            metric_items = [metric_items]\n    if areas is not None:\n        if len(areas) != 3:\n            raise ValueError('3 integers should be specified as areas, representing 3 area regions')\n    eval_results = OrderedDict()\n    cocoGt = COCO(dataset_path)\n    cat_ids = list(cocoGt.cats.keys())\n    for metric in metrics:\n        msg = f'Evaluating {metric}...'\n        msg = '\\n' + msg\n        print(msg)\n        iou_type = metric\n        with open(result_path) as json_file:\n            results = json.load(json_file)\n        try:\n            cocoDt = cocoGt.loadRes(results)\n        except IndexError:\n            print('The testing results of the whole dataset is empty.')\n            break\n        cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n        if areas is not None:\n            cocoEval.params.areaRng = [[0 ** 2, areas[2]], [0 ** 2, areas[0]], [areas[0], areas[1]], [areas[1], areas[2]]]\n        cocoEval.params.catIds = cat_ids\n        cocoEval.params.maxDets = [max_detections]\n        cocoEval.params.iouThrs = [iou_thrs] if not isinstance(iou_thrs, list) and (not isinstance(iou_thrs, np.ndarray)) else iou_thrs\n        coco_metric_names = {'mAP': 0, 'mAP75': 1, 'mAP50': 2, 'mAP_s': 3, 'mAP_m': 4, 'mAP_l': 5, 'mAP50_s': 6, 'mAP50_m': 7, 'mAP50_l': 8, 'AR_s': 9, 'AR_m': 10, 'AR_l': 11}\n        if metric_items is not None:\n            for metric_item in metric_items:\n                if metric_item not in coco_metric_names:\n                    raise KeyError(f'metric item {metric_item} is not supported')\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        mAP = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='all', maxDets=max_detections)\n        mAP50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='all', maxDets=max_detections)\n        mAP75 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.75, areaRng='all', maxDets=max_detections)\n        mAP50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='small', maxDets=max_detections)\n        mAP50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='medium', maxDets=max_detections)\n        mAP50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='large', maxDets=max_detections)\n        mAP_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='small', maxDets=max_detections)\n        mAP_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='medium', maxDets=max_detections)\n        mAP_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='large', maxDets=max_detections)\n        AR_s = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='small', maxDets=max_detections)\n        AR_m = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='medium', maxDets=max_detections)\n        AR_l = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='large', maxDets=max_detections)\n        cocoEval.stats = np.append([mAP, mAP75, mAP50, mAP_s, mAP_m, mAP_l, mAP50_s, mAP50_m, mAP50_l, AR_s, AR_m, AR_l], 0)\n        if classwise:\n            precisions = cocoEval.eval['precision']\n            if len(cat_ids) != precisions.shape[2]:\n                raise ValueError(f'The number of categories {len(cat_ids)} is not equal to the number of precisions {precisions.shape[2]}')\n            max_cat_name_len = 0\n            for (idx, catId) in enumerate(cat_ids):\n                nm = cocoGt.loadCats(catId)[0]\n                cat_name_len = len(nm['name'])\n                max_cat_name_len = cat_name_len if cat_name_len > max_cat_name_len else max_cat_name_len\n            results_per_category = []\n            for (idx, catId) in enumerate(cat_ids):\n                image_ids = cocoGt.getImgIds(catIds=[catId])\n                if len(image_ids) == 0:\n                    continue\n                nm = cocoGt.loadCats(catId)[0]\n                ap = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_s = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_m = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_l = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP\", f'{float(ap):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_s\", f'{float(ap_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_m\", f'{float(ap_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_l\", f'{float(ap_l):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50\", f'{float(ap50):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_s\", f'{float(ap50_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_m\", f'{float(ap50_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_l\", f'{float(ap50_l):0.3f}'))\n            num_columns = min(6, len(results_per_category) * 2)\n            results_flatten = list(itertools.chain(*results_per_category))\n            headers = ['category', 'AP'] * (num_columns // 2)\n            results_2d = itertools.zip_longest(*[results_flatten[i::num_columns] for i in range(num_columns)])\n            table_data = [headers]\n            table_data += [result for result in results_2d]\n            table = AsciiTable(table_data)\n            print('\\n' + table.table)\n        if metric_items is None:\n            metric_items = ['mAP', 'mAP50', 'mAP75', 'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']\n        for metric_item in metric_items:\n            key = f'{metric}_{metric_item}'\n            val = float(f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}')\n            eval_results[key] = val\n        ap = cocoEval.stats\n        eval_results[f'{metric}_mAP_copypaste'] = f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} {ap[4]:.3f} {ap[5]:.3f} {ap[6]:.3f} {ap[7]:.3f} {ap[8]:.3f}'\n        if classwise:\n            eval_results['results_per_category'] = {key: value for (key, value) in results_per_category}\n    if not out_dir:\n        out_dir = Path(result_path).parent\n    Path(out_dir).mkdir(parents=True, exist_ok=True)\n    export_path = str(Path(out_dir) / 'eval.json')\n    with open(export_path, 'w', encoding='utf-8') as outfile:\n        json.dump(eval_results, outfile, indent=4, separators=(',', ':'))\n    print(f'COCO evaluation results are successfully exported to {export_path}')\n    return {'eval_results': eval_results, 'export_path': export_path}",
            "def evaluate_core(dataset_path, result_path, metric: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs=None, metric_items=None, out_dir: str=None, areas: List[int]=[1024, 9216, 10000000000], COCO=None, COCOeval=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Evaluation in COCO protocol.\\n    Args:\\n        dataset_path (str): COCO dataset json path.\\n        result_path (str): COCO result json path.\\n        metric (str | list[str]): Metrics to be evaluated. Options are\\n            'bbox', 'segm', 'proposal'.\\n        classwise (bool): Whether to evaluating the AP for each class.\\n        max_detections (int): Maximum number of detections to consider for AP\\n            calculation.\\n            Default: 500\\n        iou_thrs (List[float], optional): IoU threshold used for\\n            evaluating recalls/mAPs. If set to a list, the average of all\\n            IoUs will also be computed. If not specified, [0.50, 0.55,\\n            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\\n            Default: None.\\n        metric_items (list[str] | str, optional): Metric items that will\\n            be returned. If not specified, ``['AR@10', 'AR@100',\\n            'AR@500', 'AR_s@500', 'AR_m@500', 'AR_l@500' ]`` will be\\n            used when ``metric=='proposal'``, ``['mAP', 'mAP50', 'mAP75',\\n            'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']``\\n            will be used when ``metric=='bbox' or metric=='segm'``.\\n        out_dir (str): Directory to save evaluation result json.\\n        areas (List[int]): area regions for coco evaluation calculations\\n    Returns:\\n        dict:\\n            eval_results (dict[str, float]): COCO style evaluation metric.\\n            export_path (str): Path for the exported eval result json.\\n\\n    \"\n    metrics = metric if isinstance(metric, list) else [metric]\n    allowed_metrics = ['bbox', 'segm']\n    for metric in metrics:\n        if metric not in allowed_metrics:\n            raise KeyError(f'metric {metric} is not supported')\n    if iou_thrs is None:\n        iou_thrs = np.linspace(0.5, 0.95, int(np.round((0.95 - 0.5) / 0.05)) + 1, endpoint=True)\n    if metric_items is not None:\n        if not isinstance(metric_items, list):\n            metric_items = [metric_items]\n    if areas is not None:\n        if len(areas) != 3:\n            raise ValueError('3 integers should be specified as areas, representing 3 area regions')\n    eval_results = OrderedDict()\n    cocoGt = COCO(dataset_path)\n    cat_ids = list(cocoGt.cats.keys())\n    for metric in metrics:\n        msg = f'Evaluating {metric}...'\n        msg = '\\n' + msg\n        print(msg)\n        iou_type = metric\n        with open(result_path) as json_file:\n            results = json.load(json_file)\n        try:\n            cocoDt = cocoGt.loadRes(results)\n        except IndexError:\n            print('The testing results of the whole dataset is empty.')\n            break\n        cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n        if areas is not None:\n            cocoEval.params.areaRng = [[0 ** 2, areas[2]], [0 ** 2, areas[0]], [areas[0], areas[1]], [areas[1], areas[2]]]\n        cocoEval.params.catIds = cat_ids\n        cocoEval.params.maxDets = [max_detections]\n        cocoEval.params.iouThrs = [iou_thrs] if not isinstance(iou_thrs, list) and (not isinstance(iou_thrs, np.ndarray)) else iou_thrs\n        coco_metric_names = {'mAP': 0, 'mAP75': 1, 'mAP50': 2, 'mAP_s': 3, 'mAP_m': 4, 'mAP_l': 5, 'mAP50_s': 6, 'mAP50_m': 7, 'mAP50_l': 8, 'AR_s': 9, 'AR_m': 10, 'AR_l': 11}\n        if metric_items is not None:\n            for metric_item in metric_items:\n                if metric_item not in coco_metric_names:\n                    raise KeyError(f'metric item {metric_item} is not supported')\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        mAP = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='all', maxDets=max_detections)\n        mAP50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='all', maxDets=max_detections)\n        mAP75 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.75, areaRng='all', maxDets=max_detections)\n        mAP50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='small', maxDets=max_detections)\n        mAP50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='medium', maxDets=max_detections)\n        mAP50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='large', maxDets=max_detections)\n        mAP_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='small', maxDets=max_detections)\n        mAP_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='medium', maxDets=max_detections)\n        mAP_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='large', maxDets=max_detections)\n        AR_s = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='small', maxDets=max_detections)\n        AR_m = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='medium', maxDets=max_detections)\n        AR_l = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='large', maxDets=max_detections)\n        cocoEval.stats = np.append([mAP, mAP75, mAP50, mAP_s, mAP_m, mAP_l, mAP50_s, mAP50_m, mAP50_l, AR_s, AR_m, AR_l], 0)\n        if classwise:\n            precisions = cocoEval.eval['precision']\n            if len(cat_ids) != precisions.shape[2]:\n                raise ValueError(f'The number of categories {len(cat_ids)} is not equal to the number of precisions {precisions.shape[2]}')\n            max_cat_name_len = 0\n            for (idx, catId) in enumerate(cat_ids):\n                nm = cocoGt.loadCats(catId)[0]\n                cat_name_len = len(nm['name'])\n                max_cat_name_len = cat_name_len if cat_name_len > max_cat_name_len else max_cat_name_len\n            results_per_category = []\n            for (idx, catId) in enumerate(cat_ids):\n                image_ids = cocoGt.getImgIds(catIds=[catId])\n                if len(image_ids) == 0:\n                    continue\n                nm = cocoGt.loadCats(catId)[0]\n                ap = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_s = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_m = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_l = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP\", f'{float(ap):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_s\", f'{float(ap_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_m\", f'{float(ap_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_l\", f'{float(ap_l):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50\", f'{float(ap50):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_s\", f'{float(ap50_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_m\", f'{float(ap50_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_l\", f'{float(ap50_l):0.3f}'))\n            num_columns = min(6, len(results_per_category) * 2)\n            results_flatten = list(itertools.chain(*results_per_category))\n            headers = ['category', 'AP'] * (num_columns // 2)\n            results_2d = itertools.zip_longest(*[results_flatten[i::num_columns] for i in range(num_columns)])\n            table_data = [headers]\n            table_data += [result for result in results_2d]\n            table = AsciiTable(table_data)\n            print('\\n' + table.table)\n        if metric_items is None:\n            metric_items = ['mAP', 'mAP50', 'mAP75', 'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']\n        for metric_item in metric_items:\n            key = f'{metric}_{metric_item}'\n            val = float(f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}')\n            eval_results[key] = val\n        ap = cocoEval.stats\n        eval_results[f'{metric}_mAP_copypaste'] = f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} {ap[4]:.3f} {ap[5]:.3f} {ap[6]:.3f} {ap[7]:.3f} {ap[8]:.3f}'\n        if classwise:\n            eval_results['results_per_category'] = {key: value for (key, value) in results_per_category}\n    if not out_dir:\n        out_dir = Path(result_path).parent\n    Path(out_dir).mkdir(parents=True, exist_ok=True)\n    export_path = str(Path(out_dir) / 'eval.json')\n    with open(export_path, 'w', encoding='utf-8') as outfile:\n        json.dump(eval_results, outfile, indent=4, separators=(',', ':'))\n    print(f'COCO evaluation results are successfully exported to {export_path}')\n    return {'eval_results': eval_results, 'export_path': export_path}",
            "def evaluate_core(dataset_path, result_path, metric: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs=None, metric_items=None, out_dir: str=None, areas: List[int]=[1024, 9216, 10000000000], COCO=None, COCOeval=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Evaluation in COCO protocol.\\n    Args:\\n        dataset_path (str): COCO dataset json path.\\n        result_path (str): COCO result json path.\\n        metric (str | list[str]): Metrics to be evaluated. Options are\\n            'bbox', 'segm', 'proposal'.\\n        classwise (bool): Whether to evaluating the AP for each class.\\n        max_detections (int): Maximum number of detections to consider for AP\\n            calculation.\\n            Default: 500\\n        iou_thrs (List[float], optional): IoU threshold used for\\n            evaluating recalls/mAPs. If set to a list, the average of all\\n            IoUs will also be computed. If not specified, [0.50, 0.55,\\n            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\\n            Default: None.\\n        metric_items (list[str] | str, optional): Metric items that will\\n            be returned. If not specified, ``['AR@10', 'AR@100',\\n            'AR@500', 'AR_s@500', 'AR_m@500', 'AR_l@500' ]`` will be\\n            used when ``metric=='proposal'``, ``['mAP', 'mAP50', 'mAP75',\\n            'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']``\\n            will be used when ``metric=='bbox' or metric=='segm'``.\\n        out_dir (str): Directory to save evaluation result json.\\n        areas (List[int]): area regions for coco evaluation calculations\\n    Returns:\\n        dict:\\n            eval_results (dict[str, float]): COCO style evaluation metric.\\n            export_path (str): Path for the exported eval result json.\\n\\n    \"\n    metrics = metric if isinstance(metric, list) else [metric]\n    allowed_metrics = ['bbox', 'segm']\n    for metric in metrics:\n        if metric not in allowed_metrics:\n            raise KeyError(f'metric {metric} is not supported')\n    if iou_thrs is None:\n        iou_thrs = np.linspace(0.5, 0.95, int(np.round((0.95 - 0.5) / 0.05)) + 1, endpoint=True)\n    if metric_items is not None:\n        if not isinstance(metric_items, list):\n            metric_items = [metric_items]\n    if areas is not None:\n        if len(areas) != 3:\n            raise ValueError('3 integers should be specified as areas, representing 3 area regions')\n    eval_results = OrderedDict()\n    cocoGt = COCO(dataset_path)\n    cat_ids = list(cocoGt.cats.keys())\n    for metric in metrics:\n        msg = f'Evaluating {metric}...'\n        msg = '\\n' + msg\n        print(msg)\n        iou_type = metric\n        with open(result_path) as json_file:\n            results = json.load(json_file)\n        try:\n            cocoDt = cocoGt.loadRes(results)\n        except IndexError:\n            print('The testing results of the whole dataset is empty.')\n            break\n        cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n        if areas is not None:\n            cocoEval.params.areaRng = [[0 ** 2, areas[2]], [0 ** 2, areas[0]], [areas[0], areas[1]], [areas[1], areas[2]]]\n        cocoEval.params.catIds = cat_ids\n        cocoEval.params.maxDets = [max_detections]\n        cocoEval.params.iouThrs = [iou_thrs] if not isinstance(iou_thrs, list) and (not isinstance(iou_thrs, np.ndarray)) else iou_thrs\n        coco_metric_names = {'mAP': 0, 'mAP75': 1, 'mAP50': 2, 'mAP_s': 3, 'mAP_m': 4, 'mAP_l': 5, 'mAP50_s': 6, 'mAP50_m': 7, 'mAP50_l': 8, 'AR_s': 9, 'AR_m': 10, 'AR_l': 11}\n        if metric_items is not None:\n            for metric_item in metric_items:\n                if metric_item not in coco_metric_names:\n                    raise KeyError(f'metric item {metric_item} is not supported')\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        mAP = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='all', maxDets=max_detections)\n        mAP50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='all', maxDets=max_detections)\n        mAP75 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.75, areaRng='all', maxDets=max_detections)\n        mAP50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='small', maxDets=max_detections)\n        mAP50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='medium', maxDets=max_detections)\n        mAP50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='large', maxDets=max_detections)\n        mAP_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='small', maxDets=max_detections)\n        mAP_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='medium', maxDets=max_detections)\n        mAP_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='large', maxDets=max_detections)\n        AR_s = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='small', maxDets=max_detections)\n        AR_m = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='medium', maxDets=max_detections)\n        AR_l = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='large', maxDets=max_detections)\n        cocoEval.stats = np.append([mAP, mAP75, mAP50, mAP_s, mAP_m, mAP_l, mAP50_s, mAP50_m, mAP50_l, AR_s, AR_m, AR_l], 0)\n        if classwise:\n            precisions = cocoEval.eval['precision']\n            if len(cat_ids) != precisions.shape[2]:\n                raise ValueError(f'The number of categories {len(cat_ids)} is not equal to the number of precisions {precisions.shape[2]}')\n            max_cat_name_len = 0\n            for (idx, catId) in enumerate(cat_ids):\n                nm = cocoGt.loadCats(catId)[0]\n                cat_name_len = len(nm['name'])\n                max_cat_name_len = cat_name_len if cat_name_len > max_cat_name_len else max_cat_name_len\n            results_per_category = []\n            for (idx, catId) in enumerate(cat_ids):\n                image_ids = cocoGt.getImgIds(catIds=[catId])\n                if len(image_ids) == 0:\n                    continue\n                nm = cocoGt.loadCats(catId)[0]\n                ap = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_s = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_m = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_l = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP\", f'{float(ap):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_s\", f'{float(ap_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_m\", f'{float(ap_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_l\", f'{float(ap_l):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50\", f'{float(ap50):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_s\", f'{float(ap50_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_m\", f'{float(ap50_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_l\", f'{float(ap50_l):0.3f}'))\n            num_columns = min(6, len(results_per_category) * 2)\n            results_flatten = list(itertools.chain(*results_per_category))\n            headers = ['category', 'AP'] * (num_columns // 2)\n            results_2d = itertools.zip_longest(*[results_flatten[i::num_columns] for i in range(num_columns)])\n            table_data = [headers]\n            table_data += [result for result in results_2d]\n            table = AsciiTable(table_data)\n            print('\\n' + table.table)\n        if metric_items is None:\n            metric_items = ['mAP', 'mAP50', 'mAP75', 'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']\n        for metric_item in metric_items:\n            key = f'{metric}_{metric_item}'\n            val = float(f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}')\n            eval_results[key] = val\n        ap = cocoEval.stats\n        eval_results[f'{metric}_mAP_copypaste'] = f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} {ap[4]:.3f} {ap[5]:.3f} {ap[6]:.3f} {ap[7]:.3f} {ap[8]:.3f}'\n        if classwise:\n            eval_results['results_per_category'] = {key: value for (key, value) in results_per_category}\n    if not out_dir:\n        out_dir = Path(result_path).parent\n    Path(out_dir).mkdir(parents=True, exist_ok=True)\n    export_path = str(Path(out_dir) / 'eval.json')\n    with open(export_path, 'w', encoding='utf-8') as outfile:\n        json.dump(eval_results, outfile, indent=4, separators=(',', ':'))\n    print(f'COCO evaluation results are successfully exported to {export_path}')\n    return {'eval_results': eval_results, 'export_path': export_path}",
            "def evaluate_core(dataset_path, result_path, metric: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs=None, metric_items=None, out_dir: str=None, areas: List[int]=[1024, 9216, 10000000000], COCO=None, COCOeval=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Evaluation in COCO protocol.\\n    Args:\\n        dataset_path (str): COCO dataset json path.\\n        result_path (str): COCO result json path.\\n        metric (str | list[str]): Metrics to be evaluated. Options are\\n            'bbox', 'segm', 'proposal'.\\n        classwise (bool): Whether to evaluating the AP for each class.\\n        max_detections (int): Maximum number of detections to consider for AP\\n            calculation.\\n            Default: 500\\n        iou_thrs (List[float], optional): IoU threshold used for\\n            evaluating recalls/mAPs. If set to a list, the average of all\\n            IoUs will also be computed. If not specified, [0.50, 0.55,\\n            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\\n            Default: None.\\n        metric_items (list[str] | str, optional): Metric items that will\\n            be returned. If not specified, ``['AR@10', 'AR@100',\\n            'AR@500', 'AR_s@500', 'AR_m@500', 'AR_l@500' ]`` will be\\n            used when ``metric=='proposal'``, ``['mAP', 'mAP50', 'mAP75',\\n            'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']``\\n            will be used when ``metric=='bbox' or metric=='segm'``.\\n        out_dir (str): Directory to save evaluation result json.\\n        areas (List[int]): area regions for coco evaluation calculations\\n    Returns:\\n        dict:\\n            eval_results (dict[str, float]): COCO style evaluation metric.\\n            export_path (str): Path for the exported eval result json.\\n\\n    \"\n    metrics = metric if isinstance(metric, list) else [metric]\n    allowed_metrics = ['bbox', 'segm']\n    for metric in metrics:\n        if metric not in allowed_metrics:\n            raise KeyError(f'metric {metric} is not supported')\n    if iou_thrs is None:\n        iou_thrs = np.linspace(0.5, 0.95, int(np.round((0.95 - 0.5) / 0.05)) + 1, endpoint=True)\n    if metric_items is not None:\n        if not isinstance(metric_items, list):\n            metric_items = [metric_items]\n    if areas is not None:\n        if len(areas) != 3:\n            raise ValueError('3 integers should be specified as areas, representing 3 area regions')\n    eval_results = OrderedDict()\n    cocoGt = COCO(dataset_path)\n    cat_ids = list(cocoGt.cats.keys())\n    for metric in metrics:\n        msg = f'Evaluating {metric}...'\n        msg = '\\n' + msg\n        print(msg)\n        iou_type = metric\n        with open(result_path) as json_file:\n            results = json.load(json_file)\n        try:\n            cocoDt = cocoGt.loadRes(results)\n        except IndexError:\n            print('The testing results of the whole dataset is empty.')\n            break\n        cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n        if areas is not None:\n            cocoEval.params.areaRng = [[0 ** 2, areas[2]], [0 ** 2, areas[0]], [areas[0], areas[1]], [areas[1], areas[2]]]\n        cocoEval.params.catIds = cat_ids\n        cocoEval.params.maxDets = [max_detections]\n        cocoEval.params.iouThrs = [iou_thrs] if not isinstance(iou_thrs, list) and (not isinstance(iou_thrs, np.ndarray)) else iou_thrs\n        coco_metric_names = {'mAP': 0, 'mAP75': 1, 'mAP50': 2, 'mAP_s': 3, 'mAP_m': 4, 'mAP_l': 5, 'mAP50_s': 6, 'mAP50_m': 7, 'mAP50_l': 8, 'AR_s': 9, 'AR_m': 10, 'AR_l': 11}\n        if metric_items is not None:\n            for metric_item in metric_items:\n                if metric_item not in coco_metric_names:\n                    raise KeyError(f'metric item {metric_item} is not supported')\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        mAP = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='all', maxDets=max_detections)\n        mAP50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='all', maxDets=max_detections)\n        mAP75 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.75, areaRng='all', maxDets=max_detections)\n        mAP50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='small', maxDets=max_detections)\n        mAP50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='medium', maxDets=max_detections)\n        mAP50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, areaRng='large', maxDets=max_detections)\n        mAP_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='small', maxDets=max_detections)\n        mAP_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='medium', maxDets=max_detections)\n        mAP_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=None, areaRng='large', maxDets=max_detections)\n        AR_s = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='small', maxDets=max_detections)\n        AR_m = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='medium', maxDets=max_detections)\n        AR_l = _cocoeval_summarize(cocoEval, ap=0, iouThr=None, areaRng='large', maxDets=max_detections)\n        cocoEval.stats = np.append([mAP, mAP75, mAP50, mAP_s, mAP_m, mAP_l, mAP50_s, mAP50_m, mAP50_l, AR_s, AR_m, AR_l], 0)\n        if classwise:\n            precisions = cocoEval.eval['precision']\n            if len(cat_ids) != precisions.shape[2]:\n                raise ValueError(f'The number of categories {len(cat_ids)} is not equal to the number of precisions {precisions.shape[2]}')\n            max_cat_name_len = 0\n            for (idx, catId) in enumerate(cat_ids):\n                nm = cocoGt.loadCats(catId)[0]\n                cat_name_len = len(nm['name'])\n                max_cat_name_len = cat_name_len if cat_name_len > max_cat_name_len else max_cat_name_len\n            results_per_category = []\n            for (idx, catId) in enumerate(cat_ids):\n                image_ids = cocoGt.getImgIds(catIds=[catId])\n                if len(image_ids) == 0:\n                    continue\n                nm = cocoGt.loadCats(catId)[0]\n                ap = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_s = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_m = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap_l = _cocoeval_summarize(cocoEval, ap=1, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50 = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='all', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_s = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='small', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_m = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='medium', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                ap50_l = _cocoeval_summarize(cocoEval, ap=1, iouThr=0.5, catIdx=idx, areaRng='large', maxDets=max_detections, catName=nm['name'], nameStrLen=max_cat_name_len)\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP\", f'{float(ap):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_s\", f'{float(ap_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_m\", f'{float(ap_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP_l\", f'{float(ap_l):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50\", f'{float(ap50):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_s\", f'{float(ap50_s):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_m\", f'{float(ap50_m):0.3f}'))\n                results_per_category.append((f\"{metric}_{nm['name']}_mAP50_l\", f'{float(ap50_l):0.3f}'))\n            num_columns = min(6, len(results_per_category) * 2)\n            results_flatten = list(itertools.chain(*results_per_category))\n            headers = ['category', 'AP'] * (num_columns // 2)\n            results_2d = itertools.zip_longest(*[results_flatten[i::num_columns] for i in range(num_columns)])\n            table_data = [headers]\n            table_data += [result for result in results_2d]\n            table = AsciiTable(table_data)\n            print('\\n' + table.table)\n        if metric_items is None:\n            metric_items = ['mAP', 'mAP50', 'mAP75', 'mAP_s', 'mAP_m', 'mAP_l', 'mAP50_s', 'mAP50_m', 'mAP50_l']\n        for metric_item in metric_items:\n            key = f'{metric}_{metric_item}'\n            val = float(f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}')\n            eval_results[key] = val\n        ap = cocoEval.stats\n        eval_results[f'{metric}_mAP_copypaste'] = f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} {ap[4]:.3f} {ap[5]:.3f} {ap[6]:.3f} {ap[7]:.3f} {ap[8]:.3f}'\n        if classwise:\n            eval_results['results_per_category'] = {key: value for (key, value) in results_per_category}\n    if not out_dir:\n        out_dir = Path(result_path).parent\n    Path(out_dir).mkdir(parents=True, exist_ok=True)\n    export_path = str(Path(out_dir) / 'eval.json')\n    with open(export_path, 'w', encoding='utf-8') as outfile:\n        json.dump(eval_results, outfile, indent=4, separators=(',', ':'))\n    print(f'COCO evaluation results are successfully exported to {export_path}')\n    return {'eval_results': eval_results, 'export_path': export_path}"
        ]
    },
    {
        "func_name": "evaluate",
        "original": "def evaluate(dataset_json_path: str, result_json_path: str, out_dir: str=None, type: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs: Union[List[float], float]=None, areas: List[int]=[1024, 9216, 10000000000], return_dict: bool=False):\n    \"\"\"\n    Args:\n        dataset_json_path (str): file path for the coco dataset json file\n        result_json_path (str): file path for the coco result json file\n        out_dir (str): dir to save eval result\n        type (bool): 'bbox' or 'segm'\n        classwise (bool): whether to evaluate the AP for each class\n        max_detections (int): Maximum number of detections to consider for AP alculation. Default: 500\n        iou_thrs (float): IoU threshold used for evaluating recalls/mAPs\n        areas (List[int]): area regions for coco evaluation calculations\n        return_dict (bool): If True, returns a dict with 'eval_results' 'export_path' fields.\n    \"\"\"\n    try:\n        from pycocotools.coco import COCO\n        from pycocotools.cocoeval import COCOeval\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please run \"pip install -U pycocotools\" to install pycocotools first for coco evaluation.')\n    result = evaluate_core(dataset_json_path, result_json_path, type, classwise, max_detections, iou_thrs, out_dir=out_dir, areas=areas, COCO=COCO, COCOeval=COCOeval)\n    if return_dict:\n        return result",
        "mutated": [
            "def evaluate(dataset_json_path: str, result_json_path: str, out_dir: str=None, type: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs: Union[List[float], float]=None, areas: List[int]=[1024, 9216, 10000000000], return_dict: bool=False):\n    if False:\n        i = 10\n    \"\\n    Args:\\n        dataset_json_path (str): file path for the coco dataset json file\\n        result_json_path (str): file path for the coco result json file\\n        out_dir (str): dir to save eval result\\n        type (bool): 'bbox' or 'segm'\\n        classwise (bool): whether to evaluate the AP for each class\\n        max_detections (int): Maximum number of detections to consider for AP alculation. Default: 500\\n        iou_thrs (float): IoU threshold used for evaluating recalls/mAPs\\n        areas (List[int]): area regions for coco evaluation calculations\\n        return_dict (bool): If True, returns a dict with 'eval_results' 'export_path' fields.\\n    \"\n    try:\n        from pycocotools.coco import COCO\n        from pycocotools.cocoeval import COCOeval\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please run \"pip install -U pycocotools\" to install pycocotools first for coco evaluation.')\n    result = evaluate_core(dataset_json_path, result_json_path, type, classwise, max_detections, iou_thrs, out_dir=out_dir, areas=areas, COCO=COCO, COCOeval=COCOeval)\n    if return_dict:\n        return result",
            "def evaluate(dataset_json_path: str, result_json_path: str, out_dir: str=None, type: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs: Union[List[float], float]=None, areas: List[int]=[1024, 9216, 10000000000], return_dict: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Args:\\n        dataset_json_path (str): file path for the coco dataset json file\\n        result_json_path (str): file path for the coco result json file\\n        out_dir (str): dir to save eval result\\n        type (bool): 'bbox' or 'segm'\\n        classwise (bool): whether to evaluate the AP for each class\\n        max_detections (int): Maximum number of detections to consider for AP alculation. Default: 500\\n        iou_thrs (float): IoU threshold used for evaluating recalls/mAPs\\n        areas (List[int]): area regions for coco evaluation calculations\\n        return_dict (bool): If True, returns a dict with 'eval_results' 'export_path' fields.\\n    \"\n    try:\n        from pycocotools.coco import COCO\n        from pycocotools.cocoeval import COCOeval\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please run \"pip install -U pycocotools\" to install pycocotools first for coco evaluation.')\n    result = evaluate_core(dataset_json_path, result_json_path, type, classwise, max_detections, iou_thrs, out_dir=out_dir, areas=areas, COCO=COCO, COCOeval=COCOeval)\n    if return_dict:\n        return result",
            "def evaluate(dataset_json_path: str, result_json_path: str, out_dir: str=None, type: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs: Union[List[float], float]=None, areas: List[int]=[1024, 9216, 10000000000], return_dict: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Args:\\n        dataset_json_path (str): file path for the coco dataset json file\\n        result_json_path (str): file path for the coco result json file\\n        out_dir (str): dir to save eval result\\n        type (bool): 'bbox' or 'segm'\\n        classwise (bool): whether to evaluate the AP for each class\\n        max_detections (int): Maximum number of detections to consider for AP alculation. Default: 500\\n        iou_thrs (float): IoU threshold used for evaluating recalls/mAPs\\n        areas (List[int]): area regions for coco evaluation calculations\\n        return_dict (bool): If True, returns a dict with 'eval_results' 'export_path' fields.\\n    \"\n    try:\n        from pycocotools.coco import COCO\n        from pycocotools.cocoeval import COCOeval\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please run \"pip install -U pycocotools\" to install pycocotools first for coco evaluation.')\n    result = evaluate_core(dataset_json_path, result_json_path, type, classwise, max_detections, iou_thrs, out_dir=out_dir, areas=areas, COCO=COCO, COCOeval=COCOeval)\n    if return_dict:\n        return result",
            "def evaluate(dataset_json_path: str, result_json_path: str, out_dir: str=None, type: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs: Union[List[float], float]=None, areas: List[int]=[1024, 9216, 10000000000], return_dict: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Args:\\n        dataset_json_path (str): file path for the coco dataset json file\\n        result_json_path (str): file path for the coco result json file\\n        out_dir (str): dir to save eval result\\n        type (bool): 'bbox' or 'segm'\\n        classwise (bool): whether to evaluate the AP for each class\\n        max_detections (int): Maximum number of detections to consider for AP alculation. Default: 500\\n        iou_thrs (float): IoU threshold used for evaluating recalls/mAPs\\n        areas (List[int]): area regions for coco evaluation calculations\\n        return_dict (bool): If True, returns a dict with 'eval_results' 'export_path' fields.\\n    \"\n    try:\n        from pycocotools.coco import COCO\n        from pycocotools.cocoeval import COCOeval\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please run \"pip install -U pycocotools\" to install pycocotools first for coco evaluation.')\n    result = evaluate_core(dataset_json_path, result_json_path, type, classwise, max_detections, iou_thrs, out_dir=out_dir, areas=areas, COCO=COCO, COCOeval=COCOeval)\n    if return_dict:\n        return result",
            "def evaluate(dataset_json_path: str, result_json_path: str, out_dir: str=None, type: str='bbox', classwise: bool=False, max_detections: int=500, iou_thrs: Union[List[float], float]=None, areas: List[int]=[1024, 9216, 10000000000], return_dict: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Args:\\n        dataset_json_path (str): file path for the coco dataset json file\\n        result_json_path (str): file path for the coco result json file\\n        out_dir (str): dir to save eval result\\n        type (bool): 'bbox' or 'segm'\\n        classwise (bool): whether to evaluate the AP for each class\\n        max_detections (int): Maximum number of detections to consider for AP alculation. Default: 500\\n        iou_thrs (float): IoU threshold used for evaluating recalls/mAPs\\n        areas (List[int]): area regions for coco evaluation calculations\\n        return_dict (bool): If True, returns a dict with 'eval_results' 'export_path' fields.\\n    \"\n    try:\n        from pycocotools.coco import COCO\n        from pycocotools.cocoeval import COCOeval\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError('Please run \"pip install -U pycocotools\" to install pycocotools first for coco evaluation.')\n    result = evaluate_core(dataset_json_path, result_json_path, type, classwise, max_detections, iou_thrs, out_dir=out_dir, areas=areas, COCO=COCO, COCOeval=COCOeval)\n    if return_dict:\n        return result"
        ]
    }
]