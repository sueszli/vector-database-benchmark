[
    {
        "func_name": "fit_single",
        "original": "def fit_single(solver, X, y, penalty='l2', single_target=True, C=1, max_iter=10, skip_slow=False, dtype=np.float64):\n    if skip_slow and solver == 'lightning' and (penalty == 'l1'):\n        print('skip_slowping l1 logistic regression with solver lightning.')\n        return\n    print('Solving %s logistic regression with penalty %s, solver %s.' % ('binary' if single_target else 'multinomial', penalty, solver))\n    if solver == 'lightning':\n        from lightning.classification import SAGAClassifier\n    if single_target or solver not in ['sag', 'saga']:\n        multi_class = 'ovr'\n    else:\n        multi_class = 'multinomial'\n    X = X.astype(dtype)\n    y = y.astype(dtype)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=42, stratify=y)\n    n_samples = X_train.shape[0]\n    n_classes = np.unique(y_train).shape[0]\n    test_scores = [1]\n    train_scores = [1]\n    accuracies = [1 / n_classes]\n    times = [0]\n    if penalty == 'l2':\n        alpha = 1.0 / (C * n_samples)\n        beta = 0\n        lightning_penalty = None\n    else:\n        alpha = 0.0\n        beta = 1.0 / (C * n_samples)\n        lightning_penalty = 'l1'\n    for this_max_iter in range(1, max_iter + 1, 2):\n        print('[%s, %s, %s] Max iter: %s' % ('binary' if single_target else 'multinomial', penalty, solver, this_max_iter))\n        if solver == 'lightning':\n            lr = SAGAClassifier(loss='log', alpha=alpha, beta=beta, penalty=lightning_penalty, tol=-1, max_iter=this_max_iter)\n        else:\n            lr = LogisticRegression(solver=solver, multi_class=multi_class, C=C, penalty=penalty, fit_intercept=False, tol=0, max_iter=this_max_iter, random_state=42)\n        X_train.max()\n        t0 = time.clock()\n        lr.fit(X_train, y_train)\n        train_time = time.clock() - t0\n        scores = []\n        for (X, y) in [(X_train, y_train), (X_test, y_test)]:\n            try:\n                y_pred = lr.predict_proba(X)\n            except NotImplementedError:\n                y_pred = _predict_proba(lr, X)\n            score = log_loss(y, y_pred, normalize=False) / n_samples\n            score += 0.5 * alpha * np.sum(lr.coef_ ** 2) + beta * np.sum(np.abs(lr.coef_))\n            scores.append(score)\n        (train_score, test_score) = tuple(scores)\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n        accuracies.append(accuracy)\n        times.append(train_time)\n    return (lr, times, train_scores, test_scores, accuracies)",
        "mutated": [
            "def fit_single(solver, X, y, penalty='l2', single_target=True, C=1, max_iter=10, skip_slow=False, dtype=np.float64):\n    if False:\n        i = 10\n    if skip_slow and solver == 'lightning' and (penalty == 'l1'):\n        print('skip_slowping l1 logistic regression with solver lightning.')\n        return\n    print('Solving %s logistic regression with penalty %s, solver %s.' % ('binary' if single_target else 'multinomial', penalty, solver))\n    if solver == 'lightning':\n        from lightning.classification import SAGAClassifier\n    if single_target or solver not in ['sag', 'saga']:\n        multi_class = 'ovr'\n    else:\n        multi_class = 'multinomial'\n    X = X.astype(dtype)\n    y = y.astype(dtype)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=42, stratify=y)\n    n_samples = X_train.shape[0]\n    n_classes = np.unique(y_train).shape[0]\n    test_scores = [1]\n    train_scores = [1]\n    accuracies = [1 / n_classes]\n    times = [0]\n    if penalty == 'l2':\n        alpha = 1.0 / (C * n_samples)\n        beta = 0\n        lightning_penalty = None\n    else:\n        alpha = 0.0\n        beta = 1.0 / (C * n_samples)\n        lightning_penalty = 'l1'\n    for this_max_iter in range(1, max_iter + 1, 2):\n        print('[%s, %s, %s] Max iter: %s' % ('binary' if single_target else 'multinomial', penalty, solver, this_max_iter))\n        if solver == 'lightning':\n            lr = SAGAClassifier(loss='log', alpha=alpha, beta=beta, penalty=lightning_penalty, tol=-1, max_iter=this_max_iter)\n        else:\n            lr = LogisticRegression(solver=solver, multi_class=multi_class, C=C, penalty=penalty, fit_intercept=False, tol=0, max_iter=this_max_iter, random_state=42)\n        X_train.max()\n        t0 = time.clock()\n        lr.fit(X_train, y_train)\n        train_time = time.clock() - t0\n        scores = []\n        for (X, y) in [(X_train, y_train), (X_test, y_test)]:\n            try:\n                y_pred = lr.predict_proba(X)\n            except NotImplementedError:\n                y_pred = _predict_proba(lr, X)\n            score = log_loss(y, y_pred, normalize=False) / n_samples\n            score += 0.5 * alpha * np.sum(lr.coef_ ** 2) + beta * np.sum(np.abs(lr.coef_))\n            scores.append(score)\n        (train_score, test_score) = tuple(scores)\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n        accuracies.append(accuracy)\n        times.append(train_time)\n    return (lr, times, train_scores, test_scores, accuracies)",
            "def fit_single(solver, X, y, penalty='l2', single_target=True, C=1, max_iter=10, skip_slow=False, dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if skip_slow and solver == 'lightning' and (penalty == 'l1'):\n        print('skip_slowping l1 logistic regression with solver lightning.')\n        return\n    print('Solving %s logistic regression with penalty %s, solver %s.' % ('binary' if single_target else 'multinomial', penalty, solver))\n    if solver == 'lightning':\n        from lightning.classification import SAGAClassifier\n    if single_target or solver not in ['sag', 'saga']:\n        multi_class = 'ovr'\n    else:\n        multi_class = 'multinomial'\n    X = X.astype(dtype)\n    y = y.astype(dtype)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=42, stratify=y)\n    n_samples = X_train.shape[0]\n    n_classes = np.unique(y_train).shape[0]\n    test_scores = [1]\n    train_scores = [1]\n    accuracies = [1 / n_classes]\n    times = [0]\n    if penalty == 'l2':\n        alpha = 1.0 / (C * n_samples)\n        beta = 0\n        lightning_penalty = None\n    else:\n        alpha = 0.0\n        beta = 1.0 / (C * n_samples)\n        lightning_penalty = 'l1'\n    for this_max_iter in range(1, max_iter + 1, 2):\n        print('[%s, %s, %s] Max iter: %s' % ('binary' if single_target else 'multinomial', penalty, solver, this_max_iter))\n        if solver == 'lightning':\n            lr = SAGAClassifier(loss='log', alpha=alpha, beta=beta, penalty=lightning_penalty, tol=-1, max_iter=this_max_iter)\n        else:\n            lr = LogisticRegression(solver=solver, multi_class=multi_class, C=C, penalty=penalty, fit_intercept=False, tol=0, max_iter=this_max_iter, random_state=42)\n        X_train.max()\n        t0 = time.clock()\n        lr.fit(X_train, y_train)\n        train_time = time.clock() - t0\n        scores = []\n        for (X, y) in [(X_train, y_train), (X_test, y_test)]:\n            try:\n                y_pred = lr.predict_proba(X)\n            except NotImplementedError:\n                y_pred = _predict_proba(lr, X)\n            score = log_loss(y, y_pred, normalize=False) / n_samples\n            score += 0.5 * alpha * np.sum(lr.coef_ ** 2) + beta * np.sum(np.abs(lr.coef_))\n            scores.append(score)\n        (train_score, test_score) = tuple(scores)\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n        accuracies.append(accuracy)\n        times.append(train_time)\n    return (lr, times, train_scores, test_scores, accuracies)",
            "def fit_single(solver, X, y, penalty='l2', single_target=True, C=1, max_iter=10, skip_slow=False, dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if skip_slow and solver == 'lightning' and (penalty == 'l1'):\n        print('skip_slowping l1 logistic regression with solver lightning.')\n        return\n    print('Solving %s logistic regression with penalty %s, solver %s.' % ('binary' if single_target else 'multinomial', penalty, solver))\n    if solver == 'lightning':\n        from lightning.classification import SAGAClassifier\n    if single_target or solver not in ['sag', 'saga']:\n        multi_class = 'ovr'\n    else:\n        multi_class = 'multinomial'\n    X = X.astype(dtype)\n    y = y.astype(dtype)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=42, stratify=y)\n    n_samples = X_train.shape[0]\n    n_classes = np.unique(y_train).shape[0]\n    test_scores = [1]\n    train_scores = [1]\n    accuracies = [1 / n_classes]\n    times = [0]\n    if penalty == 'l2':\n        alpha = 1.0 / (C * n_samples)\n        beta = 0\n        lightning_penalty = None\n    else:\n        alpha = 0.0\n        beta = 1.0 / (C * n_samples)\n        lightning_penalty = 'l1'\n    for this_max_iter in range(1, max_iter + 1, 2):\n        print('[%s, %s, %s] Max iter: %s' % ('binary' if single_target else 'multinomial', penalty, solver, this_max_iter))\n        if solver == 'lightning':\n            lr = SAGAClassifier(loss='log', alpha=alpha, beta=beta, penalty=lightning_penalty, tol=-1, max_iter=this_max_iter)\n        else:\n            lr = LogisticRegression(solver=solver, multi_class=multi_class, C=C, penalty=penalty, fit_intercept=False, tol=0, max_iter=this_max_iter, random_state=42)\n        X_train.max()\n        t0 = time.clock()\n        lr.fit(X_train, y_train)\n        train_time = time.clock() - t0\n        scores = []\n        for (X, y) in [(X_train, y_train), (X_test, y_test)]:\n            try:\n                y_pred = lr.predict_proba(X)\n            except NotImplementedError:\n                y_pred = _predict_proba(lr, X)\n            score = log_loss(y, y_pred, normalize=False) / n_samples\n            score += 0.5 * alpha * np.sum(lr.coef_ ** 2) + beta * np.sum(np.abs(lr.coef_))\n            scores.append(score)\n        (train_score, test_score) = tuple(scores)\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n        accuracies.append(accuracy)\n        times.append(train_time)\n    return (lr, times, train_scores, test_scores, accuracies)",
            "def fit_single(solver, X, y, penalty='l2', single_target=True, C=1, max_iter=10, skip_slow=False, dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if skip_slow and solver == 'lightning' and (penalty == 'l1'):\n        print('skip_slowping l1 logistic regression with solver lightning.')\n        return\n    print('Solving %s logistic regression with penalty %s, solver %s.' % ('binary' if single_target else 'multinomial', penalty, solver))\n    if solver == 'lightning':\n        from lightning.classification import SAGAClassifier\n    if single_target or solver not in ['sag', 'saga']:\n        multi_class = 'ovr'\n    else:\n        multi_class = 'multinomial'\n    X = X.astype(dtype)\n    y = y.astype(dtype)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=42, stratify=y)\n    n_samples = X_train.shape[0]\n    n_classes = np.unique(y_train).shape[0]\n    test_scores = [1]\n    train_scores = [1]\n    accuracies = [1 / n_classes]\n    times = [0]\n    if penalty == 'l2':\n        alpha = 1.0 / (C * n_samples)\n        beta = 0\n        lightning_penalty = None\n    else:\n        alpha = 0.0\n        beta = 1.0 / (C * n_samples)\n        lightning_penalty = 'l1'\n    for this_max_iter in range(1, max_iter + 1, 2):\n        print('[%s, %s, %s] Max iter: %s' % ('binary' if single_target else 'multinomial', penalty, solver, this_max_iter))\n        if solver == 'lightning':\n            lr = SAGAClassifier(loss='log', alpha=alpha, beta=beta, penalty=lightning_penalty, tol=-1, max_iter=this_max_iter)\n        else:\n            lr = LogisticRegression(solver=solver, multi_class=multi_class, C=C, penalty=penalty, fit_intercept=False, tol=0, max_iter=this_max_iter, random_state=42)\n        X_train.max()\n        t0 = time.clock()\n        lr.fit(X_train, y_train)\n        train_time = time.clock() - t0\n        scores = []\n        for (X, y) in [(X_train, y_train), (X_test, y_test)]:\n            try:\n                y_pred = lr.predict_proba(X)\n            except NotImplementedError:\n                y_pred = _predict_proba(lr, X)\n            score = log_loss(y, y_pred, normalize=False) / n_samples\n            score += 0.5 * alpha * np.sum(lr.coef_ ** 2) + beta * np.sum(np.abs(lr.coef_))\n            scores.append(score)\n        (train_score, test_score) = tuple(scores)\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n        accuracies.append(accuracy)\n        times.append(train_time)\n    return (lr, times, train_scores, test_scores, accuracies)",
            "def fit_single(solver, X, y, penalty='l2', single_target=True, C=1, max_iter=10, skip_slow=False, dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if skip_slow and solver == 'lightning' and (penalty == 'l1'):\n        print('skip_slowping l1 logistic regression with solver lightning.')\n        return\n    print('Solving %s logistic regression with penalty %s, solver %s.' % ('binary' if single_target else 'multinomial', penalty, solver))\n    if solver == 'lightning':\n        from lightning.classification import SAGAClassifier\n    if single_target or solver not in ['sag', 'saga']:\n        multi_class = 'ovr'\n    else:\n        multi_class = 'multinomial'\n    X = X.astype(dtype)\n    y = y.astype(dtype)\n    (X_train, X_test, y_train, y_test) = train_test_split(X, y, random_state=42, stratify=y)\n    n_samples = X_train.shape[0]\n    n_classes = np.unique(y_train).shape[0]\n    test_scores = [1]\n    train_scores = [1]\n    accuracies = [1 / n_classes]\n    times = [0]\n    if penalty == 'l2':\n        alpha = 1.0 / (C * n_samples)\n        beta = 0\n        lightning_penalty = None\n    else:\n        alpha = 0.0\n        beta = 1.0 / (C * n_samples)\n        lightning_penalty = 'l1'\n    for this_max_iter in range(1, max_iter + 1, 2):\n        print('[%s, %s, %s] Max iter: %s' % ('binary' if single_target else 'multinomial', penalty, solver, this_max_iter))\n        if solver == 'lightning':\n            lr = SAGAClassifier(loss='log', alpha=alpha, beta=beta, penalty=lightning_penalty, tol=-1, max_iter=this_max_iter)\n        else:\n            lr = LogisticRegression(solver=solver, multi_class=multi_class, C=C, penalty=penalty, fit_intercept=False, tol=0, max_iter=this_max_iter, random_state=42)\n        X_train.max()\n        t0 = time.clock()\n        lr.fit(X_train, y_train)\n        train_time = time.clock() - t0\n        scores = []\n        for (X, y) in [(X_train, y_train), (X_test, y_test)]:\n            try:\n                y_pred = lr.predict_proba(X)\n            except NotImplementedError:\n                y_pred = _predict_proba(lr, X)\n            score = log_loss(y, y_pred, normalize=False) / n_samples\n            score += 0.5 * alpha * np.sum(lr.coef_ ** 2) + beta * np.sum(np.abs(lr.coef_))\n            scores.append(score)\n        (train_score, test_score) = tuple(scores)\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n        accuracies.append(accuracy)\n        times.append(train_time)\n    return (lr, times, train_scores, test_scores, accuracies)"
        ]
    },
    {
        "func_name": "_predict_proba",
        "original": "def _predict_proba(lr, X):\n    pred = safe_sparse_dot(X, lr.coef_.T)\n    if hasattr(lr, 'intercept_'):\n        pred += lr.intercept_\n    return softmax(pred)",
        "mutated": [
            "def _predict_proba(lr, X):\n    if False:\n        i = 10\n    pred = safe_sparse_dot(X, lr.coef_.T)\n    if hasattr(lr, 'intercept_'):\n        pred += lr.intercept_\n    return softmax(pred)",
            "def _predict_proba(lr, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pred = safe_sparse_dot(X, lr.coef_.T)\n    if hasattr(lr, 'intercept_'):\n        pred += lr.intercept_\n    return softmax(pred)",
            "def _predict_proba(lr, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pred = safe_sparse_dot(X, lr.coef_.T)\n    if hasattr(lr, 'intercept_'):\n        pred += lr.intercept_\n    return softmax(pred)",
            "def _predict_proba(lr, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pred = safe_sparse_dot(X, lr.coef_.T)\n    if hasattr(lr, 'intercept_'):\n        pred += lr.intercept_\n    return softmax(pred)",
            "def _predict_proba(lr, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pred = safe_sparse_dot(X, lr.coef_.T)\n    if hasattr(lr, 'intercept_'):\n        pred += lr.intercept_\n    return softmax(pred)"
        ]
    },
    {
        "func_name": "exp",
        "original": "def exp(solvers, penalty, single_target, n_samples=30000, max_iter=20, dataset='rcv1', n_jobs=1, skip_slow=False):\n    dtypes_mapping = {'float64': np.float64, 'float32': np.float32}\n    if dataset == 'rcv1':\n        rcv1 = fetch_rcv1()\n        lbin = LabelBinarizer()\n        lbin.fit(rcv1.target_names)\n        X = rcv1.data\n        y = rcv1.target\n        y = lbin.inverse_transform(y)\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 16] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    elif dataset == 'digits':\n        (X, y) = load_digits(return_X_y=True)\n        if single_target:\n            y_n = y.copy()\n            y_n[y < 5] = 1\n            y_n[y >= 5] = 0\n            y = y_n\n    elif dataset == 'iris':\n        iris = load_iris()\n        (X, y) = (iris.data, iris.target)\n    elif dataset == '20newspaper':\n        ng = fetch_20newsgroups_vectorized()\n        X = ng.data\n        y = ng.target\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 4] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    X = X[:n_samples]\n    y = y[:n_samples]\n    out = Parallel(n_jobs=n_jobs, mmap_mode=None)((delayed(fit_single)(solver, X, y, penalty=penalty, single_target=single_target, dtype=dtype, C=1, max_iter=max_iter, skip_slow=skip_slow) for solver in solvers for dtype in dtypes_mapping.values()))\n    res = []\n    idx = 0\n    for dtype_name in dtypes_mapping.keys():\n        for solver in solvers:\n            if not (skip_slow and solver == 'lightning' and (penalty == 'l1')):\n                (lr, times, train_scores, test_scores, accuracies) = out[idx]\n                this_res = dict(solver=solver, penalty=penalty, dtype=dtype_name, single_target=single_target, times=times, train_scores=train_scores, test_scores=test_scores, accuracies=accuracies)\n                res.append(this_res)\n            idx += 1\n    with open('bench_saga.json', 'w+') as f:\n        json.dump(res, f)",
        "mutated": [
            "def exp(solvers, penalty, single_target, n_samples=30000, max_iter=20, dataset='rcv1', n_jobs=1, skip_slow=False):\n    if False:\n        i = 10\n    dtypes_mapping = {'float64': np.float64, 'float32': np.float32}\n    if dataset == 'rcv1':\n        rcv1 = fetch_rcv1()\n        lbin = LabelBinarizer()\n        lbin.fit(rcv1.target_names)\n        X = rcv1.data\n        y = rcv1.target\n        y = lbin.inverse_transform(y)\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 16] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    elif dataset == 'digits':\n        (X, y) = load_digits(return_X_y=True)\n        if single_target:\n            y_n = y.copy()\n            y_n[y < 5] = 1\n            y_n[y >= 5] = 0\n            y = y_n\n    elif dataset == 'iris':\n        iris = load_iris()\n        (X, y) = (iris.data, iris.target)\n    elif dataset == '20newspaper':\n        ng = fetch_20newsgroups_vectorized()\n        X = ng.data\n        y = ng.target\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 4] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    X = X[:n_samples]\n    y = y[:n_samples]\n    out = Parallel(n_jobs=n_jobs, mmap_mode=None)((delayed(fit_single)(solver, X, y, penalty=penalty, single_target=single_target, dtype=dtype, C=1, max_iter=max_iter, skip_slow=skip_slow) for solver in solvers for dtype in dtypes_mapping.values()))\n    res = []\n    idx = 0\n    for dtype_name in dtypes_mapping.keys():\n        for solver in solvers:\n            if not (skip_slow and solver == 'lightning' and (penalty == 'l1')):\n                (lr, times, train_scores, test_scores, accuracies) = out[idx]\n                this_res = dict(solver=solver, penalty=penalty, dtype=dtype_name, single_target=single_target, times=times, train_scores=train_scores, test_scores=test_scores, accuracies=accuracies)\n                res.append(this_res)\n            idx += 1\n    with open('bench_saga.json', 'w+') as f:\n        json.dump(res, f)",
            "def exp(solvers, penalty, single_target, n_samples=30000, max_iter=20, dataset='rcv1', n_jobs=1, skip_slow=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtypes_mapping = {'float64': np.float64, 'float32': np.float32}\n    if dataset == 'rcv1':\n        rcv1 = fetch_rcv1()\n        lbin = LabelBinarizer()\n        lbin.fit(rcv1.target_names)\n        X = rcv1.data\n        y = rcv1.target\n        y = lbin.inverse_transform(y)\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 16] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    elif dataset == 'digits':\n        (X, y) = load_digits(return_X_y=True)\n        if single_target:\n            y_n = y.copy()\n            y_n[y < 5] = 1\n            y_n[y >= 5] = 0\n            y = y_n\n    elif dataset == 'iris':\n        iris = load_iris()\n        (X, y) = (iris.data, iris.target)\n    elif dataset == '20newspaper':\n        ng = fetch_20newsgroups_vectorized()\n        X = ng.data\n        y = ng.target\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 4] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    X = X[:n_samples]\n    y = y[:n_samples]\n    out = Parallel(n_jobs=n_jobs, mmap_mode=None)((delayed(fit_single)(solver, X, y, penalty=penalty, single_target=single_target, dtype=dtype, C=1, max_iter=max_iter, skip_slow=skip_slow) for solver in solvers for dtype in dtypes_mapping.values()))\n    res = []\n    idx = 0\n    for dtype_name in dtypes_mapping.keys():\n        for solver in solvers:\n            if not (skip_slow and solver == 'lightning' and (penalty == 'l1')):\n                (lr, times, train_scores, test_scores, accuracies) = out[idx]\n                this_res = dict(solver=solver, penalty=penalty, dtype=dtype_name, single_target=single_target, times=times, train_scores=train_scores, test_scores=test_scores, accuracies=accuracies)\n                res.append(this_res)\n            idx += 1\n    with open('bench_saga.json', 'w+') as f:\n        json.dump(res, f)",
            "def exp(solvers, penalty, single_target, n_samples=30000, max_iter=20, dataset='rcv1', n_jobs=1, skip_slow=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtypes_mapping = {'float64': np.float64, 'float32': np.float32}\n    if dataset == 'rcv1':\n        rcv1 = fetch_rcv1()\n        lbin = LabelBinarizer()\n        lbin.fit(rcv1.target_names)\n        X = rcv1.data\n        y = rcv1.target\n        y = lbin.inverse_transform(y)\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 16] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    elif dataset == 'digits':\n        (X, y) = load_digits(return_X_y=True)\n        if single_target:\n            y_n = y.copy()\n            y_n[y < 5] = 1\n            y_n[y >= 5] = 0\n            y = y_n\n    elif dataset == 'iris':\n        iris = load_iris()\n        (X, y) = (iris.data, iris.target)\n    elif dataset == '20newspaper':\n        ng = fetch_20newsgroups_vectorized()\n        X = ng.data\n        y = ng.target\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 4] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    X = X[:n_samples]\n    y = y[:n_samples]\n    out = Parallel(n_jobs=n_jobs, mmap_mode=None)((delayed(fit_single)(solver, X, y, penalty=penalty, single_target=single_target, dtype=dtype, C=1, max_iter=max_iter, skip_slow=skip_slow) for solver in solvers for dtype in dtypes_mapping.values()))\n    res = []\n    idx = 0\n    for dtype_name in dtypes_mapping.keys():\n        for solver in solvers:\n            if not (skip_slow and solver == 'lightning' and (penalty == 'l1')):\n                (lr, times, train_scores, test_scores, accuracies) = out[idx]\n                this_res = dict(solver=solver, penalty=penalty, dtype=dtype_name, single_target=single_target, times=times, train_scores=train_scores, test_scores=test_scores, accuracies=accuracies)\n                res.append(this_res)\n            idx += 1\n    with open('bench_saga.json', 'w+') as f:\n        json.dump(res, f)",
            "def exp(solvers, penalty, single_target, n_samples=30000, max_iter=20, dataset='rcv1', n_jobs=1, skip_slow=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtypes_mapping = {'float64': np.float64, 'float32': np.float32}\n    if dataset == 'rcv1':\n        rcv1 = fetch_rcv1()\n        lbin = LabelBinarizer()\n        lbin.fit(rcv1.target_names)\n        X = rcv1.data\n        y = rcv1.target\n        y = lbin.inverse_transform(y)\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 16] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    elif dataset == 'digits':\n        (X, y) = load_digits(return_X_y=True)\n        if single_target:\n            y_n = y.copy()\n            y_n[y < 5] = 1\n            y_n[y >= 5] = 0\n            y = y_n\n    elif dataset == 'iris':\n        iris = load_iris()\n        (X, y) = (iris.data, iris.target)\n    elif dataset == '20newspaper':\n        ng = fetch_20newsgroups_vectorized()\n        X = ng.data\n        y = ng.target\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 4] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    X = X[:n_samples]\n    y = y[:n_samples]\n    out = Parallel(n_jobs=n_jobs, mmap_mode=None)((delayed(fit_single)(solver, X, y, penalty=penalty, single_target=single_target, dtype=dtype, C=1, max_iter=max_iter, skip_slow=skip_slow) for solver in solvers for dtype in dtypes_mapping.values()))\n    res = []\n    idx = 0\n    for dtype_name in dtypes_mapping.keys():\n        for solver in solvers:\n            if not (skip_slow and solver == 'lightning' and (penalty == 'l1')):\n                (lr, times, train_scores, test_scores, accuracies) = out[idx]\n                this_res = dict(solver=solver, penalty=penalty, dtype=dtype_name, single_target=single_target, times=times, train_scores=train_scores, test_scores=test_scores, accuracies=accuracies)\n                res.append(this_res)\n            idx += 1\n    with open('bench_saga.json', 'w+') as f:\n        json.dump(res, f)",
            "def exp(solvers, penalty, single_target, n_samples=30000, max_iter=20, dataset='rcv1', n_jobs=1, skip_slow=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtypes_mapping = {'float64': np.float64, 'float32': np.float32}\n    if dataset == 'rcv1':\n        rcv1 = fetch_rcv1()\n        lbin = LabelBinarizer()\n        lbin.fit(rcv1.target_names)\n        X = rcv1.data\n        y = rcv1.target\n        y = lbin.inverse_transform(y)\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 16] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    elif dataset == 'digits':\n        (X, y) = load_digits(return_X_y=True)\n        if single_target:\n            y_n = y.copy()\n            y_n[y < 5] = 1\n            y_n[y >= 5] = 0\n            y = y_n\n    elif dataset == 'iris':\n        iris = load_iris()\n        (X, y) = (iris.data, iris.target)\n    elif dataset == '20newspaper':\n        ng = fetch_20newsgroups_vectorized()\n        X = ng.data\n        y = ng.target\n        if single_target:\n            y_n = y.copy()\n            y_n[y > 4] = 1\n            y_n[y <= 16] = 0\n            y = y_n\n    X = X[:n_samples]\n    y = y[:n_samples]\n    out = Parallel(n_jobs=n_jobs, mmap_mode=None)((delayed(fit_single)(solver, X, y, penalty=penalty, single_target=single_target, dtype=dtype, C=1, max_iter=max_iter, skip_slow=skip_slow) for solver in solvers for dtype in dtypes_mapping.values()))\n    res = []\n    idx = 0\n    for dtype_name in dtypes_mapping.keys():\n        for solver in solvers:\n            if not (skip_slow and solver == 'lightning' and (penalty == 'l1')):\n                (lr, times, train_scores, test_scores, accuracies) = out[idx]\n                this_res = dict(solver=solver, penalty=penalty, dtype=dtype_name, single_target=single_target, times=times, train_scores=train_scores, test_scores=test_scores, accuracies=accuracies)\n                res.append(this_res)\n            idx += 1\n    with open('bench_saga.json', 'w+') as f:\n        json.dump(res, f)"
        ]
    },
    {
        "func_name": "plot",
        "original": "def plot(outname=None):\n    import pandas as pd\n    with open('bench_saga.json', 'r') as f:\n        f = json.load(f)\n    res = pd.DataFrame(f)\n    res.set_index(['single_target'], inplace=True)\n    grouped = res.groupby(level=['single_target'])\n    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}\n    linestyles = {'float32': '--', 'float64': '-'}\n    alpha = {'float64': 0.5, 'float32': 1}\n    for (idx, group) in grouped:\n        single_target = idx\n        (fig, axes) = plt.subplots(figsize=(12, 4), ncols=4)\n        ax = axes[0]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label='%s - %s' % (solver, dtype), color=colors[solver], alpha=alpha[dtype], marker='.', linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Training objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[1]\n        for (scores, times, solver, dtype) in zip(group['test_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label=solver, color=colors[solver], linestyle=linestyles[dtype], marker='.', alpha=alpha[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[2]\n        for (accuracy, times, solver, dtype) in zip(group['accuracies'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, accuracy, label='%s - %s' % (solver, dtype), alpha=alpha[dtype], marker='.', color=colors[solver], linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test accuracy')\n        ax.legend()\n        name = 'single_target' if single_target else 'multi_target'\n        name += '_%s' % penalty\n        plt.suptitle(name)\n        if outname is None:\n            outname = name + '.png'\n        fig.tight_layout()\n        fig.subplots_adjust(top=0.9)\n        ax = axes[3]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(np.arange(len(scores)), scores, label='%s - %s' % (solver, dtype), marker='.', alpha=alpha[dtype], color=colors[solver], linestyle=linestyles[dtype])\n        ax.set_yscale('log')\n        ax.set_xlabel('# iterations')\n        ax.set_ylabel('Objective function')\n        ax.legend()\n        plt.savefig(outname)",
        "mutated": [
            "def plot(outname=None):\n    if False:\n        i = 10\n    import pandas as pd\n    with open('bench_saga.json', 'r') as f:\n        f = json.load(f)\n    res = pd.DataFrame(f)\n    res.set_index(['single_target'], inplace=True)\n    grouped = res.groupby(level=['single_target'])\n    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}\n    linestyles = {'float32': '--', 'float64': '-'}\n    alpha = {'float64': 0.5, 'float32': 1}\n    for (idx, group) in grouped:\n        single_target = idx\n        (fig, axes) = plt.subplots(figsize=(12, 4), ncols=4)\n        ax = axes[0]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label='%s - %s' % (solver, dtype), color=colors[solver], alpha=alpha[dtype], marker='.', linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Training objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[1]\n        for (scores, times, solver, dtype) in zip(group['test_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label=solver, color=colors[solver], linestyle=linestyles[dtype], marker='.', alpha=alpha[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[2]\n        for (accuracy, times, solver, dtype) in zip(group['accuracies'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, accuracy, label='%s - %s' % (solver, dtype), alpha=alpha[dtype], marker='.', color=colors[solver], linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test accuracy')\n        ax.legend()\n        name = 'single_target' if single_target else 'multi_target'\n        name += '_%s' % penalty\n        plt.suptitle(name)\n        if outname is None:\n            outname = name + '.png'\n        fig.tight_layout()\n        fig.subplots_adjust(top=0.9)\n        ax = axes[3]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(np.arange(len(scores)), scores, label='%s - %s' % (solver, dtype), marker='.', alpha=alpha[dtype], color=colors[solver], linestyle=linestyles[dtype])\n        ax.set_yscale('log')\n        ax.set_xlabel('# iterations')\n        ax.set_ylabel('Objective function')\n        ax.legend()\n        plt.savefig(outname)",
            "def plot(outname=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    import pandas as pd\n    with open('bench_saga.json', 'r') as f:\n        f = json.load(f)\n    res = pd.DataFrame(f)\n    res.set_index(['single_target'], inplace=True)\n    grouped = res.groupby(level=['single_target'])\n    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}\n    linestyles = {'float32': '--', 'float64': '-'}\n    alpha = {'float64': 0.5, 'float32': 1}\n    for (idx, group) in grouped:\n        single_target = idx\n        (fig, axes) = plt.subplots(figsize=(12, 4), ncols=4)\n        ax = axes[0]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label='%s - %s' % (solver, dtype), color=colors[solver], alpha=alpha[dtype], marker='.', linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Training objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[1]\n        for (scores, times, solver, dtype) in zip(group['test_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label=solver, color=colors[solver], linestyle=linestyles[dtype], marker='.', alpha=alpha[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[2]\n        for (accuracy, times, solver, dtype) in zip(group['accuracies'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, accuracy, label='%s - %s' % (solver, dtype), alpha=alpha[dtype], marker='.', color=colors[solver], linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test accuracy')\n        ax.legend()\n        name = 'single_target' if single_target else 'multi_target'\n        name += '_%s' % penalty\n        plt.suptitle(name)\n        if outname is None:\n            outname = name + '.png'\n        fig.tight_layout()\n        fig.subplots_adjust(top=0.9)\n        ax = axes[3]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(np.arange(len(scores)), scores, label='%s - %s' % (solver, dtype), marker='.', alpha=alpha[dtype], color=colors[solver], linestyle=linestyles[dtype])\n        ax.set_yscale('log')\n        ax.set_xlabel('# iterations')\n        ax.set_ylabel('Objective function')\n        ax.legend()\n        plt.savefig(outname)",
            "def plot(outname=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    import pandas as pd\n    with open('bench_saga.json', 'r') as f:\n        f = json.load(f)\n    res = pd.DataFrame(f)\n    res.set_index(['single_target'], inplace=True)\n    grouped = res.groupby(level=['single_target'])\n    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}\n    linestyles = {'float32': '--', 'float64': '-'}\n    alpha = {'float64': 0.5, 'float32': 1}\n    for (idx, group) in grouped:\n        single_target = idx\n        (fig, axes) = plt.subplots(figsize=(12, 4), ncols=4)\n        ax = axes[0]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label='%s - %s' % (solver, dtype), color=colors[solver], alpha=alpha[dtype], marker='.', linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Training objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[1]\n        for (scores, times, solver, dtype) in zip(group['test_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label=solver, color=colors[solver], linestyle=linestyles[dtype], marker='.', alpha=alpha[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[2]\n        for (accuracy, times, solver, dtype) in zip(group['accuracies'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, accuracy, label='%s - %s' % (solver, dtype), alpha=alpha[dtype], marker='.', color=colors[solver], linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test accuracy')\n        ax.legend()\n        name = 'single_target' if single_target else 'multi_target'\n        name += '_%s' % penalty\n        plt.suptitle(name)\n        if outname is None:\n            outname = name + '.png'\n        fig.tight_layout()\n        fig.subplots_adjust(top=0.9)\n        ax = axes[3]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(np.arange(len(scores)), scores, label='%s - %s' % (solver, dtype), marker='.', alpha=alpha[dtype], color=colors[solver], linestyle=linestyles[dtype])\n        ax.set_yscale('log')\n        ax.set_xlabel('# iterations')\n        ax.set_ylabel('Objective function')\n        ax.legend()\n        plt.savefig(outname)",
            "def plot(outname=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    import pandas as pd\n    with open('bench_saga.json', 'r') as f:\n        f = json.load(f)\n    res = pd.DataFrame(f)\n    res.set_index(['single_target'], inplace=True)\n    grouped = res.groupby(level=['single_target'])\n    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}\n    linestyles = {'float32': '--', 'float64': '-'}\n    alpha = {'float64': 0.5, 'float32': 1}\n    for (idx, group) in grouped:\n        single_target = idx\n        (fig, axes) = plt.subplots(figsize=(12, 4), ncols=4)\n        ax = axes[0]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label='%s - %s' % (solver, dtype), color=colors[solver], alpha=alpha[dtype], marker='.', linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Training objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[1]\n        for (scores, times, solver, dtype) in zip(group['test_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label=solver, color=colors[solver], linestyle=linestyles[dtype], marker='.', alpha=alpha[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[2]\n        for (accuracy, times, solver, dtype) in zip(group['accuracies'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, accuracy, label='%s - %s' % (solver, dtype), alpha=alpha[dtype], marker='.', color=colors[solver], linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test accuracy')\n        ax.legend()\n        name = 'single_target' if single_target else 'multi_target'\n        name += '_%s' % penalty\n        plt.suptitle(name)\n        if outname is None:\n            outname = name + '.png'\n        fig.tight_layout()\n        fig.subplots_adjust(top=0.9)\n        ax = axes[3]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(np.arange(len(scores)), scores, label='%s - %s' % (solver, dtype), marker='.', alpha=alpha[dtype], color=colors[solver], linestyle=linestyles[dtype])\n        ax.set_yscale('log')\n        ax.set_xlabel('# iterations')\n        ax.set_ylabel('Objective function')\n        ax.legend()\n        plt.savefig(outname)",
            "def plot(outname=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    import pandas as pd\n    with open('bench_saga.json', 'r') as f:\n        f = json.load(f)\n    res = pd.DataFrame(f)\n    res.set_index(['single_target'], inplace=True)\n    grouped = res.groupby(level=['single_target'])\n    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}\n    linestyles = {'float32': '--', 'float64': '-'}\n    alpha = {'float64': 0.5, 'float32': 1}\n    for (idx, group) in grouped:\n        single_target = idx\n        (fig, axes) = plt.subplots(figsize=(12, 4), ncols=4)\n        ax = axes[0]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label='%s - %s' % (solver, dtype), color=colors[solver], alpha=alpha[dtype], marker='.', linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Training objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[1]\n        for (scores, times, solver, dtype) in zip(group['test_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, scores, label=solver, color=colors[solver], linestyle=linestyles[dtype], marker='.', alpha=alpha[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test objective (relative to min)')\n        ax.set_yscale('log')\n        ax = axes[2]\n        for (accuracy, times, solver, dtype) in zip(group['accuracies'], group['times'], group['solver'], group['dtype']):\n            ax.plot(times, accuracy, label='%s - %s' % (solver, dtype), alpha=alpha[dtype], marker='.', color=colors[solver], linestyle=linestyles[dtype])\n            ax.axvline(times[-1], color=colors[solver], alpha=alpha[dtype], linestyle=linestyles[dtype])\n        ax.set_xlabel('Time (s)')\n        ax.set_ylabel('Test accuracy')\n        ax.legend()\n        name = 'single_target' if single_target else 'multi_target'\n        name += '_%s' % penalty\n        plt.suptitle(name)\n        if outname is None:\n            outname = name + '.png'\n        fig.tight_layout()\n        fig.subplots_adjust(top=0.9)\n        ax = axes[3]\n        for (scores, times, solver, dtype) in zip(group['train_scores'], group['times'], group['solver'], group['dtype']):\n            ax.plot(np.arange(len(scores)), scores, label='%s - %s' % (solver, dtype), marker='.', alpha=alpha[dtype], color=colors[solver], linestyle=linestyles[dtype])\n        ax.set_yscale('log')\n        ax.set_xlabel('# iterations')\n        ax.set_ylabel('Objective function')\n        ax.legend()\n        plt.savefig(outname)"
        ]
    }
]