[
    {
        "func_name": "parse_search_arg",
        "original": "def parse_search_arg(search):\n    groups = search.split()\n    entries = dict((g.split('=') for g in groups))\n    entry_names = list(entries.keys())\n    sets = [[f'--{k} {v}' for v in vs.split(':')] for (k, vs) in entries.items()]\n    matrix = [list(x) for x in itertools.product(*sets)]\n    return (matrix, entry_names)",
        "mutated": [
            "def parse_search_arg(search):\n    if False:\n        i = 10\n    groups = search.split()\n    entries = dict((g.split('=') for g in groups))\n    entry_names = list(entries.keys())\n    sets = [[f'--{k} {v}' for v in vs.split(':')] for (k, vs) in entries.items()]\n    matrix = [list(x) for x in itertools.product(*sets)]\n    return (matrix, entry_names)",
            "def parse_search_arg(search):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    groups = search.split()\n    entries = dict((g.split('=') for g in groups))\n    entry_names = list(entries.keys())\n    sets = [[f'--{k} {v}' for v in vs.split(':')] for (k, vs) in entries.items()]\n    matrix = [list(x) for x in itertools.product(*sets)]\n    return (matrix, entry_names)",
            "def parse_search_arg(search):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    groups = search.split()\n    entries = dict((g.split('=') for g in groups))\n    entry_names = list(entries.keys())\n    sets = [[f'--{k} {v}' for v in vs.split(':')] for (k, vs) in entries.items()]\n    matrix = [list(x) for x in itertools.product(*sets)]\n    return (matrix, entry_names)",
            "def parse_search_arg(search):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    groups = search.split()\n    entries = dict((g.split('=') for g in groups))\n    entry_names = list(entries.keys())\n    sets = [[f'--{k} {v}' for v in vs.split(':')] for (k, vs) in entries.items()]\n    matrix = [list(x) for x in itertools.product(*sets)]\n    return (matrix, entry_names)",
            "def parse_search_arg(search):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    groups = search.split()\n    entries = dict((g.split('=') for g in groups))\n    entry_names = list(entries.keys())\n    sets = [[f'--{k} {v}' for v in vs.split(':')] for (k, vs) in entries.items()]\n    matrix = [list(x) for x in itertools.product(*sets)]\n    return (matrix, entry_names)"
        ]
    },
    {
        "func_name": "run_search",
        "original": "def run_search():\n    \"\"\"\n     Run parametric search over the desired hparam space with help of ``run_eval.py``.\n\n     All the arguments except ``--search`` are passed to ``run_eval.py`` as is. The values inside of \"--search\" are parsed, reformatted and fed to ``run_eval.py`` as additional args.\n\n    The format for the ``--search`` value is a simple string with hparams and colon separated values to try, e.g.:\n    ```\n     --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\n    ```\n    which will generate ``12`` ``(2*3*2)`` searches for a product of each hparam. For example the example that was just used will invoke ``run_eval.py`` repeatedly with:\n\n    ```\n     --num_beams 5 --length_penalty 0.8 --early_stopping true\n     --num_beams 5 --length_penalty 0.8 --early_stopping false\n     [...]\n     --num_beams 10 --length_penalty 1.2 --early_stopping false\n    ```\n\n    On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.\n\n\n    \"\"\"\n    prog = sys.argv[0]\n    parser = argparse.ArgumentParser(usage='\\n\\nImportant: this script accepts all arguments `run_eval.py` accepts and then a few extra, therefore refer to `run_eval.py -h` for the complete list.')\n    parser.add_argument('--search', type=str, required=False, help='param space to search, e.g. \"num_beams=5:10 length_penalty=0.8:1.0:1.2\"')\n    parser.add_argument('--bs', type=int, default=8, required=False, help=\"initial batch size (may get reduced if it's too big)\")\n    parser.add_argument('--task', type=str, help='used for task_specific_params + metrics')\n    parser.add_argument('--info', nargs='?', type=str, const=datetime_now(), help='add custom notes to be printed before the results table. If no value is passed, the current datetime string will be used.')\n    (args, args_main) = parser.parse_known_args()\n    args_main.extend(['--task', args.task])\n    args_normal = [prog] + args_main\n    task = 'translation' if 'translation' in args.task else 'summarization'\n    (matrix, col_names) = parse_search_arg(args.search)\n    col_names[0:0] = task_score_names[task]\n    col_widths = {col: len(str(col)) for col in col_names}\n    results = []\n    for r in matrix:\n        hparams = dict((x.replace('--', '').split() for x in r))\n        args_exp = ' '.join(r).split()\n        args_exp.extend(['--bs', str(args.bs)])\n        sys.argv = args_normal + args_exp\n        scores = run_generate(verbose=False)\n        result = OrderedDict()\n        for score in task_score_names[task]:\n            result[score] = scores[score]\n        result.update(hparams)\n        results.append(result)\n        for (k, v) in result.items():\n            l = len(str(v))\n            if l > col_widths[k]:\n                col_widths[k] = l\n    results_sorted = sorted(results, key=operator.itemgetter(*task_score_names[task]), reverse=True)\n    print(' | '.join([f'{col:{col_widths[col]}}' for col in col_names]))\n    print(' | '.join([f\"{'-' * col_widths[col]}\" for col in col_names]))\n    for row in results_sorted:\n        print(' | '.join([f'{row[col]:{col_widths[col]}}' for col in col_names]))\n    best = results_sorted[0]\n    for score in task_score_names[task]:\n        del best[score]\n    best_args = [f'--{k} {v}' for (k, v) in best.items()]\n    dyn_args = ['--bs', str(args.bs)]\n    if args.info:\n        print(f'\\nInfo: {args.info}')\n    print('\\nBest score args:')\n    print(' '.join(args_main + best_args + dyn_args))\n    return results_sorted",
        "mutated": [
            "def run_search():\n    if False:\n        i = 10\n    '\\n     Run parametric search over the desired hparam space with help of ``run_eval.py``.\\n\\n     All the arguments except ``--search`` are passed to ``run_eval.py`` as is. The values inside of \"--search\" are parsed, reformatted and fed to ``run_eval.py`` as additional args.\\n\\n    The format for the ``--search`` value is a simple string with hparams and colon separated values to try, e.g.:\\n    ```\\n     --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\\n    ```\\n    which will generate ``12`` ``(2*3*2)`` searches for a product of each hparam. For example the example that was just used will invoke ``run_eval.py`` repeatedly with:\\n\\n    ```\\n     --num_beams 5 --length_penalty 0.8 --early_stopping true\\n     --num_beams 5 --length_penalty 0.8 --early_stopping false\\n     [...]\\n     --num_beams 10 --length_penalty 1.2 --early_stopping false\\n    ```\\n\\n    On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.\\n\\n\\n    '\n    prog = sys.argv[0]\n    parser = argparse.ArgumentParser(usage='\\n\\nImportant: this script accepts all arguments `run_eval.py` accepts and then a few extra, therefore refer to `run_eval.py -h` for the complete list.')\n    parser.add_argument('--search', type=str, required=False, help='param space to search, e.g. \"num_beams=5:10 length_penalty=0.8:1.0:1.2\"')\n    parser.add_argument('--bs', type=int, default=8, required=False, help=\"initial batch size (may get reduced if it's too big)\")\n    parser.add_argument('--task', type=str, help='used for task_specific_params + metrics')\n    parser.add_argument('--info', nargs='?', type=str, const=datetime_now(), help='add custom notes to be printed before the results table. If no value is passed, the current datetime string will be used.')\n    (args, args_main) = parser.parse_known_args()\n    args_main.extend(['--task', args.task])\n    args_normal = [prog] + args_main\n    task = 'translation' if 'translation' in args.task else 'summarization'\n    (matrix, col_names) = parse_search_arg(args.search)\n    col_names[0:0] = task_score_names[task]\n    col_widths = {col: len(str(col)) for col in col_names}\n    results = []\n    for r in matrix:\n        hparams = dict((x.replace('--', '').split() for x in r))\n        args_exp = ' '.join(r).split()\n        args_exp.extend(['--bs', str(args.bs)])\n        sys.argv = args_normal + args_exp\n        scores = run_generate(verbose=False)\n        result = OrderedDict()\n        for score in task_score_names[task]:\n            result[score] = scores[score]\n        result.update(hparams)\n        results.append(result)\n        for (k, v) in result.items():\n            l = len(str(v))\n            if l > col_widths[k]:\n                col_widths[k] = l\n    results_sorted = sorted(results, key=operator.itemgetter(*task_score_names[task]), reverse=True)\n    print(' | '.join([f'{col:{col_widths[col]}}' for col in col_names]))\n    print(' | '.join([f\"{'-' * col_widths[col]}\" for col in col_names]))\n    for row in results_sorted:\n        print(' | '.join([f'{row[col]:{col_widths[col]}}' for col in col_names]))\n    best = results_sorted[0]\n    for score in task_score_names[task]:\n        del best[score]\n    best_args = [f'--{k} {v}' for (k, v) in best.items()]\n    dyn_args = ['--bs', str(args.bs)]\n    if args.info:\n        print(f'\\nInfo: {args.info}')\n    print('\\nBest score args:')\n    print(' '.join(args_main + best_args + dyn_args))\n    return results_sorted",
            "def run_search():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n     Run parametric search over the desired hparam space with help of ``run_eval.py``.\\n\\n     All the arguments except ``--search`` are passed to ``run_eval.py`` as is. The values inside of \"--search\" are parsed, reformatted and fed to ``run_eval.py`` as additional args.\\n\\n    The format for the ``--search`` value is a simple string with hparams and colon separated values to try, e.g.:\\n    ```\\n     --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\\n    ```\\n    which will generate ``12`` ``(2*3*2)`` searches for a product of each hparam. For example the example that was just used will invoke ``run_eval.py`` repeatedly with:\\n\\n    ```\\n     --num_beams 5 --length_penalty 0.8 --early_stopping true\\n     --num_beams 5 --length_penalty 0.8 --early_stopping false\\n     [...]\\n     --num_beams 10 --length_penalty 1.2 --early_stopping false\\n    ```\\n\\n    On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.\\n\\n\\n    '\n    prog = sys.argv[0]\n    parser = argparse.ArgumentParser(usage='\\n\\nImportant: this script accepts all arguments `run_eval.py` accepts and then a few extra, therefore refer to `run_eval.py -h` for the complete list.')\n    parser.add_argument('--search', type=str, required=False, help='param space to search, e.g. \"num_beams=5:10 length_penalty=0.8:1.0:1.2\"')\n    parser.add_argument('--bs', type=int, default=8, required=False, help=\"initial batch size (may get reduced if it's too big)\")\n    parser.add_argument('--task', type=str, help='used for task_specific_params + metrics')\n    parser.add_argument('--info', nargs='?', type=str, const=datetime_now(), help='add custom notes to be printed before the results table. If no value is passed, the current datetime string will be used.')\n    (args, args_main) = parser.parse_known_args()\n    args_main.extend(['--task', args.task])\n    args_normal = [prog] + args_main\n    task = 'translation' if 'translation' in args.task else 'summarization'\n    (matrix, col_names) = parse_search_arg(args.search)\n    col_names[0:0] = task_score_names[task]\n    col_widths = {col: len(str(col)) for col in col_names}\n    results = []\n    for r in matrix:\n        hparams = dict((x.replace('--', '').split() for x in r))\n        args_exp = ' '.join(r).split()\n        args_exp.extend(['--bs', str(args.bs)])\n        sys.argv = args_normal + args_exp\n        scores = run_generate(verbose=False)\n        result = OrderedDict()\n        for score in task_score_names[task]:\n            result[score] = scores[score]\n        result.update(hparams)\n        results.append(result)\n        for (k, v) in result.items():\n            l = len(str(v))\n            if l > col_widths[k]:\n                col_widths[k] = l\n    results_sorted = sorted(results, key=operator.itemgetter(*task_score_names[task]), reverse=True)\n    print(' | '.join([f'{col:{col_widths[col]}}' for col in col_names]))\n    print(' | '.join([f\"{'-' * col_widths[col]}\" for col in col_names]))\n    for row in results_sorted:\n        print(' | '.join([f'{row[col]:{col_widths[col]}}' for col in col_names]))\n    best = results_sorted[0]\n    for score in task_score_names[task]:\n        del best[score]\n    best_args = [f'--{k} {v}' for (k, v) in best.items()]\n    dyn_args = ['--bs', str(args.bs)]\n    if args.info:\n        print(f'\\nInfo: {args.info}')\n    print('\\nBest score args:')\n    print(' '.join(args_main + best_args + dyn_args))\n    return results_sorted",
            "def run_search():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n     Run parametric search over the desired hparam space with help of ``run_eval.py``.\\n\\n     All the arguments except ``--search`` are passed to ``run_eval.py`` as is. The values inside of \"--search\" are parsed, reformatted and fed to ``run_eval.py`` as additional args.\\n\\n    The format for the ``--search`` value is a simple string with hparams and colon separated values to try, e.g.:\\n    ```\\n     --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\\n    ```\\n    which will generate ``12`` ``(2*3*2)`` searches for a product of each hparam. For example the example that was just used will invoke ``run_eval.py`` repeatedly with:\\n\\n    ```\\n     --num_beams 5 --length_penalty 0.8 --early_stopping true\\n     --num_beams 5 --length_penalty 0.8 --early_stopping false\\n     [...]\\n     --num_beams 10 --length_penalty 1.2 --early_stopping false\\n    ```\\n\\n    On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.\\n\\n\\n    '\n    prog = sys.argv[0]\n    parser = argparse.ArgumentParser(usage='\\n\\nImportant: this script accepts all arguments `run_eval.py` accepts and then a few extra, therefore refer to `run_eval.py -h` for the complete list.')\n    parser.add_argument('--search', type=str, required=False, help='param space to search, e.g. \"num_beams=5:10 length_penalty=0.8:1.0:1.2\"')\n    parser.add_argument('--bs', type=int, default=8, required=False, help=\"initial batch size (may get reduced if it's too big)\")\n    parser.add_argument('--task', type=str, help='used for task_specific_params + metrics')\n    parser.add_argument('--info', nargs='?', type=str, const=datetime_now(), help='add custom notes to be printed before the results table. If no value is passed, the current datetime string will be used.')\n    (args, args_main) = parser.parse_known_args()\n    args_main.extend(['--task', args.task])\n    args_normal = [prog] + args_main\n    task = 'translation' if 'translation' in args.task else 'summarization'\n    (matrix, col_names) = parse_search_arg(args.search)\n    col_names[0:0] = task_score_names[task]\n    col_widths = {col: len(str(col)) for col in col_names}\n    results = []\n    for r in matrix:\n        hparams = dict((x.replace('--', '').split() for x in r))\n        args_exp = ' '.join(r).split()\n        args_exp.extend(['--bs', str(args.bs)])\n        sys.argv = args_normal + args_exp\n        scores = run_generate(verbose=False)\n        result = OrderedDict()\n        for score in task_score_names[task]:\n            result[score] = scores[score]\n        result.update(hparams)\n        results.append(result)\n        for (k, v) in result.items():\n            l = len(str(v))\n            if l > col_widths[k]:\n                col_widths[k] = l\n    results_sorted = sorted(results, key=operator.itemgetter(*task_score_names[task]), reverse=True)\n    print(' | '.join([f'{col:{col_widths[col]}}' for col in col_names]))\n    print(' | '.join([f\"{'-' * col_widths[col]}\" for col in col_names]))\n    for row in results_sorted:\n        print(' | '.join([f'{row[col]:{col_widths[col]}}' for col in col_names]))\n    best = results_sorted[0]\n    for score in task_score_names[task]:\n        del best[score]\n    best_args = [f'--{k} {v}' for (k, v) in best.items()]\n    dyn_args = ['--bs', str(args.bs)]\n    if args.info:\n        print(f'\\nInfo: {args.info}')\n    print('\\nBest score args:')\n    print(' '.join(args_main + best_args + dyn_args))\n    return results_sorted",
            "def run_search():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n     Run parametric search over the desired hparam space with help of ``run_eval.py``.\\n\\n     All the arguments except ``--search`` are passed to ``run_eval.py`` as is. The values inside of \"--search\" are parsed, reformatted and fed to ``run_eval.py`` as additional args.\\n\\n    The format for the ``--search`` value is a simple string with hparams and colon separated values to try, e.g.:\\n    ```\\n     --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\\n    ```\\n    which will generate ``12`` ``(2*3*2)`` searches for a product of each hparam. For example the example that was just used will invoke ``run_eval.py`` repeatedly with:\\n\\n    ```\\n     --num_beams 5 --length_penalty 0.8 --early_stopping true\\n     --num_beams 5 --length_penalty 0.8 --early_stopping false\\n     [...]\\n     --num_beams 10 --length_penalty 1.2 --early_stopping false\\n    ```\\n\\n    On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.\\n\\n\\n    '\n    prog = sys.argv[0]\n    parser = argparse.ArgumentParser(usage='\\n\\nImportant: this script accepts all arguments `run_eval.py` accepts and then a few extra, therefore refer to `run_eval.py -h` for the complete list.')\n    parser.add_argument('--search', type=str, required=False, help='param space to search, e.g. \"num_beams=5:10 length_penalty=0.8:1.0:1.2\"')\n    parser.add_argument('--bs', type=int, default=8, required=False, help=\"initial batch size (may get reduced if it's too big)\")\n    parser.add_argument('--task', type=str, help='used for task_specific_params + metrics')\n    parser.add_argument('--info', nargs='?', type=str, const=datetime_now(), help='add custom notes to be printed before the results table. If no value is passed, the current datetime string will be used.')\n    (args, args_main) = parser.parse_known_args()\n    args_main.extend(['--task', args.task])\n    args_normal = [prog] + args_main\n    task = 'translation' if 'translation' in args.task else 'summarization'\n    (matrix, col_names) = parse_search_arg(args.search)\n    col_names[0:0] = task_score_names[task]\n    col_widths = {col: len(str(col)) for col in col_names}\n    results = []\n    for r in matrix:\n        hparams = dict((x.replace('--', '').split() for x in r))\n        args_exp = ' '.join(r).split()\n        args_exp.extend(['--bs', str(args.bs)])\n        sys.argv = args_normal + args_exp\n        scores = run_generate(verbose=False)\n        result = OrderedDict()\n        for score in task_score_names[task]:\n            result[score] = scores[score]\n        result.update(hparams)\n        results.append(result)\n        for (k, v) in result.items():\n            l = len(str(v))\n            if l > col_widths[k]:\n                col_widths[k] = l\n    results_sorted = sorted(results, key=operator.itemgetter(*task_score_names[task]), reverse=True)\n    print(' | '.join([f'{col:{col_widths[col]}}' for col in col_names]))\n    print(' | '.join([f\"{'-' * col_widths[col]}\" for col in col_names]))\n    for row in results_sorted:\n        print(' | '.join([f'{row[col]:{col_widths[col]}}' for col in col_names]))\n    best = results_sorted[0]\n    for score in task_score_names[task]:\n        del best[score]\n    best_args = [f'--{k} {v}' for (k, v) in best.items()]\n    dyn_args = ['--bs', str(args.bs)]\n    if args.info:\n        print(f'\\nInfo: {args.info}')\n    print('\\nBest score args:')\n    print(' '.join(args_main + best_args + dyn_args))\n    return results_sorted",
            "def run_search():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n     Run parametric search over the desired hparam space with help of ``run_eval.py``.\\n\\n     All the arguments except ``--search`` are passed to ``run_eval.py`` as is. The values inside of \"--search\" are parsed, reformatted and fed to ``run_eval.py`` as additional args.\\n\\n    The format for the ``--search`` value is a simple string with hparams and colon separated values to try, e.g.:\\n    ```\\n     --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\\n    ```\\n    which will generate ``12`` ``(2*3*2)`` searches for a product of each hparam. For example the example that was just used will invoke ``run_eval.py`` repeatedly with:\\n\\n    ```\\n     --num_beams 5 --length_penalty 0.8 --early_stopping true\\n     --num_beams 5 --length_penalty 0.8 --early_stopping false\\n     [...]\\n     --num_beams 10 --length_penalty 1.2 --early_stopping false\\n    ```\\n\\n    On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.\\n\\n\\n    '\n    prog = sys.argv[0]\n    parser = argparse.ArgumentParser(usage='\\n\\nImportant: this script accepts all arguments `run_eval.py` accepts and then a few extra, therefore refer to `run_eval.py -h` for the complete list.')\n    parser.add_argument('--search', type=str, required=False, help='param space to search, e.g. \"num_beams=5:10 length_penalty=0.8:1.0:1.2\"')\n    parser.add_argument('--bs', type=int, default=8, required=False, help=\"initial batch size (may get reduced if it's too big)\")\n    parser.add_argument('--task', type=str, help='used for task_specific_params + metrics')\n    parser.add_argument('--info', nargs='?', type=str, const=datetime_now(), help='add custom notes to be printed before the results table. If no value is passed, the current datetime string will be used.')\n    (args, args_main) = parser.parse_known_args()\n    args_main.extend(['--task', args.task])\n    args_normal = [prog] + args_main\n    task = 'translation' if 'translation' in args.task else 'summarization'\n    (matrix, col_names) = parse_search_arg(args.search)\n    col_names[0:0] = task_score_names[task]\n    col_widths = {col: len(str(col)) for col in col_names}\n    results = []\n    for r in matrix:\n        hparams = dict((x.replace('--', '').split() for x in r))\n        args_exp = ' '.join(r).split()\n        args_exp.extend(['--bs', str(args.bs)])\n        sys.argv = args_normal + args_exp\n        scores = run_generate(verbose=False)\n        result = OrderedDict()\n        for score in task_score_names[task]:\n            result[score] = scores[score]\n        result.update(hparams)\n        results.append(result)\n        for (k, v) in result.items():\n            l = len(str(v))\n            if l > col_widths[k]:\n                col_widths[k] = l\n    results_sorted = sorted(results, key=operator.itemgetter(*task_score_names[task]), reverse=True)\n    print(' | '.join([f'{col:{col_widths[col]}}' for col in col_names]))\n    print(' | '.join([f\"{'-' * col_widths[col]}\" for col in col_names]))\n    for row in results_sorted:\n        print(' | '.join([f'{row[col]:{col_widths[col]}}' for col in col_names]))\n    best = results_sorted[0]\n    for score in task_score_names[task]:\n        del best[score]\n    best_args = [f'--{k} {v}' for (k, v) in best.items()]\n    dyn_args = ['--bs', str(args.bs)]\n    if args.info:\n        print(f'\\nInfo: {args.info}')\n    print('\\nBest score args:')\n    print(' '.join(args_main + best_args + dyn_args))\n    return results_sorted"
        ]
    }
]