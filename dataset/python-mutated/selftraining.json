[
    {
        "func_name": "create_pseudo_labeled_data",
        "original": "def create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir):\n    \"\"\"Create pseudeo labeled data for the next self-training iteration.\"\"\"\n    dataset = datasets.concatenate_datasets([infer_input, infer_output], axis=1)\n    if args.do_filter_by_confidence:\n        dataset = dataset.filter(lambda example: example['probability'] > args.confidence_threshold)\n    if args.do_filter_by_val_performance:\n        assert eval_result >= 0.0 and eval_result <= 1.0\n        num_selected_rows = int(eval_result * len(dataset))\n        print(num_selected_rows)\n        dataset = dataset.sort('probability', reverse=True)\n        dataset = dataset.select(range(num_selected_rows))\n    dataset = dataset.remove_columns(['label', 'probability'])\n    dataset = dataset.rename_column('prediction', 'label')\n    dataset = dataset.map(lambda example: {'label': id2label[example['label']]})\n    dataset = dataset.shuffle(seed=args.seed)\n    pseudo_labeled_data_file = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n    if args.data_file_extension == 'csv':\n        dataset.to_csv(pseudo_labeled_data_file, index=False)\n    else:\n        dataset.to_json(pseudo_labeled_data_file)",
        "mutated": [
            "def create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir):\n    if False:\n        i = 10\n    'Create pseudeo labeled data for the next self-training iteration.'\n    dataset = datasets.concatenate_datasets([infer_input, infer_output], axis=1)\n    if args.do_filter_by_confidence:\n        dataset = dataset.filter(lambda example: example['probability'] > args.confidence_threshold)\n    if args.do_filter_by_val_performance:\n        assert eval_result >= 0.0 and eval_result <= 1.0\n        num_selected_rows = int(eval_result * len(dataset))\n        print(num_selected_rows)\n        dataset = dataset.sort('probability', reverse=True)\n        dataset = dataset.select(range(num_selected_rows))\n    dataset = dataset.remove_columns(['label', 'probability'])\n    dataset = dataset.rename_column('prediction', 'label')\n    dataset = dataset.map(lambda example: {'label': id2label[example['label']]})\n    dataset = dataset.shuffle(seed=args.seed)\n    pseudo_labeled_data_file = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n    if args.data_file_extension == 'csv':\n        dataset.to_csv(pseudo_labeled_data_file, index=False)\n    else:\n        dataset.to_json(pseudo_labeled_data_file)",
            "def create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create pseudeo labeled data for the next self-training iteration.'\n    dataset = datasets.concatenate_datasets([infer_input, infer_output], axis=1)\n    if args.do_filter_by_confidence:\n        dataset = dataset.filter(lambda example: example['probability'] > args.confidence_threshold)\n    if args.do_filter_by_val_performance:\n        assert eval_result >= 0.0 and eval_result <= 1.0\n        num_selected_rows = int(eval_result * len(dataset))\n        print(num_selected_rows)\n        dataset = dataset.sort('probability', reverse=True)\n        dataset = dataset.select(range(num_selected_rows))\n    dataset = dataset.remove_columns(['label', 'probability'])\n    dataset = dataset.rename_column('prediction', 'label')\n    dataset = dataset.map(lambda example: {'label': id2label[example['label']]})\n    dataset = dataset.shuffle(seed=args.seed)\n    pseudo_labeled_data_file = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n    if args.data_file_extension == 'csv':\n        dataset.to_csv(pseudo_labeled_data_file, index=False)\n    else:\n        dataset.to_json(pseudo_labeled_data_file)",
            "def create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create pseudeo labeled data for the next self-training iteration.'\n    dataset = datasets.concatenate_datasets([infer_input, infer_output], axis=1)\n    if args.do_filter_by_confidence:\n        dataset = dataset.filter(lambda example: example['probability'] > args.confidence_threshold)\n    if args.do_filter_by_val_performance:\n        assert eval_result >= 0.0 and eval_result <= 1.0\n        num_selected_rows = int(eval_result * len(dataset))\n        print(num_selected_rows)\n        dataset = dataset.sort('probability', reverse=True)\n        dataset = dataset.select(range(num_selected_rows))\n    dataset = dataset.remove_columns(['label', 'probability'])\n    dataset = dataset.rename_column('prediction', 'label')\n    dataset = dataset.map(lambda example: {'label': id2label[example['label']]})\n    dataset = dataset.shuffle(seed=args.seed)\n    pseudo_labeled_data_file = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n    if args.data_file_extension == 'csv':\n        dataset.to_csv(pseudo_labeled_data_file, index=False)\n    else:\n        dataset.to_json(pseudo_labeled_data_file)",
            "def create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create pseudeo labeled data for the next self-training iteration.'\n    dataset = datasets.concatenate_datasets([infer_input, infer_output], axis=1)\n    if args.do_filter_by_confidence:\n        dataset = dataset.filter(lambda example: example['probability'] > args.confidence_threshold)\n    if args.do_filter_by_val_performance:\n        assert eval_result >= 0.0 and eval_result <= 1.0\n        num_selected_rows = int(eval_result * len(dataset))\n        print(num_selected_rows)\n        dataset = dataset.sort('probability', reverse=True)\n        dataset = dataset.select(range(num_selected_rows))\n    dataset = dataset.remove_columns(['label', 'probability'])\n    dataset = dataset.rename_column('prediction', 'label')\n    dataset = dataset.map(lambda example: {'label': id2label[example['label']]})\n    dataset = dataset.shuffle(seed=args.seed)\n    pseudo_labeled_data_file = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n    if args.data_file_extension == 'csv':\n        dataset.to_csv(pseudo_labeled_data_file, index=False)\n    else:\n        dataset.to_json(pseudo_labeled_data_file)",
            "def create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create pseudeo labeled data for the next self-training iteration.'\n    dataset = datasets.concatenate_datasets([infer_input, infer_output], axis=1)\n    if args.do_filter_by_confidence:\n        dataset = dataset.filter(lambda example: example['probability'] > args.confidence_threshold)\n    if args.do_filter_by_val_performance:\n        assert eval_result >= 0.0 and eval_result <= 1.0\n        num_selected_rows = int(eval_result * len(dataset))\n        print(num_selected_rows)\n        dataset = dataset.sort('probability', reverse=True)\n        dataset = dataset.select(range(num_selected_rows))\n    dataset = dataset.remove_columns(['label', 'probability'])\n    dataset = dataset.rename_column('prediction', 'label')\n    dataset = dataset.map(lambda example: {'label': id2label[example['label']]})\n    dataset = dataset.shuffle(seed=args.seed)\n    pseudo_labeled_data_file = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n    if args.data_file_extension == 'csv':\n        dataset.to_csv(pseudo_labeled_data_file, index=False)\n    else:\n        dataset.to_json(pseudo_labeled_data_file)"
        ]
    },
    {
        "func_name": "selftrain",
        "original": "def selftrain(model_name_or_path, train_file, infer_file, output_dir, **kwargs):\n    \"\"\"Self-training a pre-trained model on a downstream task.\n\n    Args:\n      model_name_or_path: Path to pretrained model or model identifier from\n        huggingface.co/models.\n      train_file: A csv or a json file containing the training data.\n      infer_file: A csv or a json file containing the data to predict on.\n      output_dir: The output directory where the model predictions and checkpoints\n        will be written.\n      **kwargs: Dictionary of key/value pairs with which to update the\n        configuration object after loading. The values in kwargs of any keys which\n        are configuration attributes will be used to override the loaded values.\n    \"\"\"\n    accelerator = Accelerator()\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    model_args = STModelArguments(model_name_or_path=model_name_or_path)\n    data_args = STDataArguments(train_file=train_file, infer_file=infer_file)\n    training_args = STTrainingArguments(output_dir=output_dir)\n    args = argparse.Namespace()\n    for arg_class in (model_args, data_args, training_args):\n        for (key, value) in vars(arg_class).items():\n            setattr(args, key, value)\n    for (key, value) in kwargs.items():\n        if hasattr(args, key):\n            setattr(args, key, value)\n    data_files = {}\n    args.data_file_extension = None\n    assert args.train_file is not None\n    assert args.infer_file is not None\n    data_files['train'] = args.train_file\n    data_files['infer'] = args.infer_file\n    if args.evaluation_strategy != IntervalStrategy.NO.value:\n        assert args.eval_file is not None\n        data_files['eval'] = args.eval_file\n    for key in data_files:\n        extension = data_files[key].split('.')[-1]\n        assert extension in ['csv', 'json'], f'`{key}_file` should be a csv or a json file.'\n        if args.data_file_extension is None:\n            args.data_file_extension = extension\n        else:\n            assert extension == args.data_file_extension, f'`{key}_file` should be a {args.data_file_extension} file`.'\n    assert args.eval_metric in datasets.list_metrics(), f'{args.eval_metric} not in the list of supported metrics {datasets.list_metrics()}.'\n    if args.seed is not None:\n        set_seed(args.seed)\n    logger.info('Creating the initial data directory for self-training...')\n    data_dir_format = f'{args.output_dir}/self-train_iter-{{}}'.format\n    initial_data_dir = data_dir_format(0)\n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n            os.makedirs(initial_data_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    best_iteration = None\n    best_eval_result = None\n    early_stopping_patience_counter = 0\n    should_training_stop = False\n    progress_bar = tqdm(range(args.max_selftrain_iterations), disable=not accelerator.is_local_main_process)\n    for iteration in range(0, int(args.max_selftrain_iterations)):\n        current_data_dir = data_dir_format(iteration)\n        assert os.path.exists(current_data_dir)\n        current_output_dir = os.path.join(current_data_dir, 'stage-1')\n        arguments_dict = {'accelerator': accelerator, 'model_name_or_path': args.model_name_or_path, 'cache_dir': args.cache_dir, 'do_train': True, 'train_file': data_files['train'] if iteration == 0 else data_files['train_pseudo'], 'do_eval': True if args.eval_file is not None else False, 'eval_file': data_files['eval'], 'do_predict': True, 'infer_file': data_files['infer'], 'task_name': args.task_name, 'label_list': args.label_list, 'output_dir': current_output_dir, 'eval_metric': args.eval_metric, 'evaluation_strategy': args.evaluation_strategy, 'early_stopping_patience': args.early_stopping_patience, 'early_stopping_threshold': args.early_stopping_threshold, 'seed': args.seed}\n        for (key, value) in kwargs.items():\n            if key not in arguments_dict and (not hasattr(training_args, key)):\n                arguments_dict.update({key: value})\n        model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n        if os.path.exists(model_bin_file_path):\n            logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 1.', model_bin_file_path, iteration)\n        else:\n            logger.info('***** Running self-training: iteration: %d, stage: 1 *****', iteration)\n            finetune(**arguments_dict)\n            accelerator.wait_for_everyone()\n            assert os.path.exists(model_bin_file_path)\n            logger.info('Self-training job completed: iteration: %d, stage: 1.', iteration)\n        if iteration > 0 and args.finetune_on_labeled_data:\n            model_path = os.path.join(current_output_dir, 'best-checkpoint')\n            current_output_dir = os.path.join(current_data_dir, 'stage-2')\n            arguments_dict['model_name_or_path'] = model_path\n            arguments_dict['train_file'] = data_files['train']\n            arguments_dict['output_dir'] = current_output_dir\n            model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n            if os.path.exists(model_bin_file_path):\n                logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 2.', model_bin_file_path, iteration)\n            else:\n                logger.info('***** Running self-training: iteration: %d, stage: 2 *****', iteration)\n                finetune(**arguments_dict)\n                accelerator.wait_for_everyone()\n                assert os.path.exists(model_bin_file_path)\n                logger.info('Self-training job completed: iteration: %d, stage: 2.', iteration)\n        new_iteration = iteration\n        next_data_dir = data_dir_format(iteration + 1)\n        config = AutoConfig.from_pretrained(os.path.join(current_output_dir, 'best-checkpoint'))\n        id2label = config.id2label\n        eval_results_file = os.path.join(current_output_dir, 'eval_results_best-checkpoint.json')\n        test_results_file = os.path.join(current_output_dir, 'test_results_best-checkpoint.json')\n        assert os.path.exists(eval_results_file)\n        with open(eval_results_file, 'r') as f:\n            eval_result = float(json.load(f)[args.eval_metric])\n        infer_output_file = os.path.join(current_output_dir, 'infer_output_best-checkpoint.csv')\n        assert os.path.exists(infer_output_file)\n        infer_input = load_dataset(args.data_file_extension, data_files={'data': data_files['infer']})['data']\n        infer_output = load_dataset('csv', data_files={'data': infer_output_file})['data']\n        if accelerator.is_main_process:\n            os.makedirs(next_data_dir, exist_ok=True)\n            shutil.copy(eval_results_file, os.path.join(output_dir, f'eval_results_iter-{iteration}.json'))\n            if os.path.exists(test_results_file):\n                shutil.copy(eval_results_file, os.path.join(output_dir, f'test_results_iter-{iteration}.json'))\n            create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir)\n        accelerator.wait_for_everyone()\n        data_files['train_pseudo'] = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n        if args.evaluation_strategy != IntervalStrategy.NO.value:\n            new_eval_result = eval_result\n            if best_iteration is None:\n                best_iteration = new_iteration\n                best_eval_result = new_eval_result\n            else:\n                if new_eval_result - best_eval_result > args.early_stopping_threshold:\n                    best_iteration = new_iteration\n                    best_eval_result = new_eval_result\n                    early_stopping_patience_counter = 0\n                else:\n                    if new_eval_result == best_eval_result:\n                        best_iteration = new_iteration\n                        best_eval_result = new_eval_result\n                    early_stopping_patience_counter += 1\n                if early_stopping_patience_counter >= args.early_stopping_patience:\n                    should_training_stop = True\n        progress_bar.update(1)\n        if should_training_stop:\n            break\n    if best_iteration is not None:\n        logger.info('Best iteration: %d', best_iteration)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, best_eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{iteration}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))\n    else:\n        logger.info('Best iteration: %d', args.max_selftrain_iterations - 1)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{args.max_selftrain_iterations - 1}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))",
        "mutated": [
            "def selftrain(model_name_or_path, train_file, infer_file, output_dir, **kwargs):\n    if False:\n        i = 10\n    'Self-training a pre-trained model on a downstream task.\\n\\n    Args:\\n      model_name_or_path: Path to pretrained model or model identifier from\\n        huggingface.co/models.\\n      train_file: A csv or a json file containing the training data.\\n      infer_file: A csv or a json file containing the data to predict on.\\n      output_dir: The output directory where the model predictions and checkpoints\\n        will be written.\\n      **kwargs: Dictionary of key/value pairs with which to update the\\n        configuration object after loading. The values in kwargs of any keys which\\n        are configuration attributes will be used to override the loaded values.\\n    '\n    accelerator = Accelerator()\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    model_args = STModelArguments(model_name_or_path=model_name_or_path)\n    data_args = STDataArguments(train_file=train_file, infer_file=infer_file)\n    training_args = STTrainingArguments(output_dir=output_dir)\n    args = argparse.Namespace()\n    for arg_class in (model_args, data_args, training_args):\n        for (key, value) in vars(arg_class).items():\n            setattr(args, key, value)\n    for (key, value) in kwargs.items():\n        if hasattr(args, key):\n            setattr(args, key, value)\n    data_files = {}\n    args.data_file_extension = None\n    assert args.train_file is not None\n    assert args.infer_file is not None\n    data_files['train'] = args.train_file\n    data_files['infer'] = args.infer_file\n    if args.evaluation_strategy != IntervalStrategy.NO.value:\n        assert args.eval_file is not None\n        data_files['eval'] = args.eval_file\n    for key in data_files:\n        extension = data_files[key].split('.')[-1]\n        assert extension in ['csv', 'json'], f'`{key}_file` should be a csv or a json file.'\n        if args.data_file_extension is None:\n            args.data_file_extension = extension\n        else:\n            assert extension == args.data_file_extension, f'`{key}_file` should be a {args.data_file_extension} file`.'\n    assert args.eval_metric in datasets.list_metrics(), f'{args.eval_metric} not in the list of supported metrics {datasets.list_metrics()}.'\n    if args.seed is not None:\n        set_seed(args.seed)\n    logger.info('Creating the initial data directory for self-training...')\n    data_dir_format = f'{args.output_dir}/self-train_iter-{{}}'.format\n    initial_data_dir = data_dir_format(0)\n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n            os.makedirs(initial_data_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    best_iteration = None\n    best_eval_result = None\n    early_stopping_patience_counter = 0\n    should_training_stop = False\n    progress_bar = tqdm(range(args.max_selftrain_iterations), disable=not accelerator.is_local_main_process)\n    for iteration in range(0, int(args.max_selftrain_iterations)):\n        current_data_dir = data_dir_format(iteration)\n        assert os.path.exists(current_data_dir)\n        current_output_dir = os.path.join(current_data_dir, 'stage-1')\n        arguments_dict = {'accelerator': accelerator, 'model_name_or_path': args.model_name_or_path, 'cache_dir': args.cache_dir, 'do_train': True, 'train_file': data_files['train'] if iteration == 0 else data_files['train_pseudo'], 'do_eval': True if args.eval_file is not None else False, 'eval_file': data_files['eval'], 'do_predict': True, 'infer_file': data_files['infer'], 'task_name': args.task_name, 'label_list': args.label_list, 'output_dir': current_output_dir, 'eval_metric': args.eval_metric, 'evaluation_strategy': args.evaluation_strategy, 'early_stopping_patience': args.early_stopping_patience, 'early_stopping_threshold': args.early_stopping_threshold, 'seed': args.seed}\n        for (key, value) in kwargs.items():\n            if key not in arguments_dict and (not hasattr(training_args, key)):\n                arguments_dict.update({key: value})\n        model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n        if os.path.exists(model_bin_file_path):\n            logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 1.', model_bin_file_path, iteration)\n        else:\n            logger.info('***** Running self-training: iteration: %d, stage: 1 *****', iteration)\n            finetune(**arguments_dict)\n            accelerator.wait_for_everyone()\n            assert os.path.exists(model_bin_file_path)\n            logger.info('Self-training job completed: iteration: %d, stage: 1.', iteration)\n        if iteration > 0 and args.finetune_on_labeled_data:\n            model_path = os.path.join(current_output_dir, 'best-checkpoint')\n            current_output_dir = os.path.join(current_data_dir, 'stage-2')\n            arguments_dict['model_name_or_path'] = model_path\n            arguments_dict['train_file'] = data_files['train']\n            arguments_dict['output_dir'] = current_output_dir\n            model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n            if os.path.exists(model_bin_file_path):\n                logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 2.', model_bin_file_path, iteration)\n            else:\n                logger.info('***** Running self-training: iteration: %d, stage: 2 *****', iteration)\n                finetune(**arguments_dict)\n                accelerator.wait_for_everyone()\n                assert os.path.exists(model_bin_file_path)\n                logger.info('Self-training job completed: iteration: %d, stage: 2.', iteration)\n        new_iteration = iteration\n        next_data_dir = data_dir_format(iteration + 1)\n        config = AutoConfig.from_pretrained(os.path.join(current_output_dir, 'best-checkpoint'))\n        id2label = config.id2label\n        eval_results_file = os.path.join(current_output_dir, 'eval_results_best-checkpoint.json')\n        test_results_file = os.path.join(current_output_dir, 'test_results_best-checkpoint.json')\n        assert os.path.exists(eval_results_file)\n        with open(eval_results_file, 'r') as f:\n            eval_result = float(json.load(f)[args.eval_metric])\n        infer_output_file = os.path.join(current_output_dir, 'infer_output_best-checkpoint.csv')\n        assert os.path.exists(infer_output_file)\n        infer_input = load_dataset(args.data_file_extension, data_files={'data': data_files['infer']})['data']\n        infer_output = load_dataset('csv', data_files={'data': infer_output_file})['data']\n        if accelerator.is_main_process:\n            os.makedirs(next_data_dir, exist_ok=True)\n            shutil.copy(eval_results_file, os.path.join(output_dir, f'eval_results_iter-{iteration}.json'))\n            if os.path.exists(test_results_file):\n                shutil.copy(eval_results_file, os.path.join(output_dir, f'test_results_iter-{iteration}.json'))\n            create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir)\n        accelerator.wait_for_everyone()\n        data_files['train_pseudo'] = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n        if args.evaluation_strategy != IntervalStrategy.NO.value:\n            new_eval_result = eval_result\n            if best_iteration is None:\n                best_iteration = new_iteration\n                best_eval_result = new_eval_result\n            else:\n                if new_eval_result - best_eval_result > args.early_stopping_threshold:\n                    best_iteration = new_iteration\n                    best_eval_result = new_eval_result\n                    early_stopping_patience_counter = 0\n                else:\n                    if new_eval_result == best_eval_result:\n                        best_iteration = new_iteration\n                        best_eval_result = new_eval_result\n                    early_stopping_patience_counter += 1\n                if early_stopping_patience_counter >= args.early_stopping_patience:\n                    should_training_stop = True\n        progress_bar.update(1)\n        if should_training_stop:\n            break\n    if best_iteration is not None:\n        logger.info('Best iteration: %d', best_iteration)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, best_eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{iteration}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))\n    else:\n        logger.info('Best iteration: %d', args.max_selftrain_iterations - 1)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{args.max_selftrain_iterations - 1}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))",
            "def selftrain(model_name_or_path, train_file, infer_file, output_dir, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Self-training a pre-trained model on a downstream task.\\n\\n    Args:\\n      model_name_or_path: Path to pretrained model or model identifier from\\n        huggingface.co/models.\\n      train_file: A csv or a json file containing the training data.\\n      infer_file: A csv or a json file containing the data to predict on.\\n      output_dir: The output directory where the model predictions and checkpoints\\n        will be written.\\n      **kwargs: Dictionary of key/value pairs with which to update the\\n        configuration object after loading. The values in kwargs of any keys which\\n        are configuration attributes will be used to override the loaded values.\\n    '\n    accelerator = Accelerator()\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    model_args = STModelArguments(model_name_or_path=model_name_or_path)\n    data_args = STDataArguments(train_file=train_file, infer_file=infer_file)\n    training_args = STTrainingArguments(output_dir=output_dir)\n    args = argparse.Namespace()\n    for arg_class in (model_args, data_args, training_args):\n        for (key, value) in vars(arg_class).items():\n            setattr(args, key, value)\n    for (key, value) in kwargs.items():\n        if hasattr(args, key):\n            setattr(args, key, value)\n    data_files = {}\n    args.data_file_extension = None\n    assert args.train_file is not None\n    assert args.infer_file is not None\n    data_files['train'] = args.train_file\n    data_files['infer'] = args.infer_file\n    if args.evaluation_strategy != IntervalStrategy.NO.value:\n        assert args.eval_file is not None\n        data_files['eval'] = args.eval_file\n    for key in data_files:\n        extension = data_files[key].split('.')[-1]\n        assert extension in ['csv', 'json'], f'`{key}_file` should be a csv or a json file.'\n        if args.data_file_extension is None:\n            args.data_file_extension = extension\n        else:\n            assert extension == args.data_file_extension, f'`{key}_file` should be a {args.data_file_extension} file`.'\n    assert args.eval_metric in datasets.list_metrics(), f'{args.eval_metric} not in the list of supported metrics {datasets.list_metrics()}.'\n    if args.seed is not None:\n        set_seed(args.seed)\n    logger.info('Creating the initial data directory for self-training...')\n    data_dir_format = f'{args.output_dir}/self-train_iter-{{}}'.format\n    initial_data_dir = data_dir_format(0)\n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n            os.makedirs(initial_data_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    best_iteration = None\n    best_eval_result = None\n    early_stopping_patience_counter = 0\n    should_training_stop = False\n    progress_bar = tqdm(range(args.max_selftrain_iterations), disable=not accelerator.is_local_main_process)\n    for iteration in range(0, int(args.max_selftrain_iterations)):\n        current_data_dir = data_dir_format(iteration)\n        assert os.path.exists(current_data_dir)\n        current_output_dir = os.path.join(current_data_dir, 'stage-1')\n        arguments_dict = {'accelerator': accelerator, 'model_name_or_path': args.model_name_or_path, 'cache_dir': args.cache_dir, 'do_train': True, 'train_file': data_files['train'] if iteration == 0 else data_files['train_pseudo'], 'do_eval': True if args.eval_file is not None else False, 'eval_file': data_files['eval'], 'do_predict': True, 'infer_file': data_files['infer'], 'task_name': args.task_name, 'label_list': args.label_list, 'output_dir': current_output_dir, 'eval_metric': args.eval_metric, 'evaluation_strategy': args.evaluation_strategy, 'early_stopping_patience': args.early_stopping_patience, 'early_stopping_threshold': args.early_stopping_threshold, 'seed': args.seed}\n        for (key, value) in kwargs.items():\n            if key not in arguments_dict and (not hasattr(training_args, key)):\n                arguments_dict.update({key: value})\n        model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n        if os.path.exists(model_bin_file_path):\n            logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 1.', model_bin_file_path, iteration)\n        else:\n            logger.info('***** Running self-training: iteration: %d, stage: 1 *****', iteration)\n            finetune(**arguments_dict)\n            accelerator.wait_for_everyone()\n            assert os.path.exists(model_bin_file_path)\n            logger.info('Self-training job completed: iteration: %d, stage: 1.', iteration)\n        if iteration > 0 and args.finetune_on_labeled_data:\n            model_path = os.path.join(current_output_dir, 'best-checkpoint')\n            current_output_dir = os.path.join(current_data_dir, 'stage-2')\n            arguments_dict['model_name_or_path'] = model_path\n            arguments_dict['train_file'] = data_files['train']\n            arguments_dict['output_dir'] = current_output_dir\n            model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n            if os.path.exists(model_bin_file_path):\n                logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 2.', model_bin_file_path, iteration)\n            else:\n                logger.info('***** Running self-training: iteration: %d, stage: 2 *****', iteration)\n                finetune(**arguments_dict)\n                accelerator.wait_for_everyone()\n                assert os.path.exists(model_bin_file_path)\n                logger.info('Self-training job completed: iteration: %d, stage: 2.', iteration)\n        new_iteration = iteration\n        next_data_dir = data_dir_format(iteration + 1)\n        config = AutoConfig.from_pretrained(os.path.join(current_output_dir, 'best-checkpoint'))\n        id2label = config.id2label\n        eval_results_file = os.path.join(current_output_dir, 'eval_results_best-checkpoint.json')\n        test_results_file = os.path.join(current_output_dir, 'test_results_best-checkpoint.json')\n        assert os.path.exists(eval_results_file)\n        with open(eval_results_file, 'r') as f:\n            eval_result = float(json.load(f)[args.eval_metric])\n        infer_output_file = os.path.join(current_output_dir, 'infer_output_best-checkpoint.csv')\n        assert os.path.exists(infer_output_file)\n        infer_input = load_dataset(args.data_file_extension, data_files={'data': data_files['infer']})['data']\n        infer_output = load_dataset('csv', data_files={'data': infer_output_file})['data']\n        if accelerator.is_main_process:\n            os.makedirs(next_data_dir, exist_ok=True)\n            shutil.copy(eval_results_file, os.path.join(output_dir, f'eval_results_iter-{iteration}.json'))\n            if os.path.exists(test_results_file):\n                shutil.copy(eval_results_file, os.path.join(output_dir, f'test_results_iter-{iteration}.json'))\n            create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir)\n        accelerator.wait_for_everyone()\n        data_files['train_pseudo'] = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n        if args.evaluation_strategy != IntervalStrategy.NO.value:\n            new_eval_result = eval_result\n            if best_iteration is None:\n                best_iteration = new_iteration\n                best_eval_result = new_eval_result\n            else:\n                if new_eval_result - best_eval_result > args.early_stopping_threshold:\n                    best_iteration = new_iteration\n                    best_eval_result = new_eval_result\n                    early_stopping_patience_counter = 0\n                else:\n                    if new_eval_result == best_eval_result:\n                        best_iteration = new_iteration\n                        best_eval_result = new_eval_result\n                    early_stopping_patience_counter += 1\n                if early_stopping_patience_counter >= args.early_stopping_patience:\n                    should_training_stop = True\n        progress_bar.update(1)\n        if should_training_stop:\n            break\n    if best_iteration is not None:\n        logger.info('Best iteration: %d', best_iteration)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, best_eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{iteration}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))\n    else:\n        logger.info('Best iteration: %d', args.max_selftrain_iterations - 1)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{args.max_selftrain_iterations - 1}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))",
            "def selftrain(model_name_or_path, train_file, infer_file, output_dir, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Self-training a pre-trained model on a downstream task.\\n\\n    Args:\\n      model_name_or_path: Path to pretrained model or model identifier from\\n        huggingface.co/models.\\n      train_file: A csv or a json file containing the training data.\\n      infer_file: A csv or a json file containing the data to predict on.\\n      output_dir: The output directory where the model predictions and checkpoints\\n        will be written.\\n      **kwargs: Dictionary of key/value pairs with which to update the\\n        configuration object after loading. The values in kwargs of any keys which\\n        are configuration attributes will be used to override the loaded values.\\n    '\n    accelerator = Accelerator()\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    model_args = STModelArguments(model_name_or_path=model_name_or_path)\n    data_args = STDataArguments(train_file=train_file, infer_file=infer_file)\n    training_args = STTrainingArguments(output_dir=output_dir)\n    args = argparse.Namespace()\n    for arg_class in (model_args, data_args, training_args):\n        for (key, value) in vars(arg_class).items():\n            setattr(args, key, value)\n    for (key, value) in kwargs.items():\n        if hasattr(args, key):\n            setattr(args, key, value)\n    data_files = {}\n    args.data_file_extension = None\n    assert args.train_file is not None\n    assert args.infer_file is not None\n    data_files['train'] = args.train_file\n    data_files['infer'] = args.infer_file\n    if args.evaluation_strategy != IntervalStrategy.NO.value:\n        assert args.eval_file is not None\n        data_files['eval'] = args.eval_file\n    for key in data_files:\n        extension = data_files[key].split('.')[-1]\n        assert extension in ['csv', 'json'], f'`{key}_file` should be a csv or a json file.'\n        if args.data_file_extension is None:\n            args.data_file_extension = extension\n        else:\n            assert extension == args.data_file_extension, f'`{key}_file` should be a {args.data_file_extension} file`.'\n    assert args.eval_metric in datasets.list_metrics(), f'{args.eval_metric} not in the list of supported metrics {datasets.list_metrics()}.'\n    if args.seed is not None:\n        set_seed(args.seed)\n    logger.info('Creating the initial data directory for self-training...')\n    data_dir_format = f'{args.output_dir}/self-train_iter-{{}}'.format\n    initial_data_dir = data_dir_format(0)\n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n            os.makedirs(initial_data_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    best_iteration = None\n    best_eval_result = None\n    early_stopping_patience_counter = 0\n    should_training_stop = False\n    progress_bar = tqdm(range(args.max_selftrain_iterations), disable=not accelerator.is_local_main_process)\n    for iteration in range(0, int(args.max_selftrain_iterations)):\n        current_data_dir = data_dir_format(iteration)\n        assert os.path.exists(current_data_dir)\n        current_output_dir = os.path.join(current_data_dir, 'stage-1')\n        arguments_dict = {'accelerator': accelerator, 'model_name_or_path': args.model_name_or_path, 'cache_dir': args.cache_dir, 'do_train': True, 'train_file': data_files['train'] if iteration == 0 else data_files['train_pseudo'], 'do_eval': True if args.eval_file is not None else False, 'eval_file': data_files['eval'], 'do_predict': True, 'infer_file': data_files['infer'], 'task_name': args.task_name, 'label_list': args.label_list, 'output_dir': current_output_dir, 'eval_metric': args.eval_metric, 'evaluation_strategy': args.evaluation_strategy, 'early_stopping_patience': args.early_stopping_patience, 'early_stopping_threshold': args.early_stopping_threshold, 'seed': args.seed}\n        for (key, value) in kwargs.items():\n            if key not in arguments_dict and (not hasattr(training_args, key)):\n                arguments_dict.update({key: value})\n        model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n        if os.path.exists(model_bin_file_path):\n            logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 1.', model_bin_file_path, iteration)\n        else:\n            logger.info('***** Running self-training: iteration: %d, stage: 1 *****', iteration)\n            finetune(**arguments_dict)\n            accelerator.wait_for_everyone()\n            assert os.path.exists(model_bin_file_path)\n            logger.info('Self-training job completed: iteration: %d, stage: 1.', iteration)\n        if iteration > 0 and args.finetune_on_labeled_data:\n            model_path = os.path.join(current_output_dir, 'best-checkpoint')\n            current_output_dir = os.path.join(current_data_dir, 'stage-2')\n            arguments_dict['model_name_or_path'] = model_path\n            arguments_dict['train_file'] = data_files['train']\n            arguments_dict['output_dir'] = current_output_dir\n            model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n            if os.path.exists(model_bin_file_path):\n                logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 2.', model_bin_file_path, iteration)\n            else:\n                logger.info('***** Running self-training: iteration: %d, stage: 2 *****', iteration)\n                finetune(**arguments_dict)\n                accelerator.wait_for_everyone()\n                assert os.path.exists(model_bin_file_path)\n                logger.info('Self-training job completed: iteration: %d, stage: 2.', iteration)\n        new_iteration = iteration\n        next_data_dir = data_dir_format(iteration + 1)\n        config = AutoConfig.from_pretrained(os.path.join(current_output_dir, 'best-checkpoint'))\n        id2label = config.id2label\n        eval_results_file = os.path.join(current_output_dir, 'eval_results_best-checkpoint.json')\n        test_results_file = os.path.join(current_output_dir, 'test_results_best-checkpoint.json')\n        assert os.path.exists(eval_results_file)\n        with open(eval_results_file, 'r') as f:\n            eval_result = float(json.load(f)[args.eval_metric])\n        infer_output_file = os.path.join(current_output_dir, 'infer_output_best-checkpoint.csv')\n        assert os.path.exists(infer_output_file)\n        infer_input = load_dataset(args.data_file_extension, data_files={'data': data_files['infer']})['data']\n        infer_output = load_dataset('csv', data_files={'data': infer_output_file})['data']\n        if accelerator.is_main_process:\n            os.makedirs(next_data_dir, exist_ok=True)\n            shutil.copy(eval_results_file, os.path.join(output_dir, f'eval_results_iter-{iteration}.json'))\n            if os.path.exists(test_results_file):\n                shutil.copy(eval_results_file, os.path.join(output_dir, f'test_results_iter-{iteration}.json'))\n            create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir)\n        accelerator.wait_for_everyone()\n        data_files['train_pseudo'] = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n        if args.evaluation_strategy != IntervalStrategy.NO.value:\n            new_eval_result = eval_result\n            if best_iteration is None:\n                best_iteration = new_iteration\n                best_eval_result = new_eval_result\n            else:\n                if new_eval_result - best_eval_result > args.early_stopping_threshold:\n                    best_iteration = new_iteration\n                    best_eval_result = new_eval_result\n                    early_stopping_patience_counter = 0\n                else:\n                    if new_eval_result == best_eval_result:\n                        best_iteration = new_iteration\n                        best_eval_result = new_eval_result\n                    early_stopping_patience_counter += 1\n                if early_stopping_patience_counter >= args.early_stopping_patience:\n                    should_training_stop = True\n        progress_bar.update(1)\n        if should_training_stop:\n            break\n    if best_iteration is not None:\n        logger.info('Best iteration: %d', best_iteration)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, best_eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{iteration}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))\n    else:\n        logger.info('Best iteration: %d', args.max_selftrain_iterations - 1)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{args.max_selftrain_iterations - 1}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))",
            "def selftrain(model_name_or_path, train_file, infer_file, output_dir, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Self-training a pre-trained model on a downstream task.\\n\\n    Args:\\n      model_name_or_path: Path to pretrained model or model identifier from\\n        huggingface.co/models.\\n      train_file: A csv or a json file containing the training data.\\n      infer_file: A csv or a json file containing the data to predict on.\\n      output_dir: The output directory where the model predictions and checkpoints\\n        will be written.\\n      **kwargs: Dictionary of key/value pairs with which to update the\\n        configuration object after loading. The values in kwargs of any keys which\\n        are configuration attributes will be used to override the loaded values.\\n    '\n    accelerator = Accelerator()\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    model_args = STModelArguments(model_name_or_path=model_name_or_path)\n    data_args = STDataArguments(train_file=train_file, infer_file=infer_file)\n    training_args = STTrainingArguments(output_dir=output_dir)\n    args = argparse.Namespace()\n    for arg_class in (model_args, data_args, training_args):\n        for (key, value) in vars(arg_class).items():\n            setattr(args, key, value)\n    for (key, value) in kwargs.items():\n        if hasattr(args, key):\n            setattr(args, key, value)\n    data_files = {}\n    args.data_file_extension = None\n    assert args.train_file is not None\n    assert args.infer_file is not None\n    data_files['train'] = args.train_file\n    data_files['infer'] = args.infer_file\n    if args.evaluation_strategy != IntervalStrategy.NO.value:\n        assert args.eval_file is not None\n        data_files['eval'] = args.eval_file\n    for key in data_files:\n        extension = data_files[key].split('.')[-1]\n        assert extension in ['csv', 'json'], f'`{key}_file` should be a csv or a json file.'\n        if args.data_file_extension is None:\n            args.data_file_extension = extension\n        else:\n            assert extension == args.data_file_extension, f'`{key}_file` should be a {args.data_file_extension} file`.'\n    assert args.eval_metric in datasets.list_metrics(), f'{args.eval_metric} not in the list of supported metrics {datasets.list_metrics()}.'\n    if args.seed is not None:\n        set_seed(args.seed)\n    logger.info('Creating the initial data directory for self-training...')\n    data_dir_format = f'{args.output_dir}/self-train_iter-{{}}'.format\n    initial_data_dir = data_dir_format(0)\n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n            os.makedirs(initial_data_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    best_iteration = None\n    best_eval_result = None\n    early_stopping_patience_counter = 0\n    should_training_stop = False\n    progress_bar = tqdm(range(args.max_selftrain_iterations), disable=not accelerator.is_local_main_process)\n    for iteration in range(0, int(args.max_selftrain_iterations)):\n        current_data_dir = data_dir_format(iteration)\n        assert os.path.exists(current_data_dir)\n        current_output_dir = os.path.join(current_data_dir, 'stage-1')\n        arguments_dict = {'accelerator': accelerator, 'model_name_or_path': args.model_name_or_path, 'cache_dir': args.cache_dir, 'do_train': True, 'train_file': data_files['train'] if iteration == 0 else data_files['train_pseudo'], 'do_eval': True if args.eval_file is not None else False, 'eval_file': data_files['eval'], 'do_predict': True, 'infer_file': data_files['infer'], 'task_name': args.task_name, 'label_list': args.label_list, 'output_dir': current_output_dir, 'eval_metric': args.eval_metric, 'evaluation_strategy': args.evaluation_strategy, 'early_stopping_patience': args.early_stopping_patience, 'early_stopping_threshold': args.early_stopping_threshold, 'seed': args.seed}\n        for (key, value) in kwargs.items():\n            if key not in arguments_dict and (not hasattr(training_args, key)):\n                arguments_dict.update({key: value})\n        model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n        if os.path.exists(model_bin_file_path):\n            logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 1.', model_bin_file_path, iteration)\n        else:\n            logger.info('***** Running self-training: iteration: %d, stage: 1 *****', iteration)\n            finetune(**arguments_dict)\n            accelerator.wait_for_everyone()\n            assert os.path.exists(model_bin_file_path)\n            logger.info('Self-training job completed: iteration: %d, stage: 1.', iteration)\n        if iteration > 0 and args.finetune_on_labeled_data:\n            model_path = os.path.join(current_output_dir, 'best-checkpoint')\n            current_output_dir = os.path.join(current_data_dir, 'stage-2')\n            arguments_dict['model_name_or_path'] = model_path\n            arguments_dict['train_file'] = data_files['train']\n            arguments_dict['output_dir'] = current_output_dir\n            model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n            if os.path.exists(model_bin_file_path):\n                logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 2.', model_bin_file_path, iteration)\n            else:\n                logger.info('***** Running self-training: iteration: %d, stage: 2 *****', iteration)\n                finetune(**arguments_dict)\n                accelerator.wait_for_everyone()\n                assert os.path.exists(model_bin_file_path)\n                logger.info('Self-training job completed: iteration: %d, stage: 2.', iteration)\n        new_iteration = iteration\n        next_data_dir = data_dir_format(iteration + 1)\n        config = AutoConfig.from_pretrained(os.path.join(current_output_dir, 'best-checkpoint'))\n        id2label = config.id2label\n        eval_results_file = os.path.join(current_output_dir, 'eval_results_best-checkpoint.json')\n        test_results_file = os.path.join(current_output_dir, 'test_results_best-checkpoint.json')\n        assert os.path.exists(eval_results_file)\n        with open(eval_results_file, 'r') as f:\n            eval_result = float(json.load(f)[args.eval_metric])\n        infer_output_file = os.path.join(current_output_dir, 'infer_output_best-checkpoint.csv')\n        assert os.path.exists(infer_output_file)\n        infer_input = load_dataset(args.data_file_extension, data_files={'data': data_files['infer']})['data']\n        infer_output = load_dataset('csv', data_files={'data': infer_output_file})['data']\n        if accelerator.is_main_process:\n            os.makedirs(next_data_dir, exist_ok=True)\n            shutil.copy(eval_results_file, os.path.join(output_dir, f'eval_results_iter-{iteration}.json'))\n            if os.path.exists(test_results_file):\n                shutil.copy(eval_results_file, os.path.join(output_dir, f'test_results_iter-{iteration}.json'))\n            create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir)\n        accelerator.wait_for_everyone()\n        data_files['train_pseudo'] = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n        if args.evaluation_strategy != IntervalStrategy.NO.value:\n            new_eval_result = eval_result\n            if best_iteration is None:\n                best_iteration = new_iteration\n                best_eval_result = new_eval_result\n            else:\n                if new_eval_result - best_eval_result > args.early_stopping_threshold:\n                    best_iteration = new_iteration\n                    best_eval_result = new_eval_result\n                    early_stopping_patience_counter = 0\n                else:\n                    if new_eval_result == best_eval_result:\n                        best_iteration = new_iteration\n                        best_eval_result = new_eval_result\n                    early_stopping_patience_counter += 1\n                if early_stopping_patience_counter >= args.early_stopping_patience:\n                    should_training_stop = True\n        progress_bar.update(1)\n        if should_training_stop:\n            break\n    if best_iteration is not None:\n        logger.info('Best iteration: %d', best_iteration)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, best_eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{iteration}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))\n    else:\n        logger.info('Best iteration: %d', args.max_selftrain_iterations - 1)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{args.max_selftrain_iterations - 1}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))",
            "def selftrain(model_name_or_path, train_file, infer_file, output_dir, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Self-training a pre-trained model on a downstream task.\\n\\n    Args:\\n      model_name_or_path: Path to pretrained model or model identifier from\\n        huggingface.co/models.\\n      train_file: A csv or a json file containing the training data.\\n      infer_file: A csv or a json file containing the data to predict on.\\n      output_dir: The output directory where the model predictions and checkpoints\\n        will be written.\\n      **kwargs: Dictionary of key/value pairs with which to update the\\n        configuration object after loading. The values in kwargs of any keys which\\n        are configuration attributes will be used to override the loaded values.\\n    '\n    accelerator = Accelerator()\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state)\n    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    model_args = STModelArguments(model_name_or_path=model_name_or_path)\n    data_args = STDataArguments(train_file=train_file, infer_file=infer_file)\n    training_args = STTrainingArguments(output_dir=output_dir)\n    args = argparse.Namespace()\n    for arg_class in (model_args, data_args, training_args):\n        for (key, value) in vars(arg_class).items():\n            setattr(args, key, value)\n    for (key, value) in kwargs.items():\n        if hasattr(args, key):\n            setattr(args, key, value)\n    data_files = {}\n    args.data_file_extension = None\n    assert args.train_file is not None\n    assert args.infer_file is not None\n    data_files['train'] = args.train_file\n    data_files['infer'] = args.infer_file\n    if args.evaluation_strategy != IntervalStrategy.NO.value:\n        assert args.eval_file is not None\n        data_files['eval'] = args.eval_file\n    for key in data_files:\n        extension = data_files[key].split('.')[-1]\n        assert extension in ['csv', 'json'], f'`{key}_file` should be a csv or a json file.'\n        if args.data_file_extension is None:\n            args.data_file_extension = extension\n        else:\n            assert extension == args.data_file_extension, f'`{key}_file` should be a {args.data_file_extension} file`.'\n    assert args.eval_metric in datasets.list_metrics(), f'{args.eval_metric} not in the list of supported metrics {datasets.list_metrics()}.'\n    if args.seed is not None:\n        set_seed(args.seed)\n    logger.info('Creating the initial data directory for self-training...')\n    data_dir_format = f'{args.output_dir}/self-train_iter-{{}}'.format\n    initial_data_dir = data_dir_format(0)\n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n            os.makedirs(initial_data_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    best_iteration = None\n    best_eval_result = None\n    early_stopping_patience_counter = 0\n    should_training_stop = False\n    progress_bar = tqdm(range(args.max_selftrain_iterations), disable=not accelerator.is_local_main_process)\n    for iteration in range(0, int(args.max_selftrain_iterations)):\n        current_data_dir = data_dir_format(iteration)\n        assert os.path.exists(current_data_dir)\n        current_output_dir = os.path.join(current_data_dir, 'stage-1')\n        arguments_dict = {'accelerator': accelerator, 'model_name_or_path': args.model_name_or_path, 'cache_dir': args.cache_dir, 'do_train': True, 'train_file': data_files['train'] if iteration == 0 else data_files['train_pseudo'], 'do_eval': True if args.eval_file is not None else False, 'eval_file': data_files['eval'], 'do_predict': True, 'infer_file': data_files['infer'], 'task_name': args.task_name, 'label_list': args.label_list, 'output_dir': current_output_dir, 'eval_metric': args.eval_metric, 'evaluation_strategy': args.evaluation_strategy, 'early_stopping_patience': args.early_stopping_patience, 'early_stopping_threshold': args.early_stopping_threshold, 'seed': args.seed}\n        for (key, value) in kwargs.items():\n            if key not in arguments_dict and (not hasattr(training_args, key)):\n                arguments_dict.update({key: value})\n        model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n        if os.path.exists(model_bin_file_path):\n            logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 1.', model_bin_file_path, iteration)\n        else:\n            logger.info('***** Running self-training: iteration: %d, stage: 1 *****', iteration)\n            finetune(**arguments_dict)\n            accelerator.wait_for_everyone()\n            assert os.path.exists(model_bin_file_path)\n            logger.info('Self-training job completed: iteration: %d, stage: 1.', iteration)\n        if iteration > 0 and args.finetune_on_labeled_data:\n            model_path = os.path.join(current_output_dir, 'best-checkpoint')\n            current_output_dir = os.path.join(current_data_dir, 'stage-2')\n            arguments_dict['model_name_or_path'] = model_path\n            arguments_dict['train_file'] = data_files['train']\n            arguments_dict['output_dir'] = current_output_dir\n            model_bin_file_path = os.path.join(current_output_dir, 'best-checkpoint', MODEL_BIN_FILE)\n            if os.path.exists(model_bin_file_path):\n                logger.info('Found existing model checkpoint at %s. Skipping self-training: iteration: %d, stage: 2.', model_bin_file_path, iteration)\n            else:\n                logger.info('***** Running self-training: iteration: %d, stage: 2 *****', iteration)\n                finetune(**arguments_dict)\n                accelerator.wait_for_everyone()\n                assert os.path.exists(model_bin_file_path)\n                logger.info('Self-training job completed: iteration: %d, stage: 2.', iteration)\n        new_iteration = iteration\n        next_data_dir = data_dir_format(iteration + 1)\n        config = AutoConfig.from_pretrained(os.path.join(current_output_dir, 'best-checkpoint'))\n        id2label = config.id2label\n        eval_results_file = os.path.join(current_output_dir, 'eval_results_best-checkpoint.json')\n        test_results_file = os.path.join(current_output_dir, 'test_results_best-checkpoint.json')\n        assert os.path.exists(eval_results_file)\n        with open(eval_results_file, 'r') as f:\n            eval_result = float(json.load(f)[args.eval_metric])\n        infer_output_file = os.path.join(current_output_dir, 'infer_output_best-checkpoint.csv')\n        assert os.path.exists(infer_output_file)\n        infer_input = load_dataset(args.data_file_extension, data_files={'data': data_files['infer']})['data']\n        infer_output = load_dataset('csv', data_files={'data': infer_output_file})['data']\n        if accelerator.is_main_process:\n            os.makedirs(next_data_dir, exist_ok=True)\n            shutil.copy(eval_results_file, os.path.join(output_dir, f'eval_results_iter-{iteration}.json'))\n            if os.path.exists(test_results_file):\n                shutil.copy(eval_results_file, os.path.join(output_dir, f'test_results_iter-{iteration}.json'))\n            create_pseudo_labeled_data(args, infer_input, infer_output, eval_result, id2label, next_data_dir)\n        accelerator.wait_for_everyone()\n        data_files['train_pseudo'] = os.path.join(next_data_dir, f'train_pseudo.{args.data_file_extension}')\n        if args.evaluation_strategy != IntervalStrategy.NO.value:\n            new_eval_result = eval_result\n            if best_iteration is None:\n                best_iteration = new_iteration\n                best_eval_result = new_eval_result\n            else:\n                if new_eval_result - best_eval_result > args.early_stopping_threshold:\n                    best_iteration = new_iteration\n                    best_eval_result = new_eval_result\n                    early_stopping_patience_counter = 0\n                else:\n                    if new_eval_result == best_eval_result:\n                        best_iteration = new_iteration\n                        best_eval_result = new_eval_result\n                    early_stopping_patience_counter += 1\n                if early_stopping_patience_counter >= args.early_stopping_patience:\n                    should_training_stop = True\n        progress_bar.update(1)\n        if should_training_stop:\n            break\n    if best_iteration is not None:\n        logger.info('Best iteration: %d', best_iteration)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, best_eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{iteration}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))\n    else:\n        logger.info('Best iteration: %d', args.max_selftrain_iterations - 1)\n        logger.info('Best evaluation result: %s = %f', args.eval_metric, eval_result)\n        accelerator.wait_for_everyone()\n        if accelerator.is_main_process:\n            shutil.copy(os.path.join(output_dir, f'eval_results_iter-{args.max_selftrain_iterations - 1}.json'), os.path.join(output_dir, 'eval_results_best-iteration.json'))"
        ]
    }
]