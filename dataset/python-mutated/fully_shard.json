[
    {
        "func_name": "fully_shard",
        "original": "@contract(state_cls=_FSDPState)\ndef fully_shard(module: nn.Module, *, process_group: Optional[dist.ProcessGroup]=None, policy: Optional[_Policy]=None, strategy: Optional[ShardingStrategy]=None, mixed_precision: Optional[MixedPrecision]=None, cpu_offload: Optional[CPUOffload]=None, ignored_modules: Optional[Iterable[torch.nn.Module]]=None, device_id: Optional[Union[int, torch.device]]=None, param_init_fn: Optional[Callable[[nn.Module], None]]=None, sync_module_states: bool=False, forward_prefetch: bool=False, ignored_states: Union[Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]]=None) -> nn.Module:\n    \"\"\"\n    Applies ``FullyShardedDataParallel` (FSDP) semantics to ``module``.\n    \"\"\"\n    torch._C._log_api_usage_once('torch.distributed.fully_shard')\n    if policy is not None and (not isinstance(policy, _Policy)):\n        raise ValueError(f'Expects a `_Policy` but got {policy}')\n    state = fully_shard.state(module)\n    state = _init_ignored_module_states(state, module, ignored_modules, ignored_states)\n    state = _init_device_handle(state, module, state._ignored_params, device_id)\n    _annotate_modules_for_dynamo(module, state._ignored_modules, True)\n    state = _init_process_group_state(state, process_group, strategy, policy)\n    if policy is not None:\n        root_kwargs = {'process_group': process_group, 'strategy': strategy, 'mixed_precision': mixed_precision, 'cpu_offload': cpu_offload, 'ignored_modules': ignored_modules, 'device_id': device_id, 'param_init_fn': param_init_fn, 'sync_module_states': sync_module_states, 'forward_prefetch': forward_prefetch, 'ignored_states': ignored_states}\n        if strategy in HYBRID_SHARDING_STRATEGIES:\n            root_kwargs['process_group'] = (state.process_group, state._inter_node_pg)\n        _auto_wrap(module, policy, state._ignored_modules, state._ignored_params, root_kwargs, fully_shard)\n    state = _init_core_state(state, strategy or ShardingStrategy.FULL_SHARD, mixed_precision, cpu_offload, limit_all_gathers=True, use_orig_params=True, backward_prefetch_limit=1, forward_prefetch_limit=1)\n    state = _init_runtime_state(state)\n    state = _init_prefetching_state(state, BackwardPrefetch.BACKWARD_PRE, forward_prefetch=forward_prefetch)\n    state = _init_buffer_state(state, module)\n    state = _init_param_handle_from_module(state, module, device_id, param_init_fn, sync_module_states)\n    state = _init_state_dict_state(state)\n    _register_all_state_dict_hooks(state)\n    _register_pre_forward_hook(state, module)\n    _register_post_forward_hook(state, module)\n    _register_root_pre_forward_hook(state, module)\n    _insert_module_state(module, state)\n    for submodule in module.modules():\n        if submodule in state._fully_sharded_module_to_handle and _get_module_state(submodule) is None:\n            _insert_module_state(submodule, state)\n    return module",
        "mutated": [
            "@contract(state_cls=_FSDPState)\ndef fully_shard(module: nn.Module, *, process_group: Optional[dist.ProcessGroup]=None, policy: Optional[_Policy]=None, strategy: Optional[ShardingStrategy]=None, mixed_precision: Optional[MixedPrecision]=None, cpu_offload: Optional[CPUOffload]=None, ignored_modules: Optional[Iterable[torch.nn.Module]]=None, device_id: Optional[Union[int, torch.device]]=None, param_init_fn: Optional[Callable[[nn.Module], None]]=None, sync_module_states: bool=False, forward_prefetch: bool=False, ignored_states: Union[Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]]=None) -> nn.Module:\n    if False:\n        i = 10\n    '\\n    Applies ``FullyShardedDataParallel` (FSDP) semantics to ``module``.\\n    '\n    torch._C._log_api_usage_once('torch.distributed.fully_shard')\n    if policy is not None and (not isinstance(policy, _Policy)):\n        raise ValueError(f'Expects a `_Policy` but got {policy}')\n    state = fully_shard.state(module)\n    state = _init_ignored_module_states(state, module, ignored_modules, ignored_states)\n    state = _init_device_handle(state, module, state._ignored_params, device_id)\n    _annotate_modules_for_dynamo(module, state._ignored_modules, True)\n    state = _init_process_group_state(state, process_group, strategy, policy)\n    if policy is not None:\n        root_kwargs = {'process_group': process_group, 'strategy': strategy, 'mixed_precision': mixed_precision, 'cpu_offload': cpu_offload, 'ignored_modules': ignored_modules, 'device_id': device_id, 'param_init_fn': param_init_fn, 'sync_module_states': sync_module_states, 'forward_prefetch': forward_prefetch, 'ignored_states': ignored_states}\n        if strategy in HYBRID_SHARDING_STRATEGIES:\n            root_kwargs['process_group'] = (state.process_group, state._inter_node_pg)\n        _auto_wrap(module, policy, state._ignored_modules, state._ignored_params, root_kwargs, fully_shard)\n    state = _init_core_state(state, strategy or ShardingStrategy.FULL_SHARD, mixed_precision, cpu_offload, limit_all_gathers=True, use_orig_params=True, backward_prefetch_limit=1, forward_prefetch_limit=1)\n    state = _init_runtime_state(state)\n    state = _init_prefetching_state(state, BackwardPrefetch.BACKWARD_PRE, forward_prefetch=forward_prefetch)\n    state = _init_buffer_state(state, module)\n    state = _init_param_handle_from_module(state, module, device_id, param_init_fn, sync_module_states)\n    state = _init_state_dict_state(state)\n    _register_all_state_dict_hooks(state)\n    _register_pre_forward_hook(state, module)\n    _register_post_forward_hook(state, module)\n    _register_root_pre_forward_hook(state, module)\n    _insert_module_state(module, state)\n    for submodule in module.modules():\n        if submodule in state._fully_sharded_module_to_handle and _get_module_state(submodule) is None:\n            _insert_module_state(submodule, state)\n    return module",
            "@contract(state_cls=_FSDPState)\ndef fully_shard(module: nn.Module, *, process_group: Optional[dist.ProcessGroup]=None, policy: Optional[_Policy]=None, strategy: Optional[ShardingStrategy]=None, mixed_precision: Optional[MixedPrecision]=None, cpu_offload: Optional[CPUOffload]=None, ignored_modules: Optional[Iterable[torch.nn.Module]]=None, device_id: Optional[Union[int, torch.device]]=None, param_init_fn: Optional[Callable[[nn.Module], None]]=None, sync_module_states: bool=False, forward_prefetch: bool=False, ignored_states: Union[Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]]=None) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Applies ``FullyShardedDataParallel` (FSDP) semantics to ``module``.\\n    '\n    torch._C._log_api_usage_once('torch.distributed.fully_shard')\n    if policy is not None and (not isinstance(policy, _Policy)):\n        raise ValueError(f'Expects a `_Policy` but got {policy}')\n    state = fully_shard.state(module)\n    state = _init_ignored_module_states(state, module, ignored_modules, ignored_states)\n    state = _init_device_handle(state, module, state._ignored_params, device_id)\n    _annotate_modules_for_dynamo(module, state._ignored_modules, True)\n    state = _init_process_group_state(state, process_group, strategy, policy)\n    if policy is not None:\n        root_kwargs = {'process_group': process_group, 'strategy': strategy, 'mixed_precision': mixed_precision, 'cpu_offload': cpu_offload, 'ignored_modules': ignored_modules, 'device_id': device_id, 'param_init_fn': param_init_fn, 'sync_module_states': sync_module_states, 'forward_prefetch': forward_prefetch, 'ignored_states': ignored_states}\n        if strategy in HYBRID_SHARDING_STRATEGIES:\n            root_kwargs['process_group'] = (state.process_group, state._inter_node_pg)\n        _auto_wrap(module, policy, state._ignored_modules, state._ignored_params, root_kwargs, fully_shard)\n    state = _init_core_state(state, strategy or ShardingStrategy.FULL_SHARD, mixed_precision, cpu_offload, limit_all_gathers=True, use_orig_params=True, backward_prefetch_limit=1, forward_prefetch_limit=1)\n    state = _init_runtime_state(state)\n    state = _init_prefetching_state(state, BackwardPrefetch.BACKWARD_PRE, forward_prefetch=forward_prefetch)\n    state = _init_buffer_state(state, module)\n    state = _init_param_handle_from_module(state, module, device_id, param_init_fn, sync_module_states)\n    state = _init_state_dict_state(state)\n    _register_all_state_dict_hooks(state)\n    _register_pre_forward_hook(state, module)\n    _register_post_forward_hook(state, module)\n    _register_root_pre_forward_hook(state, module)\n    _insert_module_state(module, state)\n    for submodule in module.modules():\n        if submodule in state._fully_sharded_module_to_handle and _get_module_state(submodule) is None:\n            _insert_module_state(submodule, state)\n    return module",
            "@contract(state_cls=_FSDPState)\ndef fully_shard(module: nn.Module, *, process_group: Optional[dist.ProcessGroup]=None, policy: Optional[_Policy]=None, strategy: Optional[ShardingStrategy]=None, mixed_precision: Optional[MixedPrecision]=None, cpu_offload: Optional[CPUOffload]=None, ignored_modules: Optional[Iterable[torch.nn.Module]]=None, device_id: Optional[Union[int, torch.device]]=None, param_init_fn: Optional[Callable[[nn.Module], None]]=None, sync_module_states: bool=False, forward_prefetch: bool=False, ignored_states: Union[Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]]=None) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Applies ``FullyShardedDataParallel` (FSDP) semantics to ``module``.\\n    '\n    torch._C._log_api_usage_once('torch.distributed.fully_shard')\n    if policy is not None and (not isinstance(policy, _Policy)):\n        raise ValueError(f'Expects a `_Policy` but got {policy}')\n    state = fully_shard.state(module)\n    state = _init_ignored_module_states(state, module, ignored_modules, ignored_states)\n    state = _init_device_handle(state, module, state._ignored_params, device_id)\n    _annotate_modules_for_dynamo(module, state._ignored_modules, True)\n    state = _init_process_group_state(state, process_group, strategy, policy)\n    if policy is not None:\n        root_kwargs = {'process_group': process_group, 'strategy': strategy, 'mixed_precision': mixed_precision, 'cpu_offload': cpu_offload, 'ignored_modules': ignored_modules, 'device_id': device_id, 'param_init_fn': param_init_fn, 'sync_module_states': sync_module_states, 'forward_prefetch': forward_prefetch, 'ignored_states': ignored_states}\n        if strategy in HYBRID_SHARDING_STRATEGIES:\n            root_kwargs['process_group'] = (state.process_group, state._inter_node_pg)\n        _auto_wrap(module, policy, state._ignored_modules, state._ignored_params, root_kwargs, fully_shard)\n    state = _init_core_state(state, strategy or ShardingStrategy.FULL_SHARD, mixed_precision, cpu_offload, limit_all_gathers=True, use_orig_params=True, backward_prefetch_limit=1, forward_prefetch_limit=1)\n    state = _init_runtime_state(state)\n    state = _init_prefetching_state(state, BackwardPrefetch.BACKWARD_PRE, forward_prefetch=forward_prefetch)\n    state = _init_buffer_state(state, module)\n    state = _init_param_handle_from_module(state, module, device_id, param_init_fn, sync_module_states)\n    state = _init_state_dict_state(state)\n    _register_all_state_dict_hooks(state)\n    _register_pre_forward_hook(state, module)\n    _register_post_forward_hook(state, module)\n    _register_root_pre_forward_hook(state, module)\n    _insert_module_state(module, state)\n    for submodule in module.modules():\n        if submodule in state._fully_sharded_module_to_handle and _get_module_state(submodule) is None:\n            _insert_module_state(submodule, state)\n    return module",
            "@contract(state_cls=_FSDPState)\ndef fully_shard(module: nn.Module, *, process_group: Optional[dist.ProcessGroup]=None, policy: Optional[_Policy]=None, strategy: Optional[ShardingStrategy]=None, mixed_precision: Optional[MixedPrecision]=None, cpu_offload: Optional[CPUOffload]=None, ignored_modules: Optional[Iterable[torch.nn.Module]]=None, device_id: Optional[Union[int, torch.device]]=None, param_init_fn: Optional[Callable[[nn.Module], None]]=None, sync_module_states: bool=False, forward_prefetch: bool=False, ignored_states: Union[Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]]=None) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Applies ``FullyShardedDataParallel` (FSDP) semantics to ``module``.\\n    '\n    torch._C._log_api_usage_once('torch.distributed.fully_shard')\n    if policy is not None and (not isinstance(policy, _Policy)):\n        raise ValueError(f'Expects a `_Policy` but got {policy}')\n    state = fully_shard.state(module)\n    state = _init_ignored_module_states(state, module, ignored_modules, ignored_states)\n    state = _init_device_handle(state, module, state._ignored_params, device_id)\n    _annotate_modules_for_dynamo(module, state._ignored_modules, True)\n    state = _init_process_group_state(state, process_group, strategy, policy)\n    if policy is not None:\n        root_kwargs = {'process_group': process_group, 'strategy': strategy, 'mixed_precision': mixed_precision, 'cpu_offload': cpu_offload, 'ignored_modules': ignored_modules, 'device_id': device_id, 'param_init_fn': param_init_fn, 'sync_module_states': sync_module_states, 'forward_prefetch': forward_prefetch, 'ignored_states': ignored_states}\n        if strategy in HYBRID_SHARDING_STRATEGIES:\n            root_kwargs['process_group'] = (state.process_group, state._inter_node_pg)\n        _auto_wrap(module, policy, state._ignored_modules, state._ignored_params, root_kwargs, fully_shard)\n    state = _init_core_state(state, strategy or ShardingStrategy.FULL_SHARD, mixed_precision, cpu_offload, limit_all_gathers=True, use_orig_params=True, backward_prefetch_limit=1, forward_prefetch_limit=1)\n    state = _init_runtime_state(state)\n    state = _init_prefetching_state(state, BackwardPrefetch.BACKWARD_PRE, forward_prefetch=forward_prefetch)\n    state = _init_buffer_state(state, module)\n    state = _init_param_handle_from_module(state, module, device_id, param_init_fn, sync_module_states)\n    state = _init_state_dict_state(state)\n    _register_all_state_dict_hooks(state)\n    _register_pre_forward_hook(state, module)\n    _register_post_forward_hook(state, module)\n    _register_root_pre_forward_hook(state, module)\n    _insert_module_state(module, state)\n    for submodule in module.modules():\n        if submodule in state._fully_sharded_module_to_handle and _get_module_state(submodule) is None:\n            _insert_module_state(submodule, state)\n    return module",
            "@contract(state_cls=_FSDPState)\ndef fully_shard(module: nn.Module, *, process_group: Optional[dist.ProcessGroup]=None, policy: Optional[_Policy]=None, strategy: Optional[ShardingStrategy]=None, mixed_precision: Optional[MixedPrecision]=None, cpu_offload: Optional[CPUOffload]=None, ignored_modules: Optional[Iterable[torch.nn.Module]]=None, device_id: Optional[Union[int, torch.device]]=None, param_init_fn: Optional[Callable[[nn.Module], None]]=None, sync_module_states: bool=False, forward_prefetch: bool=False, ignored_states: Union[Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]]=None) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Applies ``FullyShardedDataParallel` (FSDP) semantics to ``module``.\\n    '\n    torch._C._log_api_usage_once('torch.distributed.fully_shard')\n    if policy is not None and (not isinstance(policy, _Policy)):\n        raise ValueError(f'Expects a `_Policy` but got {policy}')\n    state = fully_shard.state(module)\n    state = _init_ignored_module_states(state, module, ignored_modules, ignored_states)\n    state = _init_device_handle(state, module, state._ignored_params, device_id)\n    _annotate_modules_for_dynamo(module, state._ignored_modules, True)\n    state = _init_process_group_state(state, process_group, strategy, policy)\n    if policy is not None:\n        root_kwargs = {'process_group': process_group, 'strategy': strategy, 'mixed_precision': mixed_precision, 'cpu_offload': cpu_offload, 'ignored_modules': ignored_modules, 'device_id': device_id, 'param_init_fn': param_init_fn, 'sync_module_states': sync_module_states, 'forward_prefetch': forward_prefetch, 'ignored_states': ignored_states}\n        if strategy in HYBRID_SHARDING_STRATEGIES:\n            root_kwargs['process_group'] = (state.process_group, state._inter_node_pg)\n        _auto_wrap(module, policy, state._ignored_modules, state._ignored_params, root_kwargs, fully_shard)\n    state = _init_core_state(state, strategy or ShardingStrategy.FULL_SHARD, mixed_precision, cpu_offload, limit_all_gathers=True, use_orig_params=True, backward_prefetch_limit=1, forward_prefetch_limit=1)\n    state = _init_runtime_state(state)\n    state = _init_prefetching_state(state, BackwardPrefetch.BACKWARD_PRE, forward_prefetch=forward_prefetch)\n    state = _init_buffer_state(state, module)\n    state = _init_param_handle_from_module(state, module, device_id, param_init_fn, sync_module_states)\n    state = _init_state_dict_state(state)\n    _register_all_state_dict_hooks(state)\n    _register_pre_forward_hook(state, module)\n    _register_post_forward_hook(state, module)\n    _register_root_pre_forward_hook(state, module)\n    _insert_module_state(module, state)\n    for submodule in module.modules():\n        if submodule in state._fully_sharded_module_to_handle and _get_module_state(submodule) is None:\n            _insert_module_state(submodule, state)\n    return module"
        ]
    }
]