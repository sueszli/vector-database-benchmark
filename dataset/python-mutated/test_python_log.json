[
    {
        "func_name": "test_logging_level",
        "original": "def test_logging_level(self):\n    from flaml import logger, logger_formatter\n    with tempfile.TemporaryDirectory() as d:\n        training_log = os.path.join(d, 'training.log')\n        logger.setLevel(logging.INFO)\n        buf = io.StringIO()\n        ch = logging.StreamHandler(buf)\n        ch.setFormatter(logger_formatter)\n        logger.addHandler(ch)\n        automl = AutoML()\n        automl_settings = {'time_budget': 1, 'metric': 'rmse', 'task': 'regression', 'log_file_name': training_log, 'log_training_metric': True, 'n_jobs': 1, 'model_history': True, 'keep_search_state': True, 'learner_selector': 'roundrobin'}\n        (X_train, y_train) = fetch_california_housing(return_X_y=True)\n        n = len(y_train) >> 1\n        print(automl.model, automl.classes_, automl.predict(X_train))\n        automl.fit(X_train=X_train[:n], y_train=y_train[:n], X_val=X_train[n:], y_val=y_train[n:], **automl_settings)\n        logger.info(automl.search_space)\n        logger.info(automl.low_cost_partial_config)\n        logger.info(automl.points_to_evaluate)\n        logger.info(automl.cat_hp_cost)\n        import optuna as ot\n        study = ot.create_study()\n        from flaml.tune.space import define_by_run_func, add_cost_to_space\n        sample = define_by_run_func(study.ask(), automl.search_space)\n        logger.info(sample)\n        logger.info(unflatten_hierarchical(sample, automl.search_space))\n        add_cost_to_space(automl.search_space, automl.low_cost_partial_config, automl.cat_hp_cost)\n        logger.info(automl.search_space['ml'].categories)\n        if automl.best_config:\n            config = automl.best_config.copy()\n            config['learner'] = automl.best_estimator\n            automl.trainable({'ml': config})\n        from flaml import tune, BlendSearch\n        from flaml.automl import size\n        from functools import partial\n        low_cost_partial_config = automl.low_cost_partial_config\n        search_alg = BlendSearch(metric='val_loss', mode='min', space=automl.search_space, low_cost_partial_config=low_cost_partial_config, points_to_evaluate=automl.points_to_evaluate, cat_hp_cost=automl.cat_hp_cost, resource_attr=automl.resource_attr, min_resource=automl.min_resource, max_resource=automl.max_resource, config_constraints=[(partial(size, automl._state.learner_classes), '<=', automl._mem_thres)], metric_constraints=automl.metric_constraints)\n        analysis = tune.run(automl.trainable, search_alg=search_alg, time_budget_s=1, num_samples=-1)\n        print(min((trial.last_result['val_loss'] for trial in analysis.trials)))\n        config = analysis.trials[-1].last_result['config']['ml']\n        automl._state._train_with_config(config.pop('learner'), config)\n        for _ in range(3):\n            print(search_alg._ls.complete_config(low_cost_partial_config, search_alg._ls_bound_min, search_alg._ls_bound_max))\n        self.assertTrue(len(buf.getvalue()) > 0)\n    import pickle\n    with open('automl.pkl', 'wb') as f:\n        pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n    print(automl.__version__)\n    pred1 = automl.predict(X_train)\n    with open('automl.pkl', 'rb') as f:\n        automl = pickle.load(f)\n    pred2 = automl.predict(X_train)\n    delta = pred1 - pred2\n    assert max(delta) == 0 and min(delta) == 0\n    automl.save_best_config('test/housing.json')",
        "mutated": [
            "def test_logging_level(self):\n    if False:\n        i = 10\n    from flaml import logger, logger_formatter\n    with tempfile.TemporaryDirectory() as d:\n        training_log = os.path.join(d, 'training.log')\n        logger.setLevel(logging.INFO)\n        buf = io.StringIO()\n        ch = logging.StreamHandler(buf)\n        ch.setFormatter(logger_formatter)\n        logger.addHandler(ch)\n        automl = AutoML()\n        automl_settings = {'time_budget': 1, 'metric': 'rmse', 'task': 'regression', 'log_file_name': training_log, 'log_training_metric': True, 'n_jobs': 1, 'model_history': True, 'keep_search_state': True, 'learner_selector': 'roundrobin'}\n        (X_train, y_train) = fetch_california_housing(return_X_y=True)\n        n = len(y_train) >> 1\n        print(automl.model, automl.classes_, automl.predict(X_train))\n        automl.fit(X_train=X_train[:n], y_train=y_train[:n], X_val=X_train[n:], y_val=y_train[n:], **automl_settings)\n        logger.info(automl.search_space)\n        logger.info(automl.low_cost_partial_config)\n        logger.info(automl.points_to_evaluate)\n        logger.info(automl.cat_hp_cost)\n        import optuna as ot\n        study = ot.create_study()\n        from flaml.tune.space import define_by_run_func, add_cost_to_space\n        sample = define_by_run_func(study.ask(), automl.search_space)\n        logger.info(sample)\n        logger.info(unflatten_hierarchical(sample, automl.search_space))\n        add_cost_to_space(automl.search_space, automl.low_cost_partial_config, automl.cat_hp_cost)\n        logger.info(automl.search_space['ml'].categories)\n        if automl.best_config:\n            config = automl.best_config.copy()\n            config['learner'] = automl.best_estimator\n            automl.trainable({'ml': config})\n        from flaml import tune, BlendSearch\n        from flaml.automl import size\n        from functools import partial\n        low_cost_partial_config = automl.low_cost_partial_config\n        search_alg = BlendSearch(metric='val_loss', mode='min', space=automl.search_space, low_cost_partial_config=low_cost_partial_config, points_to_evaluate=automl.points_to_evaluate, cat_hp_cost=automl.cat_hp_cost, resource_attr=automl.resource_attr, min_resource=automl.min_resource, max_resource=automl.max_resource, config_constraints=[(partial(size, automl._state.learner_classes), '<=', automl._mem_thres)], metric_constraints=automl.metric_constraints)\n        analysis = tune.run(automl.trainable, search_alg=search_alg, time_budget_s=1, num_samples=-1)\n        print(min((trial.last_result['val_loss'] for trial in analysis.trials)))\n        config = analysis.trials[-1].last_result['config']['ml']\n        automl._state._train_with_config(config.pop('learner'), config)\n        for _ in range(3):\n            print(search_alg._ls.complete_config(low_cost_partial_config, search_alg._ls_bound_min, search_alg._ls_bound_max))\n        self.assertTrue(len(buf.getvalue()) > 0)\n    import pickle\n    with open('automl.pkl', 'wb') as f:\n        pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n    print(automl.__version__)\n    pred1 = automl.predict(X_train)\n    with open('automl.pkl', 'rb') as f:\n        automl = pickle.load(f)\n    pred2 = automl.predict(X_train)\n    delta = pred1 - pred2\n    assert max(delta) == 0 and min(delta) == 0\n    automl.save_best_config('test/housing.json')",
            "def test_logging_level(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from flaml import logger, logger_formatter\n    with tempfile.TemporaryDirectory() as d:\n        training_log = os.path.join(d, 'training.log')\n        logger.setLevel(logging.INFO)\n        buf = io.StringIO()\n        ch = logging.StreamHandler(buf)\n        ch.setFormatter(logger_formatter)\n        logger.addHandler(ch)\n        automl = AutoML()\n        automl_settings = {'time_budget': 1, 'metric': 'rmse', 'task': 'regression', 'log_file_name': training_log, 'log_training_metric': True, 'n_jobs': 1, 'model_history': True, 'keep_search_state': True, 'learner_selector': 'roundrobin'}\n        (X_train, y_train) = fetch_california_housing(return_X_y=True)\n        n = len(y_train) >> 1\n        print(automl.model, automl.classes_, automl.predict(X_train))\n        automl.fit(X_train=X_train[:n], y_train=y_train[:n], X_val=X_train[n:], y_val=y_train[n:], **automl_settings)\n        logger.info(automl.search_space)\n        logger.info(automl.low_cost_partial_config)\n        logger.info(automl.points_to_evaluate)\n        logger.info(automl.cat_hp_cost)\n        import optuna as ot\n        study = ot.create_study()\n        from flaml.tune.space import define_by_run_func, add_cost_to_space\n        sample = define_by_run_func(study.ask(), automl.search_space)\n        logger.info(sample)\n        logger.info(unflatten_hierarchical(sample, automl.search_space))\n        add_cost_to_space(automl.search_space, automl.low_cost_partial_config, automl.cat_hp_cost)\n        logger.info(automl.search_space['ml'].categories)\n        if automl.best_config:\n            config = automl.best_config.copy()\n            config['learner'] = automl.best_estimator\n            automl.trainable({'ml': config})\n        from flaml import tune, BlendSearch\n        from flaml.automl import size\n        from functools import partial\n        low_cost_partial_config = automl.low_cost_partial_config\n        search_alg = BlendSearch(metric='val_loss', mode='min', space=automl.search_space, low_cost_partial_config=low_cost_partial_config, points_to_evaluate=automl.points_to_evaluate, cat_hp_cost=automl.cat_hp_cost, resource_attr=automl.resource_attr, min_resource=automl.min_resource, max_resource=automl.max_resource, config_constraints=[(partial(size, automl._state.learner_classes), '<=', automl._mem_thres)], metric_constraints=automl.metric_constraints)\n        analysis = tune.run(automl.trainable, search_alg=search_alg, time_budget_s=1, num_samples=-1)\n        print(min((trial.last_result['val_loss'] for trial in analysis.trials)))\n        config = analysis.trials[-1].last_result['config']['ml']\n        automl._state._train_with_config(config.pop('learner'), config)\n        for _ in range(3):\n            print(search_alg._ls.complete_config(low_cost_partial_config, search_alg._ls_bound_min, search_alg._ls_bound_max))\n        self.assertTrue(len(buf.getvalue()) > 0)\n    import pickle\n    with open('automl.pkl', 'wb') as f:\n        pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n    print(automl.__version__)\n    pred1 = automl.predict(X_train)\n    with open('automl.pkl', 'rb') as f:\n        automl = pickle.load(f)\n    pred2 = automl.predict(X_train)\n    delta = pred1 - pred2\n    assert max(delta) == 0 and min(delta) == 0\n    automl.save_best_config('test/housing.json')",
            "def test_logging_level(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from flaml import logger, logger_formatter\n    with tempfile.TemporaryDirectory() as d:\n        training_log = os.path.join(d, 'training.log')\n        logger.setLevel(logging.INFO)\n        buf = io.StringIO()\n        ch = logging.StreamHandler(buf)\n        ch.setFormatter(logger_formatter)\n        logger.addHandler(ch)\n        automl = AutoML()\n        automl_settings = {'time_budget': 1, 'metric': 'rmse', 'task': 'regression', 'log_file_name': training_log, 'log_training_metric': True, 'n_jobs': 1, 'model_history': True, 'keep_search_state': True, 'learner_selector': 'roundrobin'}\n        (X_train, y_train) = fetch_california_housing(return_X_y=True)\n        n = len(y_train) >> 1\n        print(automl.model, automl.classes_, automl.predict(X_train))\n        automl.fit(X_train=X_train[:n], y_train=y_train[:n], X_val=X_train[n:], y_val=y_train[n:], **automl_settings)\n        logger.info(automl.search_space)\n        logger.info(automl.low_cost_partial_config)\n        logger.info(automl.points_to_evaluate)\n        logger.info(automl.cat_hp_cost)\n        import optuna as ot\n        study = ot.create_study()\n        from flaml.tune.space import define_by_run_func, add_cost_to_space\n        sample = define_by_run_func(study.ask(), automl.search_space)\n        logger.info(sample)\n        logger.info(unflatten_hierarchical(sample, automl.search_space))\n        add_cost_to_space(automl.search_space, automl.low_cost_partial_config, automl.cat_hp_cost)\n        logger.info(automl.search_space['ml'].categories)\n        if automl.best_config:\n            config = automl.best_config.copy()\n            config['learner'] = automl.best_estimator\n            automl.trainable({'ml': config})\n        from flaml import tune, BlendSearch\n        from flaml.automl import size\n        from functools import partial\n        low_cost_partial_config = automl.low_cost_partial_config\n        search_alg = BlendSearch(metric='val_loss', mode='min', space=automl.search_space, low_cost_partial_config=low_cost_partial_config, points_to_evaluate=automl.points_to_evaluate, cat_hp_cost=automl.cat_hp_cost, resource_attr=automl.resource_attr, min_resource=automl.min_resource, max_resource=automl.max_resource, config_constraints=[(partial(size, automl._state.learner_classes), '<=', automl._mem_thres)], metric_constraints=automl.metric_constraints)\n        analysis = tune.run(automl.trainable, search_alg=search_alg, time_budget_s=1, num_samples=-1)\n        print(min((trial.last_result['val_loss'] for trial in analysis.trials)))\n        config = analysis.trials[-1].last_result['config']['ml']\n        automl._state._train_with_config(config.pop('learner'), config)\n        for _ in range(3):\n            print(search_alg._ls.complete_config(low_cost_partial_config, search_alg._ls_bound_min, search_alg._ls_bound_max))\n        self.assertTrue(len(buf.getvalue()) > 0)\n    import pickle\n    with open('automl.pkl', 'wb') as f:\n        pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n    print(automl.__version__)\n    pred1 = automl.predict(X_train)\n    with open('automl.pkl', 'rb') as f:\n        automl = pickle.load(f)\n    pred2 = automl.predict(X_train)\n    delta = pred1 - pred2\n    assert max(delta) == 0 and min(delta) == 0\n    automl.save_best_config('test/housing.json')",
            "def test_logging_level(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from flaml import logger, logger_formatter\n    with tempfile.TemporaryDirectory() as d:\n        training_log = os.path.join(d, 'training.log')\n        logger.setLevel(logging.INFO)\n        buf = io.StringIO()\n        ch = logging.StreamHandler(buf)\n        ch.setFormatter(logger_formatter)\n        logger.addHandler(ch)\n        automl = AutoML()\n        automl_settings = {'time_budget': 1, 'metric': 'rmse', 'task': 'regression', 'log_file_name': training_log, 'log_training_metric': True, 'n_jobs': 1, 'model_history': True, 'keep_search_state': True, 'learner_selector': 'roundrobin'}\n        (X_train, y_train) = fetch_california_housing(return_X_y=True)\n        n = len(y_train) >> 1\n        print(automl.model, automl.classes_, automl.predict(X_train))\n        automl.fit(X_train=X_train[:n], y_train=y_train[:n], X_val=X_train[n:], y_val=y_train[n:], **automl_settings)\n        logger.info(automl.search_space)\n        logger.info(automl.low_cost_partial_config)\n        logger.info(automl.points_to_evaluate)\n        logger.info(automl.cat_hp_cost)\n        import optuna as ot\n        study = ot.create_study()\n        from flaml.tune.space import define_by_run_func, add_cost_to_space\n        sample = define_by_run_func(study.ask(), automl.search_space)\n        logger.info(sample)\n        logger.info(unflatten_hierarchical(sample, automl.search_space))\n        add_cost_to_space(automl.search_space, automl.low_cost_partial_config, automl.cat_hp_cost)\n        logger.info(automl.search_space['ml'].categories)\n        if automl.best_config:\n            config = automl.best_config.copy()\n            config['learner'] = automl.best_estimator\n            automl.trainable({'ml': config})\n        from flaml import tune, BlendSearch\n        from flaml.automl import size\n        from functools import partial\n        low_cost_partial_config = automl.low_cost_partial_config\n        search_alg = BlendSearch(metric='val_loss', mode='min', space=automl.search_space, low_cost_partial_config=low_cost_partial_config, points_to_evaluate=automl.points_to_evaluate, cat_hp_cost=automl.cat_hp_cost, resource_attr=automl.resource_attr, min_resource=automl.min_resource, max_resource=automl.max_resource, config_constraints=[(partial(size, automl._state.learner_classes), '<=', automl._mem_thres)], metric_constraints=automl.metric_constraints)\n        analysis = tune.run(automl.trainable, search_alg=search_alg, time_budget_s=1, num_samples=-1)\n        print(min((trial.last_result['val_loss'] for trial in analysis.trials)))\n        config = analysis.trials[-1].last_result['config']['ml']\n        automl._state._train_with_config(config.pop('learner'), config)\n        for _ in range(3):\n            print(search_alg._ls.complete_config(low_cost_partial_config, search_alg._ls_bound_min, search_alg._ls_bound_max))\n        self.assertTrue(len(buf.getvalue()) > 0)\n    import pickle\n    with open('automl.pkl', 'wb') as f:\n        pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n    print(automl.__version__)\n    pred1 = automl.predict(X_train)\n    with open('automl.pkl', 'rb') as f:\n        automl = pickle.load(f)\n    pred2 = automl.predict(X_train)\n    delta = pred1 - pred2\n    assert max(delta) == 0 and min(delta) == 0\n    automl.save_best_config('test/housing.json')",
            "def test_logging_level(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from flaml import logger, logger_formatter\n    with tempfile.TemporaryDirectory() as d:\n        training_log = os.path.join(d, 'training.log')\n        logger.setLevel(logging.INFO)\n        buf = io.StringIO()\n        ch = logging.StreamHandler(buf)\n        ch.setFormatter(logger_formatter)\n        logger.addHandler(ch)\n        automl = AutoML()\n        automl_settings = {'time_budget': 1, 'metric': 'rmse', 'task': 'regression', 'log_file_name': training_log, 'log_training_metric': True, 'n_jobs': 1, 'model_history': True, 'keep_search_state': True, 'learner_selector': 'roundrobin'}\n        (X_train, y_train) = fetch_california_housing(return_X_y=True)\n        n = len(y_train) >> 1\n        print(automl.model, automl.classes_, automl.predict(X_train))\n        automl.fit(X_train=X_train[:n], y_train=y_train[:n], X_val=X_train[n:], y_val=y_train[n:], **automl_settings)\n        logger.info(automl.search_space)\n        logger.info(automl.low_cost_partial_config)\n        logger.info(automl.points_to_evaluate)\n        logger.info(automl.cat_hp_cost)\n        import optuna as ot\n        study = ot.create_study()\n        from flaml.tune.space import define_by_run_func, add_cost_to_space\n        sample = define_by_run_func(study.ask(), automl.search_space)\n        logger.info(sample)\n        logger.info(unflatten_hierarchical(sample, automl.search_space))\n        add_cost_to_space(automl.search_space, automl.low_cost_partial_config, automl.cat_hp_cost)\n        logger.info(automl.search_space['ml'].categories)\n        if automl.best_config:\n            config = automl.best_config.copy()\n            config['learner'] = automl.best_estimator\n            automl.trainable({'ml': config})\n        from flaml import tune, BlendSearch\n        from flaml.automl import size\n        from functools import partial\n        low_cost_partial_config = automl.low_cost_partial_config\n        search_alg = BlendSearch(metric='val_loss', mode='min', space=automl.search_space, low_cost_partial_config=low_cost_partial_config, points_to_evaluate=automl.points_to_evaluate, cat_hp_cost=automl.cat_hp_cost, resource_attr=automl.resource_attr, min_resource=automl.min_resource, max_resource=automl.max_resource, config_constraints=[(partial(size, automl._state.learner_classes), '<=', automl._mem_thres)], metric_constraints=automl.metric_constraints)\n        analysis = tune.run(automl.trainable, search_alg=search_alg, time_budget_s=1, num_samples=-1)\n        print(min((trial.last_result['val_loss'] for trial in analysis.trials)))\n        config = analysis.trials[-1].last_result['config']['ml']\n        automl._state._train_with_config(config.pop('learner'), config)\n        for _ in range(3):\n            print(search_alg._ls.complete_config(low_cost_partial_config, search_alg._ls_bound_min, search_alg._ls_bound_max))\n        self.assertTrue(len(buf.getvalue()) > 0)\n    import pickle\n    with open('automl.pkl', 'wb') as f:\n        pickle.dump(automl, f, pickle.HIGHEST_PROTOCOL)\n    print(automl.__version__)\n    pred1 = automl.predict(X_train)\n    with open('automl.pkl', 'rb') as f:\n        automl = pickle.load(f)\n    pred2 = automl.predict(X_train)\n    delta = pred1 - pred2\n    assert max(delta) == 0 and min(delta) == 0\n    automl.save_best_config('test/housing.json')"
        ]
    }
]