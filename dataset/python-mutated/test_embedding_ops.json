[
    {
        "func_name": "_run_embedding_op_test",
        "original": "def _run_embedding_op_test(self, shard_dim, input_size, num_embeddings, embedding_dim, **kwargs):\n    device_mesh = self.build_device_mesh()\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding.weight = torch.nn.Parameter(local_embedding.weight.clone().detach())\n    parallelize_module(module=sharded_embedding, device_mesh=device_mesh, parallelize_plan=ColwiseParallel(output_layouts=Replicate()) if shard_dim == 1 else RowwiseParallel())\n    torch.manual_seed(10)\n    inp = torch.randint(0, num_embeddings, tuple(input_size), device=self.device_type)\n    target = torch.empty(*inp.size(), embedding_dim, dtype=torch.float, device=self.device_type).random_(0, 1)\n    output = sharded_embedding(inp)\n    local_output = local_embedding(inp)\n    self.assertEqual(local_output, output)\n    loss = torch.nn.CrossEntropyLoss()\n    attn_loss = loss(output, target)\n    attn_dup_loss = loss(local_output, target)\n    attn_loss.backward()\n    attn_dup_loss.backward()\n    gradient = sharded_embedding.weight.grad.redistribute(device_mesh, [Replicate()]).to_local()\n    local_grad = local_embedding.weight.grad\n    self.assertEqual(gradient, local_grad)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, **kwargs)\n    sharded_output = torch.nn.functional.embedding(DTensor.from_local(inp, device_mesh, [Replicate()]), sharded_embedding.weight, **kwargs)\n    self.assertEqual(local_output, sharded_output.full_tensor())",
        "mutated": [
            "def _run_embedding_op_test(self, shard_dim, input_size, num_embeddings, embedding_dim, **kwargs):\n    if False:\n        i = 10\n    device_mesh = self.build_device_mesh()\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding.weight = torch.nn.Parameter(local_embedding.weight.clone().detach())\n    parallelize_module(module=sharded_embedding, device_mesh=device_mesh, parallelize_plan=ColwiseParallel(output_layouts=Replicate()) if shard_dim == 1 else RowwiseParallel())\n    torch.manual_seed(10)\n    inp = torch.randint(0, num_embeddings, tuple(input_size), device=self.device_type)\n    target = torch.empty(*inp.size(), embedding_dim, dtype=torch.float, device=self.device_type).random_(0, 1)\n    output = sharded_embedding(inp)\n    local_output = local_embedding(inp)\n    self.assertEqual(local_output, output)\n    loss = torch.nn.CrossEntropyLoss()\n    attn_loss = loss(output, target)\n    attn_dup_loss = loss(local_output, target)\n    attn_loss.backward()\n    attn_dup_loss.backward()\n    gradient = sharded_embedding.weight.grad.redistribute(device_mesh, [Replicate()]).to_local()\n    local_grad = local_embedding.weight.grad\n    self.assertEqual(gradient, local_grad)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, **kwargs)\n    sharded_output = torch.nn.functional.embedding(DTensor.from_local(inp, device_mesh, [Replicate()]), sharded_embedding.weight, **kwargs)\n    self.assertEqual(local_output, sharded_output.full_tensor())",
            "def _run_embedding_op_test(self, shard_dim, input_size, num_embeddings, embedding_dim, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    device_mesh = self.build_device_mesh()\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding.weight = torch.nn.Parameter(local_embedding.weight.clone().detach())\n    parallelize_module(module=sharded_embedding, device_mesh=device_mesh, parallelize_plan=ColwiseParallel(output_layouts=Replicate()) if shard_dim == 1 else RowwiseParallel())\n    torch.manual_seed(10)\n    inp = torch.randint(0, num_embeddings, tuple(input_size), device=self.device_type)\n    target = torch.empty(*inp.size(), embedding_dim, dtype=torch.float, device=self.device_type).random_(0, 1)\n    output = sharded_embedding(inp)\n    local_output = local_embedding(inp)\n    self.assertEqual(local_output, output)\n    loss = torch.nn.CrossEntropyLoss()\n    attn_loss = loss(output, target)\n    attn_dup_loss = loss(local_output, target)\n    attn_loss.backward()\n    attn_dup_loss.backward()\n    gradient = sharded_embedding.weight.grad.redistribute(device_mesh, [Replicate()]).to_local()\n    local_grad = local_embedding.weight.grad\n    self.assertEqual(gradient, local_grad)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, **kwargs)\n    sharded_output = torch.nn.functional.embedding(DTensor.from_local(inp, device_mesh, [Replicate()]), sharded_embedding.weight, **kwargs)\n    self.assertEqual(local_output, sharded_output.full_tensor())",
            "def _run_embedding_op_test(self, shard_dim, input_size, num_embeddings, embedding_dim, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    device_mesh = self.build_device_mesh()\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding.weight = torch.nn.Parameter(local_embedding.weight.clone().detach())\n    parallelize_module(module=sharded_embedding, device_mesh=device_mesh, parallelize_plan=ColwiseParallel(output_layouts=Replicate()) if shard_dim == 1 else RowwiseParallel())\n    torch.manual_seed(10)\n    inp = torch.randint(0, num_embeddings, tuple(input_size), device=self.device_type)\n    target = torch.empty(*inp.size(), embedding_dim, dtype=torch.float, device=self.device_type).random_(0, 1)\n    output = sharded_embedding(inp)\n    local_output = local_embedding(inp)\n    self.assertEqual(local_output, output)\n    loss = torch.nn.CrossEntropyLoss()\n    attn_loss = loss(output, target)\n    attn_dup_loss = loss(local_output, target)\n    attn_loss.backward()\n    attn_dup_loss.backward()\n    gradient = sharded_embedding.weight.grad.redistribute(device_mesh, [Replicate()]).to_local()\n    local_grad = local_embedding.weight.grad\n    self.assertEqual(gradient, local_grad)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, **kwargs)\n    sharded_output = torch.nn.functional.embedding(DTensor.from_local(inp, device_mesh, [Replicate()]), sharded_embedding.weight, **kwargs)\n    self.assertEqual(local_output, sharded_output.full_tensor())",
            "def _run_embedding_op_test(self, shard_dim, input_size, num_embeddings, embedding_dim, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    device_mesh = self.build_device_mesh()\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding.weight = torch.nn.Parameter(local_embedding.weight.clone().detach())\n    parallelize_module(module=sharded_embedding, device_mesh=device_mesh, parallelize_plan=ColwiseParallel(output_layouts=Replicate()) if shard_dim == 1 else RowwiseParallel())\n    torch.manual_seed(10)\n    inp = torch.randint(0, num_embeddings, tuple(input_size), device=self.device_type)\n    target = torch.empty(*inp.size(), embedding_dim, dtype=torch.float, device=self.device_type).random_(0, 1)\n    output = sharded_embedding(inp)\n    local_output = local_embedding(inp)\n    self.assertEqual(local_output, output)\n    loss = torch.nn.CrossEntropyLoss()\n    attn_loss = loss(output, target)\n    attn_dup_loss = loss(local_output, target)\n    attn_loss.backward()\n    attn_dup_loss.backward()\n    gradient = sharded_embedding.weight.grad.redistribute(device_mesh, [Replicate()]).to_local()\n    local_grad = local_embedding.weight.grad\n    self.assertEqual(gradient, local_grad)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, **kwargs)\n    sharded_output = torch.nn.functional.embedding(DTensor.from_local(inp, device_mesh, [Replicate()]), sharded_embedding.weight, **kwargs)\n    self.assertEqual(local_output, sharded_output.full_tensor())",
            "def _run_embedding_op_test(self, shard_dim, input_size, num_embeddings, embedding_dim, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    device_mesh = self.build_device_mesh()\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, device=self.device_type, **kwargs)\n    sharded_embedding.weight = torch.nn.Parameter(local_embedding.weight.clone().detach())\n    parallelize_module(module=sharded_embedding, device_mesh=device_mesh, parallelize_plan=ColwiseParallel(output_layouts=Replicate()) if shard_dim == 1 else RowwiseParallel())\n    torch.manual_seed(10)\n    inp = torch.randint(0, num_embeddings, tuple(input_size), device=self.device_type)\n    target = torch.empty(*inp.size(), embedding_dim, dtype=torch.float, device=self.device_type).random_(0, 1)\n    output = sharded_embedding(inp)\n    local_output = local_embedding(inp)\n    self.assertEqual(local_output, output)\n    loss = torch.nn.CrossEntropyLoss()\n    attn_loss = loss(output, target)\n    attn_dup_loss = loss(local_output, target)\n    attn_loss.backward()\n    attn_dup_loss.backward()\n    gradient = sharded_embedding.weight.grad.redistribute(device_mesh, [Replicate()]).to_local()\n    local_grad = local_embedding.weight.grad\n    self.assertEqual(gradient, local_grad)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, **kwargs)\n    sharded_output = torch.nn.functional.embedding(DTensor.from_local(inp, device_mesh, [Replicate()]), sharded_embedding.weight, **kwargs)\n    self.assertEqual(local_output, sharded_output.full_tensor())"
        ]
    },
    {
        "func_name": "test_sharded_embedding_colwise",
        "original": "@with_comms\ndef test_sharded_embedding_colwise(self):\n    self._run_embedding_op_test(1, [5, 4], 17, 12)\n    self._run_embedding_op_test(1, [6, 7, 6], 21, 11)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13)\n    self._run_embedding_op_test(1, [8, 6, 5, 4, 7], 23, 16)\n    self._run_embedding_op_test(1, [4], 15, 14)\n    self._run_embedding_op_test(1, [34], 15, 14, padding_idx=10)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12)",
        "mutated": [
            "@with_comms\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n    self._run_embedding_op_test(1, [5, 4], 17, 12)\n    self._run_embedding_op_test(1, [6, 7, 6], 21, 11)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13)\n    self._run_embedding_op_test(1, [8, 6, 5, 4, 7], 23, 16)\n    self._run_embedding_op_test(1, [4], 15, 14)\n    self._run_embedding_op_test(1, [34], 15, 14, padding_idx=10)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12)",
            "@with_comms\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self._run_embedding_op_test(1, [5, 4], 17, 12)\n    self._run_embedding_op_test(1, [6, 7, 6], 21, 11)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13)\n    self._run_embedding_op_test(1, [8, 6, 5, 4, 7], 23, 16)\n    self._run_embedding_op_test(1, [4], 15, 14)\n    self._run_embedding_op_test(1, [34], 15, 14, padding_idx=10)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12)",
            "@with_comms\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self._run_embedding_op_test(1, [5, 4], 17, 12)\n    self._run_embedding_op_test(1, [6, 7, 6], 21, 11)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13)\n    self._run_embedding_op_test(1, [8, 6, 5, 4, 7], 23, 16)\n    self._run_embedding_op_test(1, [4], 15, 14)\n    self._run_embedding_op_test(1, [34], 15, 14, padding_idx=10)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12)",
            "@with_comms\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self._run_embedding_op_test(1, [5, 4], 17, 12)\n    self._run_embedding_op_test(1, [6, 7, 6], 21, 11)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13)\n    self._run_embedding_op_test(1, [8, 6, 5, 4, 7], 23, 16)\n    self._run_embedding_op_test(1, [4], 15, 14)\n    self._run_embedding_op_test(1, [34], 15, 14, padding_idx=10)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12)",
            "@with_comms\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self._run_embedding_op_test(1, [5, 4], 17, 12)\n    self._run_embedding_op_test(1, [6, 7, 6], 21, 11)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13)\n    self._run_embedding_op_test(1, [8, 6, 5, 4, 7], 23, 16)\n    self._run_embedding_op_test(1, [4], 15, 14)\n    self._run_embedding_op_test(1, [34], 15, 14, padding_idx=10)\n    self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12)"
        ]
    },
    {
        "func_name": "test_sharded_embedding_colwise_errors",
        "original": "@with_comms\ndef test_sharded_embedding_colwise_errors(self):\n    with self.assertRaisesRegex(NotImplementedError, 'DTensor does not support sharded embedding operation with max_norm yet!'):\n        self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12, max_norm=2.0)",
        "mutated": [
            "@with_comms\ndef test_sharded_embedding_colwise_errors(self):\n    if False:\n        i = 10\n    with self.assertRaisesRegex(NotImplementedError, 'DTensor does not support sharded embedding operation with max_norm yet!'):\n        self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12, max_norm=2.0)",
            "@with_comms\ndef test_sharded_embedding_colwise_errors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with self.assertRaisesRegex(NotImplementedError, 'DTensor does not support sharded embedding operation with max_norm yet!'):\n        self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12, max_norm=2.0)",
            "@with_comms\ndef test_sharded_embedding_colwise_errors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with self.assertRaisesRegex(NotImplementedError, 'DTensor does not support sharded embedding operation with max_norm yet!'):\n        self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12, max_norm=2.0)",
            "@with_comms\ndef test_sharded_embedding_colwise_errors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with self.assertRaisesRegex(NotImplementedError, 'DTensor does not support sharded embedding operation with max_norm yet!'):\n        self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12, max_norm=2.0)",
            "@with_comms\ndef test_sharded_embedding_colwise_errors(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with self.assertRaisesRegex(NotImplementedError, 'DTensor does not support sharded embedding operation with max_norm yet!'):\n        self._run_embedding_op_test(1, [8, 6, 5, 4], 23, 13, padding_idx=12, max_norm=2.0)"
        ]
    },
    {
        "func_name": "test_sharded_embedding_rowwise",
        "original": "@with_comms\ndef test_sharded_embedding_rowwise(self):\n    with self.assertRaisesRegex(NotImplementedError, 'Only support ColwiseParallel when parallelizing Embedding now.'):\n        self._run_embedding_op_test(0, [5, 12], 16, 22)",
        "mutated": [
            "@with_comms\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n    with self.assertRaisesRegex(NotImplementedError, 'Only support ColwiseParallel when parallelizing Embedding now.'):\n        self._run_embedding_op_test(0, [5, 12], 16, 22)",
            "@with_comms\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with self.assertRaisesRegex(NotImplementedError, 'Only support ColwiseParallel when parallelizing Embedding now.'):\n        self._run_embedding_op_test(0, [5, 12], 16, 22)",
            "@with_comms\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with self.assertRaisesRegex(NotImplementedError, 'Only support ColwiseParallel when parallelizing Embedding now.'):\n        self._run_embedding_op_test(0, [5, 12], 16, 22)",
            "@with_comms\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with self.assertRaisesRegex(NotImplementedError, 'Only support ColwiseParallel when parallelizing Embedding now.'):\n        self._run_embedding_op_test(0, [5, 12], 16, 22)",
            "@with_comms\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with self.assertRaisesRegex(NotImplementedError, 'Only support ColwiseParallel when parallelizing Embedding now.'):\n        self._run_embedding_op_test(0, [5, 12], 16, 22)"
        ]
    }
]