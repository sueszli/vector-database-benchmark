[
    {
        "func_name": "average_precision",
        "original": "def average_precision(recalls, precisions, mode='area'):\n    \"\"\"Calculate average precision (for single or multiple scales).\n\n    Args:\n        recalls (np.ndarray): Recalls with shape of (num_scales, num_dets)\n            or (num_dets, ).\n        precisions (np.ndarray): Precisions with shape of\n            (num_scales, num_dets) or (num_dets, ).\n        mode (str): 'area' or '11points', 'area' means calculating the area\n            under precision-recall curve, '11points' means calculating\n            the average precision of recalls at [0, 0.1, ..., 1]\n\n    Returns:\n        float or np.ndarray: Calculated average precision.\n    \"\"\"\n    if recalls.ndim == 1:\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape\n    assert recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == 'area':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum((mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == '11points':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 0.001, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError('Unrecognized mode, only \"area\" and \"11points\" are supported')\n    return ap",
        "mutated": [
            "def average_precision(recalls, precisions, mode='area'):\n    if False:\n        i = 10\n    \"Calculate average precision (for single or multiple scales).\\n\\n    Args:\\n        recalls (np.ndarray): Recalls with shape of (num_scales, num_dets)\\n            or (num_dets, ).\\n        precisions (np.ndarray): Precisions with shape of\\n            (num_scales, num_dets) or (num_dets, ).\\n        mode (str): 'area' or '11points', 'area' means calculating the area\\n            under precision-recall curve, '11points' means calculating\\n            the average precision of recalls at [0, 0.1, ..., 1]\\n\\n    Returns:\\n        float or np.ndarray: Calculated average precision.\\n    \"\n    if recalls.ndim == 1:\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape\n    assert recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == 'area':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum((mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == '11points':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 0.001, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError('Unrecognized mode, only \"area\" and \"11points\" are supported')\n    return ap",
            "def average_precision(recalls, precisions, mode='area'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Calculate average precision (for single or multiple scales).\\n\\n    Args:\\n        recalls (np.ndarray): Recalls with shape of (num_scales, num_dets)\\n            or (num_dets, ).\\n        precisions (np.ndarray): Precisions with shape of\\n            (num_scales, num_dets) or (num_dets, ).\\n        mode (str): 'area' or '11points', 'area' means calculating the area\\n            under precision-recall curve, '11points' means calculating\\n            the average precision of recalls at [0, 0.1, ..., 1]\\n\\n    Returns:\\n        float or np.ndarray: Calculated average precision.\\n    \"\n    if recalls.ndim == 1:\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape\n    assert recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == 'area':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum((mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == '11points':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 0.001, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError('Unrecognized mode, only \"area\" and \"11points\" are supported')\n    return ap",
            "def average_precision(recalls, precisions, mode='area'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Calculate average precision (for single or multiple scales).\\n\\n    Args:\\n        recalls (np.ndarray): Recalls with shape of (num_scales, num_dets)\\n            or (num_dets, ).\\n        precisions (np.ndarray): Precisions with shape of\\n            (num_scales, num_dets) or (num_dets, ).\\n        mode (str): 'area' or '11points', 'area' means calculating the area\\n            under precision-recall curve, '11points' means calculating\\n            the average precision of recalls at [0, 0.1, ..., 1]\\n\\n    Returns:\\n        float or np.ndarray: Calculated average precision.\\n    \"\n    if recalls.ndim == 1:\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape\n    assert recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == 'area':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum((mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == '11points':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 0.001, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError('Unrecognized mode, only \"area\" and \"11points\" are supported')\n    return ap",
            "def average_precision(recalls, precisions, mode='area'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Calculate average precision (for single or multiple scales).\\n\\n    Args:\\n        recalls (np.ndarray): Recalls with shape of (num_scales, num_dets)\\n            or (num_dets, ).\\n        precisions (np.ndarray): Precisions with shape of\\n            (num_scales, num_dets) or (num_dets, ).\\n        mode (str): 'area' or '11points', 'area' means calculating the area\\n            under precision-recall curve, '11points' means calculating\\n            the average precision of recalls at [0, 0.1, ..., 1]\\n\\n    Returns:\\n        float or np.ndarray: Calculated average precision.\\n    \"\n    if recalls.ndim == 1:\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape\n    assert recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == 'area':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum((mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == '11points':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 0.001, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError('Unrecognized mode, only \"area\" and \"11points\" are supported')\n    return ap",
            "def average_precision(recalls, precisions, mode='area'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Calculate average precision (for single or multiple scales).\\n\\n    Args:\\n        recalls (np.ndarray): Recalls with shape of (num_scales, num_dets)\\n            or (num_dets, ).\\n        precisions (np.ndarray): Precisions with shape of\\n            (num_scales, num_dets) or (num_dets, ).\\n        mode (str): 'area' or '11points', 'area' means calculating the area\\n            under precision-recall curve, '11points' means calculating\\n            the average precision of recalls at [0, 0.1, ..., 1]\\n\\n    Returns:\\n        float or np.ndarray: Calculated average precision.\\n    \"\n    if recalls.ndim == 1:\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape\n    assert recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == 'area':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum((mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == '11points':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 0.001, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError('Unrecognized mode, only \"area\" and \"11points\" are supported')\n    return ap"
        ]
    },
    {
        "func_name": "eval_det_cls",
        "original": "def eval_det_cls(pred, gt, iou_thr=None):\n    \"\"\"Generic functions to compute precision/recall for object detection for a\n    single class.\n\n    Args:\n        pred (dict): Predictions mapping from image id to bounding boxes\n            and scores.\n        gt (dict): Ground truths mapping from image id to bounding boxes.\n        iou_thr (list[float]): A list of iou thresholds.\n\n    Return:\n        tuple (np.ndarray, np.ndarray, float): Recalls, precisions and\n            average precision.\n    \"\"\"\n    class_recs = {}\n    npos = 0\n    for img_id in gt.keys():\n        cur_gt_num = len(gt[img_id])\n        if cur_gt_num != 0:\n            gt_cur = torch.zeros([cur_gt_num, 7], dtype=torch.float32)\n            for i in range(cur_gt_num):\n                gt_cur[i] = gt[img_id][i].tensor\n            bbox = gt[img_id][0].new_box(gt_cur)\n        else:\n            bbox = gt[img_id]\n        det = [[False] * len(bbox) for i in iou_thr]\n        npos += len(bbox)\n        class_recs[img_id] = {'bbox': bbox, 'det': det}\n    image_ids = []\n    confidence = []\n    ious = []\n    for img_id in pred.keys():\n        cur_num = len(pred[img_id])\n        if cur_num == 0:\n            continue\n        pred_cur = torch.zeros((cur_num, 7), dtype=torch.float32)\n        box_idx = 0\n        for (box, score) in pred[img_id]:\n            image_ids.append(img_id)\n            confidence.append(score)\n            pred_cur[box_idx] = box.tensor\n            box_idx += 1\n        pred_cur = box.new_box(pred_cur)\n        gt_cur = class_recs[img_id]['bbox']\n        if len(gt_cur) > 0:\n            iou_cur = pred_cur.overlaps(pred_cur, gt_cur)\n            for i in range(cur_num):\n                ious.append(iou_cur[i])\n        else:\n            for i in range(cur_num):\n                ious.append(np.zeros(1))\n    confidence = np.array(confidence)\n    sorted_ind = np.argsort(-confidence)\n    image_ids = [image_ids[x] for x in sorted_ind]\n    ious = [ious[x] for x in sorted_ind]\n    nd = len(image_ids)\n    tp_thr = [np.zeros(nd) for i in iou_thr]\n    fp_thr = [np.zeros(nd) for i in iou_thr]\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        iou_max = -np.inf\n        BBGT = R['bbox']\n        cur_iou = ious[d]\n        if len(BBGT) > 0:\n            for j in range(len(BBGT)):\n                iou = cur_iou[j]\n                if iou > iou_max:\n                    iou_max = iou\n                    jmax = j\n        for (iou_idx, thresh) in enumerate(iou_thr):\n            if iou_max > thresh:\n                if not R['det'][iou_idx][jmax]:\n                    tp_thr[iou_idx][d] = 1.0\n                    R['det'][iou_idx][jmax] = 1\n                else:\n                    fp_thr[iou_idx][d] = 1.0\n            else:\n                fp_thr[iou_idx][d] = 1.0\n    ret = []\n    for (iou_idx, thresh) in enumerate(iou_thr):\n        fp = np.cumsum(fp_thr[iou_idx])\n        tp = np.cumsum(tp_thr[iou_idx])\n        recall = tp / float(npos)\n        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = average_precision(recall, precision)\n        ret.append((recall, precision, ap))\n    return ret",
        "mutated": [
            "def eval_det_cls(pred, gt, iou_thr=None):\n    if False:\n        i = 10\n    'Generic functions to compute precision/recall for object detection for a\\n    single class.\\n\\n    Args:\\n        pred (dict): Predictions mapping from image id to bounding boxes\\n            and scores.\\n        gt (dict): Ground truths mapping from image id to bounding boxes.\\n        iou_thr (list[float]): A list of iou thresholds.\\n\\n    Return:\\n        tuple (np.ndarray, np.ndarray, float): Recalls, precisions and\\n            average precision.\\n    '\n    class_recs = {}\n    npos = 0\n    for img_id in gt.keys():\n        cur_gt_num = len(gt[img_id])\n        if cur_gt_num != 0:\n            gt_cur = torch.zeros([cur_gt_num, 7], dtype=torch.float32)\n            for i in range(cur_gt_num):\n                gt_cur[i] = gt[img_id][i].tensor\n            bbox = gt[img_id][0].new_box(gt_cur)\n        else:\n            bbox = gt[img_id]\n        det = [[False] * len(bbox) for i in iou_thr]\n        npos += len(bbox)\n        class_recs[img_id] = {'bbox': bbox, 'det': det}\n    image_ids = []\n    confidence = []\n    ious = []\n    for img_id in pred.keys():\n        cur_num = len(pred[img_id])\n        if cur_num == 0:\n            continue\n        pred_cur = torch.zeros((cur_num, 7), dtype=torch.float32)\n        box_idx = 0\n        for (box, score) in pred[img_id]:\n            image_ids.append(img_id)\n            confidence.append(score)\n            pred_cur[box_idx] = box.tensor\n            box_idx += 1\n        pred_cur = box.new_box(pred_cur)\n        gt_cur = class_recs[img_id]['bbox']\n        if len(gt_cur) > 0:\n            iou_cur = pred_cur.overlaps(pred_cur, gt_cur)\n            for i in range(cur_num):\n                ious.append(iou_cur[i])\n        else:\n            for i in range(cur_num):\n                ious.append(np.zeros(1))\n    confidence = np.array(confidence)\n    sorted_ind = np.argsort(-confidence)\n    image_ids = [image_ids[x] for x in sorted_ind]\n    ious = [ious[x] for x in sorted_ind]\n    nd = len(image_ids)\n    tp_thr = [np.zeros(nd) for i in iou_thr]\n    fp_thr = [np.zeros(nd) for i in iou_thr]\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        iou_max = -np.inf\n        BBGT = R['bbox']\n        cur_iou = ious[d]\n        if len(BBGT) > 0:\n            for j in range(len(BBGT)):\n                iou = cur_iou[j]\n                if iou > iou_max:\n                    iou_max = iou\n                    jmax = j\n        for (iou_idx, thresh) in enumerate(iou_thr):\n            if iou_max > thresh:\n                if not R['det'][iou_idx][jmax]:\n                    tp_thr[iou_idx][d] = 1.0\n                    R['det'][iou_idx][jmax] = 1\n                else:\n                    fp_thr[iou_idx][d] = 1.0\n            else:\n                fp_thr[iou_idx][d] = 1.0\n    ret = []\n    for (iou_idx, thresh) in enumerate(iou_thr):\n        fp = np.cumsum(fp_thr[iou_idx])\n        tp = np.cumsum(tp_thr[iou_idx])\n        recall = tp / float(npos)\n        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = average_precision(recall, precision)\n        ret.append((recall, precision, ap))\n    return ret",
            "def eval_det_cls(pred, gt, iou_thr=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generic functions to compute precision/recall for object detection for a\\n    single class.\\n\\n    Args:\\n        pred (dict): Predictions mapping from image id to bounding boxes\\n            and scores.\\n        gt (dict): Ground truths mapping from image id to bounding boxes.\\n        iou_thr (list[float]): A list of iou thresholds.\\n\\n    Return:\\n        tuple (np.ndarray, np.ndarray, float): Recalls, precisions and\\n            average precision.\\n    '\n    class_recs = {}\n    npos = 0\n    for img_id in gt.keys():\n        cur_gt_num = len(gt[img_id])\n        if cur_gt_num != 0:\n            gt_cur = torch.zeros([cur_gt_num, 7], dtype=torch.float32)\n            for i in range(cur_gt_num):\n                gt_cur[i] = gt[img_id][i].tensor\n            bbox = gt[img_id][0].new_box(gt_cur)\n        else:\n            bbox = gt[img_id]\n        det = [[False] * len(bbox) for i in iou_thr]\n        npos += len(bbox)\n        class_recs[img_id] = {'bbox': bbox, 'det': det}\n    image_ids = []\n    confidence = []\n    ious = []\n    for img_id in pred.keys():\n        cur_num = len(pred[img_id])\n        if cur_num == 0:\n            continue\n        pred_cur = torch.zeros((cur_num, 7), dtype=torch.float32)\n        box_idx = 0\n        for (box, score) in pred[img_id]:\n            image_ids.append(img_id)\n            confidence.append(score)\n            pred_cur[box_idx] = box.tensor\n            box_idx += 1\n        pred_cur = box.new_box(pred_cur)\n        gt_cur = class_recs[img_id]['bbox']\n        if len(gt_cur) > 0:\n            iou_cur = pred_cur.overlaps(pred_cur, gt_cur)\n            for i in range(cur_num):\n                ious.append(iou_cur[i])\n        else:\n            for i in range(cur_num):\n                ious.append(np.zeros(1))\n    confidence = np.array(confidence)\n    sorted_ind = np.argsort(-confidence)\n    image_ids = [image_ids[x] for x in sorted_ind]\n    ious = [ious[x] for x in sorted_ind]\n    nd = len(image_ids)\n    tp_thr = [np.zeros(nd) for i in iou_thr]\n    fp_thr = [np.zeros(nd) for i in iou_thr]\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        iou_max = -np.inf\n        BBGT = R['bbox']\n        cur_iou = ious[d]\n        if len(BBGT) > 0:\n            for j in range(len(BBGT)):\n                iou = cur_iou[j]\n                if iou > iou_max:\n                    iou_max = iou\n                    jmax = j\n        for (iou_idx, thresh) in enumerate(iou_thr):\n            if iou_max > thresh:\n                if not R['det'][iou_idx][jmax]:\n                    tp_thr[iou_idx][d] = 1.0\n                    R['det'][iou_idx][jmax] = 1\n                else:\n                    fp_thr[iou_idx][d] = 1.0\n            else:\n                fp_thr[iou_idx][d] = 1.0\n    ret = []\n    for (iou_idx, thresh) in enumerate(iou_thr):\n        fp = np.cumsum(fp_thr[iou_idx])\n        tp = np.cumsum(tp_thr[iou_idx])\n        recall = tp / float(npos)\n        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = average_precision(recall, precision)\n        ret.append((recall, precision, ap))\n    return ret",
            "def eval_det_cls(pred, gt, iou_thr=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generic functions to compute precision/recall for object detection for a\\n    single class.\\n\\n    Args:\\n        pred (dict): Predictions mapping from image id to bounding boxes\\n            and scores.\\n        gt (dict): Ground truths mapping from image id to bounding boxes.\\n        iou_thr (list[float]): A list of iou thresholds.\\n\\n    Return:\\n        tuple (np.ndarray, np.ndarray, float): Recalls, precisions and\\n            average precision.\\n    '\n    class_recs = {}\n    npos = 0\n    for img_id in gt.keys():\n        cur_gt_num = len(gt[img_id])\n        if cur_gt_num != 0:\n            gt_cur = torch.zeros([cur_gt_num, 7], dtype=torch.float32)\n            for i in range(cur_gt_num):\n                gt_cur[i] = gt[img_id][i].tensor\n            bbox = gt[img_id][0].new_box(gt_cur)\n        else:\n            bbox = gt[img_id]\n        det = [[False] * len(bbox) for i in iou_thr]\n        npos += len(bbox)\n        class_recs[img_id] = {'bbox': bbox, 'det': det}\n    image_ids = []\n    confidence = []\n    ious = []\n    for img_id in pred.keys():\n        cur_num = len(pred[img_id])\n        if cur_num == 0:\n            continue\n        pred_cur = torch.zeros((cur_num, 7), dtype=torch.float32)\n        box_idx = 0\n        for (box, score) in pred[img_id]:\n            image_ids.append(img_id)\n            confidence.append(score)\n            pred_cur[box_idx] = box.tensor\n            box_idx += 1\n        pred_cur = box.new_box(pred_cur)\n        gt_cur = class_recs[img_id]['bbox']\n        if len(gt_cur) > 0:\n            iou_cur = pred_cur.overlaps(pred_cur, gt_cur)\n            for i in range(cur_num):\n                ious.append(iou_cur[i])\n        else:\n            for i in range(cur_num):\n                ious.append(np.zeros(1))\n    confidence = np.array(confidence)\n    sorted_ind = np.argsort(-confidence)\n    image_ids = [image_ids[x] for x in sorted_ind]\n    ious = [ious[x] for x in sorted_ind]\n    nd = len(image_ids)\n    tp_thr = [np.zeros(nd) for i in iou_thr]\n    fp_thr = [np.zeros(nd) for i in iou_thr]\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        iou_max = -np.inf\n        BBGT = R['bbox']\n        cur_iou = ious[d]\n        if len(BBGT) > 0:\n            for j in range(len(BBGT)):\n                iou = cur_iou[j]\n                if iou > iou_max:\n                    iou_max = iou\n                    jmax = j\n        for (iou_idx, thresh) in enumerate(iou_thr):\n            if iou_max > thresh:\n                if not R['det'][iou_idx][jmax]:\n                    tp_thr[iou_idx][d] = 1.0\n                    R['det'][iou_idx][jmax] = 1\n                else:\n                    fp_thr[iou_idx][d] = 1.0\n            else:\n                fp_thr[iou_idx][d] = 1.0\n    ret = []\n    for (iou_idx, thresh) in enumerate(iou_thr):\n        fp = np.cumsum(fp_thr[iou_idx])\n        tp = np.cumsum(tp_thr[iou_idx])\n        recall = tp / float(npos)\n        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = average_precision(recall, precision)\n        ret.append((recall, precision, ap))\n    return ret",
            "def eval_det_cls(pred, gt, iou_thr=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generic functions to compute precision/recall for object detection for a\\n    single class.\\n\\n    Args:\\n        pred (dict): Predictions mapping from image id to bounding boxes\\n            and scores.\\n        gt (dict): Ground truths mapping from image id to bounding boxes.\\n        iou_thr (list[float]): A list of iou thresholds.\\n\\n    Return:\\n        tuple (np.ndarray, np.ndarray, float): Recalls, precisions and\\n            average precision.\\n    '\n    class_recs = {}\n    npos = 0\n    for img_id in gt.keys():\n        cur_gt_num = len(gt[img_id])\n        if cur_gt_num != 0:\n            gt_cur = torch.zeros([cur_gt_num, 7], dtype=torch.float32)\n            for i in range(cur_gt_num):\n                gt_cur[i] = gt[img_id][i].tensor\n            bbox = gt[img_id][0].new_box(gt_cur)\n        else:\n            bbox = gt[img_id]\n        det = [[False] * len(bbox) for i in iou_thr]\n        npos += len(bbox)\n        class_recs[img_id] = {'bbox': bbox, 'det': det}\n    image_ids = []\n    confidence = []\n    ious = []\n    for img_id in pred.keys():\n        cur_num = len(pred[img_id])\n        if cur_num == 0:\n            continue\n        pred_cur = torch.zeros((cur_num, 7), dtype=torch.float32)\n        box_idx = 0\n        for (box, score) in pred[img_id]:\n            image_ids.append(img_id)\n            confidence.append(score)\n            pred_cur[box_idx] = box.tensor\n            box_idx += 1\n        pred_cur = box.new_box(pred_cur)\n        gt_cur = class_recs[img_id]['bbox']\n        if len(gt_cur) > 0:\n            iou_cur = pred_cur.overlaps(pred_cur, gt_cur)\n            for i in range(cur_num):\n                ious.append(iou_cur[i])\n        else:\n            for i in range(cur_num):\n                ious.append(np.zeros(1))\n    confidence = np.array(confidence)\n    sorted_ind = np.argsort(-confidence)\n    image_ids = [image_ids[x] for x in sorted_ind]\n    ious = [ious[x] for x in sorted_ind]\n    nd = len(image_ids)\n    tp_thr = [np.zeros(nd) for i in iou_thr]\n    fp_thr = [np.zeros(nd) for i in iou_thr]\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        iou_max = -np.inf\n        BBGT = R['bbox']\n        cur_iou = ious[d]\n        if len(BBGT) > 0:\n            for j in range(len(BBGT)):\n                iou = cur_iou[j]\n                if iou > iou_max:\n                    iou_max = iou\n                    jmax = j\n        for (iou_idx, thresh) in enumerate(iou_thr):\n            if iou_max > thresh:\n                if not R['det'][iou_idx][jmax]:\n                    tp_thr[iou_idx][d] = 1.0\n                    R['det'][iou_idx][jmax] = 1\n                else:\n                    fp_thr[iou_idx][d] = 1.0\n            else:\n                fp_thr[iou_idx][d] = 1.0\n    ret = []\n    for (iou_idx, thresh) in enumerate(iou_thr):\n        fp = np.cumsum(fp_thr[iou_idx])\n        tp = np.cumsum(tp_thr[iou_idx])\n        recall = tp / float(npos)\n        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = average_precision(recall, precision)\n        ret.append((recall, precision, ap))\n    return ret",
            "def eval_det_cls(pred, gt, iou_thr=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generic functions to compute precision/recall for object detection for a\\n    single class.\\n\\n    Args:\\n        pred (dict): Predictions mapping from image id to bounding boxes\\n            and scores.\\n        gt (dict): Ground truths mapping from image id to bounding boxes.\\n        iou_thr (list[float]): A list of iou thresholds.\\n\\n    Return:\\n        tuple (np.ndarray, np.ndarray, float): Recalls, precisions and\\n            average precision.\\n    '\n    class_recs = {}\n    npos = 0\n    for img_id in gt.keys():\n        cur_gt_num = len(gt[img_id])\n        if cur_gt_num != 0:\n            gt_cur = torch.zeros([cur_gt_num, 7], dtype=torch.float32)\n            for i in range(cur_gt_num):\n                gt_cur[i] = gt[img_id][i].tensor\n            bbox = gt[img_id][0].new_box(gt_cur)\n        else:\n            bbox = gt[img_id]\n        det = [[False] * len(bbox) for i in iou_thr]\n        npos += len(bbox)\n        class_recs[img_id] = {'bbox': bbox, 'det': det}\n    image_ids = []\n    confidence = []\n    ious = []\n    for img_id in pred.keys():\n        cur_num = len(pred[img_id])\n        if cur_num == 0:\n            continue\n        pred_cur = torch.zeros((cur_num, 7), dtype=torch.float32)\n        box_idx = 0\n        for (box, score) in pred[img_id]:\n            image_ids.append(img_id)\n            confidence.append(score)\n            pred_cur[box_idx] = box.tensor\n            box_idx += 1\n        pred_cur = box.new_box(pred_cur)\n        gt_cur = class_recs[img_id]['bbox']\n        if len(gt_cur) > 0:\n            iou_cur = pred_cur.overlaps(pred_cur, gt_cur)\n            for i in range(cur_num):\n                ious.append(iou_cur[i])\n        else:\n            for i in range(cur_num):\n                ious.append(np.zeros(1))\n    confidence = np.array(confidence)\n    sorted_ind = np.argsort(-confidence)\n    image_ids = [image_ids[x] for x in sorted_ind]\n    ious = [ious[x] for x in sorted_ind]\n    nd = len(image_ids)\n    tp_thr = [np.zeros(nd) for i in iou_thr]\n    fp_thr = [np.zeros(nd) for i in iou_thr]\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        iou_max = -np.inf\n        BBGT = R['bbox']\n        cur_iou = ious[d]\n        if len(BBGT) > 0:\n            for j in range(len(BBGT)):\n                iou = cur_iou[j]\n                if iou > iou_max:\n                    iou_max = iou\n                    jmax = j\n        for (iou_idx, thresh) in enumerate(iou_thr):\n            if iou_max > thresh:\n                if not R['det'][iou_idx][jmax]:\n                    tp_thr[iou_idx][d] = 1.0\n                    R['det'][iou_idx][jmax] = 1\n                else:\n                    fp_thr[iou_idx][d] = 1.0\n            else:\n                fp_thr[iou_idx][d] = 1.0\n    ret = []\n    for (iou_idx, thresh) in enumerate(iou_thr):\n        fp = np.cumsum(fp_thr[iou_idx])\n        tp = np.cumsum(tp_thr[iou_idx])\n        recall = tp / float(npos)\n        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = average_precision(recall, precision)\n        ret.append((recall, precision, ap))\n    return ret"
        ]
    },
    {
        "func_name": "eval_map_recall",
        "original": "def eval_map_recall(pred, gt, ovthresh=None):\n    \"\"\"Evaluate mAP and recall.\n\n    Generic functions to compute precision/recall for object detection\n        for multiple classes.\n\n    Args:\n        pred (dict): Information of detection results,\n            which maps class_id and predictions.\n        gt (dict): Information of ground truths, which maps class_id and\n            ground truths.\n        ovthresh (list[float], optional): iou threshold. Default: None.\n\n    Return:\n        tuple[dict]: dict results of recall, AP, and precision for all classes.\n    \"\"\"\n    ret_values = {}\n    for classname in gt.keys():\n        if classname in pred:\n            ret_values[classname] = eval_det_cls(pred[classname], gt[classname], ovthresh)\n    recall = [{} for i in ovthresh]\n    precision = [{} for i in ovthresh]\n    ap = [{} for i in ovthresh]\n    for label in gt.keys():\n        for (iou_idx, thresh) in enumerate(ovthresh):\n            if label in pred:\n                (recall[iou_idx][label], precision[iou_idx][label], ap[iou_idx][label]) = ret_values[label][iou_idx]\n            else:\n                recall[iou_idx][label] = np.zeros(1)\n                precision[iou_idx][label] = np.zeros(1)\n                ap[iou_idx][label] = np.zeros(1)\n    return (recall, precision, ap)",
        "mutated": [
            "def eval_map_recall(pred, gt, ovthresh=None):\n    if False:\n        i = 10\n    'Evaluate mAP and recall.\\n\\n    Generic functions to compute precision/recall for object detection\\n        for multiple classes.\\n\\n    Args:\\n        pred (dict): Information of detection results,\\n            which maps class_id and predictions.\\n        gt (dict): Information of ground truths, which maps class_id and\\n            ground truths.\\n        ovthresh (list[float], optional): iou threshold. Default: None.\\n\\n    Return:\\n        tuple[dict]: dict results of recall, AP, and precision for all classes.\\n    '\n    ret_values = {}\n    for classname in gt.keys():\n        if classname in pred:\n            ret_values[classname] = eval_det_cls(pred[classname], gt[classname], ovthresh)\n    recall = [{} for i in ovthresh]\n    precision = [{} for i in ovthresh]\n    ap = [{} for i in ovthresh]\n    for label in gt.keys():\n        for (iou_idx, thresh) in enumerate(ovthresh):\n            if label in pred:\n                (recall[iou_idx][label], precision[iou_idx][label], ap[iou_idx][label]) = ret_values[label][iou_idx]\n            else:\n                recall[iou_idx][label] = np.zeros(1)\n                precision[iou_idx][label] = np.zeros(1)\n                ap[iou_idx][label] = np.zeros(1)\n    return (recall, precision, ap)",
            "def eval_map_recall(pred, gt, ovthresh=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Evaluate mAP and recall.\\n\\n    Generic functions to compute precision/recall for object detection\\n        for multiple classes.\\n\\n    Args:\\n        pred (dict): Information of detection results,\\n            which maps class_id and predictions.\\n        gt (dict): Information of ground truths, which maps class_id and\\n            ground truths.\\n        ovthresh (list[float], optional): iou threshold. Default: None.\\n\\n    Return:\\n        tuple[dict]: dict results of recall, AP, and precision for all classes.\\n    '\n    ret_values = {}\n    for classname in gt.keys():\n        if classname in pred:\n            ret_values[classname] = eval_det_cls(pred[classname], gt[classname], ovthresh)\n    recall = [{} for i in ovthresh]\n    precision = [{} for i in ovthresh]\n    ap = [{} for i in ovthresh]\n    for label in gt.keys():\n        for (iou_idx, thresh) in enumerate(ovthresh):\n            if label in pred:\n                (recall[iou_idx][label], precision[iou_idx][label], ap[iou_idx][label]) = ret_values[label][iou_idx]\n            else:\n                recall[iou_idx][label] = np.zeros(1)\n                precision[iou_idx][label] = np.zeros(1)\n                ap[iou_idx][label] = np.zeros(1)\n    return (recall, precision, ap)",
            "def eval_map_recall(pred, gt, ovthresh=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Evaluate mAP and recall.\\n\\n    Generic functions to compute precision/recall for object detection\\n        for multiple classes.\\n\\n    Args:\\n        pred (dict): Information of detection results,\\n            which maps class_id and predictions.\\n        gt (dict): Information of ground truths, which maps class_id and\\n            ground truths.\\n        ovthresh (list[float], optional): iou threshold. Default: None.\\n\\n    Return:\\n        tuple[dict]: dict results of recall, AP, and precision for all classes.\\n    '\n    ret_values = {}\n    for classname in gt.keys():\n        if classname in pred:\n            ret_values[classname] = eval_det_cls(pred[classname], gt[classname], ovthresh)\n    recall = [{} for i in ovthresh]\n    precision = [{} for i in ovthresh]\n    ap = [{} for i in ovthresh]\n    for label in gt.keys():\n        for (iou_idx, thresh) in enumerate(ovthresh):\n            if label in pred:\n                (recall[iou_idx][label], precision[iou_idx][label], ap[iou_idx][label]) = ret_values[label][iou_idx]\n            else:\n                recall[iou_idx][label] = np.zeros(1)\n                precision[iou_idx][label] = np.zeros(1)\n                ap[iou_idx][label] = np.zeros(1)\n    return (recall, precision, ap)",
            "def eval_map_recall(pred, gt, ovthresh=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Evaluate mAP and recall.\\n\\n    Generic functions to compute precision/recall for object detection\\n        for multiple classes.\\n\\n    Args:\\n        pred (dict): Information of detection results,\\n            which maps class_id and predictions.\\n        gt (dict): Information of ground truths, which maps class_id and\\n            ground truths.\\n        ovthresh (list[float], optional): iou threshold. Default: None.\\n\\n    Return:\\n        tuple[dict]: dict results of recall, AP, and precision for all classes.\\n    '\n    ret_values = {}\n    for classname in gt.keys():\n        if classname in pred:\n            ret_values[classname] = eval_det_cls(pred[classname], gt[classname], ovthresh)\n    recall = [{} for i in ovthresh]\n    precision = [{} for i in ovthresh]\n    ap = [{} for i in ovthresh]\n    for label in gt.keys():\n        for (iou_idx, thresh) in enumerate(ovthresh):\n            if label in pred:\n                (recall[iou_idx][label], precision[iou_idx][label], ap[iou_idx][label]) = ret_values[label][iou_idx]\n            else:\n                recall[iou_idx][label] = np.zeros(1)\n                precision[iou_idx][label] = np.zeros(1)\n                ap[iou_idx][label] = np.zeros(1)\n    return (recall, precision, ap)",
            "def eval_map_recall(pred, gt, ovthresh=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Evaluate mAP and recall.\\n\\n    Generic functions to compute precision/recall for object detection\\n        for multiple classes.\\n\\n    Args:\\n        pred (dict): Information of detection results,\\n            which maps class_id and predictions.\\n        gt (dict): Information of ground truths, which maps class_id and\\n            ground truths.\\n        ovthresh (list[float], optional): iou threshold. Default: None.\\n\\n    Return:\\n        tuple[dict]: dict results of recall, AP, and precision for all classes.\\n    '\n    ret_values = {}\n    for classname in gt.keys():\n        if classname in pred:\n            ret_values[classname] = eval_det_cls(pred[classname], gt[classname], ovthresh)\n    recall = [{} for i in ovthresh]\n    precision = [{} for i in ovthresh]\n    ap = [{} for i in ovthresh]\n    for label in gt.keys():\n        for (iou_idx, thresh) in enumerate(ovthresh):\n            if label in pred:\n                (recall[iou_idx][label], precision[iou_idx][label], ap[iou_idx][label]) = ret_values[label][iou_idx]\n            else:\n                recall[iou_idx][label] = np.zeros(1)\n                precision[iou_idx][label] = np.zeros(1)\n                ap[iou_idx][label] = np.zeros(1)\n    return (recall, precision, ap)"
        ]
    },
    {
        "func_name": "indoor_eval",
        "original": "def indoor_eval(gt_annos, dt_annos, metric, label2cat, logger=None, box_type_3d=None, box_mode_3d=None):\n    \"\"\"Indoor Evaluation.\n\n    Evaluate the result of the detection.\n\n    Args:\n        gt_annos (list[dict]): Ground truth annotations.\n        dt_annos (list[dict]): Detection annotations. the dict\n            includes the following keys\n\n            - labels_3d (torch.Tensor): Labels of boxes.\n            - boxes_3d (:obj:`BaseInstance3DBoxes`):\n                3D bounding boxes in Depth coordinate.\n            - scores_3d (torch.Tensor): Scores of boxes.\n        metric (list[float]): IoU thresholds for computing average precisions.\n        label2cat (dict): Map from label to category.\n        logger (logging.Logger | str, optional): The way to print the mAP\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\n\n    Return:\n        dict[str, float]: Dict of results.\n    \"\"\"\n    assert len(dt_annos) == len(gt_annos)\n    pred = {}\n    gt = {}\n    for img_id in range(len(dt_annos)):\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['labels_3d'])):\n            label = det_anno['labels_3d'].numpy()[i]\n            bbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\n            score = det_anno['scores_3d'].numpy()[i]\n            if label not in pred:\n                pred[int(label)] = {}\n            if img_id not in pred[label]:\n                pred[int(label)][img_id] = []\n            if label not in gt:\n                gt[int(label)] = {}\n            if img_id not in gt[label]:\n                gt[int(label)][img_id] = []\n            pred[int(label)][img_id].append((bbox, score))\n        gt_anno = gt_annos[img_id]\n        if gt_anno['gt_num'] != 0:\n            gt_boxes = box_type_3d(gt_anno['gt_boxes_upright_depth'], box_dim=gt_anno['gt_boxes_upright_depth'].shape[-1], origin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\n            labels_3d = gt_anno['class']\n        else:\n            gt_boxes = box_type_3d(np.array([], dtype=np.float32))\n            labels_3d = np.array([], dtype=np.int64)\n        for i in range(len(labels_3d)):\n            label = labels_3d[i]\n            bbox = gt_boxes[i]\n            if label not in gt:\n                gt[label] = {}\n            if img_id not in gt[label]:\n                gt[label][img_id] = []\n            gt[label][img_id].append(bbox)\n    (rec, prec, ap) = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    table_columns = [[label2cat[label] for label in ap[0].keys()] + ['Overall']]\n    for (i, iou_thresh) in enumerate(metric):\n        header.append(f'AP_{iou_thresh:.2f}')\n        header.append(f'AR_{iou_thresh:.2f}')\n        rec_list = []\n        for label in ap[i].keys():\n            ret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(ap[i][label][0])\n        ret_dict[f'mAP_{iou_thresh:.2f}'] = float(np.mean(list(ap[i].values())))\n        table_columns.append(list(map(float, list(ap[i].values()))))\n        table_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n        for label in rec[i].keys():\n            ret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(rec[i][label][-1])\n            rec_list.append(rec[i][label][-1])\n        ret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\n        table_columns.append(list(map(float, rec_list)))\n        table_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n    table_data = [header]\n    table_rows = list(zip(*table_columns))\n    table_data += table_rows\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border = True\n    print_log('\\n' + table.table, logger=logger)\n    return ret_dict",
        "mutated": [
            "def indoor_eval(gt_annos, dt_annos, metric, label2cat, logger=None, box_type_3d=None, box_mode_3d=None):\n    if False:\n        i = 10\n    'Indoor Evaluation.\\n\\n    Evaluate the result of the detection.\\n\\n    Args:\\n        gt_annos (list[dict]): Ground truth annotations.\\n        dt_annos (list[dict]): Detection annotations. the dict\\n            includes the following keys\\n\\n            - labels_3d (torch.Tensor): Labels of boxes.\\n            - boxes_3d (:obj:`BaseInstance3DBoxes`):\\n                3D bounding boxes in Depth coordinate.\\n            - scores_3d (torch.Tensor): Scores of boxes.\\n        metric (list[float]): IoU thresholds for computing average precisions.\\n        label2cat (dict): Map from label to category.\\n        logger (logging.Logger | str, optional): The way to print the mAP\\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\\n\\n    Return:\\n        dict[str, float]: Dict of results.\\n    '\n    assert len(dt_annos) == len(gt_annos)\n    pred = {}\n    gt = {}\n    for img_id in range(len(dt_annos)):\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['labels_3d'])):\n            label = det_anno['labels_3d'].numpy()[i]\n            bbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\n            score = det_anno['scores_3d'].numpy()[i]\n            if label not in pred:\n                pred[int(label)] = {}\n            if img_id not in pred[label]:\n                pred[int(label)][img_id] = []\n            if label not in gt:\n                gt[int(label)] = {}\n            if img_id not in gt[label]:\n                gt[int(label)][img_id] = []\n            pred[int(label)][img_id].append((bbox, score))\n        gt_anno = gt_annos[img_id]\n        if gt_anno['gt_num'] != 0:\n            gt_boxes = box_type_3d(gt_anno['gt_boxes_upright_depth'], box_dim=gt_anno['gt_boxes_upright_depth'].shape[-1], origin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\n            labels_3d = gt_anno['class']\n        else:\n            gt_boxes = box_type_3d(np.array([], dtype=np.float32))\n            labels_3d = np.array([], dtype=np.int64)\n        for i in range(len(labels_3d)):\n            label = labels_3d[i]\n            bbox = gt_boxes[i]\n            if label not in gt:\n                gt[label] = {}\n            if img_id not in gt[label]:\n                gt[label][img_id] = []\n            gt[label][img_id].append(bbox)\n    (rec, prec, ap) = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    table_columns = [[label2cat[label] for label in ap[0].keys()] + ['Overall']]\n    for (i, iou_thresh) in enumerate(metric):\n        header.append(f'AP_{iou_thresh:.2f}')\n        header.append(f'AR_{iou_thresh:.2f}')\n        rec_list = []\n        for label in ap[i].keys():\n            ret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(ap[i][label][0])\n        ret_dict[f'mAP_{iou_thresh:.2f}'] = float(np.mean(list(ap[i].values())))\n        table_columns.append(list(map(float, list(ap[i].values()))))\n        table_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n        for label in rec[i].keys():\n            ret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(rec[i][label][-1])\n            rec_list.append(rec[i][label][-1])\n        ret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\n        table_columns.append(list(map(float, rec_list)))\n        table_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n    table_data = [header]\n    table_rows = list(zip(*table_columns))\n    table_data += table_rows\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border = True\n    print_log('\\n' + table.table, logger=logger)\n    return ret_dict",
            "def indoor_eval(gt_annos, dt_annos, metric, label2cat, logger=None, box_type_3d=None, box_mode_3d=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Indoor Evaluation.\\n\\n    Evaluate the result of the detection.\\n\\n    Args:\\n        gt_annos (list[dict]): Ground truth annotations.\\n        dt_annos (list[dict]): Detection annotations. the dict\\n            includes the following keys\\n\\n            - labels_3d (torch.Tensor): Labels of boxes.\\n            - boxes_3d (:obj:`BaseInstance3DBoxes`):\\n                3D bounding boxes in Depth coordinate.\\n            - scores_3d (torch.Tensor): Scores of boxes.\\n        metric (list[float]): IoU thresholds for computing average precisions.\\n        label2cat (dict): Map from label to category.\\n        logger (logging.Logger | str, optional): The way to print the mAP\\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\\n\\n    Return:\\n        dict[str, float]: Dict of results.\\n    '\n    assert len(dt_annos) == len(gt_annos)\n    pred = {}\n    gt = {}\n    for img_id in range(len(dt_annos)):\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['labels_3d'])):\n            label = det_anno['labels_3d'].numpy()[i]\n            bbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\n            score = det_anno['scores_3d'].numpy()[i]\n            if label not in pred:\n                pred[int(label)] = {}\n            if img_id not in pred[label]:\n                pred[int(label)][img_id] = []\n            if label not in gt:\n                gt[int(label)] = {}\n            if img_id not in gt[label]:\n                gt[int(label)][img_id] = []\n            pred[int(label)][img_id].append((bbox, score))\n        gt_anno = gt_annos[img_id]\n        if gt_anno['gt_num'] != 0:\n            gt_boxes = box_type_3d(gt_anno['gt_boxes_upright_depth'], box_dim=gt_anno['gt_boxes_upright_depth'].shape[-1], origin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\n            labels_3d = gt_anno['class']\n        else:\n            gt_boxes = box_type_3d(np.array([], dtype=np.float32))\n            labels_3d = np.array([], dtype=np.int64)\n        for i in range(len(labels_3d)):\n            label = labels_3d[i]\n            bbox = gt_boxes[i]\n            if label not in gt:\n                gt[label] = {}\n            if img_id not in gt[label]:\n                gt[label][img_id] = []\n            gt[label][img_id].append(bbox)\n    (rec, prec, ap) = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    table_columns = [[label2cat[label] for label in ap[0].keys()] + ['Overall']]\n    for (i, iou_thresh) in enumerate(metric):\n        header.append(f'AP_{iou_thresh:.2f}')\n        header.append(f'AR_{iou_thresh:.2f}')\n        rec_list = []\n        for label in ap[i].keys():\n            ret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(ap[i][label][0])\n        ret_dict[f'mAP_{iou_thresh:.2f}'] = float(np.mean(list(ap[i].values())))\n        table_columns.append(list(map(float, list(ap[i].values()))))\n        table_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n        for label in rec[i].keys():\n            ret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(rec[i][label][-1])\n            rec_list.append(rec[i][label][-1])\n        ret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\n        table_columns.append(list(map(float, rec_list)))\n        table_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n    table_data = [header]\n    table_rows = list(zip(*table_columns))\n    table_data += table_rows\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border = True\n    print_log('\\n' + table.table, logger=logger)\n    return ret_dict",
            "def indoor_eval(gt_annos, dt_annos, metric, label2cat, logger=None, box_type_3d=None, box_mode_3d=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Indoor Evaluation.\\n\\n    Evaluate the result of the detection.\\n\\n    Args:\\n        gt_annos (list[dict]): Ground truth annotations.\\n        dt_annos (list[dict]): Detection annotations. the dict\\n            includes the following keys\\n\\n            - labels_3d (torch.Tensor): Labels of boxes.\\n            - boxes_3d (:obj:`BaseInstance3DBoxes`):\\n                3D bounding boxes in Depth coordinate.\\n            - scores_3d (torch.Tensor): Scores of boxes.\\n        metric (list[float]): IoU thresholds for computing average precisions.\\n        label2cat (dict): Map from label to category.\\n        logger (logging.Logger | str, optional): The way to print the mAP\\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\\n\\n    Return:\\n        dict[str, float]: Dict of results.\\n    '\n    assert len(dt_annos) == len(gt_annos)\n    pred = {}\n    gt = {}\n    for img_id in range(len(dt_annos)):\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['labels_3d'])):\n            label = det_anno['labels_3d'].numpy()[i]\n            bbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\n            score = det_anno['scores_3d'].numpy()[i]\n            if label not in pred:\n                pred[int(label)] = {}\n            if img_id not in pred[label]:\n                pred[int(label)][img_id] = []\n            if label not in gt:\n                gt[int(label)] = {}\n            if img_id not in gt[label]:\n                gt[int(label)][img_id] = []\n            pred[int(label)][img_id].append((bbox, score))\n        gt_anno = gt_annos[img_id]\n        if gt_anno['gt_num'] != 0:\n            gt_boxes = box_type_3d(gt_anno['gt_boxes_upright_depth'], box_dim=gt_anno['gt_boxes_upright_depth'].shape[-1], origin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\n            labels_3d = gt_anno['class']\n        else:\n            gt_boxes = box_type_3d(np.array([], dtype=np.float32))\n            labels_3d = np.array([], dtype=np.int64)\n        for i in range(len(labels_3d)):\n            label = labels_3d[i]\n            bbox = gt_boxes[i]\n            if label not in gt:\n                gt[label] = {}\n            if img_id not in gt[label]:\n                gt[label][img_id] = []\n            gt[label][img_id].append(bbox)\n    (rec, prec, ap) = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    table_columns = [[label2cat[label] for label in ap[0].keys()] + ['Overall']]\n    for (i, iou_thresh) in enumerate(metric):\n        header.append(f'AP_{iou_thresh:.2f}')\n        header.append(f'AR_{iou_thresh:.2f}')\n        rec_list = []\n        for label in ap[i].keys():\n            ret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(ap[i][label][0])\n        ret_dict[f'mAP_{iou_thresh:.2f}'] = float(np.mean(list(ap[i].values())))\n        table_columns.append(list(map(float, list(ap[i].values()))))\n        table_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n        for label in rec[i].keys():\n            ret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(rec[i][label][-1])\n            rec_list.append(rec[i][label][-1])\n        ret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\n        table_columns.append(list(map(float, rec_list)))\n        table_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n    table_data = [header]\n    table_rows = list(zip(*table_columns))\n    table_data += table_rows\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border = True\n    print_log('\\n' + table.table, logger=logger)\n    return ret_dict",
            "def indoor_eval(gt_annos, dt_annos, metric, label2cat, logger=None, box_type_3d=None, box_mode_3d=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Indoor Evaluation.\\n\\n    Evaluate the result of the detection.\\n\\n    Args:\\n        gt_annos (list[dict]): Ground truth annotations.\\n        dt_annos (list[dict]): Detection annotations. the dict\\n            includes the following keys\\n\\n            - labels_3d (torch.Tensor): Labels of boxes.\\n            - boxes_3d (:obj:`BaseInstance3DBoxes`):\\n                3D bounding boxes in Depth coordinate.\\n            - scores_3d (torch.Tensor): Scores of boxes.\\n        metric (list[float]): IoU thresholds for computing average precisions.\\n        label2cat (dict): Map from label to category.\\n        logger (logging.Logger | str, optional): The way to print the mAP\\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\\n\\n    Return:\\n        dict[str, float]: Dict of results.\\n    '\n    assert len(dt_annos) == len(gt_annos)\n    pred = {}\n    gt = {}\n    for img_id in range(len(dt_annos)):\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['labels_3d'])):\n            label = det_anno['labels_3d'].numpy()[i]\n            bbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\n            score = det_anno['scores_3d'].numpy()[i]\n            if label not in pred:\n                pred[int(label)] = {}\n            if img_id not in pred[label]:\n                pred[int(label)][img_id] = []\n            if label not in gt:\n                gt[int(label)] = {}\n            if img_id not in gt[label]:\n                gt[int(label)][img_id] = []\n            pred[int(label)][img_id].append((bbox, score))\n        gt_anno = gt_annos[img_id]\n        if gt_anno['gt_num'] != 0:\n            gt_boxes = box_type_3d(gt_anno['gt_boxes_upright_depth'], box_dim=gt_anno['gt_boxes_upright_depth'].shape[-1], origin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\n            labels_3d = gt_anno['class']\n        else:\n            gt_boxes = box_type_3d(np.array([], dtype=np.float32))\n            labels_3d = np.array([], dtype=np.int64)\n        for i in range(len(labels_3d)):\n            label = labels_3d[i]\n            bbox = gt_boxes[i]\n            if label not in gt:\n                gt[label] = {}\n            if img_id not in gt[label]:\n                gt[label][img_id] = []\n            gt[label][img_id].append(bbox)\n    (rec, prec, ap) = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    table_columns = [[label2cat[label] for label in ap[0].keys()] + ['Overall']]\n    for (i, iou_thresh) in enumerate(metric):\n        header.append(f'AP_{iou_thresh:.2f}')\n        header.append(f'AR_{iou_thresh:.2f}')\n        rec_list = []\n        for label in ap[i].keys():\n            ret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(ap[i][label][0])\n        ret_dict[f'mAP_{iou_thresh:.2f}'] = float(np.mean(list(ap[i].values())))\n        table_columns.append(list(map(float, list(ap[i].values()))))\n        table_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n        for label in rec[i].keys():\n            ret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(rec[i][label][-1])\n            rec_list.append(rec[i][label][-1])\n        ret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\n        table_columns.append(list(map(float, rec_list)))\n        table_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n    table_data = [header]\n    table_rows = list(zip(*table_columns))\n    table_data += table_rows\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border = True\n    print_log('\\n' + table.table, logger=logger)\n    return ret_dict",
            "def indoor_eval(gt_annos, dt_annos, metric, label2cat, logger=None, box_type_3d=None, box_mode_3d=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Indoor Evaluation.\\n\\n    Evaluate the result of the detection.\\n\\n    Args:\\n        gt_annos (list[dict]): Ground truth annotations.\\n        dt_annos (list[dict]): Detection annotations. the dict\\n            includes the following keys\\n\\n            - labels_3d (torch.Tensor): Labels of boxes.\\n            - boxes_3d (:obj:`BaseInstance3DBoxes`):\\n                3D bounding boxes in Depth coordinate.\\n            - scores_3d (torch.Tensor): Scores of boxes.\\n        metric (list[float]): IoU thresholds for computing average precisions.\\n        label2cat (dict): Map from label to category.\\n        logger (logging.Logger | str, optional): The way to print the mAP\\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\\n\\n    Return:\\n        dict[str, float]: Dict of results.\\n    '\n    assert len(dt_annos) == len(gt_annos)\n    pred = {}\n    gt = {}\n    for img_id in range(len(dt_annos)):\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['labels_3d'])):\n            label = det_anno['labels_3d'].numpy()[i]\n            bbox = det_anno['boxes_3d'].convert_to(box_mode_3d)[i]\n            score = det_anno['scores_3d'].numpy()[i]\n            if label not in pred:\n                pred[int(label)] = {}\n            if img_id not in pred[label]:\n                pred[int(label)][img_id] = []\n            if label not in gt:\n                gt[int(label)] = {}\n            if img_id not in gt[label]:\n                gt[int(label)][img_id] = []\n            pred[int(label)][img_id].append((bbox, score))\n        gt_anno = gt_annos[img_id]\n        if gt_anno['gt_num'] != 0:\n            gt_boxes = box_type_3d(gt_anno['gt_boxes_upright_depth'], box_dim=gt_anno['gt_boxes_upright_depth'].shape[-1], origin=(0.5, 0.5, 0.5)).convert_to(box_mode_3d)\n            labels_3d = gt_anno['class']\n        else:\n            gt_boxes = box_type_3d(np.array([], dtype=np.float32))\n            labels_3d = np.array([], dtype=np.int64)\n        for i in range(len(labels_3d)):\n            label = labels_3d[i]\n            bbox = gt_boxes[i]\n            if label not in gt:\n                gt[label] = {}\n            if img_id not in gt[label]:\n                gt[label][img_id] = []\n            gt[label][img_id].append(bbox)\n    (rec, prec, ap) = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    table_columns = [[label2cat[label] for label in ap[0].keys()] + ['Overall']]\n    for (i, iou_thresh) in enumerate(metric):\n        header.append(f'AP_{iou_thresh:.2f}')\n        header.append(f'AR_{iou_thresh:.2f}')\n        rec_list = []\n        for label in ap[i].keys():\n            ret_dict[f'{label2cat[label]}_AP_{iou_thresh:.2f}'] = float(ap[i][label][0])\n        ret_dict[f'mAP_{iou_thresh:.2f}'] = float(np.mean(list(ap[i].values())))\n        table_columns.append(list(map(float, list(ap[i].values()))))\n        table_columns[-1] += [ret_dict[f'mAP_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n        for label in rec[i].keys():\n            ret_dict[f'{label2cat[label]}_rec_{iou_thresh:.2f}'] = float(rec[i][label][-1])\n            rec_list.append(rec[i][label][-1])\n        ret_dict[f'mAR_{iou_thresh:.2f}'] = float(np.mean(rec_list))\n        table_columns.append(list(map(float, rec_list)))\n        table_columns[-1] += [ret_dict[f'mAR_{iou_thresh:.2f}']]\n        table_columns[-1] = [f'{x:.4f}' for x in table_columns[-1]]\n    table_data = [header]\n    table_rows = list(zip(*table_columns))\n    table_data += table_rows\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border = True\n    print_log('\\n' + table.table, logger=logger)\n    return ret_dict"
        ]
    }
]