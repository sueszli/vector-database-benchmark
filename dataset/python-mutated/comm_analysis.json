[
    {
        "func_name": "get_gpu_type",
        "original": "def get_gpu_type() -> NVIDIA_GPU_TYPE:\n    gpu_info = torch.utils.collect_env.get_gpu_info(torch.utils.collect_env.run)\n    if 'V100' in gpu_info:\n        return NVIDIA_GPU_TYPE.VOLTA\n    elif 'A100' in gpu_info:\n        return NVIDIA_GPU_TYPE.AMPERE\n    elif 'H100' in gpu_info:\n        return NVIDIA_GPU_TYPE.HOPPER\n    else:\n        return NVIDIA_GPU_TYPE.AMPERE",
        "mutated": [
            "def get_gpu_type() -> NVIDIA_GPU_TYPE:\n    if False:\n        i = 10\n    gpu_info = torch.utils.collect_env.get_gpu_info(torch.utils.collect_env.run)\n    if 'V100' in gpu_info:\n        return NVIDIA_GPU_TYPE.VOLTA\n    elif 'A100' in gpu_info:\n        return NVIDIA_GPU_TYPE.AMPERE\n    elif 'H100' in gpu_info:\n        return NVIDIA_GPU_TYPE.HOPPER\n    else:\n        return NVIDIA_GPU_TYPE.AMPERE",
            "def get_gpu_type() -> NVIDIA_GPU_TYPE:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    gpu_info = torch.utils.collect_env.get_gpu_info(torch.utils.collect_env.run)\n    if 'V100' in gpu_info:\n        return NVIDIA_GPU_TYPE.VOLTA\n    elif 'A100' in gpu_info:\n        return NVIDIA_GPU_TYPE.AMPERE\n    elif 'H100' in gpu_info:\n        return NVIDIA_GPU_TYPE.HOPPER\n    else:\n        return NVIDIA_GPU_TYPE.AMPERE",
            "def get_gpu_type() -> NVIDIA_GPU_TYPE:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    gpu_info = torch.utils.collect_env.get_gpu_info(torch.utils.collect_env.run)\n    if 'V100' in gpu_info:\n        return NVIDIA_GPU_TYPE.VOLTA\n    elif 'A100' in gpu_info:\n        return NVIDIA_GPU_TYPE.AMPERE\n    elif 'H100' in gpu_info:\n        return NVIDIA_GPU_TYPE.HOPPER\n    else:\n        return NVIDIA_GPU_TYPE.AMPERE",
            "def get_gpu_type() -> NVIDIA_GPU_TYPE:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    gpu_info = torch.utils.collect_env.get_gpu_info(torch.utils.collect_env.run)\n    if 'V100' in gpu_info:\n        return NVIDIA_GPU_TYPE.VOLTA\n    elif 'A100' in gpu_info:\n        return NVIDIA_GPU_TYPE.AMPERE\n    elif 'H100' in gpu_info:\n        return NVIDIA_GPU_TYPE.HOPPER\n    else:\n        return NVIDIA_GPU_TYPE.AMPERE",
            "def get_gpu_type() -> NVIDIA_GPU_TYPE:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    gpu_info = torch.utils.collect_env.get_gpu_info(torch.utils.collect_env.run)\n    if 'V100' in gpu_info:\n        return NVIDIA_GPU_TYPE.VOLTA\n    elif 'A100' in gpu_info:\n        return NVIDIA_GPU_TYPE.AMPERE\n    elif 'H100' in gpu_info:\n        return NVIDIA_GPU_TYPE.HOPPER\n    else:\n        return NVIDIA_GPU_TYPE.AMPERE"
        ]
    },
    {
        "func_name": "get_collective_type",
        "original": "def get_collective_type(snode: 'BaseSchedulerNode') -> NCCL_COLL:\n    if isinstance(snode.node, (ir.AllReduce, ir.AllReduceCoalesced)):\n        return NCCL_COLL.ALL_REDUCE\n    elif isinstance(snode.node, (ir.AllGatherIntoTensor, ir.AllGatherIntoTensorCoalesced)):\n        return NCCL_COLL.ALL_GATHER\n    elif isinstance(snode.node, (ir.ReduceScatterTensor, ir.ReduceScatterTensorCoalesced)):\n        return NCCL_COLL.REDUCE_SCATTER\n    else:\n        raise Exception(f'Unsupported collective type: {snode.node}')",
        "mutated": [
            "def get_collective_type(snode: 'BaseSchedulerNode') -> NCCL_COLL:\n    if False:\n        i = 10\n    if isinstance(snode.node, (ir.AllReduce, ir.AllReduceCoalesced)):\n        return NCCL_COLL.ALL_REDUCE\n    elif isinstance(snode.node, (ir.AllGatherIntoTensor, ir.AllGatherIntoTensorCoalesced)):\n        return NCCL_COLL.ALL_GATHER\n    elif isinstance(snode.node, (ir.ReduceScatterTensor, ir.ReduceScatterTensorCoalesced)):\n        return NCCL_COLL.REDUCE_SCATTER\n    else:\n        raise Exception(f'Unsupported collective type: {snode.node}')",
            "def get_collective_type(snode: 'BaseSchedulerNode') -> NCCL_COLL:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(snode.node, (ir.AllReduce, ir.AllReduceCoalesced)):\n        return NCCL_COLL.ALL_REDUCE\n    elif isinstance(snode.node, (ir.AllGatherIntoTensor, ir.AllGatherIntoTensorCoalesced)):\n        return NCCL_COLL.ALL_GATHER\n    elif isinstance(snode.node, (ir.ReduceScatterTensor, ir.ReduceScatterTensorCoalesced)):\n        return NCCL_COLL.REDUCE_SCATTER\n    else:\n        raise Exception(f'Unsupported collective type: {snode.node}')",
            "def get_collective_type(snode: 'BaseSchedulerNode') -> NCCL_COLL:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(snode.node, (ir.AllReduce, ir.AllReduceCoalesced)):\n        return NCCL_COLL.ALL_REDUCE\n    elif isinstance(snode.node, (ir.AllGatherIntoTensor, ir.AllGatherIntoTensorCoalesced)):\n        return NCCL_COLL.ALL_GATHER\n    elif isinstance(snode.node, (ir.ReduceScatterTensor, ir.ReduceScatterTensorCoalesced)):\n        return NCCL_COLL.REDUCE_SCATTER\n    else:\n        raise Exception(f'Unsupported collective type: {snode.node}')",
            "def get_collective_type(snode: 'BaseSchedulerNode') -> NCCL_COLL:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(snode.node, (ir.AllReduce, ir.AllReduceCoalesced)):\n        return NCCL_COLL.ALL_REDUCE\n    elif isinstance(snode.node, (ir.AllGatherIntoTensor, ir.AllGatherIntoTensorCoalesced)):\n        return NCCL_COLL.ALL_GATHER\n    elif isinstance(snode.node, (ir.ReduceScatterTensor, ir.ReduceScatterTensorCoalesced)):\n        return NCCL_COLL.REDUCE_SCATTER\n    else:\n        raise Exception(f'Unsupported collective type: {snode.node}')",
            "def get_collective_type(snode: 'BaseSchedulerNode') -> NCCL_COLL:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(snode.node, (ir.AllReduce, ir.AllReduceCoalesced)):\n        return NCCL_COLL.ALL_REDUCE\n    elif isinstance(snode.node, (ir.AllGatherIntoTensor, ir.AllGatherIntoTensorCoalesced)):\n        return NCCL_COLL.ALL_GATHER\n    elif isinstance(snode.node, (ir.ReduceScatterTensor, ir.ReduceScatterTensorCoalesced)):\n        return NCCL_COLL.REDUCE_SCATTER\n    else:\n        raise Exception(f'Unsupported collective type: {snode.node}')"
        ]
    },
    {
        "func_name": "estimate_nccl_collective_runtime",
        "original": "def estimate_nccl_collective_runtime(snode: 'BaseSchedulerNode') -> float:\n    \"\"\"\n    Returns estimated NCCL collective runtime in nanoseconds (ns).\n\n    The following heuristics are copied from https://github.com/NVIDIA/nccl/blob/master/src/graph/tuning.cc.\n    We aim to estimate the runtime as accurately as possible.\n\n    Assumptions:\n    - only ring algorithm (NCCL_ALGO_RING) is used\n    - only Low-Latency protocol (NCCL_PROTO_LL) is used, i.e. Simple or LL128 is not used\n    - 8 gpus per node  # TODO: Need to find a way to get accurate \"gpus per node\" and \"# nodes\" info.\n    - collective is one of: allreduce, reducescatter, allgather\n    \"\"\"\n    tensor_numel = V.graph.sizevars.size_hint(sympy_product(snode.node.layout.size))\n    tensor_dtype = snode.node.layout.dtype\n    tensor_storage_size_bytes = tensor_numel * get_dtype_size(tensor_dtype)\n    tensor_storage_size_GB = tensor_storage_size_bytes / 1024 / 1024 / 1024\n    num_gpus_per_node = 8\n    (_, _, group_size) = snode.node.constant_args\n    nNodes = math.ceil(group_size / num_gpus_per_node)\n    nRanks = group_size\n    if nRanks <= 1:\n        return 0\n    nccl_algo = NCCL_ALGO.RING\n    nccl_proto = NCCL_PROTO.LL\n    coll = get_collective_type(snode)\n    bwIntra = torch._inductor.config.intra_node_bw\n    bwInter = torch._inductor.config.inter_node_bw\n    compCapIndex = get_gpu_type()\n    index2 = nNodes - 1 if nNodes <= 2 else 2\n    index1 = compCapIndex if nNodes == 1 else 0\n    llMaxBw = llMaxBws[index1][index2].item()\n    bw = bwIntra if nNodes == 1 else bwInter\n    nChannels = 2\n    busBw = nChannels * bw\n    busBw = min(llMaxBw, busBw * (1.0 / 4.0 if nNodes > 1 or coll == NCCL_COLL.ALL_REDUCE else 1.0 / 3.0))\n    if coll == NCCL_COLL.ALL_REDUCE:\n        nsteps = 2 * (nRanks - 1)\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nsteps = nRanks - 1\n    ratio = 1.0 * nRanks / nsteps\n    bandwidth = busBw * ratio\n    bandwidth_GB_per_ns = bandwidth / 1000000000.0\n    intraHw = NCCL_HW.NVLINK\n    hw = intraHw if nNodes == 1 else NCCL_HW.NET\n    if coll == NCCL_COLL.ALL_REDUCE:\n        if nNodes > 1:\n            nInterSteps = 2 * nNodes\n        else:\n            nInterSteps = 0\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nInterSteps = nNodes - 1\n    latency = baseLat[nccl_algo][nccl_proto].item()\n    intraLat = hwLat[intraHw][nccl_algo][nccl_proto].item()\n    interLat = hwLat[NCCL_HW.NET][nccl_algo][nccl_proto].item()\n    netOverhead = 0.0\n    if nNodes > 1:\n        netOverhead = 1.0\n    intraLat = max(intraLat, netOverhead)\n    latency += (nsteps - nInterSteps) * intraLat + nInterSteps * interLat\n    latency_ns = latency * 1000.0\n    transport_ns = tensor_storage_size_GB / bandwidth_GB_per_ns\n    return transport_ns + latency_ns",
        "mutated": [
            "def estimate_nccl_collective_runtime(snode: 'BaseSchedulerNode') -> float:\n    if False:\n        i = 10\n    '\\n    Returns estimated NCCL collective runtime in nanoseconds (ns).\\n\\n    The following heuristics are copied from https://github.com/NVIDIA/nccl/blob/master/src/graph/tuning.cc.\\n    We aim to estimate the runtime as accurately as possible.\\n\\n    Assumptions:\\n    - only ring algorithm (NCCL_ALGO_RING) is used\\n    - only Low-Latency protocol (NCCL_PROTO_LL) is used, i.e. Simple or LL128 is not used\\n    - 8 gpus per node  # TODO: Need to find a way to get accurate \"gpus per node\" and \"# nodes\" info.\\n    - collective is one of: allreduce, reducescatter, allgather\\n    '\n    tensor_numel = V.graph.sizevars.size_hint(sympy_product(snode.node.layout.size))\n    tensor_dtype = snode.node.layout.dtype\n    tensor_storage_size_bytes = tensor_numel * get_dtype_size(tensor_dtype)\n    tensor_storage_size_GB = tensor_storage_size_bytes / 1024 / 1024 / 1024\n    num_gpus_per_node = 8\n    (_, _, group_size) = snode.node.constant_args\n    nNodes = math.ceil(group_size / num_gpus_per_node)\n    nRanks = group_size\n    if nRanks <= 1:\n        return 0\n    nccl_algo = NCCL_ALGO.RING\n    nccl_proto = NCCL_PROTO.LL\n    coll = get_collective_type(snode)\n    bwIntra = torch._inductor.config.intra_node_bw\n    bwInter = torch._inductor.config.inter_node_bw\n    compCapIndex = get_gpu_type()\n    index2 = nNodes - 1 if nNodes <= 2 else 2\n    index1 = compCapIndex if nNodes == 1 else 0\n    llMaxBw = llMaxBws[index1][index2].item()\n    bw = bwIntra if nNodes == 1 else bwInter\n    nChannels = 2\n    busBw = nChannels * bw\n    busBw = min(llMaxBw, busBw * (1.0 / 4.0 if nNodes > 1 or coll == NCCL_COLL.ALL_REDUCE else 1.0 / 3.0))\n    if coll == NCCL_COLL.ALL_REDUCE:\n        nsteps = 2 * (nRanks - 1)\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nsteps = nRanks - 1\n    ratio = 1.0 * nRanks / nsteps\n    bandwidth = busBw * ratio\n    bandwidth_GB_per_ns = bandwidth / 1000000000.0\n    intraHw = NCCL_HW.NVLINK\n    hw = intraHw if nNodes == 1 else NCCL_HW.NET\n    if coll == NCCL_COLL.ALL_REDUCE:\n        if nNodes > 1:\n            nInterSteps = 2 * nNodes\n        else:\n            nInterSteps = 0\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nInterSteps = nNodes - 1\n    latency = baseLat[nccl_algo][nccl_proto].item()\n    intraLat = hwLat[intraHw][nccl_algo][nccl_proto].item()\n    interLat = hwLat[NCCL_HW.NET][nccl_algo][nccl_proto].item()\n    netOverhead = 0.0\n    if nNodes > 1:\n        netOverhead = 1.0\n    intraLat = max(intraLat, netOverhead)\n    latency += (nsteps - nInterSteps) * intraLat + nInterSteps * interLat\n    latency_ns = latency * 1000.0\n    transport_ns = tensor_storage_size_GB / bandwidth_GB_per_ns\n    return transport_ns + latency_ns",
            "def estimate_nccl_collective_runtime(snode: 'BaseSchedulerNode') -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Returns estimated NCCL collective runtime in nanoseconds (ns).\\n\\n    The following heuristics are copied from https://github.com/NVIDIA/nccl/blob/master/src/graph/tuning.cc.\\n    We aim to estimate the runtime as accurately as possible.\\n\\n    Assumptions:\\n    - only ring algorithm (NCCL_ALGO_RING) is used\\n    - only Low-Latency protocol (NCCL_PROTO_LL) is used, i.e. Simple or LL128 is not used\\n    - 8 gpus per node  # TODO: Need to find a way to get accurate \"gpus per node\" and \"# nodes\" info.\\n    - collective is one of: allreduce, reducescatter, allgather\\n    '\n    tensor_numel = V.graph.sizevars.size_hint(sympy_product(snode.node.layout.size))\n    tensor_dtype = snode.node.layout.dtype\n    tensor_storage_size_bytes = tensor_numel * get_dtype_size(tensor_dtype)\n    tensor_storage_size_GB = tensor_storage_size_bytes / 1024 / 1024 / 1024\n    num_gpus_per_node = 8\n    (_, _, group_size) = snode.node.constant_args\n    nNodes = math.ceil(group_size / num_gpus_per_node)\n    nRanks = group_size\n    if nRanks <= 1:\n        return 0\n    nccl_algo = NCCL_ALGO.RING\n    nccl_proto = NCCL_PROTO.LL\n    coll = get_collective_type(snode)\n    bwIntra = torch._inductor.config.intra_node_bw\n    bwInter = torch._inductor.config.inter_node_bw\n    compCapIndex = get_gpu_type()\n    index2 = nNodes - 1 if nNodes <= 2 else 2\n    index1 = compCapIndex if nNodes == 1 else 0\n    llMaxBw = llMaxBws[index1][index2].item()\n    bw = bwIntra if nNodes == 1 else bwInter\n    nChannels = 2\n    busBw = nChannels * bw\n    busBw = min(llMaxBw, busBw * (1.0 / 4.0 if nNodes > 1 or coll == NCCL_COLL.ALL_REDUCE else 1.0 / 3.0))\n    if coll == NCCL_COLL.ALL_REDUCE:\n        nsteps = 2 * (nRanks - 1)\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nsteps = nRanks - 1\n    ratio = 1.0 * nRanks / nsteps\n    bandwidth = busBw * ratio\n    bandwidth_GB_per_ns = bandwidth / 1000000000.0\n    intraHw = NCCL_HW.NVLINK\n    hw = intraHw if nNodes == 1 else NCCL_HW.NET\n    if coll == NCCL_COLL.ALL_REDUCE:\n        if nNodes > 1:\n            nInterSteps = 2 * nNodes\n        else:\n            nInterSteps = 0\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nInterSteps = nNodes - 1\n    latency = baseLat[nccl_algo][nccl_proto].item()\n    intraLat = hwLat[intraHw][nccl_algo][nccl_proto].item()\n    interLat = hwLat[NCCL_HW.NET][nccl_algo][nccl_proto].item()\n    netOverhead = 0.0\n    if nNodes > 1:\n        netOverhead = 1.0\n    intraLat = max(intraLat, netOverhead)\n    latency += (nsteps - nInterSteps) * intraLat + nInterSteps * interLat\n    latency_ns = latency * 1000.0\n    transport_ns = tensor_storage_size_GB / bandwidth_GB_per_ns\n    return transport_ns + latency_ns",
            "def estimate_nccl_collective_runtime(snode: 'BaseSchedulerNode') -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Returns estimated NCCL collective runtime in nanoseconds (ns).\\n\\n    The following heuristics are copied from https://github.com/NVIDIA/nccl/blob/master/src/graph/tuning.cc.\\n    We aim to estimate the runtime as accurately as possible.\\n\\n    Assumptions:\\n    - only ring algorithm (NCCL_ALGO_RING) is used\\n    - only Low-Latency protocol (NCCL_PROTO_LL) is used, i.e. Simple or LL128 is not used\\n    - 8 gpus per node  # TODO: Need to find a way to get accurate \"gpus per node\" and \"# nodes\" info.\\n    - collective is one of: allreduce, reducescatter, allgather\\n    '\n    tensor_numel = V.graph.sizevars.size_hint(sympy_product(snode.node.layout.size))\n    tensor_dtype = snode.node.layout.dtype\n    tensor_storage_size_bytes = tensor_numel * get_dtype_size(tensor_dtype)\n    tensor_storage_size_GB = tensor_storage_size_bytes / 1024 / 1024 / 1024\n    num_gpus_per_node = 8\n    (_, _, group_size) = snode.node.constant_args\n    nNodes = math.ceil(group_size / num_gpus_per_node)\n    nRanks = group_size\n    if nRanks <= 1:\n        return 0\n    nccl_algo = NCCL_ALGO.RING\n    nccl_proto = NCCL_PROTO.LL\n    coll = get_collective_type(snode)\n    bwIntra = torch._inductor.config.intra_node_bw\n    bwInter = torch._inductor.config.inter_node_bw\n    compCapIndex = get_gpu_type()\n    index2 = nNodes - 1 if nNodes <= 2 else 2\n    index1 = compCapIndex if nNodes == 1 else 0\n    llMaxBw = llMaxBws[index1][index2].item()\n    bw = bwIntra if nNodes == 1 else bwInter\n    nChannels = 2\n    busBw = nChannels * bw\n    busBw = min(llMaxBw, busBw * (1.0 / 4.0 if nNodes > 1 or coll == NCCL_COLL.ALL_REDUCE else 1.0 / 3.0))\n    if coll == NCCL_COLL.ALL_REDUCE:\n        nsteps = 2 * (nRanks - 1)\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nsteps = nRanks - 1\n    ratio = 1.0 * nRanks / nsteps\n    bandwidth = busBw * ratio\n    bandwidth_GB_per_ns = bandwidth / 1000000000.0\n    intraHw = NCCL_HW.NVLINK\n    hw = intraHw if nNodes == 1 else NCCL_HW.NET\n    if coll == NCCL_COLL.ALL_REDUCE:\n        if nNodes > 1:\n            nInterSteps = 2 * nNodes\n        else:\n            nInterSteps = 0\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nInterSteps = nNodes - 1\n    latency = baseLat[nccl_algo][nccl_proto].item()\n    intraLat = hwLat[intraHw][nccl_algo][nccl_proto].item()\n    interLat = hwLat[NCCL_HW.NET][nccl_algo][nccl_proto].item()\n    netOverhead = 0.0\n    if nNodes > 1:\n        netOverhead = 1.0\n    intraLat = max(intraLat, netOverhead)\n    latency += (nsteps - nInterSteps) * intraLat + nInterSteps * interLat\n    latency_ns = latency * 1000.0\n    transport_ns = tensor_storage_size_GB / bandwidth_GB_per_ns\n    return transport_ns + latency_ns",
            "def estimate_nccl_collective_runtime(snode: 'BaseSchedulerNode') -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Returns estimated NCCL collective runtime in nanoseconds (ns).\\n\\n    The following heuristics are copied from https://github.com/NVIDIA/nccl/blob/master/src/graph/tuning.cc.\\n    We aim to estimate the runtime as accurately as possible.\\n\\n    Assumptions:\\n    - only ring algorithm (NCCL_ALGO_RING) is used\\n    - only Low-Latency protocol (NCCL_PROTO_LL) is used, i.e. Simple or LL128 is not used\\n    - 8 gpus per node  # TODO: Need to find a way to get accurate \"gpus per node\" and \"# nodes\" info.\\n    - collective is one of: allreduce, reducescatter, allgather\\n    '\n    tensor_numel = V.graph.sizevars.size_hint(sympy_product(snode.node.layout.size))\n    tensor_dtype = snode.node.layout.dtype\n    tensor_storage_size_bytes = tensor_numel * get_dtype_size(tensor_dtype)\n    tensor_storage_size_GB = tensor_storage_size_bytes / 1024 / 1024 / 1024\n    num_gpus_per_node = 8\n    (_, _, group_size) = snode.node.constant_args\n    nNodes = math.ceil(group_size / num_gpus_per_node)\n    nRanks = group_size\n    if nRanks <= 1:\n        return 0\n    nccl_algo = NCCL_ALGO.RING\n    nccl_proto = NCCL_PROTO.LL\n    coll = get_collective_type(snode)\n    bwIntra = torch._inductor.config.intra_node_bw\n    bwInter = torch._inductor.config.inter_node_bw\n    compCapIndex = get_gpu_type()\n    index2 = nNodes - 1 if nNodes <= 2 else 2\n    index1 = compCapIndex if nNodes == 1 else 0\n    llMaxBw = llMaxBws[index1][index2].item()\n    bw = bwIntra if nNodes == 1 else bwInter\n    nChannels = 2\n    busBw = nChannels * bw\n    busBw = min(llMaxBw, busBw * (1.0 / 4.0 if nNodes > 1 or coll == NCCL_COLL.ALL_REDUCE else 1.0 / 3.0))\n    if coll == NCCL_COLL.ALL_REDUCE:\n        nsteps = 2 * (nRanks - 1)\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nsteps = nRanks - 1\n    ratio = 1.0 * nRanks / nsteps\n    bandwidth = busBw * ratio\n    bandwidth_GB_per_ns = bandwidth / 1000000000.0\n    intraHw = NCCL_HW.NVLINK\n    hw = intraHw if nNodes == 1 else NCCL_HW.NET\n    if coll == NCCL_COLL.ALL_REDUCE:\n        if nNodes > 1:\n            nInterSteps = 2 * nNodes\n        else:\n            nInterSteps = 0\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nInterSteps = nNodes - 1\n    latency = baseLat[nccl_algo][nccl_proto].item()\n    intraLat = hwLat[intraHw][nccl_algo][nccl_proto].item()\n    interLat = hwLat[NCCL_HW.NET][nccl_algo][nccl_proto].item()\n    netOverhead = 0.0\n    if nNodes > 1:\n        netOverhead = 1.0\n    intraLat = max(intraLat, netOverhead)\n    latency += (nsteps - nInterSteps) * intraLat + nInterSteps * interLat\n    latency_ns = latency * 1000.0\n    transport_ns = tensor_storage_size_GB / bandwidth_GB_per_ns\n    return transport_ns + latency_ns",
            "def estimate_nccl_collective_runtime(snode: 'BaseSchedulerNode') -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Returns estimated NCCL collective runtime in nanoseconds (ns).\\n\\n    The following heuristics are copied from https://github.com/NVIDIA/nccl/blob/master/src/graph/tuning.cc.\\n    We aim to estimate the runtime as accurately as possible.\\n\\n    Assumptions:\\n    - only ring algorithm (NCCL_ALGO_RING) is used\\n    - only Low-Latency protocol (NCCL_PROTO_LL) is used, i.e. Simple or LL128 is not used\\n    - 8 gpus per node  # TODO: Need to find a way to get accurate \"gpus per node\" and \"# nodes\" info.\\n    - collective is one of: allreduce, reducescatter, allgather\\n    '\n    tensor_numel = V.graph.sizevars.size_hint(sympy_product(snode.node.layout.size))\n    tensor_dtype = snode.node.layout.dtype\n    tensor_storage_size_bytes = tensor_numel * get_dtype_size(tensor_dtype)\n    tensor_storage_size_GB = tensor_storage_size_bytes / 1024 / 1024 / 1024\n    num_gpus_per_node = 8\n    (_, _, group_size) = snode.node.constant_args\n    nNodes = math.ceil(group_size / num_gpus_per_node)\n    nRanks = group_size\n    if nRanks <= 1:\n        return 0\n    nccl_algo = NCCL_ALGO.RING\n    nccl_proto = NCCL_PROTO.LL\n    coll = get_collective_type(snode)\n    bwIntra = torch._inductor.config.intra_node_bw\n    bwInter = torch._inductor.config.inter_node_bw\n    compCapIndex = get_gpu_type()\n    index2 = nNodes - 1 if nNodes <= 2 else 2\n    index1 = compCapIndex if nNodes == 1 else 0\n    llMaxBw = llMaxBws[index1][index2].item()\n    bw = bwIntra if nNodes == 1 else bwInter\n    nChannels = 2\n    busBw = nChannels * bw\n    busBw = min(llMaxBw, busBw * (1.0 / 4.0 if nNodes > 1 or coll == NCCL_COLL.ALL_REDUCE else 1.0 / 3.0))\n    if coll == NCCL_COLL.ALL_REDUCE:\n        nsteps = 2 * (nRanks - 1)\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nsteps = nRanks - 1\n    ratio = 1.0 * nRanks / nsteps\n    bandwidth = busBw * ratio\n    bandwidth_GB_per_ns = bandwidth / 1000000000.0\n    intraHw = NCCL_HW.NVLINK\n    hw = intraHw if nNodes == 1 else NCCL_HW.NET\n    if coll == NCCL_COLL.ALL_REDUCE:\n        if nNodes > 1:\n            nInterSteps = 2 * nNodes\n        else:\n            nInterSteps = 0\n    elif coll in (NCCL_COLL.REDUCE_SCATTER, NCCL_COLL.ALL_GATHER):\n        nInterSteps = nNodes - 1\n    latency = baseLat[nccl_algo][nccl_proto].item()\n    intraLat = hwLat[intraHw][nccl_algo][nccl_proto].item()\n    interLat = hwLat[NCCL_HW.NET][nccl_algo][nccl_proto].item()\n    netOverhead = 0.0\n    if nNodes > 1:\n        netOverhead = 1.0\n    intraLat = max(intraLat, netOverhead)\n    latency += (nsteps - nInterSteps) * intraLat + nInterSteps * interLat\n    latency_ns = latency * 1000.0\n    transport_ns = tensor_storage_size_GB / bandwidth_GB_per_ns\n    return transport_ns + latency_ns"
        ]
    }
]