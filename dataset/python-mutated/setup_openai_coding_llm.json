[
    {
        "func_name": "coding_llm",
        "original": "def coding_llm(messages):\n    messages = convert_to_openai_messages(messages, function_calling=True)\n    messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n    except:\n        if interpreter.context_window:\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            if len(messages) == 1:\n                display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n    if interpreter.debug_mode:\n        print('Sending this to the OpenAI LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    response = litellm.completion(**params)\n    accumulated_deltas = {}\n    language = None\n    code = ''\n    for chunk in response:\n        if interpreter.debug_mode:\n            print('Chunk from LLM', chunk)\n        if 'choices' not in chunk or len(chunk['choices']) == 0:\n            continue\n        delta = chunk['choices'][0]['delta']\n        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n        if interpreter.debug_mode:\n            print('Accumulated deltas', accumulated_deltas)\n        if 'content' in delta and delta['content']:\n            yield {'message': delta['content']}\n        if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n            if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                arguments = accumulated_deltas['function_call']['arguments']\n                arguments = parse_partial_json(arguments)\n                if arguments:\n                    if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                        language = arguments['language']\n                        yield {'language': language}\n                    if language is not None and 'code' in arguments:\n                        code_delta = arguments['code'][len(code):]\n                        code = arguments['code']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif interpreter.debug_mode:\n                    print('Arguments not a dict.')\n            elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                if interpreter.debug_mode:\n                    print('Got direct python call')\n                if language is None:\n                    language = 'python'\n                    yield {'language': language}\n                if language is not None:\n                    code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                    code = accumulated_deltas['function_call']['arguments']\n                    if code_delta:\n                        yield {'code': code_delta}\n            elif 'name' in accumulated_deltas['function_call']:\n                print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')",
        "mutated": [
            "def coding_llm(messages):\n    if False:\n        i = 10\n    messages = convert_to_openai_messages(messages, function_calling=True)\n    messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n    except:\n        if interpreter.context_window:\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            if len(messages) == 1:\n                display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n    if interpreter.debug_mode:\n        print('Sending this to the OpenAI LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    response = litellm.completion(**params)\n    accumulated_deltas = {}\n    language = None\n    code = ''\n    for chunk in response:\n        if interpreter.debug_mode:\n            print('Chunk from LLM', chunk)\n        if 'choices' not in chunk or len(chunk['choices']) == 0:\n            continue\n        delta = chunk['choices'][0]['delta']\n        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n        if interpreter.debug_mode:\n            print('Accumulated deltas', accumulated_deltas)\n        if 'content' in delta and delta['content']:\n            yield {'message': delta['content']}\n        if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n            if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                arguments = accumulated_deltas['function_call']['arguments']\n                arguments = parse_partial_json(arguments)\n                if arguments:\n                    if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                        language = arguments['language']\n                        yield {'language': language}\n                    if language is not None and 'code' in arguments:\n                        code_delta = arguments['code'][len(code):]\n                        code = arguments['code']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif interpreter.debug_mode:\n                    print('Arguments not a dict.')\n            elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                if interpreter.debug_mode:\n                    print('Got direct python call')\n                if language is None:\n                    language = 'python'\n                    yield {'language': language}\n                if language is not None:\n                    code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                    code = accumulated_deltas['function_call']['arguments']\n                    if code_delta:\n                        yield {'code': code_delta}\n            elif 'name' in accumulated_deltas['function_call']:\n                print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')",
            "def coding_llm(messages):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    messages = convert_to_openai_messages(messages, function_calling=True)\n    messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n    except:\n        if interpreter.context_window:\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            if len(messages) == 1:\n                display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n    if interpreter.debug_mode:\n        print('Sending this to the OpenAI LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    response = litellm.completion(**params)\n    accumulated_deltas = {}\n    language = None\n    code = ''\n    for chunk in response:\n        if interpreter.debug_mode:\n            print('Chunk from LLM', chunk)\n        if 'choices' not in chunk or len(chunk['choices']) == 0:\n            continue\n        delta = chunk['choices'][0]['delta']\n        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n        if interpreter.debug_mode:\n            print('Accumulated deltas', accumulated_deltas)\n        if 'content' in delta and delta['content']:\n            yield {'message': delta['content']}\n        if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n            if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                arguments = accumulated_deltas['function_call']['arguments']\n                arguments = parse_partial_json(arguments)\n                if arguments:\n                    if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                        language = arguments['language']\n                        yield {'language': language}\n                    if language is not None and 'code' in arguments:\n                        code_delta = arguments['code'][len(code):]\n                        code = arguments['code']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif interpreter.debug_mode:\n                    print('Arguments not a dict.')\n            elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                if interpreter.debug_mode:\n                    print('Got direct python call')\n                if language is None:\n                    language = 'python'\n                    yield {'language': language}\n                if language is not None:\n                    code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                    code = accumulated_deltas['function_call']['arguments']\n                    if code_delta:\n                        yield {'code': code_delta}\n            elif 'name' in accumulated_deltas['function_call']:\n                print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')",
            "def coding_llm(messages):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    messages = convert_to_openai_messages(messages, function_calling=True)\n    messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n    except:\n        if interpreter.context_window:\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            if len(messages) == 1:\n                display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n    if interpreter.debug_mode:\n        print('Sending this to the OpenAI LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    response = litellm.completion(**params)\n    accumulated_deltas = {}\n    language = None\n    code = ''\n    for chunk in response:\n        if interpreter.debug_mode:\n            print('Chunk from LLM', chunk)\n        if 'choices' not in chunk or len(chunk['choices']) == 0:\n            continue\n        delta = chunk['choices'][0]['delta']\n        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n        if interpreter.debug_mode:\n            print('Accumulated deltas', accumulated_deltas)\n        if 'content' in delta and delta['content']:\n            yield {'message': delta['content']}\n        if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n            if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                arguments = accumulated_deltas['function_call']['arguments']\n                arguments = parse_partial_json(arguments)\n                if arguments:\n                    if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                        language = arguments['language']\n                        yield {'language': language}\n                    if language is not None and 'code' in arguments:\n                        code_delta = arguments['code'][len(code):]\n                        code = arguments['code']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif interpreter.debug_mode:\n                    print('Arguments not a dict.')\n            elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                if interpreter.debug_mode:\n                    print('Got direct python call')\n                if language is None:\n                    language = 'python'\n                    yield {'language': language}\n                if language is not None:\n                    code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                    code = accumulated_deltas['function_call']['arguments']\n                    if code_delta:\n                        yield {'code': code_delta}\n            elif 'name' in accumulated_deltas['function_call']:\n                print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')",
            "def coding_llm(messages):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    messages = convert_to_openai_messages(messages, function_calling=True)\n    messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n    except:\n        if interpreter.context_window:\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            if len(messages) == 1:\n                display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n    if interpreter.debug_mode:\n        print('Sending this to the OpenAI LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    response = litellm.completion(**params)\n    accumulated_deltas = {}\n    language = None\n    code = ''\n    for chunk in response:\n        if interpreter.debug_mode:\n            print('Chunk from LLM', chunk)\n        if 'choices' not in chunk or len(chunk['choices']) == 0:\n            continue\n        delta = chunk['choices'][0]['delta']\n        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n        if interpreter.debug_mode:\n            print('Accumulated deltas', accumulated_deltas)\n        if 'content' in delta and delta['content']:\n            yield {'message': delta['content']}\n        if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n            if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                arguments = accumulated_deltas['function_call']['arguments']\n                arguments = parse_partial_json(arguments)\n                if arguments:\n                    if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                        language = arguments['language']\n                        yield {'language': language}\n                    if language is not None and 'code' in arguments:\n                        code_delta = arguments['code'][len(code):]\n                        code = arguments['code']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif interpreter.debug_mode:\n                    print('Arguments not a dict.')\n            elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                if interpreter.debug_mode:\n                    print('Got direct python call')\n                if language is None:\n                    language = 'python'\n                    yield {'language': language}\n                if language is not None:\n                    code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                    code = accumulated_deltas['function_call']['arguments']\n                    if code_delta:\n                        yield {'code': code_delta}\n            elif 'name' in accumulated_deltas['function_call']:\n                print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')",
            "def coding_llm(messages):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    messages = convert_to_openai_messages(messages, function_calling=True)\n    messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n    system_message = messages[0]['content']\n    messages = messages[1:]\n    try:\n        messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n    except:\n        if interpreter.context_window:\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n        else:\n            if len(messages) == 1:\n                display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n            messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n    if interpreter.debug_mode:\n        print('Sending this to the OpenAI LLM:', messages)\n    params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n    if interpreter.api_base:\n        params['api_base'] = interpreter.api_base\n    if interpreter.api_key:\n        params['api_key'] = interpreter.api_key\n    if interpreter.max_tokens:\n        params['max_tokens'] = interpreter.max_tokens\n    if interpreter.temperature is not None:\n        params['temperature'] = interpreter.temperature\n    else:\n        params['temperature'] = 0.0\n    if interpreter.max_budget:\n        litellm.max_budget = interpreter.max_budget\n    if interpreter.debug_mode:\n        litellm.set_verbose = True\n    if interpreter.debug_mode:\n        print('Sending this to LiteLLM:', params)\n    response = litellm.completion(**params)\n    accumulated_deltas = {}\n    language = None\n    code = ''\n    for chunk in response:\n        if interpreter.debug_mode:\n            print('Chunk from LLM', chunk)\n        if 'choices' not in chunk or len(chunk['choices']) == 0:\n            continue\n        delta = chunk['choices'][0]['delta']\n        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n        if interpreter.debug_mode:\n            print('Accumulated deltas', accumulated_deltas)\n        if 'content' in delta and delta['content']:\n            yield {'message': delta['content']}\n        if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n            if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                arguments = accumulated_deltas['function_call']['arguments']\n                arguments = parse_partial_json(arguments)\n                if arguments:\n                    if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                        language = arguments['language']\n                        yield {'language': language}\n                    if language is not None and 'code' in arguments:\n                        code_delta = arguments['code'][len(code):]\n                        code = arguments['code']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif interpreter.debug_mode:\n                    print('Arguments not a dict.')\n            elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                if interpreter.debug_mode:\n                    print('Got direct python call')\n                if language is None:\n                    language = 'python'\n                    yield {'language': language}\n                if language is not None:\n                    code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                    code = accumulated_deltas['function_call']['arguments']\n                    if code_delta:\n                        yield {'code': code_delta}\n            elif 'name' in accumulated_deltas['function_call']:\n                print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')"
        ]
    },
    {
        "func_name": "setup_openai_coding_llm",
        "original": "def setup_openai_coding_llm(interpreter):\n    \"\"\"\n    Takes an Interpreter (which includes a ton of LLM settings),\n    returns a OI Coding LLM (a generator that takes OI messages and streams deltas with `message`, `language`, and `code`).\n    \"\"\"\n\n    def coding_llm(messages):\n        messages = convert_to_openai_messages(messages, function_calling=True)\n        messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n        except:\n            if interpreter.context_window:\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n        if interpreter.debug_mode:\n            print('Sending this to the OpenAI LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        response = litellm.completion(**params)\n        accumulated_deltas = {}\n        language = None\n        code = ''\n        for chunk in response:\n            if interpreter.debug_mode:\n                print('Chunk from LLM', chunk)\n            if 'choices' not in chunk or len(chunk['choices']) == 0:\n                continue\n            delta = chunk['choices'][0]['delta']\n            accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n            if interpreter.debug_mode:\n                print('Accumulated deltas', accumulated_deltas)\n            if 'content' in delta and delta['content']:\n                yield {'message': delta['content']}\n            if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n                if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                    arguments = accumulated_deltas['function_call']['arguments']\n                    arguments = parse_partial_json(arguments)\n                    if arguments:\n                        if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                            language = arguments['language']\n                            yield {'language': language}\n                        if language is not None and 'code' in arguments:\n                            code_delta = arguments['code'][len(code):]\n                            code = arguments['code']\n                            if code_delta:\n                                yield {'code': code_delta}\n                    elif interpreter.debug_mode:\n                        print('Arguments not a dict.')\n                elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                    if interpreter.debug_mode:\n                        print('Got direct python call')\n                    if language is None:\n                        language = 'python'\n                        yield {'language': language}\n                    if language is not None:\n                        code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                        code = accumulated_deltas['function_call']['arguments']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif 'name' in accumulated_deltas['function_call']:\n                    print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')\n    return coding_llm",
        "mutated": [
            "def setup_openai_coding_llm(interpreter):\n    if False:\n        i = 10\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a OI Coding LLM (a generator that takes OI messages and streams deltas with `message`, `language`, and `code`).\\n    '\n\n    def coding_llm(messages):\n        messages = convert_to_openai_messages(messages, function_calling=True)\n        messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n        except:\n            if interpreter.context_window:\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n        if interpreter.debug_mode:\n            print('Sending this to the OpenAI LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        response = litellm.completion(**params)\n        accumulated_deltas = {}\n        language = None\n        code = ''\n        for chunk in response:\n            if interpreter.debug_mode:\n                print('Chunk from LLM', chunk)\n            if 'choices' not in chunk or len(chunk['choices']) == 0:\n                continue\n            delta = chunk['choices'][0]['delta']\n            accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n            if interpreter.debug_mode:\n                print('Accumulated deltas', accumulated_deltas)\n            if 'content' in delta and delta['content']:\n                yield {'message': delta['content']}\n            if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n                if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                    arguments = accumulated_deltas['function_call']['arguments']\n                    arguments = parse_partial_json(arguments)\n                    if arguments:\n                        if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                            language = arguments['language']\n                            yield {'language': language}\n                        if language is not None and 'code' in arguments:\n                            code_delta = arguments['code'][len(code):]\n                            code = arguments['code']\n                            if code_delta:\n                                yield {'code': code_delta}\n                    elif interpreter.debug_mode:\n                        print('Arguments not a dict.')\n                elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                    if interpreter.debug_mode:\n                        print('Got direct python call')\n                    if language is None:\n                        language = 'python'\n                        yield {'language': language}\n                    if language is not None:\n                        code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                        code = accumulated_deltas['function_call']['arguments']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif 'name' in accumulated_deltas['function_call']:\n                    print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')\n    return coding_llm",
            "def setup_openai_coding_llm(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a OI Coding LLM (a generator that takes OI messages and streams deltas with `message`, `language`, and `code`).\\n    '\n\n    def coding_llm(messages):\n        messages = convert_to_openai_messages(messages, function_calling=True)\n        messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n        except:\n            if interpreter.context_window:\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n        if interpreter.debug_mode:\n            print('Sending this to the OpenAI LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        response = litellm.completion(**params)\n        accumulated_deltas = {}\n        language = None\n        code = ''\n        for chunk in response:\n            if interpreter.debug_mode:\n                print('Chunk from LLM', chunk)\n            if 'choices' not in chunk or len(chunk['choices']) == 0:\n                continue\n            delta = chunk['choices'][0]['delta']\n            accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n            if interpreter.debug_mode:\n                print('Accumulated deltas', accumulated_deltas)\n            if 'content' in delta and delta['content']:\n                yield {'message': delta['content']}\n            if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n                if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                    arguments = accumulated_deltas['function_call']['arguments']\n                    arguments = parse_partial_json(arguments)\n                    if arguments:\n                        if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                            language = arguments['language']\n                            yield {'language': language}\n                        if language is not None and 'code' in arguments:\n                            code_delta = arguments['code'][len(code):]\n                            code = arguments['code']\n                            if code_delta:\n                                yield {'code': code_delta}\n                    elif interpreter.debug_mode:\n                        print('Arguments not a dict.')\n                elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                    if interpreter.debug_mode:\n                        print('Got direct python call')\n                    if language is None:\n                        language = 'python'\n                        yield {'language': language}\n                    if language is not None:\n                        code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                        code = accumulated_deltas['function_call']['arguments']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif 'name' in accumulated_deltas['function_call']:\n                    print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')\n    return coding_llm",
            "def setup_openai_coding_llm(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a OI Coding LLM (a generator that takes OI messages and streams deltas with `message`, `language`, and `code`).\\n    '\n\n    def coding_llm(messages):\n        messages = convert_to_openai_messages(messages, function_calling=True)\n        messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n        except:\n            if interpreter.context_window:\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n        if interpreter.debug_mode:\n            print('Sending this to the OpenAI LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        response = litellm.completion(**params)\n        accumulated_deltas = {}\n        language = None\n        code = ''\n        for chunk in response:\n            if interpreter.debug_mode:\n                print('Chunk from LLM', chunk)\n            if 'choices' not in chunk or len(chunk['choices']) == 0:\n                continue\n            delta = chunk['choices'][0]['delta']\n            accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n            if interpreter.debug_mode:\n                print('Accumulated deltas', accumulated_deltas)\n            if 'content' in delta and delta['content']:\n                yield {'message': delta['content']}\n            if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n                if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                    arguments = accumulated_deltas['function_call']['arguments']\n                    arguments = parse_partial_json(arguments)\n                    if arguments:\n                        if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                            language = arguments['language']\n                            yield {'language': language}\n                        if language is not None and 'code' in arguments:\n                            code_delta = arguments['code'][len(code):]\n                            code = arguments['code']\n                            if code_delta:\n                                yield {'code': code_delta}\n                    elif interpreter.debug_mode:\n                        print('Arguments not a dict.')\n                elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                    if interpreter.debug_mode:\n                        print('Got direct python call')\n                    if language is None:\n                        language = 'python'\n                        yield {'language': language}\n                    if language is not None:\n                        code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                        code = accumulated_deltas['function_call']['arguments']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif 'name' in accumulated_deltas['function_call']:\n                    print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')\n    return coding_llm",
            "def setup_openai_coding_llm(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a OI Coding LLM (a generator that takes OI messages and streams deltas with `message`, `language`, and `code`).\\n    '\n\n    def coding_llm(messages):\n        messages = convert_to_openai_messages(messages, function_calling=True)\n        messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n        except:\n            if interpreter.context_window:\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n        if interpreter.debug_mode:\n            print('Sending this to the OpenAI LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        response = litellm.completion(**params)\n        accumulated_deltas = {}\n        language = None\n        code = ''\n        for chunk in response:\n            if interpreter.debug_mode:\n                print('Chunk from LLM', chunk)\n            if 'choices' not in chunk or len(chunk['choices']) == 0:\n                continue\n            delta = chunk['choices'][0]['delta']\n            accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n            if interpreter.debug_mode:\n                print('Accumulated deltas', accumulated_deltas)\n            if 'content' in delta and delta['content']:\n                yield {'message': delta['content']}\n            if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n                if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                    arguments = accumulated_deltas['function_call']['arguments']\n                    arguments = parse_partial_json(arguments)\n                    if arguments:\n                        if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                            language = arguments['language']\n                            yield {'language': language}\n                        if language is not None and 'code' in arguments:\n                            code_delta = arguments['code'][len(code):]\n                            code = arguments['code']\n                            if code_delta:\n                                yield {'code': code_delta}\n                    elif interpreter.debug_mode:\n                        print('Arguments not a dict.')\n                elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                    if interpreter.debug_mode:\n                        print('Got direct python call')\n                    if language is None:\n                        language = 'python'\n                        yield {'language': language}\n                    if language is not None:\n                        code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                        code = accumulated_deltas['function_call']['arguments']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif 'name' in accumulated_deltas['function_call']:\n                    print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')\n    return coding_llm",
            "def setup_openai_coding_llm(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Takes an Interpreter (which includes a ton of LLM settings),\\n    returns a OI Coding LLM (a generator that takes OI messages and streams deltas with `message`, `language`, and `code`).\\n    '\n\n    def coding_llm(messages):\n        messages = convert_to_openai_messages(messages, function_calling=True)\n        messages[0]['content'] += '\\n\\nOnly use the function you have been provided with.'\n        system_message = messages[0]['content']\n        messages = messages[1:]\n        try:\n            messages = tt.trim(messages=messages, system_message=system_message, model=interpreter.model)\n        except:\n            if interpreter.context_window:\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=interpreter.context_window)\n            else:\n                if len(messages) == 1:\n                    display_markdown_message('\\n                    **We were unable to determine the context window of this model.** Defaulting to 3000.\\n                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.\\n                    ')\n                messages = tt.trim(messages=messages, system_message=system_message, max_tokens=3000)\n        if interpreter.debug_mode:\n            print('Sending this to the OpenAI LLM:', messages)\n        params = {'model': interpreter.model, 'messages': messages, 'stream': True, 'functions': [function_schema]}\n        if interpreter.api_base:\n            params['api_base'] = interpreter.api_base\n        if interpreter.api_key:\n            params['api_key'] = interpreter.api_key\n        if interpreter.max_tokens:\n            params['max_tokens'] = interpreter.max_tokens\n        if interpreter.temperature is not None:\n            params['temperature'] = interpreter.temperature\n        else:\n            params['temperature'] = 0.0\n        if interpreter.max_budget:\n            litellm.max_budget = interpreter.max_budget\n        if interpreter.debug_mode:\n            litellm.set_verbose = True\n        if interpreter.debug_mode:\n            print('Sending this to LiteLLM:', params)\n        response = litellm.completion(**params)\n        accumulated_deltas = {}\n        language = None\n        code = ''\n        for chunk in response:\n            if interpreter.debug_mode:\n                print('Chunk from LLM', chunk)\n            if 'choices' not in chunk or len(chunk['choices']) == 0:\n                continue\n            delta = chunk['choices'][0]['delta']\n            accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n            if interpreter.debug_mode:\n                print('Accumulated deltas', accumulated_deltas)\n            if 'content' in delta and delta['content']:\n                yield {'message': delta['content']}\n            if 'function_call' in accumulated_deltas and 'arguments' in accumulated_deltas['function_call']:\n                if 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'execute':\n                    arguments = accumulated_deltas['function_call']['arguments']\n                    arguments = parse_partial_json(arguments)\n                    if arguments:\n                        if language is None and 'language' in arguments and ('code' in arguments) and arguments['language']:\n                            language = arguments['language']\n                            yield {'language': language}\n                        if language is not None and 'code' in arguments:\n                            code_delta = arguments['code'][len(code):]\n                            code = arguments['code']\n                            if code_delta:\n                                yield {'code': code_delta}\n                    elif interpreter.debug_mode:\n                        print('Arguments not a dict.')\n                elif 'name' in accumulated_deltas['function_call'] and accumulated_deltas['function_call']['name'] == 'python':\n                    if interpreter.debug_mode:\n                        print('Got direct python call')\n                    if language is None:\n                        language = 'python'\n                        yield {'language': language}\n                    if language is not None:\n                        code_delta = accumulated_deltas['function_call']['arguments'][len(code):]\n                        code = accumulated_deltas['function_call']['arguments']\n                        if code_delta:\n                            yield {'code': code_delta}\n                elif 'name' in accumulated_deltas['function_call']:\n                    print('Encountered an unexpected function call: ', accumulated_deltas['function_call'], '\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter')\n    return coding_llm"
        ]
    }
]