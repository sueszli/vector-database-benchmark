[
    {
        "func_name": "__init__",
        "original": "def __init__(self, op_type):\n    super().__init__(op_type)",
        "mutated": [
            "def __init__(self, op_type):\n    if False:\n        i = 10\n    super().__init__(op_type)",
            "def __init__(self, op_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(op_type)",
            "def __init__(self, op_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(op_type)",
            "def __init__(self, op_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(op_type)",
            "def __init__(self, op_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(op_type)"
        ]
    },
    {
        "func_name": "update_dims_mapping",
        "original": "@staticmethod\ndef update_dims_mapping(dist_op):\n    op_desc = dist_op.serial_op.desc\n    assert dist_op.serial_op.type == 'lookup_table_v2', f'{dist_op.serial_op.type} is not supported by dist embedding yet.'\n    x_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    padding_idx = op_desc.attr('padding_idx')\n    is_sparse = op_desc.attr('is_sparse')\n    x_spec = get_dist_tensor_spec(dist_op, x_name)\n    w_spec = get_dist_tensor_spec(dist_op, w_name)\n    output_spec = get_dist_tensor_spec(dist_op, out_name, False)\n    rule = get_phi_spmd_rule('embedding')\n    fw_results = rule.infer_forward(x_spec, w_spec, padding_idx, is_sparse)\n    bw_results = rule.infer_backward(x_spec, w_spec, output_spec, padding_idx, is_sparse)\n    changed = update_op_dims_mapping(dist_op, [x_name, w_name], [out_name], fw_results, bw_results)\n    return changed",
        "mutated": [
            "@staticmethod\ndef update_dims_mapping(dist_op):\n    if False:\n        i = 10\n    op_desc = dist_op.serial_op.desc\n    assert dist_op.serial_op.type == 'lookup_table_v2', f'{dist_op.serial_op.type} is not supported by dist embedding yet.'\n    x_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    padding_idx = op_desc.attr('padding_idx')\n    is_sparse = op_desc.attr('is_sparse')\n    x_spec = get_dist_tensor_spec(dist_op, x_name)\n    w_spec = get_dist_tensor_spec(dist_op, w_name)\n    output_spec = get_dist_tensor_spec(dist_op, out_name, False)\n    rule = get_phi_spmd_rule('embedding')\n    fw_results = rule.infer_forward(x_spec, w_spec, padding_idx, is_sparse)\n    bw_results = rule.infer_backward(x_spec, w_spec, output_spec, padding_idx, is_sparse)\n    changed = update_op_dims_mapping(dist_op, [x_name, w_name], [out_name], fw_results, bw_results)\n    return changed",
            "@staticmethod\ndef update_dims_mapping(dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    op_desc = dist_op.serial_op.desc\n    assert dist_op.serial_op.type == 'lookup_table_v2', f'{dist_op.serial_op.type} is not supported by dist embedding yet.'\n    x_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    padding_idx = op_desc.attr('padding_idx')\n    is_sparse = op_desc.attr('is_sparse')\n    x_spec = get_dist_tensor_spec(dist_op, x_name)\n    w_spec = get_dist_tensor_spec(dist_op, w_name)\n    output_spec = get_dist_tensor_spec(dist_op, out_name, False)\n    rule = get_phi_spmd_rule('embedding')\n    fw_results = rule.infer_forward(x_spec, w_spec, padding_idx, is_sparse)\n    bw_results = rule.infer_backward(x_spec, w_spec, output_spec, padding_idx, is_sparse)\n    changed = update_op_dims_mapping(dist_op, [x_name, w_name], [out_name], fw_results, bw_results)\n    return changed",
            "@staticmethod\ndef update_dims_mapping(dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    op_desc = dist_op.serial_op.desc\n    assert dist_op.serial_op.type == 'lookup_table_v2', f'{dist_op.serial_op.type} is not supported by dist embedding yet.'\n    x_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    padding_idx = op_desc.attr('padding_idx')\n    is_sparse = op_desc.attr('is_sparse')\n    x_spec = get_dist_tensor_spec(dist_op, x_name)\n    w_spec = get_dist_tensor_spec(dist_op, w_name)\n    output_spec = get_dist_tensor_spec(dist_op, out_name, False)\n    rule = get_phi_spmd_rule('embedding')\n    fw_results = rule.infer_forward(x_spec, w_spec, padding_idx, is_sparse)\n    bw_results = rule.infer_backward(x_spec, w_spec, output_spec, padding_idx, is_sparse)\n    changed = update_op_dims_mapping(dist_op, [x_name, w_name], [out_name], fw_results, bw_results)\n    return changed",
            "@staticmethod\ndef update_dims_mapping(dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    op_desc = dist_op.serial_op.desc\n    assert dist_op.serial_op.type == 'lookup_table_v2', f'{dist_op.serial_op.type} is not supported by dist embedding yet.'\n    x_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    padding_idx = op_desc.attr('padding_idx')\n    is_sparse = op_desc.attr('is_sparse')\n    x_spec = get_dist_tensor_spec(dist_op, x_name)\n    w_spec = get_dist_tensor_spec(dist_op, w_name)\n    output_spec = get_dist_tensor_spec(dist_op, out_name, False)\n    rule = get_phi_spmd_rule('embedding')\n    fw_results = rule.infer_forward(x_spec, w_spec, padding_idx, is_sparse)\n    bw_results = rule.infer_backward(x_spec, w_spec, output_spec, padding_idx, is_sparse)\n    changed = update_op_dims_mapping(dist_op, [x_name, w_name], [out_name], fw_results, bw_results)\n    return changed",
            "@staticmethod\ndef update_dims_mapping(dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    op_desc = dist_op.serial_op.desc\n    assert dist_op.serial_op.type == 'lookup_table_v2', f'{dist_op.serial_op.type} is not supported by dist embedding yet.'\n    x_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    padding_idx = op_desc.attr('padding_idx')\n    is_sparse = op_desc.attr('is_sparse')\n    x_spec = get_dist_tensor_spec(dist_op, x_name)\n    w_spec = get_dist_tensor_spec(dist_op, w_name)\n    output_spec = get_dist_tensor_spec(dist_op, out_name, False)\n    rule = get_phi_spmd_rule('embedding')\n    fw_results = rule.infer_forward(x_spec, w_spec, padding_idx, is_sparse)\n    bw_results = rule.infer_backward(x_spec, w_spec, output_spec, padding_idx, is_sparse)\n    changed = update_op_dims_mapping(dist_op, [x_name, w_name], [out_name], fw_results, bw_results)\n    return changed"
        ]
    },
    {
        "func_name": "mapping_to_dist_operator_impl",
        "original": "@staticmethod\ndef mapping_to_dist_operator_impl(dist_op, original_op_dist_attr):\n    reverted = False\n    op_dist_attr = dist_op.dist_attr\n    op_desc = dist_op.serial_op.desc\n    out_name = op_desc.output('Out')[0]\n    out_dist_attr = op_dist_attr.get_output_dist_attr(out_name)\n    if out_dist_attr._is_partial():\n        op_dist_attr.impl_type = op_desc.type()\n        op_dist_attr.impl_idx = 0\n    else:\n        default_impl = get_default_distributed_operator_impl()\n        op_dist_attr.impl_type = default_impl.type\n        op_dist_attr.impl_idx = default_impl.idx\n    return reverted",
        "mutated": [
            "@staticmethod\ndef mapping_to_dist_operator_impl(dist_op, original_op_dist_attr):\n    if False:\n        i = 10\n    reverted = False\n    op_dist_attr = dist_op.dist_attr\n    op_desc = dist_op.serial_op.desc\n    out_name = op_desc.output('Out')[0]\n    out_dist_attr = op_dist_attr.get_output_dist_attr(out_name)\n    if out_dist_attr._is_partial():\n        op_dist_attr.impl_type = op_desc.type()\n        op_dist_attr.impl_idx = 0\n    else:\n        default_impl = get_default_distributed_operator_impl()\n        op_dist_attr.impl_type = default_impl.type\n        op_dist_attr.impl_idx = default_impl.idx\n    return reverted",
            "@staticmethod\ndef mapping_to_dist_operator_impl(dist_op, original_op_dist_attr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    reverted = False\n    op_dist_attr = dist_op.dist_attr\n    op_desc = dist_op.serial_op.desc\n    out_name = op_desc.output('Out')[0]\n    out_dist_attr = op_dist_attr.get_output_dist_attr(out_name)\n    if out_dist_attr._is_partial():\n        op_dist_attr.impl_type = op_desc.type()\n        op_dist_attr.impl_idx = 0\n    else:\n        default_impl = get_default_distributed_operator_impl()\n        op_dist_attr.impl_type = default_impl.type\n        op_dist_attr.impl_idx = default_impl.idx\n    return reverted",
            "@staticmethod\ndef mapping_to_dist_operator_impl(dist_op, original_op_dist_attr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    reverted = False\n    op_dist_attr = dist_op.dist_attr\n    op_desc = dist_op.serial_op.desc\n    out_name = op_desc.output('Out')[0]\n    out_dist_attr = op_dist_attr.get_output_dist_attr(out_name)\n    if out_dist_attr._is_partial():\n        op_dist_attr.impl_type = op_desc.type()\n        op_dist_attr.impl_idx = 0\n    else:\n        default_impl = get_default_distributed_operator_impl()\n        op_dist_attr.impl_type = default_impl.type\n        op_dist_attr.impl_idx = default_impl.idx\n    return reverted",
            "@staticmethod\ndef mapping_to_dist_operator_impl(dist_op, original_op_dist_attr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    reverted = False\n    op_dist_attr = dist_op.dist_attr\n    op_desc = dist_op.serial_op.desc\n    out_name = op_desc.output('Out')[0]\n    out_dist_attr = op_dist_attr.get_output_dist_attr(out_name)\n    if out_dist_attr._is_partial():\n        op_dist_attr.impl_type = op_desc.type()\n        op_dist_attr.impl_idx = 0\n    else:\n        default_impl = get_default_distributed_operator_impl()\n        op_dist_attr.impl_type = default_impl.type\n        op_dist_attr.impl_idx = default_impl.idx\n    return reverted",
            "@staticmethod\ndef mapping_to_dist_operator_impl(dist_op, original_op_dist_attr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    reverted = False\n    op_dist_attr = dist_op.dist_attr\n    op_desc = dist_op.serial_op.desc\n    out_name = op_desc.output('Out')[0]\n    out_dist_attr = op_dist_attr.get_output_dist_attr(out_name)\n    if out_dist_attr._is_partial():\n        op_dist_attr.impl_type = op_desc.type()\n        op_dist_attr.impl_idx = 0\n    else:\n        default_impl = get_default_distributed_operator_impl()\n        op_dist_attr.impl_type = default_impl.type\n        op_dist_attr.impl_idx = default_impl.idx\n    return reverted"
        ]
    },
    {
        "func_name": "adopt_lookup_table_v1",
        "original": "def adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var):\n    assert len(Ids_var.shape) == 3, 'input Ids to lookup_table should have 3 dimensions but got [{}] with shape [{}]'.format(Ids_var.name, Ids_var.shape)\n    if not Ids_var.stop_gradient:\n        raise NotImplementedError('Requiring the gradient of Ids of lookup_table(v1) dist op is not currently supported. Please open an issue with details on your use case so that we can prioritize adding this (for instance, adversarial training for language model).')\n    target_shape = list(Ids_var.shape[:-1])\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_reshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    target_shape = [0] + list(Ids_var.shape[:-1])\n    xshape_var = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_Xshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    reshape_op = main_block.append_op(type='reshape2', inputs={'X': [Ids_var]}, outputs={'Out': [intermediate_var_0], 'XShape': [xshape_var]}, attrs={'shape': [0, -1]})\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    Ids_var_dist_attr = op_dist_attr.get_input_dist_attr(Ids_var.name)\n    assert Ids_var_dist_attr is not None\n    intermediate_var_0_dist_attr = set_var_dist_attr(ctx, intermediate_var_0, Ids_var_dist_attr.dims_mapping, Ids_var_dist_attr.process_mesh)\n    set_var_dist_attr(ctx, xshape_var, [-1] + list(Ids_var_dist_attr.dims_mapping), Ids_var_dist_attr.process_mesh)\n    op_dist_attr.del_input_dist_attr(Ids_var.name)\n    op_dist_attr.set_input_dist_attr(intermediate_var_0.name, intermediate_var_0_dist_attr)\n    new_op_dist_attr = OperatorDistAttr()\n    new_op_dist_attr.process_mesh = Ids_var_dist_attr.process_mesh\n    new_op_dist_attr.impl_type = 'default'\n    new_op_dist_attr.impl_idx = 0\n    new_op_dist_attr.set_input_dims_mapping(Ids_var.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(intermediate_var_0.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(xshape_var.name, [-1] + list(Ids_var_dist_attr.dims_mapping))\n    ctx.set_op_dist_attr_for_program(reshape_op, new_op_dist_attr)\n    return intermediate_var_0",
        "mutated": [
            "def adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var):\n    if False:\n        i = 10\n    assert len(Ids_var.shape) == 3, 'input Ids to lookup_table should have 3 dimensions but got [{}] with shape [{}]'.format(Ids_var.name, Ids_var.shape)\n    if not Ids_var.stop_gradient:\n        raise NotImplementedError('Requiring the gradient of Ids of lookup_table(v1) dist op is not currently supported. Please open an issue with details on your use case so that we can prioritize adding this (for instance, adversarial training for language model).')\n    target_shape = list(Ids_var.shape[:-1])\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_reshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    target_shape = [0] + list(Ids_var.shape[:-1])\n    xshape_var = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_Xshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    reshape_op = main_block.append_op(type='reshape2', inputs={'X': [Ids_var]}, outputs={'Out': [intermediate_var_0], 'XShape': [xshape_var]}, attrs={'shape': [0, -1]})\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    Ids_var_dist_attr = op_dist_attr.get_input_dist_attr(Ids_var.name)\n    assert Ids_var_dist_attr is not None\n    intermediate_var_0_dist_attr = set_var_dist_attr(ctx, intermediate_var_0, Ids_var_dist_attr.dims_mapping, Ids_var_dist_attr.process_mesh)\n    set_var_dist_attr(ctx, xshape_var, [-1] + list(Ids_var_dist_attr.dims_mapping), Ids_var_dist_attr.process_mesh)\n    op_dist_attr.del_input_dist_attr(Ids_var.name)\n    op_dist_attr.set_input_dist_attr(intermediate_var_0.name, intermediate_var_0_dist_attr)\n    new_op_dist_attr = OperatorDistAttr()\n    new_op_dist_attr.process_mesh = Ids_var_dist_attr.process_mesh\n    new_op_dist_attr.impl_type = 'default'\n    new_op_dist_attr.impl_idx = 0\n    new_op_dist_attr.set_input_dims_mapping(Ids_var.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(intermediate_var_0.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(xshape_var.name, [-1] + list(Ids_var_dist_attr.dims_mapping))\n    ctx.set_op_dist_attr_for_program(reshape_op, new_op_dist_attr)\n    return intermediate_var_0",
            "def adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert len(Ids_var.shape) == 3, 'input Ids to lookup_table should have 3 dimensions but got [{}] with shape [{}]'.format(Ids_var.name, Ids_var.shape)\n    if not Ids_var.stop_gradient:\n        raise NotImplementedError('Requiring the gradient of Ids of lookup_table(v1) dist op is not currently supported. Please open an issue with details on your use case so that we can prioritize adding this (for instance, adversarial training for language model).')\n    target_shape = list(Ids_var.shape[:-1])\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_reshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    target_shape = [0] + list(Ids_var.shape[:-1])\n    xshape_var = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_Xshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    reshape_op = main_block.append_op(type='reshape2', inputs={'X': [Ids_var]}, outputs={'Out': [intermediate_var_0], 'XShape': [xshape_var]}, attrs={'shape': [0, -1]})\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    Ids_var_dist_attr = op_dist_attr.get_input_dist_attr(Ids_var.name)\n    assert Ids_var_dist_attr is not None\n    intermediate_var_0_dist_attr = set_var_dist_attr(ctx, intermediate_var_0, Ids_var_dist_attr.dims_mapping, Ids_var_dist_attr.process_mesh)\n    set_var_dist_attr(ctx, xshape_var, [-1] + list(Ids_var_dist_attr.dims_mapping), Ids_var_dist_attr.process_mesh)\n    op_dist_attr.del_input_dist_attr(Ids_var.name)\n    op_dist_attr.set_input_dist_attr(intermediate_var_0.name, intermediate_var_0_dist_attr)\n    new_op_dist_attr = OperatorDistAttr()\n    new_op_dist_attr.process_mesh = Ids_var_dist_attr.process_mesh\n    new_op_dist_attr.impl_type = 'default'\n    new_op_dist_attr.impl_idx = 0\n    new_op_dist_attr.set_input_dims_mapping(Ids_var.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(intermediate_var_0.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(xshape_var.name, [-1] + list(Ids_var_dist_attr.dims_mapping))\n    ctx.set_op_dist_attr_for_program(reshape_op, new_op_dist_attr)\n    return intermediate_var_0",
            "def adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert len(Ids_var.shape) == 3, 'input Ids to lookup_table should have 3 dimensions but got [{}] with shape [{}]'.format(Ids_var.name, Ids_var.shape)\n    if not Ids_var.stop_gradient:\n        raise NotImplementedError('Requiring the gradient of Ids of lookup_table(v1) dist op is not currently supported. Please open an issue with details on your use case so that we can prioritize adding this (for instance, adversarial training for language model).')\n    target_shape = list(Ids_var.shape[:-1])\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_reshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    target_shape = [0] + list(Ids_var.shape[:-1])\n    xshape_var = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_Xshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    reshape_op = main_block.append_op(type='reshape2', inputs={'X': [Ids_var]}, outputs={'Out': [intermediate_var_0], 'XShape': [xshape_var]}, attrs={'shape': [0, -1]})\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    Ids_var_dist_attr = op_dist_attr.get_input_dist_attr(Ids_var.name)\n    assert Ids_var_dist_attr is not None\n    intermediate_var_0_dist_attr = set_var_dist_attr(ctx, intermediate_var_0, Ids_var_dist_attr.dims_mapping, Ids_var_dist_attr.process_mesh)\n    set_var_dist_attr(ctx, xshape_var, [-1] + list(Ids_var_dist_attr.dims_mapping), Ids_var_dist_attr.process_mesh)\n    op_dist_attr.del_input_dist_attr(Ids_var.name)\n    op_dist_attr.set_input_dist_attr(intermediate_var_0.name, intermediate_var_0_dist_attr)\n    new_op_dist_attr = OperatorDistAttr()\n    new_op_dist_attr.process_mesh = Ids_var_dist_attr.process_mesh\n    new_op_dist_attr.impl_type = 'default'\n    new_op_dist_attr.impl_idx = 0\n    new_op_dist_attr.set_input_dims_mapping(Ids_var.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(intermediate_var_0.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(xshape_var.name, [-1] + list(Ids_var_dist_attr.dims_mapping))\n    ctx.set_op_dist_attr_for_program(reshape_op, new_op_dist_attr)\n    return intermediate_var_0",
            "def adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert len(Ids_var.shape) == 3, 'input Ids to lookup_table should have 3 dimensions but got [{}] with shape [{}]'.format(Ids_var.name, Ids_var.shape)\n    if not Ids_var.stop_gradient:\n        raise NotImplementedError('Requiring the gradient of Ids of lookup_table(v1) dist op is not currently supported. Please open an issue with details on your use case so that we can prioritize adding this (for instance, adversarial training for language model).')\n    target_shape = list(Ids_var.shape[:-1])\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_reshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    target_shape = [0] + list(Ids_var.shape[:-1])\n    xshape_var = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_Xshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    reshape_op = main_block.append_op(type='reshape2', inputs={'X': [Ids_var]}, outputs={'Out': [intermediate_var_0], 'XShape': [xshape_var]}, attrs={'shape': [0, -1]})\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    Ids_var_dist_attr = op_dist_attr.get_input_dist_attr(Ids_var.name)\n    assert Ids_var_dist_attr is not None\n    intermediate_var_0_dist_attr = set_var_dist_attr(ctx, intermediate_var_0, Ids_var_dist_attr.dims_mapping, Ids_var_dist_attr.process_mesh)\n    set_var_dist_attr(ctx, xshape_var, [-1] + list(Ids_var_dist_attr.dims_mapping), Ids_var_dist_attr.process_mesh)\n    op_dist_attr.del_input_dist_attr(Ids_var.name)\n    op_dist_attr.set_input_dist_attr(intermediate_var_0.name, intermediate_var_0_dist_attr)\n    new_op_dist_attr = OperatorDistAttr()\n    new_op_dist_attr.process_mesh = Ids_var_dist_attr.process_mesh\n    new_op_dist_attr.impl_type = 'default'\n    new_op_dist_attr.impl_idx = 0\n    new_op_dist_attr.set_input_dims_mapping(Ids_var.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(intermediate_var_0.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(xshape_var.name, [-1] + list(Ids_var_dist_attr.dims_mapping))\n    ctx.set_op_dist_attr_for_program(reshape_op, new_op_dist_attr)\n    return intermediate_var_0",
            "def adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert len(Ids_var.shape) == 3, 'input Ids to lookup_table should have 3 dimensions but got [{}] with shape [{}]'.format(Ids_var.name, Ids_var.shape)\n    if not Ids_var.stop_gradient:\n        raise NotImplementedError('Requiring the gradient of Ids of lookup_table(v1) dist op is not currently supported. Please open an issue with details on your use case so that we can prioritize adding this (for instance, adversarial training for language model).')\n    target_shape = list(Ids_var.shape[:-1])\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_reshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    target_shape = [0] + list(Ids_var.shape[:-1])\n    xshape_var = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['dist_Xshape', 'tmp'])), dtype=Ids_var.dtype, shape=target_shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=True)\n    reshape_op = main_block.append_op(type='reshape2', inputs={'X': [Ids_var]}, outputs={'Out': [intermediate_var_0], 'XShape': [xshape_var]}, attrs={'shape': [0, -1]})\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    Ids_var_dist_attr = op_dist_attr.get_input_dist_attr(Ids_var.name)\n    assert Ids_var_dist_attr is not None\n    intermediate_var_0_dist_attr = set_var_dist_attr(ctx, intermediate_var_0, Ids_var_dist_attr.dims_mapping, Ids_var_dist_attr.process_mesh)\n    set_var_dist_attr(ctx, xshape_var, [-1] + list(Ids_var_dist_attr.dims_mapping), Ids_var_dist_attr.process_mesh)\n    op_dist_attr.del_input_dist_attr(Ids_var.name)\n    op_dist_attr.set_input_dist_attr(intermediate_var_0.name, intermediate_var_0_dist_attr)\n    new_op_dist_attr = OperatorDistAttr()\n    new_op_dist_attr.process_mesh = Ids_var_dist_attr.process_mesh\n    new_op_dist_attr.impl_type = 'default'\n    new_op_dist_attr.impl_idx = 0\n    new_op_dist_attr.set_input_dims_mapping(Ids_var.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(intermediate_var_0.name, Ids_var_dist_attr.dims_mapping)\n    new_op_dist_attr.set_output_dims_mapping(xshape_var.name, [-1] + list(Ids_var_dist_attr.dims_mapping))\n    ctx.set_op_dist_attr_for_program(reshape_op, new_op_dist_attr)\n    return intermediate_var_0"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, name):\n    super().__init__(name)\n    self._forward_implemented = True\n    self._backward_implemented = True",
        "mutated": [
            "def __init__(self, name):\n    if False:\n        i = 10\n    super().__init__(name)\n    self._forward_implemented = True\n    self._backward_implemented = True",
            "def __init__(self, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(name)\n    self._forward_implemented = True\n    self._backward_implemented = True",
            "def __init__(self, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(name)\n    self._forward_implemented = True\n    self._backward_implemented = True",
            "def __init__(self, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(name)\n    self._forward_implemented = True\n    self._backward_implemented = True",
            "def __init__(self, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(name)\n    self._forward_implemented = True\n    self._backward_implemented = True"
        ]
    },
    {
        "func_name": "calc_cost",
        "original": "def calc_cost(self, op_role, dist_op, ctx, cluster):\n    \"\"\"Calculate the cost by the op role.\"\"\"\n    cost = None\n    if int(op_role) == int(OpRole.Forward):\n        cost = self.calc_fwd_cost(dist_op, ctx, cluster)\n    elif int(op_role) == int(OpRole.Backward):\n        cost = self.calc_bwd_cost(dist_op, ctx, cluster)\n    assert cost is not None\n    return cost",
        "mutated": [
            "def calc_cost(self, op_role, dist_op, ctx, cluster):\n    if False:\n        i = 10\n    'Calculate the cost by the op role.'\n    cost = None\n    if int(op_role) == int(OpRole.Forward):\n        cost = self.calc_fwd_cost(dist_op, ctx, cluster)\n    elif int(op_role) == int(OpRole.Backward):\n        cost = self.calc_bwd_cost(dist_op, ctx, cluster)\n    assert cost is not None\n    return cost",
            "def calc_cost(self, op_role, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate the cost by the op role.'\n    cost = None\n    if int(op_role) == int(OpRole.Forward):\n        cost = self.calc_fwd_cost(dist_op, ctx, cluster)\n    elif int(op_role) == int(OpRole.Backward):\n        cost = self.calc_bwd_cost(dist_op, ctx, cluster)\n    assert cost is not None\n    return cost",
            "def calc_cost(self, op_role, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate the cost by the op role.'\n    cost = None\n    if int(op_role) == int(OpRole.Forward):\n        cost = self.calc_fwd_cost(dist_op, ctx, cluster)\n    elif int(op_role) == int(OpRole.Backward):\n        cost = self.calc_bwd_cost(dist_op, ctx, cluster)\n    assert cost is not None\n    return cost",
            "def calc_cost(self, op_role, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate the cost by the op role.'\n    cost = None\n    if int(op_role) == int(OpRole.Forward):\n        cost = self.calc_fwd_cost(dist_op, ctx, cluster)\n    elif int(op_role) == int(OpRole.Backward):\n        cost = self.calc_bwd_cost(dist_op, ctx, cluster)\n    assert cost is not None\n    return cost",
            "def calc_cost(self, op_role, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate the cost by the op role.'\n    cost = None\n    if int(op_role) == int(OpRole.Forward):\n        cost = self.calc_fwd_cost(dist_op, ctx, cluster)\n    elif int(op_role) == int(OpRole.Backward):\n        cost = self.calc_bwd_cost(dist_op, ctx, cluster)\n    assert cost is not None\n    return cost"
        ]
    },
    {
        "func_name": "calc_fwd_cost",
        "original": "def calc_fwd_cost(self, dist_op, ctx, cluster):\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    processes = dist_op.dist_attr.process_mesh.process_ids\n    cost_mapping = build_comp_costs_from_descs(EmbeddingOpCost, ctx, processes, desc_mapping, cluster)\n    serial_op = dist_op.serial_op\n    parallel_axis = dist_op.dist_attr.get_input_dims_mapping(serial_op.input('W')[0])[0]\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = serial_op.output('Out')\n    c_allreduce_sum_desc_mapping = build_comm_desc_from_dist_op('c_allreduce_sum', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    comm_op_cost_list = build_comm_costs_from_descs(AllreduceSumOpCost, ctx, processes, c_allreduce_sum_desc_mapping, cluster)\n    res_cost = [cost_mapping, comm_op_cost_list]\n    return res_cost",
        "mutated": [
            "def calc_fwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    processes = dist_op.dist_attr.process_mesh.process_ids\n    cost_mapping = build_comp_costs_from_descs(EmbeddingOpCost, ctx, processes, desc_mapping, cluster)\n    serial_op = dist_op.serial_op\n    parallel_axis = dist_op.dist_attr.get_input_dims_mapping(serial_op.input('W')[0])[0]\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = serial_op.output('Out')\n    c_allreduce_sum_desc_mapping = build_comm_desc_from_dist_op('c_allreduce_sum', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    comm_op_cost_list = build_comm_costs_from_descs(AllreduceSumOpCost, ctx, processes, c_allreduce_sum_desc_mapping, cluster)\n    res_cost = [cost_mapping, comm_op_cost_list]\n    return res_cost",
            "def calc_fwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    processes = dist_op.dist_attr.process_mesh.process_ids\n    cost_mapping = build_comp_costs_from_descs(EmbeddingOpCost, ctx, processes, desc_mapping, cluster)\n    serial_op = dist_op.serial_op\n    parallel_axis = dist_op.dist_attr.get_input_dims_mapping(serial_op.input('W')[0])[0]\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = serial_op.output('Out')\n    c_allreduce_sum_desc_mapping = build_comm_desc_from_dist_op('c_allreduce_sum', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    comm_op_cost_list = build_comm_costs_from_descs(AllreduceSumOpCost, ctx, processes, c_allreduce_sum_desc_mapping, cluster)\n    res_cost = [cost_mapping, comm_op_cost_list]\n    return res_cost",
            "def calc_fwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    processes = dist_op.dist_attr.process_mesh.process_ids\n    cost_mapping = build_comp_costs_from_descs(EmbeddingOpCost, ctx, processes, desc_mapping, cluster)\n    serial_op = dist_op.serial_op\n    parallel_axis = dist_op.dist_attr.get_input_dims_mapping(serial_op.input('W')[0])[0]\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = serial_op.output('Out')\n    c_allreduce_sum_desc_mapping = build_comm_desc_from_dist_op('c_allreduce_sum', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    comm_op_cost_list = build_comm_costs_from_descs(AllreduceSumOpCost, ctx, processes, c_allreduce_sum_desc_mapping, cluster)\n    res_cost = [cost_mapping, comm_op_cost_list]\n    return res_cost",
            "def calc_fwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    processes = dist_op.dist_attr.process_mesh.process_ids\n    cost_mapping = build_comp_costs_from_descs(EmbeddingOpCost, ctx, processes, desc_mapping, cluster)\n    serial_op = dist_op.serial_op\n    parallel_axis = dist_op.dist_attr.get_input_dims_mapping(serial_op.input('W')[0])[0]\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = serial_op.output('Out')\n    c_allreduce_sum_desc_mapping = build_comm_desc_from_dist_op('c_allreduce_sum', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    comm_op_cost_list = build_comm_costs_from_descs(AllreduceSumOpCost, ctx, processes, c_allreduce_sum_desc_mapping, cluster)\n    res_cost = [cost_mapping, comm_op_cost_list]\n    return res_cost",
            "def calc_fwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    processes = dist_op.dist_attr.process_mesh.process_ids\n    cost_mapping = build_comp_costs_from_descs(EmbeddingOpCost, ctx, processes, desc_mapping, cluster)\n    serial_op = dist_op.serial_op\n    parallel_axis = dist_op.dist_attr.get_input_dims_mapping(serial_op.input('W')[0])[0]\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = serial_op.output('Out')\n    c_allreduce_sum_desc_mapping = build_comm_desc_from_dist_op('c_allreduce_sum', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    comm_op_cost_list = build_comm_costs_from_descs(AllreduceSumOpCost, ctx, processes, c_allreduce_sum_desc_mapping, cluster)\n    res_cost = [cost_mapping, comm_op_cost_list]\n    return res_cost"
        ]
    },
    {
        "func_name": "calc_bwd_cost",
        "original": "def calc_bwd_cost(self, dist_op, ctx, cluster):\n    res = []\n    backward_op = dist_op.serial_op\n    main_block = backward_op.block\n    dist_attr = dist_op.dist_attr\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('W')[0])[0]\n    parallel_axis = embedding_row_dim_mapping\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = [backward_op.input('Out@GRAD')[0]]\n    c_identity_desc_mapping = build_comm_desc_from_dist_op('c_identity', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    process_mesh = dist_attr.process_mesh\n    processes = process_mesh.process_ids\n    comm_op_cost_list = build_comm_costs_from_descs(IdentityOpCost, ctx, processes, c_identity_desc_mapping, cluster)\n    res.append(comm_op_cost_list)\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    cost_mapping = build_comp_costs_from_descs(EmbeddingGradOpCost, ctx, processes, desc_mapping, cluster)\n    res.append(cost_mapping)\n    var_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('Ids')[0])\n    mesh_shape = process_mesh.shape\n    batch_size_axis = var_dim_mapping[0] if len(var_dim_mapping) > 0 else -1\n    if batch_size_axis > -1 and mesh_shape[batch_size_axis] > 1:\n        parallel_axis = batch_size_axis\n        attrs = {'use_calc_stream': True}\n        var_names = [backward_op.output('W@GRAD')[0]]\n        build_dp_costs(res, dist_op, ctx, var_names, attrs, parallel_axis, cluster)\n    return res",
        "mutated": [
            "def calc_bwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n    res = []\n    backward_op = dist_op.serial_op\n    main_block = backward_op.block\n    dist_attr = dist_op.dist_attr\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('W')[0])[0]\n    parallel_axis = embedding_row_dim_mapping\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = [backward_op.input('Out@GRAD')[0]]\n    c_identity_desc_mapping = build_comm_desc_from_dist_op('c_identity', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    process_mesh = dist_attr.process_mesh\n    processes = process_mesh.process_ids\n    comm_op_cost_list = build_comm_costs_from_descs(IdentityOpCost, ctx, processes, c_identity_desc_mapping, cluster)\n    res.append(comm_op_cost_list)\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    cost_mapping = build_comp_costs_from_descs(EmbeddingGradOpCost, ctx, processes, desc_mapping, cluster)\n    res.append(cost_mapping)\n    var_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('Ids')[0])\n    mesh_shape = process_mesh.shape\n    batch_size_axis = var_dim_mapping[0] if len(var_dim_mapping) > 0 else -1\n    if batch_size_axis > -1 and mesh_shape[batch_size_axis] > 1:\n        parallel_axis = batch_size_axis\n        attrs = {'use_calc_stream': True}\n        var_names = [backward_op.output('W@GRAD')[0]]\n        build_dp_costs(res, dist_op, ctx, var_names, attrs, parallel_axis, cluster)\n    return res",
            "def calc_bwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    res = []\n    backward_op = dist_op.serial_op\n    main_block = backward_op.block\n    dist_attr = dist_op.dist_attr\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('W')[0])[0]\n    parallel_axis = embedding_row_dim_mapping\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = [backward_op.input('Out@GRAD')[0]]\n    c_identity_desc_mapping = build_comm_desc_from_dist_op('c_identity', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    process_mesh = dist_attr.process_mesh\n    processes = process_mesh.process_ids\n    comm_op_cost_list = build_comm_costs_from_descs(IdentityOpCost, ctx, processes, c_identity_desc_mapping, cluster)\n    res.append(comm_op_cost_list)\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    cost_mapping = build_comp_costs_from_descs(EmbeddingGradOpCost, ctx, processes, desc_mapping, cluster)\n    res.append(cost_mapping)\n    var_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('Ids')[0])\n    mesh_shape = process_mesh.shape\n    batch_size_axis = var_dim_mapping[0] if len(var_dim_mapping) > 0 else -1\n    if batch_size_axis > -1 and mesh_shape[batch_size_axis] > 1:\n        parallel_axis = batch_size_axis\n        attrs = {'use_calc_stream': True}\n        var_names = [backward_op.output('W@GRAD')[0]]\n        build_dp_costs(res, dist_op, ctx, var_names, attrs, parallel_axis, cluster)\n    return res",
            "def calc_bwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    res = []\n    backward_op = dist_op.serial_op\n    main_block = backward_op.block\n    dist_attr = dist_op.dist_attr\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('W')[0])[0]\n    parallel_axis = embedding_row_dim_mapping\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = [backward_op.input('Out@GRAD')[0]]\n    c_identity_desc_mapping = build_comm_desc_from_dist_op('c_identity', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    process_mesh = dist_attr.process_mesh\n    processes = process_mesh.process_ids\n    comm_op_cost_list = build_comm_costs_from_descs(IdentityOpCost, ctx, processes, c_identity_desc_mapping, cluster)\n    res.append(comm_op_cost_list)\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    cost_mapping = build_comp_costs_from_descs(EmbeddingGradOpCost, ctx, processes, desc_mapping, cluster)\n    res.append(cost_mapping)\n    var_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('Ids')[0])\n    mesh_shape = process_mesh.shape\n    batch_size_axis = var_dim_mapping[0] if len(var_dim_mapping) > 0 else -1\n    if batch_size_axis > -1 and mesh_shape[batch_size_axis] > 1:\n        parallel_axis = batch_size_axis\n        attrs = {'use_calc_stream': True}\n        var_names = [backward_op.output('W@GRAD')[0]]\n        build_dp_costs(res, dist_op, ctx, var_names, attrs, parallel_axis, cluster)\n    return res",
            "def calc_bwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    res = []\n    backward_op = dist_op.serial_op\n    main_block = backward_op.block\n    dist_attr = dist_op.dist_attr\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('W')[0])[0]\n    parallel_axis = embedding_row_dim_mapping\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = [backward_op.input('Out@GRAD')[0]]\n    c_identity_desc_mapping = build_comm_desc_from_dist_op('c_identity', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    process_mesh = dist_attr.process_mesh\n    processes = process_mesh.process_ids\n    comm_op_cost_list = build_comm_costs_from_descs(IdentityOpCost, ctx, processes, c_identity_desc_mapping, cluster)\n    res.append(comm_op_cost_list)\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    cost_mapping = build_comp_costs_from_descs(EmbeddingGradOpCost, ctx, processes, desc_mapping, cluster)\n    res.append(cost_mapping)\n    var_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('Ids')[0])\n    mesh_shape = process_mesh.shape\n    batch_size_axis = var_dim_mapping[0] if len(var_dim_mapping) > 0 else -1\n    if batch_size_axis > -1 and mesh_shape[batch_size_axis] > 1:\n        parallel_axis = batch_size_axis\n        attrs = {'use_calc_stream': True}\n        var_names = [backward_op.output('W@GRAD')[0]]\n        build_dp_costs(res, dist_op, ctx, var_names, attrs, parallel_axis, cluster)\n    return res",
            "def calc_bwd_cost(self, dist_op, ctx, cluster):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    res = []\n    backward_op = dist_op.serial_op\n    main_block = backward_op.block\n    dist_attr = dist_op.dist_attr\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('W')[0])[0]\n    parallel_axis = embedding_row_dim_mapping\n    attrs = {'use_calc_stream': True, 'use_model_parallel': True}\n    var_names = [backward_op.input('Out@GRAD')[0]]\n    c_identity_desc_mapping = build_comm_desc_from_dist_op('c_identity', dist_op, ctx, var_names, attrs=attrs, parallel_axis=parallel_axis)\n    process_mesh = dist_attr.process_mesh\n    processes = process_mesh.process_ids\n    comm_op_cost_list = build_comm_costs_from_descs(IdentityOpCost, ctx, processes, c_identity_desc_mapping, cluster)\n    res.append(comm_op_cost_list)\n    desc_mapping = build_comp_desc_from_dist_op(dist_op=dist_op, dist_context=ctx)\n    cost_mapping = build_comp_costs_from_descs(EmbeddingGradOpCost, ctx, processes, desc_mapping, cluster)\n    res.append(cost_mapping)\n    var_dim_mapping = dist_attr.get_input_dims_mapping(backward_op.input('Ids')[0])\n    mesh_shape = process_mesh.shape\n    batch_size_axis = var_dim_mapping[0] if len(var_dim_mapping) > 0 else -1\n    if batch_size_axis > -1 and mesh_shape[batch_size_axis] > 1:\n        parallel_axis = batch_size_axis\n        attrs = {'use_calc_stream': True}\n        var_names = [backward_op.output('W@GRAD')[0]]\n        build_dp_costs(res, dist_op, ctx, var_names, attrs, parallel_axis, cluster)\n    return res"
        ]
    },
    {
        "func_name": "is_input_compatible",
        "original": "def is_input_compatible(self, dist_op):\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if is_dim_replicate(w_dims_mapping[-2]) or is_dim_shard(w_dims_mapping[-1]):\n        return False\n    for mapping in ids_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    if is_dim_shard(ids_dims_mapping[0]) and is_dim_shard(w_dims_mapping[-2]):\n        if ids_dims_mapping[0] == w_dims_mapping[-2]:\n            return False\n    return True",
        "mutated": [
            "def is_input_compatible(self, dist_op):\n    if False:\n        i = 10\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if is_dim_replicate(w_dims_mapping[-2]) or is_dim_shard(w_dims_mapping[-1]):\n        return False\n    for mapping in ids_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    if is_dim_shard(ids_dims_mapping[0]) and is_dim_shard(w_dims_mapping[-2]):\n        if ids_dims_mapping[0] == w_dims_mapping[-2]:\n            return False\n    return True",
            "def is_input_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if is_dim_replicate(w_dims_mapping[-2]) or is_dim_shard(w_dims_mapping[-1]):\n        return False\n    for mapping in ids_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    if is_dim_shard(ids_dims_mapping[0]) and is_dim_shard(w_dims_mapping[-2]):\n        if ids_dims_mapping[0] == w_dims_mapping[-2]:\n            return False\n    return True",
            "def is_input_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if is_dim_replicate(w_dims_mapping[-2]) or is_dim_shard(w_dims_mapping[-1]):\n        return False\n    for mapping in ids_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    if is_dim_shard(ids_dims_mapping[0]) and is_dim_shard(w_dims_mapping[-2]):\n        if ids_dims_mapping[0] == w_dims_mapping[-2]:\n            return False\n    return True",
            "def is_input_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if is_dim_replicate(w_dims_mapping[-2]) or is_dim_shard(w_dims_mapping[-1]):\n        return False\n    for mapping in ids_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    if is_dim_shard(ids_dims_mapping[0]) and is_dim_shard(w_dims_mapping[-2]):\n        if ids_dims_mapping[0] == w_dims_mapping[-2]:\n            return False\n    return True",
            "def is_input_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if is_dim_replicate(w_dims_mapping[-2]) or is_dim_shard(w_dims_mapping[-1]):\n        return False\n    for mapping in ids_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    if is_dim_shard(ids_dims_mapping[0]) and is_dim_shard(w_dims_mapping[-2]):\n        if ids_dims_mapping[0] == w_dims_mapping[-2]:\n            return False\n    return True"
        ]
    },
    {
        "func_name": "is_output_compatible",
        "original": "def is_output_compatible(self, dist_op):\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for mapping in out_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    return True",
        "mutated": [
            "def is_output_compatible(self, dist_op):\n    if False:\n        i = 10\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for mapping in out_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    return True",
            "def is_output_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for mapping in out_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    return True",
            "def is_output_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for mapping in out_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    return True",
            "def is_output_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for mapping in out_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    return True",
            "def is_output_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for mapping in out_dims_mapping[1:]:\n        if is_dim_shard(mapping):\n            return False\n    return True"
        ]
    },
    {
        "func_name": "is_auto_compatible",
        "original": "def is_auto_compatible(self, dist_op):\n    if not self.is_input_compatible(dist_op) or not self.is_output_compatible(dist_op):\n        return False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if ids_dims_mapping != out_dims_mapping[:len(ids_dims_mapping)]:\n        return False\n    return True",
        "mutated": [
            "def is_auto_compatible(self, dist_op):\n    if False:\n        i = 10\n    if not self.is_input_compatible(dist_op) or not self.is_output_compatible(dist_op):\n        return False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if ids_dims_mapping != out_dims_mapping[:len(ids_dims_mapping)]:\n        return False\n    return True",
            "def is_auto_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.is_input_compatible(dist_op) or not self.is_output_compatible(dist_op):\n        return False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if ids_dims_mapping != out_dims_mapping[:len(ids_dims_mapping)]:\n        return False\n    return True",
            "def is_auto_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.is_input_compatible(dist_op) or not self.is_output_compatible(dist_op):\n        return False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if ids_dims_mapping != out_dims_mapping[:len(ids_dims_mapping)]:\n        return False\n    return True",
            "def is_auto_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.is_input_compatible(dist_op) or not self.is_output_compatible(dist_op):\n        return False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if ids_dims_mapping != out_dims_mapping[:len(ids_dims_mapping)]:\n        return False\n    return True",
            "def is_auto_compatible(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.is_input_compatible(dist_op) or not self.is_output_compatible(dist_op):\n        return False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    if ids_dims_mapping != out_dims_mapping[:len(ids_dims_mapping)]:\n        return False\n    return True"
        ]
    },
    {
        "func_name": "update_dims_mapping",
        "original": "def update_dims_mapping(self, dist_op):\n    changed = False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for i in range(len(ids_dims_mapping)):\n        dim_changed = compute_compatible_and_update_dim_mapping([ids_dims_mapping, out_dims_mapping], [i, i])\n        if dim_changed:\n            changed = True\n    dim_changed = compute_compatible_and_update_dim_mapping([w_dims_mapping, out_dims_mapping], [-1, -1])\n    if dim_changed:\n        changed = True\n    if changed:\n        op_dist_attr.set_input_dims_mapping(ids_name, ids_dims_mapping)\n        op_dist_attr.set_input_dims_mapping(w_name, w_dims_mapping)\n        op_dist_attr.set_output_dims_mapping(out_name, out_dims_mapping)\n    return changed",
        "mutated": [
            "def update_dims_mapping(self, dist_op):\n    if False:\n        i = 10\n    changed = False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for i in range(len(ids_dims_mapping)):\n        dim_changed = compute_compatible_and_update_dim_mapping([ids_dims_mapping, out_dims_mapping], [i, i])\n        if dim_changed:\n            changed = True\n    dim_changed = compute_compatible_and_update_dim_mapping([w_dims_mapping, out_dims_mapping], [-1, -1])\n    if dim_changed:\n        changed = True\n    if changed:\n        op_dist_attr.set_input_dims_mapping(ids_name, ids_dims_mapping)\n        op_dist_attr.set_input_dims_mapping(w_name, w_dims_mapping)\n        op_dist_attr.set_output_dims_mapping(out_name, out_dims_mapping)\n    return changed",
            "def update_dims_mapping(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    changed = False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for i in range(len(ids_dims_mapping)):\n        dim_changed = compute_compatible_and_update_dim_mapping([ids_dims_mapping, out_dims_mapping], [i, i])\n        if dim_changed:\n            changed = True\n    dim_changed = compute_compatible_and_update_dim_mapping([w_dims_mapping, out_dims_mapping], [-1, -1])\n    if dim_changed:\n        changed = True\n    if changed:\n        op_dist_attr.set_input_dims_mapping(ids_name, ids_dims_mapping)\n        op_dist_attr.set_input_dims_mapping(w_name, w_dims_mapping)\n        op_dist_attr.set_output_dims_mapping(out_name, out_dims_mapping)\n    return changed",
            "def update_dims_mapping(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    changed = False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for i in range(len(ids_dims_mapping)):\n        dim_changed = compute_compatible_and_update_dim_mapping([ids_dims_mapping, out_dims_mapping], [i, i])\n        if dim_changed:\n            changed = True\n    dim_changed = compute_compatible_and_update_dim_mapping([w_dims_mapping, out_dims_mapping], [-1, -1])\n    if dim_changed:\n        changed = True\n    if changed:\n        op_dist_attr.set_input_dims_mapping(ids_name, ids_dims_mapping)\n        op_dist_attr.set_input_dims_mapping(w_name, w_dims_mapping)\n        op_dist_attr.set_output_dims_mapping(out_name, out_dims_mapping)\n    return changed",
            "def update_dims_mapping(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    changed = False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for i in range(len(ids_dims_mapping)):\n        dim_changed = compute_compatible_and_update_dim_mapping([ids_dims_mapping, out_dims_mapping], [i, i])\n        if dim_changed:\n            changed = True\n    dim_changed = compute_compatible_and_update_dim_mapping([w_dims_mapping, out_dims_mapping], [-1, -1])\n    if dim_changed:\n        changed = True\n    if changed:\n        op_dist_attr.set_input_dims_mapping(ids_name, ids_dims_mapping)\n        op_dist_attr.set_input_dims_mapping(w_name, w_dims_mapping)\n        op_dist_attr.set_output_dims_mapping(out_name, out_dims_mapping)\n    return changed",
            "def update_dims_mapping(self, dist_op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    changed = False\n    op_desc = dist_op.serial_op.desc\n    op_dist_attr = dist_op.dist_attr\n    ids_name = op_desc.input('Ids')[0]\n    w_name = op_desc.input('W')[0]\n    out_name = op_desc.output('Out')[0]\n    ids_dims_mapping = op_dist_attr.get_input_dims_mapping(ids_name)\n    w_dims_mapping = op_dist_attr.get_input_dims_mapping(w_name)\n    out_dims_mapping = op_dist_attr.get_output_dims_mapping(out_name)\n    for i in range(len(ids_dims_mapping)):\n        dim_changed = compute_compatible_and_update_dim_mapping([ids_dims_mapping, out_dims_mapping], [i, i])\n        if dim_changed:\n            changed = True\n    dim_changed = compute_compatible_and_update_dim_mapping([w_dims_mapping, out_dims_mapping], [-1, -1])\n    if dim_changed:\n        changed = True\n    if changed:\n        op_dist_attr.set_input_dims_mapping(ids_name, ids_dims_mapping)\n        op_dist_attr.set_input_dims_mapping(w_name, w_dims_mapping)\n        op_dist_attr.set_output_dims_mapping(out_name, out_dims_mapping)\n    return changed"
        ]
    },
    {
        "func_name": "forward",
        "original": "@staticmethod\ndef forward(ctx, *args, **kwargs):\n    \"\"\"\n        kwargs: inputname_mapping & outputname_mapping\n        \"\"\"\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    startup_block = dist_op_context.startup_block\n    src_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    assert op_dist_attr is not None, f\"forward op [{str(src_op)}] don't have dist attribute !\"\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out' in kwargs, 'output [{}] is not given'.format('Out')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input W take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out']) == 1, 'row_parallel_embedding output Out take 1 variable but got {}'.format(kwargs['Out'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_var = main_block._var_recursive(kwargs['Out'][0])\n    if src_op.type == 'lookup_table':\n        Ids_var = adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var)\n    embedding_row_dim_mapping = op_dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = op_dist_attr.process_mesh.shape\n    process_mesh_group = op_dist_attr.process_mesh.process_ids\n    if rank_id not in process_mesh_group:\n        rank_id = _get_corresponding_rank(ctx, op_dist_attr.process_mesh, rank_id)\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    parallel_axis = embedding_row_dim_mapping\n    group_ranks = _get_comm_group(process_mesh_group, process_mesh_shape, parallel_axis, rank_id)\n    group = new_process_group(group_ranks)\n    check_variable_and_dtype(Ids_var, 'input', ['int32', 'int64'], 'c_embedding')\n    out_tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(Out_var)\n    assert out_tensor_dist_attr is not None\n    out_var_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert out_var_dist_attr is not None\n    ref_shape = infer_shape(main_block, Out_var, out_tensor_dist_attr, out_var_dist_attr)\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['c_embedding', 'tmp'])), dtype=Weight_var.dtype, shape=Out_var.shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=Out_var.stop_gradient)\n    ctx.set_tensor_dist_attr_for_program(intermediate_var_0, out_var_dist_attr)\n    check_variable_and_dtype(Out_var, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], 'c_allreduce_sum')\n    c_embedding_op = main_block.append_op(type='c_embedding', inputs={'Ids': [Ids_var], 'W': [Weight_var]}, outputs={'Out': [intermediate_var_0]}, attrs={'start_index': relative_idx, OP_ROLE_KEY: src_op.attr('op_role')})\n    if intermediate_var_0.shape != ref_shape:\n        intermediate_var_0.desc.set_shape(ref_shape)\n    c_allreduce_sum_op = main_block.append_op(type='c_allreduce_sum', inputs={'X': [intermediate_var_0]}, outputs={'Out': [Out_var]}, attrs={'ring_id': group.id, 'use_calc_stream': True, 'use_model_parallel': True, OP_ROLE_KEY: src_op.attr('op_role')})\n    c_allreduce_sum_op._set_attr('op_namescope', '/' + ParallelMode.TensorParallel)\n    if Out_var.shape != ref_shape:\n        Out_var.desc.set_shape(ref_shape)\n    embedding_op_dist_attr = OperatorDistAttr()\n    embedding_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    embedding_op_dist_attr.impl_type = op_dist_attr.impl_type\n    embedding_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_embedding_op.desc.input_arg_names():\n        input_dist_attr = op_dist_attr.get_input_dist_attr(input_varname)\n        assert input_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        embedding_op_dist_attr.set_input_dist_attr(input_varname, input_dist_attr)\n    output_varname = c_embedding_op.desc.output_arg_names()[0]\n    output_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n    embedding_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_embedding_op, embedding_op_dist_attr)\n    allreduce_op_dist_attr = OperatorDistAttr()\n    allreduce_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    allreduce_op_dist_attr.impl_type = op_dist_attr.impl_type\n    allreduce_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_allreduce_sum_op.desc.input_arg_names():\n        input_var = main_block._var_recursive(input_varname)\n        tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(input_var)\n        assert tensor_dist_attr is not None\n        allreduce_op_dist_attr.set_input_dist_attr(input_varname, tensor_dist_attr)\n    for output_varname in c_allreduce_sum_op.desc.output_arg_names():\n        output_dist_attr = op_dist_attr.get_output_dist_attr(output_varname)\n        assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        allreduce_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_allreduce_sum_op, allreduce_op_dist_attr)\n    if Weight_var.is_parameter and (not op_dist_attr.is_recompute):\n        if Weight_var.name in dist_op_context.already_init_sync_vars:\n            return\n        dist_op_context.already_init_sync_vars.add(Weight_var.name)\n        param = startup_block.var(Weight_var.name)\n        param_dist_attr = ctx.get_tensor_dist_attr_for_program(param)\n        process_mesh = param_dist_attr.process_mesh\n        dim_mapping = param_dist_attr.dims_mapping\n        for (axis, size) in enumerate(process_mesh.shape):\n            if size <= 1 or axis in dim_mapping:\n                pass\n            else:\n                group_ranks = _get_comm_group(process_mesh.process_ids, process_mesh.shape, axis, rank_id)\n                sync_group = new_process_group(group_ranks)\n                startup_block.append_op(type='c_broadcast', inputs={'X': param}, outputs={'Out': param}, attrs={'ring_id': sync_group.id, 'root': 0, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Forward})",
        "mutated": [
            "@staticmethod\ndef forward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n    '\\n        kwargs: inputname_mapping & outputname_mapping\\n        '\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    startup_block = dist_op_context.startup_block\n    src_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    assert op_dist_attr is not None, f\"forward op [{str(src_op)}] don't have dist attribute !\"\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out' in kwargs, 'output [{}] is not given'.format('Out')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input W take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out']) == 1, 'row_parallel_embedding output Out take 1 variable but got {}'.format(kwargs['Out'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_var = main_block._var_recursive(kwargs['Out'][0])\n    if src_op.type == 'lookup_table':\n        Ids_var = adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var)\n    embedding_row_dim_mapping = op_dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = op_dist_attr.process_mesh.shape\n    process_mesh_group = op_dist_attr.process_mesh.process_ids\n    if rank_id not in process_mesh_group:\n        rank_id = _get_corresponding_rank(ctx, op_dist_attr.process_mesh, rank_id)\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    parallel_axis = embedding_row_dim_mapping\n    group_ranks = _get_comm_group(process_mesh_group, process_mesh_shape, parallel_axis, rank_id)\n    group = new_process_group(group_ranks)\n    check_variable_and_dtype(Ids_var, 'input', ['int32', 'int64'], 'c_embedding')\n    out_tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(Out_var)\n    assert out_tensor_dist_attr is not None\n    out_var_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert out_var_dist_attr is not None\n    ref_shape = infer_shape(main_block, Out_var, out_tensor_dist_attr, out_var_dist_attr)\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['c_embedding', 'tmp'])), dtype=Weight_var.dtype, shape=Out_var.shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=Out_var.stop_gradient)\n    ctx.set_tensor_dist_attr_for_program(intermediate_var_0, out_var_dist_attr)\n    check_variable_and_dtype(Out_var, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], 'c_allreduce_sum')\n    c_embedding_op = main_block.append_op(type='c_embedding', inputs={'Ids': [Ids_var], 'W': [Weight_var]}, outputs={'Out': [intermediate_var_0]}, attrs={'start_index': relative_idx, OP_ROLE_KEY: src_op.attr('op_role')})\n    if intermediate_var_0.shape != ref_shape:\n        intermediate_var_0.desc.set_shape(ref_shape)\n    c_allreduce_sum_op = main_block.append_op(type='c_allreduce_sum', inputs={'X': [intermediate_var_0]}, outputs={'Out': [Out_var]}, attrs={'ring_id': group.id, 'use_calc_stream': True, 'use_model_parallel': True, OP_ROLE_KEY: src_op.attr('op_role')})\n    c_allreduce_sum_op._set_attr('op_namescope', '/' + ParallelMode.TensorParallel)\n    if Out_var.shape != ref_shape:\n        Out_var.desc.set_shape(ref_shape)\n    embedding_op_dist_attr = OperatorDistAttr()\n    embedding_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    embedding_op_dist_attr.impl_type = op_dist_attr.impl_type\n    embedding_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_embedding_op.desc.input_arg_names():\n        input_dist_attr = op_dist_attr.get_input_dist_attr(input_varname)\n        assert input_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        embedding_op_dist_attr.set_input_dist_attr(input_varname, input_dist_attr)\n    output_varname = c_embedding_op.desc.output_arg_names()[0]\n    output_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n    embedding_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_embedding_op, embedding_op_dist_attr)\n    allreduce_op_dist_attr = OperatorDistAttr()\n    allreduce_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    allreduce_op_dist_attr.impl_type = op_dist_attr.impl_type\n    allreduce_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_allreduce_sum_op.desc.input_arg_names():\n        input_var = main_block._var_recursive(input_varname)\n        tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(input_var)\n        assert tensor_dist_attr is not None\n        allreduce_op_dist_attr.set_input_dist_attr(input_varname, tensor_dist_attr)\n    for output_varname in c_allreduce_sum_op.desc.output_arg_names():\n        output_dist_attr = op_dist_attr.get_output_dist_attr(output_varname)\n        assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        allreduce_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_allreduce_sum_op, allreduce_op_dist_attr)\n    if Weight_var.is_parameter and (not op_dist_attr.is_recompute):\n        if Weight_var.name in dist_op_context.already_init_sync_vars:\n            return\n        dist_op_context.already_init_sync_vars.add(Weight_var.name)\n        param = startup_block.var(Weight_var.name)\n        param_dist_attr = ctx.get_tensor_dist_attr_for_program(param)\n        process_mesh = param_dist_attr.process_mesh\n        dim_mapping = param_dist_attr.dims_mapping\n        for (axis, size) in enumerate(process_mesh.shape):\n            if size <= 1 or axis in dim_mapping:\n                pass\n            else:\n                group_ranks = _get_comm_group(process_mesh.process_ids, process_mesh.shape, axis, rank_id)\n                sync_group = new_process_group(group_ranks)\n                startup_block.append_op(type='c_broadcast', inputs={'X': param}, outputs={'Out': param}, attrs={'ring_id': sync_group.id, 'root': 0, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Forward})",
            "@staticmethod\ndef forward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        kwargs: inputname_mapping & outputname_mapping\\n        '\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    startup_block = dist_op_context.startup_block\n    src_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    assert op_dist_attr is not None, f\"forward op [{str(src_op)}] don't have dist attribute !\"\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out' in kwargs, 'output [{}] is not given'.format('Out')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input W take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out']) == 1, 'row_parallel_embedding output Out take 1 variable but got {}'.format(kwargs['Out'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_var = main_block._var_recursive(kwargs['Out'][0])\n    if src_op.type == 'lookup_table':\n        Ids_var = adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var)\n    embedding_row_dim_mapping = op_dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = op_dist_attr.process_mesh.shape\n    process_mesh_group = op_dist_attr.process_mesh.process_ids\n    if rank_id not in process_mesh_group:\n        rank_id = _get_corresponding_rank(ctx, op_dist_attr.process_mesh, rank_id)\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    parallel_axis = embedding_row_dim_mapping\n    group_ranks = _get_comm_group(process_mesh_group, process_mesh_shape, parallel_axis, rank_id)\n    group = new_process_group(group_ranks)\n    check_variable_and_dtype(Ids_var, 'input', ['int32', 'int64'], 'c_embedding')\n    out_tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(Out_var)\n    assert out_tensor_dist_attr is not None\n    out_var_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert out_var_dist_attr is not None\n    ref_shape = infer_shape(main_block, Out_var, out_tensor_dist_attr, out_var_dist_attr)\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['c_embedding', 'tmp'])), dtype=Weight_var.dtype, shape=Out_var.shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=Out_var.stop_gradient)\n    ctx.set_tensor_dist_attr_for_program(intermediate_var_0, out_var_dist_attr)\n    check_variable_and_dtype(Out_var, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], 'c_allreduce_sum')\n    c_embedding_op = main_block.append_op(type='c_embedding', inputs={'Ids': [Ids_var], 'W': [Weight_var]}, outputs={'Out': [intermediate_var_0]}, attrs={'start_index': relative_idx, OP_ROLE_KEY: src_op.attr('op_role')})\n    if intermediate_var_0.shape != ref_shape:\n        intermediate_var_0.desc.set_shape(ref_shape)\n    c_allreduce_sum_op = main_block.append_op(type='c_allreduce_sum', inputs={'X': [intermediate_var_0]}, outputs={'Out': [Out_var]}, attrs={'ring_id': group.id, 'use_calc_stream': True, 'use_model_parallel': True, OP_ROLE_KEY: src_op.attr('op_role')})\n    c_allreduce_sum_op._set_attr('op_namescope', '/' + ParallelMode.TensorParallel)\n    if Out_var.shape != ref_shape:\n        Out_var.desc.set_shape(ref_shape)\n    embedding_op_dist_attr = OperatorDistAttr()\n    embedding_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    embedding_op_dist_attr.impl_type = op_dist_attr.impl_type\n    embedding_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_embedding_op.desc.input_arg_names():\n        input_dist_attr = op_dist_attr.get_input_dist_attr(input_varname)\n        assert input_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        embedding_op_dist_attr.set_input_dist_attr(input_varname, input_dist_attr)\n    output_varname = c_embedding_op.desc.output_arg_names()[0]\n    output_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n    embedding_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_embedding_op, embedding_op_dist_attr)\n    allreduce_op_dist_attr = OperatorDistAttr()\n    allreduce_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    allreduce_op_dist_attr.impl_type = op_dist_attr.impl_type\n    allreduce_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_allreduce_sum_op.desc.input_arg_names():\n        input_var = main_block._var_recursive(input_varname)\n        tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(input_var)\n        assert tensor_dist_attr is not None\n        allreduce_op_dist_attr.set_input_dist_attr(input_varname, tensor_dist_attr)\n    for output_varname in c_allreduce_sum_op.desc.output_arg_names():\n        output_dist_attr = op_dist_attr.get_output_dist_attr(output_varname)\n        assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        allreduce_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_allreduce_sum_op, allreduce_op_dist_attr)\n    if Weight_var.is_parameter and (not op_dist_attr.is_recompute):\n        if Weight_var.name in dist_op_context.already_init_sync_vars:\n            return\n        dist_op_context.already_init_sync_vars.add(Weight_var.name)\n        param = startup_block.var(Weight_var.name)\n        param_dist_attr = ctx.get_tensor_dist_attr_for_program(param)\n        process_mesh = param_dist_attr.process_mesh\n        dim_mapping = param_dist_attr.dims_mapping\n        for (axis, size) in enumerate(process_mesh.shape):\n            if size <= 1 or axis in dim_mapping:\n                pass\n            else:\n                group_ranks = _get_comm_group(process_mesh.process_ids, process_mesh.shape, axis, rank_id)\n                sync_group = new_process_group(group_ranks)\n                startup_block.append_op(type='c_broadcast', inputs={'X': param}, outputs={'Out': param}, attrs={'ring_id': sync_group.id, 'root': 0, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Forward})",
            "@staticmethod\ndef forward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        kwargs: inputname_mapping & outputname_mapping\\n        '\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    startup_block = dist_op_context.startup_block\n    src_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    assert op_dist_attr is not None, f\"forward op [{str(src_op)}] don't have dist attribute !\"\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out' in kwargs, 'output [{}] is not given'.format('Out')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input W take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out']) == 1, 'row_parallel_embedding output Out take 1 variable but got {}'.format(kwargs['Out'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_var = main_block._var_recursive(kwargs['Out'][0])\n    if src_op.type == 'lookup_table':\n        Ids_var = adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var)\n    embedding_row_dim_mapping = op_dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = op_dist_attr.process_mesh.shape\n    process_mesh_group = op_dist_attr.process_mesh.process_ids\n    if rank_id not in process_mesh_group:\n        rank_id = _get_corresponding_rank(ctx, op_dist_attr.process_mesh, rank_id)\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    parallel_axis = embedding_row_dim_mapping\n    group_ranks = _get_comm_group(process_mesh_group, process_mesh_shape, parallel_axis, rank_id)\n    group = new_process_group(group_ranks)\n    check_variable_and_dtype(Ids_var, 'input', ['int32', 'int64'], 'c_embedding')\n    out_tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(Out_var)\n    assert out_tensor_dist_attr is not None\n    out_var_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert out_var_dist_attr is not None\n    ref_shape = infer_shape(main_block, Out_var, out_tensor_dist_attr, out_var_dist_attr)\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['c_embedding', 'tmp'])), dtype=Weight_var.dtype, shape=Out_var.shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=Out_var.stop_gradient)\n    ctx.set_tensor_dist_attr_for_program(intermediate_var_0, out_var_dist_attr)\n    check_variable_and_dtype(Out_var, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], 'c_allreduce_sum')\n    c_embedding_op = main_block.append_op(type='c_embedding', inputs={'Ids': [Ids_var], 'W': [Weight_var]}, outputs={'Out': [intermediate_var_0]}, attrs={'start_index': relative_idx, OP_ROLE_KEY: src_op.attr('op_role')})\n    if intermediate_var_0.shape != ref_shape:\n        intermediate_var_0.desc.set_shape(ref_shape)\n    c_allreduce_sum_op = main_block.append_op(type='c_allreduce_sum', inputs={'X': [intermediate_var_0]}, outputs={'Out': [Out_var]}, attrs={'ring_id': group.id, 'use_calc_stream': True, 'use_model_parallel': True, OP_ROLE_KEY: src_op.attr('op_role')})\n    c_allreduce_sum_op._set_attr('op_namescope', '/' + ParallelMode.TensorParallel)\n    if Out_var.shape != ref_shape:\n        Out_var.desc.set_shape(ref_shape)\n    embedding_op_dist_attr = OperatorDistAttr()\n    embedding_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    embedding_op_dist_attr.impl_type = op_dist_attr.impl_type\n    embedding_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_embedding_op.desc.input_arg_names():\n        input_dist_attr = op_dist_attr.get_input_dist_attr(input_varname)\n        assert input_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        embedding_op_dist_attr.set_input_dist_attr(input_varname, input_dist_attr)\n    output_varname = c_embedding_op.desc.output_arg_names()[0]\n    output_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n    embedding_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_embedding_op, embedding_op_dist_attr)\n    allreduce_op_dist_attr = OperatorDistAttr()\n    allreduce_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    allreduce_op_dist_attr.impl_type = op_dist_attr.impl_type\n    allreduce_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_allreduce_sum_op.desc.input_arg_names():\n        input_var = main_block._var_recursive(input_varname)\n        tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(input_var)\n        assert tensor_dist_attr is not None\n        allreduce_op_dist_attr.set_input_dist_attr(input_varname, tensor_dist_attr)\n    for output_varname in c_allreduce_sum_op.desc.output_arg_names():\n        output_dist_attr = op_dist_attr.get_output_dist_attr(output_varname)\n        assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        allreduce_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_allreduce_sum_op, allreduce_op_dist_attr)\n    if Weight_var.is_parameter and (not op_dist_attr.is_recompute):\n        if Weight_var.name in dist_op_context.already_init_sync_vars:\n            return\n        dist_op_context.already_init_sync_vars.add(Weight_var.name)\n        param = startup_block.var(Weight_var.name)\n        param_dist_attr = ctx.get_tensor_dist_attr_for_program(param)\n        process_mesh = param_dist_attr.process_mesh\n        dim_mapping = param_dist_attr.dims_mapping\n        for (axis, size) in enumerate(process_mesh.shape):\n            if size <= 1 or axis in dim_mapping:\n                pass\n            else:\n                group_ranks = _get_comm_group(process_mesh.process_ids, process_mesh.shape, axis, rank_id)\n                sync_group = new_process_group(group_ranks)\n                startup_block.append_op(type='c_broadcast', inputs={'X': param}, outputs={'Out': param}, attrs={'ring_id': sync_group.id, 'root': 0, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Forward})",
            "@staticmethod\ndef forward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        kwargs: inputname_mapping & outputname_mapping\\n        '\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    startup_block = dist_op_context.startup_block\n    src_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    assert op_dist_attr is not None, f\"forward op [{str(src_op)}] don't have dist attribute !\"\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out' in kwargs, 'output [{}] is not given'.format('Out')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input W take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out']) == 1, 'row_parallel_embedding output Out take 1 variable but got {}'.format(kwargs['Out'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_var = main_block._var_recursive(kwargs['Out'][0])\n    if src_op.type == 'lookup_table':\n        Ids_var = adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var)\n    embedding_row_dim_mapping = op_dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = op_dist_attr.process_mesh.shape\n    process_mesh_group = op_dist_attr.process_mesh.process_ids\n    if rank_id not in process_mesh_group:\n        rank_id = _get_corresponding_rank(ctx, op_dist_attr.process_mesh, rank_id)\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    parallel_axis = embedding_row_dim_mapping\n    group_ranks = _get_comm_group(process_mesh_group, process_mesh_shape, parallel_axis, rank_id)\n    group = new_process_group(group_ranks)\n    check_variable_and_dtype(Ids_var, 'input', ['int32', 'int64'], 'c_embedding')\n    out_tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(Out_var)\n    assert out_tensor_dist_attr is not None\n    out_var_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert out_var_dist_attr is not None\n    ref_shape = infer_shape(main_block, Out_var, out_tensor_dist_attr, out_var_dist_attr)\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['c_embedding', 'tmp'])), dtype=Weight_var.dtype, shape=Out_var.shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=Out_var.stop_gradient)\n    ctx.set_tensor_dist_attr_for_program(intermediate_var_0, out_var_dist_attr)\n    check_variable_and_dtype(Out_var, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], 'c_allreduce_sum')\n    c_embedding_op = main_block.append_op(type='c_embedding', inputs={'Ids': [Ids_var], 'W': [Weight_var]}, outputs={'Out': [intermediate_var_0]}, attrs={'start_index': relative_idx, OP_ROLE_KEY: src_op.attr('op_role')})\n    if intermediate_var_0.shape != ref_shape:\n        intermediate_var_0.desc.set_shape(ref_shape)\n    c_allreduce_sum_op = main_block.append_op(type='c_allreduce_sum', inputs={'X': [intermediate_var_0]}, outputs={'Out': [Out_var]}, attrs={'ring_id': group.id, 'use_calc_stream': True, 'use_model_parallel': True, OP_ROLE_KEY: src_op.attr('op_role')})\n    c_allreduce_sum_op._set_attr('op_namescope', '/' + ParallelMode.TensorParallel)\n    if Out_var.shape != ref_shape:\n        Out_var.desc.set_shape(ref_shape)\n    embedding_op_dist_attr = OperatorDistAttr()\n    embedding_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    embedding_op_dist_attr.impl_type = op_dist_attr.impl_type\n    embedding_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_embedding_op.desc.input_arg_names():\n        input_dist_attr = op_dist_attr.get_input_dist_attr(input_varname)\n        assert input_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        embedding_op_dist_attr.set_input_dist_attr(input_varname, input_dist_attr)\n    output_varname = c_embedding_op.desc.output_arg_names()[0]\n    output_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n    embedding_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_embedding_op, embedding_op_dist_attr)\n    allreduce_op_dist_attr = OperatorDistAttr()\n    allreduce_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    allreduce_op_dist_attr.impl_type = op_dist_attr.impl_type\n    allreduce_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_allreduce_sum_op.desc.input_arg_names():\n        input_var = main_block._var_recursive(input_varname)\n        tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(input_var)\n        assert tensor_dist_attr is not None\n        allreduce_op_dist_attr.set_input_dist_attr(input_varname, tensor_dist_attr)\n    for output_varname in c_allreduce_sum_op.desc.output_arg_names():\n        output_dist_attr = op_dist_attr.get_output_dist_attr(output_varname)\n        assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        allreduce_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_allreduce_sum_op, allreduce_op_dist_attr)\n    if Weight_var.is_parameter and (not op_dist_attr.is_recompute):\n        if Weight_var.name in dist_op_context.already_init_sync_vars:\n            return\n        dist_op_context.already_init_sync_vars.add(Weight_var.name)\n        param = startup_block.var(Weight_var.name)\n        param_dist_attr = ctx.get_tensor_dist_attr_for_program(param)\n        process_mesh = param_dist_attr.process_mesh\n        dim_mapping = param_dist_attr.dims_mapping\n        for (axis, size) in enumerate(process_mesh.shape):\n            if size <= 1 or axis in dim_mapping:\n                pass\n            else:\n                group_ranks = _get_comm_group(process_mesh.process_ids, process_mesh.shape, axis, rank_id)\n                sync_group = new_process_group(group_ranks)\n                startup_block.append_op(type='c_broadcast', inputs={'X': param}, outputs={'Out': param}, attrs={'ring_id': sync_group.id, 'root': 0, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Forward})",
            "@staticmethod\ndef forward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        kwargs: inputname_mapping & outputname_mapping\\n        '\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    startup_block = dist_op_context.startup_block\n    src_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    op_dist_attr = ctx.get_op_dist_attr_for_program(src_op)\n    assert op_dist_attr is not None, f\"forward op [{str(src_op)}] don't have dist attribute !\"\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out' in kwargs, 'output [{}] is not given'.format('Out')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input W take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out']) == 1, 'row_parallel_embedding output Out take 1 variable but got {}'.format(kwargs['Out'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_var = main_block._var_recursive(kwargs['Out'][0])\n    if src_op.type == 'lookup_table':\n        Ids_var = adopt_lookup_table_v1(ctx, main_block, src_op, Ids_var)\n    embedding_row_dim_mapping = op_dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = op_dist_attr.process_mesh.shape\n    process_mesh_group = op_dist_attr.process_mesh.process_ids\n    if rank_id not in process_mesh_group:\n        rank_id = _get_corresponding_rank(ctx, op_dist_attr.process_mesh, rank_id)\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    parallel_axis = embedding_row_dim_mapping\n    group_ranks = _get_comm_group(process_mesh_group, process_mesh_shape, parallel_axis, rank_id)\n    group = new_process_group(group_ranks)\n    check_variable_and_dtype(Ids_var, 'input', ['int32', 'int64'], 'c_embedding')\n    out_tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(Out_var)\n    assert out_tensor_dist_attr is not None\n    out_var_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert out_var_dist_attr is not None\n    ref_shape = infer_shape(main_block, Out_var, out_tensor_dist_attr, out_var_dist_attr)\n    intermediate_var_0 = main_block.create_var(name=unique_name.generate_with_ignorable_key('.'.join(['c_embedding', 'tmp'])), dtype=Weight_var.dtype, shape=Out_var.shape, type=core.VarDesc.VarType.LOD_TENSOR, persistable=False, stop_gradient=Out_var.stop_gradient)\n    ctx.set_tensor_dist_attr_for_program(intermediate_var_0, out_var_dist_attr)\n    check_variable_and_dtype(Out_var, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], 'c_allreduce_sum')\n    c_embedding_op = main_block.append_op(type='c_embedding', inputs={'Ids': [Ids_var], 'W': [Weight_var]}, outputs={'Out': [intermediate_var_0]}, attrs={'start_index': relative_idx, OP_ROLE_KEY: src_op.attr('op_role')})\n    if intermediate_var_0.shape != ref_shape:\n        intermediate_var_0.desc.set_shape(ref_shape)\n    c_allreduce_sum_op = main_block.append_op(type='c_allreduce_sum', inputs={'X': [intermediate_var_0]}, outputs={'Out': [Out_var]}, attrs={'ring_id': group.id, 'use_calc_stream': True, 'use_model_parallel': True, OP_ROLE_KEY: src_op.attr('op_role')})\n    c_allreduce_sum_op._set_attr('op_namescope', '/' + ParallelMode.TensorParallel)\n    if Out_var.shape != ref_shape:\n        Out_var.desc.set_shape(ref_shape)\n    embedding_op_dist_attr = OperatorDistAttr()\n    embedding_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    embedding_op_dist_attr.impl_type = op_dist_attr.impl_type\n    embedding_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_embedding_op.desc.input_arg_names():\n        input_dist_attr = op_dist_attr.get_input_dist_attr(input_varname)\n        assert input_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        embedding_op_dist_attr.set_input_dist_attr(input_varname, input_dist_attr)\n    output_varname = c_embedding_op.desc.output_arg_names()[0]\n    output_dist_attr = op_dist_attr.get_output_dist_attr(Out_var.name)\n    assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n    embedding_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_embedding_op, embedding_op_dist_attr)\n    allreduce_op_dist_attr = OperatorDistAttr()\n    allreduce_op_dist_attr.process_mesh = op_dist_attr.process_mesh\n    allreduce_op_dist_attr.impl_type = op_dist_attr.impl_type\n    allreduce_op_dist_attr.impl_idx = op_dist_attr.impl_idx\n    for input_varname in c_allreduce_sum_op.desc.input_arg_names():\n        input_var = main_block._var_recursive(input_varname)\n        tensor_dist_attr = ctx.get_tensor_dist_attr_for_program(input_var)\n        assert tensor_dist_attr is not None\n        allreduce_op_dist_attr.set_input_dist_attr(input_varname, tensor_dist_attr)\n    for output_varname in c_allreduce_sum_op.desc.output_arg_names():\n        output_dist_attr = op_dist_attr.get_output_dist_attr(output_varname)\n        assert output_dist_attr is not None, f'dist_attr is {op_dist_attr}'\n        allreduce_op_dist_attr.set_output_dist_attr(output_varname, output_dist_attr)\n    ctx.set_op_dist_attr_for_program(c_allreduce_sum_op, allreduce_op_dist_attr)\n    if Weight_var.is_parameter and (not op_dist_attr.is_recompute):\n        if Weight_var.name in dist_op_context.already_init_sync_vars:\n            return\n        dist_op_context.already_init_sync_vars.add(Weight_var.name)\n        param = startup_block.var(Weight_var.name)\n        param_dist_attr = ctx.get_tensor_dist_attr_for_program(param)\n        process_mesh = param_dist_attr.process_mesh\n        dim_mapping = param_dist_attr.dims_mapping\n        for (axis, size) in enumerate(process_mesh.shape):\n            if size <= 1 or axis in dim_mapping:\n                pass\n            else:\n                group_ranks = _get_comm_group(process_mesh.process_ids, process_mesh.shape, axis, rank_id)\n                sync_group = new_process_group(group_ranks)\n                startup_block.append_op(type='c_broadcast', inputs={'X': param}, outputs={'Out': param}, attrs={'ring_id': sync_group.id, 'root': 0, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Forward})"
        ]
    },
    {
        "func_name": "backward",
        "original": "@staticmethod\ndef backward(ctx, *args, **kwargs):\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    backward_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    dist_attr = ctx.get_op_dist_attr_for_program(backward_op)\n    assert dist_attr is not None, f\"backward op [{str(backward_op)}] don't have dist attribute !\"\n    if rank_id not in dist_attr.process_mesh.process_ids:\n        rank_id = _get_corresponding_rank(ctx, dist_attr.process_mesh, rank_id)\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out@GRAD' in kwargs, 'input [{}] is not given'.format('Out')\n    assert 'W@GRAD' in kwargs, 'output [{}] is not given'.format('W@GRAD')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out@GRAD']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Out'])\n    assert len(kwargs['W@GRAD']) == 1, 'row_parallel_embedding output Ids take 1 variable but got {}'.format(kwargs['W@GRAD'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_grad = main_block._var_recursive(kwargs['Out@GRAD'][0])\n    Weight_grad = main_block._var_recursive(kwargs['W@GRAD'][0])\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = dist_attr.process_mesh.shape\n    process_mesh_group = dist_attr.process_mesh.process_ids\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    check_variable_and_dtype(Out_grad, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], '_c_identity')\n    c_embedding_grad_op_desc = main_block.append_op(type='nop').desc\n    c_embedding_grad_op_desc.set_type('c_embedding_grad')\n    c_embedding_grad_op_desc.set_input('Ids', [Ids_var.name])\n    c_embedding_grad_op_desc.set_input('W', [Weight_var.name])\n    c_embedding_grad_op_desc.set_input('Out@GRAD', [Out_grad.name])\n    c_embedding_grad_op_desc.set_output('W@GRAD', [Weight_grad.name])\n    c_embedding_grad_op_desc._set_attr('start_index', relative_idx)\n    c_embedding_grad_op_desc._set_attr(OP_ROLE_KEY, OpRole.Backward)\n    c_embedding_grad_op = main_block.ops[-1]\n    assert c_embedding_grad_op.type == 'c_embedding_grad'\n    naive_copy_op_dist_attr_for_program(c_embedding_grad_op, backward_op, ctx)\n    act_grad_names = [Ids_var.name]\n    out_grad_names = [kwargs['W@GRAD'][0]]\n    gradient_synchronization(ctx, backward_op, act_grad_names, out_grad_names, rank_id)",
        "mutated": [
            "@staticmethod\ndef backward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    backward_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    dist_attr = ctx.get_op_dist_attr_for_program(backward_op)\n    assert dist_attr is not None, f\"backward op [{str(backward_op)}] don't have dist attribute !\"\n    if rank_id not in dist_attr.process_mesh.process_ids:\n        rank_id = _get_corresponding_rank(ctx, dist_attr.process_mesh, rank_id)\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out@GRAD' in kwargs, 'input [{}] is not given'.format('Out')\n    assert 'W@GRAD' in kwargs, 'output [{}] is not given'.format('W@GRAD')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out@GRAD']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Out'])\n    assert len(kwargs['W@GRAD']) == 1, 'row_parallel_embedding output Ids take 1 variable but got {}'.format(kwargs['W@GRAD'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_grad = main_block._var_recursive(kwargs['Out@GRAD'][0])\n    Weight_grad = main_block._var_recursive(kwargs['W@GRAD'][0])\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = dist_attr.process_mesh.shape\n    process_mesh_group = dist_attr.process_mesh.process_ids\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    check_variable_and_dtype(Out_grad, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], '_c_identity')\n    c_embedding_grad_op_desc = main_block.append_op(type='nop').desc\n    c_embedding_grad_op_desc.set_type('c_embedding_grad')\n    c_embedding_grad_op_desc.set_input('Ids', [Ids_var.name])\n    c_embedding_grad_op_desc.set_input('W', [Weight_var.name])\n    c_embedding_grad_op_desc.set_input('Out@GRAD', [Out_grad.name])\n    c_embedding_grad_op_desc.set_output('W@GRAD', [Weight_grad.name])\n    c_embedding_grad_op_desc._set_attr('start_index', relative_idx)\n    c_embedding_grad_op_desc._set_attr(OP_ROLE_KEY, OpRole.Backward)\n    c_embedding_grad_op = main_block.ops[-1]\n    assert c_embedding_grad_op.type == 'c_embedding_grad'\n    naive_copy_op_dist_attr_for_program(c_embedding_grad_op, backward_op, ctx)\n    act_grad_names = [Ids_var.name]\n    out_grad_names = [kwargs['W@GRAD'][0]]\n    gradient_synchronization(ctx, backward_op, act_grad_names, out_grad_names, rank_id)",
            "@staticmethod\ndef backward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    backward_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    dist_attr = ctx.get_op_dist_attr_for_program(backward_op)\n    assert dist_attr is not None, f\"backward op [{str(backward_op)}] don't have dist attribute !\"\n    if rank_id not in dist_attr.process_mesh.process_ids:\n        rank_id = _get_corresponding_rank(ctx, dist_attr.process_mesh, rank_id)\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out@GRAD' in kwargs, 'input [{}] is not given'.format('Out')\n    assert 'W@GRAD' in kwargs, 'output [{}] is not given'.format('W@GRAD')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out@GRAD']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Out'])\n    assert len(kwargs['W@GRAD']) == 1, 'row_parallel_embedding output Ids take 1 variable but got {}'.format(kwargs['W@GRAD'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_grad = main_block._var_recursive(kwargs['Out@GRAD'][0])\n    Weight_grad = main_block._var_recursive(kwargs['W@GRAD'][0])\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = dist_attr.process_mesh.shape\n    process_mesh_group = dist_attr.process_mesh.process_ids\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    check_variable_and_dtype(Out_grad, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], '_c_identity')\n    c_embedding_grad_op_desc = main_block.append_op(type='nop').desc\n    c_embedding_grad_op_desc.set_type('c_embedding_grad')\n    c_embedding_grad_op_desc.set_input('Ids', [Ids_var.name])\n    c_embedding_grad_op_desc.set_input('W', [Weight_var.name])\n    c_embedding_grad_op_desc.set_input('Out@GRAD', [Out_grad.name])\n    c_embedding_grad_op_desc.set_output('W@GRAD', [Weight_grad.name])\n    c_embedding_grad_op_desc._set_attr('start_index', relative_idx)\n    c_embedding_grad_op_desc._set_attr(OP_ROLE_KEY, OpRole.Backward)\n    c_embedding_grad_op = main_block.ops[-1]\n    assert c_embedding_grad_op.type == 'c_embedding_grad'\n    naive_copy_op_dist_attr_for_program(c_embedding_grad_op, backward_op, ctx)\n    act_grad_names = [Ids_var.name]\n    out_grad_names = [kwargs['W@GRAD'][0]]\n    gradient_synchronization(ctx, backward_op, act_grad_names, out_grad_names, rank_id)",
            "@staticmethod\ndef backward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    backward_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    dist_attr = ctx.get_op_dist_attr_for_program(backward_op)\n    assert dist_attr is not None, f\"backward op [{str(backward_op)}] don't have dist attribute !\"\n    if rank_id not in dist_attr.process_mesh.process_ids:\n        rank_id = _get_corresponding_rank(ctx, dist_attr.process_mesh, rank_id)\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out@GRAD' in kwargs, 'input [{}] is not given'.format('Out')\n    assert 'W@GRAD' in kwargs, 'output [{}] is not given'.format('W@GRAD')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out@GRAD']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Out'])\n    assert len(kwargs['W@GRAD']) == 1, 'row_parallel_embedding output Ids take 1 variable but got {}'.format(kwargs['W@GRAD'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_grad = main_block._var_recursive(kwargs['Out@GRAD'][0])\n    Weight_grad = main_block._var_recursive(kwargs['W@GRAD'][0])\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = dist_attr.process_mesh.shape\n    process_mesh_group = dist_attr.process_mesh.process_ids\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    check_variable_and_dtype(Out_grad, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], '_c_identity')\n    c_embedding_grad_op_desc = main_block.append_op(type='nop').desc\n    c_embedding_grad_op_desc.set_type('c_embedding_grad')\n    c_embedding_grad_op_desc.set_input('Ids', [Ids_var.name])\n    c_embedding_grad_op_desc.set_input('W', [Weight_var.name])\n    c_embedding_grad_op_desc.set_input('Out@GRAD', [Out_grad.name])\n    c_embedding_grad_op_desc.set_output('W@GRAD', [Weight_grad.name])\n    c_embedding_grad_op_desc._set_attr('start_index', relative_idx)\n    c_embedding_grad_op_desc._set_attr(OP_ROLE_KEY, OpRole.Backward)\n    c_embedding_grad_op = main_block.ops[-1]\n    assert c_embedding_grad_op.type == 'c_embedding_grad'\n    naive_copy_op_dist_attr_for_program(c_embedding_grad_op, backward_op, ctx)\n    act_grad_names = [Ids_var.name]\n    out_grad_names = [kwargs['W@GRAD'][0]]\n    gradient_synchronization(ctx, backward_op, act_grad_names, out_grad_names, rank_id)",
            "@staticmethod\ndef backward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    backward_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    dist_attr = ctx.get_op_dist_attr_for_program(backward_op)\n    assert dist_attr is not None, f\"backward op [{str(backward_op)}] don't have dist attribute !\"\n    if rank_id not in dist_attr.process_mesh.process_ids:\n        rank_id = _get_corresponding_rank(ctx, dist_attr.process_mesh, rank_id)\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out@GRAD' in kwargs, 'input [{}] is not given'.format('Out')\n    assert 'W@GRAD' in kwargs, 'output [{}] is not given'.format('W@GRAD')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out@GRAD']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Out'])\n    assert len(kwargs['W@GRAD']) == 1, 'row_parallel_embedding output Ids take 1 variable but got {}'.format(kwargs['W@GRAD'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_grad = main_block._var_recursive(kwargs['Out@GRAD'][0])\n    Weight_grad = main_block._var_recursive(kwargs['W@GRAD'][0])\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = dist_attr.process_mesh.shape\n    process_mesh_group = dist_attr.process_mesh.process_ids\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    check_variable_and_dtype(Out_grad, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], '_c_identity')\n    c_embedding_grad_op_desc = main_block.append_op(type='nop').desc\n    c_embedding_grad_op_desc.set_type('c_embedding_grad')\n    c_embedding_grad_op_desc.set_input('Ids', [Ids_var.name])\n    c_embedding_grad_op_desc.set_input('W', [Weight_var.name])\n    c_embedding_grad_op_desc.set_input('Out@GRAD', [Out_grad.name])\n    c_embedding_grad_op_desc.set_output('W@GRAD', [Weight_grad.name])\n    c_embedding_grad_op_desc._set_attr('start_index', relative_idx)\n    c_embedding_grad_op_desc._set_attr(OP_ROLE_KEY, OpRole.Backward)\n    c_embedding_grad_op = main_block.ops[-1]\n    assert c_embedding_grad_op.type == 'c_embedding_grad'\n    naive_copy_op_dist_attr_for_program(c_embedding_grad_op, backward_op, ctx)\n    act_grad_names = [Ids_var.name]\n    out_grad_names = [kwargs['W@GRAD'][0]]\n    gradient_synchronization(ctx, backward_op, act_grad_names, out_grad_names, rank_id)",
            "@staticmethod\ndef backward(ctx, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dist_op_context = ctx.dist_op_context\n    main_block = dist_op_context.work_block\n    backward_op = dist_op_context.cur_src_op\n    rank_id = dist_op_context.rank_id\n    dist_attr = ctx.get_op_dist_attr_for_program(backward_op)\n    assert dist_attr is not None, f\"backward op [{str(backward_op)}] don't have dist attribute !\"\n    if rank_id not in dist_attr.process_mesh.process_ids:\n        rank_id = _get_corresponding_rank(ctx, dist_attr.process_mesh, rank_id)\n    assert 'Ids' in kwargs, 'input [{}] is not given'.format('Ids')\n    assert 'W' in kwargs, 'input [{}] is not given'.format('W')\n    assert 'Out@GRAD' in kwargs, 'input [{}] is not given'.format('Out')\n    assert 'W@GRAD' in kwargs, 'output [{}] is not given'.format('W@GRAD')\n    assert len(kwargs['Ids']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Ids'])\n    assert len(kwargs['W']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['W'])\n    assert len(kwargs['Out@GRAD']) == 1, 'row_parallel_embedding input Ids take 1 variable but got {}'.format(kwargs['Out'])\n    assert len(kwargs['W@GRAD']) == 1, 'row_parallel_embedding output Ids take 1 variable but got {}'.format(kwargs['W@GRAD'])\n    Ids_var = main_block._var_recursive(kwargs['Ids'][0])\n    Weight_var = main_block._var_recursive(kwargs['W'][0])\n    Out_grad = main_block._var_recursive(kwargs['Out@GRAD'][0])\n    Weight_grad = main_block._var_recursive(kwargs['W@GRAD'][0])\n    embedding_row_dim_mapping = dist_attr.get_input_dims_mapping(Weight_var.name)[0]\n    assert embedding_row_dim_mapping >= 0, \"row_parallel_embedding's row should be divided by a specific mesh axis, but got [{}]\".format(embedding_row_dim_mapping)\n    process_mesh_shape = dist_attr.process_mesh.shape\n    process_mesh_group = dist_attr.process_mesh.process_ids\n    relative_idx = _get_idx_in_axis(process_mesh_group, process_mesh_shape, embedding_row_dim_mapping, rank_id)\n    per_part_size = Weight_var.shape[0]\n    relative_idx = relative_idx * per_part_size\n    check_variable_and_dtype(Out_grad, 'tensor', ['float16', 'float32', 'float64', 'int32', 'int64', 'uint16'], '_c_identity')\n    c_embedding_grad_op_desc = main_block.append_op(type='nop').desc\n    c_embedding_grad_op_desc.set_type('c_embedding_grad')\n    c_embedding_grad_op_desc.set_input('Ids', [Ids_var.name])\n    c_embedding_grad_op_desc.set_input('W', [Weight_var.name])\n    c_embedding_grad_op_desc.set_input('Out@GRAD', [Out_grad.name])\n    c_embedding_grad_op_desc.set_output('W@GRAD', [Weight_grad.name])\n    c_embedding_grad_op_desc._set_attr('start_index', relative_idx)\n    c_embedding_grad_op_desc._set_attr(OP_ROLE_KEY, OpRole.Backward)\n    c_embedding_grad_op = main_block.ops[-1]\n    assert c_embedding_grad_op.type == 'c_embedding_grad'\n    naive_copy_op_dist_attr_for_program(c_embedding_grad_op, backward_op, ctx)\n    act_grad_names = [Ids_var.name]\n    out_grad_names = [kwargs['W@GRAD'][0]]\n    gradient_synchronization(ctx, backward_op, act_grad_names, out_grad_names, rank_id)"
        ]
    }
]