[
    {
        "func_name": "__init__",
        "original": "def __init__(self, **kwds):\n    dict.__init__(self, kwds)\n    self.__dict__ = self\n    self._initialize()",
        "mutated": [
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n    dict.__init__(self, kwds)\n    self.__dict__ = self\n    self._initialize()",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dict.__init__(self, kwds)\n    self.__dict__ = self\n    self._initialize()",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dict.__init__(self, kwds)\n    self.__dict__ = self\n    self._initialize()",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dict.__init__(self, kwds)\n    self.__dict__ = self\n    self._initialize()",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dict.__init__(self, kwds)\n    self.__dict__ = self\n    self._initialize()"
        ]
    },
    {
        "func_name": "_initialize",
        "original": "def _initialize(self):\n    pass",
        "mutated": [
            "def _initialize(self):\n    if False:\n        i = 10\n    pass",
            "def _initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "def _initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "def _initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "def _initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "__str__",
        "original": "def __str__(self):\n    return self.template % self",
        "mutated": [
            "def __str__(self):\n    if False:\n        i = 10\n    return self.template % self",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.template % self",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.template % self",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.template % self",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.template % self"
        ]
    },
    {
        "func_name": "_int_ifclose",
        "original": "def _int_ifclose(x, dec=1, width=4):\n    \"\"\"helper function for creating result string for int or float\n\n    only dec=1 and width=4 is implemented\n\n    Parameters\n    ----------\n    x : int or float\n        value to format\n    dec : 1\n        number of decimals to print if x is not an integer\n    width : 4\n        width of string\n\n    Returns\n    -------\n    xint : int or float\n        x is converted to int if it is within 1e-14 of an integer\n    x_string : str\n        x formatted as string, either '%4d' or '%4.1f'\n\n    \"\"\"\n    xint = int(round(x))\n    if np.max(np.abs(xint - x)) < 1e-14:\n        return (xint, '%4d' % xint)\n    else:\n        return (x, '%4.1f' % x)",
        "mutated": [
            "def _int_ifclose(x, dec=1, width=4):\n    if False:\n        i = 10\n    \"helper function for creating result string for int or float\\n\\n    only dec=1 and width=4 is implemented\\n\\n    Parameters\\n    ----------\\n    x : int or float\\n        value to format\\n    dec : 1\\n        number of decimals to print if x is not an integer\\n    width : 4\\n        width of string\\n\\n    Returns\\n    -------\\n    xint : int or float\\n        x is converted to int if it is within 1e-14 of an integer\\n    x_string : str\\n        x formatted as string, either '%4d' or '%4.1f'\\n\\n    \"\n    xint = int(round(x))\n    if np.max(np.abs(xint - x)) < 1e-14:\n        return (xint, '%4d' % xint)\n    else:\n        return (x, '%4.1f' % x)",
            "def _int_ifclose(x, dec=1, width=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"helper function for creating result string for int or float\\n\\n    only dec=1 and width=4 is implemented\\n\\n    Parameters\\n    ----------\\n    x : int or float\\n        value to format\\n    dec : 1\\n        number of decimals to print if x is not an integer\\n    width : 4\\n        width of string\\n\\n    Returns\\n    -------\\n    xint : int or float\\n        x is converted to int if it is within 1e-14 of an integer\\n    x_string : str\\n        x formatted as string, either '%4d' or '%4.1f'\\n\\n    \"\n    xint = int(round(x))\n    if np.max(np.abs(xint - x)) < 1e-14:\n        return (xint, '%4d' % xint)\n    else:\n        return (x, '%4.1f' % x)",
            "def _int_ifclose(x, dec=1, width=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"helper function for creating result string for int or float\\n\\n    only dec=1 and width=4 is implemented\\n\\n    Parameters\\n    ----------\\n    x : int or float\\n        value to format\\n    dec : 1\\n        number of decimals to print if x is not an integer\\n    width : 4\\n        width of string\\n\\n    Returns\\n    -------\\n    xint : int or float\\n        x is converted to int if it is within 1e-14 of an integer\\n    x_string : str\\n        x formatted as string, either '%4d' or '%4.1f'\\n\\n    \"\n    xint = int(round(x))\n    if np.max(np.abs(xint - x)) < 1e-14:\n        return (xint, '%4d' % xint)\n    else:\n        return (x, '%4.1f' % x)",
            "def _int_ifclose(x, dec=1, width=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"helper function for creating result string for int or float\\n\\n    only dec=1 and width=4 is implemented\\n\\n    Parameters\\n    ----------\\n    x : int or float\\n        value to format\\n    dec : 1\\n        number of decimals to print if x is not an integer\\n    width : 4\\n        width of string\\n\\n    Returns\\n    -------\\n    xint : int or float\\n        x is converted to int if it is within 1e-14 of an integer\\n    x_string : str\\n        x formatted as string, either '%4d' or '%4.1f'\\n\\n    \"\n    xint = int(round(x))\n    if np.max(np.abs(xint - x)) < 1e-14:\n        return (xint, '%4d' % xint)\n    else:\n        return (x, '%4.1f' % x)",
            "def _int_ifclose(x, dec=1, width=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"helper function for creating result string for int or float\\n\\n    only dec=1 and width=4 is implemented\\n\\n    Parameters\\n    ----------\\n    x : int or float\\n        value to format\\n    dec : 1\\n        number of decimals to print if x is not an integer\\n    width : 4\\n        width of string\\n\\n    Returns\\n    -------\\n    xint : int or float\\n        x is converted to int if it is within 1e-14 of an integer\\n    x_string : str\\n        x formatted as string, either '%4d' or '%4.1f'\\n\\n    \"\n    xint = int(round(x))\n    if np.max(np.abs(xint - x)) < 1e-14:\n        return (xint, '%4d' % xint)\n    else:\n        return (x, '%4.1f' % x)"
        ]
    },
    {
        "func_name": "aggregate_raters",
        "original": "def aggregate_raters(data, n_cat=None):\n    \"\"\"convert raw data with shape (subject, rater) to (subject, cat_counts)\n\n    brings data into correct format for fleiss_kappa\n\n    bincount will raise exception if data cannot be converted to integer.\n\n    Parameters\n    ----------\n    data : array_like, 2-Dim\n        data containing category assignment with subjects in rows and raters\n        in columns.\n    n_cat : None or int\n        If None, then the data is converted to integer categories,\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\n        with non-zero counts are included.\n        If this is an integer, then the category levels in the data are already\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\n        returned array may contain columns with zero count, if no subject\n        has been categorized with this level.\n\n    Returns\n    -------\n    arr : nd_array, (n_rows, n_cat)\n        Contains counts of raters that assigned a category level to individuals.\n        Subjects are in rows, category levels in columns.\n    categories : nd_array, (n_category_levels,)\n        Contains the category levels.\n\n    \"\"\"\n    data = np.asarray(data)\n    n_rows = data.shape[0]\n    if n_cat is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n    else:\n        cat_uni = np.arange(n_cat)\n        data_ = data\n    tt = np.zeros((n_rows, n_cat), int)\n    for (idx, row) in enumerate(data_):\n        ro = np.bincount(row)\n        tt[idx, :len(ro)] = ro\n    return (tt, cat_uni)",
        "mutated": [
            "def aggregate_raters(data, n_cat=None):\n    if False:\n        i = 10\n    'convert raw data with shape (subject, rater) to (subject, cat_counts)\\n\\n    brings data into correct format for fleiss_kappa\\n\\n    bincount will raise exception if data cannot be converted to integer.\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    n_cat : None or int\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_rows, n_cat)\\n        Contains counts of raters that assigned a category level to individuals.\\n        Subjects are in rows, category levels in columns.\\n    categories : nd_array, (n_category_levels,)\\n        Contains the category levels.\\n\\n    '\n    data = np.asarray(data)\n    n_rows = data.shape[0]\n    if n_cat is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n    else:\n        cat_uni = np.arange(n_cat)\n        data_ = data\n    tt = np.zeros((n_rows, n_cat), int)\n    for (idx, row) in enumerate(data_):\n        ro = np.bincount(row)\n        tt[idx, :len(ro)] = ro\n    return (tt, cat_uni)",
            "def aggregate_raters(data, n_cat=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'convert raw data with shape (subject, rater) to (subject, cat_counts)\\n\\n    brings data into correct format for fleiss_kappa\\n\\n    bincount will raise exception if data cannot be converted to integer.\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    n_cat : None or int\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_rows, n_cat)\\n        Contains counts of raters that assigned a category level to individuals.\\n        Subjects are in rows, category levels in columns.\\n    categories : nd_array, (n_category_levels,)\\n        Contains the category levels.\\n\\n    '\n    data = np.asarray(data)\n    n_rows = data.shape[0]\n    if n_cat is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n    else:\n        cat_uni = np.arange(n_cat)\n        data_ = data\n    tt = np.zeros((n_rows, n_cat), int)\n    for (idx, row) in enumerate(data_):\n        ro = np.bincount(row)\n        tt[idx, :len(ro)] = ro\n    return (tt, cat_uni)",
            "def aggregate_raters(data, n_cat=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'convert raw data with shape (subject, rater) to (subject, cat_counts)\\n\\n    brings data into correct format for fleiss_kappa\\n\\n    bincount will raise exception if data cannot be converted to integer.\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    n_cat : None or int\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_rows, n_cat)\\n        Contains counts of raters that assigned a category level to individuals.\\n        Subjects are in rows, category levels in columns.\\n    categories : nd_array, (n_category_levels,)\\n        Contains the category levels.\\n\\n    '\n    data = np.asarray(data)\n    n_rows = data.shape[0]\n    if n_cat is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n    else:\n        cat_uni = np.arange(n_cat)\n        data_ = data\n    tt = np.zeros((n_rows, n_cat), int)\n    for (idx, row) in enumerate(data_):\n        ro = np.bincount(row)\n        tt[idx, :len(ro)] = ro\n    return (tt, cat_uni)",
            "def aggregate_raters(data, n_cat=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'convert raw data with shape (subject, rater) to (subject, cat_counts)\\n\\n    brings data into correct format for fleiss_kappa\\n\\n    bincount will raise exception if data cannot be converted to integer.\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    n_cat : None or int\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_rows, n_cat)\\n        Contains counts of raters that assigned a category level to individuals.\\n        Subjects are in rows, category levels in columns.\\n    categories : nd_array, (n_category_levels,)\\n        Contains the category levels.\\n\\n    '\n    data = np.asarray(data)\n    n_rows = data.shape[0]\n    if n_cat is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n    else:\n        cat_uni = np.arange(n_cat)\n        data_ = data\n    tt = np.zeros((n_rows, n_cat), int)\n    for (idx, row) in enumerate(data_):\n        ro = np.bincount(row)\n        tt[idx, :len(ro)] = ro\n    return (tt, cat_uni)",
            "def aggregate_raters(data, n_cat=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'convert raw data with shape (subject, rater) to (subject, cat_counts)\\n\\n    brings data into correct format for fleiss_kappa\\n\\n    bincount will raise exception if data cannot be converted to integer.\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    n_cat : None or int\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_rows, n_cat)\\n        Contains counts of raters that assigned a category level to individuals.\\n        Subjects are in rows, category levels in columns.\\n    categories : nd_array, (n_category_levels,)\\n        Contains the category levels.\\n\\n    '\n    data = np.asarray(data)\n    n_rows = data.shape[0]\n    if n_cat is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n    else:\n        cat_uni = np.arange(n_cat)\n        data_ = data\n    tt = np.zeros((n_rows, n_cat), int)\n    for (idx, row) in enumerate(data_):\n        ro = np.bincount(row)\n        tt[idx, :len(ro)] = ro\n    return (tt, cat_uni)"
        ]
    },
    {
        "func_name": "to_table",
        "original": "def to_table(data, bins=None):\n    \"\"\"convert raw data with shape (subject, rater) to (rater1, rater2)\n\n    brings data into correct format for cohens_kappa\n\n    Parameters\n    ----------\n    data : array_like, 2-Dim\n        data containing category assignment with subjects in rows and raters\n        in columns.\n    bins : None, int or tuple of array_like\n        If None, then the data is converted to integer categories,\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\n        with non-zero counts are included.\n        If this is an integer, then the category levels in the data are already\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\n        returned array may contain columns with zero count, if no subject\n        has been categorized with this level.\n        If bins are a tuple of two array_like, then the bins are directly used\n        by ``numpy.histogramdd``. This is useful if we want to merge categories.\n\n    Returns\n    -------\n    arr : nd_array, (n_cat, n_cat)\n        Contingency table that contains counts of category level with rater1\n        in rows and rater2 in columns.\n\n    Notes\n    -----\n    no NaN handling, delete rows with missing values\n\n    This works also for more than two raters. In that case the dimension of\n    the resulting contingency table is the same as the number of raters\n    instead of 2-dimensional.\n\n    \"\"\"\n    data = np.asarray(data)\n    (n_rows, n_cols) = data.shape\n    if bins is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n        bins_ = np.arange(n_cat + 1) - 0.5\n    elif np.isscalar(bins):\n        bins_ = np.arange(bins + 1) - 0.5\n        data_ = data\n    else:\n        bins_ = bins\n        data_ = data\n    tt = np.histogramdd(data_, (bins_,) * n_cols)\n    return (tt[0], bins_)",
        "mutated": [
            "def to_table(data, bins=None):\n    if False:\n        i = 10\n    'convert raw data with shape (subject, rater) to (rater1, rater2)\\n\\n    brings data into correct format for cohens_kappa\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    bins : None, int or tuple of array_like\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n        If bins are a tuple of two array_like, then the bins are directly used\\n        by ``numpy.histogramdd``. This is useful if we want to merge categories.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_cat, n_cat)\\n        Contingency table that contains counts of category level with rater1\\n        in rows and rater2 in columns.\\n\\n    Notes\\n    -----\\n    no NaN handling, delete rows with missing values\\n\\n    This works also for more than two raters. In that case the dimension of\\n    the resulting contingency table is the same as the number of raters\\n    instead of 2-dimensional.\\n\\n    '\n    data = np.asarray(data)\n    (n_rows, n_cols) = data.shape\n    if bins is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n        bins_ = np.arange(n_cat + 1) - 0.5\n    elif np.isscalar(bins):\n        bins_ = np.arange(bins + 1) - 0.5\n        data_ = data\n    else:\n        bins_ = bins\n        data_ = data\n    tt = np.histogramdd(data_, (bins_,) * n_cols)\n    return (tt[0], bins_)",
            "def to_table(data, bins=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'convert raw data with shape (subject, rater) to (rater1, rater2)\\n\\n    brings data into correct format for cohens_kappa\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    bins : None, int or tuple of array_like\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n        If bins are a tuple of two array_like, then the bins are directly used\\n        by ``numpy.histogramdd``. This is useful if we want to merge categories.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_cat, n_cat)\\n        Contingency table that contains counts of category level with rater1\\n        in rows and rater2 in columns.\\n\\n    Notes\\n    -----\\n    no NaN handling, delete rows with missing values\\n\\n    This works also for more than two raters. In that case the dimension of\\n    the resulting contingency table is the same as the number of raters\\n    instead of 2-dimensional.\\n\\n    '\n    data = np.asarray(data)\n    (n_rows, n_cols) = data.shape\n    if bins is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n        bins_ = np.arange(n_cat + 1) - 0.5\n    elif np.isscalar(bins):\n        bins_ = np.arange(bins + 1) - 0.5\n        data_ = data\n    else:\n        bins_ = bins\n        data_ = data\n    tt = np.histogramdd(data_, (bins_,) * n_cols)\n    return (tt[0], bins_)",
            "def to_table(data, bins=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'convert raw data with shape (subject, rater) to (rater1, rater2)\\n\\n    brings data into correct format for cohens_kappa\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    bins : None, int or tuple of array_like\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n        If bins are a tuple of two array_like, then the bins are directly used\\n        by ``numpy.histogramdd``. This is useful if we want to merge categories.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_cat, n_cat)\\n        Contingency table that contains counts of category level with rater1\\n        in rows and rater2 in columns.\\n\\n    Notes\\n    -----\\n    no NaN handling, delete rows with missing values\\n\\n    This works also for more than two raters. In that case the dimension of\\n    the resulting contingency table is the same as the number of raters\\n    instead of 2-dimensional.\\n\\n    '\n    data = np.asarray(data)\n    (n_rows, n_cols) = data.shape\n    if bins is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n        bins_ = np.arange(n_cat + 1) - 0.5\n    elif np.isscalar(bins):\n        bins_ = np.arange(bins + 1) - 0.5\n        data_ = data\n    else:\n        bins_ = bins\n        data_ = data\n    tt = np.histogramdd(data_, (bins_,) * n_cols)\n    return (tt[0], bins_)",
            "def to_table(data, bins=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'convert raw data with shape (subject, rater) to (rater1, rater2)\\n\\n    brings data into correct format for cohens_kappa\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    bins : None, int or tuple of array_like\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n        If bins are a tuple of two array_like, then the bins are directly used\\n        by ``numpy.histogramdd``. This is useful if we want to merge categories.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_cat, n_cat)\\n        Contingency table that contains counts of category level with rater1\\n        in rows and rater2 in columns.\\n\\n    Notes\\n    -----\\n    no NaN handling, delete rows with missing values\\n\\n    This works also for more than two raters. In that case the dimension of\\n    the resulting contingency table is the same as the number of raters\\n    instead of 2-dimensional.\\n\\n    '\n    data = np.asarray(data)\n    (n_rows, n_cols) = data.shape\n    if bins is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n        bins_ = np.arange(n_cat + 1) - 0.5\n    elif np.isscalar(bins):\n        bins_ = np.arange(bins + 1) - 0.5\n        data_ = data\n    else:\n        bins_ = bins\n        data_ = data\n    tt = np.histogramdd(data_, (bins_,) * n_cols)\n    return (tt[0], bins_)",
            "def to_table(data, bins=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'convert raw data with shape (subject, rater) to (rater1, rater2)\\n\\n    brings data into correct format for cohens_kappa\\n\\n    Parameters\\n    ----------\\n    data : array_like, 2-Dim\\n        data containing category assignment with subjects in rows and raters\\n        in columns.\\n    bins : None, int or tuple of array_like\\n        If None, then the data is converted to integer categories,\\n        0,1,2,...,n_cat-1. Because of the relabeling only category levels\\n        with non-zero counts are included.\\n        If this is an integer, then the category levels in the data are already\\n        assumed to be in integers, 0,1,2,...,n_cat-1. In this case, the\\n        returned array may contain columns with zero count, if no subject\\n        has been categorized with this level.\\n        If bins are a tuple of two array_like, then the bins are directly used\\n        by ``numpy.histogramdd``. This is useful if we want to merge categories.\\n\\n    Returns\\n    -------\\n    arr : nd_array, (n_cat, n_cat)\\n        Contingency table that contains counts of category level with rater1\\n        in rows and rater2 in columns.\\n\\n    Notes\\n    -----\\n    no NaN handling, delete rows with missing values\\n\\n    This works also for more than two raters. In that case the dimension of\\n    the resulting contingency table is the same as the number of raters\\n    instead of 2-dimensional.\\n\\n    '\n    data = np.asarray(data)\n    (n_rows, n_cols) = data.shape\n    if bins is None:\n        (cat_uni, cat_int) = np.unique(data.ravel(), return_inverse=True)\n        n_cat = len(cat_uni)\n        data_ = cat_int.reshape(data.shape)\n        bins_ = np.arange(n_cat + 1) - 0.5\n    elif np.isscalar(bins):\n        bins_ = np.arange(bins + 1) - 0.5\n        data_ = data\n    else:\n        bins_ = bins\n        data_ = data\n    tt = np.histogramdd(data_, (bins_,) * n_cols)\n    return (tt[0], bins_)"
        ]
    },
    {
        "func_name": "fleiss_kappa",
        "original": "def fleiss_kappa(table, method='fleiss'):\n    \"\"\"Fleiss' and Randolph's kappa multi-rater agreement measure\n\n    Parameters\n    ----------\n    table : array_like, 2-D\n        assumes subjects in rows, and categories in columns. Convert raw data\n        into this format by using\n        :func:`statsmodels.stats.inter_rater.aggregate_raters`\n    method : str\n        Method 'fleiss' returns Fleiss' kappa which uses the sample margin\n        to define the chance outcome.\n        Method 'randolph' or 'uniform' (only first 4 letters are needed)\n        returns Randolph's (2005) multirater kappa which assumes a uniform\n        distribution of the categories to define the chance outcome.\n\n    Returns\n    -------\n    kappa : float\n        Fleiss's or Randolph's kappa statistic for inter rater agreement\n\n    Notes\n    -----\n    no variance or hypothesis tests yet\n\n    Interrater agreement measures like Fleiss's kappa measure agreement relative\n    to chance agreement. Different authors have proposed ways of defining\n    these chance agreements. Fleiss' is based on the marginal sample distribution\n    of categories, while Randolph uses a uniform distribution of categories as\n    benchmark. Warrens (2010) showed that Randolph's kappa is always larger or\n    equal to Fleiss' kappa. Under some commonly observed condition, Fleiss' and\n    Randolph's kappa provide lower and upper bounds for two similar kappa_like\n    measures by Light (1971) and Hubert (1977).\n\n    References\n    ----------\n    Wikipedia https://en.wikipedia.org/wiki/Fleiss%27_kappa\n\n    Fleiss, Joseph L. 1971. \"Measuring Nominal Scale Agreement among Many\n    Raters.\" Psychological Bulletin 76 (5): 378-82.\n    https://doi.org/10.1037/h0031619.\n\n    Randolph, Justus J. 2005 \"Free-Marginal Multirater Kappa (multirater\n    K [free]): An Alternative to Fleiss' Fixed-Marginal Multirater Kappa.\"\n    Presented at the Joensuu Learning and Instruction Symposium, vol. 2005\n    https://eric.ed.gov/?id=ED490661\n\n    Warrens, Matthijs J. 2010. \"Inequalities between Multi-Rater Kappas.\"\n    Advances in Data Analysis and Classification 4 (4): 271-86.\n    https://doi.org/10.1007/s11634-010-0073-4.\n    \"\"\"\n    table = 1.0 * np.asarray(table)\n    (n_sub, n_cat) = table.shape\n    n_total = table.sum()\n    n_rater = table.sum(1)\n    n_rat = n_rater.max()\n    assert n_total == n_sub * n_rat\n    p_cat = table.sum(0) / n_total\n    table2 = table * table\n    p_rat = (table2.sum(1) - n_rat) / (n_rat * (n_rat - 1.0))\n    p_mean = p_rat.mean()\n    if method == 'fleiss':\n        p_mean_exp = (p_cat * p_cat).sum()\n    elif method.startswith('rand') or method.startswith('unif'):\n        p_mean_exp = 1 / n_cat\n    kappa = (p_mean - p_mean_exp) / (1 - p_mean_exp)\n    return kappa",
        "mutated": [
            "def fleiss_kappa(table, method='fleiss'):\n    if False:\n        i = 10\n    'Fleiss\\' and Randolph\\'s kappa multi-rater agreement measure\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-D\\n        assumes subjects in rows, and categories in columns. Convert raw data\\n        into this format by using\\n        :func:`statsmodels.stats.inter_rater.aggregate_raters`\\n    method : str\\n        Method \\'fleiss\\' returns Fleiss\\' kappa which uses the sample margin\\n        to define the chance outcome.\\n        Method \\'randolph\\' or \\'uniform\\' (only first 4 letters are needed)\\n        returns Randolph\\'s (2005) multirater kappa which assumes a uniform\\n        distribution of the categories to define the chance outcome.\\n\\n    Returns\\n    -------\\n    kappa : float\\n        Fleiss\\'s or Randolph\\'s kappa statistic for inter rater agreement\\n\\n    Notes\\n    -----\\n    no variance or hypothesis tests yet\\n\\n    Interrater agreement measures like Fleiss\\'s kappa measure agreement relative\\n    to chance agreement. Different authors have proposed ways of defining\\n    these chance agreements. Fleiss\\' is based on the marginal sample distribution\\n    of categories, while Randolph uses a uniform distribution of categories as\\n    benchmark. Warrens (2010) showed that Randolph\\'s kappa is always larger or\\n    equal to Fleiss\\' kappa. Under some commonly observed condition, Fleiss\\' and\\n    Randolph\\'s kappa provide lower and upper bounds for two similar kappa_like\\n    measures by Light (1971) and Hubert (1977).\\n\\n    References\\n    ----------\\n    Wikipedia https://en.wikipedia.org/wiki/Fleiss%27_kappa\\n\\n    Fleiss, Joseph L. 1971. \"Measuring Nominal Scale Agreement among Many\\n    Raters.\" Psychological Bulletin 76 (5): 378-82.\\n    https://doi.org/10.1037/h0031619.\\n\\n    Randolph, Justus J. 2005 \"Free-Marginal Multirater Kappa (multirater\\n    K [free]): An Alternative to Fleiss\\' Fixed-Marginal Multirater Kappa.\"\\n    Presented at the Joensuu Learning and Instruction Symposium, vol. 2005\\n    https://eric.ed.gov/?id=ED490661\\n\\n    Warrens, Matthijs J. 2010. \"Inequalities between Multi-Rater Kappas.\"\\n    Advances in Data Analysis and Classification 4 (4): 271-86.\\n    https://doi.org/10.1007/s11634-010-0073-4.\\n    '\n    table = 1.0 * np.asarray(table)\n    (n_sub, n_cat) = table.shape\n    n_total = table.sum()\n    n_rater = table.sum(1)\n    n_rat = n_rater.max()\n    assert n_total == n_sub * n_rat\n    p_cat = table.sum(0) / n_total\n    table2 = table * table\n    p_rat = (table2.sum(1) - n_rat) / (n_rat * (n_rat - 1.0))\n    p_mean = p_rat.mean()\n    if method == 'fleiss':\n        p_mean_exp = (p_cat * p_cat).sum()\n    elif method.startswith('rand') or method.startswith('unif'):\n        p_mean_exp = 1 / n_cat\n    kappa = (p_mean - p_mean_exp) / (1 - p_mean_exp)\n    return kappa",
            "def fleiss_kappa(table, method='fleiss'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Fleiss\\' and Randolph\\'s kappa multi-rater agreement measure\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-D\\n        assumes subjects in rows, and categories in columns. Convert raw data\\n        into this format by using\\n        :func:`statsmodels.stats.inter_rater.aggregate_raters`\\n    method : str\\n        Method \\'fleiss\\' returns Fleiss\\' kappa which uses the sample margin\\n        to define the chance outcome.\\n        Method \\'randolph\\' or \\'uniform\\' (only first 4 letters are needed)\\n        returns Randolph\\'s (2005) multirater kappa which assumes a uniform\\n        distribution of the categories to define the chance outcome.\\n\\n    Returns\\n    -------\\n    kappa : float\\n        Fleiss\\'s or Randolph\\'s kappa statistic for inter rater agreement\\n\\n    Notes\\n    -----\\n    no variance or hypothesis tests yet\\n\\n    Interrater agreement measures like Fleiss\\'s kappa measure agreement relative\\n    to chance agreement. Different authors have proposed ways of defining\\n    these chance agreements. Fleiss\\' is based on the marginal sample distribution\\n    of categories, while Randolph uses a uniform distribution of categories as\\n    benchmark. Warrens (2010) showed that Randolph\\'s kappa is always larger or\\n    equal to Fleiss\\' kappa. Under some commonly observed condition, Fleiss\\' and\\n    Randolph\\'s kappa provide lower and upper bounds for two similar kappa_like\\n    measures by Light (1971) and Hubert (1977).\\n\\n    References\\n    ----------\\n    Wikipedia https://en.wikipedia.org/wiki/Fleiss%27_kappa\\n\\n    Fleiss, Joseph L. 1971. \"Measuring Nominal Scale Agreement among Many\\n    Raters.\" Psychological Bulletin 76 (5): 378-82.\\n    https://doi.org/10.1037/h0031619.\\n\\n    Randolph, Justus J. 2005 \"Free-Marginal Multirater Kappa (multirater\\n    K [free]): An Alternative to Fleiss\\' Fixed-Marginal Multirater Kappa.\"\\n    Presented at the Joensuu Learning and Instruction Symposium, vol. 2005\\n    https://eric.ed.gov/?id=ED490661\\n\\n    Warrens, Matthijs J. 2010. \"Inequalities between Multi-Rater Kappas.\"\\n    Advances in Data Analysis and Classification 4 (4): 271-86.\\n    https://doi.org/10.1007/s11634-010-0073-4.\\n    '\n    table = 1.0 * np.asarray(table)\n    (n_sub, n_cat) = table.shape\n    n_total = table.sum()\n    n_rater = table.sum(1)\n    n_rat = n_rater.max()\n    assert n_total == n_sub * n_rat\n    p_cat = table.sum(0) / n_total\n    table2 = table * table\n    p_rat = (table2.sum(1) - n_rat) / (n_rat * (n_rat - 1.0))\n    p_mean = p_rat.mean()\n    if method == 'fleiss':\n        p_mean_exp = (p_cat * p_cat).sum()\n    elif method.startswith('rand') or method.startswith('unif'):\n        p_mean_exp = 1 / n_cat\n    kappa = (p_mean - p_mean_exp) / (1 - p_mean_exp)\n    return kappa",
            "def fleiss_kappa(table, method='fleiss'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Fleiss\\' and Randolph\\'s kappa multi-rater agreement measure\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-D\\n        assumes subjects in rows, and categories in columns. Convert raw data\\n        into this format by using\\n        :func:`statsmodels.stats.inter_rater.aggregate_raters`\\n    method : str\\n        Method \\'fleiss\\' returns Fleiss\\' kappa which uses the sample margin\\n        to define the chance outcome.\\n        Method \\'randolph\\' or \\'uniform\\' (only first 4 letters are needed)\\n        returns Randolph\\'s (2005) multirater kappa which assumes a uniform\\n        distribution of the categories to define the chance outcome.\\n\\n    Returns\\n    -------\\n    kappa : float\\n        Fleiss\\'s or Randolph\\'s kappa statistic for inter rater agreement\\n\\n    Notes\\n    -----\\n    no variance or hypothesis tests yet\\n\\n    Interrater agreement measures like Fleiss\\'s kappa measure agreement relative\\n    to chance agreement. Different authors have proposed ways of defining\\n    these chance agreements. Fleiss\\' is based on the marginal sample distribution\\n    of categories, while Randolph uses a uniform distribution of categories as\\n    benchmark. Warrens (2010) showed that Randolph\\'s kappa is always larger or\\n    equal to Fleiss\\' kappa. Under some commonly observed condition, Fleiss\\' and\\n    Randolph\\'s kappa provide lower and upper bounds for two similar kappa_like\\n    measures by Light (1971) and Hubert (1977).\\n\\n    References\\n    ----------\\n    Wikipedia https://en.wikipedia.org/wiki/Fleiss%27_kappa\\n\\n    Fleiss, Joseph L. 1971. \"Measuring Nominal Scale Agreement among Many\\n    Raters.\" Psychological Bulletin 76 (5): 378-82.\\n    https://doi.org/10.1037/h0031619.\\n\\n    Randolph, Justus J. 2005 \"Free-Marginal Multirater Kappa (multirater\\n    K [free]): An Alternative to Fleiss\\' Fixed-Marginal Multirater Kappa.\"\\n    Presented at the Joensuu Learning and Instruction Symposium, vol. 2005\\n    https://eric.ed.gov/?id=ED490661\\n\\n    Warrens, Matthijs J. 2010. \"Inequalities between Multi-Rater Kappas.\"\\n    Advances in Data Analysis and Classification 4 (4): 271-86.\\n    https://doi.org/10.1007/s11634-010-0073-4.\\n    '\n    table = 1.0 * np.asarray(table)\n    (n_sub, n_cat) = table.shape\n    n_total = table.sum()\n    n_rater = table.sum(1)\n    n_rat = n_rater.max()\n    assert n_total == n_sub * n_rat\n    p_cat = table.sum(0) / n_total\n    table2 = table * table\n    p_rat = (table2.sum(1) - n_rat) / (n_rat * (n_rat - 1.0))\n    p_mean = p_rat.mean()\n    if method == 'fleiss':\n        p_mean_exp = (p_cat * p_cat).sum()\n    elif method.startswith('rand') or method.startswith('unif'):\n        p_mean_exp = 1 / n_cat\n    kappa = (p_mean - p_mean_exp) / (1 - p_mean_exp)\n    return kappa",
            "def fleiss_kappa(table, method='fleiss'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Fleiss\\' and Randolph\\'s kappa multi-rater agreement measure\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-D\\n        assumes subjects in rows, and categories in columns. Convert raw data\\n        into this format by using\\n        :func:`statsmodels.stats.inter_rater.aggregate_raters`\\n    method : str\\n        Method \\'fleiss\\' returns Fleiss\\' kappa which uses the sample margin\\n        to define the chance outcome.\\n        Method \\'randolph\\' or \\'uniform\\' (only first 4 letters are needed)\\n        returns Randolph\\'s (2005) multirater kappa which assumes a uniform\\n        distribution of the categories to define the chance outcome.\\n\\n    Returns\\n    -------\\n    kappa : float\\n        Fleiss\\'s or Randolph\\'s kappa statistic for inter rater agreement\\n\\n    Notes\\n    -----\\n    no variance or hypothesis tests yet\\n\\n    Interrater agreement measures like Fleiss\\'s kappa measure agreement relative\\n    to chance agreement. Different authors have proposed ways of defining\\n    these chance agreements. Fleiss\\' is based on the marginal sample distribution\\n    of categories, while Randolph uses a uniform distribution of categories as\\n    benchmark. Warrens (2010) showed that Randolph\\'s kappa is always larger or\\n    equal to Fleiss\\' kappa. Under some commonly observed condition, Fleiss\\' and\\n    Randolph\\'s kappa provide lower and upper bounds for two similar kappa_like\\n    measures by Light (1971) and Hubert (1977).\\n\\n    References\\n    ----------\\n    Wikipedia https://en.wikipedia.org/wiki/Fleiss%27_kappa\\n\\n    Fleiss, Joseph L. 1971. \"Measuring Nominal Scale Agreement among Many\\n    Raters.\" Psychological Bulletin 76 (5): 378-82.\\n    https://doi.org/10.1037/h0031619.\\n\\n    Randolph, Justus J. 2005 \"Free-Marginal Multirater Kappa (multirater\\n    K [free]): An Alternative to Fleiss\\' Fixed-Marginal Multirater Kappa.\"\\n    Presented at the Joensuu Learning and Instruction Symposium, vol. 2005\\n    https://eric.ed.gov/?id=ED490661\\n\\n    Warrens, Matthijs J. 2010. \"Inequalities between Multi-Rater Kappas.\"\\n    Advances in Data Analysis and Classification 4 (4): 271-86.\\n    https://doi.org/10.1007/s11634-010-0073-4.\\n    '\n    table = 1.0 * np.asarray(table)\n    (n_sub, n_cat) = table.shape\n    n_total = table.sum()\n    n_rater = table.sum(1)\n    n_rat = n_rater.max()\n    assert n_total == n_sub * n_rat\n    p_cat = table.sum(0) / n_total\n    table2 = table * table\n    p_rat = (table2.sum(1) - n_rat) / (n_rat * (n_rat - 1.0))\n    p_mean = p_rat.mean()\n    if method == 'fleiss':\n        p_mean_exp = (p_cat * p_cat).sum()\n    elif method.startswith('rand') or method.startswith('unif'):\n        p_mean_exp = 1 / n_cat\n    kappa = (p_mean - p_mean_exp) / (1 - p_mean_exp)\n    return kappa",
            "def fleiss_kappa(table, method='fleiss'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Fleiss\\' and Randolph\\'s kappa multi-rater agreement measure\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-D\\n        assumes subjects in rows, and categories in columns. Convert raw data\\n        into this format by using\\n        :func:`statsmodels.stats.inter_rater.aggregate_raters`\\n    method : str\\n        Method \\'fleiss\\' returns Fleiss\\' kappa which uses the sample margin\\n        to define the chance outcome.\\n        Method \\'randolph\\' or \\'uniform\\' (only first 4 letters are needed)\\n        returns Randolph\\'s (2005) multirater kappa which assumes a uniform\\n        distribution of the categories to define the chance outcome.\\n\\n    Returns\\n    -------\\n    kappa : float\\n        Fleiss\\'s or Randolph\\'s kappa statistic for inter rater agreement\\n\\n    Notes\\n    -----\\n    no variance or hypothesis tests yet\\n\\n    Interrater agreement measures like Fleiss\\'s kappa measure agreement relative\\n    to chance agreement. Different authors have proposed ways of defining\\n    these chance agreements. Fleiss\\' is based on the marginal sample distribution\\n    of categories, while Randolph uses a uniform distribution of categories as\\n    benchmark. Warrens (2010) showed that Randolph\\'s kappa is always larger or\\n    equal to Fleiss\\' kappa. Under some commonly observed condition, Fleiss\\' and\\n    Randolph\\'s kappa provide lower and upper bounds for two similar kappa_like\\n    measures by Light (1971) and Hubert (1977).\\n\\n    References\\n    ----------\\n    Wikipedia https://en.wikipedia.org/wiki/Fleiss%27_kappa\\n\\n    Fleiss, Joseph L. 1971. \"Measuring Nominal Scale Agreement among Many\\n    Raters.\" Psychological Bulletin 76 (5): 378-82.\\n    https://doi.org/10.1037/h0031619.\\n\\n    Randolph, Justus J. 2005 \"Free-Marginal Multirater Kappa (multirater\\n    K [free]): An Alternative to Fleiss\\' Fixed-Marginal Multirater Kappa.\"\\n    Presented at the Joensuu Learning and Instruction Symposium, vol. 2005\\n    https://eric.ed.gov/?id=ED490661\\n\\n    Warrens, Matthijs J. 2010. \"Inequalities between Multi-Rater Kappas.\"\\n    Advances in Data Analysis and Classification 4 (4): 271-86.\\n    https://doi.org/10.1007/s11634-010-0073-4.\\n    '\n    table = 1.0 * np.asarray(table)\n    (n_sub, n_cat) = table.shape\n    n_total = table.sum()\n    n_rater = table.sum(1)\n    n_rat = n_rater.max()\n    assert n_total == n_sub * n_rat\n    p_cat = table.sum(0) / n_total\n    table2 = table * table\n    p_rat = (table2.sum(1) - n_rat) / (n_rat * (n_rat - 1.0))\n    p_mean = p_rat.mean()\n    if method == 'fleiss':\n        p_mean_exp = (p_cat * p_cat).sum()\n    elif method.startswith('rand') or method.startswith('unif'):\n        p_mean_exp = 1 / n_cat\n    kappa = (p_mean - p_mean_exp) / (1 - p_mean_exp)\n    return kappa"
        ]
    },
    {
        "func_name": "cohens_kappa",
        "original": "def cohens_kappa(table, weights=None, return_results=True, wt=None):\n    \"\"\"Compute Cohen's kappa with variance and equal-zero test\n\n    Parameters\n    ----------\n    table : array_like, 2-Dim\n        square array with results of two raters, one rater in rows, second\n        rater in columns\n    weights : array_like\n        The interpretation of weights depends on the wt argument.\n        If both are None, then the simple kappa is computed.\n        see wt for the case when wt is not None\n        If weights is two dimensional, then it is directly used as a weight\n        matrix. For computing the variance of kappa, the maximum of the\n        weights is assumed to be smaller or equal to one.\n        TODO: fix conflicting definitions in the 2-Dim case for\n    wt : {None, str}\n        If wt and weights are None, then the simple kappa is computed.\n        If wt is given, but weights is None, then the weights are set to\n        be [0, 1, 2, ..., k].\n        If weights is a one-dimensional array, then it is used to construct\n        the weight matrix given the following options.\n\n        wt in ['linear', 'ca' or None] : use linear weights, Cicchetti-Allison\n            actual weights are linear in the score \"weights\" difference\n        wt in ['quadratic', 'fc'] : use linear weights, Fleiss-Cohen\n            actual weights are squared in the score \"weights\" difference\n        wt = 'toeplitz' : weight matrix is constructed as a toeplitz matrix\n            from the one dimensional weights.\n\n    return_results : bool\n        If True (default), then an instance of KappaResults is returned.\n        If False, then only kappa is computed and returned.\n\n    Returns\n    -------\n    results or kappa\n        If return_results is True (default), then a results instance with all\n        statistics is returned\n        If return_results is False, then only kappa is calculated and returned.\n\n    Notes\n    -----\n    There are two conflicting definitions of the weight matrix, Wikipedia\n    versus SAS manual. However, the computation are invariant to rescaling\n    of the weights matrix, so there is no difference in the results.\n\n    Weights for 'linear' and 'quadratic' are interpreted as scores for the\n    categories, the weights in the computation are based on the pairwise\n    difference between the scores.\n    Weights for 'toeplitz' are a interpreted as weighted distance. The distance\n    only depends on how many levels apart two entries in the table are but\n    not on the levels themselves.\n\n    example:\n\n    weights = '0, 1, 2, 3' and wt is either linear or toeplitz means that the\n    weighting only depends on the simple distance of levels.\n\n    weights = '0, 0, 1, 1' and wt = 'linear' means that the first two levels\n    are zero distance apart and the same for the last two levels. This is\n    the sample as forming two aggregated levels by merging the first two and\n    the last two levels, respectively.\n\n    weights = [0, 1, 2, 3] and wt = 'quadratic' is the same as squaring these\n    weights and using wt = 'toeplitz'.\n\n    References\n    ----------\n    Wikipedia\n    SAS Manual\n\n    \"\"\"\n    table = np.asarray(table, float)\n    agree = np.diag(table).sum()\n    nobs = table.sum()\n    probs = table / nobs\n    freqs = probs\n    probs_diag = np.diag(probs)\n    freq_row = table.sum(1) / nobs\n    freq_col = table.sum(0) / nobs\n    prob_exp = freq_col * freq_row[:, None]\n    assert np.allclose(prob_exp.sum(), 1)\n    agree_exp = np.diag(prob_exp).sum()\n    if weights is None and wt is None:\n        kind = 'Simple'\n        kappa = (agree / nobs - agree_exp) / (1 - agree_exp)\n        if return_results:\n            term_a = probs_diag * (1 - (freq_row + freq_col) * (1 - kappa)) ** 2\n            term_a = term_a.sum()\n            term_b = probs * (freq_col[:, None] + freq_row) ** 2\n            d_idx = np.arange(table.shape[0])\n            term_b[d_idx, d_idx] = 0\n            term_b = (1 - kappa) ** 2 * term_b.sum()\n            term_c = (kappa - agree_exp * (1 - kappa)) ** 2\n            var_kappa = (term_a + term_b - term_c) / (1 - agree_exp) ** 2 / nobs\n            term_c = freq_col * freq_row * (freq_col + freq_row)\n            var_kappa0 = agree_exp + agree_exp ** 2 - term_c.sum()\n            var_kappa0 /= (1 - agree_exp) ** 2 * nobs\n    else:\n        if weights is None:\n            weights = np.arange(table.shape[0])\n        kind = 'Weighted'\n        weights = np.asarray(weights, float)\n        if weights.ndim == 1:\n            if wt in ['ca', 'linear', None]:\n                weights = np.abs(weights[:, None] - weights) / (weights[-1] - weights[0])\n            elif wt in ['fc', 'quadratic']:\n                weights = (weights[:, None] - weights) ** 2 / (weights[-1] - weights[0]) ** 2\n            elif wt == 'toeplitz':\n                from scipy.linalg import toeplitz\n                weights = toeplitz(weights)\n            else:\n                raise ValueError('wt option is not known')\n        else:\n            (rows, cols) = table.shape\n            if table.shape != weights.shape:\n                raise ValueError('weights are not square')\n        kappa = 1 - (weights * table).sum() / nobs / (weights * prob_exp).sum()\n        if return_results:\n            var_kappa = np.nan\n            var_kappa0 = np.nan\n            w = 1.0 - weights\n            w_row = (freq_col * w).sum(1)\n            w_col = (freq_row[:, None] * w).sum(0)\n            agree_wexp = (w * freq_col * freq_row[:, None]).sum()\n            term_a = freqs * (w - (w_col + w_row[:, None]) * (1 - kappa)) ** 2\n            fac = 1.0 / ((1 - agree_wexp) ** 2 * nobs)\n            var_kappa = term_a.sum() - (kappa - agree_wexp * (1 - kappa)) ** 2\n            var_kappa *= fac\n            freqse = freq_col * freq_row[:, None]\n            var_kappa0 = (freqse * (w - (w_col + w_row[:, None])) ** 2).sum()\n            var_kappa0 -= agree_wexp ** 2\n            var_kappa0 *= fac\n    kappa_max = (np.minimum(freq_row, freq_col).sum() - agree_exp) / (1 - agree_exp)\n    if return_results:\n        res = KappaResults(kind=kind, kappa=kappa, kappa_max=kappa_max, weights=weights, var_kappa=var_kappa, var_kappa0=var_kappa0)\n        return res\n    else:\n        return kappa",
        "mutated": [
            "def cohens_kappa(table, weights=None, return_results=True, wt=None):\n    if False:\n        i = 10\n    'Compute Cohen\\'s kappa with variance and equal-zero test\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-Dim\\n        square array with results of two raters, one rater in rows, second\\n        rater in columns\\n    weights : array_like\\n        The interpretation of weights depends on the wt argument.\\n        If both are None, then the simple kappa is computed.\\n        see wt for the case when wt is not None\\n        If weights is two dimensional, then it is directly used as a weight\\n        matrix. For computing the variance of kappa, the maximum of the\\n        weights is assumed to be smaller or equal to one.\\n        TODO: fix conflicting definitions in the 2-Dim case for\\n    wt : {None, str}\\n        If wt and weights are None, then the simple kappa is computed.\\n        If wt is given, but weights is None, then the weights are set to\\n        be [0, 1, 2, ..., k].\\n        If weights is a one-dimensional array, then it is used to construct\\n        the weight matrix given the following options.\\n\\n        wt in [\\'linear\\', \\'ca\\' or None] : use linear weights, Cicchetti-Allison\\n            actual weights are linear in the score \"weights\" difference\\n        wt in [\\'quadratic\\', \\'fc\\'] : use linear weights, Fleiss-Cohen\\n            actual weights are squared in the score \"weights\" difference\\n        wt = \\'toeplitz\\' : weight matrix is constructed as a toeplitz matrix\\n            from the one dimensional weights.\\n\\n    return_results : bool\\n        If True (default), then an instance of KappaResults is returned.\\n        If False, then only kappa is computed and returned.\\n\\n    Returns\\n    -------\\n    results or kappa\\n        If return_results is True (default), then a results instance with all\\n        statistics is returned\\n        If return_results is False, then only kappa is calculated and returned.\\n\\n    Notes\\n    -----\\n    There are two conflicting definitions of the weight matrix, Wikipedia\\n    versus SAS manual. However, the computation are invariant to rescaling\\n    of the weights matrix, so there is no difference in the results.\\n\\n    Weights for \\'linear\\' and \\'quadratic\\' are interpreted as scores for the\\n    categories, the weights in the computation are based on the pairwise\\n    difference between the scores.\\n    Weights for \\'toeplitz\\' are a interpreted as weighted distance. The distance\\n    only depends on how many levels apart two entries in the table are but\\n    not on the levels themselves.\\n\\n    example:\\n\\n    weights = \\'0, 1, 2, 3\\' and wt is either linear or toeplitz means that the\\n    weighting only depends on the simple distance of levels.\\n\\n    weights = \\'0, 0, 1, 1\\' and wt = \\'linear\\' means that the first two levels\\n    are zero distance apart and the same for the last two levels. This is\\n    the sample as forming two aggregated levels by merging the first two and\\n    the last two levels, respectively.\\n\\n    weights = [0, 1, 2, 3] and wt = \\'quadratic\\' is the same as squaring these\\n    weights and using wt = \\'toeplitz\\'.\\n\\n    References\\n    ----------\\n    Wikipedia\\n    SAS Manual\\n\\n    '\n    table = np.asarray(table, float)\n    agree = np.diag(table).sum()\n    nobs = table.sum()\n    probs = table / nobs\n    freqs = probs\n    probs_diag = np.diag(probs)\n    freq_row = table.sum(1) / nobs\n    freq_col = table.sum(0) / nobs\n    prob_exp = freq_col * freq_row[:, None]\n    assert np.allclose(prob_exp.sum(), 1)\n    agree_exp = np.diag(prob_exp).sum()\n    if weights is None and wt is None:\n        kind = 'Simple'\n        kappa = (agree / nobs - agree_exp) / (1 - agree_exp)\n        if return_results:\n            term_a = probs_diag * (1 - (freq_row + freq_col) * (1 - kappa)) ** 2\n            term_a = term_a.sum()\n            term_b = probs * (freq_col[:, None] + freq_row) ** 2\n            d_idx = np.arange(table.shape[0])\n            term_b[d_idx, d_idx] = 0\n            term_b = (1 - kappa) ** 2 * term_b.sum()\n            term_c = (kappa - agree_exp * (1 - kappa)) ** 2\n            var_kappa = (term_a + term_b - term_c) / (1 - agree_exp) ** 2 / nobs\n            term_c = freq_col * freq_row * (freq_col + freq_row)\n            var_kappa0 = agree_exp + agree_exp ** 2 - term_c.sum()\n            var_kappa0 /= (1 - agree_exp) ** 2 * nobs\n    else:\n        if weights is None:\n            weights = np.arange(table.shape[0])\n        kind = 'Weighted'\n        weights = np.asarray(weights, float)\n        if weights.ndim == 1:\n            if wt in ['ca', 'linear', None]:\n                weights = np.abs(weights[:, None] - weights) / (weights[-1] - weights[0])\n            elif wt in ['fc', 'quadratic']:\n                weights = (weights[:, None] - weights) ** 2 / (weights[-1] - weights[0]) ** 2\n            elif wt == 'toeplitz':\n                from scipy.linalg import toeplitz\n                weights = toeplitz(weights)\n            else:\n                raise ValueError('wt option is not known')\n        else:\n            (rows, cols) = table.shape\n            if table.shape != weights.shape:\n                raise ValueError('weights are not square')\n        kappa = 1 - (weights * table).sum() / nobs / (weights * prob_exp).sum()\n        if return_results:\n            var_kappa = np.nan\n            var_kappa0 = np.nan\n            w = 1.0 - weights\n            w_row = (freq_col * w).sum(1)\n            w_col = (freq_row[:, None] * w).sum(0)\n            agree_wexp = (w * freq_col * freq_row[:, None]).sum()\n            term_a = freqs * (w - (w_col + w_row[:, None]) * (1 - kappa)) ** 2\n            fac = 1.0 / ((1 - agree_wexp) ** 2 * nobs)\n            var_kappa = term_a.sum() - (kappa - agree_wexp * (1 - kappa)) ** 2\n            var_kappa *= fac\n            freqse = freq_col * freq_row[:, None]\n            var_kappa0 = (freqse * (w - (w_col + w_row[:, None])) ** 2).sum()\n            var_kappa0 -= agree_wexp ** 2\n            var_kappa0 *= fac\n    kappa_max = (np.minimum(freq_row, freq_col).sum() - agree_exp) / (1 - agree_exp)\n    if return_results:\n        res = KappaResults(kind=kind, kappa=kappa, kappa_max=kappa_max, weights=weights, var_kappa=var_kappa, var_kappa0=var_kappa0)\n        return res\n    else:\n        return kappa",
            "def cohens_kappa(table, weights=None, return_results=True, wt=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute Cohen\\'s kappa with variance and equal-zero test\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-Dim\\n        square array with results of two raters, one rater in rows, second\\n        rater in columns\\n    weights : array_like\\n        The interpretation of weights depends on the wt argument.\\n        If both are None, then the simple kappa is computed.\\n        see wt for the case when wt is not None\\n        If weights is two dimensional, then it is directly used as a weight\\n        matrix. For computing the variance of kappa, the maximum of the\\n        weights is assumed to be smaller or equal to one.\\n        TODO: fix conflicting definitions in the 2-Dim case for\\n    wt : {None, str}\\n        If wt and weights are None, then the simple kappa is computed.\\n        If wt is given, but weights is None, then the weights are set to\\n        be [0, 1, 2, ..., k].\\n        If weights is a one-dimensional array, then it is used to construct\\n        the weight matrix given the following options.\\n\\n        wt in [\\'linear\\', \\'ca\\' or None] : use linear weights, Cicchetti-Allison\\n            actual weights are linear in the score \"weights\" difference\\n        wt in [\\'quadratic\\', \\'fc\\'] : use linear weights, Fleiss-Cohen\\n            actual weights are squared in the score \"weights\" difference\\n        wt = \\'toeplitz\\' : weight matrix is constructed as a toeplitz matrix\\n            from the one dimensional weights.\\n\\n    return_results : bool\\n        If True (default), then an instance of KappaResults is returned.\\n        If False, then only kappa is computed and returned.\\n\\n    Returns\\n    -------\\n    results or kappa\\n        If return_results is True (default), then a results instance with all\\n        statistics is returned\\n        If return_results is False, then only kappa is calculated and returned.\\n\\n    Notes\\n    -----\\n    There are two conflicting definitions of the weight matrix, Wikipedia\\n    versus SAS manual. However, the computation are invariant to rescaling\\n    of the weights matrix, so there is no difference in the results.\\n\\n    Weights for \\'linear\\' and \\'quadratic\\' are interpreted as scores for the\\n    categories, the weights in the computation are based on the pairwise\\n    difference between the scores.\\n    Weights for \\'toeplitz\\' are a interpreted as weighted distance. The distance\\n    only depends on how many levels apart two entries in the table are but\\n    not on the levels themselves.\\n\\n    example:\\n\\n    weights = \\'0, 1, 2, 3\\' and wt is either linear or toeplitz means that the\\n    weighting only depends on the simple distance of levels.\\n\\n    weights = \\'0, 0, 1, 1\\' and wt = \\'linear\\' means that the first two levels\\n    are zero distance apart and the same for the last two levels. This is\\n    the sample as forming two aggregated levels by merging the first two and\\n    the last two levels, respectively.\\n\\n    weights = [0, 1, 2, 3] and wt = \\'quadratic\\' is the same as squaring these\\n    weights and using wt = \\'toeplitz\\'.\\n\\n    References\\n    ----------\\n    Wikipedia\\n    SAS Manual\\n\\n    '\n    table = np.asarray(table, float)\n    agree = np.diag(table).sum()\n    nobs = table.sum()\n    probs = table / nobs\n    freqs = probs\n    probs_diag = np.diag(probs)\n    freq_row = table.sum(1) / nobs\n    freq_col = table.sum(0) / nobs\n    prob_exp = freq_col * freq_row[:, None]\n    assert np.allclose(prob_exp.sum(), 1)\n    agree_exp = np.diag(prob_exp).sum()\n    if weights is None and wt is None:\n        kind = 'Simple'\n        kappa = (agree / nobs - agree_exp) / (1 - agree_exp)\n        if return_results:\n            term_a = probs_diag * (1 - (freq_row + freq_col) * (1 - kappa)) ** 2\n            term_a = term_a.sum()\n            term_b = probs * (freq_col[:, None] + freq_row) ** 2\n            d_idx = np.arange(table.shape[0])\n            term_b[d_idx, d_idx] = 0\n            term_b = (1 - kappa) ** 2 * term_b.sum()\n            term_c = (kappa - agree_exp * (1 - kappa)) ** 2\n            var_kappa = (term_a + term_b - term_c) / (1 - agree_exp) ** 2 / nobs\n            term_c = freq_col * freq_row * (freq_col + freq_row)\n            var_kappa0 = agree_exp + agree_exp ** 2 - term_c.sum()\n            var_kappa0 /= (1 - agree_exp) ** 2 * nobs\n    else:\n        if weights is None:\n            weights = np.arange(table.shape[0])\n        kind = 'Weighted'\n        weights = np.asarray(weights, float)\n        if weights.ndim == 1:\n            if wt in ['ca', 'linear', None]:\n                weights = np.abs(weights[:, None] - weights) / (weights[-1] - weights[0])\n            elif wt in ['fc', 'quadratic']:\n                weights = (weights[:, None] - weights) ** 2 / (weights[-1] - weights[0]) ** 2\n            elif wt == 'toeplitz':\n                from scipy.linalg import toeplitz\n                weights = toeplitz(weights)\n            else:\n                raise ValueError('wt option is not known')\n        else:\n            (rows, cols) = table.shape\n            if table.shape != weights.shape:\n                raise ValueError('weights are not square')\n        kappa = 1 - (weights * table).sum() / nobs / (weights * prob_exp).sum()\n        if return_results:\n            var_kappa = np.nan\n            var_kappa0 = np.nan\n            w = 1.0 - weights\n            w_row = (freq_col * w).sum(1)\n            w_col = (freq_row[:, None] * w).sum(0)\n            agree_wexp = (w * freq_col * freq_row[:, None]).sum()\n            term_a = freqs * (w - (w_col + w_row[:, None]) * (1 - kappa)) ** 2\n            fac = 1.0 / ((1 - agree_wexp) ** 2 * nobs)\n            var_kappa = term_a.sum() - (kappa - agree_wexp * (1 - kappa)) ** 2\n            var_kappa *= fac\n            freqse = freq_col * freq_row[:, None]\n            var_kappa0 = (freqse * (w - (w_col + w_row[:, None])) ** 2).sum()\n            var_kappa0 -= agree_wexp ** 2\n            var_kappa0 *= fac\n    kappa_max = (np.minimum(freq_row, freq_col).sum() - agree_exp) / (1 - agree_exp)\n    if return_results:\n        res = KappaResults(kind=kind, kappa=kappa, kappa_max=kappa_max, weights=weights, var_kappa=var_kappa, var_kappa0=var_kappa0)\n        return res\n    else:\n        return kappa",
            "def cohens_kappa(table, weights=None, return_results=True, wt=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute Cohen\\'s kappa with variance and equal-zero test\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-Dim\\n        square array with results of two raters, one rater in rows, second\\n        rater in columns\\n    weights : array_like\\n        The interpretation of weights depends on the wt argument.\\n        If both are None, then the simple kappa is computed.\\n        see wt for the case when wt is not None\\n        If weights is two dimensional, then it is directly used as a weight\\n        matrix. For computing the variance of kappa, the maximum of the\\n        weights is assumed to be smaller or equal to one.\\n        TODO: fix conflicting definitions in the 2-Dim case for\\n    wt : {None, str}\\n        If wt and weights are None, then the simple kappa is computed.\\n        If wt is given, but weights is None, then the weights are set to\\n        be [0, 1, 2, ..., k].\\n        If weights is a one-dimensional array, then it is used to construct\\n        the weight matrix given the following options.\\n\\n        wt in [\\'linear\\', \\'ca\\' or None] : use linear weights, Cicchetti-Allison\\n            actual weights are linear in the score \"weights\" difference\\n        wt in [\\'quadratic\\', \\'fc\\'] : use linear weights, Fleiss-Cohen\\n            actual weights are squared in the score \"weights\" difference\\n        wt = \\'toeplitz\\' : weight matrix is constructed as a toeplitz matrix\\n            from the one dimensional weights.\\n\\n    return_results : bool\\n        If True (default), then an instance of KappaResults is returned.\\n        If False, then only kappa is computed and returned.\\n\\n    Returns\\n    -------\\n    results or kappa\\n        If return_results is True (default), then a results instance with all\\n        statistics is returned\\n        If return_results is False, then only kappa is calculated and returned.\\n\\n    Notes\\n    -----\\n    There are two conflicting definitions of the weight matrix, Wikipedia\\n    versus SAS manual. However, the computation are invariant to rescaling\\n    of the weights matrix, so there is no difference in the results.\\n\\n    Weights for \\'linear\\' and \\'quadratic\\' are interpreted as scores for the\\n    categories, the weights in the computation are based on the pairwise\\n    difference between the scores.\\n    Weights for \\'toeplitz\\' are a interpreted as weighted distance. The distance\\n    only depends on how many levels apart two entries in the table are but\\n    not on the levels themselves.\\n\\n    example:\\n\\n    weights = \\'0, 1, 2, 3\\' and wt is either linear or toeplitz means that the\\n    weighting only depends on the simple distance of levels.\\n\\n    weights = \\'0, 0, 1, 1\\' and wt = \\'linear\\' means that the first two levels\\n    are zero distance apart and the same for the last two levels. This is\\n    the sample as forming two aggregated levels by merging the first two and\\n    the last two levels, respectively.\\n\\n    weights = [0, 1, 2, 3] and wt = \\'quadratic\\' is the same as squaring these\\n    weights and using wt = \\'toeplitz\\'.\\n\\n    References\\n    ----------\\n    Wikipedia\\n    SAS Manual\\n\\n    '\n    table = np.asarray(table, float)\n    agree = np.diag(table).sum()\n    nobs = table.sum()\n    probs = table / nobs\n    freqs = probs\n    probs_diag = np.diag(probs)\n    freq_row = table.sum(1) / nobs\n    freq_col = table.sum(0) / nobs\n    prob_exp = freq_col * freq_row[:, None]\n    assert np.allclose(prob_exp.sum(), 1)\n    agree_exp = np.diag(prob_exp).sum()\n    if weights is None and wt is None:\n        kind = 'Simple'\n        kappa = (agree / nobs - agree_exp) / (1 - agree_exp)\n        if return_results:\n            term_a = probs_diag * (1 - (freq_row + freq_col) * (1 - kappa)) ** 2\n            term_a = term_a.sum()\n            term_b = probs * (freq_col[:, None] + freq_row) ** 2\n            d_idx = np.arange(table.shape[0])\n            term_b[d_idx, d_idx] = 0\n            term_b = (1 - kappa) ** 2 * term_b.sum()\n            term_c = (kappa - agree_exp * (1 - kappa)) ** 2\n            var_kappa = (term_a + term_b - term_c) / (1 - agree_exp) ** 2 / nobs\n            term_c = freq_col * freq_row * (freq_col + freq_row)\n            var_kappa0 = agree_exp + agree_exp ** 2 - term_c.sum()\n            var_kappa0 /= (1 - agree_exp) ** 2 * nobs\n    else:\n        if weights is None:\n            weights = np.arange(table.shape[0])\n        kind = 'Weighted'\n        weights = np.asarray(weights, float)\n        if weights.ndim == 1:\n            if wt in ['ca', 'linear', None]:\n                weights = np.abs(weights[:, None] - weights) / (weights[-1] - weights[0])\n            elif wt in ['fc', 'quadratic']:\n                weights = (weights[:, None] - weights) ** 2 / (weights[-1] - weights[0]) ** 2\n            elif wt == 'toeplitz':\n                from scipy.linalg import toeplitz\n                weights = toeplitz(weights)\n            else:\n                raise ValueError('wt option is not known')\n        else:\n            (rows, cols) = table.shape\n            if table.shape != weights.shape:\n                raise ValueError('weights are not square')\n        kappa = 1 - (weights * table).sum() / nobs / (weights * prob_exp).sum()\n        if return_results:\n            var_kappa = np.nan\n            var_kappa0 = np.nan\n            w = 1.0 - weights\n            w_row = (freq_col * w).sum(1)\n            w_col = (freq_row[:, None] * w).sum(0)\n            agree_wexp = (w * freq_col * freq_row[:, None]).sum()\n            term_a = freqs * (w - (w_col + w_row[:, None]) * (1 - kappa)) ** 2\n            fac = 1.0 / ((1 - agree_wexp) ** 2 * nobs)\n            var_kappa = term_a.sum() - (kappa - agree_wexp * (1 - kappa)) ** 2\n            var_kappa *= fac\n            freqse = freq_col * freq_row[:, None]\n            var_kappa0 = (freqse * (w - (w_col + w_row[:, None])) ** 2).sum()\n            var_kappa0 -= agree_wexp ** 2\n            var_kappa0 *= fac\n    kappa_max = (np.minimum(freq_row, freq_col).sum() - agree_exp) / (1 - agree_exp)\n    if return_results:\n        res = KappaResults(kind=kind, kappa=kappa, kappa_max=kappa_max, weights=weights, var_kappa=var_kappa, var_kappa0=var_kappa0)\n        return res\n    else:\n        return kappa",
            "def cohens_kappa(table, weights=None, return_results=True, wt=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute Cohen\\'s kappa with variance and equal-zero test\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-Dim\\n        square array with results of two raters, one rater in rows, second\\n        rater in columns\\n    weights : array_like\\n        The interpretation of weights depends on the wt argument.\\n        If both are None, then the simple kappa is computed.\\n        see wt for the case when wt is not None\\n        If weights is two dimensional, then it is directly used as a weight\\n        matrix. For computing the variance of kappa, the maximum of the\\n        weights is assumed to be smaller or equal to one.\\n        TODO: fix conflicting definitions in the 2-Dim case for\\n    wt : {None, str}\\n        If wt and weights are None, then the simple kappa is computed.\\n        If wt is given, but weights is None, then the weights are set to\\n        be [0, 1, 2, ..., k].\\n        If weights is a one-dimensional array, then it is used to construct\\n        the weight matrix given the following options.\\n\\n        wt in [\\'linear\\', \\'ca\\' or None] : use linear weights, Cicchetti-Allison\\n            actual weights are linear in the score \"weights\" difference\\n        wt in [\\'quadratic\\', \\'fc\\'] : use linear weights, Fleiss-Cohen\\n            actual weights are squared in the score \"weights\" difference\\n        wt = \\'toeplitz\\' : weight matrix is constructed as a toeplitz matrix\\n            from the one dimensional weights.\\n\\n    return_results : bool\\n        If True (default), then an instance of KappaResults is returned.\\n        If False, then only kappa is computed and returned.\\n\\n    Returns\\n    -------\\n    results or kappa\\n        If return_results is True (default), then a results instance with all\\n        statistics is returned\\n        If return_results is False, then only kappa is calculated and returned.\\n\\n    Notes\\n    -----\\n    There are two conflicting definitions of the weight matrix, Wikipedia\\n    versus SAS manual. However, the computation are invariant to rescaling\\n    of the weights matrix, so there is no difference in the results.\\n\\n    Weights for \\'linear\\' and \\'quadratic\\' are interpreted as scores for the\\n    categories, the weights in the computation are based on the pairwise\\n    difference between the scores.\\n    Weights for \\'toeplitz\\' are a interpreted as weighted distance. The distance\\n    only depends on how many levels apart two entries in the table are but\\n    not on the levels themselves.\\n\\n    example:\\n\\n    weights = \\'0, 1, 2, 3\\' and wt is either linear or toeplitz means that the\\n    weighting only depends on the simple distance of levels.\\n\\n    weights = \\'0, 0, 1, 1\\' and wt = \\'linear\\' means that the first two levels\\n    are zero distance apart and the same for the last two levels. This is\\n    the sample as forming two aggregated levels by merging the first two and\\n    the last two levels, respectively.\\n\\n    weights = [0, 1, 2, 3] and wt = \\'quadratic\\' is the same as squaring these\\n    weights and using wt = \\'toeplitz\\'.\\n\\n    References\\n    ----------\\n    Wikipedia\\n    SAS Manual\\n\\n    '\n    table = np.asarray(table, float)\n    agree = np.diag(table).sum()\n    nobs = table.sum()\n    probs = table / nobs\n    freqs = probs\n    probs_diag = np.diag(probs)\n    freq_row = table.sum(1) / nobs\n    freq_col = table.sum(0) / nobs\n    prob_exp = freq_col * freq_row[:, None]\n    assert np.allclose(prob_exp.sum(), 1)\n    agree_exp = np.diag(prob_exp).sum()\n    if weights is None and wt is None:\n        kind = 'Simple'\n        kappa = (agree / nobs - agree_exp) / (1 - agree_exp)\n        if return_results:\n            term_a = probs_diag * (1 - (freq_row + freq_col) * (1 - kappa)) ** 2\n            term_a = term_a.sum()\n            term_b = probs * (freq_col[:, None] + freq_row) ** 2\n            d_idx = np.arange(table.shape[0])\n            term_b[d_idx, d_idx] = 0\n            term_b = (1 - kappa) ** 2 * term_b.sum()\n            term_c = (kappa - agree_exp * (1 - kappa)) ** 2\n            var_kappa = (term_a + term_b - term_c) / (1 - agree_exp) ** 2 / nobs\n            term_c = freq_col * freq_row * (freq_col + freq_row)\n            var_kappa0 = agree_exp + agree_exp ** 2 - term_c.sum()\n            var_kappa0 /= (1 - agree_exp) ** 2 * nobs\n    else:\n        if weights is None:\n            weights = np.arange(table.shape[0])\n        kind = 'Weighted'\n        weights = np.asarray(weights, float)\n        if weights.ndim == 1:\n            if wt in ['ca', 'linear', None]:\n                weights = np.abs(weights[:, None] - weights) / (weights[-1] - weights[0])\n            elif wt in ['fc', 'quadratic']:\n                weights = (weights[:, None] - weights) ** 2 / (weights[-1] - weights[0]) ** 2\n            elif wt == 'toeplitz':\n                from scipy.linalg import toeplitz\n                weights = toeplitz(weights)\n            else:\n                raise ValueError('wt option is not known')\n        else:\n            (rows, cols) = table.shape\n            if table.shape != weights.shape:\n                raise ValueError('weights are not square')\n        kappa = 1 - (weights * table).sum() / nobs / (weights * prob_exp).sum()\n        if return_results:\n            var_kappa = np.nan\n            var_kappa0 = np.nan\n            w = 1.0 - weights\n            w_row = (freq_col * w).sum(1)\n            w_col = (freq_row[:, None] * w).sum(0)\n            agree_wexp = (w * freq_col * freq_row[:, None]).sum()\n            term_a = freqs * (w - (w_col + w_row[:, None]) * (1 - kappa)) ** 2\n            fac = 1.0 / ((1 - agree_wexp) ** 2 * nobs)\n            var_kappa = term_a.sum() - (kappa - agree_wexp * (1 - kappa)) ** 2\n            var_kappa *= fac\n            freqse = freq_col * freq_row[:, None]\n            var_kappa0 = (freqse * (w - (w_col + w_row[:, None])) ** 2).sum()\n            var_kappa0 -= agree_wexp ** 2\n            var_kappa0 *= fac\n    kappa_max = (np.minimum(freq_row, freq_col).sum() - agree_exp) / (1 - agree_exp)\n    if return_results:\n        res = KappaResults(kind=kind, kappa=kappa, kappa_max=kappa_max, weights=weights, var_kappa=var_kappa, var_kappa0=var_kappa0)\n        return res\n    else:\n        return kappa",
            "def cohens_kappa(table, weights=None, return_results=True, wt=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute Cohen\\'s kappa with variance and equal-zero test\\n\\n    Parameters\\n    ----------\\n    table : array_like, 2-Dim\\n        square array with results of two raters, one rater in rows, second\\n        rater in columns\\n    weights : array_like\\n        The interpretation of weights depends on the wt argument.\\n        If both are None, then the simple kappa is computed.\\n        see wt for the case when wt is not None\\n        If weights is two dimensional, then it is directly used as a weight\\n        matrix. For computing the variance of kappa, the maximum of the\\n        weights is assumed to be smaller or equal to one.\\n        TODO: fix conflicting definitions in the 2-Dim case for\\n    wt : {None, str}\\n        If wt and weights are None, then the simple kappa is computed.\\n        If wt is given, but weights is None, then the weights are set to\\n        be [0, 1, 2, ..., k].\\n        If weights is a one-dimensional array, then it is used to construct\\n        the weight matrix given the following options.\\n\\n        wt in [\\'linear\\', \\'ca\\' or None] : use linear weights, Cicchetti-Allison\\n            actual weights are linear in the score \"weights\" difference\\n        wt in [\\'quadratic\\', \\'fc\\'] : use linear weights, Fleiss-Cohen\\n            actual weights are squared in the score \"weights\" difference\\n        wt = \\'toeplitz\\' : weight matrix is constructed as a toeplitz matrix\\n            from the one dimensional weights.\\n\\n    return_results : bool\\n        If True (default), then an instance of KappaResults is returned.\\n        If False, then only kappa is computed and returned.\\n\\n    Returns\\n    -------\\n    results or kappa\\n        If return_results is True (default), then a results instance with all\\n        statistics is returned\\n        If return_results is False, then only kappa is calculated and returned.\\n\\n    Notes\\n    -----\\n    There are two conflicting definitions of the weight matrix, Wikipedia\\n    versus SAS manual. However, the computation are invariant to rescaling\\n    of the weights matrix, so there is no difference in the results.\\n\\n    Weights for \\'linear\\' and \\'quadratic\\' are interpreted as scores for the\\n    categories, the weights in the computation are based on the pairwise\\n    difference between the scores.\\n    Weights for \\'toeplitz\\' are a interpreted as weighted distance. The distance\\n    only depends on how many levels apart two entries in the table are but\\n    not on the levels themselves.\\n\\n    example:\\n\\n    weights = \\'0, 1, 2, 3\\' and wt is either linear or toeplitz means that the\\n    weighting only depends on the simple distance of levels.\\n\\n    weights = \\'0, 0, 1, 1\\' and wt = \\'linear\\' means that the first two levels\\n    are zero distance apart and the same for the last two levels. This is\\n    the sample as forming two aggregated levels by merging the first two and\\n    the last two levels, respectively.\\n\\n    weights = [0, 1, 2, 3] and wt = \\'quadratic\\' is the same as squaring these\\n    weights and using wt = \\'toeplitz\\'.\\n\\n    References\\n    ----------\\n    Wikipedia\\n    SAS Manual\\n\\n    '\n    table = np.asarray(table, float)\n    agree = np.diag(table).sum()\n    nobs = table.sum()\n    probs = table / nobs\n    freqs = probs\n    probs_diag = np.diag(probs)\n    freq_row = table.sum(1) / nobs\n    freq_col = table.sum(0) / nobs\n    prob_exp = freq_col * freq_row[:, None]\n    assert np.allclose(prob_exp.sum(), 1)\n    agree_exp = np.diag(prob_exp).sum()\n    if weights is None and wt is None:\n        kind = 'Simple'\n        kappa = (agree / nobs - agree_exp) / (1 - agree_exp)\n        if return_results:\n            term_a = probs_diag * (1 - (freq_row + freq_col) * (1 - kappa)) ** 2\n            term_a = term_a.sum()\n            term_b = probs * (freq_col[:, None] + freq_row) ** 2\n            d_idx = np.arange(table.shape[0])\n            term_b[d_idx, d_idx] = 0\n            term_b = (1 - kappa) ** 2 * term_b.sum()\n            term_c = (kappa - agree_exp * (1 - kappa)) ** 2\n            var_kappa = (term_a + term_b - term_c) / (1 - agree_exp) ** 2 / nobs\n            term_c = freq_col * freq_row * (freq_col + freq_row)\n            var_kappa0 = agree_exp + agree_exp ** 2 - term_c.sum()\n            var_kappa0 /= (1 - agree_exp) ** 2 * nobs\n    else:\n        if weights is None:\n            weights = np.arange(table.shape[0])\n        kind = 'Weighted'\n        weights = np.asarray(weights, float)\n        if weights.ndim == 1:\n            if wt in ['ca', 'linear', None]:\n                weights = np.abs(weights[:, None] - weights) / (weights[-1] - weights[0])\n            elif wt in ['fc', 'quadratic']:\n                weights = (weights[:, None] - weights) ** 2 / (weights[-1] - weights[0]) ** 2\n            elif wt == 'toeplitz':\n                from scipy.linalg import toeplitz\n                weights = toeplitz(weights)\n            else:\n                raise ValueError('wt option is not known')\n        else:\n            (rows, cols) = table.shape\n            if table.shape != weights.shape:\n                raise ValueError('weights are not square')\n        kappa = 1 - (weights * table).sum() / nobs / (weights * prob_exp).sum()\n        if return_results:\n            var_kappa = np.nan\n            var_kappa0 = np.nan\n            w = 1.0 - weights\n            w_row = (freq_col * w).sum(1)\n            w_col = (freq_row[:, None] * w).sum(0)\n            agree_wexp = (w * freq_col * freq_row[:, None]).sum()\n            term_a = freqs * (w - (w_col + w_row[:, None]) * (1 - kappa)) ** 2\n            fac = 1.0 / ((1 - agree_wexp) ** 2 * nobs)\n            var_kappa = term_a.sum() - (kappa - agree_wexp * (1 - kappa)) ** 2\n            var_kappa *= fac\n            freqse = freq_col * freq_row[:, None]\n            var_kappa0 = (freqse * (w - (w_col + w_row[:, None])) ** 2).sum()\n            var_kappa0 -= agree_wexp ** 2\n            var_kappa0 *= fac\n    kappa_max = (np.minimum(freq_row, freq_col).sum() - agree_exp) / (1 - agree_exp)\n    if return_results:\n        res = KappaResults(kind=kind, kappa=kappa, kappa_max=kappa_max, weights=weights, var_kappa=var_kappa, var_kappa0=var_kappa0)\n        return res\n    else:\n        return kappa"
        ]
    },
    {
        "func_name": "_initialize",
        "original": "def _initialize(self):\n    if 'alpha' not in self:\n        self['alpha'] = 0.025\n        self['alpha_ci'] = _int_ifclose(100 - 0.025 * 200)[1]\n    self['std_kappa'] = np.sqrt(self['var_kappa'])\n    self['std_kappa0'] = np.sqrt(self['var_kappa0'])\n    self['z_value'] = self['kappa'] / self['std_kappa0']\n    self['pvalue_one_sided'] = stats.norm.sf(self['z_value'])\n    self['pvalue_two_sided'] = stats.norm.sf(np.abs(self['z_value'])) * 2\n    delta = stats.norm.isf(self['alpha']) * self['std_kappa']\n    self['kappa_low'] = self['kappa'] - delta\n    self['kappa_upp'] = self['kappa'] + delta\n    self['distribution_kappa'] = stats.norm(loc=self['kappa'], scale=self['std_kappa'])\n    self['distribution_zero_null'] = stats.norm(loc=0, scale=self['std_kappa0'])",
        "mutated": [
            "def _initialize(self):\n    if False:\n        i = 10\n    if 'alpha' not in self:\n        self['alpha'] = 0.025\n        self['alpha_ci'] = _int_ifclose(100 - 0.025 * 200)[1]\n    self['std_kappa'] = np.sqrt(self['var_kappa'])\n    self['std_kappa0'] = np.sqrt(self['var_kappa0'])\n    self['z_value'] = self['kappa'] / self['std_kappa0']\n    self['pvalue_one_sided'] = stats.norm.sf(self['z_value'])\n    self['pvalue_two_sided'] = stats.norm.sf(np.abs(self['z_value'])) * 2\n    delta = stats.norm.isf(self['alpha']) * self['std_kappa']\n    self['kappa_low'] = self['kappa'] - delta\n    self['kappa_upp'] = self['kappa'] + delta\n    self['distribution_kappa'] = stats.norm(loc=self['kappa'], scale=self['std_kappa'])\n    self['distribution_zero_null'] = stats.norm(loc=0, scale=self['std_kappa0'])",
            "def _initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if 'alpha' not in self:\n        self['alpha'] = 0.025\n        self['alpha_ci'] = _int_ifclose(100 - 0.025 * 200)[1]\n    self['std_kappa'] = np.sqrt(self['var_kappa'])\n    self['std_kappa0'] = np.sqrt(self['var_kappa0'])\n    self['z_value'] = self['kappa'] / self['std_kappa0']\n    self['pvalue_one_sided'] = stats.norm.sf(self['z_value'])\n    self['pvalue_two_sided'] = stats.norm.sf(np.abs(self['z_value'])) * 2\n    delta = stats.norm.isf(self['alpha']) * self['std_kappa']\n    self['kappa_low'] = self['kappa'] - delta\n    self['kappa_upp'] = self['kappa'] + delta\n    self['distribution_kappa'] = stats.norm(loc=self['kappa'], scale=self['std_kappa'])\n    self['distribution_zero_null'] = stats.norm(loc=0, scale=self['std_kappa0'])",
            "def _initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if 'alpha' not in self:\n        self['alpha'] = 0.025\n        self['alpha_ci'] = _int_ifclose(100 - 0.025 * 200)[1]\n    self['std_kappa'] = np.sqrt(self['var_kappa'])\n    self['std_kappa0'] = np.sqrt(self['var_kappa0'])\n    self['z_value'] = self['kappa'] / self['std_kappa0']\n    self['pvalue_one_sided'] = stats.norm.sf(self['z_value'])\n    self['pvalue_two_sided'] = stats.norm.sf(np.abs(self['z_value'])) * 2\n    delta = stats.norm.isf(self['alpha']) * self['std_kappa']\n    self['kappa_low'] = self['kappa'] - delta\n    self['kappa_upp'] = self['kappa'] + delta\n    self['distribution_kappa'] = stats.norm(loc=self['kappa'], scale=self['std_kappa'])\n    self['distribution_zero_null'] = stats.norm(loc=0, scale=self['std_kappa0'])",
            "def _initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if 'alpha' not in self:\n        self['alpha'] = 0.025\n        self['alpha_ci'] = _int_ifclose(100 - 0.025 * 200)[1]\n    self['std_kappa'] = np.sqrt(self['var_kappa'])\n    self['std_kappa0'] = np.sqrt(self['var_kappa0'])\n    self['z_value'] = self['kappa'] / self['std_kappa0']\n    self['pvalue_one_sided'] = stats.norm.sf(self['z_value'])\n    self['pvalue_two_sided'] = stats.norm.sf(np.abs(self['z_value'])) * 2\n    delta = stats.norm.isf(self['alpha']) * self['std_kappa']\n    self['kappa_low'] = self['kappa'] - delta\n    self['kappa_upp'] = self['kappa'] + delta\n    self['distribution_kappa'] = stats.norm(loc=self['kappa'], scale=self['std_kappa'])\n    self['distribution_zero_null'] = stats.norm(loc=0, scale=self['std_kappa0'])",
            "def _initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if 'alpha' not in self:\n        self['alpha'] = 0.025\n        self['alpha_ci'] = _int_ifclose(100 - 0.025 * 200)[1]\n    self['std_kappa'] = np.sqrt(self['var_kappa'])\n    self['std_kappa0'] = np.sqrt(self['var_kappa0'])\n    self['z_value'] = self['kappa'] / self['std_kappa0']\n    self['pvalue_one_sided'] = stats.norm.sf(self['z_value'])\n    self['pvalue_two_sided'] = stats.norm.sf(np.abs(self['z_value'])) * 2\n    delta = stats.norm.isf(self['alpha']) * self['std_kappa']\n    self['kappa_low'] = self['kappa'] - delta\n    self['kappa_upp'] = self['kappa'] + delta\n    self['distribution_kappa'] = stats.norm(loc=self['kappa'], scale=self['std_kappa'])\n    self['distribution_zero_null'] = stats.norm(loc=0, scale=self['std_kappa0'])"
        ]
    },
    {
        "func_name": "__str__",
        "original": "def __str__(self):\n    return self.template % self",
        "mutated": [
            "def __str__(self):\n    if False:\n        i = 10\n    return self.template % self",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.template % self",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.template % self",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.template % self",
            "def __str__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.template % self"
        ]
    }
]