[
    {
        "func_name": "rewrite_dict_keys",
        "original": "def rewrite_dict_keys(d):\n    d2 = dict(((re.sub('@@$', '', k), v) if k.endswith('@@') else (re.sub('$', '</w>', k), v) for (k, v) in d.items()))\n    keep_keys = '<s> <pad> </s> <unk>'.split()\n    for k in keep_keys:\n        del d2[f'{k}</w>']\n        d2[k] = d[k]\n    return d2",
        "mutated": [
            "def rewrite_dict_keys(d):\n    if False:\n        i = 10\n    d2 = dict(((re.sub('@@$', '', k), v) if k.endswith('@@') else (re.sub('$', '</w>', k), v) for (k, v) in d.items()))\n    keep_keys = '<s> <pad> </s> <unk>'.split()\n    for k in keep_keys:\n        del d2[f'{k}</w>']\n        d2[k] = d[k]\n    return d2",
            "def rewrite_dict_keys(d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    d2 = dict(((re.sub('@@$', '', k), v) if k.endswith('@@') else (re.sub('$', '</w>', k), v) for (k, v) in d.items()))\n    keep_keys = '<s> <pad> </s> <unk>'.split()\n    for k in keep_keys:\n        del d2[f'{k}</w>']\n        d2[k] = d[k]\n    return d2",
            "def rewrite_dict_keys(d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    d2 = dict(((re.sub('@@$', '', k), v) if k.endswith('@@') else (re.sub('$', '</w>', k), v) for (k, v) in d.items()))\n    keep_keys = '<s> <pad> </s> <unk>'.split()\n    for k in keep_keys:\n        del d2[f'{k}</w>']\n        d2[k] = d[k]\n    return d2",
            "def rewrite_dict_keys(d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    d2 = dict(((re.sub('@@$', '', k), v) if k.endswith('@@') else (re.sub('$', '</w>', k), v) for (k, v) in d.items()))\n    keep_keys = '<s> <pad> </s> <unk>'.split()\n    for k in keep_keys:\n        del d2[f'{k}</w>']\n        d2[k] = d[k]\n    return d2",
            "def rewrite_dict_keys(d):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    d2 = dict(((re.sub('@@$', '', k), v) if k.endswith('@@') else (re.sub('$', '</w>', k), v) for (k, v) in d.items()))\n    keep_keys = '<s> <pad> </s> <unk>'.split()\n    for k in keep_keys:\n        del d2[f'{k}</w>']\n        d2[k] = d[k]\n    return d2"
        ]
    },
    {
        "func_name": "convert_fsmt_checkpoint_to_pytorch",
        "original": "def convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path, pytorch_dump_folder_path):\n    assert os.path.exists(fsmt_checkpoint_path)\n    os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n    print(f'Writing results to {pytorch_dump_folder_path}')\n    checkpoint_file = basename(fsmt_checkpoint_path)\n    fsmt_folder_path = dirname(fsmt_checkpoint_path)\n    cls = fairseq.model_parallel.models.transformer.ModelParallelTransformerModel\n    models = cls.hub_models()\n    kwargs = {'bpe': 'fastbpe', 'tokenizer': 'moses'}\n    data_name_or_path = '.'\n    print(f'using checkpoint {checkpoint_file}')\n    chkpt = hub_utils.from_pretrained(fsmt_folder_path, checkpoint_file, data_name_or_path, archive_map=models, **kwargs)\n    args = vars(chkpt['args']['model'])\n    src_lang = args['source_lang']\n    tgt_lang = args['target_lang']\n    data_root = dirname(pytorch_dump_folder_path)\n    model_dir = basename(pytorch_dump_folder_path)\n    src_dict_file = os.path.join(fsmt_folder_path, f'dict.{src_lang}.txt')\n    tgt_dict_file = os.path.join(fsmt_folder_path, f'dict.{tgt_lang}.txt')\n    src_dict = Dictionary.load(src_dict_file)\n    src_vocab = rewrite_dict_keys(src_dict.indices)\n    src_vocab_size = len(src_vocab)\n    src_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-src.json')\n    print(f'Generating {src_vocab_file} of {src_vocab_size} of {src_lang} records')\n    with open(src_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(src_vocab, ensure_ascii=False, indent=json_indent))\n    do_lower_case = True\n    for k in src_vocab.keys():\n        if not k.islower():\n            do_lower_case = False\n            break\n    tgt_dict = Dictionary.load(tgt_dict_file)\n    tgt_vocab = rewrite_dict_keys(tgt_dict.indices)\n    tgt_vocab_size = len(tgt_vocab)\n    tgt_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-tgt.json')\n    print(f'Generating {tgt_vocab_file} of {tgt_vocab_size} of {tgt_lang} records')\n    with open(tgt_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tgt_vocab, ensure_ascii=False, indent=json_indent))\n    merges_file = os.path.join(pytorch_dump_folder_path, VOCAB_FILES_NAMES['merges_file'])\n    for fn in ['bpecodes', 'code']:\n        fsmt_merges_file = os.path.join(fsmt_folder_path, fn)\n        if os.path.exists(fsmt_merges_file):\n            break\n    with open(fsmt_merges_file, encoding='utf-8') as fin:\n        merges = fin.read()\n    merges = re.sub(' \\\\d+$', '', merges, 0, re.M)\n    print(f'Generating {merges_file}')\n    with open(merges_file, 'w', encoding='utf-8') as fout:\n        fout.write(merges)\n    fsmt_model_config_file = os.path.join(pytorch_dump_folder_path, 'config.json')\n    assert args['bpe'] == 'fastbpe', f\"need to extend tokenizer to support bpe={args['bpe']}\"\n    assert args['tokenizer'] == 'moses', f\"need to extend tokenizer to support bpe={args['tokenizer']}\"\n    model_conf = {'architectures': ['FSMTForConditionalGeneration'], 'model_type': 'fsmt', 'activation_dropout': args['activation_dropout'], 'activation_function': 'relu', 'attention_dropout': args['attention_dropout'], 'd_model': args['decoder_embed_dim'], 'dropout': args['dropout'], 'init_std': 0.02, 'max_position_embeddings': args['max_source_positions'], 'num_hidden_layers': args['encoder_layers'], 'src_vocab_size': src_vocab_size, 'tgt_vocab_size': tgt_vocab_size, 'langs': [src_lang, tgt_lang], 'encoder_attention_heads': args['encoder_attention_heads'], 'encoder_ffn_dim': args['encoder_ffn_embed_dim'], 'encoder_layerdrop': args['encoder_layerdrop'], 'encoder_layers': args['encoder_layers'], 'decoder_attention_heads': args['decoder_attention_heads'], 'decoder_ffn_dim': args['decoder_ffn_embed_dim'], 'decoder_layerdrop': args['decoder_layerdrop'], 'decoder_layers': args['decoder_layers'], 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'is_encoder_decoder': True, 'scale_embedding': not args['no_scale_embedding'], 'tie_word_embeddings': args['share_all_embeddings']}\n    model_conf['num_beams'] = 5\n    model_conf['early_stopping'] = False\n    if model_dir in best_score_hparams and 'length_penalty' in best_score_hparams[model_dir]:\n        model_conf['length_penalty'] = best_score_hparams[model_dir]['length_penalty']\n    else:\n        model_conf['length_penalty'] = 1.0\n    print(f'Generating {fsmt_model_config_file}')\n    with open(fsmt_model_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(model_conf, ensure_ascii=False, indent=json_indent))\n    fsmt_tokenizer_config_file = os.path.join(pytorch_dump_folder_path, TOKENIZER_CONFIG_FILE)\n    tokenizer_conf = {'langs': [src_lang, tgt_lang], 'model_max_length': 1024, 'do_lower_case': do_lower_case}\n    print(f'Generating {fsmt_tokenizer_config_file}')\n    with open(fsmt_tokenizer_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tokenizer_conf, ensure_ascii=False, indent=json_indent))\n    model = chkpt['models'][0]\n    model_state_dict = model.state_dict()\n    model_state_dict = OrderedDict((('model.' + k, v) for (k, v) in model_state_dict.items()))\n    ignore_keys = ['model.model', 'model.encoder.version', 'model.decoder.version', 'model.encoder_embed_tokens.weight', 'model.decoder_embed_tokens.weight', 'model.encoder.embed_positions._float_tensor', 'model.decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        model_state_dict.pop(k, None)\n    config = FSMTConfig.from_pretrained(pytorch_dump_folder_path)\n    model_new = FSMTForConditionalGeneration(config)\n    model_new.load_state_dict(model_state_dict, strict=False)\n    pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)\n    print(f'Generating {pytorch_weights_dump_path}')\n    torch.save(model_state_dict, pytorch_weights_dump_path)\n    print('Conversion is done!')\n    print('\\nLast step is to upload the files to s3')\n    print(f'cd {data_root}')\n    print(f'transformers-cli upload {model_dir}')",
        "mutated": [
            "def convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n    assert os.path.exists(fsmt_checkpoint_path)\n    os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n    print(f'Writing results to {pytorch_dump_folder_path}')\n    checkpoint_file = basename(fsmt_checkpoint_path)\n    fsmt_folder_path = dirname(fsmt_checkpoint_path)\n    cls = fairseq.model_parallel.models.transformer.ModelParallelTransformerModel\n    models = cls.hub_models()\n    kwargs = {'bpe': 'fastbpe', 'tokenizer': 'moses'}\n    data_name_or_path = '.'\n    print(f'using checkpoint {checkpoint_file}')\n    chkpt = hub_utils.from_pretrained(fsmt_folder_path, checkpoint_file, data_name_or_path, archive_map=models, **kwargs)\n    args = vars(chkpt['args']['model'])\n    src_lang = args['source_lang']\n    tgt_lang = args['target_lang']\n    data_root = dirname(pytorch_dump_folder_path)\n    model_dir = basename(pytorch_dump_folder_path)\n    src_dict_file = os.path.join(fsmt_folder_path, f'dict.{src_lang}.txt')\n    tgt_dict_file = os.path.join(fsmt_folder_path, f'dict.{tgt_lang}.txt')\n    src_dict = Dictionary.load(src_dict_file)\n    src_vocab = rewrite_dict_keys(src_dict.indices)\n    src_vocab_size = len(src_vocab)\n    src_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-src.json')\n    print(f'Generating {src_vocab_file} of {src_vocab_size} of {src_lang} records')\n    with open(src_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(src_vocab, ensure_ascii=False, indent=json_indent))\n    do_lower_case = True\n    for k in src_vocab.keys():\n        if not k.islower():\n            do_lower_case = False\n            break\n    tgt_dict = Dictionary.load(tgt_dict_file)\n    tgt_vocab = rewrite_dict_keys(tgt_dict.indices)\n    tgt_vocab_size = len(tgt_vocab)\n    tgt_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-tgt.json')\n    print(f'Generating {tgt_vocab_file} of {tgt_vocab_size} of {tgt_lang} records')\n    with open(tgt_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tgt_vocab, ensure_ascii=False, indent=json_indent))\n    merges_file = os.path.join(pytorch_dump_folder_path, VOCAB_FILES_NAMES['merges_file'])\n    for fn in ['bpecodes', 'code']:\n        fsmt_merges_file = os.path.join(fsmt_folder_path, fn)\n        if os.path.exists(fsmt_merges_file):\n            break\n    with open(fsmt_merges_file, encoding='utf-8') as fin:\n        merges = fin.read()\n    merges = re.sub(' \\\\d+$', '', merges, 0, re.M)\n    print(f'Generating {merges_file}')\n    with open(merges_file, 'w', encoding='utf-8') as fout:\n        fout.write(merges)\n    fsmt_model_config_file = os.path.join(pytorch_dump_folder_path, 'config.json')\n    assert args['bpe'] == 'fastbpe', f\"need to extend tokenizer to support bpe={args['bpe']}\"\n    assert args['tokenizer'] == 'moses', f\"need to extend tokenizer to support bpe={args['tokenizer']}\"\n    model_conf = {'architectures': ['FSMTForConditionalGeneration'], 'model_type': 'fsmt', 'activation_dropout': args['activation_dropout'], 'activation_function': 'relu', 'attention_dropout': args['attention_dropout'], 'd_model': args['decoder_embed_dim'], 'dropout': args['dropout'], 'init_std': 0.02, 'max_position_embeddings': args['max_source_positions'], 'num_hidden_layers': args['encoder_layers'], 'src_vocab_size': src_vocab_size, 'tgt_vocab_size': tgt_vocab_size, 'langs': [src_lang, tgt_lang], 'encoder_attention_heads': args['encoder_attention_heads'], 'encoder_ffn_dim': args['encoder_ffn_embed_dim'], 'encoder_layerdrop': args['encoder_layerdrop'], 'encoder_layers': args['encoder_layers'], 'decoder_attention_heads': args['decoder_attention_heads'], 'decoder_ffn_dim': args['decoder_ffn_embed_dim'], 'decoder_layerdrop': args['decoder_layerdrop'], 'decoder_layers': args['decoder_layers'], 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'is_encoder_decoder': True, 'scale_embedding': not args['no_scale_embedding'], 'tie_word_embeddings': args['share_all_embeddings']}\n    model_conf['num_beams'] = 5\n    model_conf['early_stopping'] = False\n    if model_dir in best_score_hparams and 'length_penalty' in best_score_hparams[model_dir]:\n        model_conf['length_penalty'] = best_score_hparams[model_dir]['length_penalty']\n    else:\n        model_conf['length_penalty'] = 1.0\n    print(f'Generating {fsmt_model_config_file}')\n    with open(fsmt_model_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(model_conf, ensure_ascii=False, indent=json_indent))\n    fsmt_tokenizer_config_file = os.path.join(pytorch_dump_folder_path, TOKENIZER_CONFIG_FILE)\n    tokenizer_conf = {'langs': [src_lang, tgt_lang], 'model_max_length': 1024, 'do_lower_case': do_lower_case}\n    print(f'Generating {fsmt_tokenizer_config_file}')\n    with open(fsmt_tokenizer_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tokenizer_conf, ensure_ascii=False, indent=json_indent))\n    model = chkpt['models'][0]\n    model_state_dict = model.state_dict()\n    model_state_dict = OrderedDict((('model.' + k, v) for (k, v) in model_state_dict.items()))\n    ignore_keys = ['model.model', 'model.encoder.version', 'model.decoder.version', 'model.encoder_embed_tokens.weight', 'model.decoder_embed_tokens.weight', 'model.encoder.embed_positions._float_tensor', 'model.decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        model_state_dict.pop(k, None)\n    config = FSMTConfig.from_pretrained(pytorch_dump_folder_path)\n    model_new = FSMTForConditionalGeneration(config)\n    model_new.load_state_dict(model_state_dict, strict=False)\n    pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)\n    print(f'Generating {pytorch_weights_dump_path}')\n    torch.save(model_state_dict, pytorch_weights_dump_path)\n    print('Conversion is done!')\n    print('\\nLast step is to upload the files to s3')\n    print(f'cd {data_root}')\n    print(f'transformers-cli upload {model_dir}')",
            "def convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert os.path.exists(fsmt_checkpoint_path)\n    os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n    print(f'Writing results to {pytorch_dump_folder_path}')\n    checkpoint_file = basename(fsmt_checkpoint_path)\n    fsmt_folder_path = dirname(fsmt_checkpoint_path)\n    cls = fairseq.model_parallel.models.transformer.ModelParallelTransformerModel\n    models = cls.hub_models()\n    kwargs = {'bpe': 'fastbpe', 'tokenizer': 'moses'}\n    data_name_or_path = '.'\n    print(f'using checkpoint {checkpoint_file}')\n    chkpt = hub_utils.from_pretrained(fsmt_folder_path, checkpoint_file, data_name_or_path, archive_map=models, **kwargs)\n    args = vars(chkpt['args']['model'])\n    src_lang = args['source_lang']\n    tgt_lang = args['target_lang']\n    data_root = dirname(pytorch_dump_folder_path)\n    model_dir = basename(pytorch_dump_folder_path)\n    src_dict_file = os.path.join(fsmt_folder_path, f'dict.{src_lang}.txt')\n    tgt_dict_file = os.path.join(fsmt_folder_path, f'dict.{tgt_lang}.txt')\n    src_dict = Dictionary.load(src_dict_file)\n    src_vocab = rewrite_dict_keys(src_dict.indices)\n    src_vocab_size = len(src_vocab)\n    src_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-src.json')\n    print(f'Generating {src_vocab_file} of {src_vocab_size} of {src_lang} records')\n    with open(src_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(src_vocab, ensure_ascii=False, indent=json_indent))\n    do_lower_case = True\n    for k in src_vocab.keys():\n        if not k.islower():\n            do_lower_case = False\n            break\n    tgt_dict = Dictionary.load(tgt_dict_file)\n    tgt_vocab = rewrite_dict_keys(tgt_dict.indices)\n    tgt_vocab_size = len(tgt_vocab)\n    tgt_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-tgt.json')\n    print(f'Generating {tgt_vocab_file} of {tgt_vocab_size} of {tgt_lang} records')\n    with open(tgt_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tgt_vocab, ensure_ascii=False, indent=json_indent))\n    merges_file = os.path.join(pytorch_dump_folder_path, VOCAB_FILES_NAMES['merges_file'])\n    for fn in ['bpecodes', 'code']:\n        fsmt_merges_file = os.path.join(fsmt_folder_path, fn)\n        if os.path.exists(fsmt_merges_file):\n            break\n    with open(fsmt_merges_file, encoding='utf-8') as fin:\n        merges = fin.read()\n    merges = re.sub(' \\\\d+$', '', merges, 0, re.M)\n    print(f'Generating {merges_file}')\n    with open(merges_file, 'w', encoding='utf-8') as fout:\n        fout.write(merges)\n    fsmt_model_config_file = os.path.join(pytorch_dump_folder_path, 'config.json')\n    assert args['bpe'] == 'fastbpe', f\"need to extend tokenizer to support bpe={args['bpe']}\"\n    assert args['tokenizer'] == 'moses', f\"need to extend tokenizer to support bpe={args['tokenizer']}\"\n    model_conf = {'architectures': ['FSMTForConditionalGeneration'], 'model_type': 'fsmt', 'activation_dropout': args['activation_dropout'], 'activation_function': 'relu', 'attention_dropout': args['attention_dropout'], 'd_model': args['decoder_embed_dim'], 'dropout': args['dropout'], 'init_std': 0.02, 'max_position_embeddings': args['max_source_positions'], 'num_hidden_layers': args['encoder_layers'], 'src_vocab_size': src_vocab_size, 'tgt_vocab_size': tgt_vocab_size, 'langs': [src_lang, tgt_lang], 'encoder_attention_heads': args['encoder_attention_heads'], 'encoder_ffn_dim': args['encoder_ffn_embed_dim'], 'encoder_layerdrop': args['encoder_layerdrop'], 'encoder_layers': args['encoder_layers'], 'decoder_attention_heads': args['decoder_attention_heads'], 'decoder_ffn_dim': args['decoder_ffn_embed_dim'], 'decoder_layerdrop': args['decoder_layerdrop'], 'decoder_layers': args['decoder_layers'], 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'is_encoder_decoder': True, 'scale_embedding': not args['no_scale_embedding'], 'tie_word_embeddings': args['share_all_embeddings']}\n    model_conf['num_beams'] = 5\n    model_conf['early_stopping'] = False\n    if model_dir in best_score_hparams and 'length_penalty' in best_score_hparams[model_dir]:\n        model_conf['length_penalty'] = best_score_hparams[model_dir]['length_penalty']\n    else:\n        model_conf['length_penalty'] = 1.0\n    print(f'Generating {fsmt_model_config_file}')\n    with open(fsmt_model_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(model_conf, ensure_ascii=False, indent=json_indent))\n    fsmt_tokenizer_config_file = os.path.join(pytorch_dump_folder_path, TOKENIZER_CONFIG_FILE)\n    tokenizer_conf = {'langs': [src_lang, tgt_lang], 'model_max_length': 1024, 'do_lower_case': do_lower_case}\n    print(f'Generating {fsmt_tokenizer_config_file}')\n    with open(fsmt_tokenizer_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tokenizer_conf, ensure_ascii=False, indent=json_indent))\n    model = chkpt['models'][0]\n    model_state_dict = model.state_dict()\n    model_state_dict = OrderedDict((('model.' + k, v) for (k, v) in model_state_dict.items()))\n    ignore_keys = ['model.model', 'model.encoder.version', 'model.decoder.version', 'model.encoder_embed_tokens.weight', 'model.decoder_embed_tokens.weight', 'model.encoder.embed_positions._float_tensor', 'model.decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        model_state_dict.pop(k, None)\n    config = FSMTConfig.from_pretrained(pytorch_dump_folder_path)\n    model_new = FSMTForConditionalGeneration(config)\n    model_new.load_state_dict(model_state_dict, strict=False)\n    pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)\n    print(f'Generating {pytorch_weights_dump_path}')\n    torch.save(model_state_dict, pytorch_weights_dump_path)\n    print('Conversion is done!')\n    print('\\nLast step is to upload the files to s3')\n    print(f'cd {data_root}')\n    print(f'transformers-cli upload {model_dir}')",
            "def convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert os.path.exists(fsmt_checkpoint_path)\n    os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n    print(f'Writing results to {pytorch_dump_folder_path}')\n    checkpoint_file = basename(fsmt_checkpoint_path)\n    fsmt_folder_path = dirname(fsmt_checkpoint_path)\n    cls = fairseq.model_parallel.models.transformer.ModelParallelTransformerModel\n    models = cls.hub_models()\n    kwargs = {'bpe': 'fastbpe', 'tokenizer': 'moses'}\n    data_name_or_path = '.'\n    print(f'using checkpoint {checkpoint_file}')\n    chkpt = hub_utils.from_pretrained(fsmt_folder_path, checkpoint_file, data_name_or_path, archive_map=models, **kwargs)\n    args = vars(chkpt['args']['model'])\n    src_lang = args['source_lang']\n    tgt_lang = args['target_lang']\n    data_root = dirname(pytorch_dump_folder_path)\n    model_dir = basename(pytorch_dump_folder_path)\n    src_dict_file = os.path.join(fsmt_folder_path, f'dict.{src_lang}.txt')\n    tgt_dict_file = os.path.join(fsmt_folder_path, f'dict.{tgt_lang}.txt')\n    src_dict = Dictionary.load(src_dict_file)\n    src_vocab = rewrite_dict_keys(src_dict.indices)\n    src_vocab_size = len(src_vocab)\n    src_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-src.json')\n    print(f'Generating {src_vocab_file} of {src_vocab_size} of {src_lang} records')\n    with open(src_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(src_vocab, ensure_ascii=False, indent=json_indent))\n    do_lower_case = True\n    for k in src_vocab.keys():\n        if not k.islower():\n            do_lower_case = False\n            break\n    tgt_dict = Dictionary.load(tgt_dict_file)\n    tgt_vocab = rewrite_dict_keys(tgt_dict.indices)\n    tgt_vocab_size = len(tgt_vocab)\n    tgt_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-tgt.json')\n    print(f'Generating {tgt_vocab_file} of {tgt_vocab_size} of {tgt_lang} records')\n    with open(tgt_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tgt_vocab, ensure_ascii=False, indent=json_indent))\n    merges_file = os.path.join(pytorch_dump_folder_path, VOCAB_FILES_NAMES['merges_file'])\n    for fn in ['bpecodes', 'code']:\n        fsmt_merges_file = os.path.join(fsmt_folder_path, fn)\n        if os.path.exists(fsmt_merges_file):\n            break\n    with open(fsmt_merges_file, encoding='utf-8') as fin:\n        merges = fin.read()\n    merges = re.sub(' \\\\d+$', '', merges, 0, re.M)\n    print(f'Generating {merges_file}')\n    with open(merges_file, 'w', encoding='utf-8') as fout:\n        fout.write(merges)\n    fsmt_model_config_file = os.path.join(pytorch_dump_folder_path, 'config.json')\n    assert args['bpe'] == 'fastbpe', f\"need to extend tokenizer to support bpe={args['bpe']}\"\n    assert args['tokenizer'] == 'moses', f\"need to extend tokenizer to support bpe={args['tokenizer']}\"\n    model_conf = {'architectures': ['FSMTForConditionalGeneration'], 'model_type': 'fsmt', 'activation_dropout': args['activation_dropout'], 'activation_function': 'relu', 'attention_dropout': args['attention_dropout'], 'd_model': args['decoder_embed_dim'], 'dropout': args['dropout'], 'init_std': 0.02, 'max_position_embeddings': args['max_source_positions'], 'num_hidden_layers': args['encoder_layers'], 'src_vocab_size': src_vocab_size, 'tgt_vocab_size': tgt_vocab_size, 'langs': [src_lang, tgt_lang], 'encoder_attention_heads': args['encoder_attention_heads'], 'encoder_ffn_dim': args['encoder_ffn_embed_dim'], 'encoder_layerdrop': args['encoder_layerdrop'], 'encoder_layers': args['encoder_layers'], 'decoder_attention_heads': args['decoder_attention_heads'], 'decoder_ffn_dim': args['decoder_ffn_embed_dim'], 'decoder_layerdrop': args['decoder_layerdrop'], 'decoder_layers': args['decoder_layers'], 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'is_encoder_decoder': True, 'scale_embedding': not args['no_scale_embedding'], 'tie_word_embeddings': args['share_all_embeddings']}\n    model_conf['num_beams'] = 5\n    model_conf['early_stopping'] = False\n    if model_dir in best_score_hparams and 'length_penalty' in best_score_hparams[model_dir]:\n        model_conf['length_penalty'] = best_score_hparams[model_dir]['length_penalty']\n    else:\n        model_conf['length_penalty'] = 1.0\n    print(f'Generating {fsmt_model_config_file}')\n    with open(fsmt_model_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(model_conf, ensure_ascii=False, indent=json_indent))\n    fsmt_tokenizer_config_file = os.path.join(pytorch_dump_folder_path, TOKENIZER_CONFIG_FILE)\n    tokenizer_conf = {'langs': [src_lang, tgt_lang], 'model_max_length': 1024, 'do_lower_case': do_lower_case}\n    print(f'Generating {fsmt_tokenizer_config_file}')\n    with open(fsmt_tokenizer_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tokenizer_conf, ensure_ascii=False, indent=json_indent))\n    model = chkpt['models'][0]\n    model_state_dict = model.state_dict()\n    model_state_dict = OrderedDict((('model.' + k, v) for (k, v) in model_state_dict.items()))\n    ignore_keys = ['model.model', 'model.encoder.version', 'model.decoder.version', 'model.encoder_embed_tokens.weight', 'model.decoder_embed_tokens.weight', 'model.encoder.embed_positions._float_tensor', 'model.decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        model_state_dict.pop(k, None)\n    config = FSMTConfig.from_pretrained(pytorch_dump_folder_path)\n    model_new = FSMTForConditionalGeneration(config)\n    model_new.load_state_dict(model_state_dict, strict=False)\n    pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)\n    print(f'Generating {pytorch_weights_dump_path}')\n    torch.save(model_state_dict, pytorch_weights_dump_path)\n    print('Conversion is done!')\n    print('\\nLast step is to upload the files to s3')\n    print(f'cd {data_root}')\n    print(f'transformers-cli upload {model_dir}')",
            "def convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert os.path.exists(fsmt_checkpoint_path)\n    os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n    print(f'Writing results to {pytorch_dump_folder_path}')\n    checkpoint_file = basename(fsmt_checkpoint_path)\n    fsmt_folder_path = dirname(fsmt_checkpoint_path)\n    cls = fairseq.model_parallel.models.transformer.ModelParallelTransformerModel\n    models = cls.hub_models()\n    kwargs = {'bpe': 'fastbpe', 'tokenizer': 'moses'}\n    data_name_or_path = '.'\n    print(f'using checkpoint {checkpoint_file}')\n    chkpt = hub_utils.from_pretrained(fsmt_folder_path, checkpoint_file, data_name_or_path, archive_map=models, **kwargs)\n    args = vars(chkpt['args']['model'])\n    src_lang = args['source_lang']\n    tgt_lang = args['target_lang']\n    data_root = dirname(pytorch_dump_folder_path)\n    model_dir = basename(pytorch_dump_folder_path)\n    src_dict_file = os.path.join(fsmt_folder_path, f'dict.{src_lang}.txt')\n    tgt_dict_file = os.path.join(fsmt_folder_path, f'dict.{tgt_lang}.txt')\n    src_dict = Dictionary.load(src_dict_file)\n    src_vocab = rewrite_dict_keys(src_dict.indices)\n    src_vocab_size = len(src_vocab)\n    src_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-src.json')\n    print(f'Generating {src_vocab_file} of {src_vocab_size} of {src_lang} records')\n    with open(src_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(src_vocab, ensure_ascii=False, indent=json_indent))\n    do_lower_case = True\n    for k in src_vocab.keys():\n        if not k.islower():\n            do_lower_case = False\n            break\n    tgt_dict = Dictionary.load(tgt_dict_file)\n    tgt_vocab = rewrite_dict_keys(tgt_dict.indices)\n    tgt_vocab_size = len(tgt_vocab)\n    tgt_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-tgt.json')\n    print(f'Generating {tgt_vocab_file} of {tgt_vocab_size} of {tgt_lang} records')\n    with open(tgt_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tgt_vocab, ensure_ascii=False, indent=json_indent))\n    merges_file = os.path.join(pytorch_dump_folder_path, VOCAB_FILES_NAMES['merges_file'])\n    for fn in ['bpecodes', 'code']:\n        fsmt_merges_file = os.path.join(fsmt_folder_path, fn)\n        if os.path.exists(fsmt_merges_file):\n            break\n    with open(fsmt_merges_file, encoding='utf-8') as fin:\n        merges = fin.read()\n    merges = re.sub(' \\\\d+$', '', merges, 0, re.M)\n    print(f'Generating {merges_file}')\n    with open(merges_file, 'w', encoding='utf-8') as fout:\n        fout.write(merges)\n    fsmt_model_config_file = os.path.join(pytorch_dump_folder_path, 'config.json')\n    assert args['bpe'] == 'fastbpe', f\"need to extend tokenizer to support bpe={args['bpe']}\"\n    assert args['tokenizer'] == 'moses', f\"need to extend tokenizer to support bpe={args['tokenizer']}\"\n    model_conf = {'architectures': ['FSMTForConditionalGeneration'], 'model_type': 'fsmt', 'activation_dropout': args['activation_dropout'], 'activation_function': 'relu', 'attention_dropout': args['attention_dropout'], 'd_model': args['decoder_embed_dim'], 'dropout': args['dropout'], 'init_std': 0.02, 'max_position_embeddings': args['max_source_positions'], 'num_hidden_layers': args['encoder_layers'], 'src_vocab_size': src_vocab_size, 'tgt_vocab_size': tgt_vocab_size, 'langs': [src_lang, tgt_lang], 'encoder_attention_heads': args['encoder_attention_heads'], 'encoder_ffn_dim': args['encoder_ffn_embed_dim'], 'encoder_layerdrop': args['encoder_layerdrop'], 'encoder_layers': args['encoder_layers'], 'decoder_attention_heads': args['decoder_attention_heads'], 'decoder_ffn_dim': args['decoder_ffn_embed_dim'], 'decoder_layerdrop': args['decoder_layerdrop'], 'decoder_layers': args['decoder_layers'], 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'is_encoder_decoder': True, 'scale_embedding': not args['no_scale_embedding'], 'tie_word_embeddings': args['share_all_embeddings']}\n    model_conf['num_beams'] = 5\n    model_conf['early_stopping'] = False\n    if model_dir in best_score_hparams and 'length_penalty' in best_score_hparams[model_dir]:\n        model_conf['length_penalty'] = best_score_hparams[model_dir]['length_penalty']\n    else:\n        model_conf['length_penalty'] = 1.0\n    print(f'Generating {fsmt_model_config_file}')\n    with open(fsmt_model_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(model_conf, ensure_ascii=False, indent=json_indent))\n    fsmt_tokenizer_config_file = os.path.join(pytorch_dump_folder_path, TOKENIZER_CONFIG_FILE)\n    tokenizer_conf = {'langs': [src_lang, tgt_lang], 'model_max_length': 1024, 'do_lower_case': do_lower_case}\n    print(f'Generating {fsmt_tokenizer_config_file}')\n    with open(fsmt_tokenizer_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tokenizer_conf, ensure_ascii=False, indent=json_indent))\n    model = chkpt['models'][0]\n    model_state_dict = model.state_dict()\n    model_state_dict = OrderedDict((('model.' + k, v) for (k, v) in model_state_dict.items()))\n    ignore_keys = ['model.model', 'model.encoder.version', 'model.decoder.version', 'model.encoder_embed_tokens.weight', 'model.decoder_embed_tokens.weight', 'model.encoder.embed_positions._float_tensor', 'model.decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        model_state_dict.pop(k, None)\n    config = FSMTConfig.from_pretrained(pytorch_dump_folder_path)\n    model_new = FSMTForConditionalGeneration(config)\n    model_new.load_state_dict(model_state_dict, strict=False)\n    pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)\n    print(f'Generating {pytorch_weights_dump_path}')\n    torch.save(model_state_dict, pytorch_weights_dump_path)\n    print('Conversion is done!')\n    print('\\nLast step is to upload the files to s3')\n    print(f'cd {data_root}')\n    print(f'transformers-cli upload {model_dir}')",
            "def convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path, pytorch_dump_folder_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert os.path.exists(fsmt_checkpoint_path)\n    os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n    print(f'Writing results to {pytorch_dump_folder_path}')\n    checkpoint_file = basename(fsmt_checkpoint_path)\n    fsmt_folder_path = dirname(fsmt_checkpoint_path)\n    cls = fairseq.model_parallel.models.transformer.ModelParallelTransformerModel\n    models = cls.hub_models()\n    kwargs = {'bpe': 'fastbpe', 'tokenizer': 'moses'}\n    data_name_or_path = '.'\n    print(f'using checkpoint {checkpoint_file}')\n    chkpt = hub_utils.from_pretrained(fsmt_folder_path, checkpoint_file, data_name_or_path, archive_map=models, **kwargs)\n    args = vars(chkpt['args']['model'])\n    src_lang = args['source_lang']\n    tgt_lang = args['target_lang']\n    data_root = dirname(pytorch_dump_folder_path)\n    model_dir = basename(pytorch_dump_folder_path)\n    src_dict_file = os.path.join(fsmt_folder_path, f'dict.{src_lang}.txt')\n    tgt_dict_file = os.path.join(fsmt_folder_path, f'dict.{tgt_lang}.txt')\n    src_dict = Dictionary.load(src_dict_file)\n    src_vocab = rewrite_dict_keys(src_dict.indices)\n    src_vocab_size = len(src_vocab)\n    src_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-src.json')\n    print(f'Generating {src_vocab_file} of {src_vocab_size} of {src_lang} records')\n    with open(src_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(src_vocab, ensure_ascii=False, indent=json_indent))\n    do_lower_case = True\n    for k in src_vocab.keys():\n        if not k.islower():\n            do_lower_case = False\n            break\n    tgt_dict = Dictionary.load(tgt_dict_file)\n    tgt_vocab = rewrite_dict_keys(tgt_dict.indices)\n    tgt_vocab_size = len(tgt_vocab)\n    tgt_vocab_file = os.path.join(pytorch_dump_folder_path, 'vocab-tgt.json')\n    print(f'Generating {tgt_vocab_file} of {tgt_vocab_size} of {tgt_lang} records')\n    with open(tgt_vocab_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tgt_vocab, ensure_ascii=False, indent=json_indent))\n    merges_file = os.path.join(pytorch_dump_folder_path, VOCAB_FILES_NAMES['merges_file'])\n    for fn in ['bpecodes', 'code']:\n        fsmt_merges_file = os.path.join(fsmt_folder_path, fn)\n        if os.path.exists(fsmt_merges_file):\n            break\n    with open(fsmt_merges_file, encoding='utf-8') as fin:\n        merges = fin.read()\n    merges = re.sub(' \\\\d+$', '', merges, 0, re.M)\n    print(f'Generating {merges_file}')\n    with open(merges_file, 'w', encoding='utf-8') as fout:\n        fout.write(merges)\n    fsmt_model_config_file = os.path.join(pytorch_dump_folder_path, 'config.json')\n    assert args['bpe'] == 'fastbpe', f\"need to extend tokenizer to support bpe={args['bpe']}\"\n    assert args['tokenizer'] == 'moses', f\"need to extend tokenizer to support bpe={args['tokenizer']}\"\n    model_conf = {'architectures': ['FSMTForConditionalGeneration'], 'model_type': 'fsmt', 'activation_dropout': args['activation_dropout'], 'activation_function': 'relu', 'attention_dropout': args['attention_dropout'], 'd_model': args['decoder_embed_dim'], 'dropout': args['dropout'], 'init_std': 0.02, 'max_position_embeddings': args['max_source_positions'], 'num_hidden_layers': args['encoder_layers'], 'src_vocab_size': src_vocab_size, 'tgt_vocab_size': tgt_vocab_size, 'langs': [src_lang, tgt_lang], 'encoder_attention_heads': args['encoder_attention_heads'], 'encoder_ffn_dim': args['encoder_ffn_embed_dim'], 'encoder_layerdrop': args['encoder_layerdrop'], 'encoder_layers': args['encoder_layers'], 'decoder_attention_heads': args['decoder_attention_heads'], 'decoder_ffn_dim': args['decoder_ffn_embed_dim'], 'decoder_layerdrop': args['decoder_layerdrop'], 'decoder_layers': args['decoder_layers'], 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'is_encoder_decoder': True, 'scale_embedding': not args['no_scale_embedding'], 'tie_word_embeddings': args['share_all_embeddings']}\n    model_conf['num_beams'] = 5\n    model_conf['early_stopping'] = False\n    if model_dir in best_score_hparams and 'length_penalty' in best_score_hparams[model_dir]:\n        model_conf['length_penalty'] = best_score_hparams[model_dir]['length_penalty']\n    else:\n        model_conf['length_penalty'] = 1.0\n    print(f'Generating {fsmt_model_config_file}')\n    with open(fsmt_model_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(model_conf, ensure_ascii=False, indent=json_indent))\n    fsmt_tokenizer_config_file = os.path.join(pytorch_dump_folder_path, TOKENIZER_CONFIG_FILE)\n    tokenizer_conf = {'langs': [src_lang, tgt_lang], 'model_max_length': 1024, 'do_lower_case': do_lower_case}\n    print(f'Generating {fsmt_tokenizer_config_file}')\n    with open(fsmt_tokenizer_config_file, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(tokenizer_conf, ensure_ascii=False, indent=json_indent))\n    model = chkpt['models'][0]\n    model_state_dict = model.state_dict()\n    model_state_dict = OrderedDict((('model.' + k, v) for (k, v) in model_state_dict.items()))\n    ignore_keys = ['model.model', 'model.encoder.version', 'model.decoder.version', 'model.encoder_embed_tokens.weight', 'model.decoder_embed_tokens.weight', 'model.encoder.embed_positions._float_tensor', 'model.decoder.embed_positions._float_tensor']\n    for k in ignore_keys:\n        model_state_dict.pop(k, None)\n    config = FSMTConfig.from_pretrained(pytorch_dump_folder_path)\n    model_new = FSMTForConditionalGeneration(config)\n    model_new.load_state_dict(model_state_dict, strict=False)\n    pytorch_weights_dump_path = os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)\n    print(f'Generating {pytorch_weights_dump_path}')\n    torch.save(model_state_dict, pytorch_weights_dump_path)\n    print('Conversion is done!')\n    print('\\nLast step is to upload the files to s3')\n    print(f'cd {data_root}')\n    print(f'transformers-cli upload {model_dir}')"
        ]
    }
]