[
    {
        "func_name": "load",
        "original": "@dygraph_only\ndef load(path, **configs):\n    \"\"\"\n    Load an object can be used in paddle from specified path.\n    The file is saved by distributed.save\n\n    Note:\n        The file to load must be saved bu the API paddle.incubate.distributed.utils.io.save\n\n    Args:\n        path(str|BytesIO) : The path/buffer to load the target object. Generally, the path is the target\n            file path. When loading state_dict from the saved result of the API used to save\n            the inference model, the path may be a file prefix or directory.\n        **configs (dict, optional): other load configuration options for compatibility. We do not\n            recommend using these configurations, they may be removed in the future. If not necessary,\n            DO NOT use them. Default None.\n            The following options are currently supported:\n                (1) place: where to place the loaded state dict.\n                     If the state dict is too large, the palce should be set 'cpu'.\n            Note:\n                Other config value may cause some error.Please don't use any more config options.\n    Returns:\n        Object(Object): a target object can be used in paddle\n\n    Examples:\n        import paddle\n        paddle.distributed.init_process_group(backend='nccl')\n        paddle.distributed.fleet.init(is_collective=True)\n\n        model = build_model()\n        optimizer = build_optimizer(model)\n\n        dist_model = paddle.distributed_optimizer(model)\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\n\n\n        # load model state dict\n        model_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdparams\")\n        dist_model.set_state_dict(model_state_dict)\n\n        # load optimizer satte dict\n        optimizer_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdopt\")\n        dist_optimizer.set_state_dict(optimizer_state_dict)\n\n    \"\"\"\n    if dist.get_world_size() == 1:\n        return paddle.load(path, **configs)\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, 'Sharding and DP are supported only now'\n    if 'place' not in configs:\n        configs['place'] = 'cpu'\n    place = configs['place']\n    assert isinstance(place, str), f'configs[place] must be a str, but this is a {type(place)}'\n    assert re.search('^(cpu|gpu:[0-9]*)$', place), 'configs[place] must be cpu, gpu:0, gpu:1 ...'\n    return load_with_place(path, **configs)",
        "mutated": [
            "@dygraph_only\ndef load(path, **configs):\n    if False:\n        i = 10\n    '\\n    Load an object can be used in paddle from specified path.\\n    The file is saved by distributed.save\\n\\n    Note:\\n        The file to load must be saved bu the API paddle.incubate.distributed.utils.io.save\\n\\n    Args:\\n        path(str|BytesIO) : The path/buffer to load the target object. Generally, the path is the target\\n            file path. When loading state_dict from the saved result of the API used to save\\n            the inference model, the path may be a file prefix or directory.\\n        **configs (dict, optional): other load configuration options for compatibility. We do not\\n            recommend using these configurations, they may be removed in the future. If not necessary,\\n            DO NOT use them. Default None.\\n            The following options are currently supported:\\n                (1) place: where to place the loaded state dict.\\n                     If the state dict is too large, the palce should be set \\'cpu\\'.\\n            Note:\\n                Other config value may cause some error.Please don\\'t use any more config options.\\n    Returns:\\n        Object(Object): a target object can be used in paddle\\n\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_model = paddle.distributed_optimizer(model)\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n\\n\\n        # load model state dict\\n        model_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdparams\")\\n        dist_model.set_state_dict(model_state_dict)\\n\\n        # load optimizer satte dict\\n        optimizer_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdopt\")\\n        dist_optimizer.set_state_dict(optimizer_state_dict)\\n\\n    '\n    if dist.get_world_size() == 1:\n        return paddle.load(path, **configs)\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, 'Sharding and DP are supported only now'\n    if 'place' not in configs:\n        configs['place'] = 'cpu'\n    place = configs['place']\n    assert isinstance(place, str), f'configs[place] must be a str, but this is a {type(place)}'\n    assert re.search('^(cpu|gpu:[0-9]*)$', place), 'configs[place] must be cpu, gpu:0, gpu:1 ...'\n    return load_with_place(path, **configs)",
            "@dygraph_only\ndef load(path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Load an object can be used in paddle from specified path.\\n    The file is saved by distributed.save\\n\\n    Note:\\n        The file to load must be saved bu the API paddle.incubate.distributed.utils.io.save\\n\\n    Args:\\n        path(str|BytesIO) : The path/buffer to load the target object. Generally, the path is the target\\n            file path. When loading state_dict from the saved result of the API used to save\\n            the inference model, the path may be a file prefix or directory.\\n        **configs (dict, optional): other load configuration options for compatibility. We do not\\n            recommend using these configurations, they may be removed in the future. If not necessary,\\n            DO NOT use them. Default None.\\n            The following options are currently supported:\\n                (1) place: where to place the loaded state dict.\\n                     If the state dict is too large, the palce should be set \\'cpu\\'.\\n            Note:\\n                Other config value may cause some error.Please don\\'t use any more config options.\\n    Returns:\\n        Object(Object): a target object can be used in paddle\\n\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_model = paddle.distributed_optimizer(model)\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n\\n\\n        # load model state dict\\n        model_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdparams\")\\n        dist_model.set_state_dict(model_state_dict)\\n\\n        # load optimizer satte dict\\n        optimizer_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdopt\")\\n        dist_optimizer.set_state_dict(optimizer_state_dict)\\n\\n    '\n    if dist.get_world_size() == 1:\n        return paddle.load(path, **configs)\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, 'Sharding and DP are supported only now'\n    if 'place' not in configs:\n        configs['place'] = 'cpu'\n    place = configs['place']\n    assert isinstance(place, str), f'configs[place] must be a str, but this is a {type(place)}'\n    assert re.search('^(cpu|gpu:[0-9]*)$', place), 'configs[place] must be cpu, gpu:0, gpu:1 ...'\n    return load_with_place(path, **configs)",
            "@dygraph_only\ndef load(path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Load an object can be used in paddle from specified path.\\n    The file is saved by distributed.save\\n\\n    Note:\\n        The file to load must be saved bu the API paddle.incubate.distributed.utils.io.save\\n\\n    Args:\\n        path(str|BytesIO) : The path/buffer to load the target object. Generally, the path is the target\\n            file path. When loading state_dict from the saved result of the API used to save\\n            the inference model, the path may be a file prefix or directory.\\n        **configs (dict, optional): other load configuration options for compatibility. We do not\\n            recommend using these configurations, they may be removed in the future. If not necessary,\\n            DO NOT use them. Default None.\\n            The following options are currently supported:\\n                (1) place: where to place the loaded state dict.\\n                     If the state dict is too large, the palce should be set \\'cpu\\'.\\n            Note:\\n                Other config value may cause some error.Please don\\'t use any more config options.\\n    Returns:\\n        Object(Object): a target object can be used in paddle\\n\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_model = paddle.distributed_optimizer(model)\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n\\n\\n        # load model state dict\\n        model_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdparams\")\\n        dist_model.set_state_dict(model_state_dict)\\n\\n        # load optimizer satte dict\\n        optimizer_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdopt\")\\n        dist_optimizer.set_state_dict(optimizer_state_dict)\\n\\n    '\n    if dist.get_world_size() == 1:\n        return paddle.load(path, **configs)\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, 'Sharding and DP are supported only now'\n    if 'place' not in configs:\n        configs['place'] = 'cpu'\n    place = configs['place']\n    assert isinstance(place, str), f'configs[place] must be a str, but this is a {type(place)}'\n    assert re.search('^(cpu|gpu:[0-9]*)$', place), 'configs[place] must be cpu, gpu:0, gpu:1 ...'\n    return load_with_place(path, **configs)",
            "@dygraph_only\ndef load(path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Load an object can be used in paddle from specified path.\\n    The file is saved by distributed.save\\n\\n    Note:\\n        The file to load must be saved bu the API paddle.incubate.distributed.utils.io.save\\n\\n    Args:\\n        path(str|BytesIO) : The path/buffer to load the target object. Generally, the path is the target\\n            file path. When loading state_dict from the saved result of the API used to save\\n            the inference model, the path may be a file prefix or directory.\\n        **configs (dict, optional): other load configuration options for compatibility. We do not\\n            recommend using these configurations, they may be removed in the future. If not necessary,\\n            DO NOT use them. Default None.\\n            The following options are currently supported:\\n                (1) place: where to place the loaded state dict.\\n                     If the state dict is too large, the palce should be set \\'cpu\\'.\\n            Note:\\n                Other config value may cause some error.Please don\\'t use any more config options.\\n    Returns:\\n        Object(Object): a target object can be used in paddle\\n\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_model = paddle.distributed_optimizer(model)\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n\\n\\n        # load model state dict\\n        model_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdparams\")\\n        dist_model.set_state_dict(model_state_dict)\\n\\n        # load optimizer satte dict\\n        optimizer_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdopt\")\\n        dist_optimizer.set_state_dict(optimizer_state_dict)\\n\\n    '\n    if dist.get_world_size() == 1:\n        return paddle.load(path, **configs)\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, 'Sharding and DP are supported only now'\n    if 'place' not in configs:\n        configs['place'] = 'cpu'\n    place = configs['place']\n    assert isinstance(place, str), f'configs[place] must be a str, but this is a {type(place)}'\n    assert re.search('^(cpu|gpu:[0-9]*)$', place), 'configs[place] must be cpu, gpu:0, gpu:1 ...'\n    return load_with_place(path, **configs)",
            "@dygraph_only\ndef load(path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Load an object can be used in paddle from specified path.\\n    The file is saved by distributed.save\\n\\n    Note:\\n        The file to load must be saved bu the API paddle.incubate.distributed.utils.io.save\\n\\n    Args:\\n        path(str|BytesIO) : The path/buffer to load the target object. Generally, the path is the target\\n            file path. When loading state_dict from the saved result of the API used to save\\n            the inference model, the path may be a file prefix or directory.\\n        **configs (dict, optional): other load configuration options for compatibility. We do not\\n            recommend using these configurations, they may be removed in the future. If not necessary,\\n            DO NOT use them. Default None.\\n            The following options are currently supported:\\n                (1) place: where to place the loaded state dict.\\n                     If the state dict is too large, the palce should be set \\'cpu\\'.\\n            Note:\\n                Other config value may cause some error.Please don\\'t use any more config options.\\n    Returns:\\n        Object(Object): a target object can be used in paddle\\n\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_model = paddle.distributed_optimizer(model)\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n\\n\\n        # load model state dict\\n        model_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdparams\")\\n        dist_model.set_state_dict(model_state_dict)\\n\\n        # load optimizer satte dict\\n        optimizer_state_dict = paddle.incubate.distributed.utils.io.load(path=\"path/to/load.pdopt\")\\n        dist_optimizer.set_state_dict(optimizer_state_dict)\\n\\n    '\n    if dist.get_world_size() == 1:\n        return paddle.load(path, **configs)\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, 'Sharding and DP are supported only now'\n    if 'place' not in configs:\n        configs['place'] = 'cpu'\n    place = configs['place']\n    assert isinstance(place, str), f'configs[place] must be a str, but this is a {type(place)}'\n    assert re.search('^(cpu|gpu:[0-9]*)$', place), 'configs[place] must be cpu, gpu:0, gpu:1 ...'\n    return load_with_place(path, **configs)"
        ]
    },
    {
        "func_name": "load_with_place",
        "original": "def load_with_place(path, **configs):\n    place = configs['place']\n    if place is None:\n        return paddle.load(path)\n    origin_place = paddle.get_device()\n    paddle.set_device(place)\n    configs = _remove_not_supported_itmes(configs)\n    state_dict = paddle.load(path, **configs)\n    paddle.set_device(origin_place)\n    return state_dict",
        "mutated": [
            "def load_with_place(path, **configs):\n    if False:\n        i = 10\n    place = configs['place']\n    if place is None:\n        return paddle.load(path)\n    origin_place = paddle.get_device()\n    paddle.set_device(place)\n    configs = _remove_not_supported_itmes(configs)\n    state_dict = paddle.load(path, **configs)\n    paddle.set_device(origin_place)\n    return state_dict",
            "def load_with_place(path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    place = configs['place']\n    if place is None:\n        return paddle.load(path)\n    origin_place = paddle.get_device()\n    paddle.set_device(place)\n    configs = _remove_not_supported_itmes(configs)\n    state_dict = paddle.load(path, **configs)\n    paddle.set_device(origin_place)\n    return state_dict",
            "def load_with_place(path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    place = configs['place']\n    if place is None:\n        return paddle.load(path)\n    origin_place = paddle.get_device()\n    paddle.set_device(place)\n    configs = _remove_not_supported_itmes(configs)\n    state_dict = paddle.load(path, **configs)\n    paddle.set_device(origin_place)\n    return state_dict",
            "def load_with_place(path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    place = configs['place']\n    if place is None:\n        return paddle.load(path)\n    origin_place = paddle.get_device()\n    paddle.set_device(place)\n    configs = _remove_not_supported_itmes(configs)\n    state_dict = paddle.load(path, **configs)\n    paddle.set_device(origin_place)\n    return state_dict",
            "def load_with_place(path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    place = configs['place']\n    if place is None:\n        return paddle.load(path)\n    origin_place = paddle.get_device()\n    paddle.set_device(place)\n    configs = _remove_not_supported_itmes(configs)\n    state_dict = paddle.load(path, **configs)\n    paddle.set_device(origin_place)\n    return state_dict"
        ]
    },
    {
        "func_name": "_remove_not_supported_itmes",
        "original": "def _remove_not_supported_itmes(configs):\n    __supported_by_load__ = ['model_filename', 'params_filename', 'return_numpy']\n    _configs = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_load__:\n            _configs.pop(k, None)\n    return _configs",
        "mutated": [
            "def _remove_not_supported_itmes(configs):\n    if False:\n        i = 10\n    __supported_by_load__ = ['model_filename', 'params_filename', 'return_numpy']\n    _configs = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_load__:\n            _configs.pop(k, None)\n    return _configs",
            "def _remove_not_supported_itmes(configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    __supported_by_load__ = ['model_filename', 'params_filename', 'return_numpy']\n    _configs = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_load__:\n            _configs.pop(k, None)\n    return _configs",
            "def _remove_not_supported_itmes(configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    __supported_by_load__ = ['model_filename', 'params_filename', 'return_numpy']\n    _configs = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_load__:\n            _configs.pop(k, None)\n    return _configs",
            "def _remove_not_supported_itmes(configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    __supported_by_load__ = ['model_filename', 'params_filename', 'return_numpy']\n    _configs = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_load__:\n            _configs.pop(k, None)\n    return _configs",
            "def _remove_not_supported_itmes(configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    __supported_by_load__ = ['model_filename', 'params_filename', 'return_numpy']\n    _configs = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_load__:\n            _configs.pop(k, None)\n    return _configs"
        ]
    }
]