[
    {
        "func_name": "respond",
        "original": "def respond(interpreter):\n    \"\"\"\n    Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon\n    Responds until it decides not to run any more code or say anything else.\n    \"\"\"\n    last_unsupported_code = ''\n    while True:\n        system_message = interpreter.generate_system_message()\n        system_message = {'role': 'system', 'message': system_message}\n        messages_for_llm = interpreter.messages.copy()\n        messages_for_llm = [system_message] + messages_for_llm\n        for message in messages_for_llm:\n            if 'output' in message and message['output'] == '':\n                message['output'] = 'No output'\n        interpreter.messages.append({'role': 'assistant'})\n        try:\n            chunk_type = None\n            for chunk in interpreter._llm(messages_for_llm):\n                interpreter.messages[-1] = merge_deltas(interpreter.messages[-1], chunk)\n                for new_chunk_type in ['message', 'language', 'code']:\n                    if new_chunk_type in chunk and chunk_type != new_chunk_type:\n                        if chunk_type != None:\n                            yield {f'end_of_{chunk_type}': True}\n                        if new_chunk_type == 'language':\n                            new_chunk_type = 'code'\n                        chunk_type = new_chunk_type\n                        yield {f'start_of_{chunk_type}': True}\n                yield chunk\n            if chunk_type:\n                yield {f'end_of_{chunk_type}': True}\n        except litellm.exceptions.BudgetExceededError:\n            display_markdown_message(f'> Max budget exceeded\\n\\n                **Session spend:** ${litellm._current_cost}\\n                **Max budget:** ${interpreter.max_budget}\\n\\n                Press CTRL-C then run `interpreter --max_budget [higher USD amount]` to proceed.\\n            ')\n            break\n        except Exception as e:\n            if interpreter.local == False and 'auth' in str(e).lower() or 'api key' in str(e).lower():\n                output = traceback.format_exc()\n                raise Exception(f\"{output}\\n\\nThere might be an issue with your API key(s).\\n\\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here',\\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\\n\\n\")\n            elif interpreter.local:\n                raise Exception(str(e) + \"\\n\\nPlease make sure LM Studio's local server is running by following the steps above.\\n\\nIf LM Studio's local server is running, please try a language model with a different architecture.\\n\\n                    \")\n            else:\n                raise\n        if 'code' in interpreter.messages[-1]:\n            if interpreter.debug_mode:\n                print('Running code:', interpreter.messages[-1])\n            try:\n                code = interpreter.messages[-1]['code']\n                if interpreter.messages[-1]['language'] == 'python' and code.startswith('!'):\n                    code = code[1:]\n                    interpreter.messages[-1]['code'] = code\n                    interpreter.messages[-1]['language'] = 'shell'\n                language = interpreter.messages[-1]['language'].lower().strip()\n                if language in language_map:\n                    if language not in interpreter._code_interpreters:\n                        config = {'language': language, 'vision': interpreter.vision}\n                        interpreter._code_interpreters[language] = create_code_interpreter(config)\n                    code_interpreter = interpreter._code_interpreters[language]\n                else:\n                    output = f'Open Interpreter does not currently support `{language}`.'\n                    yield {'output': output}\n                    interpreter.messages[-1]['output'] = output\n                    if code != last_unsupported_code:\n                        last_unsupported_code = code\n                        continue\n                    else:\n                        break\n                try:\n                    yield {'executing': {'code': code, 'language': language}}\n                except GeneratorExit:\n                    break\n                interpreter.messages[-1]['output'] = ''\n                for line in code_interpreter.run(code):\n                    yield line\n                    if 'output' in line:\n                        output = interpreter.messages[-1]['output']\n                        output += '\\n' + line['output']\n                        output = truncate_output(output, interpreter.max_output)\n                        interpreter.messages[-1]['output'] = output.strip()\n                    if interpreter.vision:\n                        base64_image = None\n                        if 'image' in line:\n                            base64_image = line['image']\n                        if 'html' in line:\n                            base64_image = html_to_base64(line['html'])\n                        if base64_image:\n                            yield {'output': 'Sending image output to GPT-4V...'}\n                            interpreter.messages[-1]['image'] = f'data:image/jpeg;base64,{base64_image}'\n            except:\n                output = traceback.format_exc()\n                yield {'output': output.strip()}\n                interpreter.messages[-1]['output'] = output.strip()\n            yield {'active_line': None}\n            yield {'end_of_execution': True}\n        else:\n            break\n    return",
        "mutated": [
            "def respond(interpreter):\n    if False:\n        i = 10\n    '\\n    Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon\\n    Responds until it decides not to run any more code or say anything else.\\n    '\n    last_unsupported_code = ''\n    while True:\n        system_message = interpreter.generate_system_message()\n        system_message = {'role': 'system', 'message': system_message}\n        messages_for_llm = interpreter.messages.copy()\n        messages_for_llm = [system_message] + messages_for_llm\n        for message in messages_for_llm:\n            if 'output' in message and message['output'] == '':\n                message['output'] = 'No output'\n        interpreter.messages.append({'role': 'assistant'})\n        try:\n            chunk_type = None\n            for chunk in interpreter._llm(messages_for_llm):\n                interpreter.messages[-1] = merge_deltas(interpreter.messages[-1], chunk)\n                for new_chunk_type in ['message', 'language', 'code']:\n                    if new_chunk_type in chunk and chunk_type != new_chunk_type:\n                        if chunk_type != None:\n                            yield {f'end_of_{chunk_type}': True}\n                        if new_chunk_type == 'language':\n                            new_chunk_type = 'code'\n                        chunk_type = new_chunk_type\n                        yield {f'start_of_{chunk_type}': True}\n                yield chunk\n            if chunk_type:\n                yield {f'end_of_{chunk_type}': True}\n        except litellm.exceptions.BudgetExceededError:\n            display_markdown_message(f'> Max budget exceeded\\n\\n                **Session spend:** ${litellm._current_cost}\\n                **Max budget:** ${interpreter.max_budget}\\n\\n                Press CTRL-C then run `interpreter --max_budget [higher USD amount]` to proceed.\\n            ')\n            break\n        except Exception as e:\n            if interpreter.local == False and 'auth' in str(e).lower() or 'api key' in str(e).lower():\n                output = traceback.format_exc()\n                raise Exception(f\"{output}\\n\\nThere might be an issue with your API key(s).\\n\\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here',\\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\\n\\n\")\n            elif interpreter.local:\n                raise Exception(str(e) + \"\\n\\nPlease make sure LM Studio's local server is running by following the steps above.\\n\\nIf LM Studio's local server is running, please try a language model with a different architecture.\\n\\n                    \")\n            else:\n                raise\n        if 'code' in interpreter.messages[-1]:\n            if interpreter.debug_mode:\n                print('Running code:', interpreter.messages[-1])\n            try:\n                code = interpreter.messages[-1]['code']\n                if interpreter.messages[-1]['language'] == 'python' and code.startswith('!'):\n                    code = code[1:]\n                    interpreter.messages[-1]['code'] = code\n                    interpreter.messages[-1]['language'] = 'shell'\n                language = interpreter.messages[-1]['language'].lower().strip()\n                if language in language_map:\n                    if language not in interpreter._code_interpreters:\n                        config = {'language': language, 'vision': interpreter.vision}\n                        interpreter._code_interpreters[language] = create_code_interpreter(config)\n                    code_interpreter = interpreter._code_interpreters[language]\n                else:\n                    output = f'Open Interpreter does not currently support `{language}`.'\n                    yield {'output': output}\n                    interpreter.messages[-1]['output'] = output\n                    if code != last_unsupported_code:\n                        last_unsupported_code = code\n                        continue\n                    else:\n                        break\n                try:\n                    yield {'executing': {'code': code, 'language': language}}\n                except GeneratorExit:\n                    break\n                interpreter.messages[-1]['output'] = ''\n                for line in code_interpreter.run(code):\n                    yield line\n                    if 'output' in line:\n                        output = interpreter.messages[-1]['output']\n                        output += '\\n' + line['output']\n                        output = truncate_output(output, interpreter.max_output)\n                        interpreter.messages[-1]['output'] = output.strip()\n                    if interpreter.vision:\n                        base64_image = None\n                        if 'image' in line:\n                            base64_image = line['image']\n                        if 'html' in line:\n                            base64_image = html_to_base64(line['html'])\n                        if base64_image:\n                            yield {'output': 'Sending image output to GPT-4V...'}\n                            interpreter.messages[-1]['image'] = f'data:image/jpeg;base64,{base64_image}'\n            except:\n                output = traceback.format_exc()\n                yield {'output': output.strip()}\n                interpreter.messages[-1]['output'] = output.strip()\n            yield {'active_line': None}\n            yield {'end_of_execution': True}\n        else:\n            break\n    return",
            "def respond(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon\\n    Responds until it decides not to run any more code or say anything else.\\n    '\n    last_unsupported_code = ''\n    while True:\n        system_message = interpreter.generate_system_message()\n        system_message = {'role': 'system', 'message': system_message}\n        messages_for_llm = interpreter.messages.copy()\n        messages_for_llm = [system_message] + messages_for_llm\n        for message in messages_for_llm:\n            if 'output' in message and message['output'] == '':\n                message['output'] = 'No output'\n        interpreter.messages.append({'role': 'assistant'})\n        try:\n            chunk_type = None\n            for chunk in interpreter._llm(messages_for_llm):\n                interpreter.messages[-1] = merge_deltas(interpreter.messages[-1], chunk)\n                for new_chunk_type in ['message', 'language', 'code']:\n                    if new_chunk_type in chunk and chunk_type != new_chunk_type:\n                        if chunk_type != None:\n                            yield {f'end_of_{chunk_type}': True}\n                        if new_chunk_type == 'language':\n                            new_chunk_type = 'code'\n                        chunk_type = new_chunk_type\n                        yield {f'start_of_{chunk_type}': True}\n                yield chunk\n            if chunk_type:\n                yield {f'end_of_{chunk_type}': True}\n        except litellm.exceptions.BudgetExceededError:\n            display_markdown_message(f'> Max budget exceeded\\n\\n                **Session spend:** ${litellm._current_cost}\\n                **Max budget:** ${interpreter.max_budget}\\n\\n                Press CTRL-C then run `interpreter --max_budget [higher USD amount]` to proceed.\\n            ')\n            break\n        except Exception as e:\n            if interpreter.local == False and 'auth' in str(e).lower() or 'api key' in str(e).lower():\n                output = traceback.format_exc()\n                raise Exception(f\"{output}\\n\\nThere might be an issue with your API key(s).\\n\\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here',\\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\\n\\n\")\n            elif interpreter.local:\n                raise Exception(str(e) + \"\\n\\nPlease make sure LM Studio's local server is running by following the steps above.\\n\\nIf LM Studio's local server is running, please try a language model with a different architecture.\\n\\n                    \")\n            else:\n                raise\n        if 'code' in interpreter.messages[-1]:\n            if interpreter.debug_mode:\n                print('Running code:', interpreter.messages[-1])\n            try:\n                code = interpreter.messages[-1]['code']\n                if interpreter.messages[-1]['language'] == 'python' and code.startswith('!'):\n                    code = code[1:]\n                    interpreter.messages[-1]['code'] = code\n                    interpreter.messages[-1]['language'] = 'shell'\n                language = interpreter.messages[-1]['language'].lower().strip()\n                if language in language_map:\n                    if language not in interpreter._code_interpreters:\n                        config = {'language': language, 'vision': interpreter.vision}\n                        interpreter._code_interpreters[language] = create_code_interpreter(config)\n                    code_interpreter = interpreter._code_interpreters[language]\n                else:\n                    output = f'Open Interpreter does not currently support `{language}`.'\n                    yield {'output': output}\n                    interpreter.messages[-1]['output'] = output\n                    if code != last_unsupported_code:\n                        last_unsupported_code = code\n                        continue\n                    else:\n                        break\n                try:\n                    yield {'executing': {'code': code, 'language': language}}\n                except GeneratorExit:\n                    break\n                interpreter.messages[-1]['output'] = ''\n                for line in code_interpreter.run(code):\n                    yield line\n                    if 'output' in line:\n                        output = interpreter.messages[-1]['output']\n                        output += '\\n' + line['output']\n                        output = truncate_output(output, interpreter.max_output)\n                        interpreter.messages[-1]['output'] = output.strip()\n                    if interpreter.vision:\n                        base64_image = None\n                        if 'image' in line:\n                            base64_image = line['image']\n                        if 'html' in line:\n                            base64_image = html_to_base64(line['html'])\n                        if base64_image:\n                            yield {'output': 'Sending image output to GPT-4V...'}\n                            interpreter.messages[-1]['image'] = f'data:image/jpeg;base64,{base64_image}'\n            except:\n                output = traceback.format_exc()\n                yield {'output': output.strip()}\n                interpreter.messages[-1]['output'] = output.strip()\n            yield {'active_line': None}\n            yield {'end_of_execution': True}\n        else:\n            break\n    return",
            "def respond(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon\\n    Responds until it decides not to run any more code or say anything else.\\n    '\n    last_unsupported_code = ''\n    while True:\n        system_message = interpreter.generate_system_message()\n        system_message = {'role': 'system', 'message': system_message}\n        messages_for_llm = interpreter.messages.copy()\n        messages_for_llm = [system_message] + messages_for_llm\n        for message in messages_for_llm:\n            if 'output' in message and message['output'] == '':\n                message['output'] = 'No output'\n        interpreter.messages.append({'role': 'assistant'})\n        try:\n            chunk_type = None\n            for chunk in interpreter._llm(messages_for_llm):\n                interpreter.messages[-1] = merge_deltas(interpreter.messages[-1], chunk)\n                for new_chunk_type in ['message', 'language', 'code']:\n                    if new_chunk_type in chunk and chunk_type != new_chunk_type:\n                        if chunk_type != None:\n                            yield {f'end_of_{chunk_type}': True}\n                        if new_chunk_type == 'language':\n                            new_chunk_type = 'code'\n                        chunk_type = new_chunk_type\n                        yield {f'start_of_{chunk_type}': True}\n                yield chunk\n            if chunk_type:\n                yield {f'end_of_{chunk_type}': True}\n        except litellm.exceptions.BudgetExceededError:\n            display_markdown_message(f'> Max budget exceeded\\n\\n                **Session spend:** ${litellm._current_cost}\\n                **Max budget:** ${interpreter.max_budget}\\n\\n                Press CTRL-C then run `interpreter --max_budget [higher USD amount]` to proceed.\\n            ')\n            break\n        except Exception as e:\n            if interpreter.local == False and 'auth' in str(e).lower() or 'api key' in str(e).lower():\n                output = traceback.format_exc()\n                raise Exception(f\"{output}\\n\\nThere might be an issue with your API key(s).\\n\\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here',\\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\\n\\n\")\n            elif interpreter.local:\n                raise Exception(str(e) + \"\\n\\nPlease make sure LM Studio's local server is running by following the steps above.\\n\\nIf LM Studio's local server is running, please try a language model with a different architecture.\\n\\n                    \")\n            else:\n                raise\n        if 'code' in interpreter.messages[-1]:\n            if interpreter.debug_mode:\n                print('Running code:', interpreter.messages[-1])\n            try:\n                code = interpreter.messages[-1]['code']\n                if interpreter.messages[-1]['language'] == 'python' and code.startswith('!'):\n                    code = code[1:]\n                    interpreter.messages[-1]['code'] = code\n                    interpreter.messages[-1]['language'] = 'shell'\n                language = interpreter.messages[-1]['language'].lower().strip()\n                if language in language_map:\n                    if language not in interpreter._code_interpreters:\n                        config = {'language': language, 'vision': interpreter.vision}\n                        interpreter._code_interpreters[language] = create_code_interpreter(config)\n                    code_interpreter = interpreter._code_interpreters[language]\n                else:\n                    output = f'Open Interpreter does not currently support `{language}`.'\n                    yield {'output': output}\n                    interpreter.messages[-1]['output'] = output\n                    if code != last_unsupported_code:\n                        last_unsupported_code = code\n                        continue\n                    else:\n                        break\n                try:\n                    yield {'executing': {'code': code, 'language': language}}\n                except GeneratorExit:\n                    break\n                interpreter.messages[-1]['output'] = ''\n                for line in code_interpreter.run(code):\n                    yield line\n                    if 'output' in line:\n                        output = interpreter.messages[-1]['output']\n                        output += '\\n' + line['output']\n                        output = truncate_output(output, interpreter.max_output)\n                        interpreter.messages[-1]['output'] = output.strip()\n                    if interpreter.vision:\n                        base64_image = None\n                        if 'image' in line:\n                            base64_image = line['image']\n                        if 'html' in line:\n                            base64_image = html_to_base64(line['html'])\n                        if base64_image:\n                            yield {'output': 'Sending image output to GPT-4V...'}\n                            interpreter.messages[-1]['image'] = f'data:image/jpeg;base64,{base64_image}'\n            except:\n                output = traceback.format_exc()\n                yield {'output': output.strip()}\n                interpreter.messages[-1]['output'] = output.strip()\n            yield {'active_line': None}\n            yield {'end_of_execution': True}\n        else:\n            break\n    return",
            "def respond(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon\\n    Responds until it decides not to run any more code or say anything else.\\n    '\n    last_unsupported_code = ''\n    while True:\n        system_message = interpreter.generate_system_message()\n        system_message = {'role': 'system', 'message': system_message}\n        messages_for_llm = interpreter.messages.copy()\n        messages_for_llm = [system_message] + messages_for_llm\n        for message in messages_for_llm:\n            if 'output' in message and message['output'] == '':\n                message['output'] = 'No output'\n        interpreter.messages.append({'role': 'assistant'})\n        try:\n            chunk_type = None\n            for chunk in interpreter._llm(messages_for_llm):\n                interpreter.messages[-1] = merge_deltas(interpreter.messages[-1], chunk)\n                for new_chunk_type in ['message', 'language', 'code']:\n                    if new_chunk_type in chunk and chunk_type != new_chunk_type:\n                        if chunk_type != None:\n                            yield {f'end_of_{chunk_type}': True}\n                        if new_chunk_type == 'language':\n                            new_chunk_type = 'code'\n                        chunk_type = new_chunk_type\n                        yield {f'start_of_{chunk_type}': True}\n                yield chunk\n            if chunk_type:\n                yield {f'end_of_{chunk_type}': True}\n        except litellm.exceptions.BudgetExceededError:\n            display_markdown_message(f'> Max budget exceeded\\n\\n                **Session spend:** ${litellm._current_cost}\\n                **Max budget:** ${interpreter.max_budget}\\n\\n                Press CTRL-C then run `interpreter --max_budget [higher USD amount]` to proceed.\\n            ')\n            break\n        except Exception as e:\n            if interpreter.local == False and 'auth' in str(e).lower() or 'api key' in str(e).lower():\n                output = traceback.format_exc()\n                raise Exception(f\"{output}\\n\\nThere might be an issue with your API key(s).\\n\\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here',\\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\\n\\n\")\n            elif interpreter.local:\n                raise Exception(str(e) + \"\\n\\nPlease make sure LM Studio's local server is running by following the steps above.\\n\\nIf LM Studio's local server is running, please try a language model with a different architecture.\\n\\n                    \")\n            else:\n                raise\n        if 'code' in interpreter.messages[-1]:\n            if interpreter.debug_mode:\n                print('Running code:', interpreter.messages[-1])\n            try:\n                code = interpreter.messages[-1]['code']\n                if interpreter.messages[-1]['language'] == 'python' and code.startswith('!'):\n                    code = code[1:]\n                    interpreter.messages[-1]['code'] = code\n                    interpreter.messages[-1]['language'] = 'shell'\n                language = interpreter.messages[-1]['language'].lower().strip()\n                if language in language_map:\n                    if language not in interpreter._code_interpreters:\n                        config = {'language': language, 'vision': interpreter.vision}\n                        interpreter._code_interpreters[language] = create_code_interpreter(config)\n                    code_interpreter = interpreter._code_interpreters[language]\n                else:\n                    output = f'Open Interpreter does not currently support `{language}`.'\n                    yield {'output': output}\n                    interpreter.messages[-1]['output'] = output\n                    if code != last_unsupported_code:\n                        last_unsupported_code = code\n                        continue\n                    else:\n                        break\n                try:\n                    yield {'executing': {'code': code, 'language': language}}\n                except GeneratorExit:\n                    break\n                interpreter.messages[-1]['output'] = ''\n                for line in code_interpreter.run(code):\n                    yield line\n                    if 'output' in line:\n                        output = interpreter.messages[-1]['output']\n                        output += '\\n' + line['output']\n                        output = truncate_output(output, interpreter.max_output)\n                        interpreter.messages[-1]['output'] = output.strip()\n                    if interpreter.vision:\n                        base64_image = None\n                        if 'image' in line:\n                            base64_image = line['image']\n                        if 'html' in line:\n                            base64_image = html_to_base64(line['html'])\n                        if base64_image:\n                            yield {'output': 'Sending image output to GPT-4V...'}\n                            interpreter.messages[-1]['image'] = f'data:image/jpeg;base64,{base64_image}'\n            except:\n                output = traceback.format_exc()\n                yield {'output': output.strip()}\n                interpreter.messages[-1]['output'] = output.strip()\n            yield {'active_line': None}\n            yield {'end_of_execution': True}\n        else:\n            break\n    return",
            "def respond(interpreter):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Yields tokens, but also adds them to interpreter.messages. TBH probably would be good to seperate those two responsibilities someday soon\\n    Responds until it decides not to run any more code or say anything else.\\n    '\n    last_unsupported_code = ''\n    while True:\n        system_message = interpreter.generate_system_message()\n        system_message = {'role': 'system', 'message': system_message}\n        messages_for_llm = interpreter.messages.copy()\n        messages_for_llm = [system_message] + messages_for_llm\n        for message in messages_for_llm:\n            if 'output' in message and message['output'] == '':\n                message['output'] = 'No output'\n        interpreter.messages.append({'role': 'assistant'})\n        try:\n            chunk_type = None\n            for chunk in interpreter._llm(messages_for_llm):\n                interpreter.messages[-1] = merge_deltas(interpreter.messages[-1], chunk)\n                for new_chunk_type in ['message', 'language', 'code']:\n                    if new_chunk_type in chunk and chunk_type != new_chunk_type:\n                        if chunk_type != None:\n                            yield {f'end_of_{chunk_type}': True}\n                        if new_chunk_type == 'language':\n                            new_chunk_type = 'code'\n                        chunk_type = new_chunk_type\n                        yield {f'start_of_{chunk_type}': True}\n                yield chunk\n            if chunk_type:\n                yield {f'end_of_{chunk_type}': True}\n        except litellm.exceptions.BudgetExceededError:\n            display_markdown_message(f'> Max budget exceeded\\n\\n                **Session spend:** ${litellm._current_cost}\\n                **Max budget:** ${interpreter.max_budget}\\n\\n                Press CTRL-C then run `interpreter --max_budget [higher USD amount]` to proceed.\\n            ')\n            break\n        except Exception as e:\n            if interpreter.local == False and 'auth' in str(e).lower() or 'api key' in str(e).lower():\n                output = traceback.format_exc()\n                raise Exception(f\"{output}\\n\\nThere might be an issue with your API key(s).\\n\\nTo reset your API key (we'll use OPENAI_API_KEY for this example, but you may need to reset your ANTHROPIC_API_KEY, HUGGINGFACE_API_KEY, etc):\\n        Mac/Linux: 'export OPENAI_API_KEY=your-key-here',\\n        Windows: 'setx OPENAI_API_KEY your-key-here' then restart terminal.\\n\\n\")\n            elif interpreter.local:\n                raise Exception(str(e) + \"\\n\\nPlease make sure LM Studio's local server is running by following the steps above.\\n\\nIf LM Studio's local server is running, please try a language model with a different architecture.\\n\\n                    \")\n            else:\n                raise\n        if 'code' in interpreter.messages[-1]:\n            if interpreter.debug_mode:\n                print('Running code:', interpreter.messages[-1])\n            try:\n                code = interpreter.messages[-1]['code']\n                if interpreter.messages[-1]['language'] == 'python' and code.startswith('!'):\n                    code = code[1:]\n                    interpreter.messages[-1]['code'] = code\n                    interpreter.messages[-1]['language'] = 'shell'\n                language = interpreter.messages[-1]['language'].lower().strip()\n                if language in language_map:\n                    if language not in interpreter._code_interpreters:\n                        config = {'language': language, 'vision': interpreter.vision}\n                        interpreter._code_interpreters[language] = create_code_interpreter(config)\n                    code_interpreter = interpreter._code_interpreters[language]\n                else:\n                    output = f'Open Interpreter does not currently support `{language}`.'\n                    yield {'output': output}\n                    interpreter.messages[-1]['output'] = output\n                    if code != last_unsupported_code:\n                        last_unsupported_code = code\n                        continue\n                    else:\n                        break\n                try:\n                    yield {'executing': {'code': code, 'language': language}}\n                except GeneratorExit:\n                    break\n                interpreter.messages[-1]['output'] = ''\n                for line in code_interpreter.run(code):\n                    yield line\n                    if 'output' in line:\n                        output = interpreter.messages[-1]['output']\n                        output += '\\n' + line['output']\n                        output = truncate_output(output, interpreter.max_output)\n                        interpreter.messages[-1]['output'] = output.strip()\n                    if interpreter.vision:\n                        base64_image = None\n                        if 'image' in line:\n                            base64_image = line['image']\n                        if 'html' in line:\n                            base64_image = html_to_base64(line['html'])\n                        if base64_image:\n                            yield {'output': 'Sending image output to GPT-4V...'}\n                            interpreter.messages[-1]['image'] = f'data:image/jpeg;base64,{base64_image}'\n            except:\n                output = traceback.format_exc()\n                yield {'output': output.strip()}\n                interpreter.messages[-1]['output'] = output.strip()\n            yield {'active_line': None}\n            yield {'end_of_execution': True}\n        else:\n            break\n    return"
        ]
    }
]