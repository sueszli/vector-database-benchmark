[
    {
        "func_name": "split_text",
        "original": "def split_text(text: str, n=100, character=' ') -> List[str]:\n    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n    text = text.split(character)\n    return [character.join(text[i:i + n]).strip() for i in range(0, len(text), n)]",
        "mutated": [
            "def split_text(text: str, n=100, character=' ') -> List[str]:\n    if False:\n        i = 10\n    'Split the text every ``n``-th occurrence of ``character``'\n    text = text.split(character)\n    return [character.join(text[i:i + n]).strip() for i in range(0, len(text), n)]",
            "def split_text(text: str, n=100, character=' ') -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Split the text every ``n``-th occurrence of ``character``'\n    text = text.split(character)\n    return [character.join(text[i:i + n]).strip() for i in range(0, len(text), n)]",
            "def split_text(text: str, n=100, character=' ') -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Split the text every ``n``-th occurrence of ``character``'\n    text = text.split(character)\n    return [character.join(text[i:i + n]).strip() for i in range(0, len(text), n)]",
            "def split_text(text: str, n=100, character=' ') -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Split the text every ``n``-th occurrence of ``character``'\n    text = text.split(character)\n    return [character.join(text[i:i + n]).strip() for i in range(0, len(text), n)]",
            "def split_text(text: str, n=100, character=' ') -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Split the text every ``n``-th occurrence of ``character``'\n    text = text.split(character)\n    return [character.join(text[i:i + n]).strip() for i in range(0, len(text), n)]"
        ]
    },
    {
        "func_name": "split_documents",
        "original": "def split_documents(documents: dict) -> dict:\n    \"\"\"Split documents into passages\"\"\"\n    (titles, texts) = ([], [])\n    for (title, text) in zip(documents['title'], documents['text']):\n        if text is not None:\n            for passage in split_text(text):\n                titles.append(title if title is not None else '')\n                texts.append(passage)\n    return {'title': titles, 'text': texts}",
        "mutated": [
            "def split_documents(documents: dict) -> dict:\n    if False:\n        i = 10\n    'Split documents into passages'\n    (titles, texts) = ([], [])\n    for (title, text) in zip(documents['title'], documents['text']):\n        if text is not None:\n            for passage in split_text(text):\n                titles.append(title if title is not None else '')\n                texts.append(passage)\n    return {'title': titles, 'text': texts}",
            "def split_documents(documents: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Split documents into passages'\n    (titles, texts) = ([], [])\n    for (title, text) in zip(documents['title'], documents['text']):\n        if text is not None:\n            for passage in split_text(text):\n                titles.append(title if title is not None else '')\n                texts.append(passage)\n    return {'title': titles, 'text': texts}",
            "def split_documents(documents: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Split documents into passages'\n    (titles, texts) = ([], [])\n    for (title, text) in zip(documents['title'], documents['text']):\n        if text is not None:\n            for passage in split_text(text):\n                titles.append(title if title is not None else '')\n                texts.append(passage)\n    return {'title': titles, 'text': texts}",
            "def split_documents(documents: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Split documents into passages'\n    (titles, texts) = ([], [])\n    for (title, text) in zip(documents['title'], documents['text']):\n        if text is not None:\n            for passage in split_text(text):\n                titles.append(title if title is not None else '')\n                texts.append(passage)\n    return {'title': titles, 'text': texts}",
            "def split_documents(documents: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Split documents into passages'\n    (titles, texts) = ([], [])\n    for (title, text) in zip(documents['title'], documents['text']):\n        if text is not None:\n            for passage in split_text(text):\n                titles.append(title if title is not None else '')\n                texts.append(passage)\n    return {'title': titles, 'text': texts}"
        ]
    },
    {
        "func_name": "embed",
        "original": "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n    \"\"\"Compute the DPR embeddings of document passages\"\"\"\n    input_ids = ctx_tokenizer(documents['title'], documents['text'], truncation=True, padding='longest', return_tensors='pt')['input_ids']\n    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n    return {'embeddings': embeddings.detach().cpu().numpy()}",
        "mutated": [
            "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n    if False:\n        i = 10\n    'Compute the DPR embeddings of document passages'\n    input_ids = ctx_tokenizer(documents['title'], documents['text'], truncation=True, padding='longest', return_tensors='pt')['input_ids']\n    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n    return {'embeddings': embeddings.detach().cpu().numpy()}",
            "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the DPR embeddings of document passages'\n    input_ids = ctx_tokenizer(documents['title'], documents['text'], truncation=True, padding='longest', return_tensors='pt')['input_ids']\n    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n    return {'embeddings': embeddings.detach().cpu().numpy()}",
            "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the DPR embeddings of document passages'\n    input_ids = ctx_tokenizer(documents['title'], documents['text'], truncation=True, padding='longest', return_tensors='pt')['input_ids']\n    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n    return {'embeddings': embeddings.detach().cpu().numpy()}",
            "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the DPR embeddings of document passages'\n    input_ids = ctx_tokenizer(documents['title'], documents['text'], truncation=True, padding='longest', return_tensors='pt')['input_ids']\n    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n    return {'embeddings': embeddings.detach().cpu().numpy()}",
            "def embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the DPR embeddings of document passages'\n    input_ids = ctx_tokenizer(documents['title'], documents['text'], truncation=True, padding='longest', return_tensors='pt')['input_ids']\n    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n    return {'embeddings': embeddings.detach().cpu().numpy()}"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(rag_example_args: 'RagExampleArguments', processing_args: 'ProcessingArguments', index_hnsw_args: 'IndexHnswArguments'):\n    logger.info('Step 1 - Create the dataset')\n    assert os.path.isfile(rag_example_args.csv_path), 'Please provide a valid path to a csv file'\n    dataset = load_dataset('csv', data_files=[rag_example_args.csv_path], split='train', delimiter='\\t', column_names=['title', 'text'])\n    dataset = dataset.map(split_documents, batched=True, num_proc=processing_args.num_proc)\n    ctx_encoder = DPRContextEncoder.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name).to(device=device)\n    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name)\n    new_features = Features({'text': Value('string'), 'title': Value('string'), 'embeddings': Sequence(Value('float32'))})\n    dataset = dataset.map(partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer), batched=True, batch_size=processing_args.batch_size, features=new_features)\n    passages_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset')\n    dataset.save_to_disk(passages_path)\n    logger.info('Step 2 - Index the dataset')\n    index = faiss.IndexHNSWFlat(index_hnsw_args.d, index_hnsw_args.m, faiss.METRIC_INNER_PRODUCT)\n    dataset.add_faiss_index('embeddings', custom_index=index)\n    index_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset_hnsw_index.faiss')\n    dataset.get_index('embeddings').save(index_path)\n    logger.info('Step 3 - Load RAG')\n    retriever = RagRetriever.from_pretrained(rag_example_args.rag_model_name, index_name='custom', indexed_dataset=dataset)\n    model = RagSequenceForGeneration.from_pretrained(rag_example_args.rag_model_name, retriever=retriever)\n    tokenizer = RagTokenizer.from_pretrained(rag_example_args.rag_model_name)\n    logger.info('Step 4 - Have fun')\n    question = rag_example_args.question or \"What does Moses' rod turn into ?\"\n    input_ids = tokenizer.question_encoder(question, return_tensors='pt')['input_ids']\n    generated = model.generate(input_ids)\n    generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n    logger.info('Q: ' + question)\n    logger.info('A: ' + generated_string)",
        "mutated": [
            "def main(rag_example_args: 'RagExampleArguments', processing_args: 'ProcessingArguments', index_hnsw_args: 'IndexHnswArguments'):\n    if False:\n        i = 10\n    logger.info('Step 1 - Create the dataset')\n    assert os.path.isfile(rag_example_args.csv_path), 'Please provide a valid path to a csv file'\n    dataset = load_dataset('csv', data_files=[rag_example_args.csv_path], split='train', delimiter='\\t', column_names=['title', 'text'])\n    dataset = dataset.map(split_documents, batched=True, num_proc=processing_args.num_proc)\n    ctx_encoder = DPRContextEncoder.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name).to(device=device)\n    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name)\n    new_features = Features({'text': Value('string'), 'title': Value('string'), 'embeddings': Sequence(Value('float32'))})\n    dataset = dataset.map(partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer), batched=True, batch_size=processing_args.batch_size, features=new_features)\n    passages_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset')\n    dataset.save_to_disk(passages_path)\n    logger.info('Step 2 - Index the dataset')\n    index = faiss.IndexHNSWFlat(index_hnsw_args.d, index_hnsw_args.m, faiss.METRIC_INNER_PRODUCT)\n    dataset.add_faiss_index('embeddings', custom_index=index)\n    index_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset_hnsw_index.faiss')\n    dataset.get_index('embeddings').save(index_path)\n    logger.info('Step 3 - Load RAG')\n    retriever = RagRetriever.from_pretrained(rag_example_args.rag_model_name, index_name='custom', indexed_dataset=dataset)\n    model = RagSequenceForGeneration.from_pretrained(rag_example_args.rag_model_name, retriever=retriever)\n    tokenizer = RagTokenizer.from_pretrained(rag_example_args.rag_model_name)\n    logger.info('Step 4 - Have fun')\n    question = rag_example_args.question or \"What does Moses' rod turn into ?\"\n    input_ids = tokenizer.question_encoder(question, return_tensors='pt')['input_ids']\n    generated = model.generate(input_ids)\n    generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n    logger.info('Q: ' + question)\n    logger.info('A: ' + generated_string)",
            "def main(rag_example_args: 'RagExampleArguments', processing_args: 'ProcessingArguments', index_hnsw_args: 'IndexHnswArguments'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    logger.info('Step 1 - Create the dataset')\n    assert os.path.isfile(rag_example_args.csv_path), 'Please provide a valid path to a csv file'\n    dataset = load_dataset('csv', data_files=[rag_example_args.csv_path], split='train', delimiter='\\t', column_names=['title', 'text'])\n    dataset = dataset.map(split_documents, batched=True, num_proc=processing_args.num_proc)\n    ctx_encoder = DPRContextEncoder.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name).to(device=device)\n    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name)\n    new_features = Features({'text': Value('string'), 'title': Value('string'), 'embeddings': Sequence(Value('float32'))})\n    dataset = dataset.map(partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer), batched=True, batch_size=processing_args.batch_size, features=new_features)\n    passages_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset')\n    dataset.save_to_disk(passages_path)\n    logger.info('Step 2 - Index the dataset')\n    index = faiss.IndexHNSWFlat(index_hnsw_args.d, index_hnsw_args.m, faiss.METRIC_INNER_PRODUCT)\n    dataset.add_faiss_index('embeddings', custom_index=index)\n    index_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset_hnsw_index.faiss')\n    dataset.get_index('embeddings').save(index_path)\n    logger.info('Step 3 - Load RAG')\n    retriever = RagRetriever.from_pretrained(rag_example_args.rag_model_name, index_name='custom', indexed_dataset=dataset)\n    model = RagSequenceForGeneration.from_pretrained(rag_example_args.rag_model_name, retriever=retriever)\n    tokenizer = RagTokenizer.from_pretrained(rag_example_args.rag_model_name)\n    logger.info('Step 4 - Have fun')\n    question = rag_example_args.question or \"What does Moses' rod turn into ?\"\n    input_ids = tokenizer.question_encoder(question, return_tensors='pt')['input_ids']\n    generated = model.generate(input_ids)\n    generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n    logger.info('Q: ' + question)\n    logger.info('A: ' + generated_string)",
            "def main(rag_example_args: 'RagExampleArguments', processing_args: 'ProcessingArguments', index_hnsw_args: 'IndexHnswArguments'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    logger.info('Step 1 - Create the dataset')\n    assert os.path.isfile(rag_example_args.csv_path), 'Please provide a valid path to a csv file'\n    dataset = load_dataset('csv', data_files=[rag_example_args.csv_path], split='train', delimiter='\\t', column_names=['title', 'text'])\n    dataset = dataset.map(split_documents, batched=True, num_proc=processing_args.num_proc)\n    ctx_encoder = DPRContextEncoder.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name).to(device=device)\n    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name)\n    new_features = Features({'text': Value('string'), 'title': Value('string'), 'embeddings': Sequence(Value('float32'))})\n    dataset = dataset.map(partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer), batched=True, batch_size=processing_args.batch_size, features=new_features)\n    passages_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset')\n    dataset.save_to_disk(passages_path)\n    logger.info('Step 2 - Index the dataset')\n    index = faiss.IndexHNSWFlat(index_hnsw_args.d, index_hnsw_args.m, faiss.METRIC_INNER_PRODUCT)\n    dataset.add_faiss_index('embeddings', custom_index=index)\n    index_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset_hnsw_index.faiss')\n    dataset.get_index('embeddings').save(index_path)\n    logger.info('Step 3 - Load RAG')\n    retriever = RagRetriever.from_pretrained(rag_example_args.rag_model_name, index_name='custom', indexed_dataset=dataset)\n    model = RagSequenceForGeneration.from_pretrained(rag_example_args.rag_model_name, retriever=retriever)\n    tokenizer = RagTokenizer.from_pretrained(rag_example_args.rag_model_name)\n    logger.info('Step 4 - Have fun')\n    question = rag_example_args.question or \"What does Moses' rod turn into ?\"\n    input_ids = tokenizer.question_encoder(question, return_tensors='pt')['input_ids']\n    generated = model.generate(input_ids)\n    generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n    logger.info('Q: ' + question)\n    logger.info('A: ' + generated_string)",
            "def main(rag_example_args: 'RagExampleArguments', processing_args: 'ProcessingArguments', index_hnsw_args: 'IndexHnswArguments'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    logger.info('Step 1 - Create the dataset')\n    assert os.path.isfile(rag_example_args.csv_path), 'Please provide a valid path to a csv file'\n    dataset = load_dataset('csv', data_files=[rag_example_args.csv_path], split='train', delimiter='\\t', column_names=['title', 'text'])\n    dataset = dataset.map(split_documents, batched=True, num_proc=processing_args.num_proc)\n    ctx_encoder = DPRContextEncoder.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name).to(device=device)\n    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name)\n    new_features = Features({'text': Value('string'), 'title': Value('string'), 'embeddings': Sequence(Value('float32'))})\n    dataset = dataset.map(partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer), batched=True, batch_size=processing_args.batch_size, features=new_features)\n    passages_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset')\n    dataset.save_to_disk(passages_path)\n    logger.info('Step 2 - Index the dataset')\n    index = faiss.IndexHNSWFlat(index_hnsw_args.d, index_hnsw_args.m, faiss.METRIC_INNER_PRODUCT)\n    dataset.add_faiss_index('embeddings', custom_index=index)\n    index_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset_hnsw_index.faiss')\n    dataset.get_index('embeddings').save(index_path)\n    logger.info('Step 3 - Load RAG')\n    retriever = RagRetriever.from_pretrained(rag_example_args.rag_model_name, index_name='custom', indexed_dataset=dataset)\n    model = RagSequenceForGeneration.from_pretrained(rag_example_args.rag_model_name, retriever=retriever)\n    tokenizer = RagTokenizer.from_pretrained(rag_example_args.rag_model_name)\n    logger.info('Step 4 - Have fun')\n    question = rag_example_args.question or \"What does Moses' rod turn into ?\"\n    input_ids = tokenizer.question_encoder(question, return_tensors='pt')['input_ids']\n    generated = model.generate(input_ids)\n    generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n    logger.info('Q: ' + question)\n    logger.info('A: ' + generated_string)",
            "def main(rag_example_args: 'RagExampleArguments', processing_args: 'ProcessingArguments', index_hnsw_args: 'IndexHnswArguments'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    logger.info('Step 1 - Create the dataset')\n    assert os.path.isfile(rag_example_args.csv_path), 'Please provide a valid path to a csv file'\n    dataset = load_dataset('csv', data_files=[rag_example_args.csv_path], split='train', delimiter='\\t', column_names=['title', 'text'])\n    dataset = dataset.map(split_documents, batched=True, num_proc=processing_args.num_proc)\n    ctx_encoder = DPRContextEncoder.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name).to(device=device)\n    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(rag_example_args.dpr_ctx_encoder_model_name)\n    new_features = Features({'text': Value('string'), 'title': Value('string'), 'embeddings': Sequence(Value('float32'))})\n    dataset = dataset.map(partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer), batched=True, batch_size=processing_args.batch_size, features=new_features)\n    passages_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset')\n    dataset.save_to_disk(passages_path)\n    logger.info('Step 2 - Index the dataset')\n    index = faiss.IndexHNSWFlat(index_hnsw_args.d, index_hnsw_args.m, faiss.METRIC_INNER_PRODUCT)\n    dataset.add_faiss_index('embeddings', custom_index=index)\n    index_path = os.path.join(rag_example_args.output_dir, 'my_knowledge_dataset_hnsw_index.faiss')\n    dataset.get_index('embeddings').save(index_path)\n    logger.info('Step 3 - Load RAG')\n    retriever = RagRetriever.from_pretrained(rag_example_args.rag_model_name, index_name='custom', indexed_dataset=dataset)\n    model = RagSequenceForGeneration.from_pretrained(rag_example_args.rag_model_name, retriever=retriever)\n    tokenizer = RagTokenizer.from_pretrained(rag_example_args.rag_model_name)\n    logger.info('Step 4 - Have fun')\n    question = rag_example_args.question or \"What does Moses' rod turn into ?\"\n    input_ids = tokenizer.question_encoder(question, return_tensors='pt')['input_ids']\n    generated = model.generate(input_ids)\n    generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n    logger.info('Q: ' + question)\n    logger.info('A: ' + generated_string)"
        ]
    }
]