[
    {
        "func_name": "__init__",
        "original": "def __init__(self, use_depth_classifier=True, use_onlyreg_proj=False, weight_dim=-1, weight_branch=((256,),), depth_branch=(64,), depth_range=(0, 70), depth_unit=10, division='uniform', depth_bins=8, loss_depth=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_bbox2d=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_consistency=dict(type='GIoULoss', loss_weight=1.0), pred_bbox2d=True, pred_keypoints=False, bbox_coder=dict(type='PGDBBoxCoder', base_depths=((28.01, 16.32),), base_dims=((0.8, 1.73, 0.6), (1.76, 1.73, 0.6), (3.9, 1.56, 1.6)), code_size=7), **kwargs):\n    self.use_depth_classifier = use_depth_classifier\n    self.use_onlyreg_proj = use_onlyreg_proj\n    self.depth_branch = depth_branch\n    self.pred_keypoints = pred_keypoints\n    self.weight_dim = weight_dim\n    self.weight_branch = weight_branch\n    self.weight_out_channels = []\n    for weight_branch_channels in weight_branch:\n        if len(weight_branch_channels) > 0:\n            self.weight_out_channels.append(weight_branch_channels[-1])\n        else:\n            self.weight_out_channels.append(-1)\n    self.depth_range = depth_range\n    self.depth_unit = depth_unit\n    self.division = division\n    if self.division == 'uniform':\n        self.num_depth_cls = int((depth_range[1] - depth_range[0]) / depth_unit) + 1\n        if self.num_depth_cls != depth_bins:\n            print('Warning: The number of bins computed from ' + 'depth_unit is different from given parameter! ' + 'Depth_unit will be considered with priority in ' + 'Uniform Division.')\n    else:\n        self.num_depth_cls = depth_bins\n    super().__init__(pred_bbox2d=pred_bbox2d, bbox_coder=bbox_coder, **kwargs)\n    self.loss_depth = build_loss(loss_depth)\n    if self.pred_bbox2d:\n        self.loss_bbox2d = build_loss(loss_bbox2d)\n        self.loss_consistency = build_loss(loss_consistency)\n    if self.pred_keypoints:\n        self.kpts_start = 9 if self.pred_velo else 7",
        "mutated": [
            "def __init__(self, use_depth_classifier=True, use_onlyreg_proj=False, weight_dim=-1, weight_branch=((256,),), depth_branch=(64,), depth_range=(0, 70), depth_unit=10, division='uniform', depth_bins=8, loss_depth=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_bbox2d=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_consistency=dict(type='GIoULoss', loss_weight=1.0), pred_bbox2d=True, pred_keypoints=False, bbox_coder=dict(type='PGDBBoxCoder', base_depths=((28.01, 16.32),), base_dims=((0.8, 1.73, 0.6), (1.76, 1.73, 0.6), (3.9, 1.56, 1.6)), code_size=7), **kwargs):\n    if False:\n        i = 10\n    self.use_depth_classifier = use_depth_classifier\n    self.use_onlyreg_proj = use_onlyreg_proj\n    self.depth_branch = depth_branch\n    self.pred_keypoints = pred_keypoints\n    self.weight_dim = weight_dim\n    self.weight_branch = weight_branch\n    self.weight_out_channels = []\n    for weight_branch_channels in weight_branch:\n        if len(weight_branch_channels) > 0:\n            self.weight_out_channels.append(weight_branch_channels[-1])\n        else:\n            self.weight_out_channels.append(-1)\n    self.depth_range = depth_range\n    self.depth_unit = depth_unit\n    self.division = division\n    if self.division == 'uniform':\n        self.num_depth_cls = int((depth_range[1] - depth_range[0]) / depth_unit) + 1\n        if self.num_depth_cls != depth_bins:\n            print('Warning: The number of bins computed from ' + 'depth_unit is different from given parameter! ' + 'Depth_unit will be considered with priority in ' + 'Uniform Division.')\n    else:\n        self.num_depth_cls = depth_bins\n    super().__init__(pred_bbox2d=pred_bbox2d, bbox_coder=bbox_coder, **kwargs)\n    self.loss_depth = build_loss(loss_depth)\n    if self.pred_bbox2d:\n        self.loss_bbox2d = build_loss(loss_bbox2d)\n        self.loss_consistency = build_loss(loss_consistency)\n    if self.pred_keypoints:\n        self.kpts_start = 9 if self.pred_velo else 7",
            "def __init__(self, use_depth_classifier=True, use_onlyreg_proj=False, weight_dim=-1, weight_branch=((256,),), depth_branch=(64,), depth_range=(0, 70), depth_unit=10, division='uniform', depth_bins=8, loss_depth=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_bbox2d=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_consistency=dict(type='GIoULoss', loss_weight=1.0), pred_bbox2d=True, pred_keypoints=False, bbox_coder=dict(type='PGDBBoxCoder', base_depths=((28.01, 16.32),), base_dims=((0.8, 1.73, 0.6), (1.76, 1.73, 0.6), (3.9, 1.56, 1.6)), code_size=7), **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.use_depth_classifier = use_depth_classifier\n    self.use_onlyreg_proj = use_onlyreg_proj\n    self.depth_branch = depth_branch\n    self.pred_keypoints = pred_keypoints\n    self.weight_dim = weight_dim\n    self.weight_branch = weight_branch\n    self.weight_out_channels = []\n    for weight_branch_channels in weight_branch:\n        if len(weight_branch_channels) > 0:\n            self.weight_out_channels.append(weight_branch_channels[-1])\n        else:\n            self.weight_out_channels.append(-1)\n    self.depth_range = depth_range\n    self.depth_unit = depth_unit\n    self.division = division\n    if self.division == 'uniform':\n        self.num_depth_cls = int((depth_range[1] - depth_range[0]) / depth_unit) + 1\n        if self.num_depth_cls != depth_bins:\n            print('Warning: The number of bins computed from ' + 'depth_unit is different from given parameter! ' + 'Depth_unit will be considered with priority in ' + 'Uniform Division.')\n    else:\n        self.num_depth_cls = depth_bins\n    super().__init__(pred_bbox2d=pred_bbox2d, bbox_coder=bbox_coder, **kwargs)\n    self.loss_depth = build_loss(loss_depth)\n    if self.pred_bbox2d:\n        self.loss_bbox2d = build_loss(loss_bbox2d)\n        self.loss_consistency = build_loss(loss_consistency)\n    if self.pred_keypoints:\n        self.kpts_start = 9 if self.pred_velo else 7",
            "def __init__(self, use_depth_classifier=True, use_onlyreg_proj=False, weight_dim=-1, weight_branch=((256,),), depth_branch=(64,), depth_range=(0, 70), depth_unit=10, division='uniform', depth_bins=8, loss_depth=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_bbox2d=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_consistency=dict(type='GIoULoss', loss_weight=1.0), pred_bbox2d=True, pred_keypoints=False, bbox_coder=dict(type='PGDBBoxCoder', base_depths=((28.01, 16.32),), base_dims=((0.8, 1.73, 0.6), (1.76, 1.73, 0.6), (3.9, 1.56, 1.6)), code_size=7), **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.use_depth_classifier = use_depth_classifier\n    self.use_onlyreg_proj = use_onlyreg_proj\n    self.depth_branch = depth_branch\n    self.pred_keypoints = pred_keypoints\n    self.weight_dim = weight_dim\n    self.weight_branch = weight_branch\n    self.weight_out_channels = []\n    for weight_branch_channels in weight_branch:\n        if len(weight_branch_channels) > 0:\n            self.weight_out_channels.append(weight_branch_channels[-1])\n        else:\n            self.weight_out_channels.append(-1)\n    self.depth_range = depth_range\n    self.depth_unit = depth_unit\n    self.division = division\n    if self.division == 'uniform':\n        self.num_depth_cls = int((depth_range[1] - depth_range[0]) / depth_unit) + 1\n        if self.num_depth_cls != depth_bins:\n            print('Warning: The number of bins computed from ' + 'depth_unit is different from given parameter! ' + 'Depth_unit will be considered with priority in ' + 'Uniform Division.')\n    else:\n        self.num_depth_cls = depth_bins\n    super().__init__(pred_bbox2d=pred_bbox2d, bbox_coder=bbox_coder, **kwargs)\n    self.loss_depth = build_loss(loss_depth)\n    if self.pred_bbox2d:\n        self.loss_bbox2d = build_loss(loss_bbox2d)\n        self.loss_consistency = build_loss(loss_consistency)\n    if self.pred_keypoints:\n        self.kpts_start = 9 if self.pred_velo else 7",
            "def __init__(self, use_depth_classifier=True, use_onlyreg_proj=False, weight_dim=-1, weight_branch=((256,),), depth_branch=(64,), depth_range=(0, 70), depth_unit=10, division='uniform', depth_bins=8, loss_depth=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_bbox2d=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_consistency=dict(type='GIoULoss', loss_weight=1.0), pred_bbox2d=True, pred_keypoints=False, bbox_coder=dict(type='PGDBBoxCoder', base_depths=((28.01, 16.32),), base_dims=((0.8, 1.73, 0.6), (1.76, 1.73, 0.6), (3.9, 1.56, 1.6)), code_size=7), **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.use_depth_classifier = use_depth_classifier\n    self.use_onlyreg_proj = use_onlyreg_proj\n    self.depth_branch = depth_branch\n    self.pred_keypoints = pred_keypoints\n    self.weight_dim = weight_dim\n    self.weight_branch = weight_branch\n    self.weight_out_channels = []\n    for weight_branch_channels in weight_branch:\n        if len(weight_branch_channels) > 0:\n            self.weight_out_channels.append(weight_branch_channels[-1])\n        else:\n            self.weight_out_channels.append(-1)\n    self.depth_range = depth_range\n    self.depth_unit = depth_unit\n    self.division = division\n    if self.division == 'uniform':\n        self.num_depth_cls = int((depth_range[1] - depth_range[0]) / depth_unit) + 1\n        if self.num_depth_cls != depth_bins:\n            print('Warning: The number of bins computed from ' + 'depth_unit is different from given parameter! ' + 'Depth_unit will be considered with priority in ' + 'Uniform Division.')\n    else:\n        self.num_depth_cls = depth_bins\n    super().__init__(pred_bbox2d=pred_bbox2d, bbox_coder=bbox_coder, **kwargs)\n    self.loss_depth = build_loss(loss_depth)\n    if self.pred_bbox2d:\n        self.loss_bbox2d = build_loss(loss_bbox2d)\n        self.loss_consistency = build_loss(loss_consistency)\n    if self.pred_keypoints:\n        self.kpts_start = 9 if self.pred_velo else 7",
            "def __init__(self, use_depth_classifier=True, use_onlyreg_proj=False, weight_dim=-1, weight_branch=((256,),), depth_branch=(64,), depth_range=(0, 70), depth_unit=10, division='uniform', depth_bins=8, loss_depth=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_bbox2d=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0), loss_consistency=dict(type='GIoULoss', loss_weight=1.0), pred_bbox2d=True, pred_keypoints=False, bbox_coder=dict(type='PGDBBoxCoder', base_depths=((28.01, 16.32),), base_dims=((0.8, 1.73, 0.6), (1.76, 1.73, 0.6), (3.9, 1.56, 1.6)), code_size=7), **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.use_depth_classifier = use_depth_classifier\n    self.use_onlyreg_proj = use_onlyreg_proj\n    self.depth_branch = depth_branch\n    self.pred_keypoints = pred_keypoints\n    self.weight_dim = weight_dim\n    self.weight_branch = weight_branch\n    self.weight_out_channels = []\n    for weight_branch_channels in weight_branch:\n        if len(weight_branch_channels) > 0:\n            self.weight_out_channels.append(weight_branch_channels[-1])\n        else:\n            self.weight_out_channels.append(-1)\n    self.depth_range = depth_range\n    self.depth_unit = depth_unit\n    self.division = division\n    if self.division == 'uniform':\n        self.num_depth_cls = int((depth_range[1] - depth_range[0]) / depth_unit) + 1\n        if self.num_depth_cls != depth_bins:\n            print('Warning: The number of bins computed from ' + 'depth_unit is different from given parameter! ' + 'Depth_unit will be considered with priority in ' + 'Uniform Division.')\n    else:\n        self.num_depth_cls = depth_bins\n    super().__init__(pred_bbox2d=pred_bbox2d, bbox_coder=bbox_coder, **kwargs)\n    self.loss_depth = build_loss(loss_depth)\n    if self.pred_bbox2d:\n        self.loss_bbox2d = build_loss(loss_bbox2d)\n        self.loss_consistency = build_loss(loss_consistency)\n    if self.pred_keypoints:\n        self.kpts_start = 9 if self.pred_velo else 7"
        ]
    },
    {
        "func_name": "_init_layers",
        "original": "def _init_layers(self):\n    \"\"\"Initialize layers of the head.\"\"\"\n    super()._init_layers()\n    if self.pred_bbox2d:\n        self.scale_dim += 1\n    if self.pred_keypoints:\n        self.scale_dim += 1\n    self.scales = nn.ModuleList([nn.ModuleList([Scale(1.0) for _ in range(self.scale_dim)]) for _ in self.strides])",
        "mutated": [
            "def _init_layers(self):\n    if False:\n        i = 10\n    'Initialize layers of the head.'\n    super()._init_layers()\n    if self.pred_bbox2d:\n        self.scale_dim += 1\n    if self.pred_keypoints:\n        self.scale_dim += 1\n    self.scales = nn.ModuleList([nn.ModuleList([Scale(1.0) for _ in range(self.scale_dim)]) for _ in self.strides])",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize layers of the head.'\n    super()._init_layers()\n    if self.pred_bbox2d:\n        self.scale_dim += 1\n    if self.pred_keypoints:\n        self.scale_dim += 1\n    self.scales = nn.ModuleList([nn.ModuleList([Scale(1.0) for _ in range(self.scale_dim)]) for _ in self.strides])",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize layers of the head.'\n    super()._init_layers()\n    if self.pred_bbox2d:\n        self.scale_dim += 1\n    if self.pred_keypoints:\n        self.scale_dim += 1\n    self.scales = nn.ModuleList([nn.ModuleList([Scale(1.0) for _ in range(self.scale_dim)]) for _ in self.strides])",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize layers of the head.'\n    super()._init_layers()\n    if self.pred_bbox2d:\n        self.scale_dim += 1\n    if self.pred_keypoints:\n        self.scale_dim += 1\n    self.scales = nn.ModuleList([nn.ModuleList([Scale(1.0) for _ in range(self.scale_dim)]) for _ in self.strides])",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize layers of the head.'\n    super()._init_layers()\n    if self.pred_bbox2d:\n        self.scale_dim += 1\n    if self.pred_keypoints:\n        self.scale_dim += 1\n    self.scales = nn.ModuleList([nn.ModuleList([Scale(1.0) for _ in range(self.scale_dim)]) for _ in self.strides])"
        ]
    },
    {
        "func_name": "_init_predictor",
        "original": "def _init_predictor(self):\n    \"\"\"Initialize predictor layers of the head.\"\"\"\n    super()._init_predictor()\n    if self.use_depth_classifier:\n        self.conv_depth_cls_prev = self._init_branch(conv_channels=self.depth_branch, conv_strides=(1,) * len(self.depth_branch))\n        self.conv_depth_cls = nn.Conv2d(self.depth_branch[-1], self.num_depth_cls, 1)\n        self.fuse_lambda = nn.Parameter(torch.tensor(0.0001))\n    if self.weight_dim != -1:\n        self.conv_weight_prevs = nn.ModuleList()\n        self.conv_weights = nn.ModuleList()\n        for i in range(self.weight_dim):\n            weight_branch_channels = self.weight_branch[i]\n            weight_out_channel = self.weight_out_channels[i]\n            if len(weight_branch_channels) > 0:\n                self.conv_weight_prevs.append(self._init_branch(conv_channels=weight_branch_channels, conv_strides=(1,) * len(weight_branch_channels)))\n                self.conv_weights.append(nn.Conv2d(weight_out_channel, 1, 1))\n            else:\n                self.conv_weight_prevs.append(None)\n                self.conv_weights.append(nn.Conv2d(self.feat_channels, 1, 1))",
        "mutated": [
            "def _init_predictor(self):\n    if False:\n        i = 10\n    'Initialize predictor layers of the head.'\n    super()._init_predictor()\n    if self.use_depth_classifier:\n        self.conv_depth_cls_prev = self._init_branch(conv_channels=self.depth_branch, conv_strides=(1,) * len(self.depth_branch))\n        self.conv_depth_cls = nn.Conv2d(self.depth_branch[-1], self.num_depth_cls, 1)\n        self.fuse_lambda = nn.Parameter(torch.tensor(0.0001))\n    if self.weight_dim != -1:\n        self.conv_weight_prevs = nn.ModuleList()\n        self.conv_weights = nn.ModuleList()\n        for i in range(self.weight_dim):\n            weight_branch_channels = self.weight_branch[i]\n            weight_out_channel = self.weight_out_channels[i]\n            if len(weight_branch_channels) > 0:\n                self.conv_weight_prevs.append(self._init_branch(conv_channels=weight_branch_channels, conv_strides=(1,) * len(weight_branch_channels)))\n                self.conv_weights.append(nn.Conv2d(weight_out_channel, 1, 1))\n            else:\n                self.conv_weight_prevs.append(None)\n                self.conv_weights.append(nn.Conv2d(self.feat_channels, 1, 1))",
            "def _init_predictor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize predictor layers of the head.'\n    super()._init_predictor()\n    if self.use_depth_classifier:\n        self.conv_depth_cls_prev = self._init_branch(conv_channels=self.depth_branch, conv_strides=(1,) * len(self.depth_branch))\n        self.conv_depth_cls = nn.Conv2d(self.depth_branch[-1], self.num_depth_cls, 1)\n        self.fuse_lambda = nn.Parameter(torch.tensor(0.0001))\n    if self.weight_dim != -1:\n        self.conv_weight_prevs = nn.ModuleList()\n        self.conv_weights = nn.ModuleList()\n        for i in range(self.weight_dim):\n            weight_branch_channels = self.weight_branch[i]\n            weight_out_channel = self.weight_out_channels[i]\n            if len(weight_branch_channels) > 0:\n                self.conv_weight_prevs.append(self._init_branch(conv_channels=weight_branch_channels, conv_strides=(1,) * len(weight_branch_channels)))\n                self.conv_weights.append(nn.Conv2d(weight_out_channel, 1, 1))\n            else:\n                self.conv_weight_prevs.append(None)\n                self.conv_weights.append(nn.Conv2d(self.feat_channels, 1, 1))",
            "def _init_predictor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize predictor layers of the head.'\n    super()._init_predictor()\n    if self.use_depth_classifier:\n        self.conv_depth_cls_prev = self._init_branch(conv_channels=self.depth_branch, conv_strides=(1,) * len(self.depth_branch))\n        self.conv_depth_cls = nn.Conv2d(self.depth_branch[-1], self.num_depth_cls, 1)\n        self.fuse_lambda = nn.Parameter(torch.tensor(0.0001))\n    if self.weight_dim != -1:\n        self.conv_weight_prevs = nn.ModuleList()\n        self.conv_weights = nn.ModuleList()\n        for i in range(self.weight_dim):\n            weight_branch_channels = self.weight_branch[i]\n            weight_out_channel = self.weight_out_channels[i]\n            if len(weight_branch_channels) > 0:\n                self.conv_weight_prevs.append(self._init_branch(conv_channels=weight_branch_channels, conv_strides=(1,) * len(weight_branch_channels)))\n                self.conv_weights.append(nn.Conv2d(weight_out_channel, 1, 1))\n            else:\n                self.conv_weight_prevs.append(None)\n                self.conv_weights.append(nn.Conv2d(self.feat_channels, 1, 1))",
            "def _init_predictor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize predictor layers of the head.'\n    super()._init_predictor()\n    if self.use_depth_classifier:\n        self.conv_depth_cls_prev = self._init_branch(conv_channels=self.depth_branch, conv_strides=(1,) * len(self.depth_branch))\n        self.conv_depth_cls = nn.Conv2d(self.depth_branch[-1], self.num_depth_cls, 1)\n        self.fuse_lambda = nn.Parameter(torch.tensor(0.0001))\n    if self.weight_dim != -1:\n        self.conv_weight_prevs = nn.ModuleList()\n        self.conv_weights = nn.ModuleList()\n        for i in range(self.weight_dim):\n            weight_branch_channels = self.weight_branch[i]\n            weight_out_channel = self.weight_out_channels[i]\n            if len(weight_branch_channels) > 0:\n                self.conv_weight_prevs.append(self._init_branch(conv_channels=weight_branch_channels, conv_strides=(1,) * len(weight_branch_channels)))\n                self.conv_weights.append(nn.Conv2d(weight_out_channel, 1, 1))\n            else:\n                self.conv_weight_prevs.append(None)\n                self.conv_weights.append(nn.Conv2d(self.feat_channels, 1, 1))",
            "def _init_predictor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize predictor layers of the head.'\n    super()._init_predictor()\n    if self.use_depth_classifier:\n        self.conv_depth_cls_prev = self._init_branch(conv_channels=self.depth_branch, conv_strides=(1,) * len(self.depth_branch))\n        self.conv_depth_cls = nn.Conv2d(self.depth_branch[-1], self.num_depth_cls, 1)\n        self.fuse_lambda = nn.Parameter(torch.tensor(0.0001))\n    if self.weight_dim != -1:\n        self.conv_weight_prevs = nn.ModuleList()\n        self.conv_weights = nn.ModuleList()\n        for i in range(self.weight_dim):\n            weight_branch_channels = self.weight_branch[i]\n            weight_out_channel = self.weight_out_channels[i]\n            if len(weight_branch_channels) > 0:\n                self.conv_weight_prevs.append(self._init_branch(conv_channels=weight_branch_channels, conv_strides=(1,) * len(weight_branch_channels)))\n                self.conv_weights.append(nn.Conv2d(weight_out_channel, 1, 1))\n            else:\n                self.conv_weight_prevs.append(None)\n                self.conv_weights.append(nn.Conv2d(self.feat_channels, 1, 1))"
        ]
    },
    {
        "func_name": "init_weights",
        "original": "def init_weights(self):\n    \"\"\"Initialize weights of the head.\n\n        We currently still use the customized defined init_weights because the\n        default init of DCN triggered by the init_cfg will init\n        conv_offset.weight, which mistakenly affects the training stability.\n        \"\"\"\n    super().init_weights()\n    bias_cls = bias_init_with_prob(0.01)\n    if self.use_depth_classifier:\n        for m in self.conv_depth_cls_prev:\n            if isinstance(m.conv, nn.Conv2d):\n                normal_init(m.conv, std=0.01)\n        normal_init(self.conv_depth_cls, std=0.01, bias=bias_cls)\n    if self.weight_dim != -1:\n        for conv_weight_prev in self.conv_weight_prevs:\n            if conv_weight_prev is None:\n                continue\n            for m in conv_weight_prev:\n                if isinstance(m.conv, nn.Conv2d):\n                    normal_init(m.conv, std=0.01)\n        for conv_weight in self.conv_weights:\n            normal_init(conv_weight, std=0.01)",
        "mutated": [
            "def init_weights(self):\n    if False:\n        i = 10\n    'Initialize weights of the head.\\n\\n        We currently still use the customized defined init_weights because the\\n        default init of DCN triggered by the init_cfg will init\\n        conv_offset.weight, which mistakenly affects the training stability.\\n        '\n    super().init_weights()\n    bias_cls = bias_init_with_prob(0.01)\n    if self.use_depth_classifier:\n        for m in self.conv_depth_cls_prev:\n            if isinstance(m.conv, nn.Conv2d):\n                normal_init(m.conv, std=0.01)\n        normal_init(self.conv_depth_cls, std=0.01, bias=bias_cls)\n    if self.weight_dim != -1:\n        for conv_weight_prev in self.conv_weight_prevs:\n            if conv_weight_prev is None:\n                continue\n            for m in conv_weight_prev:\n                if isinstance(m.conv, nn.Conv2d):\n                    normal_init(m.conv, std=0.01)\n        for conv_weight in self.conv_weights:\n            normal_init(conv_weight, std=0.01)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize weights of the head.\\n\\n        We currently still use the customized defined init_weights because the\\n        default init of DCN triggered by the init_cfg will init\\n        conv_offset.weight, which mistakenly affects the training stability.\\n        '\n    super().init_weights()\n    bias_cls = bias_init_with_prob(0.01)\n    if self.use_depth_classifier:\n        for m in self.conv_depth_cls_prev:\n            if isinstance(m.conv, nn.Conv2d):\n                normal_init(m.conv, std=0.01)\n        normal_init(self.conv_depth_cls, std=0.01, bias=bias_cls)\n    if self.weight_dim != -1:\n        for conv_weight_prev in self.conv_weight_prevs:\n            if conv_weight_prev is None:\n                continue\n            for m in conv_weight_prev:\n                if isinstance(m.conv, nn.Conv2d):\n                    normal_init(m.conv, std=0.01)\n        for conv_weight in self.conv_weights:\n            normal_init(conv_weight, std=0.01)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize weights of the head.\\n\\n        We currently still use the customized defined init_weights because the\\n        default init of DCN triggered by the init_cfg will init\\n        conv_offset.weight, which mistakenly affects the training stability.\\n        '\n    super().init_weights()\n    bias_cls = bias_init_with_prob(0.01)\n    if self.use_depth_classifier:\n        for m in self.conv_depth_cls_prev:\n            if isinstance(m.conv, nn.Conv2d):\n                normal_init(m.conv, std=0.01)\n        normal_init(self.conv_depth_cls, std=0.01, bias=bias_cls)\n    if self.weight_dim != -1:\n        for conv_weight_prev in self.conv_weight_prevs:\n            if conv_weight_prev is None:\n                continue\n            for m in conv_weight_prev:\n                if isinstance(m.conv, nn.Conv2d):\n                    normal_init(m.conv, std=0.01)\n        for conv_weight in self.conv_weights:\n            normal_init(conv_weight, std=0.01)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize weights of the head.\\n\\n        We currently still use the customized defined init_weights because the\\n        default init of DCN triggered by the init_cfg will init\\n        conv_offset.weight, which mistakenly affects the training stability.\\n        '\n    super().init_weights()\n    bias_cls = bias_init_with_prob(0.01)\n    if self.use_depth_classifier:\n        for m in self.conv_depth_cls_prev:\n            if isinstance(m.conv, nn.Conv2d):\n                normal_init(m.conv, std=0.01)\n        normal_init(self.conv_depth_cls, std=0.01, bias=bias_cls)\n    if self.weight_dim != -1:\n        for conv_weight_prev in self.conv_weight_prevs:\n            if conv_weight_prev is None:\n                continue\n            for m in conv_weight_prev:\n                if isinstance(m.conv, nn.Conv2d):\n                    normal_init(m.conv, std=0.01)\n        for conv_weight in self.conv_weights:\n            normal_init(conv_weight, std=0.01)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize weights of the head.\\n\\n        We currently still use the customized defined init_weights because the\\n        default init of DCN triggered by the init_cfg will init\\n        conv_offset.weight, which mistakenly affects the training stability.\\n        '\n    super().init_weights()\n    bias_cls = bias_init_with_prob(0.01)\n    if self.use_depth_classifier:\n        for m in self.conv_depth_cls_prev:\n            if isinstance(m.conv, nn.Conv2d):\n                normal_init(m.conv, std=0.01)\n        normal_init(self.conv_depth_cls, std=0.01, bias=bias_cls)\n    if self.weight_dim != -1:\n        for conv_weight_prev in self.conv_weight_prevs:\n            if conv_weight_prev is None:\n                continue\n            for m in conv_weight_prev:\n                if isinstance(m.conv, nn.Conv2d):\n                    normal_init(m.conv, std=0.01)\n        for conv_weight in self.conv_weights:\n            normal_init(conv_weight, std=0.01)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, feats):\n    \"\"\"Forward features from the upstream network.\n\n        Args:\n            feats (tuple[Tensor]): Features from the upstream network, each is\n                a 4D-tensor.\n\n        Returns:\n            tuple:\n                cls_scores (list[Tensor]): Box scores for each scale level,\n                    each is a 4D-tensor, the channel number is\n                    num_points * num_classes.\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                    level, each is a 4D-tensor, the channel number is\n                    num_points * bbox_code_size.\n                dir_cls_preds (list[Tensor]): Box scores for direction class\n                    predictions on each scale level, each is a 4D-tensor,\n                    the channel number is num_points * 2. (bin = 2).\n                weight (list[Tensor]): Location-aware weight maps on each\n                    scale level, each is a 4D-tensor, the channel number is\n                    num_points * 1.\n                depth_cls_preds (list[Tensor]): Box scores for depth class\n                    predictions on each scale level, each is a 4D-tensor,\n                    the channel number is num_points * self.num_depth_cls.\n                attr_preds (list[Tensor]): Attribute scores for each scale\n                    level, each is a 4D-tensor, the channel number is\n                    num_points * num_attrs.\n                centernesses (list[Tensor]): Centerness for each scale level,\n                    each is a 4D-tensor, the channel number is num_points * 1.\n        \"\"\"\n    return multi_apply(self.forward_single, feats, self.scales, self.strides)",
        "mutated": [
            "def forward(self, feats):\n    if False:\n        i = 10\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n                dir_cls_preds (list[Tensor]): Box scores for direction class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * 2. (bin = 2).\\n                weight (list[Tensor]): Location-aware weight maps on each\\n                    scale level, each is a 4D-tensor, the channel number is\\n                    num_points * 1.\\n                depth_cls_preds (list[Tensor]): Box scores for depth class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * self.num_depth_cls.\\n                attr_preds (list[Tensor]): Attribute scores for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * num_attrs.\\n                centernesses (list[Tensor]): Centerness for each scale level,\\n                    each is a 4D-tensor, the channel number is num_points * 1.\\n        '\n    return multi_apply(self.forward_single, feats, self.scales, self.strides)",
            "def forward(self, feats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n                dir_cls_preds (list[Tensor]): Box scores for direction class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * 2. (bin = 2).\\n                weight (list[Tensor]): Location-aware weight maps on each\\n                    scale level, each is a 4D-tensor, the channel number is\\n                    num_points * 1.\\n                depth_cls_preds (list[Tensor]): Box scores for depth class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * self.num_depth_cls.\\n                attr_preds (list[Tensor]): Attribute scores for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * num_attrs.\\n                centernesses (list[Tensor]): Centerness for each scale level,\\n                    each is a 4D-tensor, the channel number is num_points * 1.\\n        '\n    return multi_apply(self.forward_single, feats, self.scales, self.strides)",
            "def forward(self, feats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n                dir_cls_preds (list[Tensor]): Box scores for direction class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * 2. (bin = 2).\\n                weight (list[Tensor]): Location-aware weight maps on each\\n                    scale level, each is a 4D-tensor, the channel number is\\n                    num_points * 1.\\n                depth_cls_preds (list[Tensor]): Box scores for depth class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * self.num_depth_cls.\\n                attr_preds (list[Tensor]): Attribute scores for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * num_attrs.\\n                centernesses (list[Tensor]): Centerness for each scale level,\\n                    each is a 4D-tensor, the channel number is num_points * 1.\\n        '\n    return multi_apply(self.forward_single, feats, self.scales, self.strides)",
            "def forward(self, feats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n                dir_cls_preds (list[Tensor]): Box scores for direction class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * 2. (bin = 2).\\n                weight (list[Tensor]): Location-aware weight maps on each\\n                    scale level, each is a 4D-tensor, the channel number is\\n                    num_points * 1.\\n                depth_cls_preds (list[Tensor]): Box scores for depth class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * self.num_depth_cls.\\n                attr_preds (list[Tensor]): Attribute scores for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * num_attrs.\\n                centernesses (list[Tensor]): Centerness for each scale level,\\n                    each is a 4D-tensor, the channel number is num_points * 1.\\n        '\n    return multi_apply(self.forward_single, feats, self.scales, self.strides)",
            "def forward(self, feats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n                dir_cls_preds (list[Tensor]): Box scores for direction class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * 2. (bin = 2).\\n                weight (list[Tensor]): Location-aware weight maps on each\\n                    scale level, each is a 4D-tensor, the channel number is\\n                    num_points * 1.\\n                depth_cls_preds (list[Tensor]): Box scores for depth class\\n                    predictions on each scale level, each is a 4D-tensor,\\n                    the channel number is num_points * self.num_depth_cls.\\n                attr_preds (list[Tensor]): Attribute scores for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * num_attrs.\\n                centernesses (list[Tensor]): Centerness for each scale level,\\n                    each is a 4D-tensor, the channel number is num_points * 1.\\n        '\n    return multi_apply(self.forward_single, feats, self.scales, self.strides)"
        ]
    },
    {
        "func_name": "forward_single",
        "original": "def forward_single(self, x, scale, stride):\n    \"\"\"Forward features of a single scale level.\n\n        Args:\n            x (Tensor): FPN feature maps of the specified stride.\n            scale (:obj: `mmcv.cnn.Scale`): Learnable scale module to resize\n                the bbox prediction.\n            stride (int): The corresponding stride for feature maps, only\n                used to normalize the bbox prediction when self.norm_on_bbox\n                is True.\n\n        Returns:\n            tuple: scores for each class, bbox and direction class\n                predictions, depth class predictions, location-aware weights,\n                attribute and centerness predictions of input feature maps.\n        \"\"\"\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, centerness, cls_feat, reg_feat) = super().forward_single(x, scale, stride)\n    max_regress_range = stride * self.regress_ranges[0][1] / self.strides[0]\n    bbox_pred = self.bbox_coder.decode_2d(bbox_pred, scale, stride, max_regress_range, self.training, self.pred_keypoints, self.pred_bbox2d)\n    depth_cls_pred = None\n    if self.use_depth_classifier:\n        clone_reg_feat = reg_feat.clone()\n        for conv_depth_cls_prev_layer in self.conv_depth_cls_prev:\n            clone_reg_feat = conv_depth_cls_prev_layer(clone_reg_feat)\n        depth_cls_pred = self.conv_depth_cls(clone_reg_feat)\n    weight = None\n    if self.weight_dim != -1:\n        weight = []\n        for i in range(self.weight_dim):\n            clone_reg_feat = reg_feat.clone()\n            if len(self.weight_branch[i]) > 0:\n                for conv_weight_prev_layer in self.conv_weight_prevs[i]:\n                    clone_reg_feat = conv_weight_prev_layer(clone_reg_feat)\n            weight.append(self.conv_weights[i](clone_reg_feat))\n        weight = torch.cat(weight, dim=1)\n    return (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness)",
        "mutated": [
            "def forward_single(self, x, scale, stride):\n    if False:\n        i = 10\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): FPN feature maps of the specified stride.\\n            scale (:obj: `mmcv.cnn.Scale`): Learnable scale module to resize\\n                the bbox prediction.\\n            stride (int): The corresponding stride for feature maps, only\\n                used to normalize the bbox prediction when self.norm_on_bbox\\n                is True.\\n\\n        Returns:\\n            tuple: scores for each class, bbox and direction class\\n                predictions, depth class predictions, location-aware weights,\\n                attribute and centerness predictions of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, centerness, cls_feat, reg_feat) = super().forward_single(x, scale, stride)\n    max_regress_range = stride * self.regress_ranges[0][1] / self.strides[0]\n    bbox_pred = self.bbox_coder.decode_2d(bbox_pred, scale, stride, max_regress_range, self.training, self.pred_keypoints, self.pred_bbox2d)\n    depth_cls_pred = None\n    if self.use_depth_classifier:\n        clone_reg_feat = reg_feat.clone()\n        for conv_depth_cls_prev_layer in self.conv_depth_cls_prev:\n            clone_reg_feat = conv_depth_cls_prev_layer(clone_reg_feat)\n        depth_cls_pred = self.conv_depth_cls(clone_reg_feat)\n    weight = None\n    if self.weight_dim != -1:\n        weight = []\n        for i in range(self.weight_dim):\n            clone_reg_feat = reg_feat.clone()\n            if len(self.weight_branch[i]) > 0:\n                for conv_weight_prev_layer in self.conv_weight_prevs[i]:\n                    clone_reg_feat = conv_weight_prev_layer(clone_reg_feat)\n            weight.append(self.conv_weights[i](clone_reg_feat))\n        weight = torch.cat(weight, dim=1)\n    return (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness)",
            "def forward_single(self, x, scale, stride):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): FPN feature maps of the specified stride.\\n            scale (:obj: `mmcv.cnn.Scale`): Learnable scale module to resize\\n                the bbox prediction.\\n            stride (int): The corresponding stride for feature maps, only\\n                used to normalize the bbox prediction when self.norm_on_bbox\\n                is True.\\n\\n        Returns:\\n            tuple: scores for each class, bbox and direction class\\n                predictions, depth class predictions, location-aware weights,\\n                attribute and centerness predictions of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, centerness, cls_feat, reg_feat) = super().forward_single(x, scale, stride)\n    max_regress_range = stride * self.regress_ranges[0][1] / self.strides[0]\n    bbox_pred = self.bbox_coder.decode_2d(bbox_pred, scale, stride, max_regress_range, self.training, self.pred_keypoints, self.pred_bbox2d)\n    depth_cls_pred = None\n    if self.use_depth_classifier:\n        clone_reg_feat = reg_feat.clone()\n        for conv_depth_cls_prev_layer in self.conv_depth_cls_prev:\n            clone_reg_feat = conv_depth_cls_prev_layer(clone_reg_feat)\n        depth_cls_pred = self.conv_depth_cls(clone_reg_feat)\n    weight = None\n    if self.weight_dim != -1:\n        weight = []\n        for i in range(self.weight_dim):\n            clone_reg_feat = reg_feat.clone()\n            if len(self.weight_branch[i]) > 0:\n                for conv_weight_prev_layer in self.conv_weight_prevs[i]:\n                    clone_reg_feat = conv_weight_prev_layer(clone_reg_feat)\n            weight.append(self.conv_weights[i](clone_reg_feat))\n        weight = torch.cat(weight, dim=1)\n    return (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness)",
            "def forward_single(self, x, scale, stride):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): FPN feature maps of the specified stride.\\n            scale (:obj: `mmcv.cnn.Scale`): Learnable scale module to resize\\n                the bbox prediction.\\n            stride (int): The corresponding stride for feature maps, only\\n                used to normalize the bbox prediction when self.norm_on_bbox\\n                is True.\\n\\n        Returns:\\n            tuple: scores for each class, bbox and direction class\\n                predictions, depth class predictions, location-aware weights,\\n                attribute and centerness predictions of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, centerness, cls_feat, reg_feat) = super().forward_single(x, scale, stride)\n    max_regress_range = stride * self.regress_ranges[0][1] / self.strides[0]\n    bbox_pred = self.bbox_coder.decode_2d(bbox_pred, scale, stride, max_regress_range, self.training, self.pred_keypoints, self.pred_bbox2d)\n    depth_cls_pred = None\n    if self.use_depth_classifier:\n        clone_reg_feat = reg_feat.clone()\n        for conv_depth_cls_prev_layer in self.conv_depth_cls_prev:\n            clone_reg_feat = conv_depth_cls_prev_layer(clone_reg_feat)\n        depth_cls_pred = self.conv_depth_cls(clone_reg_feat)\n    weight = None\n    if self.weight_dim != -1:\n        weight = []\n        for i in range(self.weight_dim):\n            clone_reg_feat = reg_feat.clone()\n            if len(self.weight_branch[i]) > 0:\n                for conv_weight_prev_layer in self.conv_weight_prevs[i]:\n                    clone_reg_feat = conv_weight_prev_layer(clone_reg_feat)\n            weight.append(self.conv_weights[i](clone_reg_feat))\n        weight = torch.cat(weight, dim=1)\n    return (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness)",
            "def forward_single(self, x, scale, stride):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): FPN feature maps of the specified stride.\\n            scale (:obj: `mmcv.cnn.Scale`): Learnable scale module to resize\\n                the bbox prediction.\\n            stride (int): The corresponding stride for feature maps, only\\n                used to normalize the bbox prediction when self.norm_on_bbox\\n                is True.\\n\\n        Returns:\\n            tuple: scores for each class, bbox and direction class\\n                predictions, depth class predictions, location-aware weights,\\n                attribute and centerness predictions of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, centerness, cls_feat, reg_feat) = super().forward_single(x, scale, stride)\n    max_regress_range = stride * self.regress_ranges[0][1] / self.strides[0]\n    bbox_pred = self.bbox_coder.decode_2d(bbox_pred, scale, stride, max_regress_range, self.training, self.pred_keypoints, self.pred_bbox2d)\n    depth_cls_pred = None\n    if self.use_depth_classifier:\n        clone_reg_feat = reg_feat.clone()\n        for conv_depth_cls_prev_layer in self.conv_depth_cls_prev:\n            clone_reg_feat = conv_depth_cls_prev_layer(clone_reg_feat)\n        depth_cls_pred = self.conv_depth_cls(clone_reg_feat)\n    weight = None\n    if self.weight_dim != -1:\n        weight = []\n        for i in range(self.weight_dim):\n            clone_reg_feat = reg_feat.clone()\n            if len(self.weight_branch[i]) > 0:\n                for conv_weight_prev_layer in self.conv_weight_prevs[i]:\n                    clone_reg_feat = conv_weight_prev_layer(clone_reg_feat)\n            weight.append(self.conv_weights[i](clone_reg_feat))\n        weight = torch.cat(weight, dim=1)\n    return (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness)",
            "def forward_single(self, x, scale, stride):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): FPN feature maps of the specified stride.\\n            scale (:obj: `mmcv.cnn.Scale`): Learnable scale module to resize\\n                the bbox prediction.\\n            stride (int): The corresponding stride for feature maps, only\\n                used to normalize the bbox prediction when self.norm_on_bbox\\n                is True.\\n\\n        Returns:\\n            tuple: scores for each class, bbox and direction class\\n                predictions, depth class predictions, location-aware weights,\\n                attribute and centerness predictions of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, centerness, cls_feat, reg_feat) = super().forward_single(x, scale, stride)\n    max_regress_range = stride * self.regress_ranges[0][1] / self.strides[0]\n    bbox_pred = self.bbox_coder.decode_2d(bbox_pred, scale, stride, max_regress_range, self.training, self.pred_keypoints, self.pred_bbox2d)\n    depth_cls_pred = None\n    if self.use_depth_classifier:\n        clone_reg_feat = reg_feat.clone()\n        for conv_depth_cls_prev_layer in self.conv_depth_cls_prev:\n            clone_reg_feat = conv_depth_cls_prev_layer(clone_reg_feat)\n        depth_cls_pred = self.conv_depth_cls(clone_reg_feat)\n    weight = None\n    if self.weight_dim != -1:\n        weight = []\n        for i in range(self.weight_dim):\n            clone_reg_feat = reg_feat.clone()\n            if len(self.weight_branch[i]) > 0:\n                for conv_weight_prev_layer in self.conv_weight_prevs[i]:\n                    clone_reg_feat = conv_weight_prev_layer(clone_reg_feat)\n            weight.append(self.conv_weights[i](clone_reg_feat))\n        weight = torch.cat(weight, dim=1)\n    return (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness)"
        ]
    },
    {
        "func_name": "get_proj_bbox2d",
        "original": "def get_proj_bbox2d(self, bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas, pos_depth_cls_preds=None, pos_weights=None, pos_cls_scores=None, with_kpts=False):\n    \"\"\"Decode box predictions and get projected 2D attributes.\n\n        Args:\n            bbox_preds (list[Tensor]): Box predictions for each scale\n                level, each is a 4D-tensor, the channel number is\n                num_points * bbox_code_size.\n            pos_dir_cls_preds (Tensor): Box scores for direction class\n                predictions of positive boxes on all the scale levels in shape\n                (num_pos_points, 2).\n            labels_3d (list[Tensor]): 3D box category labels for each scale\n                level, each is a 4D-tensor.\n            bbox_targets_3d (list[Tensor]): 3D box targets for each scale\n                level, each is a 4D-tensor, the channel number is\n                num_points * bbox_code_size.\n            pos_points (Tensor): Foreground points.\n            pos_inds (Tensor): Index of foreground points from flattened\n                tensors.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            pos_depth_cls_preds (Tensor, optional): Probabilistic depth map of\n                positive boxes on all the scale levels in shape\n                (num_pos_points, self.num_depth_cls). Defaults to None.\n            pos_weights (Tensor, optional): Location-aware weights of positive\n                boxes in shape (num_pos_points, self.weight_dim). Defaults to\n                None.\n            pos_cls_scores (Tensor, optional): Classification scores of\n                positive boxes in shape (num_pos_points, self.num_classes).\n                Defaults to None.\n            with_kpts (bool, optional): Whether to output keypoints targets.\n                Defaults to False.\n\n        Returns:\n            tuple[Tensor]: Exterior 2D boxes from projected 3D boxes,\n                predicted 2D boxes and keypoint targets (if necessary).\n        \"\"\"\n    views = [np.array(img_meta['cam2img']) for img_meta in img_metas]\n    num_imgs = len(img_metas)\n    img_idx = []\n    for label in labels_3d:\n        for idx in range(num_imgs):\n            img_idx.append(labels_3d[0].new_ones(int(len(label) / num_imgs)) * idx)\n    img_idx = torch.cat(img_idx)\n    pos_img_idx = img_idx[pos_inds]\n    flatten_strided_bbox_preds = []\n    flatten_strided_bbox2d_preds = []\n    flatten_bbox_targets_3d = []\n    flatten_strides = []\n    for (stride_idx, bbox_pred) in enumerate(bbox_preds):\n        flatten_bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims))\n        flatten_bbox_pred[:, :2] *= self.strides[stride_idx]\n        flatten_bbox_pred[:, -4:] *= self.strides[stride_idx]\n        flatten_strided_bbox_preds.append(flatten_bbox_pred[:, :self.bbox_coder.bbox_code_size])\n        flatten_strided_bbox2d_preds.append(flatten_bbox_pred[:, -4:])\n        bbox_target_3d = bbox_targets_3d[stride_idx].clone()\n        bbox_target_3d[:, :2] *= self.strides[stride_idx]\n        bbox_target_3d[:, -4:] *= self.strides[stride_idx]\n        flatten_bbox_targets_3d.append(bbox_target_3d)\n        flatten_stride = flatten_bbox_pred.new_ones(*flatten_bbox_pred.shape[:-1], 1) * self.strides[stride_idx]\n        flatten_strides.append(flatten_stride)\n    flatten_strided_bbox_preds = torch.cat(flatten_strided_bbox_preds)\n    flatten_strided_bbox2d_preds = torch.cat(flatten_strided_bbox2d_preds)\n    flatten_bbox_targets_3d = torch.cat(flatten_bbox_targets_3d)\n    flatten_strides = torch.cat(flatten_strides)\n    pos_strided_bbox_preds = flatten_strided_bbox_preds[pos_inds]\n    pos_strided_bbox2d_preds = flatten_strided_bbox2d_preds[pos_inds]\n    pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n    pos_strides = flatten_strides[pos_inds]\n    pos_decoded_bbox2d_preds = distance2bbox(pos_points, pos_strided_bbox2d_preds)\n    pos_strided_bbox_preds[:, :2] = pos_points - pos_strided_bbox_preds[:, :2]\n    pos_bbox_targets_3d[:, :2] = pos_points - pos_bbox_targets_3d[:, :2]\n    if self.use_depth_classifier and (not self.use_onlyreg_proj):\n        pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n        sig_alpha = torch.sigmoid(self.fuse_lambda)\n        pos_strided_bbox_preds[:, 2] = sig_alpha * pos_strided_bbox_preds.clone()[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds\n    box_corners_in_image = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    box_corners_in_image_gt = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    for idx in range(num_imgs):\n        mask = pos_img_idx == idx\n        if pos_strided_bbox_preds[mask].shape[0] == 0:\n            continue\n        cam2img = torch.eye(4, dtype=pos_strided_bbox_preds.dtype, device=pos_strided_bbox_preds.device)\n        view_shape = views[idx].shape\n        cam2img[:view_shape[0], :view_shape[1]] = pos_strided_bbox_preds.new_tensor(views[idx])\n        centers2d_preds = pos_strided_bbox_preds.clone()[mask, :2]\n        centers2d_targets = pos_bbox_targets_3d.clone()[mask, :2]\n        centers3d_targets = points_img2cam(pos_bbox_targets_3d[mask, :3], views[idx])\n        pos_strided_bbox_preds[mask, :3] = points_img2cam(pos_strided_bbox_preds[mask, :3], views[idx])\n        pos_bbox_targets_3d[mask, :3] = centers3d_targets\n        pos_strided_bbox_preds[mask, 2] = pos_bbox_targets_3d.clone()[mask, 2]\n        if self.use_direction_classifier:\n            pos_dir_cls_scores = torch.max(pos_dir_cls_preds[mask], dim=-1)[1]\n            pos_strided_bbox_preds[mask] = self.bbox_coder.decode_yaw(pos_strided_bbox_preds[mask], centers2d_preds, pos_dir_cls_scores, self.dir_offset, cam2img)\n        pos_bbox_targets_3d[mask, 6] = torch.atan2(centers2d_targets[:, 0] - cam2img[0, 2], cam2img[0, 0]) + pos_bbox_targets_3d[mask, 6]\n        corners = img_metas[0]['box_type_3d'](pos_strided_bbox_preds[mask], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image[mask] = points_cam2img(corners, cam2img)\n        corners_gt = img_metas[0]['box_type_3d'](pos_bbox_targets_3d[mask, :self.bbox_code_size], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image_gt[mask] = points_cam2img(corners_gt, cam2img)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    proj_bbox2d_preds = torch.cat([minxy, maxxy], dim=1)\n    outputs = (proj_bbox2d_preds, pos_decoded_bbox2d_preds)\n    if with_kpts:\n        norm_strides = pos_strides * self.regress_ranges[0][1] / self.strides[0]\n        kpts_targets = box_corners_in_image_gt - pos_points[..., None, :]\n        kpts_targets = kpts_targets.view((*pos_strided_bbox_preds.shape[:-1], 16))\n        kpts_targets /= norm_strides\n        outputs += (kpts_targets,)\n    return outputs",
        "mutated": [
            "def get_proj_bbox2d(self, bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas, pos_depth_cls_preds=None, pos_weights=None, pos_cls_scores=None, with_kpts=False):\n    if False:\n        i = 10\n    'Decode box predictions and get projected 2D attributes.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box predictions for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_dir_cls_preds (Tensor): Box scores for direction class\\n                predictions of positive boxes on all the scale levels in shape\\n                (num_pos_points, 2).\\n            labels_3d (list[Tensor]): 3D box category labels for each scale\\n                level, each is a 4D-tensor.\\n            bbox_targets_3d (list[Tensor]): 3D box targets for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_points (Tensor): Foreground points.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            pos_depth_cls_preds (Tensor, optional): Probabilistic depth map of\\n                positive boxes on all the scale levels in shape\\n                (num_pos_points, self.num_depth_cls). Defaults to None.\\n            pos_weights (Tensor, optional): Location-aware weights of positive\\n                boxes in shape (num_pos_points, self.weight_dim). Defaults to\\n                None.\\n            pos_cls_scores (Tensor, optional): Classification scores of\\n                positive boxes in shape (num_pos_points, self.num_classes).\\n                Defaults to None.\\n            with_kpts (bool, optional): Whether to output keypoints targets.\\n                Defaults to False.\\n\\n        Returns:\\n            tuple[Tensor]: Exterior 2D boxes from projected 3D boxes,\\n                predicted 2D boxes and keypoint targets (if necessary).\\n        '\n    views = [np.array(img_meta['cam2img']) for img_meta in img_metas]\n    num_imgs = len(img_metas)\n    img_idx = []\n    for label in labels_3d:\n        for idx in range(num_imgs):\n            img_idx.append(labels_3d[0].new_ones(int(len(label) / num_imgs)) * idx)\n    img_idx = torch.cat(img_idx)\n    pos_img_idx = img_idx[pos_inds]\n    flatten_strided_bbox_preds = []\n    flatten_strided_bbox2d_preds = []\n    flatten_bbox_targets_3d = []\n    flatten_strides = []\n    for (stride_idx, bbox_pred) in enumerate(bbox_preds):\n        flatten_bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims))\n        flatten_bbox_pred[:, :2] *= self.strides[stride_idx]\n        flatten_bbox_pred[:, -4:] *= self.strides[stride_idx]\n        flatten_strided_bbox_preds.append(flatten_bbox_pred[:, :self.bbox_coder.bbox_code_size])\n        flatten_strided_bbox2d_preds.append(flatten_bbox_pred[:, -4:])\n        bbox_target_3d = bbox_targets_3d[stride_idx].clone()\n        bbox_target_3d[:, :2] *= self.strides[stride_idx]\n        bbox_target_3d[:, -4:] *= self.strides[stride_idx]\n        flatten_bbox_targets_3d.append(bbox_target_3d)\n        flatten_stride = flatten_bbox_pred.new_ones(*flatten_bbox_pred.shape[:-1], 1) * self.strides[stride_idx]\n        flatten_strides.append(flatten_stride)\n    flatten_strided_bbox_preds = torch.cat(flatten_strided_bbox_preds)\n    flatten_strided_bbox2d_preds = torch.cat(flatten_strided_bbox2d_preds)\n    flatten_bbox_targets_3d = torch.cat(flatten_bbox_targets_3d)\n    flatten_strides = torch.cat(flatten_strides)\n    pos_strided_bbox_preds = flatten_strided_bbox_preds[pos_inds]\n    pos_strided_bbox2d_preds = flatten_strided_bbox2d_preds[pos_inds]\n    pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n    pos_strides = flatten_strides[pos_inds]\n    pos_decoded_bbox2d_preds = distance2bbox(pos_points, pos_strided_bbox2d_preds)\n    pos_strided_bbox_preds[:, :2] = pos_points - pos_strided_bbox_preds[:, :2]\n    pos_bbox_targets_3d[:, :2] = pos_points - pos_bbox_targets_3d[:, :2]\n    if self.use_depth_classifier and (not self.use_onlyreg_proj):\n        pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n        sig_alpha = torch.sigmoid(self.fuse_lambda)\n        pos_strided_bbox_preds[:, 2] = sig_alpha * pos_strided_bbox_preds.clone()[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds\n    box_corners_in_image = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    box_corners_in_image_gt = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    for idx in range(num_imgs):\n        mask = pos_img_idx == idx\n        if pos_strided_bbox_preds[mask].shape[0] == 0:\n            continue\n        cam2img = torch.eye(4, dtype=pos_strided_bbox_preds.dtype, device=pos_strided_bbox_preds.device)\n        view_shape = views[idx].shape\n        cam2img[:view_shape[0], :view_shape[1]] = pos_strided_bbox_preds.new_tensor(views[idx])\n        centers2d_preds = pos_strided_bbox_preds.clone()[mask, :2]\n        centers2d_targets = pos_bbox_targets_3d.clone()[mask, :2]\n        centers3d_targets = points_img2cam(pos_bbox_targets_3d[mask, :3], views[idx])\n        pos_strided_bbox_preds[mask, :3] = points_img2cam(pos_strided_bbox_preds[mask, :3], views[idx])\n        pos_bbox_targets_3d[mask, :3] = centers3d_targets\n        pos_strided_bbox_preds[mask, 2] = pos_bbox_targets_3d.clone()[mask, 2]\n        if self.use_direction_classifier:\n            pos_dir_cls_scores = torch.max(pos_dir_cls_preds[mask], dim=-1)[1]\n            pos_strided_bbox_preds[mask] = self.bbox_coder.decode_yaw(pos_strided_bbox_preds[mask], centers2d_preds, pos_dir_cls_scores, self.dir_offset, cam2img)\n        pos_bbox_targets_3d[mask, 6] = torch.atan2(centers2d_targets[:, 0] - cam2img[0, 2], cam2img[0, 0]) + pos_bbox_targets_3d[mask, 6]\n        corners = img_metas[0]['box_type_3d'](pos_strided_bbox_preds[mask], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image[mask] = points_cam2img(corners, cam2img)\n        corners_gt = img_metas[0]['box_type_3d'](pos_bbox_targets_3d[mask, :self.bbox_code_size], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image_gt[mask] = points_cam2img(corners_gt, cam2img)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    proj_bbox2d_preds = torch.cat([minxy, maxxy], dim=1)\n    outputs = (proj_bbox2d_preds, pos_decoded_bbox2d_preds)\n    if with_kpts:\n        norm_strides = pos_strides * self.regress_ranges[0][1] / self.strides[0]\n        kpts_targets = box_corners_in_image_gt - pos_points[..., None, :]\n        kpts_targets = kpts_targets.view((*pos_strided_bbox_preds.shape[:-1], 16))\n        kpts_targets /= norm_strides\n        outputs += (kpts_targets,)\n    return outputs",
            "def get_proj_bbox2d(self, bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas, pos_depth_cls_preds=None, pos_weights=None, pos_cls_scores=None, with_kpts=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Decode box predictions and get projected 2D attributes.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box predictions for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_dir_cls_preds (Tensor): Box scores for direction class\\n                predictions of positive boxes on all the scale levels in shape\\n                (num_pos_points, 2).\\n            labels_3d (list[Tensor]): 3D box category labels for each scale\\n                level, each is a 4D-tensor.\\n            bbox_targets_3d (list[Tensor]): 3D box targets for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_points (Tensor): Foreground points.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            pos_depth_cls_preds (Tensor, optional): Probabilistic depth map of\\n                positive boxes on all the scale levels in shape\\n                (num_pos_points, self.num_depth_cls). Defaults to None.\\n            pos_weights (Tensor, optional): Location-aware weights of positive\\n                boxes in shape (num_pos_points, self.weight_dim). Defaults to\\n                None.\\n            pos_cls_scores (Tensor, optional): Classification scores of\\n                positive boxes in shape (num_pos_points, self.num_classes).\\n                Defaults to None.\\n            with_kpts (bool, optional): Whether to output keypoints targets.\\n                Defaults to False.\\n\\n        Returns:\\n            tuple[Tensor]: Exterior 2D boxes from projected 3D boxes,\\n                predicted 2D boxes and keypoint targets (if necessary).\\n        '\n    views = [np.array(img_meta['cam2img']) for img_meta in img_metas]\n    num_imgs = len(img_metas)\n    img_idx = []\n    for label in labels_3d:\n        for idx in range(num_imgs):\n            img_idx.append(labels_3d[0].new_ones(int(len(label) / num_imgs)) * idx)\n    img_idx = torch.cat(img_idx)\n    pos_img_idx = img_idx[pos_inds]\n    flatten_strided_bbox_preds = []\n    flatten_strided_bbox2d_preds = []\n    flatten_bbox_targets_3d = []\n    flatten_strides = []\n    for (stride_idx, bbox_pred) in enumerate(bbox_preds):\n        flatten_bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims))\n        flatten_bbox_pred[:, :2] *= self.strides[stride_idx]\n        flatten_bbox_pred[:, -4:] *= self.strides[stride_idx]\n        flatten_strided_bbox_preds.append(flatten_bbox_pred[:, :self.bbox_coder.bbox_code_size])\n        flatten_strided_bbox2d_preds.append(flatten_bbox_pred[:, -4:])\n        bbox_target_3d = bbox_targets_3d[stride_idx].clone()\n        bbox_target_3d[:, :2] *= self.strides[stride_idx]\n        bbox_target_3d[:, -4:] *= self.strides[stride_idx]\n        flatten_bbox_targets_3d.append(bbox_target_3d)\n        flatten_stride = flatten_bbox_pred.new_ones(*flatten_bbox_pred.shape[:-1], 1) * self.strides[stride_idx]\n        flatten_strides.append(flatten_stride)\n    flatten_strided_bbox_preds = torch.cat(flatten_strided_bbox_preds)\n    flatten_strided_bbox2d_preds = torch.cat(flatten_strided_bbox2d_preds)\n    flatten_bbox_targets_3d = torch.cat(flatten_bbox_targets_3d)\n    flatten_strides = torch.cat(flatten_strides)\n    pos_strided_bbox_preds = flatten_strided_bbox_preds[pos_inds]\n    pos_strided_bbox2d_preds = flatten_strided_bbox2d_preds[pos_inds]\n    pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n    pos_strides = flatten_strides[pos_inds]\n    pos_decoded_bbox2d_preds = distance2bbox(pos_points, pos_strided_bbox2d_preds)\n    pos_strided_bbox_preds[:, :2] = pos_points - pos_strided_bbox_preds[:, :2]\n    pos_bbox_targets_3d[:, :2] = pos_points - pos_bbox_targets_3d[:, :2]\n    if self.use_depth_classifier and (not self.use_onlyreg_proj):\n        pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n        sig_alpha = torch.sigmoid(self.fuse_lambda)\n        pos_strided_bbox_preds[:, 2] = sig_alpha * pos_strided_bbox_preds.clone()[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds\n    box_corners_in_image = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    box_corners_in_image_gt = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    for idx in range(num_imgs):\n        mask = pos_img_idx == idx\n        if pos_strided_bbox_preds[mask].shape[0] == 0:\n            continue\n        cam2img = torch.eye(4, dtype=pos_strided_bbox_preds.dtype, device=pos_strided_bbox_preds.device)\n        view_shape = views[idx].shape\n        cam2img[:view_shape[0], :view_shape[1]] = pos_strided_bbox_preds.new_tensor(views[idx])\n        centers2d_preds = pos_strided_bbox_preds.clone()[mask, :2]\n        centers2d_targets = pos_bbox_targets_3d.clone()[mask, :2]\n        centers3d_targets = points_img2cam(pos_bbox_targets_3d[mask, :3], views[idx])\n        pos_strided_bbox_preds[mask, :3] = points_img2cam(pos_strided_bbox_preds[mask, :3], views[idx])\n        pos_bbox_targets_3d[mask, :3] = centers3d_targets\n        pos_strided_bbox_preds[mask, 2] = pos_bbox_targets_3d.clone()[mask, 2]\n        if self.use_direction_classifier:\n            pos_dir_cls_scores = torch.max(pos_dir_cls_preds[mask], dim=-1)[1]\n            pos_strided_bbox_preds[mask] = self.bbox_coder.decode_yaw(pos_strided_bbox_preds[mask], centers2d_preds, pos_dir_cls_scores, self.dir_offset, cam2img)\n        pos_bbox_targets_3d[mask, 6] = torch.atan2(centers2d_targets[:, 0] - cam2img[0, 2], cam2img[0, 0]) + pos_bbox_targets_3d[mask, 6]\n        corners = img_metas[0]['box_type_3d'](pos_strided_bbox_preds[mask], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image[mask] = points_cam2img(corners, cam2img)\n        corners_gt = img_metas[0]['box_type_3d'](pos_bbox_targets_3d[mask, :self.bbox_code_size], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image_gt[mask] = points_cam2img(corners_gt, cam2img)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    proj_bbox2d_preds = torch.cat([minxy, maxxy], dim=1)\n    outputs = (proj_bbox2d_preds, pos_decoded_bbox2d_preds)\n    if with_kpts:\n        norm_strides = pos_strides * self.regress_ranges[0][1] / self.strides[0]\n        kpts_targets = box_corners_in_image_gt - pos_points[..., None, :]\n        kpts_targets = kpts_targets.view((*pos_strided_bbox_preds.shape[:-1], 16))\n        kpts_targets /= norm_strides\n        outputs += (kpts_targets,)\n    return outputs",
            "def get_proj_bbox2d(self, bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas, pos_depth_cls_preds=None, pos_weights=None, pos_cls_scores=None, with_kpts=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Decode box predictions and get projected 2D attributes.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box predictions for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_dir_cls_preds (Tensor): Box scores for direction class\\n                predictions of positive boxes on all the scale levels in shape\\n                (num_pos_points, 2).\\n            labels_3d (list[Tensor]): 3D box category labels for each scale\\n                level, each is a 4D-tensor.\\n            bbox_targets_3d (list[Tensor]): 3D box targets for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_points (Tensor): Foreground points.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            pos_depth_cls_preds (Tensor, optional): Probabilistic depth map of\\n                positive boxes on all the scale levels in shape\\n                (num_pos_points, self.num_depth_cls). Defaults to None.\\n            pos_weights (Tensor, optional): Location-aware weights of positive\\n                boxes in shape (num_pos_points, self.weight_dim). Defaults to\\n                None.\\n            pos_cls_scores (Tensor, optional): Classification scores of\\n                positive boxes in shape (num_pos_points, self.num_classes).\\n                Defaults to None.\\n            with_kpts (bool, optional): Whether to output keypoints targets.\\n                Defaults to False.\\n\\n        Returns:\\n            tuple[Tensor]: Exterior 2D boxes from projected 3D boxes,\\n                predicted 2D boxes and keypoint targets (if necessary).\\n        '\n    views = [np.array(img_meta['cam2img']) for img_meta in img_metas]\n    num_imgs = len(img_metas)\n    img_idx = []\n    for label in labels_3d:\n        for idx in range(num_imgs):\n            img_idx.append(labels_3d[0].new_ones(int(len(label) / num_imgs)) * idx)\n    img_idx = torch.cat(img_idx)\n    pos_img_idx = img_idx[pos_inds]\n    flatten_strided_bbox_preds = []\n    flatten_strided_bbox2d_preds = []\n    flatten_bbox_targets_3d = []\n    flatten_strides = []\n    for (stride_idx, bbox_pred) in enumerate(bbox_preds):\n        flatten_bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims))\n        flatten_bbox_pred[:, :2] *= self.strides[stride_idx]\n        flatten_bbox_pred[:, -4:] *= self.strides[stride_idx]\n        flatten_strided_bbox_preds.append(flatten_bbox_pred[:, :self.bbox_coder.bbox_code_size])\n        flatten_strided_bbox2d_preds.append(flatten_bbox_pred[:, -4:])\n        bbox_target_3d = bbox_targets_3d[stride_idx].clone()\n        bbox_target_3d[:, :2] *= self.strides[stride_idx]\n        bbox_target_3d[:, -4:] *= self.strides[stride_idx]\n        flatten_bbox_targets_3d.append(bbox_target_3d)\n        flatten_stride = flatten_bbox_pred.new_ones(*flatten_bbox_pred.shape[:-1], 1) * self.strides[stride_idx]\n        flatten_strides.append(flatten_stride)\n    flatten_strided_bbox_preds = torch.cat(flatten_strided_bbox_preds)\n    flatten_strided_bbox2d_preds = torch.cat(flatten_strided_bbox2d_preds)\n    flatten_bbox_targets_3d = torch.cat(flatten_bbox_targets_3d)\n    flatten_strides = torch.cat(flatten_strides)\n    pos_strided_bbox_preds = flatten_strided_bbox_preds[pos_inds]\n    pos_strided_bbox2d_preds = flatten_strided_bbox2d_preds[pos_inds]\n    pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n    pos_strides = flatten_strides[pos_inds]\n    pos_decoded_bbox2d_preds = distance2bbox(pos_points, pos_strided_bbox2d_preds)\n    pos_strided_bbox_preds[:, :2] = pos_points - pos_strided_bbox_preds[:, :2]\n    pos_bbox_targets_3d[:, :2] = pos_points - pos_bbox_targets_3d[:, :2]\n    if self.use_depth_classifier and (not self.use_onlyreg_proj):\n        pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n        sig_alpha = torch.sigmoid(self.fuse_lambda)\n        pos_strided_bbox_preds[:, 2] = sig_alpha * pos_strided_bbox_preds.clone()[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds\n    box_corners_in_image = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    box_corners_in_image_gt = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    for idx in range(num_imgs):\n        mask = pos_img_idx == idx\n        if pos_strided_bbox_preds[mask].shape[0] == 0:\n            continue\n        cam2img = torch.eye(4, dtype=pos_strided_bbox_preds.dtype, device=pos_strided_bbox_preds.device)\n        view_shape = views[idx].shape\n        cam2img[:view_shape[0], :view_shape[1]] = pos_strided_bbox_preds.new_tensor(views[idx])\n        centers2d_preds = pos_strided_bbox_preds.clone()[mask, :2]\n        centers2d_targets = pos_bbox_targets_3d.clone()[mask, :2]\n        centers3d_targets = points_img2cam(pos_bbox_targets_3d[mask, :3], views[idx])\n        pos_strided_bbox_preds[mask, :3] = points_img2cam(pos_strided_bbox_preds[mask, :3], views[idx])\n        pos_bbox_targets_3d[mask, :3] = centers3d_targets\n        pos_strided_bbox_preds[mask, 2] = pos_bbox_targets_3d.clone()[mask, 2]\n        if self.use_direction_classifier:\n            pos_dir_cls_scores = torch.max(pos_dir_cls_preds[mask], dim=-1)[1]\n            pos_strided_bbox_preds[mask] = self.bbox_coder.decode_yaw(pos_strided_bbox_preds[mask], centers2d_preds, pos_dir_cls_scores, self.dir_offset, cam2img)\n        pos_bbox_targets_3d[mask, 6] = torch.atan2(centers2d_targets[:, 0] - cam2img[0, 2], cam2img[0, 0]) + pos_bbox_targets_3d[mask, 6]\n        corners = img_metas[0]['box_type_3d'](pos_strided_bbox_preds[mask], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image[mask] = points_cam2img(corners, cam2img)\n        corners_gt = img_metas[0]['box_type_3d'](pos_bbox_targets_3d[mask, :self.bbox_code_size], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image_gt[mask] = points_cam2img(corners_gt, cam2img)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    proj_bbox2d_preds = torch.cat([minxy, maxxy], dim=1)\n    outputs = (proj_bbox2d_preds, pos_decoded_bbox2d_preds)\n    if with_kpts:\n        norm_strides = pos_strides * self.regress_ranges[0][1] / self.strides[0]\n        kpts_targets = box_corners_in_image_gt - pos_points[..., None, :]\n        kpts_targets = kpts_targets.view((*pos_strided_bbox_preds.shape[:-1], 16))\n        kpts_targets /= norm_strides\n        outputs += (kpts_targets,)\n    return outputs",
            "def get_proj_bbox2d(self, bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas, pos_depth_cls_preds=None, pos_weights=None, pos_cls_scores=None, with_kpts=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Decode box predictions and get projected 2D attributes.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box predictions for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_dir_cls_preds (Tensor): Box scores for direction class\\n                predictions of positive boxes on all the scale levels in shape\\n                (num_pos_points, 2).\\n            labels_3d (list[Tensor]): 3D box category labels for each scale\\n                level, each is a 4D-tensor.\\n            bbox_targets_3d (list[Tensor]): 3D box targets for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_points (Tensor): Foreground points.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            pos_depth_cls_preds (Tensor, optional): Probabilistic depth map of\\n                positive boxes on all the scale levels in shape\\n                (num_pos_points, self.num_depth_cls). Defaults to None.\\n            pos_weights (Tensor, optional): Location-aware weights of positive\\n                boxes in shape (num_pos_points, self.weight_dim). Defaults to\\n                None.\\n            pos_cls_scores (Tensor, optional): Classification scores of\\n                positive boxes in shape (num_pos_points, self.num_classes).\\n                Defaults to None.\\n            with_kpts (bool, optional): Whether to output keypoints targets.\\n                Defaults to False.\\n\\n        Returns:\\n            tuple[Tensor]: Exterior 2D boxes from projected 3D boxes,\\n                predicted 2D boxes and keypoint targets (if necessary).\\n        '\n    views = [np.array(img_meta['cam2img']) for img_meta in img_metas]\n    num_imgs = len(img_metas)\n    img_idx = []\n    for label in labels_3d:\n        for idx in range(num_imgs):\n            img_idx.append(labels_3d[0].new_ones(int(len(label) / num_imgs)) * idx)\n    img_idx = torch.cat(img_idx)\n    pos_img_idx = img_idx[pos_inds]\n    flatten_strided_bbox_preds = []\n    flatten_strided_bbox2d_preds = []\n    flatten_bbox_targets_3d = []\n    flatten_strides = []\n    for (stride_idx, bbox_pred) in enumerate(bbox_preds):\n        flatten_bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims))\n        flatten_bbox_pred[:, :2] *= self.strides[stride_idx]\n        flatten_bbox_pred[:, -4:] *= self.strides[stride_idx]\n        flatten_strided_bbox_preds.append(flatten_bbox_pred[:, :self.bbox_coder.bbox_code_size])\n        flatten_strided_bbox2d_preds.append(flatten_bbox_pred[:, -4:])\n        bbox_target_3d = bbox_targets_3d[stride_idx].clone()\n        bbox_target_3d[:, :2] *= self.strides[stride_idx]\n        bbox_target_3d[:, -4:] *= self.strides[stride_idx]\n        flatten_bbox_targets_3d.append(bbox_target_3d)\n        flatten_stride = flatten_bbox_pred.new_ones(*flatten_bbox_pred.shape[:-1], 1) * self.strides[stride_idx]\n        flatten_strides.append(flatten_stride)\n    flatten_strided_bbox_preds = torch.cat(flatten_strided_bbox_preds)\n    flatten_strided_bbox2d_preds = torch.cat(flatten_strided_bbox2d_preds)\n    flatten_bbox_targets_3d = torch.cat(flatten_bbox_targets_3d)\n    flatten_strides = torch.cat(flatten_strides)\n    pos_strided_bbox_preds = flatten_strided_bbox_preds[pos_inds]\n    pos_strided_bbox2d_preds = flatten_strided_bbox2d_preds[pos_inds]\n    pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n    pos_strides = flatten_strides[pos_inds]\n    pos_decoded_bbox2d_preds = distance2bbox(pos_points, pos_strided_bbox2d_preds)\n    pos_strided_bbox_preds[:, :2] = pos_points - pos_strided_bbox_preds[:, :2]\n    pos_bbox_targets_3d[:, :2] = pos_points - pos_bbox_targets_3d[:, :2]\n    if self.use_depth_classifier and (not self.use_onlyreg_proj):\n        pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n        sig_alpha = torch.sigmoid(self.fuse_lambda)\n        pos_strided_bbox_preds[:, 2] = sig_alpha * pos_strided_bbox_preds.clone()[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds\n    box_corners_in_image = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    box_corners_in_image_gt = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    for idx in range(num_imgs):\n        mask = pos_img_idx == idx\n        if pos_strided_bbox_preds[mask].shape[0] == 0:\n            continue\n        cam2img = torch.eye(4, dtype=pos_strided_bbox_preds.dtype, device=pos_strided_bbox_preds.device)\n        view_shape = views[idx].shape\n        cam2img[:view_shape[0], :view_shape[1]] = pos_strided_bbox_preds.new_tensor(views[idx])\n        centers2d_preds = pos_strided_bbox_preds.clone()[mask, :2]\n        centers2d_targets = pos_bbox_targets_3d.clone()[mask, :2]\n        centers3d_targets = points_img2cam(pos_bbox_targets_3d[mask, :3], views[idx])\n        pos_strided_bbox_preds[mask, :3] = points_img2cam(pos_strided_bbox_preds[mask, :3], views[idx])\n        pos_bbox_targets_3d[mask, :3] = centers3d_targets\n        pos_strided_bbox_preds[mask, 2] = pos_bbox_targets_3d.clone()[mask, 2]\n        if self.use_direction_classifier:\n            pos_dir_cls_scores = torch.max(pos_dir_cls_preds[mask], dim=-1)[1]\n            pos_strided_bbox_preds[mask] = self.bbox_coder.decode_yaw(pos_strided_bbox_preds[mask], centers2d_preds, pos_dir_cls_scores, self.dir_offset, cam2img)\n        pos_bbox_targets_3d[mask, 6] = torch.atan2(centers2d_targets[:, 0] - cam2img[0, 2], cam2img[0, 0]) + pos_bbox_targets_3d[mask, 6]\n        corners = img_metas[0]['box_type_3d'](pos_strided_bbox_preds[mask], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image[mask] = points_cam2img(corners, cam2img)\n        corners_gt = img_metas[0]['box_type_3d'](pos_bbox_targets_3d[mask, :self.bbox_code_size], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image_gt[mask] = points_cam2img(corners_gt, cam2img)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    proj_bbox2d_preds = torch.cat([minxy, maxxy], dim=1)\n    outputs = (proj_bbox2d_preds, pos_decoded_bbox2d_preds)\n    if with_kpts:\n        norm_strides = pos_strides * self.regress_ranges[0][1] / self.strides[0]\n        kpts_targets = box_corners_in_image_gt - pos_points[..., None, :]\n        kpts_targets = kpts_targets.view((*pos_strided_bbox_preds.shape[:-1], 16))\n        kpts_targets /= norm_strides\n        outputs += (kpts_targets,)\n    return outputs",
            "def get_proj_bbox2d(self, bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas, pos_depth_cls_preds=None, pos_weights=None, pos_cls_scores=None, with_kpts=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Decode box predictions and get projected 2D attributes.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box predictions for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_dir_cls_preds (Tensor): Box scores for direction class\\n                predictions of positive boxes on all the scale levels in shape\\n                (num_pos_points, 2).\\n            labels_3d (list[Tensor]): 3D box category labels for each scale\\n                level, each is a 4D-tensor.\\n            bbox_targets_3d (list[Tensor]): 3D box targets for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            pos_points (Tensor): Foreground points.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            pos_depth_cls_preds (Tensor, optional): Probabilistic depth map of\\n                positive boxes on all the scale levels in shape\\n                (num_pos_points, self.num_depth_cls). Defaults to None.\\n            pos_weights (Tensor, optional): Location-aware weights of positive\\n                boxes in shape (num_pos_points, self.weight_dim). Defaults to\\n                None.\\n            pos_cls_scores (Tensor, optional): Classification scores of\\n                positive boxes in shape (num_pos_points, self.num_classes).\\n                Defaults to None.\\n            with_kpts (bool, optional): Whether to output keypoints targets.\\n                Defaults to False.\\n\\n        Returns:\\n            tuple[Tensor]: Exterior 2D boxes from projected 3D boxes,\\n                predicted 2D boxes and keypoint targets (if necessary).\\n        '\n    views = [np.array(img_meta['cam2img']) for img_meta in img_metas]\n    num_imgs = len(img_metas)\n    img_idx = []\n    for label in labels_3d:\n        for idx in range(num_imgs):\n            img_idx.append(labels_3d[0].new_ones(int(len(label) / num_imgs)) * idx)\n    img_idx = torch.cat(img_idx)\n    pos_img_idx = img_idx[pos_inds]\n    flatten_strided_bbox_preds = []\n    flatten_strided_bbox2d_preds = []\n    flatten_bbox_targets_3d = []\n    flatten_strides = []\n    for (stride_idx, bbox_pred) in enumerate(bbox_preds):\n        flatten_bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims))\n        flatten_bbox_pred[:, :2] *= self.strides[stride_idx]\n        flatten_bbox_pred[:, -4:] *= self.strides[stride_idx]\n        flatten_strided_bbox_preds.append(flatten_bbox_pred[:, :self.bbox_coder.bbox_code_size])\n        flatten_strided_bbox2d_preds.append(flatten_bbox_pred[:, -4:])\n        bbox_target_3d = bbox_targets_3d[stride_idx].clone()\n        bbox_target_3d[:, :2] *= self.strides[stride_idx]\n        bbox_target_3d[:, -4:] *= self.strides[stride_idx]\n        flatten_bbox_targets_3d.append(bbox_target_3d)\n        flatten_stride = flatten_bbox_pred.new_ones(*flatten_bbox_pred.shape[:-1], 1) * self.strides[stride_idx]\n        flatten_strides.append(flatten_stride)\n    flatten_strided_bbox_preds = torch.cat(flatten_strided_bbox_preds)\n    flatten_strided_bbox2d_preds = torch.cat(flatten_strided_bbox2d_preds)\n    flatten_bbox_targets_3d = torch.cat(flatten_bbox_targets_3d)\n    flatten_strides = torch.cat(flatten_strides)\n    pos_strided_bbox_preds = flatten_strided_bbox_preds[pos_inds]\n    pos_strided_bbox2d_preds = flatten_strided_bbox2d_preds[pos_inds]\n    pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n    pos_strides = flatten_strides[pos_inds]\n    pos_decoded_bbox2d_preds = distance2bbox(pos_points, pos_strided_bbox2d_preds)\n    pos_strided_bbox_preds[:, :2] = pos_points - pos_strided_bbox_preds[:, :2]\n    pos_bbox_targets_3d[:, :2] = pos_points - pos_bbox_targets_3d[:, :2]\n    if self.use_depth_classifier and (not self.use_onlyreg_proj):\n        pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n        sig_alpha = torch.sigmoid(self.fuse_lambda)\n        pos_strided_bbox_preds[:, 2] = sig_alpha * pos_strided_bbox_preds.clone()[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds\n    box_corners_in_image = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    box_corners_in_image_gt = pos_strided_bbox_preds.new_zeros((*pos_strided_bbox_preds.shape[:-1], 8, 2))\n    for idx in range(num_imgs):\n        mask = pos_img_idx == idx\n        if pos_strided_bbox_preds[mask].shape[0] == 0:\n            continue\n        cam2img = torch.eye(4, dtype=pos_strided_bbox_preds.dtype, device=pos_strided_bbox_preds.device)\n        view_shape = views[idx].shape\n        cam2img[:view_shape[0], :view_shape[1]] = pos_strided_bbox_preds.new_tensor(views[idx])\n        centers2d_preds = pos_strided_bbox_preds.clone()[mask, :2]\n        centers2d_targets = pos_bbox_targets_3d.clone()[mask, :2]\n        centers3d_targets = points_img2cam(pos_bbox_targets_3d[mask, :3], views[idx])\n        pos_strided_bbox_preds[mask, :3] = points_img2cam(pos_strided_bbox_preds[mask, :3], views[idx])\n        pos_bbox_targets_3d[mask, :3] = centers3d_targets\n        pos_strided_bbox_preds[mask, 2] = pos_bbox_targets_3d.clone()[mask, 2]\n        if self.use_direction_classifier:\n            pos_dir_cls_scores = torch.max(pos_dir_cls_preds[mask], dim=-1)[1]\n            pos_strided_bbox_preds[mask] = self.bbox_coder.decode_yaw(pos_strided_bbox_preds[mask], centers2d_preds, pos_dir_cls_scores, self.dir_offset, cam2img)\n        pos_bbox_targets_3d[mask, 6] = torch.atan2(centers2d_targets[:, 0] - cam2img[0, 2], cam2img[0, 0]) + pos_bbox_targets_3d[mask, 6]\n        corners = img_metas[0]['box_type_3d'](pos_strided_bbox_preds[mask], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image[mask] = points_cam2img(corners, cam2img)\n        corners_gt = img_metas[0]['box_type_3d'](pos_bbox_targets_3d[mask, :self.bbox_code_size], box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).corners\n        box_corners_in_image_gt[mask] = points_cam2img(corners_gt, cam2img)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    proj_bbox2d_preds = torch.cat([minxy, maxxy], dim=1)\n    outputs = (proj_bbox2d_preds, pos_decoded_bbox2d_preds)\n    if with_kpts:\n        norm_strides = pos_strides * self.regress_ranges[0][1] / self.strides[0]\n        kpts_targets = box_corners_in_image_gt - pos_points[..., None, :]\n        kpts_targets = kpts_targets.view((*pos_strided_bbox_preds.shape[:-1], 16))\n        kpts_targets /= norm_strides\n        outputs += (kpts_targets,)\n    return outputs"
        ]
    },
    {
        "func_name": "get_pos_predictions",
        "original": "def get_pos_predictions(self, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas):\n    \"\"\"Flatten predictions and get positive ones.\n\n        Args:\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level, each is a 4D-tensor, the channel number is\n                num_points * bbox_code_size.\n            dir_cls_preds (list[Tensor]): Box scores for direction class\n                predictions on each scale level, each is a 4D-tensor,\n                the channel number is num_points * 2. (bin = 2)\n            depth_cls_preds (list[Tensor]): Box scores for direction class\n                predictions on each scale level, each is a 4D-tensor,\n                the channel number is num_points * self.num_depth_cls.\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\n                each is a 4D-tensor, the channel number is\n                num_points * num_attrs.\n            centernesses (list[Tensor]): Centerness for each scale level, each\n                is a 4D-tensor, the channel number is num_points * 1.\n            pos_inds (Tensor): Index of foreground points from flattened\n                tensors.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n\n        Returns:\n            tuple[Tensor]: Box predictions, direction classes, probabilistic\n                depth maps, location-aware weight maps, attributes and\n                centerness predictions.\n        \"\"\"\n    flatten_bbox_preds = [bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims)) for bbox_pred in bbox_preds]\n    flatten_dir_cls_preds = [dir_cls_pred.permute(0, 2, 3, 1).reshape(-1, 2) for dir_cls_pred in dir_cls_preds]\n    flatten_centerness = [centerness.permute(0, 2, 3, 1).reshape(-1) for centerness in centernesses]\n    flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n    flatten_dir_cls_preds = torch.cat(flatten_dir_cls_preds)\n    flatten_centerness = torch.cat(flatten_centerness)\n    pos_bbox_preds = flatten_bbox_preds[pos_inds]\n    pos_dir_cls_preds = flatten_dir_cls_preds[pos_inds]\n    pos_centerness = flatten_centerness[pos_inds]\n    pos_depth_cls_preds = None\n    if self.use_depth_classifier:\n        flatten_depth_cls_preds = [depth_cls_pred.permute(0, 2, 3, 1).reshape(-1, self.num_depth_cls) for depth_cls_pred in depth_cls_preds]\n        flatten_depth_cls_preds = torch.cat(flatten_depth_cls_preds)\n        pos_depth_cls_preds = flatten_depth_cls_preds[pos_inds]\n    pos_weights = None\n    if self.weight_dim != -1:\n        flatten_weights = [weight.permute(0, 2, 3, 1).reshape(-1, self.weight_dim) for weight in weights]\n        flatten_weights = torch.cat(flatten_weights)\n        pos_weights = flatten_weights[pos_inds]\n    pos_attr_preds = None\n    if self.pred_attrs:\n        flatten_attr_preds = [attr_pred.permute(0, 2, 3, 1).reshape(-1, self.num_attrs) for attr_pred in attr_preds]\n        flatten_attr_preds = torch.cat(flatten_attr_preds)\n        pos_attr_preds = flatten_attr_preds[pos_inds]\n    return (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness)",
        "mutated": [
            "def get_pos_predictions(self, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas):\n    if False:\n        i = 10\n    'Flatten predictions and get positive ones.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor]: Box predictions, direction classes, probabilistic\\n                depth maps, location-aware weight maps, attributes and\\n                centerness predictions.\\n        '\n    flatten_bbox_preds = [bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims)) for bbox_pred in bbox_preds]\n    flatten_dir_cls_preds = [dir_cls_pred.permute(0, 2, 3, 1).reshape(-1, 2) for dir_cls_pred in dir_cls_preds]\n    flatten_centerness = [centerness.permute(0, 2, 3, 1).reshape(-1) for centerness in centernesses]\n    flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n    flatten_dir_cls_preds = torch.cat(flatten_dir_cls_preds)\n    flatten_centerness = torch.cat(flatten_centerness)\n    pos_bbox_preds = flatten_bbox_preds[pos_inds]\n    pos_dir_cls_preds = flatten_dir_cls_preds[pos_inds]\n    pos_centerness = flatten_centerness[pos_inds]\n    pos_depth_cls_preds = None\n    if self.use_depth_classifier:\n        flatten_depth_cls_preds = [depth_cls_pred.permute(0, 2, 3, 1).reshape(-1, self.num_depth_cls) for depth_cls_pred in depth_cls_preds]\n        flatten_depth_cls_preds = torch.cat(flatten_depth_cls_preds)\n        pos_depth_cls_preds = flatten_depth_cls_preds[pos_inds]\n    pos_weights = None\n    if self.weight_dim != -1:\n        flatten_weights = [weight.permute(0, 2, 3, 1).reshape(-1, self.weight_dim) for weight in weights]\n        flatten_weights = torch.cat(flatten_weights)\n        pos_weights = flatten_weights[pos_inds]\n    pos_attr_preds = None\n    if self.pred_attrs:\n        flatten_attr_preds = [attr_pred.permute(0, 2, 3, 1).reshape(-1, self.num_attrs) for attr_pred in attr_preds]\n        flatten_attr_preds = torch.cat(flatten_attr_preds)\n        pos_attr_preds = flatten_attr_preds[pos_inds]\n    return (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness)",
            "def get_pos_predictions(self, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Flatten predictions and get positive ones.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor]: Box predictions, direction classes, probabilistic\\n                depth maps, location-aware weight maps, attributes and\\n                centerness predictions.\\n        '\n    flatten_bbox_preds = [bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims)) for bbox_pred in bbox_preds]\n    flatten_dir_cls_preds = [dir_cls_pred.permute(0, 2, 3, 1).reshape(-1, 2) for dir_cls_pred in dir_cls_preds]\n    flatten_centerness = [centerness.permute(0, 2, 3, 1).reshape(-1) for centerness in centernesses]\n    flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n    flatten_dir_cls_preds = torch.cat(flatten_dir_cls_preds)\n    flatten_centerness = torch.cat(flatten_centerness)\n    pos_bbox_preds = flatten_bbox_preds[pos_inds]\n    pos_dir_cls_preds = flatten_dir_cls_preds[pos_inds]\n    pos_centerness = flatten_centerness[pos_inds]\n    pos_depth_cls_preds = None\n    if self.use_depth_classifier:\n        flatten_depth_cls_preds = [depth_cls_pred.permute(0, 2, 3, 1).reshape(-1, self.num_depth_cls) for depth_cls_pred in depth_cls_preds]\n        flatten_depth_cls_preds = torch.cat(flatten_depth_cls_preds)\n        pos_depth_cls_preds = flatten_depth_cls_preds[pos_inds]\n    pos_weights = None\n    if self.weight_dim != -1:\n        flatten_weights = [weight.permute(0, 2, 3, 1).reshape(-1, self.weight_dim) for weight in weights]\n        flatten_weights = torch.cat(flatten_weights)\n        pos_weights = flatten_weights[pos_inds]\n    pos_attr_preds = None\n    if self.pred_attrs:\n        flatten_attr_preds = [attr_pred.permute(0, 2, 3, 1).reshape(-1, self.num_attrs) for attr_pred in attr_preds]\n        flatten_attr_preds = torch.cat(flatten_attr_preds)\n        pos_attr_preds = flatten_attr_preds[pos_inds]\n    return (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness)",
            "def get_pos_predictions(self, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Flatten predictions and get positive ones.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor]: Box predictions, direction classes, probabilistic\\n                depth maps, location-aware weight maps, attributes and\\n                centerness predictions.\\n        '\n    flatten_bbox_preds = [bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims)) for bbox_pred in bbox_preds]\n    flatten_dir_cls_preds = [dir_cls_pred.permute(0, 2, 3, 1).reshape(-1, 2) for dir_cls_pred in dir_cls_preds]\n    flatten_centerness = [centerness.permute(0, 2, 3, 1).reshape(-1) for centerness in centernesses]\n    flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n    flatten_dir_cls_preds = torch.cat(flatten_dir_cls_preds)\n    flatten_centerness = torch.cat(flatten_centerness)\n    pos_bbox_preds = flatten_bbox_preds[pos_inds]\n    pos_dir_cls_preds = flatten_dir_cls_preds[pos_inds]\n    pos_centerness = flatten_centerness[pos_inds]\n    pos_depth_cls_preds = None\n    if self.use_depth_classifier:\n        flatten_depth_cls_preds = [depth_cls_pred.permute(0, 2, 3, 1).reshape(-1, self.num_depth_cls) for depth_cls_pred in depth_cls_preds]\n        flatten_depth_cls_preds = torch.cat(flatten_depth_cls_preds)\n        pos_depth_cls_preds = flatten_depth_cls_preds[pos_inds]\n    pos_weights = None\n    if self.weight_dim != -1:\n        flatten_weights = [weight.permute(0, 2, 3, 1).reshape(-1, self.weight_dim) for weight in weights]\n        flatten_weights = torch.cat(flatten_weights)\n        pos_weights = flatten_weights[pos_inds]\n    pos_attr_preds = None\n    if self.pred_attrs:\n        flatten_attr_preds = [attr_pred.permute(0, 2, 3, 1).reshape(-1, self.num_attrs) for attr_pred in attr_preds]\n        flatten_attr_preds = torch.cat(flatten_attr_preds)\n        pos_attr_preds = flatten_attr_preds[pos_inds]\n    return (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness)",
            "def get_pos_predictions(self, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Flatten predictions and get positive ones.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor]: Box predictions, direction classes, probabilistic\\n                depth maps, location-aware weight maps, attributes and\\n                centerness predictions.\\n        '\n    flatten_bbox_preds = [bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims)) for bbox_pred in bbox_preds]\n    flatten_dir_cls_preds = [dir_cls_pred.permute(0, 2, 3, 1).reshape(-1, 2) for dir_cls_pred in dir_cls_preds]\n    flatten_centerness = [centerness.permute(0, 2, 3, 1).reshape(-1) for centerness in centernesses]\n    flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n    flatten_dir_cls_preds = torch.cat(flatten_dir_cls_preds)\n    flatten_centerness = torch.cat(flatten_centerness)\n    pos_bbox_preds = flatten_bbox_preds[pos_inds]\n    pos_dir_cls_preds = flatten_dir_cls_preds[pos_inds]\n    pos_centerness = flatten_centerness[pos_inds]\n    pos_depth_cls_preds = None\n    if self.use_depth_classifier:\n        flatten_depth_cls_preds = [depth_cls_pred.permute(0, 2, 3, 1).reshape(-1, self.num_depth_cls) for depth_cls_pred in depth_cls_preds]\n        flatten_depth_cls_preds = torch.cat(flatten_depth_cls_preds)\n        pos_depth_cls_preds = flatten_depth_cls_preds[pos_inds]\n    pos_weights = None\n    if self.weight_dim != -1:\n        flatten_weights = [weight.permute(0, 2, 3, 1).reshape(-1, self.weight_dim) for weight in weights]\n        flatten_weights = torch.cat(flatten_weights)\n        pos_weights = flatten_weights[pos_inds]\n    pos_attr_preds = None\n    if self.pred_attrs:\n        flatten_attr_preds = [attr_pred.permute(0, 2, 3, 1).reshape(-1, self.num_attrs) for attr_pred in attr_preds]\n        flatten_attr_preds = torch.cat(flatten_attr_preds)\n        pos_attr_preds = flatten_attr_preds[pos_inds]\n    return (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness)",
            "def get_pos_predictions(self, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Flatten predictions and get positive ones.\\n\\n        Args:\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            pos_inds (Tensor): Index of foreground points from flattened\\n                tensors.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor]: Box predictions, direction classes, probabilistic\\n                depth maps, location-aware weight maps, attributes and\\n                centerness predictions.\\n        '\n    flatten_bbox_preds = [bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims)) for bbox_pred in bbox_preds]\n    flatten_dir_cls_preds = [dir_cls_pred.permute(0, 2, 3, 1).reshape(-1, 2) for dir_cls_pred in dir_cls_preds]\n    flatten_centerness = [centerness.permute(0, 2, 3, 1).reshape(-1) for centerness in centernesses]\n    flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n    flatten_dir_cls_preds = torch.cat(flatten_dir_cls_preds)\n    flatten_centerness = torch.cat(flatten_centerness)\n    pos_bbox_preds = flatten_bbox_preds[pos_inds]\n    pos_dir_cls_preds = flatten_dir_cls_preds[pos_inds]\n    pos_centerness = flatten_centerness[pos_inds]\n    pos_depth_cls_preds = None\n    if self.use_depth_classifier:\n        flatten_depth_cls_preds = [depth_cls_pred.permute(0, 2, 3, 1).reshape(-1, self.num_depth_cls) for depth_cls_pred in depth_cls_preds]\n        flatten_depth_cls_preds = torch.cat(flatten_depth_cls_preds)\n        pos_depth_cls_preds = flatten_depth_cls_preds[pos_inds]\n    pos_weights = None\n    if self.weight_dim != -1:\n        flatten_weights = [weight.permute(0, 2, 3, 1).reshape(-1, self.weight_dim) for weight in weights]\n        flatten_weights = torch.cat(flatten_weights)\n        pos_weights = flatten_weights[pos_inds]\n    pos_attr_preds = None\n    if self.pred_attrs:\n        flatten_attr_preds = [attr_pred.permute(0, 2, 3, 1).reshape(-1, self.num_attrs) for attr_pred in attr_preds]\n        flatten_attr_preds = torch.cat(flatten_attr_preds)\n        pos_attr_preds = flatten_attr_preds[pos_inds]\n    return (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness)"
        ]
    },
    {
        "func_name": "loss",
        "original": "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef loss(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    \"\"\"Compute loss of the head.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level,\n                each is a 4D-tensor, the channel number is\n                num_points * num_classes.\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level, each is a 4D-tensor, the channel number is\n                num_points * bbox_code_size.\n            dir_cls_preds (list[Tensor]): Box scores for direction class\n                predictions on each scale level, each is a 4D-tensor,\n                the channel number is num_points * 2. (bin = 2)\n            depth_cls_preds (list[Tensor]): Box scores for direction class\n                predictions on each scale level, each is a 4D-tensor,\n                the channel number is num_points * self.num_depth_cls.\n            weights (list[Tensor]): Location-aware weights for each scale\n                level, each is a 4D-tensor, the channel number is\n                num_points * self.weight_dim.\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\n                each is a 4D-tensor, the channel number is\n                num_points * num_attrs.\n            centernesses (list[Tensor]): Centerness for each scale level, each\n                is a 4D-tensor, the channel number is num_points * 1.\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): class indices corresponding to each box\n            gt_bboxes_3d (list[Tensor]): 3D boxes ground truth with shape of\n                (num_gts, code_size).\n            gt_labels_3d (list[Tensor]): same as gt_labels\n            centers2d (list[Tensor]): 2D centers on the image with shape of\n                (num_gts, 2).\n            depths (list[Tensor]): Depth ground truth with shape of\n                (num_gts, ).\n            attr_labels (list[Tensor]): Attributes indices of each box.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes_ignore (list[Tensor]): specify which bounding boxes can\n                be ignored when computing the loss. Defaults to None.\n\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    all_level_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    (labels_3d, bbox_targets_3d, centerness_targets, attr_targets) = self.get_targets(all_level_points, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels)\n    num_imgs = cls_scores[0].size(0)\n    flatten_cls_scores = [cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels) for cls_score in cls_scores]\n    flatten_cls_scores = torch.cat(flatten_cls_scores)\n    flatten_labels_3d = torch.cat(labels_3d)\n    flatten_bbox_targets_3d = torch.cat(bbox_targets_3d)\n    flatten_centerness_targets = torch.cat(centerness_targets)\n    flatten_points = torch.cat([points.repeat(num_imgs, 1) for points in all_level_points])\n    if self.pred_attrs:\n        flatten_attr_targets = torch.cat(attr_targets)\n    bg_class_ind = self.num_classes\n    pos_inds = ((flatten_labels_3d >= 0) & (flatten_labels_3d < bg_class_ind)).nonzero().reshape(-1)\n    num_pos = len(pos_inds)\n    loss_dict = dict()\n    loss_dict['loss_cls'] = self.loss_cls(flatten_cls_scores, flatten_labels_3d, avg_factor=num_pos + num_imgs)\n    (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness) = self.get_pos_predictions(bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas)\n    if num_pos > 0:\n        pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n        pos_centerness_targets = flatten_centerness_targets[pos_inds]\n        pos_points = flatten_points[pos_inds]\n        if self.pred_attrs:\n            pos_attr_targets = flatten_attr_targets[pos_inds]\n        if self.use_direction_classifier:\n            pos_dir_cls_targets = self.get_direction_target(pos_bbox_targets_3d, self.dir_offset, one_hot=False)\n        bbox_weights = pos_centerness_targets.new_ones(len(pos_centerness_targets), sum(self.group_reg_dims))\n        equal_weights = pos_centerness_targets.new_ones(pos_centerness_targets.shape)\n        code_weight = self.train_cfg.get('code_weight', None)\n        if code_weight:\n            assert len(code_weight) == sum(self.group_reg_dims)\n            bbox_weights = bbox_weights * bbox_weights.new_tensor(code_weight)\n        if self.diff_rad_by_sin:\n            (pos_bbox_preds, pos_bbox_targets_3d) = self.add_sin_difference(pos_bbox_preds, pos_bbox_targets_3d)\n        loss_dict['loss_offset'] = self.loss_bbox(pos_bbox_preds[:, :2], pos_bbox_targets_3d[:, :2], weight=bbox_weights[:, :2], avg_factor=equal_weights.sum())\n        loss_dict['loss_size'] = self.loss_bbox(pos_bbox_preds[:, 3:6], pos_bbox_targets_3d[:, 3:6], weight=bbox_weights[:, 3:6], avg_factor=equal_weights.sum())\n        loss_dict['loss_rotsin'] = self.loss_bbox(pos_bbox_preds[:, 6], pos_bbox_targets_3d[:, 6], weight=bbox_weights[:, 6], avg_factor=equal_weights.sum())\n        if self.pred_velo:\n            loss_dict['loss_velo'] = self.loss_bbox(pos_bbox_preds[:, 7:9], pos_bbox_targets_3d[:, 7:9], weight=bbox_weights[:, 7:9], avg_factor=equal_weights.sum())\n        proj_bbox2d_inputs = (bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas)\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = self.loss_dir(pos_dir_cls_preds, pos_dir_cls_targets, equal_weights, avg_factor=equal_weights.sum())\n        loss_dict['loss_depth'] = self.loss_bbox(pos_bbox_preds[:, 2], pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n        if self.use_depth_classifier:\n            pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            if self.weight_dim != -1:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], sigma=pos_weights[:, 0], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            else:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n            proj_bbox2d_inputs += (pos_depth_cls_preds,)\n        if self.pred_keypoints:\n            (proj_bbox2d_preds, pos_decoded_bbox2d_preds, kpts_targets) = self.get_proj_bbox2d(*proj_bbox2d_inputs, with_kpts=True)\n            loss_dict['loss_kpts'] = self.loss_bbox(pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16], kpts_targets, weight=bbox_weights[:, self.kpts_start:self.kpts_start + 16], avg_factor=equal_weights.sum())\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = self.loss_bbox2d(pos_bbox_preds[:, -4:], pos_bbox_targets_3d[:, -4:], weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n            if not self.pred_keypoints:\n                (proj_bbox2d_preds, pos_decoded_bbox2d_preds) = self.get_proj_bbox2d(*proj_bbox2d_inputs)\n            loss_dict['loss_consistency'] = self.loss_consistency(proj_bbox2d_preds, pos_decoded_bbox2d_preds, weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n        loss_dict['loss_centerness'] = self.loss_centerness(pos_centerness, pos_centerness_targets)\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = self.loss_attr(pos_attr_preds, pos_attr_targets, pos_centerness_targets, avg_factor=pos_centerness_targets.sum())\n    else:\n        loss_dict['loss_offset'] = pos_bbox_preds[:, :2].sum()\n        loss_dict['loss_size'] = pos_bbox_preds[:, 3:6].sum()\n        loss_dict['loss_rotsin'] = pos_bbox_preds[:, 6].sum()\n        loss_dict['loss_depth'] = pos_bbox_preds[:, 2].sum()\n        if self.pred_velo:\n            loss_dict['loss_velo'] = pos_bbox_preds[:, 7:9].sum()\n        if self.pred_keypoints:\n            loss_dict['loss_kpts'] = pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16].sum()\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = pos_bbox_preds[:, -4:].sum()\n            loss_dict['loss_consistency'] = pos_bbox_preds[:, -4:].sum()\n        loss_dict['loss_centerness'] = pos_centerness.sum()\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = pos_dir_cls_preds.sum()\n        if self.use_depth_classifier:\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            loss_fuse_depth = sig_alpha * pos_bbox_preds[:, 2].sum() + (1 - sig_alpha) * pos_depth_cls_preds.sum()\n            if self.weight_dim != -1:\n                loss_fuse_depth *= torch.exp(-pos_weights[:, 0].sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = pos_attr_preds.sum()\n    return loss_dict",
        "mutated": [
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef loss(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n    'Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_classes.\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): class indices corresponding to each box\\n            gt_bboxes_3d (list[Tensor]): 3D boxes ground truth with shape of\\n                (num_gts, code_size).\\n            gt_labels_3d (list[Tensor]): same as gt_labels\\n            centers2d (list[Tensor]): 2D centers on the image with shape of\\n                (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth with shape of\\n                (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (list[Tensor]): specify which bounding boxes can\\n                be ignored when computing the loss. Defaults to None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    all_level_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    (labels_3d, bbox_targets_3d, centerness_targets, attr_targets) = self.get_targets(all_level_points, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels)\n    num_imgs = cls_scores[0].size(0)\n    flatten_cls_scores = [cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels) for cls_score in cls_scores]\n    flatten_cls_scores = torch.cat(flatten_cls_scores)\n    flatten_labels_3d = torch.cat(labels_3d)\n    flatten_bbox_targets_3d = torch.cat(bbox_targets_3d)\n    flatten_centerness_targets = torch.cat(centerness_targets)\n    flatten_points = torch.cat([points.repeat(num_imgs, 1) for points in all_level_points])\n    if self.pred_attrs:\n        flatten_attr_targets = torch.cat(attr_targets)\n    bg_class_ind = self.num_classes\n    pos_inds = ((flatten_labels_3d >= 0) & (flatten_labels_3d < bg_class_ind)).nonzero().reshape(-1)\n    num_pos = len(pos_inds)\n    loss_dict = dict()\n    loss_dict['loss_cls'] = self.loss_cls(flatten_cls_scores, flatten_labels_3d, avg_factor=num_pos + num_imgs)\n    (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness) = self.get_pos_predictions(bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas)\n    if num_pos > 0:\n        pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n        pos_centerness_targets = flatten_centerness_targets[pos_inds]\n        pos_points = flatten_points[pos_inds]\n        if self.pred_attrs:\n            pos_attr_targets = flatten_attr_targets[pos_inds]\n        if self.use_direction_classifier:\n            pos_dir_cls_targets = self.get_direction_target(pos_bbox_targets_3d, self.dir_offset, one_hot=False)\n        bbox_weights = pos_centerness_targets.new_ones(len(pos_centerness_targets), sum(self.group_reg_dims))\n        equal_weights = pos_centerness_targets.new_ones(pos_centerness_targets.shape)\n        code_weight = self.train_cfg.get('code_weight', None)\n        if code_weight:\n            assert len(code_weight) == sum(self.group_reg_dims)\n            bbox_weights = bbox_weights * bbox_weights.new_tensor(code_weight)\n        if self.diff_rad_by_sin:\n            (pos_bbox_preds, pos_bbox_targets_3d) = self.add_sin_difference(pos_bbox_preds, pos_bbox_targets_3d)\n        loss_dict['loss_offset'] = self.loss_bbox(pos_bbox_preds[:, :2], pos_bbox_targets_3d[:, :2], weight=bbox_weights[:, :2], avg_factor=equal_weights.sum())\n        loss_dict['loss_size'] = self.loss_bbox(pos_bbox_preds[:, 3:6], pos_bbox_targets_3d[:, 3:6], weight=bbox_weights[:, 3:6], avg_factor=equal_weights.sum())\n        loss_dict['loss_rotsin'] = self.loss_bbox(pos_bbox_preds[:, 6], pos_bbox_targets_3d[:, 6], weight=bbox_weights[:, 6], avg_factor=equal_weights.sum())\n        if self.pred_velo:\n            loss_dict['loss_velo'] = self.loss_bbox(pos_bbox_preds[:, 7:9], pos_bbox_targets_3d[:, 7:9], weight=bbox_weights[:, 7:9], avg_factor=equal_weights.sum())\n        proj_bbox2d_inputs = (bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas)\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = self.loss_dir(pos_dir_cls_preds, pos_dir_cls_targets, equal_weights, avg_factor=equal_weights.sum())\n        loss_dict['loss_depth'] = self.loss_bbox(pos_bbox_preds[:, 2], pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n        if self.use_depth_classifier:\n            pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            if self.weight_dim != -1:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], sigma=pos_weights[:, 0], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            else:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n            proj_bbox2d_inputs += (pos_depth_cls_preds,)\n        if self.pred_keypoints:\n            (proj_bbox2d_preds, pos_decoded_bbox2d_preds, kpts_targets) = self.get_proj_bbox2d(*proj_bbox2d_inputs, with_kpts=True)\n            loss_dict['loss_kpts'] = self.loss_bbox(pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16], kpts_targets, weight=bbox_weights[:, self.kpts_start:self.kpts_start + 16], avg_factor=equal_weights.sum())\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = self.loss_bbox2d(pos_bbox_preds[:, -4:], pos_bbox_targets_3d[:, -4:], weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n            if not self.pred_keypoints:\n                (proj_bbox2d_preds, pos_decoded_bbox2d_preds) = self.get_proj_bbox2d(*proj_bbox2d_inputs)\n            loss_dict['loss_consistency'] = self.loss_consistency(proj_bbox2d_preds, pos_decoded_bbox2d_preds, weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n        loss_dict['loss_centerness'] = self.loss_centerness(pos_centerness, pos_centerness_targets)\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = self.loss_attr(pos_attr_preds, pos_attr_targets, pos_centerness_targets, avg_factor=pos_centerness_targets.sum())\n    else:\n        loss_dict['loss_offset'] = pos_bbox_preds[:, :2].sum()\n        loss_dict['loss_size'] = pos_bbox_preds[:, 3:6].sum()\n        loss_dict['loss_rotsin'] = pos_bbox_preds[:, 6].sum()\n        loss_dict['loss_depth'] = pos_bbox_preds[:, 2].sum()\n        if self.pred_velo:\n            loss_dict['loss_velo'] = pos_bbox_preds[:, 7:9].sum()\n        if self.pred_keypoints:\n            loss_dict['loss_kpts'] = pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16].sum()\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = pos_bbox_preds[:, -4:].sum()\n            loss_dict['loss_consistency'] = pos_bbox_preds[:, -4:].sum()\n        loss_dict['loss_centerness'] = pos_centerness.sum()\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = pos_dir_cls_preds.sum()\n        if self.use_depth_classifier:\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            loss_fuse_depth = sig_alpha * pos_bbox_preds[:, 2].sum() + (1 - sig_alpha) * pos_depth_cls_preds.sum()\n            if self.weight_dim != -1:\n                loss_fuse_depth *= torch.exp(-pos_weights[:, 0].sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = pos_attr_preds.sum()\n    return loss_dict",
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef loss(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_classes.\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): class indices corresponding to each box\\n            gt_bboxes_3d (list[Tensor]): 3D boxes ground truth with shape of\\n                (num_gts, code_size).\\n            gt_labels_3d (list[Tensor]): same as gt_labels\\n            centers2d (list[Tensor]): 2D centers on the image with shape of\\n                (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth with shape of\\n                (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (list[Tensor]): specify which bounding boxes can\\n                be ignored when computing the loss. Defaults to None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    all_level_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    (labels_3d, bbox_targets_3d, centerness_targets, attr_targets) = self.get_targets(all_level_points, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels)\n    num_imgs = cls_scores[0].size(0)\n    flatten_cls_scores = [cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels) for cls_score in cls_scores]\n    flatten_cls_scores = torch.cat(flatten_cls_scores)\n    flatten_labels_3d = torch.cat(labels_3d)\n    flatten_bbox_targets_3d = torch.cat(bbox_targets_3d)\n    flatten_centerness_targets = torch.cat(centerness_targets)\n    flatten_points = torch.cat([points.repeat(num_imgs, 1) for points in all_level_points])\n    if self.pred_attrs:\n        flatten_attr_targets = torch.cat(attr_targets)\n    bg_class_ind = self.num_classes\n    pos_inds = ((flatten_labels_3d >= 0) & (flatten_labels_3d < bg_class_ind)).nonzero().reshape(-1)\n    num_pos = len(pos_inds)\n    loss_dict = dict()\n    loss_dict['loss_cls'] = self.loss_cls(flatten_cls_scores, flatten_labels_3d, avg_factor=num_pos + num_imgs)\n    (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness) = self.get_pos_predictions(bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas)\n    if num_pos > 0:\n        pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n        pos_centerness_targets = flatten_centerness_targets[pos_inds]\n        pos_points = flatten_points[pos_inds]\n        if self.pred_attrs:\n            pos_attr_targets = flatten_attr_targets[pos_inds]\n        if self.use_direction_classifier:\n            pos_dir_cls_targets = self.get_direction_target(pos_bbox_targets_3d, self.dir_offset, one_hot=False)\n        bbox_weights = pos_centerness_targets.new_ones(len(pos_centerness_targets), sum(self.group_reg_dims))\n        equal_weights = pos_centerness_targets.new_ones(pos_centerness_targets.shape)\n        code_weight = self.train_cfg.get('code_weight', None)\n        if code_weight:\n            assert len(code_weight) == sum(self.group_reg_dims)\n            bbox_weights = bbox_weights * bbox_weights.new_tensor(code_weight)\n        if self.diff_rad_by_sin:\n            (pos_bbox_preds, pos_bbox_targets_3d) = self.add_sin_difference(pos_bbox_preds, pos_bbox_targets_3d)\n        loss_dict['loss_offset'] = self.loss_bbox(pos_bbox_preds[:, :2], pos_bbox_targets_3d[:, :2], weight=bbox_weights[:, :2], avg_factor=equal_weights.sum())\n        loss_dict['loss_size'] = self.loss_bbox(pos_bbox_preds[:, 3:6], pos_bbox_targets_3d[:, 3:6], weight=bbox_weights[:, 3:6], avg_factor=equal_weights.sum())\n        loss_dict['loss_rotsin'] = self.loss_bbox(pos_bbox_preds[:, 6], pos_bbox_targets_3d[:, 6], weight=bbox_weights[:, 6], avg_factor=equal_weights.sum())\n        if self.pred_velo:\n            loss_dict['loss_velo'] = self.loss_bbox(pos_bbox_preds[:, 7:9], pos_bbox_targets_3d[:, 7:9], weight=bbox_weights[:, 7:9], avg_factor=equal_weights.sum())\n        proj_bbox2d_inputs = (bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas)\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = self.loss_dir(pos_dir_cls_preds, pos_dir_cls_targets, equal_weights, avg_factor=equal_weights.sum())\n        loss_dict['loss_depth'] = self.loss_bbox(pos_bbox_preds[:, 2], pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n        if self.use_depth_classifier:\n            pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            if self.weight_dim != -1:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], sigma=pos_weights[:, 0], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            else:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n            proj_bbox2d_inputs += (pos_depth_cls_preds,)\n        if self.pred_keypoints:\n            (proj_bbox2d_preds, pos_decoded_bbox2d_preds, kpts_targets) = self.get_proj_bbox2d(*proj_bbox2d_inputs, with_kpts=True)\n            loss_dict['loss_kpts'] = self.loss_bbox(pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16], kpts_targets, weight=bbox_weights[:, self.kpts_start:self.kpts_start + 16], avg_factor=equal_weights.sum())\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = self.loss_bbox2d(pos_bbox_preds[:, -4:], pos_bbox_targets_3d[:, -4:], weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n            if not self.pred_keypoints:\n                (proj_bbox2d_preds, pos_decoded_bbox2d_preds) = self.get_proj_bbox2d(*proj_bbox2d_inputs)\n            loss_dict['loss_consistency'] = self.loss_consistency(proj_bbox2d_preds, pos_decoded_bbox2d_preds, weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n        loss_dict['loss_centerness'] = self.loss_centerness(pos_centerness, pos_centerness_targets)\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = self.loss_attr(pos_attr_preds, pos_attr_targets, pos_centerness_targets, avg_factor=pos_centerness_targets.sum())\n    else:\n        loss_dict['loss_offset'] = pos_bbox_preds[:, :2].sum()\n        loss_dict['loss_size'] = pos_bbox_preds[:, 3:6].sum()\n        loss_dict['loss_rotsin'] = pos_bbox_preds[:, 6].sum()\n        loss_dict['loss_depth'] = pos_bbox_preds[:, 2].sum()\n        if self.pred_velo:\n            loss_dict['loss_velo'] = pos_bbox_preds[:, 7:9].sum()\n        if self.pred_keypoints:\n            loss_dict['loss_kpts'] = pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16].sum()\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = pos_bbox_preds[:, -4:].sum()\n            loss_dict['loss_consistency'] = pos_bbox_preds[:, -4:].sum()\n        loss_dict['loss_centerness'] = pos_centerness.sum()\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = pos_dir_cls_preds.sum()\n        if self.use_depth_classifier:\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            loss_fuse_depth = sig_alpha * pos_bbox_preds[:, 2].sum() + (1 - sig_alpha) * pos_depth_cls_preds.sum()\n            if self.weight_dim != -1:\n                loss_fuse_depth *= torch.exp(-pos_weights[:, 0].sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = pos_attr_preds.sum()\n    return loss_dict",
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef loss(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_classes.\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): class indices corresponding to each box\\n            gt_bboxes_3d (list[Tensor]): 3D boxes ground truth with shape of\\n                (num_gts, code_size).\\n            gt_labels_3d (list[Tensor]): same as gt_labels\\n            centers2d (list[Tensor]): 2D centers on the image with shape of\\n                (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth with shape of\\n                (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (list[Tensor]): specify which bounding boxes can\\n                be ignored when computing the loss. Defaults to None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    all_level_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    (labels_3d, bbox_targets_3d, centerness_targets, attr_targets) = self.get_targets(all_level_points, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels)\n    num_imgs = cls_scores[0].size(0)\n    flatten_cls_scores = [cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels) for cls_score in cls_scores]\n    flatten_cls_scores = torch.cat(flatten_cls_scores)\n    flatten_labels_3d = torch.cat(labels_3d)\n    flatten_bbox_targets_3d = torch.cat(bbox_targets_3d)\n    flatten_centerness_targets = torch.cat(centerness_targets)\n    flatten_points = torch.cat([points.repeat(num_imgs, 1) for points in all_level_points])\n    if self.pred_attrs:\n        flatten_attr_targets = torch.cat(attr_targets)\n    bg_class_ind = self.num_classes\n    pos_inds = ((flatten_labels_3d >= 0) & (flatten_labels_3d < bg_class_ind)).nonzero().reshape(-1)\n    num_pos = len(pos_inds)\n    loss_dict = dict()\n    loss_dict['loss_cls'] = self.loss_cls(flatten_cls_scores, flatten_labels_3d, avg_factor=num_pos + num_imgs)\n    (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness) = self.get_pos_predictions(bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas)\n    if num_pos > 0:\n        pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n        pos_centerness_targets = flatten_centerness_targets[pos_inds]\n        pos_points = flatten_points[pos_inds]\n        if self.pred_attrs:\n            pos_attr_targets = flatten_attr_targets[pos_inds]\n        if self.use_direction_classifier:\n            pos_dir_cls_targets = self.get_direction_target(pos_bbox_targets_3d, self.dir_offset, one_hot=False)\n        bbox_weights = pos_centerness_targets.new_ones(len(pos_centerness_targets), sum(self.group_reg_dims))\n        equal_weights = pos_centerness_targets.new_ones(pos_centerness_targets.shape)\n        code_weight = self.train_cfg.get('code_weight', None)\n        if code_weight:\n            assert len(code_weight) == sum(self.group_reg_dims)\n            bbox_weights = bbox_weights * bbox_weights.new_tensor(code_weight)\n        if self.diff_rad_by_sin:\n            (pos_bbox_preds, pos_bbox_targets_3d) = self.add_sin_difference(pos_bbox_preds, pos_bbox_targets_3d)\n        loss_dict['loss_offset'] = self.loss_bbox(pos_bbox_preds[:, :2], pos_bbox_targets_3d[:, :2], weight=bbox_weights[:, :2], avg_factor=equal_weights.sum())\n        loss_dict['loss_size'] = self.loss_bbox(pos_bbox_preds[:, 3:6], pos_bbox_targets_3d[:, 3:6], weight=bbox_weights[:, 3:6], avg_factor=equal_weights.sum())\n        loss_dict['loss_rotsin'] = self.loss_bbox(pos_bbox_preds[:, 6], pos_bbox_targets_3d[:, 6], weight=bbox_weights[:, 6], avg_factor=equal_weights.sum())\n        if self.pred_velo:\n            loss_dict['loss_velo'] = self.loss_bbox(pos_bbox_preds[:, 7:9], pos_bbox_targets_3d[:, 7:9], weight=bbox_weights[:, 7:9], avg_factor=equal_weights.sum())\n        proj_bbox2d_inputs = (bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas)\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = self.loss_dir(pos_dir_cls_preds, pos_dir_cls_targets, equal_weights, avg_factor=equal_weights.sum())\n        loss_dict['loss_depth'] = self.loss_bbox(pos_bbox_preds[:, 2], pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n        if self.use_depth_classifier:\n            pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            if self.weight_dim != -1:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], sigma=pos_weights[:, 0], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            else:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n            proj_bbox2d_inputs += (pos_depth_cls_preds,)\n        if self.pred_keypoints:\n            (proj_bbox2d_preds, pos_decoded_bbox2d_preds, kpts_targets) = self.get_proj_bbox2d(*proj_bbox2d_inputs, with_kpts=True)\n            loss_dict['loss_kpts'] = self.loss_bbox(pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16], kpts_targets, weight=bbox_weights[:, self.kpts_start:self.kpts_start + 16], avg_factor=equal_weights.sum())\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = self.loss_bbox2d(pos_bbox_preds[:, -4:], pos_bbox_targets_3d[:, -4:], weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n            if not self.pred_keypoints:\n                (proj_bbox2d_preds, pos_decoded_bbox2d_preds) = self.get_proj_bbox2d(*proj_bbox2d_inputs)\n            loss_dict['loss_consistency'] = self.loss_consistency(proj_bbox2d_preds, pos_decoded_bbox2d_preds, weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n        loss_dict['loss_centerness'] = self.loss_centerness(pos_centerness, pos_centerness_targets)\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = self.loss_attr(pos_attr_preds, pos_attr_targets, pos_centerness_targets, avg_factor=pos_centerness_targets.sum())\n    else:\n        loss_dict['loss_offset'] = pos_bbox_preds[:, :2].sum()\n        loss_dict['loss_size'] = pos_bbox_preds[:, 3:6].sum()\n        loss_dict['loss_rotsin'] = pos_bbox_preds[:, 6].sum()\n        loss_dict['loss_depth'] = pos_bbox_preds[:, 2].sum()\n        if self.pred_velo:\n            loss_dict['loss_velo'] = pos_bbox_preds[:, 7:9].sum()\n        if self.pred_keypoints:\n            loss_dict['loss_kpts'] = pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16].sum()\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = pos_bbox_preds[:, -4:].sum()\n            loss_dict['loss_consistency'] = pos_bbox_preds[:, -4:].sum()\n        loss_dict['loss_centerness'] = pos_centerness.sum()\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = pos_dir_cls_preds.sum()\n        if self.use_depth_classifier:\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            loss_fuse_depth = sig_alpha * pos_bbox_preds[:, 2].sum() + (1 - sig_alpha) * pos_depth_cls_preds.sum()\n            if self.weight_dim != -1:\n                loss_fuse_depth *= torch.exp(-pos_weights[:, 0].sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = pos_attr_preds.sum()\n    return loss_dict",
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef loss(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_classes.\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): class indices corresponding to each box\\n            gt_bboxes_3d (list[Tensor]): 3D boxes ground truth with shape of\\n                (num_gts, code_size).\\n            gt_labels_3d (list[Tensor]): same as gt_labels\\n            centers2d (list[Tensor]): 2D centers on the image with shape of\\n                (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth with shape of\\n                (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (list[Tensor]): specify which bounding boxes can\\n                be ignored when computing the loss. Defaults to None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    all_level_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    (labels_3d, bbox_targets_3d, centerness_targets, attr_targets) = self.get_targets(all_level_points, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels)\n    num_imgs = cls_scores[0].size(0)\n    flatten_cls_scores = [cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels) for cls_score in cls_scores]\n    flatten_cls_scores = torch.cat(flatten_cls_scores)\n    flatten_labels_3d = torch.cat(labels_3d)\n    flatten_bbox_targets_3d = torch.cat(bbox_targets_3d)\n    flatten_centerness_targets = torch.cat(centerness_targets)\n    flatten_points = torch.cat([points.repeat(num_imgs, 1) for points in all_level_points])\n    if self.pred_attrs:\n        flatten_attr_targets = torch.cat(attr_targets)\n    bg_class_ind = self.num_classes\n    pos_inds = ((flatten_labels_3d >= 0) & (flatten_labels_3d < bg_class_ind)).nonzero().reshape(-1)\n    num_pos = len(pos_inds)\n    loss_dict = dict()\n    loss_dict['loss_cls'] = self.loss_cls(flatten_cls_scores, flatten_labels_3d, avg_factor=num_pos + num_imgs)\n    (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness) = self.get_pos_predictions(bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas)\n    if num_pos > 0:\n        pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n        pos_centerness_targets = flatten_centerness_targets[pos_inds]\n        pos_points = flatten_points[pos_inds]\n        if self.pred_attrs:\n            pos_attr_targets = flatten_attr_targets[pos_inds]\n        if self.use_direction_classifier:\n            pos_dir_cls_targets = self.get_direction_target(pos_bbox_targets_3d, self.dir_offset, one_hot=False)\n        bbox_weights = pos_centerness_targets.new_ones(len(pos_centerness_targets), sum(self.group_reg_dims))\n        equal_weights = pos_centerness_targets.new_ones(pos_centerness_targets.shape)\n        code_weight = self.train_cfg.get('code_weight', None)\n        if code_weight:\n            assert len(code_weight) == sum(self.group_reg_dims)\n            bbox_weights = bbox_weights * bbox_weights.new_tensor(code_weight)\n        if self.diff_rad_by_sin:\n            (pos_bbox_preds, pos_bbox_targets_3d) = self.add_sin_difference(pos_bbox_preds, pos_bbox_targets_3d)\n        loss_dict['loss_offset'] = self.loss_bbox(pos_bbox_preds[:, :2], pos_bbox_targets_3d[:, :2], weight=bbox_weights[:, :2], avg_factor=equal_weights.sum())\n        loss_dict['loss_size'] = self.loss_bbox(pos_bbox_preds[:, 3:6], pos_bbox_targets_3d[:, 3:6], weight=bbox_weights[:, 3:6], avg_factor=equal_weights.sum())\n        loss_dict['loss_rotsin'] = self.loss_bbox(pos_bbox_preds[:, 6], pos_bbox_targets_3d[:, 6], weight=bbox_weights[:, 6], avg_factor=equal_weights.sum())\n        if self.pred_velo:\n            loss_dict['loss_velo'] = self.loss_bbox(pos_bbox_preds[:, 7:9], pos_bbox_targets_3d[:, 7:9], weight=bbox_weights[:, 7:9], avg_factor=equal_weights.sum())\n        proj_bbox2d_inputs = (bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas)\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = self.loss_dir(pos_dir_cls_preds, pos_dir_cls_targets, equal_weights, avg_factor=equal_weights.sum())\n        loss_dict['loss_depth'] = self.loss_bbox(pos_bbox_preds[:, 2], pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n        if self.use_depth_classifier:\n            pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            if self.weight_dim != -1:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], sigma=pos_weights[:, 0], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            else:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n            proj_bbox2d_inputs += (pos_depth_cls_preds,)\n        if self.pred_keypoints:\n            (proj_bbox2d_preds, pos_decoded_bbox2d_preds, kpts_targets) = self.get_proj_bbox2d(*proj_bbox2d_inputs, with_kpts=True)\n            loss_dict['loss_kpts'] = self.loss_bbox(pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16], kpts_targets, weight=bbox_weights[:, self.kpts_start:self.kpts_start + 16], avg_factor=equal_weights.sum())\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = self.loss_bbox2d(pos_bbox_preds[:, -4:], pos_bbox_targets_3d[:, -4:], weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n            if not self.pred_keypoints:\n                (proj_bbox2d_preds, pos_decoded_bbox2d_preds) = self.get_proj_bbox2d(*proj_bbox2d_inputs)\n            loss_dict['loss_consistency'] = self.loss_consistency(proj_bbox2d_preds, pos_decoded_bbox2d_preds, weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n        loss_dict['loss_centerness'] = self.loss_centerness(pos_centerness, pos_centerness_targets)\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = self.loss_attr(pos_attr_preds, pos_attr_targets, pos_centerness_targets, avg_factor=pos_centerness_targets.sum())\n    else:\n        loss_dict['loss_offset'] = pos_bbox_preds[:, :2].sum()\n        loss_dict['loss_size'] = pos_bbox_preds[:, 3:6].sum()\n        loss_dict['loss_rotsin'] = pos_bbox_preds[:, 6].sum()\n        loss_dict['loss_depth'] = pos_bbox_preds[:, 2].sum()\n        if self.pred_velo:\n            loss_dict['loss_velo'] = pos_bbox_preds[:, 7:9].sum()\n        if self.pred_keypoints:\n            loss_dict['loss_kpts'] = pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16].sum()\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = pos_bbox_preds[:, -4:].sum()\n            loss_dict['loss_consistency'] = pos_bbox_preds[:, -4:].sum()\n        loss_dict['loss_centerness'] = pos_centerness.sum()\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = pos_dir_cls_preds.sum()\n        if self.use_depth_classifier:\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            loss_fuse_depth = sig_alpha * pos_bbox_preds[:, 2].sum() + (1 - sig_alpha) * pos_depth_cls_preds.sum()\n            if self.weight_dim != -1:\n                loss_fuse_depth *= torch.exp(-pos_weights[:, 0].sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = pos_attr_preds.sum()\n    return loss_dict",
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef loss(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_classes.\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * bbox_code_size.\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level,\\n                each is a 4D-tensor, the channel number is\\n                num_points * num_attrs.\\n            centernesses (list[Tensor]): Centerness for each scale level, each\\n                is a 4D-tensor, the channel number is num_points * 1.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): class indices corresponding to each box\\n            gt_bboxes_3d (list[Tensor]): 3D boxes ground truth with shape of\\n                (num_gts, code_size).\\n            gt_labels_3d (list[Tensor]): same as gt_labels\\n            centers2d (list[Tensor]): 2D centers on the image with shape of\\n                (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth with shape of\\n                (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (list[Tensor]): specify which bounding boxes can\\n                be ignored when computing the loss. Defaults to None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    all_level_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    (labels_3d, bbox_targets_3d, centerness_targets, attr_targets) = self.get_targets(all_level_points, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels)\n    num_imgs = cls_scores[0].size(0)\n    flatten_cls_scores = [cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels) for cls_score in cls_scores]\n    flatten_cls_scores = torch.cat(flatten_cls_scores)\n    flatten_labels_3d = torch.cat(labels_3d)\n    flatten_bbox_targets_3d = torch.cat(bbox_targets_3d)\n    flatten_centerness_targets = torch.cat(centerness_targets)\n    flatten_points = torch.cat([points.repeat(num_imgs, 1) for points in all_level_points])\n    if self.pred_attrs:\n        flatten_attr_targets = torch.cat(attr_targets)\n    bg_class_ind = self.num_classes\n    pos_inds = ((flatten_labels_3d >= 0) & (flatten_labels_3d < bg_class_ind)).nonzero().reshape(-1)\n    num_pos = len(pos_inds)\n    loss_dict = dict()\n    loss_dict['loss_cls'] = self.loss_cls(flatten_cls_scores, flatten_labels_3d, avg_factor=num_pos + num_imgs)\n    (pos_bbox_preds, pos_dir_cls_preds, pos_depth_cls_preds, pos_weights, pos_attr_preds, pos_centerness) = self.get_pos_predictions(bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, pos_inds, img_metas)\n    if num_pos > 0:\n        pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n        pos_centerness_targets = flatten_centerness_targets[pos_inds]\n        pos_points = flatten_points[pos_inds]\n        if self.pred_attrs:\n            pos_attr_targets = flatten_attr_targets[pos_inds]\n        if self.use_direction_classifier:\n            pos_dir_cls_targets = self.get_direction_target(pos_bbox_targets_3d, self.dir_offset, one_hot=False)\n        bbox_weights = pos_centerness_targets.new_ones(len(pos_centerness_targets), sum(self.group_reg_dims))\n        equal_weights = pos_centerness_targets.new_ones(pos_centerness_targets.shape)\n        code_weight = self.train_cfg.get('code_weight', None)\n        if code_weight:\n            assert len(code_weight) == sum(self.group_reg_dims)\n            bbox_weights = bbox_weights * bbox_weights.new_tensor(code_weight)\n        if self.diff_rad_by_sin:\n            (pos_bbox_preds, pos_bbox_targets_3d) = self.add_sin_difference(pos_bbox_preds, pos_bbox_targets_3d)\n        loss_dict['loss_offset'] = self.loss_bbox(pos_bbox_preds[:, :2], pos_bbox_targets_3d[:, :2], weight=bbox_weights[:, :2], avg_factor=equal_weights.sum())\n        loss_dict['loss_size'] = self.loss_bbox(pos_bbox_preds[:, 3:6], pos_bbox_targets_3d[:, 3:6], weight=bbox_weights[:, 3:6], avg_factor=equal_weights.sum())\n        loss_dict['loss_rotsin'] = self.loss_bbox(pos_bbox_preds[:, 6], pos_bbox_targets_3d[:, 6], weight=bbox_weights[:, 6], avg_factor=equal_weights.sum())\n        if self.pred_velo:\n            loss_dict['loss_velo'] = self.loss_bbox(pos_bbox_preds[:, 7:9], pos_bbox_targets_3d[:, 7:9], weight=bbox_weights[:, 7:9], avg_factor=equal_weights.sum())\n        proj_bbox2d_inputs = (bbox_preds, pos_dir_cls_preds, labels_3d, bbox_targets_3d, pos_points, pos_inds, img_metas)\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = self.loss_dir(pos_dir_cls_preds, pos_dir_cls_targets, equal_weights, avg_factor=equal_weights.sum())\n        loss_dict['loss_depth'] = self.loss_bbox(pos_bbox_preds[:, 2], pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n        if self.use_depth_classifier:\n            pos_prob_depth_preds = self.bbox_coder.decode_prob_depth(pos_depth_cls_preds, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            if self.weight_dim != -1:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], sigma=pos_weights[:, 0], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            else:\n                loss_fuse_depth = self.loss_depth(sig_alpha * pos_bbox_preds[:, 2] + (1 - sig_alpha) * pos_prob_depth_preds, pos_bbox_targets_3d[:, 2], weight=bbox_weights[:, 2], avg_factor=equal_weights.sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n            proj_bbox2d_inputs += (pos_depth_cls_preds,)\n        if self.pred_keypoints:\n            (proj_bbox2d_preds, pos_decoded_bbox2d_preds, kpts_targets) = self.get_proj_bbox2d(*proj_bbox2d_inputs, with_kpts=True)\n            loss_dict['loss_kpts'] = self.loss_bbox(pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16], kpts_targets, weight=bbox_weights[:, self.kpts_start:self.kpts_start + 16], avg_factor=equal_weights.sum())\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = self.loss_bbox2d(pos_bbox_preds[:, -4:], pos_bbox_targets_3d[:, -4:], weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n            if not self.pred_keypoints:\n                (proj_bbox2d_preds, pos_decoded_bbox2d_preds) = self.get_proj_bbox2d(*proj_bbox2d_inputs)\n            loss_dict['loss_consistency'] = self.loss_consistency(proj_bbox2d_preds, pos_decoded_bbox2d_preds, weight=bbox_weights[:, -4:], avg_factor=equal_weights.sum())\n        loss_dict['loss_centerness'] = self.loss_centerness(pos_centerness, pos_centerness_targets)\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = self.loss_attr(pos_attr_preds, pos_attr_targets, pos_centerness_targets, avg_factor=pos_centerness_targets.sum())\n    else:\n        loss_dict['loss_offset'] = pos_bbox_preds[:, :2].sum()\n        loss_dict['loss_size'] = pos_bbox_preds[:, 3:6].sum()\n        loss_dict['loss_rotsin'] = pos_bbox_preds[:, 6].sum()\n        loss_dict['loss_depth'] = pos_bbox_preds[:, 2].sum()\n        if self.pred_velo:\n            loss_dict['loss_velo'] = pos_bbox_preds[:, 7:9].sum()\n        if self.pred_keypoints:\n            loss_dict['loss_kpts'] = pos_bbox_preds[:, self.kpts_start:self.kpts_start + 16].sum()\n        if self.pred_bbox2d:\n            loss_dict['loss_bbox2d'] = pos_bbox_preds[:, -4:].sum()\n            loss_dict['loss_consistency'] = pos_bbox_preds[:, -4:].sum()\n        loss_dict['loss_centerness'] = pos_centerness.sum()\n        if self.use_direction_classifier:\n            loss_dict['loss_dir'] = pos_dir_cls_preds.sum()\n        if self.use_depth_classifier:\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            loss_fuse_depth = sig_alpha * pos_bbox_preds[:, 2].sum() + (1 - sig_alpha) * pos_depth_cls_preds.sum()\n            if self.weight_dim != -1:\n                loss_fuse_depth *= torch.exp(-pos_weights[:, 0].sum())\n            loss_dict['loss_depth'] = loss_fuse_depth\n        if self.pred_attrs:\n            loss_dict['loss_attr'] = pos_attr_preds.sum()\n    return loss_dict"
        ]
    },
    {
        "func_name": "get_bboxes",
        "original": "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef get_bboxes(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, img_metas, cfg=None, rescale=None):\n    \"\"\"Transform network output for a batch into bbox predictions.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level\n                Has shape (N, num_points * num_classes, H, W)\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level with shape (N, num_points * 4, H, W)\n            dir_cls_preds (list[Tensor]): Box scores for direction class\n                predictions on each scale level, each is a 4D-tensor,\n                the channel number is num_points * 2. (bin = 2)\n            depth_cls_preds (list[Tensor]): Box scores for direction class\n                predictions on each scale level, each is a 4D-tensor,\n                the channel number is num_points * self.num_depth_cls.\n            weights (list[Tensor]): Location-aware weights for each scale\n                level, each is a 4D-tensor, the channel number is\n                num_points * self.weight_dim.\n            attr_preds (list[Tensor]): Attribute scores for each scale level\n                Has shape (N, num_points * num_attrs, H, W)\n            centernesses (list[Tensor]): Centerness for each scale level with\n                shape (N, num_points * 1, H, W)\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            cfg (mmcv.Config, optional): Test / postprocessing configuration,\n                if None, test_cfg would be used. Defaults to None.\n            rescale (bool, optional): If True, return boxes in original image\n                space. Defaults to None.\n\n        Returns:\n            list[tuple[Tensor]]: Each item in result_list is a tuple, which\n                consists of predicted 3D boxes, scores, labels, attributes and\n                2D boxes (if necessary).\n        \"\"\"\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    num_levels = len(cls_scores)\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n        bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n        if self.use_direction_classifier:\n            dir_cls_pred_list = [dir_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            dir_cls_pred_list = [cls_scores[i][img_id].new_full([2, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.use_depth_classifier:\n            depth_cls_pred_list = [depth_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            depth_cls_pred_list = [cls_scores[i][img_id].new_full([self.num_depth_cls, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.weight_dim != -1:\n            weight_list = [weights[i][img_id].detach() for i in range(num_levels)]\n        else:\n            weight_list = [cls_scores[i][img_id].new_full([1, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.pred_attrs:\n            attr_pred_list = [attr_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            attr_pred_list = [cls_scores[i][img_id].new_full([self.num_attrs, *cls_scores[i][img_id].shape[1:]], self.attr_background_label).detach() for i in range(num_levels)]\n        centerness_pred_list = [centernesses[i][img_id].detach() for i in range(num_levels)]\n        input_meta = img_metas[img_id]\n        det_bboxes = self._get_bboxes_single(cls_score_list, bbox_pred_list, dir_cls_pred_list, depth_cls_pred_list, weight_list, attr_pred_list, centerness_pred_list, mlvl_points, input_meta, cfg, rescale)\n        result_list.append(det_bboxes)\n    return result_list",
        "mutated": [
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef get_bboxes(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, img_metas, cfg=None, rescale=None):\n    if False:\n        i = 10\n    'Transform network output for a batch into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level\\n                Has shape (N, num_points * num_classes, H, W)\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level with shape (N, num_points * 4, H, W)\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for each scale level with\\n                shape (N, num_points * 1, H, W)\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cfg (mmcv.Config, optional): Test / postprocessing configuration,\\n                if None, test_cfg would be used. Defaults to None.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to None.\\n\\n        Returns:\\n            list[tuple[Tensor]]: Each item in result_list is a tuple, which\\n                consists of predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    num_levels = len(cls_scores)\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n        bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n        if self.use_direction_classifier:\n            dir_cls_pred_list = [dir_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            dir_cls_pred_list = [cls_scores[i][img_id].new_full([2, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.use_depth_classifier:\n            depth_cls_pred_list = [depth_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            depth_cls_pred_list = [cls_scores[i][img_id].new_full([self.num_depth_cls, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.weight_dim != -1:\n            weight_list = [weights[i][img_id].detach() for i in range(num_levels)]\n        else:\n            weight_list = [cls_scores[i][img_id].new_full([1, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.pred_attrs:\n            attr_pred_list = [attr_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            attr_pred_list = [cls_scores[i][img_id].new_full([self.num_attrs, *cls_scores[i][img_id].shape[1:]], self.attr_background_label).detach() for i in range(num_levels)]\n        centerness_pred_list = [centernesses[i][img_id].detach() for i in range(num_levels)]\n        input_meta = img_metas[img_id]\n        det_bboxes = self._get_bboxes_single(cls_score_list, bbox_pred_list, dir_cls_pred_list, depth_cls_pred_list, weight_list, attr_pred_list, centerness_pred_list, mlvl_points, input_meta, cfg, rescale)\n        result_list.append(det_bboxes)\n    return result_list",
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef get_bboxes(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, img_metas, cfg=None, rescale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transform network output for a batch into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level\\n                Has shape (N, num_points * num_classes, H, W)\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level with shape (N, num_points * 4, H, W)\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for each scale level with\\n                shape (N, num_points * 1, H, W)\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cfg (mmcv.Config, optional): Test / postprocessing configuration,\\n                if None, test_cfg would be used. Defaults to None.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to None.\\n\\n        Returns:\\n            list[tuple[Tensor]]: Each item in result_list is a tuple, which\\n                consists of predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    num_levels = len(cls_scores)\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n        bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n        if self.use_direction_classifier:\n            dir_cls_pred_list = [dir_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            dir_cls_pred_list = [cls_scores[i][img_id].new_full([2, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.use_depth_classifier:\n            depth_cls_pred_list = [depth_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            depth_cls_pred_list = [cls_scores[i][img_id].new_full([self.num_depth_cls, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.weight_dim != -1:\n            weight_list = [weights[i][img_id].detach() for i in range(num_levels)]\n        else:\n            weight_list = [cls_scores[i][img_id].new_full([1, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.pred_attrs:\n            attr_pred_list = [attr_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            attr_pred_list = [cls_scores[i][img_id].new_full([self.num_attrs, *cls_scores[i][img_id].shape[1:]], self.attr_background_label).detach() for i in range(num_levels)]\n        centerness_pred_list = [centernesses[i][img_id].detach() for i in range(num_levels)]\n        input_meta = img_metas[img_id]\n        det_bboxes = self._get_bboxes_single(cls_score_list, bbox_pred_list, dir_cls_pred_list, depth_cls_pred_list, weight_list, attr_pred_list, centerness_pred_list, mlvl_points, input_meta, cfg, rescale)\n        result_list.append(det_bboxes)\n    return result_list",
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef get_bboxes(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, img_metas, cfg=None, rescale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transform network output for a batch into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level\\n                Has shape (N, num_points * num_classes, H, W)\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level with shape (N, num_points * 4, H, W)\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for each scale level with\\n                shape (N, num_points * 1, H, W)\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cfg (mmcv.Config, optional): Test / postprocessing configuration,\\n                if None, test_cfg would be used. Defaults to None.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to None.\\n\\n        Returns:\\n            list[tuple[Tensor]]: Each item in result_list is a tuple, which\\n                consists of predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    num_levels = len(cls_scores)\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n        bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n        if self.use_direction_classifier:\n            dir_cls_pred_list = [dir_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            dir_cls_pred_list = [cls_scores[i][img_id].new_full([2, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.use_depth_classifier:\n            depth_cls_pred_list = [depth_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            depth_cls_pred_list = [cls_scores[i][img_id].new_full([self.num_depth_cls, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.weight_dim != -1:\n            weight_list = [weights[i][img_id].detach() for i in range(num_levels)]\n        else:\n            weight_list = [cls_scores[i][img_id].new_full([1, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.pred_attrs:\n            attr_pred_list = [attr_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            attr_pred_list = [cls_scores[i][img_id].new_full([self.num_attrs, *cls_scores[i][img_id].shape[1:]], self.attr_background_label).detach() for i in range(num_levels)]\n        centerness_pred_list = [centernesses[i][img_id].detach() for i in range(num_levels)]\n        input_meta = img_metas[img_id]\n        det_bboxes = self._get_bboxes_single(cls_score_list, bbox_pred_list, dir_cls_pred_list, depth_cls_pred_list, weight_list, attr_pred_list, centerness_pred_list, mlvl_points, input_meta, cfg, rescale)\n        result_list.append(det_bboxes)\n    return result_list",
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef get_bboxes(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, img_metas, cfg=None, rescale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transform network output for a batch into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level\\n                Has shape (N, num_points * num_classes, H, W)\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level with shape (N, num_points * 4, H, W)\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for each scale level with\\n                shape (N, num_points * 1, H, W)\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cfg (mmcv.Config, optional): Test / postprocessing configuration,\\n                if None, test_cfg would be used. Defaults to None.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to None.\\n\\n        Returns:\\n            list[tuple[Tensor]]: Each item in result_list is a tuple, which\\n                consists of predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    num_levels = len(cls_scores)\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n        bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n        if self.use_direction_classifier:\n            dir_cls_pred_list = [dir_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            dir_cls_pred_list = [cls_scores[i][img_id].new_full([2, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.use_depth_classifier:\n            depth_cls_pred_list = [depth_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            depth_cls_pred_list = [cls_scores[i][img_id].new_full([self.num_depth_cls, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.weight_dim != -1:\n            weight_list = [weights[i][img_id].detach() for i in range(num_levels)]\n        else:\n            weight_list = [cls_scores[i][img_id].new_full([1, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.pred_attrs:\n            attr_pred_list = [attr_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            attr_pred_list = [cls_scores[i][img_id].new_full([self.num_attrs, *cls_scores[i][img_id].shape[1:]], self.attr_background_label).detach() for i in range(num_levels)]\n        centerness_pred_list = [centernesses[i][img_id].detach() for i in range(num_levels)]\n        input_meta = img_metas[img_id]\n        det_bboxes = self._get_bboxes_single(cls_score_list, bbox_pred_list, dir_cls_pred_list, depth_cls_pred_list, weight_list, attr_pred_list, centerness_pred_list, mlvl_points, input_meta, cfg, rescale)\n        result_list.append(det_bboxes)\n    return result_list",
            "@force_fp32(apply_to=('cls_scores', 'bbox_preds', 'dir_cls_preds', 'depth_cls_preds', 'weights', 'attr_preds', 'centernesses'))\ndef get_bboxes(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, img_metas, cfg=None, rescale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transform network output for a batch into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level\\n                Has shape (N, num_points * num_classes, H, W)\\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                level with shape (N, num_points * 4, H, W)\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * 2. (bin = 2)\\n            depth_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on each scale level, each is a 4D-tensor,\\n                the channel number is num_points * self.num_depth_cls.\\n            weights (list[Tensor]): Location-aware weights for each scale\\n                level, each is a 4D-tensor, the channel number is\\n                num_points * self.weight_dim.\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for each scale level with\\n                shape (N, num_points * 1, H, W)\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cfg (mmcv.Config, optional): Test / postprocessing configuration,\\n                if None, test_cfg would be used. Defaults to None.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to None.\\n\\n        Returns:\\n            list[tuple[Tensor]]: Each item in result_list is a tuple, which\\n                consists of predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(depth_cls_preds) == len(weights) == len(centernesses) == len(attr_preds), f'The length of cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, centernesses, andattr_preds: {len(cls_scores)}, {len(bbox_preds)}, {len(dir_cls_preds)}, {len(depth_cls_preds)}, {len(weights)}{len(centernesses)}, {len(attr_preds)} are inconsistent.'\n    num_levels = len(cls_scores)\n    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n    mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n        bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n        if self.use_direction_classifier:\n            dir_cls_pred_list = [dir_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            dir_cls_pred_list = [cls_scores[i][img_id].new_full([2, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.use_depth_classifier:\n            depth_cls_pred_list = [depth_cls_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            depth_cls_pred_list = [cls_scores[i][img_id].new_full([self.num_depth_cls, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.weight_dim != -1:\n            weight_list = [weights[i][img_id].detach() for i in range(num_levels)]\n        else:\n            weight_list = [cls_scores[i][img_id].new_full([1, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n        if self.pred_attrs:\n            attr_pred_list = [attr_preds[i][img_id].detach() for i in range(num_levels)]\n        else:\n            attr_pred_list = [cls_scores[i][img_id].new_full([self.num_attrs, *cls_scores[i][img_id].shape[1:]], self.attr_background_label).detach() for i in range(num_levels)]\n        centerness_pred_list = [centernesses[i][img_id].detach() for i in range(num_levels)]\n        input_meta = img_metas[img_id]\n        det_bboxes = self._get_bboxes_single(cls_score_list, bbox_pred_list, dir_cls_pred_list, depth_cls_pred_list, weight_list, attr_pred_list, centerness_pred_list, mlvl_points, input_meta, cfg, rescale)\n        result_list.append(det_bboxes)\n    return result_list"
        ]
    },
    {
        "func_name": "_get_bboxes_single",
        "original": "def _get_bboxes_single(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points, input_meta, cfg, rescale=False):\n    \"\"\"Transform outputs for a single batch item into bbox predictions.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for a single scale level\n                Has shape (num_points * num_classes, H, W).\n            bbox_preds (list[Tensor]): Box energies / deltas for a single scale\n                level with shape (num_points * bbox_code_size, H, W).\n            dir_cls_preds (list[Tensor]): Box scores for direction class\n                predictions on a single scale level with shape\n                (num_points * 2, H, W)\n            depth_cls_preds (list[Tensor]): Box scores for probabilistic depth\n                predictions on a single scale level with shape\n                (num_points * self.num_depth_cls, H, W)\n            weights (list[Tensor]): Location-aware weight maps on a single\n                scale level with shape (num_points * self.weight_dim, H, W).\n            attr_preds (list[Tensor]): Attribute scores for each scale level\n                Has shape (N, num_points * num_attrs, H, W)\n            centernesses (list[Tensor]): Centerness for a single scale level\n                with shape (num_points, H, W).\n            mlvl_points (list[Tensor]): Box reference for a single scale level\n                with shape (num_total_points, 2).\n            input_meta (dict): Metadata of input image.\n            cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used.\n            rescale (bool, optional): If True, return boxes in original image\n                space. Defaults to False.\n\n        Returns:\n            tuples[Tensor]: Predicted 3D boxes, scores, labels, attributes and\n                2D boxes (if necessary).\n        \"\"\"\n    view = np.array(input_meta['cam2img'])\n    scale_factor = input_meta['scale_factor']\n    cfg = self.test_cfg if cfg is None else cfg\n    assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n    mlvl_centers2d = []\n    mlvl_bboxes = []\n    mlvl_scores = []\n    mlvl_dir_scores = []\n    mlvl_attr_scores = []\n    mlvl_centerness = []\n    mlvl_depth_cls_scores = []\n    mlvl_depth_uncertainty = []\n    mlvl_bboxes2d = None\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = []\n    for (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness, points) in zip(cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points):\n        assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n        scores = cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels).sigmoid()\n        dir_cls_pred = dir_cls_pred.permute(1, 2, 0).reshape(-1, 2)\n        dir_cls_score = torch.max(dir_cls_pred, dim=-1)[1]\n        depth_cls_pred = depth_cls_pred.permute(1, 2, 0).reshape(-1, self.num_depth_cls)\n        depth_cls_score = F.softmax(depth_cls_pred, dim=-1).topk(k=2, dim=-1)[0].mean(dim=-1)\n        if self.weight_dim != -1:\n            weight = weight.permute(1, 2, 0).reshape(-1, self.weight_dim)\n        else:\n            weight = weight.permute(1, 2, 0).reshape(-1, 1)\n        depth_uncertainty = torch.exp(-weight[:, -1])\n        attr_pred = attr_pred.permute(1, 2, 0).reshape(-1, self.num_attrs)\n        attr_score = torch.max(attr_pred, dim=-1)[1]\n        centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n        bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, sum(self.group_reg_dims))\n        bbox_pred3d = bbox_pred[:, :self.bbox_coder.bbox_code_size]\n        if self.pred_bbox2d:\n            bbox_pred2d = bbox_pred[:, -4:]\n        nms_pre = cfg.get('nms_pre', -1)\n        if nms_pre > 0 and scores.shape[0] > nms_pre:\n            merged_scores = scores * centerness[:, None]\n            if self.use_depth_classifier:\n                merged_scores *= depth_cls_score[:, None]\n                if self.weight_dim != -1:\n                    merged_scores *= depth_uncertainty[:, None]\n            (max_scores, _) = merged_scores.max(dim=1)\n            (_, topk_inds) = max_scores.topk(nms_pre)\n            points = points[topk_inds, :]\n            bbox_pred3d = bbox_pred3d[topk_inds, :]\n            scores = scores[topk_inds, :]\n            dir_cls_pred = dir_cls_pred[topk_inds, :]\n            depth_cls_pred = depth_cls_pred[topk_inds, :]\n            centerness = centerness[topk_inds]\n            dir_cls_score = dir_cls_score[topk_inds]\n            depth_cls_score = depth_cls_score[topk_inds]\n            depth_uncertainty = depth_uncertainty[topk_inds]\n            attr_score = attr_score[topk_inds]\n            if self.pred_bbox2d:\n                bbox_pred2d = bbox_pred2d[topk_inds, :]\n        bbox_pred3d[:, :2] = points - bbox_pred3d[:, :2]\n        if rescale:\n            bbox_pred3d[:, :2] /= bbox_pred3d[:, :2].new_tensor(scale_factor)\n            if self.pred_bbox2d:\n                bbox_pred2d /= bbox_pred2d.new_tensor(scale_factor)\n        if self.use_depth_classifier:\n            prob_depth_pred = self.bbox_coder.decode_prob_depth(depth_cls_pred, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            bbox_pred3d[:, 2] = sig_alpha * bbox_pred3d[:, 2] + (1 - sig_alpha) * prob_depth_pred\n        pred_center2d = bbox_pred3d[:, :3].clone()\n        bbox_pred3d[:, :3] = points_img2cam(bbox_pred3d[:, :3], view)\n        mlvl_centers2d.append(pred_center2d)\n        mlvl_bboxes.append(bbox_pred3d)\n        mlvl_scores.append(scores)\n        mlvl_dir_scores.append(dir_cls_score)\n        mlvl_depth_cls_scores.append(depth_cls_score)\n        mlvl_attr_scores.append(attr_score)\n        mlvl_centerness.append(centerness)\n        mlvl_depth_uncertainty.append(depth_uncertainty)\n        if self.pred_bbox2d:\n            bbox_pred2d = distance2bbox(points, bbox_pred2d, max_shape=input_meta['img_shape'])\n            mlvl_bboxes2d.append(bbox_pred2d)\n    mlvl_centers2d = torch.cat(mlvl_centers2d)\n    mlvl_bboxes = torch.cat(mlvl_bboxes)\n    mlvl_dir_scores = torch.cat(mlvl_dir_scores)\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = torch.cat(mlvl_bboxes2d)\n    cam2img = torch.eye(4, dtype=mlvl_centers2d.dtype, device=mlvl_centers2d.device)\n    cam2img[:view.shape[0], :view.shape[1]] = mlvl_centers2d.new_tensor(view)\n    mlvl_bboxes = self.bbox_coder.decode_yaw(mlvl_bboxes, mlvl_centers2d, mlvl_dir_scores, self.dir_offset, cam2img)\n    mlvl_bboxes_for_nms = xywhr2xyxyr(input_meta['box_type_3d'](mlvl_bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).bev)\n    mlvl_scores = torch.cat(mlvl_scores)\n    padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n    mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n    mlvl_attr_scores = torch.cat(mlvl_attr_scores)\n    mlvl_centerness = torch.cat(mlvl_centerness)\n    mlvl_nms_scores = mlvl_scores * mlvl_centerness[:, None]\n    if self.use_depth_classifier:\n        mlvl_depth_cls_scores = torch.cat(mlvl_depth_cls_scores)\n        mlvl_nms_scores *= mlvl_depth_cls_scores[:, None]\n        if self.weight_dim != -1:\n            mlvl_depth_uncertainty = torch.cat(mlvl_depth_uncertainty)\n            mlvl_nms_scores *= mlvl_depth_uncertainty[:, None]\n    results = box3d_multiclass_nms(mlvl_bboxes, mlvl_bboxes_for_nms, mlvl_nms_scores, cfg.score_thr, cfg.max_per_img, cfg, mlvl_dir_scores, mlvl_attr_scores, mlvl_bboxes2d)\n    (bboxes, scores, labels, dir_scores, attrs) = results[0:5]\n    attrs = attrs.to(labels.dtype)\n    bboxes = input_meta['box_type_3d'](bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5))\n    if not self.pred_attrs:\n        attrs = None\n    outputs = (bboxes, scores, labels, attrs)\n    if self.pred_bbox2d:\n        bboxes2d = results[-1]\n        bboxes2d = torch.cat([bboxes2d, scores[:, None]], dim=1)\n        outputs = outputs + (bboxes2d,)\n    return outputs",
        "mutated": [
            "def _get_bboxes_single(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points, input_meta, cfg, rescale=False):\n    if False:\n        i = 10\n    'Transform outputs for a single batch item into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for a single scale level\\n                Has shape (num_points * num_classes, H, W).\\n            bbox_preds (list[Tensor]): Box energies / deltas for a single scale\\n                level with shape (num_points * bbox_code_size, H, W).\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on a single scale level with shape\\n                (num_points * 2, H, W)\\n            depth_cls_preds (list[Tensor]): Box scores for probabilistic depth\\n                predictions on a single scale level with shape\\n                (num_points * self.num_depth_cls, H, W)\\n            weights (list[Tensor]): Location-aware weight maps on a single\\n                scale level with shape (num_points * self.weight_dim, H, W).\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for a single scale level\\n                with shape (num_points, H, W).\\n            mlvl_points (list[Tensor]): Box reference for a single scale level\\n                with shape (num_total_points, 2).\\n            input_meta (dict): Metadata of input image.\\n            cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to False.\\n\\n        Returns:\\n            tuples[Tensor]: Predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    view = np.array(input_meta['cam2img'])\n    scale_factor = input_meta['scale_factor']\n    cfg = self.test_cfg if cfg is None else cfg\n    assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n    mlvl_centers2d = []\n    mlvl_bboxes = []\n    mlvl_scores = []\n    mlvl_dir_scores = []\n    mlvl_attr_scores = []\n    mlvl_centerness = []\n    mlvl_depth_cls_scores = []\n    mlvl_depth_uncertainty = []\n    mlvl_bboxes2d = None\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = []\n    for (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness, points) in zip(cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points):\n        assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n        scores = cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels).sigmoid()\n        dir_cls_pred = dir_cls_pred.permute(1, 2, 0).reshape(-1, 2)\n        dir_cls_score = torch.max(dir_cls_pred, dim=-1)[1]\n        depth_cls_pred = depth_cls_pred.permute(1, 2, 0).reshape(-1, self.num_depth_cls)\n        depth_cls_score = F.softmax(depth_cls_pred, dim=-1).topk(k=2, dim=-1)[0].mean(dim=-1)\n        if self.weight_dim != -1:\n            weight = weight.permute(1, 2, 0).reshape(-1, self.weight_dim)\n        else:\n            weight = weight.permute(1, 2, 0).reshape(-1, 1)\n        depth_uncertainty = torch.exp(-weight[:, -1])\n        attr_pred = attr_pred.permute(1, 2, 0).reshape(-1, self.num_attrs)\n        attr_score = torch.max(attr_pred, dim=-1)[1]\n        centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n        bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, sum(self.group_reg_dims))\n        bbox_pred3d = bbox_pred[:, :self.bbox_coder.bbox_code_size]\n        if self.pred_bbox2d:\n            bbox_pred2d = bbox_pred[:, -4:]\n        nms_pre = cfg.get('nms_pre', -1)\n        if nms_pre > 0 and scores.shape[0] > nms_pre:\n            merged_scores = scores * centerness[:, None]\n            if self.use_depth_classifier:\n                merged_scores *= depth_cls_score[:, None]\n                if self.weight_dim != -1:\n                    merged_scores *= depth_uncertainty[:, None]\n            (max_scores, _) = merged_scores.max(dim=1)\n            (_, topk_inds) = max_scores.topk(nms_pre)\n            points = points[topk_inds, :]\n            bbox_pred3d = bbox_pred3d[topk_inds, :]\n            scores = scores[topk_inds, :]\n            dir_cls_pred = dir_cls_pred[topk_inds, :]\n            depth_cls_pred = depth_cls_pred[topk_inds, :]\n            centerness = centerness[topk_inds]\n            dir_cls_score = dir_cls_score[topk_inds]\n            depth_cls_score = depth_cls_score[topk_inds]\n            depth_uncertainty = depth_uncertainty[topk_inds]\n            attr_score = attr_score[topk_inds]\n            if self.pred_bbox2d:\n                bbox_pred2d = bbox_pred2d[topk_inds, :]\n        bbox_pred3d[:, :2] = points - bbox_pred3d[:, :2]\n        if rescale:\n            bbox_pred3d[:, :2] /= bbox_pred3d[:, :2].new_tensor(scale_factor)\n            if self.pred_bbox2d:\n                bbox_pred2d /= bbox_pred2d.new_tensor(scale_factor)\n        if self.use_depth_classifier:\n            prob_depth_pred = self.bbox_coder.decode_prob_depth(depth_cls_pred, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            bbox_pred3d[:, 2] = sig_alpha * bbox_pred3d[:, 2] + (1 - sig_alpha) * prob_depth_pred\n        pred_center2d = bbox_pred3d[:, :3].clone()\n        bbox_pred3d[:, :3] = points_img2cam(bbox_pred3d[:, :3], view)\n        mlvl_centers2d.append(pred_center2d)\n        mlvl_bboxes.append(bbox_pred3d)\n        mlvl_scores.append(scores)\n        mlvl_dir_scores.append(dir_cls_score)\n        mlvl_depth_cls_scores.append(depth_cls_score)\n        mlvl_attr_scores.append(attr_score)\n        mlvl_centerness.append(centerness)\n        mlvl_depth_uncertainty.append(depth_uncertainty)\n        if self.pred_bbox2d:\n            bbox_pred2d = distance2bbox(points, bbox_pred2d, max_shape=input_meta['img_shape'])\n            mlvl_bboxes2d.append(bbox_pred2d)\n    mlvl_centers2d = torch.cat(mlvl_centers2d)\n    mlvl_bboxes = torch.cat(mlvl_bboxes)\n    mlvl_dir_scores = torch.cat(mlvl_dir_scores)\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = torch.cat(mlvl_bboxes2d)\n    cam2img = torch.eye(4, dtype=mlvl_centers2d.dtype, device=mlvl_centers2d.device)\n    cam2img[:view.shape[0], :view.shape[1]] = mlvl_centers2d.new_tensor(view)\n    mlvl_bboxes = self.bbox_coder.decode_yaw(mlvl_bboxes, mlvl_centers2d, mlvl_dir_scores, self.dir_offset, cam2img)\n    mlvl_bboxes_for_nms = xywhr2xyxyr(input_meta['box_type_3d'](mlvl_bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).bev)\n    mlvl_scores = torch.cat(mlvl_scores)\n    padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n    mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n    mlvl_attr_scores = torch.cat(mlvl_attr_scores)\n    mlvl_centerness = torch.cat(mlvl_centerness)\n    mlvl_nms_scores = mlvl_scores * mlvl_centerness[:, None]\n    if self.use_depth_classifier:\n        mlvl_depth_cls_scores = torch.cat(mlvl_depth_cls_scores)\n        mlvl_nms_scores *= mlvl_depth_cls_scores[:, None]\n        if self.weight_dim != -1:\n            mlvl_depth_uncertainty = torch.cat(mlvl_depth_uncertainty)\n            mlvl_nms_scores *= mlvl_depth_uncertainty[:, None]\n    results = box3d_multiclass_nms(mlvl_bboxes, mlvl_bboxes_for_nms, mlvl_nms_scores, cfg.score_thr, cfg.max_per_img, cfg, mlvl_dir_scores, mlvl_attr_scores, mlvl_bboxes2d)\n    (bboxes, scores, labels, dir_scores, attrs) = results[0:5]\n    attrs = attrs.to(labels.dtype)\n    bboxes = input_meta['box_type_3d'](bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5))\n    if not self.pred_attrs:\n        attrs = None\n    outputs = (bboxes, scores, labels, attrs)\n    if self.pred_bbox2d:\n        bboxes2d = results[-1]\n        bboxes2d = torch.cat([bboxes2d, scores[:, None]], dim=1)\n        outputs = outputs + (bboxes2d,)\n    return outputs",
            "def _get_bboxes_single(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points, input_meta, cfg, rescale=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transform outputs for a single batch item into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for a single scale level\\n                Has shape (num_points * num_classes, H, W).\\n            bbox_preds (list[Tensor]): Box energies / deltas for a single scale\\n                level with shape (num_points * bbox_code_size, H, W).\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on a single scale level with shape\\n                (num_points * 2, H, W)\\n            depth_cls_preds (list[Tensor]): Box scores for probabilistic depth\\n                predictions on a single scale level with shape\\n                (num_points * self.num_depth_cls, H, W)\\n            weights (list[Tensor]): Location-aware weight maps on a single\\n                scale level with shape (num_points * self.weight_dim, H, W).\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for a single scale level\\n                with shape (num_points, H, W).\\n            mlvl_points (list[Tensor]): Box reference for a single scale level\\n                with shape (num_total_points, 2).\\n            input_meta (dict): Metadata of input image.\\n            cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to False.\\n\\n        Returns:\\n            tuples[Tensor]: Predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    view = np.array(input_meta['cam2img'])\n    scale_factor = input_meta['scale_factor']\n    cfg = self.test_cfg if cfg is None else cfg\n    assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n    mlvl_centers2d = []\n    mlvl_bboxes = []\n    mlvl_scores = []\n    mlvl_dir_scores = []\n    mlvl_attr_scores = []\n    mlvl_centerness = []\n    mlvl_depth_cls_scores = []\n    mlvl_depth_uncertainty = []\n    mlvl_bboxes2d = None\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = []\n    for (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness, points) in zip(cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points):\n        assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n        scores = cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels).sigmoid()\n        dir_cls_pred = dir_cls_pred.permute(1, 2, 0).reshape(-1, 2)\n        dir_cls_score = torch.max(dir_cls_pred, dim=-1)[1]\n        depth_cls_pred = depth_cls_pred.permute(1, 2, 0).reshape(-1, self.num_depth_cls)\n        depth_cls_score = F.softmax(depth_cls_pred, dim=-1).topk(k=2, dim=-1)[0].mean(dim=-1)\n        if self.weight_dim != -1:\n            weight = weight.permute(1, 2, 0).reshape(-1, self.weight_dim)\n        else:\n            weight = weight.permute(1, 2, 0).reshape(-1, 1)\n        depth_uncertainty = torch.exp(-weight[:, -1])\n        attr_pred = attr_pred.permute(1, 2, 0).reshape(-1, self.num_attrs)\n        attr_score = torch.max(attr_pred, dim=-1)[1]\n        centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n        bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, sum(self.group_reg_dims))\n        bbox_pred3d = bbox_pred[:, :self.bbox_coder.bbox_code_size]\n        if self.pred_bbox2d:\n            bbox_pred2d = bbox_pred[:, -4:]\n        nms_pre = cfg.get('nms_pre', -1)\n        if nms_pre > 0 and scores.shape[0] > nms_pre:\n            merged_scores = scores * centerness[:, None]\n            if self.use_depth_classifier:\n                merged_scores *= depth_cls_score[:, None]\n                if self.weight_dim != -1:\n                    merged_scores *= depth_uncertainty[:, None]\n            (max_scores, _) = merged_scores.max(dim=1)\n            (_, topk_inds) = max_scores.topk(nms_pre)\n            points = points[topk_inds, :]\n            bbox_pred3d = bbox_pred3d[topk_inds, :]\n            scores = scores[topk_inds, :]\n            dir_cls_pred = dir_cls_pred[topk_inds, :]\n            depth_cls_pred = depth_cls_pred[topk_inds, :]\n            centerness = centerness[topk_inds]\n            dir_cls_score = dir_cls_score[topk_inds]\n            depth_cls_score = depth_cls_score[topk_inds]\n            depth_uncertainty = depth_uncertainty[topk_inds]\n            attr_score = attr_score[topk_inds]\n            if self.pred_bbox2d:\n                bbox_pred2d = bbox_pred2d[topk_inds, :]\n        bbox_pred3d[:, :2] = points - bbox_pred3d[:, :2]\n        if rescale:\n            bbox_pred3d[:, :2] /= bbox_pred3d[:, :2].new_tensor(scale_factor)\n            if self.pred_bbox2d:\n                bbox_pred2d /= bbox_pred2d.new_tensor(scale_factor)\n        if self.use_depth_classifier:\n            prob_depth_pred = self.bbox_coder.decode_prob_depth(depth_cls_pred, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            bbox_pred3d[:, 2] = sig_alpha * bbox_pred3d[:, 2] + (1 - sig_alpha) * prob_depth_pred\n        pred_center2d = bbox_pred3d[:, :3].clone()\n        bbox_pred3d[:, :3] = points_img2cam(bbox_pred3d[:, :3], view)\n        mlvl_centers2d.append(pred_center2d)\n        mlvl_bboxes.append(bbox_pred3d)\n        mlvl_scores.append(scores)\n        mlvl_dir_scores.append(dir_cls_score)\n        mlvl_depth_cls_scores.append(depth_cls_score)\n        mlvl_attr_scores.append(attr_score)\n        mlvl_centerness.append(centerness)\n        mlvl_depth_uncertainty.append(depth_uncertainty)\n        if self.pred_bbox2d:\n            bbox_pred2d = distance2bbox(points, bbox_pred2d, max_shape=input_meta['img_shape'])\n            mlvl_bboxes2d.append(bbox_pred2d)\n    mlvl_centers2d = torch.cat(mlvl_centers2d)\n    mlvl_bboxes = torch.cat(mlvl_bboxes)\n    mlvl_dir_scores = torch.cat(mlvl_dir_scores)\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = torch.cat(mlvl_bboxes2d)\n    cam2img = torch.eye(4, dtype=mlvl_centers2d.dtype, device=mlvl_centers2d.device)\n    cam2img[:view.shape[0], :view.shape[1]] = mlvl_centers2d.new_tensor(view)\n    mlvl_bboxes = self.bbox_coder.decode_yaw(mlvl_bboxes, mlvl_centers2d, mlvl_dir_scores, self.dir_offset, cam2img)\n    mlvl_bboxes_for_nms = xywhr2xyxyr(input_meta['box_type_3d'](mlvl_bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).bev)\n    mlvl_scores = torch.cat(mlvl_scores)\n    padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n    mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n    mlvl_attr_scores = torch.cat(mlvl_attr_scores)\n    mlvl_centerness = torch.cat(mlvl_centerness)\n    mlvl_nms_scores = mlvl_scores * mlvl_centerness[:, None]\n    if self.use_depth_classifier:\n        mlvl_depth_cls_scores = torch.cat(mlvl_depth_cls_scores)\n        mlvl_nms_scores *= mlvl_depth_cls_scores[:, None]\n        if self.weight_dim != -1:\n            mlvl_depth_uncertainty = torch.cat(mlvl_depth_uncertainty)\n            mlvl_nms_scores *= mlvl_depth_uncertainty[:, None]\n    results = box3d_multiclass_nms(mlvl_bboxes, mlvl_bboxes_for_nms, mlvl_nms_scores, cfg.score_thr, cfg.max_per_img, cfg, mlvl_dir_scores, mlvl_attr_scores, mlvl_bboxes2d)\n    (bboxes, scores, labels, dir_scores, attrs) = results[0:5]\n    attrs = attrs.to(labels.dtype)\n    bboxes = input_meta['box_type_3d'](bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5))\n    if not self.pred_attrs:\n        attrs = None\n    outputs = (bboxes, scores, labels, attrs)\n    if self.pred_bbox2d:\n        bboxes2d = results[-1]\n        bboxes2d = torch.cat([bboxes2d, scores[:, None]], dim=1)\n        outputs = outputs + (bboxes2d,)\n    return outputs",
            "def _get_bboxes_single(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points, input_meta, cfg, rescale=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transform outputs for a single batch item into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for a single scale level\\n                Has shape (num_points * num_classes, H, W).\\n            bbox_preds (list[Tensor]): Box energies / deltas for a single scale\\n                level with shape (num_points * bbox_code_size, H, W).\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on a single scale level with shape\\n                (num_points * 2, H, W)\\n            depth_cls_preds (list[Tensor]): Box scores for probabilistic depth\\n                predictions on a single scale level with shape\\n                (num_points * self.num_depth_cls, H, W)\\n            weights (list[Tensor]): Location-aware weight maps on a single\\n                scale level with shape (num_points * self.weight_dim, H, W).\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for a single scale level\\n                with shape (num_points, H, W).\\n            mlvl_points (list[Tensor]): Box reference for a single scale level\\n                with shape (num_total_points, 2).\\n            input_meta (dict): Metadata of input image.\\n            cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to False.\\n\\n        Returns:\\n            tuples[Tensor]: Predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    view = np.array(input_meta['cam2img'])\n    scale_factor = input_meta['scale_factor']\n    cfg = self.test_cfg if cfg is None else cfg\n    assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n    mlvl_centers2d = []\n    mlvl_bboxes = []\n    mlvl_scores = []\n    mlvl_dir_scores = []\n    mlvl_attr_scores = []\n    mlvl_centerness = []\n    mlvl_depth_cls_scores = []\n    mlvl_depth_uncertainty = []\n    mlvl_bboxes2d = None\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = []\n    for (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness, points) in zip(cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points):\n        assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n        scores = cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels).sigmoid()\n        dir_cls_pred = dir_cls_pred.permute(1, 2, 0).reshape(-1, 2)\n        dir_cls_score = torch.max(dir_cls_pred, dim=-1)[1]\n        depth_cls_pred = depth_cls_pred.permute(1, 2, 0).reshape(-1, self.num_depth_cls)\n        depth_cls_score = F.softmax(depth_cls_pred, dim=-1).topk(k=2, dim=-1)[0].mean(dim=-1)\n        if self.weight_dim != -1:\n            weight = weight.permute(1, 2, 0).reshape(-1, self.weight_dim)\n        else:\n            weight = weight.permute(1, 2, 0).reshape(-1, 1)\n        depth_uncertainty = torch.exp(-weight[:, -1])\n        attr_pred = attr_pred.permute(1, 2, 0).reshape(-1, self.num_attrs)\n        attr_score = torch.max(attr_pred, dim=-1)[1]\n        centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n        bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, sum(self.group_reg_dims))\n        bbox_pred3d = bbox_pred[:, :self.bbox_coder.bbox_code_size]\n        if self.pred_bbox2d:\n            bbox_pred2d = bbox_pred[:, -4:]\n        nms_pre = cfg.get('nms_pre', -1)\n        if nms_pre > 0 and scores.shape[0] > nms_pre:\n            merged_scores = scores * centerness[:, None]\n            if self.use_depth_classifier:\n                merged_scores *= depth_cls_score[:, None]\n                if self.weight_dim != -1:\n                    merged_scores *= depth_uncertainty[:, None]\n            (max_scores, _) = merged_scores.max(dim=1)\n            (_, topk_inds) = max_scores.topk(nms_pre)\n            points = points[topk_inds, :]\n            bbox_pred3d = bbox_pred3d[topk_inds, :]\n            scores = scores[topk_inds, :]\n            dir_cls_pred = dir_cls_pred[topk_inds, :]\n            depth_cls_pred = depth_cls_pred[topk_inds, :]\n            centerness = centerness[topk_inds]\n            dir_cls_score = dir_cls_score[topk_inds]\n            depth_cls_score = depth_cls_score[topk_inds]\n            depth_uncertainty = depth_uncertainty[topk_inds]\n            attr_score = attr_score[topk_inds]\n            if self.pred_bbox2d:\n                bbox_pred2d = bbox_pred2d[topk_inds, :]\n        bbox_pred3d[:, :2] = points - bbox_pred3d[:, :2]\n        if rescale:\n            bbox_pred3d[:, :2] /= bbox_pred3d[:, :2].new_tensor(scale_factor)\n            if self.pred_bbox2d:\n                bbox_pred2d /= bbox_pred2d.new_tensor(scale_factor)\n        if self.use_depth_classifier:\n            prob_depth_pred = self.bbox_coder.decode_prob_depth(depth_cls_pred, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            bbox_pred3d[:, 2] = sig_alpha * bbox_pred3d[:, 2] + (1 - sig_alpha) * prob_depth_pred\n        pred_center2d = bbox_pred3d[:, :3].clone()\n        bbox_pred3d[:, :3] = points_img2cam(bbox_pred3d[:, :3], view)\n        mlvl_centers2d.append(pred_center2d)\n        mlvl_bboxes.append(bbox_pred3d)\n        mlvl_scores.append(scores)\n        mlvl_dir_scores.append(dir_cls_score)\n        mlvl_depth_cls_scores.append(depth_cls_score)\n        mlvl_attr_scores.append(attr_score)\n        mlvl_centerness.append(centerness)\n        mlvl_depth_uncertainty.append(depth_uncertainty)\n        if self.pred_bbox2d:\n            bbox_pred2d = distance2bbox(points, bbox_pred2d, max_shape=input_meta['img_shape'])\n            mlvl_bboxes2d.append(bbox_pred2d)\n    mlvl_centers2d = torch.cat(mlvl_centers2d)\n    mlvl_bboxes = torch.cat(mlvl_bboxes)\n    mlvl_dir_scores = torch.cat(mlvl_dir_scores)\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = torch.cat(mlvl_bboxes2d)\n    cam2img = torch.eye(4, dtype=mlvl_centers2d.dtype, device=mlvl_centers2d.device)\n    cam2img[:view.shape[0], :view.shape[1]] = mlvl_centers2d.new_tensor(view)\n    mlvl_bboxes = self.bbox_coder.decode_yaw(mlvl_bboxes, mlvl_centers2d, mlvl_dir_scores, self.dir_offset, cam2img)\n    mlvl_bboxes_for_nms = xywhr2xyxyr(input_meta['box_type_3d'](mlvl_bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).bev)\n    mlvl_scores = torch.cat(mlvl_scores)\n    padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n    mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n    mlvl_attr_scores = torch.cat(mlvl_attr_scores)\n    mlvl_centerness = torch.cat(mlvl_centerness)\n    mlvl_nms_scores = mlvl_scores * mlvl_centerness[:, None]\n    if self.use_depth_classifier:\n        mlvl_depth_cls_scores = torch.cat(mlvl_depth_cls_scores)\n        mlvl_nms_scores *= mlvl_depth_cls_scores[:, None]\n        if self.weight_dim != -1:\n            mlvl_depth_uncertainty = torch.cat(mlvl_depth_uncertainty)\n            mlvl_nms_scores *= mlvl_depth_uncertainty[:, None]\n    results = box3d_multiclass_nms(mlvl_bboxes, mlvl_bboxes_for_nms, mlvl_nms_scores, cfg.score_thr, cfg.max_per_img, cfg, mlvl_dir_scores, mlvl_attr_scores, mlvl_bboxes2d)\n    (bboxes, scores, labels, dir_scores, attrs) = results[0:5]\n    attrs = attrs.to(labels.dtype)\n    bboxes = input_meta['box_type_3d'](bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5))\n    if not self.pred_attrs:\n        attrs = None\n    outputs = (bboxes, scores, labels, attrs)\n    if self.pred_bbox2d:\n        bboxes2d = results[-1]\n        bboxes2d = torch.cat([bboxes2d, scores[:, None]], dim=1)\n        outputs = outputs + (bboxes2d,)\n    return outputs",
            "def _get_bboxes_single(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points, input_meta, cfg, rescale=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transform outputs for a single batch item into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for a single scale level\\n                Has shape (num_points * num_classes, H, W).\\n            bbox_preds (list[Tensor]): Box energies / deltas for a single scale\\n                level with shape (num_points * bbox_code_size, H, W).\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on a single scale level with shape\\n                (num_points * 2, H, W)\\n            depth_cls_preds (list[Tensor]): Box scores for probabilistic depth\\n                predictions on a single scale level with shape\\n                (num_points * self.num_depth_cls, H, W)\\n            weights (list[Tensor]): Location-aware weight maps on a single\\n                scale level with shape (num_points * self.weight_dim, H, W).\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for a single scale level\\n                with shape (num_points, H, W).\\n            mlvl_points (list[Tensor]): Box reference for a single scale level\\n                with shape (num_total_points, 2).\\n            input_meta (dict): Metadata of input image.\\n            cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to False.\\n\\n        Returns:\\n            tuples[Tensor]: Predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    view = np.array(input_meta['cam2img'])\n    scale_factor = input_meta['scale_factor']\n    cfg = self.test_cfg if cfg is None else cfg\n    assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n    mlvl_centers2d = []\n    mlvl_bboxes = []\n    mlvl_scores = []\n    mlvl_dir_scores = []\n    mlvl_attr_scores = []\n    mlvl_centerness = []\n    mlvl_depth_cls_scores = []\n    mlvl_depth_uncertainty = []\n    mlvl_bboxes2d = None\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = []\n    for (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness, points) in zip(cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points):\n        assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n        scores = cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels).sigmoid()\n        dir_cls_pred = dir_cls_pred.permute(1, 2, 0).reshape(-1, 2)\n        dir_cls_score = torch.max(dir_cls_pred, dim=-1)[1]\n        depth_cls_pred = depth_cls_pred.permute(1, 2, 0).reshape(-1, self.num_depth_cls)\n        depth_cls_score = F.softmax(depth_cls_pred, dim=-1).topk(k=2, dim=-1)[0].mean(dim=-1)\n        if self.weight_dim != -1:\n            weight = weight.permute(1, 2, 0).reshape(-1, self.weight_dim)\n        else:\n            weight = weight.permute(1, 2, 0).reshape(-1, 1)\n        depth_uncertainty = torch.exp(-weight[:, -1])\n        attr_pred = attr_pred.permute(1, 2, 0).reshape(-1, self.num_attrs)\n        attr_score = torch.max(attr_pred, dim=-1)[1]\n        centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n        bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, sum(self.group_reg_dims))\n        bbox_pred3d = bbox_pred[:, :self.bbox_coder.bbox_code_size]\n        if self.pred_bbox2d:\n            bbox_pred2d = bbox_pred[:, -4:]\n        nms_pre = cfg.get('nms_pre', -1)\n        if nms_pre > 0 and scores.shape[0] > nms_pre:\n            merged_scores = scores * centerness[:, None]\n            if self.use_depth_classifier:\n                merged_scores *= depth_cls_score[:, None]\n                if self.weight_dim != -1:\n                    merged_scores *= depth_uncertainty[:, None]\n            (max_scores, _) = merged_scores.max(dim=1)\n            (_, topk_inds) = max_scores.topk(nms_pre)\n            points = points[topk_inds, :]\n            bbox_pred3d = bbox_pred3d[topk_inds, :]\n            scores = scores[topk_inds, :]\n            dir_cls_pred = dir_cls_pred[topk_inds, :]\n            depth_cls_pred = depth_cls_pred[topk_inds, :]\n            centerness = centerness[topk_inds]\n            dir_cls_score = dir_cls_score[topk_inds]\n            depth_cls_score = depth_cls_score[topk_inds]\n            depth_uncertainty = depth_uncertainty[topk_inds]\n            attr_score = attr_score[topk_inds]\n            if self.pred_bbox2d:\n                bbox_pred2d = bbox_pred2d[topk_inds, :]\n        bbox_pred3d[:, :2] = points - bbox_pred3d[:, :2]\n        if rescale:\n            bbox_pred3d[:, :2] /= bbox_pred3d[:, :2].new_tensor(scale_factor)\n            if self.pred_bbox2d:\n                bbox_pred2d /= bbox_pred2d.new_tensor(scale_factor)\n        if self.use_depth_classifier:\n            prob_depth_pred = self.bbox_coder.decode_prob_depth(depth_cls_pred, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            bbox_pred3d[:, 2] = sig_alpha * bbox_pred3d[:, 2] + (1 - sig_alpha) * prob_depth_pred\n        pred_center2d = bbox_pred3d[:, :3].clone()\n        bbox_pred3d[:, :3] = points_img2cam(bbox_pred3d[:, :3], view)\n        mlvl_centers2d.append(pred_center2d)\n        mlvl_bboxes.append(bbox_pred3d)\n        mlvl_scores.append(scores)\n        mlvl_dir_scores.append(dir_cls_score)\n        mlvl_depth_cls_scores.append(depth_cls_score)\n        mlvl_attr_scores.append(attr_score)\n        mlvl_centerness.append(centerness)\n        mlvl_depth_uncertainty.append(depth_uncertainty)\n        if self.pred_bbox2d:\n            bbox_pred2d = distance2bbox(points, bbox_pred2d, max_shape=input_meta['img_shape'])\n            mlvl_bboxes2d.append(bbox_pred2d)\n    mlvl_centers2d = torch.cat(mlvl_centers2d)\n    mlvl_bboxes = torch.cat(mlvl_bboxes)\n    mlvl_dir_scores = torch.cat(mlvl_dir_scores)\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = torch.cat(mlvl_bboxes2d)\n    cam2img = torch.eye(4, dtype=mlvl_centers2d.dtype, device=mlvl_centers2d.device)\n    cam2img[:view.shape[0], :view.shape[1]] = mlvl_centers2d.new_tensor(view)\n    mlvl_bboxes = self.bbox_coder.decode_yaw(mlvl_bboxes, mlvl_centers2d, mlvl_dir_scores, self.dir_offset, cam2img)\n    mlvl_bboxes_for_nms = xywhr2xyxyr(input_meta['box_type_3d'](mlvl_bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).bev)\n    mlvl_scores = torch.cat(mlvl_scores)\n    padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n    mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n    mlvl_attr_scores = torch.cat(mlvl_attr_scores)\n    mlvl_centerness = torch.cat(mlvl_centerness)\n    mlvl_nms_scores = mlvl_scores * mlvl_centerness[:, None]\n    if self.use_depth_classifier:\n        mlvl_depth_cls_scores = torch.cat(mlvl_depth_cls_scores)\n        mlvl_nms_scores *= mlvl_depth_cls_scores[:, None]\n        if self.weight_dim != -1:\n            mlvl_depth_uncertainty = torch.cat(mlvl_depth_uncertainty)\n            mlvl_nms_scores *= mlvl_depth_uncertainty[:, None]\n    results = box3d_multiclass_nms(mlvl_bboxes, mlvl_bboxes_for_nms, mlvl_nms_scores, cfg.score_thr, cfg.max_per_img, cfg, mlvl_dir_scores, mlvl_attr_scores, mlvl_bboxes2d)\n    (bboxes, scores, labels, dir_scores, attrs) = results[0:5]\n    attrs = attrs.to(labels.dtype)\n    bboxes = input_meta['box_type_3d'](bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5))\n    if not self.pred_attrs:\n        attrs = None\n    outputs = (bboxes, scores, labels, attrs)\n    if self.pred_bbox2d:\n        bboxes2d = results[-1]\n        bboxes2d = torch.cat([bboxes2d, scores[:, None]], dim=1)\n        outputs = outputs + (bboxes2d,)\n    return outputs",
            "def _get_bboxes_single(self, cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points, input_meta, cfg, rescale=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transform outputs for a single batch item into bbox predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for a single scale level\\n                Has shape (num_points * num_classes, H, W).\\n            bbox_preds (list[Tensor]): Box energies / deltas for a single scale\\n                level with shape (num_points * bbox_code_size, H, W).\\n            dir_cls_preds (list[Tensor]): Box scores for direction class\\n                predictions on a single scale level with shape\\n                (num_points * 2, H, W)\\n            depth_cls_preds (list[Tensor]): Box scores for probabilistic depth\\n                predictions on a single scale level with shape\\n                (num_points * self.num_depth_cls, H, W)\\n            weights (list[Tensor]): Location-aware weight maps on a single\\n                scale level with shape (num_points * self.weight_dim, H, W).\\n            attr_preds (list[Tensor]): Attribute scores for each scale level\\n                Has shape (N, num_points * num_attrs, H, W)\\n            centernesses (list[Tensor]): Centerness for a single scale level\\n                with shape (num_points, H, W).\\n            mlvl_points (list[Tensor]): Box reference for a single scale level\\n                with shape (num_total_points, 2).\\n            input_meta (dict): Metadata of input image.\\n            cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used.\\n            rescale (bool, optional): If True, return boxes in original image\\n                space. Defaults to False.\\n\\n        Returns:\\n            tuples[Tensor]: Predicted 3D boxes, scores, labels, attributes and\\n                2D boxes (if necessary).\\n        '\n    view = np.array(input_meta['cam2img'])\n    scale_factor = input_meta['scale_factor']\n    cfg = self.test_cfg if cfg is None else cfg\n    assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n    mlvl_centers2d = []\n    mlvl_bboxes = []\n    mlvl_scores = []\n    mlvl_dir_scores = []\n    mlvl_attr_scores = []\n    mlvl_centerness = []\n    mlvl_depth_cls_scores = []\n    mlvl_depth_uncertainty = []\n    mlvl_bboxes2d = None\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = []\n    for (cls_score, bbox_pred, dir_cls_pred, depth_cls_pred, weight, attr_pred, centerness, points) in zip(cls_scores, bbox_preds, dir_cls_preds, depth_cls_preds, weights, attr_preds, centernesses, mlvl_points):\n        assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n        scores = cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels).sigmoid()\n        dir_cls_pred = dir_cls_pred.permute(1, 2, 0).reshape(-1, 2)\n        dir_cls_score = torch.max(dir_cls_pred, dim=-1)[1]\n        depth_cls_pred = depth_cls_pred.permute(1, 2, 0).reshape(-1, self.num_depth_cls)\n        depth_cls_score = F.softmax(depth_cls_pred, dim=-1).topk(k=2, dim=-1)[0].mean(dim=-1)\n        if self.weight_dim != -1:\n            weight = weight.permute(1, 2, 0).reshape(-1, self.weight_dim)\n        else:\n            weight = weight.permute(1, 2, 0).reshape(-1, 1)\n        depth_uncertainty = torch.exp(-weight[:, -1])\n        attr_pred = attr_pred.permute(1, 2, 0).reshape(-1, self.num_attrs)\n        attr_score = torch.max(attr_pred, dim=-1)[1]\n        centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n        bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, sum(self.group_reg_dims))\n        bbox_pred3d = bbox_pred[:, :self.bbox_coder.bbox_code_size]\n        if self.pred_bbox2d:\n            bbox_pred2d = bbox_pred[:, -4:]\n        nms_pre = cfg.get('nms_pre', -1)\n        if nms_pre > 0 and scores.shape[0] > nms_pre:\n            merged_scores = scores * centerness[:, None]\n            if self.use_depth_classifier:\n                merged_scores *= depth_cls_score[:, None]\n                if self.weight_dim != -1:\n                    merged_scores *= depth_uncertainty[:, None]\n            (max_scores, _) = merged_scores.max(dim=1)\n            (_, topk_inds) = max_scores.topk(nms_pre)\n            points = points[topk_inds, :]\n            bbox_pred3d = bbox_pred3d[topk_inds, :]\n            scores = scores[topk_inds, :]\n            dir_cls_pred = dir_cls_pred[topk_inds, :]\n            depth_cls_pred = depth_cls_pred[topk_inds, :]\n            centerness = centerness[topk_inds]\n            dir_cls_score = dir_cls_score[topk_inds]\n            depth_cls_score = depth_cls_score[topk_inds]\n            depth_uncertainty = depth_uncertainty[topk_inds]\n            attr_score = attr_score[topk_inds]\n            if self.pred_bbox2d:\n                bbox_pred2d = bbox_pred2d[topk_inds, :]\n        bbox_pred3d[:, :2] = points - bbox_pred3d[:, :2]\n        if rescale:\n            bbox_pred3d[:, :2] /= bbox_pred3d[:, :2].new_tensor(scale_factor)\n            if self.pred_bbox2d:\n                bbox_pred2d /= bbox_pred2d.new_tensor(scale_factor)\n        if self.use_depth_classifier:\n            prob_depth_pred = self.bbox_coder.decode_prob_depth(depth_cls_pred, self.depth_range, self.depth_unit, self.division, self.num_depth_cls)\n            sig_alpha = torch.sigmoid(self.fuse_lambda)\n            bbox_pred3d[:, 2] = sig_alpha * bbox_pred3d[:, 2] + (1 - sig_alpha) * prob_depth_pred\n        pred_center2d = bbox_pred3d[:, :3].clone()\n        bbox_pred3d[:, :3] = points_img2cam(bbox_pred3d[:, :3], view)\n        mlvl_centers2d.append(pred_center2d)\n        mlvl_bboxes.append(bbox_pred3d)\n        mlvl_scores.append(scores)\n        mlvl_dir_scores.append(dir_cls_score)\n        mlvl_depth_cls_scores.append(depth_cls_score)\n        mlvl_attr_scores.append(attr_score)\n        mlvl_centerness.append(centerness)\n        mlvl_depth_uncertainty.append(depth_uncertainty)\n        if self.pred_bbox2d:\n            bbox_pred2d = distance2bbox(points, bbox_pred2d, max_shape=input_meta['img_shape'])\n            mlvl_bboxes2d.append(bbox_pred2d)\n    mlvl_centers2d = torch.cat(mlvl_centers2d)\n    mlvl_bboxes = torch.cat(mlvl_bboxes)\n    mlvl_dir_scores = torch.cat(mlvl_dir_scores)\n    if self.pred_bbox2d:\n        mlvl_bboxes2d = torch.cat(mlvl_bboxes2d)\n    cam2img = torch.eye(4, dtype=mlvl_centers2d.dtype, device=mlvl_centers2d.device)\n    cam2img[:view.shape[0], :view.shape[1]] = mlvl_centers2d.new_tensor(view)\n    mlvl_bboxes = self.bbox_coder.decode_yaw(mlvl_bboxes, mlvl_centers2d, mlvl_dir_scores, self.dir_offset, cam2img)\n    mlvl_bboxes_for_nms = xywhr2xyxyr(input_meta['box_type_3d'](mlvl_bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5)).bev)\n    mlvl_scores = torch.cat(mlvl_scores)\n    padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n    mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n    mlvl_attr_scores = torch.cat(mlvl_attr_scores)\n    mlvl_centerness = torch.cat(mlvl_centerness)\n    mlvl_nms_scores = mlvl_scores * mlvl_centerness[:, None]\n    if self.use_depth_classifier:\n        mlvl_depth_cls_scores = torch.cat(mlvl_depth_cls_scores)\n        mlvl_nms_scores *= mlvl_depth_cls_scores[:, None]\n        if self.weight_dim != -1:\n            mlvl_depth_uncertainty = torch.cat(mlvl_depth_uncertainty)\n            mlvl_nms_scores *= mlvl_depth_uncertainty[:, None]\n    results = box3d_multiclass_nms(mlvl_bboxes, mlvl_bboxes_for_nms, mlvl_nms_scores, cfg.score_thr, cfg.max_per_img, cfg, mlvl_dir_scores, mlvl_attr_scores, mlvl_bboxes2d)\n    (bboxes, scores, labels, dir_scores, attrs) = results[0:5]\n    attrs = attrs.to(labels.dtype)\n    bboxes = input_meta['box_type_3d'](bboxes, box_dim=self.bbox_coder.bbox_code_size, origin=(0.5, 0.5, 0.5))\n    if not self.pred_attrs:\n        attrs = None\n    outputs = (bboxes, scores, labels, attrs)\n    if self.pred_bbox2d:\n        bboxes2d = results[-1]\n        bboxes2d = torch.cat([bboxes2d, scores[:, None]], dim=1)\n        outputs = outputs + (bboxes2d,)\n    return outputs"
        ]
    },
    {
        "func_name": "get_targets",
        "original": "def get_targets(self, points, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list):\n    \"\"\"Compute regression, classification and centerss targets for points\n        in multiple images.\n\n        Args:\n            points (list[Tensor]): Points of each fpn level, each has shape\n                (num_points, 2).\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,\n                each has shape (num_gt, 4).\n            gt_labels_list (list[Tensor]): Ground truth labels of each box,\n                each has shape (num_gt,).\n            gt_bboxes_3d_list (list[Tensor]): 3D Ground truth bboxes of each\n                image, each has shape (num_gt, bbox_code_size).\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of each\n                box, each has shape (num_gt,).\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,\n                each has shape (num_gt, 2).\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\n                image, each has shape (num_gt, 1).\n            attr_labels_list (list[Tensor]): Attribute labels of each box,\n                each has shape (num_gt,).\n\n        Returns:\n            tuple:\n                concat_lvl_labels (list[Tensor]): Labels of each level.                 concat_lvl_bbox_targets (list[Tensor]): BBox targets of each                     level.\n        \"\"\"\n    assert len(points) == len(self.regress_ranges)\n    num_levels = len(points)\n    expanded_regress_ranges = [points[i].new_tensor(self.regress_ranges[i])[None].expand_as(points[i]) for i in range(num_levels)]\n    concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)\n    concat_points = torch.cat(points, dim=0)\n    num_points = [center.size(0) for center in points]\n    if attr_labels_list is None:\n        attr_labels_list = [gt_labels.new_full(gt_labels.shape, self.attr_background_label) for gt_labels in gt_labels_list]\n    (_, bbox_targets_list, labels_3d_list, bbox_targets_3d_list, centerness_targets_list, attr_targets_list) = multi_apply(self._get_target_single, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list, points=concat_points, regress_ranges=concat_regress_ranges, num_points_per_lvl=num_points)\n    bbox_targets_list = [bbox_targets.split(num_points, 0) for bbox_targets in bbox_targets_list]\n    labels_3d_list = [labels_3d.split(num_points, 0) for labels_3d in labels_3d_list]\n    bbox_targets_3d_list = [bbox_targets_3d.split(num_points, 0) for bbox_targets_3d in bbox_targets_3d_list]\n    centerness_targets_list = [centerness_targets.split(num_points, 0) for centerness_targets in centerness_targets_list]\n    attr_targets_list = [attr_targets.split(num_points, 0) for attr_targets in attr_targets_list]\n    concat_lvl_labels_3d = []\n    concat_lvl_bbox_targets_3d = []\n    concat_lvl_centerness_targets = []\n    concat_lvl_attr_targets = []\n    for i in range(num_levels):\n        concat_lvl_labels_3d.append(torch.cat([labels[i] for labels in labels_3d_list]))\n        concat_lvl_centerness_targets.append(torch.cat([centerness_targets[i] for centerness_targets in centerness_targets_list]))\n        bbox_targets_3d = torch.cat([bbox_targets_3d[i] for bbox_targets_3d in bbox_targets_3d_list])\n        if self.pred_bbox2d:\n            bbox_targets = torch.cat([bbox_targets[i] for bbox_targets in bbox_targets_list])\n            bbox_targets_3d = torch.cat([bbox_targets_3d, bbox_targets], dim=1)\n        concat_lvl_attr_targets.append(torch.cat([attr_targets[i] for attr_targets in attr_targets_list]))\n        if self.norm_on_bbox:\n            bbox_targets_3d[:, :2] = bbox_targets_3d[:, :2] / self.strides[i]\n            if self.pred_bbox2d:\n                bbox_targets_3d[:, -4:] = bbox_targets_3d[:, -4:] / self.strides[i]\n        concat_lvl_bbox_targets_3d.append(bbox_targets_3d)\n    return (concat_lvl_labels_3d, concat_lvl_bbox_targets_3d, concat_lvl_centerness_targets, concat_lvl_attr_targets)",
        "mutated": [
            "def get_targets(self, points, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list):\n    if False:\n        i = 10\n    'Compute regression, classification and centerss targets for points\\n        in multiple images.\\n\\n        Args:\\n            points (list[Tensor]): Points of each fpn level, each has shape\\n                (num_points, 2).\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,\\n                each has shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each box,\\n                each has shape (num_gt,).\\n            gt_bboxes_3d_list (list[Tensor]): 3D Ground truth bboxes of each\\n                image, each has shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of each\\n                box, each has shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,\\n                each has shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            attr_labels_list (list[Tensor]): Attribute labels of each box,\\n                each has shape (num_gt,).\\n\\n        Returns:\\n            tuple:\\n                concat_lvl_labels (list[Tensor]): Labels of each level.                 concat_lvl_bbox_targets (list[Tensor]): BBox targets of each                     level.\\n        '\n    assert len(points) == len(self.regress_ranges)\n    num_levels = len(points)\n    expanded_regress_ranges = [points[i].new_tensor(self.regress_ranges[i])[None].expand_as(points[i]) for i in range(num_levels)]\n    concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)\n    concat_points = torch.cat(points, dim=0)\n    num_points = [center.size(0) for center in points]\n    if attr_labels_list is None:\n        attr_labels_list = [gt_labels.new_full(gt_labels.shape, self.attr_background_label) for gt_labels in gt_labels_list]\n    (_, bbox_targets_list, labels_3d_list, bbox_targets_3d_list, centerness_targets_list, attr_targets_list) = multi_apply(self._get_target_single, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list, points=concat_points, regress_ranges=concat_regress_ranges, num_points_per_lvl=num_points)\n    bbox_targets_list = [bbox_targets.split(num_points, 0) for bbox_targets in bbox_targets_list]\n    labels_3d_list = [labels_3d.split(num_points, 0) for labels_3d in labels_3d_list]\n    bbox_targets_3d_list = [bbox_targets_3d.split(num_points, 0) for bbox_targets_3d in bbox_targets_3d_list]\n    centerness_targets_list = [centerness_targets.split(num_points, 0) for centerness_targets in centerness_targets_list]\n    attr_targets_list = [attr_targets.split(num_points, 0) for attr_targets in attr_targets_list]\n    concat_lvl_labels_3d = []\n    concat_lvl_bbox_targets_3d = []\n    concat_lvl_centerness_targets = []\n    concat_lvl_attr_targets = []\n    for i in range(num_levels):\n        concat_lvl_labels_3d.append(torch.cat([labels[i] for labels in labels_3d_list]))\n        concat_lvl_centerness_targets.append(torch.cat([centerness_targets[i] for centerness_targets in centerness_targets_list]))\n        bbox_targets_3d = torch.cat([bbox_targets_3d[i] for bbox_targets_3d in bbox_targets_3d_list])\n        if self.pred_bbox2d:\n            bbox_targets = torch.cat([bbox_targets[i] for bbox_targets in bbox_targets_list])\n            bbox_targets_3d = torch.cat([bbox_targets_3d, bbox_targets], dim=1)\n        concat_lvl_attr_targets.append(torch.cat([attr_targets[i] for attr_targets in attr_targets_list]))\n        if self.norm_on_bbox:\n            bbox_targets_3d[:, :2] = bbox_targets_3d[:, :2] / self.strides[i]\n            if self.pred_bbox2d:\n                bbox_targets_3d[:, -4:] = bbox_targets_3d[:, -4:] / self.strides[i]\n        concat_lvl_bbox_targets_3d.append(bbox_targets_3d)\n    return (concat_lvl_labels_3d, concat_lvl_bbox_targets_3d, concat_lvl_centerness_targets, concat_lvl_attr_targets)",
            "def get_targets(self, points, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute regression, classification and centerss targets for points\\n        in multiple images.\\n\\n        Args:\\n            points (list[Tensor]): Points of each fpn level, each has shape\\n                (num_points, 2).\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,\\n                each has shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each box,\\n                each has shape (num_gt,).\\n            gt_bboxes_3d_list (list[Tensor]): 3D Ground truth bboxes of each\\n                image, each has shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of each\\n                box, each has shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,\\n                each has shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            attr_labels_list (list[Tensor]): Attribute labels of each box,\\n                each has shape (num_gt,).\\n\\n        Returns:\\n            tuple:\\n                concat_lvl_labels (list[Tensor]): Labels of each level.                 concat_lvl_bbox_targets (list[Tensor]): BBox targets of each                     level.\\n        '\n    assert len(points) == len(self.regress_ranges)\n    num_levels = len(points)\n    expanded_regress_ranges = [points[i].new_tensor(self.regress_ranges[i])[None].expand_as(points[i]) for i in range(num_levels)]\n    concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)\n    concat_points = torch.cat(points, dim=0)\n    num_points = [center.size(0) for center in points]\n    if attr_labels_list is None:\n        attr_labels_list = [gt_labels.new_full(gt_labels.shape, self.attr_background_label) for gt_labels in gt_labels_list]\n    (_, bbox_targets_list, labels_3d_list, bbox_targets_3d_list, centerness_targets_list, attr_targets_list) = multi_apply(self._get_target_single, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list, points=concat_points, regress_ranges=concat_regress_ranges, num_points_per_lvl=num_points)\n    bbox_targets_list = [bbox_targets.split(num_points, 0) for bbox_targets in bbox_targets_list]\n    labels_3d_list = [labels_3d.split(num_points, 0) for labels_3d in labels_3d_list]\n    bbox_targets_3d_list = [bbox_targets_3d.split(num_points, 0) for bbox_targets_3d in bbox_targets_3d_list]\n    centerness_targets_list = [centerness_targets.split(num_points, 0) for centerness_targets in centerness_targets_list]\n    attr_targets_list = [attr_targets.split(num_points, 0) for attr_targets in attr_targets_list]\n    concat_lvl_labels_3d = []\n    concat_lvl_bbox_targets_3d = []\n    concat_lvl_centerness_targets = []\n    concat_lvl_attr_targets = []\n    for i in range(num_levels):\n        concat_lvl_labels_3d.append(torch.cat([labels[i] for labels in labels_3d_list]))\n        concat_lvl_centerness_targets.append(torch.cat([centerness_targets[i] for centerness_targets in centerness_targets_list]))\n        bbox_targets_3d = torch.cat([bbox_targets_3d[i] for bbox_targets_3d in bbox_targets_3d_list])\n        if self.pred_bbox2d:\n            bbox_targets = torch.cat([bbox_targets[i] for bbox_targets in bbox_targets_list])\n            bbox_targets_3d = torch.cat([bbox_targets_3d, bbox_targets], dim=1)\n        concat_lvl_attr_targets.append(torch.cat([attr_targets[i] for attr_targets in attr_targets_list]))\n        if self.norm_on_bbox:\n            bbox_targets_3d[:, :2] = bbox_targets_3d[:, :2] / self.strides[i]\n            if self.pred_bbox2d:\n                bbox_targets_3d[:, -4:] = bbox_targets_3d[:, -4:] / self.strides[i]\n        concat_lvl_bbox_targets_3d.append(bbox_targets_3d)\n    return (concat_lvl_labels_3d, concat_lvl_bbox_targets_3d, concat_lvl_centerness_targets, concat_lvl_attr_targets)",
            "def get_targets(self, points, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute regression, classification and centerss targets for points\\n        in multiple images.\\n\\n        Args:\\n            points (list[Tensor]): Points of each fpn level, each has shape\\n                (num_points, 2).\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,\\n                each has shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each box,\\n                each has shape (num_gt,).\\n            gt_bboxes_3d_list (list[Tensor]): 3D Ground truth bboxes of each\\n                image, each has shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of each\\n                box, each has shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,\\n                each has shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            attr_labels_list (list[Tensor]): Attribute labels of each box,\\n                each has shape (num_gt,).\\n\\n        Returns:\\n            tuple:\\n                concat_lvl_labels (list[Tensor]): Labels of each level.                 concat_lvl_bbox_targets (list[Tensor]): BBox targets of each                     level.\\n        '\n    assert len(points) == len(self.regress_ranges)\n    num_levels = len(points)\n    expanded_regress_ranges = [points[i].new_tensor(self.regress_ranges[i])[None].expand_as(points[i]) for i in range(num_levels)]\n    concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)\n    concat_points = torch.cat(points, dim=0)\n    num_points = [center.size(0) for center in points]\n    if attr_labels_list is None:\n        attr_labels_list = [gt_labels.new_full(gt_labels.shape, self.attr_background_label) for gt_labels in gt_labels_list]\n    (_, bbox_targets_list, labels_3d_list, bbox_targets_3d_list, centerness_targets_list, attr_targets_list) = multi_apply(self._get_target_single, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list, points=concat_points, regress_ranges=concat_regress_ranges, num_points_per_lvl=num_points)\n    bbox_targets_list = [bbox_targets.split(num_points, 0) for bbox_targets in bbox_targets_list]\n    labels_3d_list = [labels_3d.split(num_points, 0) for labels_3d in labels_3d_list]\n    bbox_targets_3d_list = [bbox_targets_3d.split(num_points, 0) for bbox_targets_3d in bbox_targets_3d_list]\n    centerness_targets_list = [centerness_targets.split(num_points, 0) for centerness_targets in centerness_targets_list]\n    attr_targets_list = [attr_targets.split(num_points, 0) for attr_targets in attr_targets_list]\n    concat_lvl_labels_3d = []\n    concat_lvl_bbox_targets_3d = []\n    concat_lvl_centerness_targets = []\n    concat_lvl_attr_targets = []\n    for i in range(num_levels):\n        concat_lvl_labels_3d.append(torch.cat([labels[i] for labels in labels_3d_list]))\n        concat_lvl_centerness_targets.append(torch.cat([centerness_targets[i] for centerness_targets in centerness_targets_list]))\n        bbox_targets_3d = torch.cat([bbox_targets_3d[i] for bbox_targets_3d in bbox_targets_3d_list])\n        if self.pred_bbox2d:\n            bbox_targets = torch.cat([bbox_targets[i] for bbox_targets in bbox_targets_list])\n            bbox_targets_3d = torch.cat([bbox_targets_3d, bbox_targets], dim=1)\n        concat_lvl_attr_targets.append(torch.cat([attr_targets[i] for attr_targets in attr_targets_list]))\n        if self.norm_on_bbox:\n            bbox_targets_3d[:, :2] = bbox_targets_3d[:, :2] / self.strides[i]\n            if self.pred_bbox2d:\n                bbox_targets_3d[:, -4:] = bbox_targets_3d[:, -4:] / self.strides[i]\n        concat_lvl_bbox_targets_3d.append(bbox_targets_3d)\n    return (concat_lvl_labels_3d, concat_lvl_bbox_targets_3d, concat_lvl_centerness_targets, concat_lvl_attr_targets)",
            "def get_targets(self, points, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute regression, classification and centerss targets for points\\n        in multiple images.\\n\\n        Args:\\n            points (list[Tensor]): Points of each fpn level, each has shape\\n                (num_points, 2).\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,\\n                each has shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each box,\\n                each has shape (num_gt,).\\n            gt_bboxes_3d_list (list[Tensor]): 3D Ground truth bboxes of each\\n                image, each has shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of each\\n                box, each has shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,\\n                each has shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            attr_labels_list (list[Tensor]): Attribute labels of each box,\\n                each has shape (num_gt,).\\n\\n        Returns:\\n            tuple:\\n                concat_lvl_labels (list[Tensor]): Labels of each level.                 concat_lvl_bbox_targets (list[Tensor]): BBox targets of each                     level.\\n        '\n    assert len(points) == len(self.regress_ranges)\n    num_levels = len(points)\n    expanded_regress_ranges = [points[i].new_tensor(self.regress_ranges[i])[None].expand_as(points[i]) for i in range(num_levels)]\n    concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)\n    concat_points = torch.cat(points, dim=0)\n    num_points = [center.size(0) for center in points]\n    if attr_labels_list is None:\n        attr_labels_list = [gt_labels.new_full(gt_labels.shape, self.attr_background_label) for gt_labels in gt_labels_list]\n    (_, bbox_targets_list, labels_3d_list, bbox_targets_3d_list, centerness_targets_list, attr_targets_list) = multi_apply(self._get_target_single, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list, points=concat_points, regress_ranges=concat_regress_ranges, num_points_per_lvl=num_points)\n    bbox_targets_list = [bbox_targets.split(num_points, 0) for bbox_targets in bbox_targets_list]\n    labels_3d_list = [labels_3d.split(num_points, 0) for labels_3d in labels_3d_list]\n    bbox_targets_3d_list = [bbox_targets_3d.split(num_points, 0) for bbox_targets_3d in bbox_targets_3d_list]\n    centerness_targets_list = [centerness_targets.split(num_points, 0) for centerness_targets in centerness_targets_list]\n    attr_targets_list = [attr_targets.split(num_points, 0) for attr_targets in attr_targets_list]\n    concat_lvl_labels_3d = []\n    concat_lvl_bbox_targets_3d = []\n    concat_lvl_centerness_targets = []\n    concat_lvl_attr_targets = []\n    for i in range(num_levels):\n        concat_lvl_labels_3d.append(torch.cat([labels[i] for labels in labels_3d_list]))\n        concat_lvl_centerness_targets.append(torch.cat([centerness_targets[i] for centerness_targets in centerness_targets_list]))\n        bbox_targets_3d = torch.cat([bbox_targets_3d[i] for bbox_targets_3d in bbox_targets_3d_list])\n        if self.pred_bbox2d:\n            bbox_targets = torch.cat([bbox_targets[i] for bbox_targets in bbox_targets_list])\n            bbox_targets_3d = torch.cat([bbox_targets_3d, bbox_targets], dim=1)\n        concat_lvl_attr_targets.append(torch.cat([attr_targets[i] for attr_targets in attr_targets_list]))\n        if self.norm_on_bbox:\n            bbox_targets_3d[:, :2] = bbox_targets_3d[:, :2] / self.strides[i]\n            if self.pred_bbox2d:\n                bbox_targets_3d[:, -4:] = bbox_targets_3d[:, -4:] / self.strides[i]\n        concat_lvl_bbox_targets_3d.append(bbox_targets_3d)\n    return (concat_lvl_labels_3d, concat_lvl_bbox_targets_3d, concat_lvl_centerness_targets, concat_lvl_attr_targets)",
            "def get_targets(self, points, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute regression, classification and centerss targets for points\\n        in multiple images.\\n\\n        Args:\\n            points (list[Tensor]): Points of each fpn level, each has shape\\n                (num_points, 2).\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,\\n                each has shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each box,\\n                each has shape (num_gt,).\\n            gt_bboxes_3d_list (list[Tensor]): 3D Ground truth bboxes of each\\n                image, each has shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of each\\n                box, each has shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,\\n                each has shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            attr_labels_list (list[Tensor]): Attribute labels of each box,\\n                each has shape (num_gt,).\\n\\n        Returns:\\n            tuple:\\n                concat_lvl_labels (list[Tensor]): Labels of each level.                 concat_lvl_bbox_targets (list[Tensor]): BBox targets of each                     level.\\n        '\n    assert len(points) == len(self.regress_ranges)\n    num_levels = len(points)\n    expanded_regress_ranges = [points[i].new_tensor(self.regress_ranges[i])[None].expand_as(points[i]) for i in range(num_levels)]\n    concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)\n    concat_points = torch.cat(points, dim=0)\n    num_points = [center.size(0) for center in points]\n    if attr_labels_list is None:\n        attr_labels_list = [gt_labels.new_full(gt_labels.shape, self.attr_background_label) for gt_labels in gt_labels_list]\n    (_, bbox_targets_list, labels_3d_list, bbox_targets_3d_list, centerness_targets_list, attr_targets_list) = multi_apply(self._get_target_single, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list, points=concat_points, regress_ranges=concat_regress_ranges, num_points_per_lvl=num_points)\n    bbox_targets_list = [bbox_targets.split(num_points, 0) for bbox_targets in bbox_targets_list]\n    labels_3d_list = [labels_3d.split(num_points, 0) for labels_3d in labels_3d_list]\n    bbox_targets_3d_list = [bbox_targets_3d.split(num_points, 0) for bbox_targets_3d in bbox_targets_3d_list]\n    centerness_targets_list = [centerness_targets.split(num_points, 0) for centerness_targets in centerness_targets_list]\n    attr_targets_list = [attr_targets.split(num_points, 0) for attr_targets in attr_targets_list]\n    concat_lvl_labels_3d = []\n    concat_lvl_bbox_targets_3d = []\n    concat_lvl_centerness_targets = []\n    concat_lvl_attr_targets = []\n    for i in range(num_levels):\n        concat_lvl_labels_3d.append(torch.cat([labels[i] for labels in labels_3d_list]))\n        concat_lvl_centerness_targets.append(torch.cat([centerness_targets[i] for centerness_targets in centerness_targets_list]))\n        bbox_targets_3d = torch.cat([bbox_targets_3d[i] for bbox_targets_3d in bbox_targets_3d_list])\n        if self.pred_bbox2d:\n            bbox_targets = torch.cat([bbox_targets[i] for bbox_targets in bbox_targets_list])\n            bbox_targets_3d = torch.cat([bbox_targets_3d, bbox_targets], dim=1)\n        concat_lvl_attr_targets.append(torch.cat([attr_targets[i] for attr_targets in attr_targets_list]))\n        if self.norm_on_bbox:\n            bbox_targets_3d[:, :2] = bbox_targets_3d[:, :2] / self.strides[i]\n            if self.pred_bbox2d:\n                bbox_targets_3d[:, -4:] = bbox_targets_3d[:, -4:] / self.strides[i]\n        concat_lvl_bbox_targets_3d.append(bbox_targets_3d)\n    return (concat_lvl_labels_3d, concat_lvl_bbox_targets_3d, concat_lvl_centerness_targets, concat_lvl_attr_targets)"
        ]
    }
]