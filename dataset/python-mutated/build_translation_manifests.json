[
    {
        "func_name": "get_fname",
        "original": "def get_fname(s):\n    return s.split('\\t')[0]",
        "mutated": [
            "def get_fname(s):\n    if False:\n        i = 10\n    return s.split('\\t')[0]",
            "def get_fname(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return s.split('\\t')[0]",
            "def get_fname(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return s.split('\\t')[0]",
            "def get_fname(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return s.split('\\t')[0]",
            "def get_fname(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return s.split('\\t')[0]"
        ]
    },
    {
        "func_name": "get_emotion",
        "original": "def get_emotion(s):\n    return get_fname(s).split('_')[0].split('/')[1].lower()",
        "mutated": [
            "def get_emotion(s):\n    if False:\n        i = 10\n    return get_fname(s).split('_')[0].split('/')[1].lower()",
            "def get_emotion(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return get_fname(s).split('_')[0].split('/')[1].lower()",
            "def get_emotion(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return get_fname(s).split('_')[0].split('/')[1].lower()",
            "def get_emotion(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return get_fname(s).split('_')[0].split('/')[1].lower()",
            "def get_emotion(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return get_fname(s).split('_')[0].split('/')[1].lower()"
        ]
    },
    {
        "func_name": "get_utt_id",
        "original": "def get_utt_id(s):\n    return get_fname(s).split('.')[0].split('_')[-1]",
        "mutated": [
            "def get_utt_id(s):\n    if False:\n        i = 10\n    return get_fname(s).split('.')[0].split('_')[-1]",
            "def get_utt_id(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return get_fname(s).split('.')[0].split('_')[-1]",
            "def get_utt_id(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return get_fname(s).split('.')[0].split('_')[-1]",
            "def get_utt_id(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return get_fname(s).split('.')[0].split('_')[-1]",
            "def get_utt_id(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return get_fname(s).split('.')[0].split('_')[-1]"
        ]
    },
    {
        "func_name": "dedup",
        "original": "def dedup(seq):\n    \"\"\" >> remove_repetitions(\"1 2 2 3 100 2 2 1\")\n    '1 2 3 100 2 1' \"\"\"\n    seq = seq.strip().split(' ')\n    result = seq[:1]\n    reps = []\n    rep_counter = 1\n    for k in seq[1:]:\n        if k != result[-1]:\n            result += [k]\n            reps += [rep_counter]\n            rep_counter = 1\n        else:\n            rep_counter += 1\n    reps += [rep_counter]\n    assert len(reps) == len(result) and sum(reps) == len(seq)\n    return ' '.join(result) + '\\n'",
        "mutated": [
            "def dedup(seq):\n    if False:\n        i = 10\n    ' >> remove_repetitions(\"1 2 2 3 100 2 2 1\")\\n    \\'1 2 3 100 2 1\\' '\n    seq = seq.strip().split(' ')\n    result = seq[:1]\n    reps = []\n    rep_counter = 1\n    for k in seq[1:]:\n        if k != result[-1]:\n            result += [k]\n            reps += [rep_counter]\n            rep_counter = 1\n        else:\n            rep_counter += 1\n    reps += [rep_counter]\n    assert len(reps) == len(result) and sum(reps) == len(seq)\n    return ' '.join(result) + '\\n'",
            "def dedup(seq):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' >> remove_repetitions(\"1 2 2 3 100 2 2 1\")\\n    \\'1 2 3 100 2 1\\' '\n    seq = seq.strip().split(' ')\n    result = seq[:1]\n    reps = []\n    rep_counter = 1\n    for k in seq[1:]:\n        if k != result[-1]:\n            result += [k]\n            reps += [rep_counter]\n            rep_counter = 1\n        else:\n            rep_counter += 1\n    reps += [rep_counter]\n    assert len(reps) == len(result) and sum(reps) == len(seq)\n    return ' '.join(result) + '\\n'",
            "def dedup(seq):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' >> remove_repetitions(\"1 2 2 3 100 2 2 1\")\\n    \\'1 2 3 100 2 1\\' '\n    seq = seq.strip().split(' ')\n    result = seq[:1]\n    reps = []\n    rep_counter = 1\n    for k in seq[1:]:\n        if k != result[-1]:\n            result += [k]\n            reps += [rep_counter]\n            rep_counter = 1\n        else:\n            rep_counter += 1\n    reps += [rep_counter]\n    assert len(reps) == len(result) and sum(reps) == len(seq)\n    return ' '.join(result) + '\\n'",
            "def dedup(seq):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' >> remove_repetitions(\"1 2 2 3 100 2 2 1\")\\n    \\'1 2 3 100 2 1\\' '\n    seq = seq.strip().split(' ')\n    result = seq[:1]\n    reps = []\n    rep_counter = 1\n    for k in seq[1:]:\n        if k != result[-1]:\n            result += [k]\n            reps += [rep_counter]\n            rep_counter = 1\n        else:\n            rep_counter += 1\n    reps += [rep_counter]\n    assert len(reps) == len(result) and sum(reps) == len(seq)\n    return ' '.join(result) + '\\n'",
            "def dedup(seq):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' >> remove_repetitions(\"1 2 2 3 100 2 2 1\")\\n    \\'1 2 3 100 2 1\\' '\n    seq = seq.strip().split(' ')\n    result = seq[:1]\n    reps = []\n    rep_counter = 1\n    for k in seq[1:]:\n        if k != result[-1]:\n            result += [k]\n            reps += [rep_counter]\n            rep_counter = 1\n        else:\n            rep_counter += 1\n    reps += [rep_counter]\n    assert len(reps) == len(result) and sum(reps) == len(seq)\n    return ' '.join(result) + '\\n'"
        ]
    },
    {
        "func_name": "remove_under_k",
        "original": "def remove_under_k(seq, k):\n    \"\"\" remove tokens that repeat less then k times in a row\n    >> remove_under_k(\"a a a a b c c c\", 1) ==> a a a a c c c \"\"\"\n    seq = seq.strip().split(' ')\n    result = []\n    freqs = [(k, len(list(g))) for (k, g) in groupby(seq)]\n    for (c, f) in freqs:\n        if f > k:\n            result += [c for _ in range(f)]\n    return ' '.join(result) + '\\n'",
        "mutated": [
            "def remove_under_k(seq, k):\n    if False:\n        i = 10\n    ' remove tokens that repeat less then k times in a row\\n    >> remove_under_k(\"a a a a b c c c\", 1) ==> a a a a c c c '\n    seq = seq.strip().split(' ')\n    result = []\n    freqs = [(k, len(list(g))) for (k, g) in groupby(seq)]\n    for (c, f) in freqs:\n        if f > k:\n            result += [c for _ in range(f)]\n    return ' '.join(result) + '\\n'",
            "def remove_under_k(seq, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' remove tokens that repeat less then k times in a row\\n    >> remove_under_k(\"a a a a b c c c\", 1) ==> a a a a c c c '\n    seq = seq.strip().split(' ')\n    result = []\n    freqs = [(k, len(list(g))) for (k, g) in groupby(seq)]\n    for (c, f) in freqs:\n        if f > k:\n            result += [c for _ in range(f)]\n    return ' '.join(result) + '\\n'",
            "def remove_under_k(seq, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' remove tokens that repeat less then k times in a row\\n    >> remove_under_k(\"a a a a b c c c\", 1) ==> a a a a c c c '\n    seq = seq.strip().split(' ')\n    result = []\n    freqs = [(k, len(list(g))) for (k, g) in groupby(seq)]\n    for (c, f) in freqs:\n        if f > k:\n            result += [c for _ in range(f)]\n    return ' '.join(result) + '\\n'",
            "def remove_under_k(seq, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' remove tokens that repeat less then k times in a row\\n    >> remove_under_k(\"a a a a b c c c\", 1) ==> a a a a c c c '\n    seq = seq.strip().split(' ')\n    result = []\n    freqs = [(k, len(list(g))) for (k, g) in groupby(seq)]\n    for (c, f) in freqs:\n        if f > k:\n            result += [c for _ in range(f)]\n    return ' '.join(result) + '\\n'",
            "def remove_under_k(seq, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' remove tokens that repeat less then k times in a row\\n    >> remove_under_k(\"a a a a b c c c\", 1) ==> a a a a c c c '\n    seq = seq.strip().split(' ')\n    result = []\n    freqs = [(k, len(list(g))) for (k, g) in groupby(seq)]\n    for (c, f) in freqs:\n        if f > k:\n            result += [c for _ in range(f)]\n    return ' '.join(result) + '\\n'"
        ]
    },
    {
        "func_name": "call",
        "original": "def call(cmd):\n    print(cmd)\n    check_call(cmd, shell=True)",
        "mutated": [
            "def call(cmd):\n    if False:\n        i = 10\n    print(cmd)\n    check_call(cmd, shell=True)",
            "def call(cmd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print(cmd)\n    check_call(cmd, shell=True)",
            "def call(cmd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print(cmd)\n    check_call(cmd, shell=True)",
            "def call(cmd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print(cmd)\n    check_call(cmd, shell=True)",
            "def call(cmd):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print(cmd)\n    check_call(cmd, shell=True)"
        ]
    },
    {
        "func_name": "denoising_preprocess",
        "original": "def denoising_preprocess(path, lang, dict):\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--trainpref {path}/train.{lang} --validpref {path}/valid.{lang} --testpref {path}/test.{lang}', f'--destdir {path}/tokenized/{lang}', '--only-source', '--task multilingual_denoising', '--workers 40']\n    if dict != '':\n        cmd += [f'--srcdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
        "mutated": [
            "def denoising_preprocess(path, lang, dict):\n    if False:\n        i = 10\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--trainpref {path}/train.{lang} --validpref {path}/valid.{lang} --testpref {path}/test.{lang}', f'--destdir {path}/tokenized/{lang}', '--only-source', '--task multilingual_denoising', '--workers 40']\n    if dict != '':\n        cmd += [f'--srcdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
            "def denoising_preprocess(path, lang, dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--trainpref {path}/train.{lang} --validpref {path}/valid.{lang} --testpref {path}/test.{lang}', f'--destdir {path}/tokenized/{lang}', '--only-source', '--task multilingual_denoising', '--workers 40']\n    if dict != '':\n        cmd += [f'--srcdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
            "def denoising_preprocess(path, lang, dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--trainpref {path}/train.{lang} --validpref {path}/valid.{lang} --testpref {path}/test.{lang}', f'--destdir {path}/tokenized/{lang}', '--only-source', '--task multilingual_denoising', '--workers 40']\n    if dict != '':\n        cmd += [f'--srcdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
            "def denoising_preprocess(path, lang, dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--trainpref {path}/train.{lang} --validpref {path}/valid.{lang} --testpref {path}/test.{lang}', f'--destdir {path}/tokenized/{lang}', '--only-source', '--task multilingual_denoising', '--workers 40']\n    if dict != '':\n        cmd += [f'--srcdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
            "def denoising_preprocess(path, lang, dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--trainpref {path}/train.{lang} --validpref {path}/valid.{lang} --testpref {path}/test.{lang}', f'--destdir {path}/tokenized/{lang}', '--only-source', '--task multilingual_denoising', '--workers 40']\n    if dict != '':\n        cmd += [f'--srcdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)"
        ]
    },
    {
        "func_name": "translation_preprocess",
        "original": "def translation_preprocess(path, src_lang, trg_lang, dict, only_train=False):\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--source-lang {src_lang} --target-lang {trg_lang}', f'--trainpref {path}/train', f'--destdir {path}/tokenized', '--workers 40']\n    if not only_train:\n        cmd += [f'--validpref {path}/valid --testpref {path}/test']\n    if dict != '':\n        cmd += [f'--srcdict {dict}', f'--tgtdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
        "mutated": [
            "def translation_preprocess(path, src_lang, trg_lang, dict, only_train=False):\n    if False:\n        i = 10\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--source-lang {src_lang} --target-lang {trg_lang}', f'--trainpref {path}/train', f'--destdir {path}/tokenized', '--workers 40']\n    if not only_train:\n        cmd += [f'--validpref {path}/valid --testpref {path}/test']\n    if dict != '':\n        cmd += [f'--srcdict {dict}', f'--tgtdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
            "def translation_preprocess(path, src_lang, trg_lang, dict, only_train=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--source-lang {src_lang} --target-lang {trg_lang}', f'--trainpref {path}/train', f'--destdir {path}/tokenized', '--workers 40']\n    if not only_train:\n        cmd += [f'--validpref {path}/valid --testpref {path}/test']\n    if dict != '':\n        cmd += [f'--srcdict {dict}', f'--tgtdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
            "def translation_preprocess(path, src_lang, trg_lang, dict, only_train=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--source-lang {src_lang} --target-lang {trg_lang}', f'--trainpref {path}/train', f'--destdir {path}/tokenized', '--workers 40']\n    if not only_train:\n        cmd += [f'--validpref {path}/valid --testpref {path}/test']\n    if dict != '':\n        cmd += [f'--srcdict {dict}', f'--tgtdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
            "def translation_preprocess(path, src_lang, trg_lang, dict, only_train=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--source-lang {src_lang} --target-lang {trg_lang}', f'--trainpref {path}/train', f'--destdir {path}/tokenized', '--workers 40']\n    if not only_train:\n        cmd += [f'--validpref {path}/valid --testpref {path}/test']\n    if dict != '':\n        cmd += [f'--srcdict {dict}', f'--tgtdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)",
            "def translation_preprocess(path, src_lang, trg_lang, dict, only_train=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bin = 'fairseq-preprocess'\n    cmd = [bin, f'--source-lang {src_lang} --target-lang {trg_lang}', f'--trainpref {path}/train', f'--destdir {path}/tokenized', '--workers 40']\n    if not only_train:\n        cmd += [f'--validpref {path}/valid --testpref {path}/test']\n    if dict != '':\n        cmd += [f'--srcdict {dict}', f'--tgtdict {dict}']\n    cmd = ' '.join(cmd)\n    call(cmd)"
        ]
    },
    {
        "func_name": "load_tsv_km",
        "original": "def load_tsv_km(tsv_path, km_path):\n    assert tsv_path.exists() and km_path.exists()\n    tsv_lines = open(tsv_path, 'r').readlines()\n    (root, tsv_lines) = (tsv_lines[0], tsv_lines[1:])\n    km_lines = open(km_path, 'r').readlines()\n    assert len(tsv_lines) == len(km_lines), '.tsv and .km should be the same length!'\n    return (root, tsv_lines, km_lines)",
        "mutated": [
            "def load_tsv_km(tsv_path, km_path):\n    if False:\n        i = 10\n    assert tsv_path.exists() and km_path.exists()\n    tsv_lines = open(tsv_path, 'r').readlines()\n    (root, tsv_lines) = (tsv_lines[0], tsv_lines[1:])\n    km_lines = open(km_path, 'r').readlines()\n    assert len(tsv_lines) == len(km_lines), '.tsv and .km should be the same length!'\n    return (root, tsv_lines, km_lines)",
            "def load_tsv_km(tsv_path, km_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert tsv_path.exists() and km_path.exists()\n    tsv_lines = open(tsv_path, 'r').readlines()\n    (root, tsv_lines) = (tsv_lines[0], tsv_lines[1:])\n    km_lines = open(km_path, 'r').readlines()\n    assert len(tsv_lines) == len(km_lines), '.tsv and .km should be the same length!'\n    return (root, tsv_lines, km_lines)",
            "def load_tsv_km(tsv_path, km_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert tsv_path.exists() and km_path.exists()\n    tsv_lines = open(tsv_path, 'r').readlines()\n    (root, tsv_lines) = (tsv_lines[0], tsv_lines[1:])\n    km_lines = open(km_path, 'r').readlines()\n    assert len(tsv_lines) == len(km_lines), '.tsv and .km should be the same length!'\n    return (root, tsv_lines, km_lines)",
            "def load_tsv_km(tsv_path, km_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert tsv_path.exists() and km_path.exists()\n    tsv_lines = open(tsv_path, 'r').readlines()\n    (root, tsv_lines) = (tsv_lines[0], tsv_lines[1:])\n    km_lines = open(km_path, 'r').readlines()\n    assert len(tsv_lines) == len(km_lines), '.tsv and .km should be the same length!'\n    return (root, tsv_lines, km_lines)",
            "def load_tsv_km(tsv_path, km_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert tsv_path.exists() and km_path.exists()\n    tsv_lines = open(tsv_path, 'r').readlines()\n    (root, tsv_lines) = (tsv_lines[0], tsv_lines[1:])\n    km_lines = open(km_path, 'r').readlines()\n    assert len(tsv_lines) == len(km_lines), '.tsv and .km should be the same length!'\n    return (root, tsv_lines, km_lines)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    desc = '\\n    this script takes as input .tsv and .km files for EMOV dataset, and a pairs of emotions.\\n    it generates parallel .tsv and .km files for these emotions. for exmaple:\\n    \u276f python build_emov_translation_manifests.py             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/train.tsv             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/emov_16khz_km_100/train.km             ~/tmp/emov_pairs             --src-emotion amused --trg-emotion neutral             --dedup --shuffle --cross-speaker --dry-run\\n    '\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('data', type=Path, help='path to a dir containing .tsv and .km files containing emov dataset')\n    parser.add_argument('output_path', type=Path, help='output directory with the manifests will be created')\n    parser.add_argument('-cs', '--cross-speaker', action='store_true', help='if set then translation will occur also between speakers, meaning the same sentence can be translated between different speakers (default: false)')\n    parser.add_argument('-dd', '--dedup', action='store_true', help=\"remove repeated tokens (example: 'aaabc=>abc')\")\n    parser.add_argument('-sh', '--shuffle', action='store_true', help='shuffle the data')\n    parser.add_argument('-ae', '--autoencode', action='store_true', help='include training pairs from the same emotion (this includes examples of the same sentence uttered by different people and examples where the src and trg are the exact same seq)')\n    parser.add_argument('-dr', '--dry-run', action='store_true', help=\"don't write anything to disk\")\n    parser.add_argument('-zs', '--zero-shot', action='store_true', help='if true, the denoising task will train on the same splits as the translation task (split by utterance id). if false, the denoising task will train on randomly sampled splits (not split by utterance id)')\n    parser.add_argument('--km-ext', default='km', help='')\n    parser.add_argument('--dict', default='/checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/fairseq.dict.txt', help='')\n    args = parser.parse_args()\n    SPEAKERS = ['bea', 'jenie', 'josh', 'sam', 'SAME']\n    EMOTIONS = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n    suffix = ''\n    if args.cross_speaker:\n        suffix += '_cross-speaker'\n    if args.dedup:\n        suffix += '_dedup'\n    translation_suffix = ''\n    if args.autoencode:\n        translation_suffix += '_autoencode'\n    denoising_suffix = ''\n    denoising_suffix += '_zeroshot' if args.zero_shot else '_nonzeroshot'\n    translation_dir = Path(args.output_path) / ('emov_multilingual_translation' + suffix + translation_suffix)\n    os.makedirs(translation_dir, exist_ok=True)\n    denoising_dir = Path(args.output_path) / ('emov_multilingual_denoising' + suffix + denoising_suffix)\n    os.makedirs(denoising_dir, exist_ok=True)\n    denoising_data = [p.name for p in (args.data / 'denoising').glob('*') if 'emov' not in p.name]\n    for split in ['train', 'valid', 'test']:\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'denoising' / 'emov' / f'{split}.tsv', km_path=args.data / 'denoising' / 'emov' / f'{split}.{args.km_ext}')\n        for EMOTION in EMOTIONS:\n            print('---')\n            print(split)\n            print(f'denoising: {EMOTION}')\n            (emotion_tsv, emotion_km) = ([], [])\n            for (tsv_line, km_line) in zip(tsv_lines, km_lines):\n                if EMOTION.lower() in tsv_line.lower():\n                    km_line = km_line if not args.dedup else dedup(km_line)\n                    emotion_tsv.append(tsv_line)\n                    emotion_km.append(km_line)\n            print(f'{len(emotion_km)} samples')\n            open(denoising_dir / f'files.{split}.{EMOTION}', 'w').writelines([root] + emotion_tsv)\n            open(denoising_dir / f'{split}.{EMOTION}', 'w').writelines(emotion_km)\n        for data in denoising_data:\n            with open(args.data / 'denoising' / data / f'{split}.{args.km_ext}', 'r') as f1:\n                with open(denoising_dir / f'{split}.{data}', 'w') as f2:\n                    f2.writelines([l if not args.dedup else dedup(l) for l in f1.readlines()])\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'translation' / f'{split}.tsv', km_path=args.data / 'translation' / f'{split}.{args.km_ext}')\n        for SRC_EMOTION in EMOTIONS:\n            TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n            for TRG_EMOTION in TRG_EMOTIONS:\n                print('---')\n                print(split)\n                print(f'src emotions: {SRC_EMOTION}\\ntrg emotions: {TRG_EMOTION}')\n                spkr2utts = defaultdict(lambda : defaultdict(list))\n                for (i, tsv_line) in enumerate(tsv_lines):\n                    speaker = tsv_line.split('/')[0]\n                    if args.cross_speaker:\n                        speaker = 'SAME'\n                    assert speaker in SPEAKERS, 'unknown speaker! make sure the .tsv contains EMOV data'\n                    utt_id = get_utt_id(tsv_line)\n                    spkr2utts[speaker][utt_id].append(i)\n                (src_tsv, trg_tsv, src_km, trg_km) = ([], [], [], [])\n                for (speaker, utt_ids) in spkr2utts.items():\n                    for (utt_id, indices) in utt_ids.items():\n                        pairs = [(x, y) for x in indices for y in indices]\n                        if SRC_EMOTION == TRG_EMOTION:\n                            pairs = [(x, y) for (x, y) in pairs if x == y]\n                        pairs = [(x, y) for (x, y) in pairs if get_emotion(tsv_lines[x]) == SRC_EMOTION and get_emotion(tsv_lines[y]) == TRG_EMOTION]\n                        for (idx1, idx2) in pairs:\n                            assert get_utt_id(tsv_lines[idx1]) == get_utt_id(tsv_lines[idx2])\n                            src_tsv.append(tsv_lines[idx1])\n                            trg_tsv.append(tsv_lines[idx2])\n                            km_line_idx1 = km_lines[idx1]\n                            km_line_idx2 = km_lines[idx2]\n                            km_line_idx1 = km_line_idx1 if not args.dedup else dedup(km_line_idx1)\n                            km_line_idx2 = km_line_idx2 if not args.dedup else dedup(km_line_idx2)\n                            src_km.append(km_line_idx1)\n                            trg_km.append(km_line_idx2)\n                assert len(src_tsv) == len(trg_tsv) == len(src_km) == len(trg_km)\n                print(f'{len(src_tsv)} pairs')\n                if len(src_tsv) == 0:\n                    raise Exception('ERROR: generated 0 pairs!')\n                if args.dry_run:\n                    continue\n                os.makedirs(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', exist_ok=True)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{SRC_EMOTION}', 'w').writelines([root] + src_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{TRG_EMOTION}', 'w').writelines([root] + trg_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{SRC_EMOTION}', 'w').writelines(src_km)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{TRG_EMOTION}', 'w').writelines(trg_km)\n    for EMOTION in EMOTIONS + denoising_data:\n        denoising_preprocess(denoising_dir, EMOTION, args.dict)\n    os.system(f'cp {args.dict} {denoising_dir}/tokenized/dict.txt')\n    os.makedirs(translation_dir / 'tokenized', exist_ok=True)\n    for SRC_EMOTION in EMOTIONS:\n        TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n        for TRG_EMOTION in TRG_EMOTIONS:\n            translation_preprocess(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', SRC_EMOTION, TRG_EMOTION, args.dict)\n    os.system(f'cp -rf {translation_dir}/**/tokenized/* {translation_dir}/tokenized')",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    desc = '\\n    this script takes as input .tsv and .km files for EMOV dataset, and a pairs of emotions.\\n    it generates parallel .tsv and .km files for these emotions. for exmaple:\\n    \u276f python build_emov_translation_manifests.py             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/train.tsv             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/emov_16khz_km_100/train.km             ~/tmp/emov_pairs             --src-emotion amused --trg-emotion neutral             --dedup --shuffle --cross-speaker --dry-run\\n    '\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('data', type=Path, help='path to a dir containing .tsv and .km files containing emov dataset')\n    parser.add_argument('output_path', type=Path, help='output directory with the manifests will be created')\n    parser.add_argument('-cs', '--cross-speaker', action='store_true', help='if set then translation will occur also between speakers, meaning the same sentence can be translated between different speakers (default: false)')\n    parser.add_argument('-dd', '--dedup', action='store_true', help=\"remove repeated tokens (example: 'aaabc=>abc')\")\n    parser.add_argument('-sh', '--shuffle', action='store_true', help='shuffle the data')\n    parser.add_argument('-ae', '--autoencode', action='store_true', help='include training pairs from the same emotion (this includes examples of the same sentence uttered by different people and examples where the src and trg are the exact same seq)')\n    parser.add_argument('-dr', '--dry-run', action='store_true', help=\"don't write anything to disk\")\n    parser.add_argument('-zs', '--zero-shot', action='store_true', help='if true, the denoising task will train on the same splits as the translation task (split by utterance id). if false, the denoising task will train on randomly sampled splits (not split by utterance id)')\n    parser.add_argument('--km-ext', default='km', help='')\n    parser.add_argument('--dict', default='/checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/fairseq.dict.txt', help='')\n    args = parser.parse_args()\n    SPEAKERS = ['bea', 'jenie', 'josh', 'sam', 'SAME']\n    EMOTIONS = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n    suffix = ''\n    if args.cross_speaker:\n        suffix += '_cross-speaker'\n    if args.dedup:\n        suffix += '_dedup'\n    translation_suffix = ''\n    if args.autoencode:\n        translation_suffix += '_autoencode'\n    denoising_suffix = ''\n    denoising_suffix += '_zeroshot' if args.zero_shot else '_nonzeroshot'\n    translation_dir = Path(args.output_path) / ('emov_multilingual_translation' + suffix + translation_suffix)\n    os.makedirs(translation_dir, exist_ok=True)\n    denoising_dir = Path(args.output_path) / ('emov_multilingual_denoising' + suffix + denoising_suffix)\n    os.makedirs(denoising_dir, exist_ok=True)\n    denoising_data = [p.name for p in (args.data / 'denoising').glob('*') if 'emov' not in p.name]\n    for split in ['train', 'valid', 'test']:\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'denoising' / 'emov' / f'{split}.tsv', km_path=args.data / 'denoising' / 'emov' / f'{split}.{args.km_ext}')\n        for EMOTION in EMOTIONS:\n            print('---')\n            print(split)\n            print(f'denoising: {EMOTION}')\n            (emotion_tsv, emotion_km) = ([], [])\n            for (tsv_line, km_line) in zip(tsv_lines, km_lines):\n                if EMOTION.lower() in tsv_line.lower():\n                    km_line = km_line if not args.dedup else dedup(km_line)\n                    emotion_tsv.append(tsv_line)\n                    emotion_km.append(km_line)\n            print(f'{len(emotion_km)} samples')\n            open(denoising_dir / f'files.{split}.{EMOTION}', 'w').writelines([root] + emotion_tsv)\n            open(denoising_dir / f'{split}.{EMOTION}', 'w').writelines(emotion_km)\n        for data in denoising_data:\n            with open(args.data / 'denoising' / data / f'{split}.{args.km_ext}', 'r') as f1:\n                with open(denoising_dir / f'{split}.{data}', 'w') as f2:\n                    f2.writelines([l if not args.dedup else dedup(l) for l in f1.readlines()])\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'translation' / f'{split}.tsv', km_path=args.data / 'translation' / f'{split}.{args.km_ext}')\n        for SRC_EMOTION in EMOTIONS:\n            TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n            for TRG_EMOTION in TRG_EMOTIONS:\n                print('---')\n                print(split)\n                print(f'src emotions: {SRC_EMOTION}\\ntrg emotions: {TRG_EMOTION}')\n                spkr2utts = defaultdict(lambda : defaultdict(list))\n                for (i, tsv_line) in enumerate(tsv_lines):\n                    speaker = tsv_line.split('/')[0]\n                    if args.cross_speaker:\n                        speaker = 'SAME'\n                    assert speaker in SPEAKERS, 'unknown speaker! make sure the .tsv contains EMOV data'\n                    utt_id = get_utt_id(tsv_line)\n                    spkr2utts[speaker][utt_id].append(i)\n                (src_tsv, trg_tsv, src_km, trg_km) = ([], [], [], [])\n                for (speaker, utt_ids) in spkr2utts.items():\n                    for (utt_id, indices) in utt_ids.items():\n                        pairs = [(x, y) for x in indices for y in indices]\n                        if SRC_EMOTION == TRG_EMOTION:\n                            pairs = [(x, y) for (x, y) in pairs if x == y]\n                        pairs = [(x, y) for (x, y) in pairs if get_emotion(tsv_lines[x]) == SRC_EMOTION and get_emotion(tsv_lines[y]) == TRG_EMOTION]\n                        for (idx1, idx2) in pairs:\n                            assert get_utt_id(tsv_lines[idx1]) == get_utt_id(tsv_lines[idx2])\n                            src_tsv.append(tsv_lines[idx1])\n                            trg_tsv.append(tsv_lines[idx2])\n                            km_line_idx1 = km_lines[idx1]\n                            km_line_idx2 = km_lines[idx2]\n                            km_line_idx1 = km_line_idx1 if not args.dedup else dedup(km_line_idx1)\n                            km_line_idx2 = km_line_idx2 if not args.dedup else dedup(km_line_idx2)\n                            src_km.append(km_line_idx1)\n                            trg_km.append(km_line_idx2)\n                assert len(src_tsv) == len(trg_tsv) == len(src_km) == len(trg_km)\n                print(f'{len(src_tsv)} pairs')\n                if len(src_tsv) == 0:\n                    raise Exception('ERROR: generated 0 pairs!')\n                if args.dry_run:\n                    continue\n                os.makedirs(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', exist_ok=True)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{SRC_EMOTION}', 'w').writelines([root] + src_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{TRG_EMOTION}', 'w').writelines([root] + trg_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{SRC_EMOTION}', 'w').writelines(src_km)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{TRG_EMOTION}', 'w').writelines(trg_km)\n    for EMOTION in EMOTIONS + denoising_data:\n        denoising_preprocess(denoising_dir, EMOTION, args.dict)\n    os.system(f'cp {args.dict} {denoising_dir}/tokenized/dict.txt')\n    os.makedirs(translation_dir / 'tokenized', exist_ok=True)\n    for SRC_EMOTION in EMOTIONS:\n        TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n        for TRG_EMOTION in TRG_EMOTIONS:\n            translation_preprocess(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', SRC_EMOTION, TRG_EMOTION, args.dict)\n    os.system(f'cp -rf {translation_dir}/**/tokenized/* {translation_dir}/tokenized')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    desc = '\\n    this script takes as input .tsv and .km files for EMOV dataset, and a pairs of emotions.\\n    it generates parallel .tsv and .km files for these emotions. for exmaple:\\n    \u276f python build_emov_translation_manifests.py             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/train.tsv             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/emov_16khz_km_100/train.km             ~/tmp/emov_pairs             --src-emotion amused --trg-emotion neutral             --dedup --shuffle --cross-speaker --dry-run\\n    '\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('data', type=Path, help='path to a dir containing .tsv and .km files containing emov dataset')\n    parser.add_argument('output_path', type=Path, help='output directory with the manifests will be created')\n    parser.add_argument('-cs', '--cross-speaker', action='store_true', help='if set then translation will occur also between speakers, meaning the same sentence can be translated between different speakers (default: false)')\n    parser.add_argument('-dd', '--dedup', action='store_true', help=\"remove repeated tokens (example: 'aaabc=>abc')\")\n    parser.add_argument('-sh', '--shuffle', action='store_true', help='shuffle the data')\n    parser.add_argument('-ae', '--autoencode', action='store_true', help='include training pairs from the same emotion (this includes examples of the same sentence uttered by different people and examples where the src and trg are the exact same seq)')\n    parser.add_argument('-dr', '--dry-run', action='store_true', help=\"don't write anything to disk\")\n    parser.add_argument('-zs', '--zero-shot', action='store_true', help='if true, the denoising task will train on the same splits as the translation task (split by utterance id). if false, the denoising task will train on randomly sampled splits (not split by utterance id)')\n    parser.add_argument('--km-ext', default='km', help='')\n    parser.add_argument('--dict', default='/checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/fairseq.dict.txt', help='')\n    args = parser.parse_args()\n    SPEAKERS = ['bea', 'jenie', 'josh', 'sam', 'SAME']\n    EMOTIONS = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n    suffix = ''\n    if args.cross_speaker:\n        suffix += '_cross-speaker'\n    if args.dedup:\n        suffix += '_dedup'\n    translation_suffix = ''\n    if args.autoencode:\n        translation_suffix += '_autoencode'\n    denoising_suffix = ''\n    denoising_suffix += '_zeroshot' if args.zero_shot else '_nonzeroshot'\n    translation_dir = Path(args.output_path) / ('emov_multilingual_translation' + suffix + translation_suffix)\n    os.makedirs(translation_dir, exist_ok=True)\n    denoising_dir = Path(args.output_path) / ('emov_multilingual_denoising' + suffix + denoising_suffix)\n    os.makedirs(denoising_dir, exist_ok=True)\n    denoising_data = [p.name for p in (args.data / 'denoising').glob('*') if 'emov' not in p.name]\n    for split in ['train', 'valid', 'test']:\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'denoising' / 'emov' / f'{split}.tsv', km_path=args.data / 'denoising' / 'emov' / f'{split}.{args.km_ext}')\n        for EMOTION in EMOTIONS:\n            print('---')\n            print(split)\n            print(f'denoising: {EMOTION}')\n            (emotion_tsv, emotion_km) = ([], [])\n            for (tsv_line, km_line) in zip(tsv_lines, km_lines):\n                if EMOTION.lower() in tsv_line.lower():\n                    km_line = km_line if not args.dedup else dedup(km_line)\n                    emotion_tsv.append(tsv_line)\n                    emotion_km.append(km_line)\n            print(f'{len(emotion_km)} samples')\n            open(denoising_dir / f'files.{split}.{EMOTION}', 'w').writelines([root] + emotion_tsv)\n            open(denoising_dir / f'{split}.{EMOTION}', 'w').writelines(emotion_km)\n        for data in denoising_data:\n            with open(args.data / 'denoising' / data / f'{split}.{args.km_ext}', 'r') as f1:\n                with open(denoising_dir / f'{split}.{data}', 'w') as f2:\n                    f2.writelines([l if not args.dedup else dedup(l) for l in f1.readlines()])\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'translation' / f'{split}.tsv', km_path=args.data / 'translation' / f'{split}.{args.km_ext}')\n        for SRC_EMOTION in EMOTIONS:\n            TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n            for TRG_EMOTION in TRG_EMOTIONS:\n                print('---')\n                print(split)\n                print(f'src emotions: {SRC_EMOTION}\\ntrg emotions: {TRG_EMOTION}')\n                spkr2utts = defaultdict(lambda : defaultdict(list))\n                for (i, tsv_line) in enumerate(tsv_lines):\n                    speaker = tsv_line.split('/')[0]\n                    if args.cross_speaker:\n                        speaker = 'SAME'\n                    assert speaker in SPEAKERS, 'unknown speaker! make sure the .tsv contains EMOV data'\n                    utt_id = get_utt_id(tsv_line)\n                    spkr2utts[speaker][utt_id].append(i)\n                (src_tsv, trg_tsv, src_km, trg_km) = ([], [], [], [])\n                for (speaker, utt_ids) in spkr2utts.items():\n                    for (utt_id, indices) in utt_ids.items():\n                        pairs = [(x, y) for x in indices for y in indices]\n                        if SRC_EMOTION == TRG_EMOTION:\n                            pairs = [(x, y) for (x, y) in pairs if x == y]\n                        pairs = [(x, y) for (x, y) in pairs if get_emotion(tsv_lines[x]) == SRC_EMOTION and get_emotion(tsv_lines[y]) == TRG_EMOTION]\n                        for (idx1, idx2) in pairs:\n                            assert get_utt_id(tsv_lines[idx1]) == get_utt_id(tsv_lines[idx2])\n                            src_tsv.append(tsv_lines[idx1])\n                            trg_tsv.append(tsv_lines[idx2])\n                            km_line_idx1 = km_lines[idx1]\n                            km_line_idx2 = km_lines[idx2]\n                            km_line_idx1 = km_line_idx1 if not args.dedup else dedup(km_line_idx1)\n                            km_line_idx2 = km_line_idx2 if not args.dedup else dedup(km_line_idx2)\n                            src_km.append(km_line_idx1)\n                            trg_km.append(km_line_idx2)\n                assert len(src_tsv) == len(trg_tsv) == len(src_km) == len(trg_km)\n                print(f'{len(src_tsv)} pairs')\n                if len(src_tsv) == 0:\n                    raise Exception('ERROR: generated 0 pairs!')\n                if args.dry_run:\n                    continue\n                os.makedirs(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', exist_ok=True)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{SRC_EMOTION}', 'w').writelines([root] + src_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{TRG_EMOTION}', 'w').writelines([root] + trg_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{SRC_EMOTION}', 'w').writelines(src_km)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{TRG_EMOTION}', 'w').writelines(trg_km)\n    for EMOTION in EMOTIONS + denoising_data:\n        denoising_preprocess(denoising_dir, EMOTION, args.dict)\n    os.system(f'cp {args.dict} {denoising_dir}/tokenized/dict.txt')\n    os.makedirs(translation_dir / 'tokenized', exist_ok=True)\n    for SRC_EMOTION in EMOTIONS:\n        TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n        for TRG_EMOTION in TRG_EMOTIONS:\n            translation_preprocess(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', SRC_EMOTION, TRG_EMOTION, args.dict)\n    os.system(f'cp -rf {translation_dir}/**/tokenized/* {translation_dir}/tokenized')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    desc = '\\n    this script takes as input .tsv and .km files for EMOV dataset, and a pairs of emotions.\\n    it generates parallel .tsv and .km files for these emotions. for exmaple:\\n    \u276f python build_emov_translation_manifests.py             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/train.tsv             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/emov_16khz_km_100/train.km             ~/tmp/emov_pairs             --src-emotion amused --trg-emotion neutral             --dedup --shuffle --cross-speaker --dry-run\\n    '\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('data', type=Path, help='path to a dir containing .tsv and .km files containing emov dataset')\n    parser.add_argument('output_path', type=Path, help='output directory with the manifests will be created')\n    parser.add_argument('-cs', '--cross-speaker', action='store_true', help='if set then translation will occur also between speakers, meaning the same sentence can be translated between different speakers (default: false)')\n    parser.add_argument('-dd', '--dedup', action='store_true', help=\"remove repeated tokens (example: 'aaabc=>abc')\")\n    parser.add_argument('-sh', '--shuffle', action='store_true', help='shuffle the data')\n    parser.add_argument('-ae', '--autoencode', action='store_true', help='include training pairs from the same emotion (this includes examples of the same sentence uttered by different people and examples where the src and trg are the exact same seq)')\n    parser.add_argument('-dr', '--dry-run', action='store_true', help=\"don't write anything to disk\")\n    parser.add_argument('-zs', '--zero-shot', action='store_true', help='if true, the denoising task will train on the same splits as the translation task (split by utterance id). if false, the denoising task will train on randomly sampled splits (not split by utterance id)')\n    parser.add_argument('--km-ext', default='km', help='')\n    parser.add_argument('--dict', default='/checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/fairseq.dict.txt', help='')\n    args = parser.parse_args()\n    SPEAKERS = ['bea', 'jenie', 'josh', 'sam', 'SAME']\n    EMOTIONS = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n    suffix = ''\n    if args.cross_speaker:\n        suffix += '_cross-speaker'\n    if args.dedup:\n        suffix += '_dedup'\n    translation_suffix = ''\n    if args.autoencode:\n        translation_suffix += '_autoencode'\n    denoising_suffix = ''\n    denoising_suffix += '_zeroshot' if args.zero_shot else '_nonzeroshot'\n    translation_dir = Path(args.output_path) / ('emov_multilingual_translation' + suffix + translation_suffix)\n    os.makedirs(translation_dir, exist_ok=True)\n    denoising_dir = Path(args.output_path) / ('emov_multilingual_denoising' + suffix + denoising_suffix)\n    os.makedirs(denoising_dir, exist_ok=True)\n    denoising_data = [p.name for p in (args.data / 'denoising').glob('*') if 'emov' not in p.name]\n    for split in ['train', 'valid', 'test']:\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'denoising' / 'emov' / f'{split}.tsv', km_path=args.data / 'denoising' / 'emov' / f'{split}.{args.km_ext}')\n        for EMOTION in EMOTIONS:\n            print('---')\n            print(split)\n            print(f'denoising: {EMOTION}')\n            (emotion_tsv, emotion_km) = ([], [])\n            for (tsv_line, km_line) in zip(tsv_lines, km_lines):\n                if EMOTION.lower() in tsv_line.lower():\n                    km_line = km_line if not args.dedup else dedup(km_line)\n                    emotion_tsv.append(tsv_line)\n                    emotion_km.append(km_line)\n            print(f'{len(emotion_km)} samples')\n            open(denoising_dir / f'files.{split}.{EMOTION}', 'w').writelines([root] + emotion_tsv)\n            open(denoising_dir / f'{split}.{EMOTION}', 'w').writelines(emotion_km)\n        for data in denoising_data:\n            with open(args.data / 'denoising' / data / f'{split}.{args.km_ext}', 'r') as f1:\n                with open(denoising_dir / f'{split}.{data}', 'w') as f2:\n                    f2.writelines([l if not args.dedup else dedup(l) for l in f1.readlines()])\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'translation' / f'{split}.tsv', km_path=args.data / 'translation' / f'{split}.{args.km_ext}')\n        for SRC_EMOTION in EMOTIONS:\n            TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n            for TRG_EMOTION in TRG_EMOTIONS:\n                print('---')\n                print(split)\n                print(f'src emotions: {SRC_EMOTION}\\ntrg emotions: {TRG_EMOTION}')\n                spkr2utts = defaultdict(lambda : defaultdict(list))\n                for (i, tsv_line) in enumerate(tsv_lines):\n                    speaker = tsv_line.split('/')[0]\n                    if args.cross_speaker:\n                        speaker = 'SAME'\n                    assert speaker in SPEAKERS, 'unknown speaker! make sure the .tsv contains EMOV data'\n                    utt_id = get_utt_id(tsv_line)\n                    spkr2utts[speaker][utt_id].append(i)\n                (src_tsv, trg_tsv, src_km, trg_km) = ([], [], [], [])\n                for (speaker, utt_ids) in spkr2utts.items():\n                    for (utt_id, indices) in utt_ids.items():\n                        pairs = [(x, y) for x in indices for y in indices]\n                        if SRC_EMOTION == TRG_EMOTION:\n                            pairs = [(x, y) for (x, y) in pairs if x == y]\n                        pairs = [(x, y) for (x, y) in pairs if get_emotion(tsv_lines[x]) == SRC_EMOTION and get_emotion(tsv_lines[y]) == TRG_EMOTION]\n                        for (idx1, idx2) in pairs:\n                            assert get_utt_id(tsv_lines[idx1]) == get_utt_id(tsv_lines[idx2])\n                            src_tsv.append(tsv_lines[idx1])\n                            trg_tsv.append(tsv_lines[idx2])\n                            km_line_idx1 = km_lines[idx1]\n                            km_line_idx2 = km_lines[idx2]\n                            km_line_idx1 = km_line_idx1 if not args.dedup else dedup(km_line_idx1)\n                            km_line_idx2 = km_line_idx2 if not args.dedup else dedup(km_line_idx2)\n                            src_km.append(km_line_idx1)\n                            trg_km.append(km_line_idx2)\n                assert len(src_tsv) == len(trg_tsv) == len(src_km) == len(trg_km)\n                print(f'{len(src_tsv)} pairs')\n                if len(src_tsv) == 0:\n                    raise Exception('ERROR: generated 0 pairs!')\n                if args.dry_run:\n                    continue\n                os.makedirs(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', exist_ok=True)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{SRC_EMOTION}', 'w').writelines([root] + src_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{TRG_EMOTION}', 'w').writelines([root] + trg_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{SRC_EMOTION}', 'w').writelines(src_km)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{TRG_EMOTION}', 'w').writelines(trg_km)\n    for EMOTION in EMOTIONS + denoising_data:\n        denoising_preprocess(denoising_dir, EMOTION, args.dict)\n    os.system(f'cp {args.dict} {denoising_dir}/tokenized/dict.txt')\n    os.makedirs(translation_dir / 'tokenized', exist_ok=True)\n    for SRC_EMOTION in EMOTIONS:\n        TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n        for TRG_EMOTION in TRG_EMOTIONS:\n            translation_preprocess(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', SRC_EMOTION, TRG_EMOTION, args.dict)\n    os.system(f'cp -rf {translation_dir}/**/tokenized/* {translation_dir}/tokenized')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    desc = '\\n    this script takes as input .tsv and .km files for EMOV dataset, and a pairs of emotions.\\n    it generates parallel .tsv and .km files for these emotions. for exmaple:\\n    \u276f python build_emov_translation_manifests.py             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/train.tsv             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/emov_16khz_km_100/train.km             ~/tmp/emov_pairs             --src-emotion amused --trg-emotion neutral             --dedup --shuffle --cross-speaker --dry-run\\n    '\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('data', type=Path, help='path to a dir containing .tsv and .km files containing emov dataset')\n    parser.add_argument('output_path', type=Path, help='output directory with the manifests will be created')\n    parser.add_argument('-cs', '--cross-speaker', action='store_true', help='if set then translation will occur also between speakers, meaning the same sentence can be translated between different speakers (default: false)')\n    parser.add_argument('-dd', '--dedup', action='store_true', help=\"remove repeated tokens (example: 'aaabc=>abc')\")\n    parser.add_argument('-sh', '--shuffle', action='store_true', help='shuffle the data')\n    parser.add_argument('-ae', '--autoencode', action='store_true', help='include training pairs from the same emotion (this includes examples of the same sentence uttered by different people and examples where the src and trg are the exact same seq)')\n    parser.add_argument('-dr', '--dry-run', action='store_true', help=\"don't write anything to disk\")\n    parser.add_argument('-zs', '--zero-shot', action='store_true', help='if true, the denoising task will train on the same splits as the translation task (split by utterance id). if false, the denoising task will train on randomly sampled splits (not split by utterance id)')\n    parser.add_argument('--km-ext', default='km', help='')\n    parser.add_argument('--dict', default='/checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/fairseq.dict.txt', help='')\n    args = parser.parse_args()\n    SPEAKERS = ['bea', 'jenie', 'josh', 'sam', 'SAME']\n    EMOTIONS = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n    suffix = ''\n    if args.cross_speaker:\n        suffix += '_cross-speaker'\n    if args.dedup:\n        suffix += '_dedup'\n    translation_suffix = ''\n    if args.autoencode:\n        translation_suffix += '_autoencode'\n    denoising_suffix = ''\n    denoising_suffix += '_zeroshot' if args.zero_shot else '_nonzeroshot'\n    translation_dir = Path(args.output_path) / ('emov_multilingual_translation' + suffix + translation_suffix)\n    os.makedirs(translation_dir, exist_ok=True)\n    denoising_dir = Path(args.output_path) / ('emov_multilingual_denoising' + suffix + denoising_suffix)\n    os.makedirs(denoising_dir, exist_ok=True)\n    denoising_data = [p.name for p in (args.data / 'denoising').glob('*') if 'emov' not in p.name]\n    for split in ['train', 'valid', 'test']:\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'denoising' / 'emov' / f'{split}.tsv', km_path=args.data / 'denoising' / 'emov' / f'{split}.{args.km_ext}')\n        for EMOTION in EMOTIONS:\n            print('---')\n            print(split)\n            print(f'denoising: {EMOTION}')\n            (emotion_tsv, emotion_km) = ([], [])\n            for (tsv_line, km_line) in zip(tsv_lines, km_lines):\n                if EMOTION.lower() in tsv_line.lower():\n                    km_line = km_line if not args.dedup else dedup(km_line)\n                    emotion_tsv.append(tsv_line)\n                    emotion_km.append(km_line)\n            print(f'{len(emotion_km)} samples')\n            open(denoising_dir / f'files.{split}.{EMOTION}', 'w').writelines([root] + emotion_tsv)\n            open(denoising_dir / f'{split}.{EMOTION}', 'w').writelines(emotion_km)\n        for data in denoising_data:\n            with open(args.data / 'denoising' / data / f'{split}.{args.km_ext}', 'r') as f1:\n                with open(denoising_dir / f'{split}.{data}', 'w') as f2:\n                    f2.writelines([l if not args.dedup else dedup(l) for l in f1.readlines()])\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'translation' / f'{split}.tsv', km_path=args.data / 'translation' / f'{split}.{args.km_ext}')\n        for SRC_EMOTION in EMOTIONS:\n            TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n            for TRG_EMOTION in TRG_EMOTIONS:\n                print('---')\n                print(split)\n                print(f'src emotions: {SRC_EMOTION}\\ntrg emotions: {TRG_EMOTION}')\n                spkr2utts = defaultdict(lambda : defaultdict(list))\n                for (i, tsv_line) in enumerate(tsv_lines):\n                    speaker = tsv_line.split('/')[0]\n                    if args.cross_speaker:\n                        speaker = 'SAME'\n                    assert speaker in SPEAKERS, 'unknown speaker! make sure the .tsv contains EMOV data'\n                    utt_id = get_utt_id(tsv_line)\n                    spkr2utts[speaker][utt_id].append(i)\n                (src_tsv, trg_tsv, src_km, trg_km) = ([], [], [], [])\n                for (speaker, utt_ids) in spkr2utts.items():\n                    for (utt_id, indices) in utt_ids.items():\n                        pairs = [(x, y) for x in indices for y in indices]\n                        if SRC_EMOTION == TRG_EMOTION:\n                            pairs = [(x, y) for (x, y) in pairs if x == y]\n                        pairs = [(x, y) for (x, y) in pairs if get_emotion(tsv_lines[x]) == SRC_EMOTION and get_emotion(tsv_lines[y]) == TRG_EMOTION]\n                        for (idx1, idx2) in pairs:\n                            assert get_utt_id(tsv_lines[idx1]) == get_utt_id(tsv_lines[idx2])\n                            src_tsv.append(tsv_lines[idx1])\n                            trg_tsv.append(tsv_lines[idx2])\n                            km_line_idx1 = km_lines[idx1]\n                            km_line_idx2 = km_lines[idx2]\n                            km_line_idx1 = km_line_idx1 if not args.dedup else dedup(km_line_idx1)\n                            km_line_idx2 = km_line_idx2 if not args.dedup else dedup(km_line_idx2)\n                            src_km.append(km_line_idx1)\n                            trg_km.append(km_line_idx2)\n                assert len(src_tsv) == len(trg_tsv) == len(src_km) == len(trg_km)\n                print(f'{len(src_tsv)} pairs')\n                if len(src_tsv) == 0:\n                    raise Exception('ERROR: generated 0 pairs!')\n                if args.dry_run:\n                    continue\n                os.makedirs(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', exist_ok=True)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{SRC_EMOTION}', 'w').writelines([root] + src_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{TRG_EMOTION}', 'w').writelines([root] + trg_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{SRC_EMOTION}', 'w').writelines(src_km)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{TRG_EMOTION}', 'w').writelines(trg_km)\n    for EMOTION in EMOTIONS + denoising_data:\n        denoising_preprocess(denoising_dir, EMOTION, args.dict)\n    os.system(f'cp {args.dict} {denoising_dir}/tokenized/dict.txt')\n    os.makedirs(translation_dir / 'tokenized', exist_ok=True)\n    for SRC_EMOTION in EMOTIONS:\n        TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n        for TRG_EMOTION in TRG_EMOTIONS:\n            translation_preprocess(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', SRC_EMOTION, TRG_EMOTION, args.dict)\n    os.system(f'cp -rf {translation_dir}/**/tokenized/* {translation_dir}/tokenized')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    desc = '\\n    this script takes as input .tsv and .km files for EMOV dataset, and a pairs of emotions.\\n    it generates parallel .tsv and .km files for these emotions. for exmaple:\\n    \u276f python build_emov_translation_manifests.py             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/train.tsv             /checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/emov_16khz_km_100/train.km             ~/tmp/emov_pairs             --src-emotion amused --trg-emotion neutral             --dedup --shuffle --cross-speaker --dry-run\\n    '\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('data', type=Path, help='path to a dir containing .tsv and .km files containing emov dataset')\n    parser.add_argument('output_path', type=Path, help='output directory with the manifests will be created')\n    parser.add_argument('-cs', '--cross-speaker', action='store_true', help='if set then translation will occur also between speakers, meaning the same sentence can be translated between different speakers (default: false)')\n    parser.add_argument('-dd', '--dedup', action='store_true', help=\"remove repeated tokens (example: 'aaabc=>abc')\")\n    parser.add_argument('-sh', '--shuffle', action='store_true', help='shuffle the data')\n    parser.add_argument('-ae', '--autoencode', action='store_true', help='include training pairs from the same emotion (this includes examples of the same sentence uttered by different people and examples where the src and trg are the exact same seq)')\n    parser.add_argument('-dr', '--dry-run', action='store_true', help=\"don't write anything to disk\")\n    parser.add_argument('-zs', '--zero-shot', action='store_true', help='if true, the denoising task will train on the same splits as the translation task (split by utterance id). if false, the denoising task will train on randomly sampled splits (not split by utterance id)')\n    parser.add_argument('--km-ext', default='km', help='')\n    parser.add_argument('--dict', default='/checkpoint/felixkreuk/datasets/emov/manifests/emov_16khz/fairseq.dict.txt', help='')\n    args = parser.parse_args()\n    SPEAKERS = ['bea', 'jenie', 'josh', 'sam', 'SAME']\n    EMOTIONS = ['neutral', 'amused', 'angry', 'disgusted', 'sleepy']\n    suffix = ''\n    if args.cross_speaker:\n        suffix += '_cross-speaker'\n    if args.dedup:\n        suffix += '_dedup'\n    translation_suffix = ''\n    if args.autoencode:\n        translation_suffix += '_autoencode'\n    denoising_suffix = ''\n    denoising_suffix += '_zeroshot' if args.zero_shot else '_nonzeroshot'\n    translation_dir = Path(args.output_path) / ('emov_multilingual_translation' + suffix + translation_suffix)\n    os.makedirs(translation_dir, exist_ok=True)\n    denoising_dir = Path(args.output_path) / ('emov_multilingual_denoising' + suffix + denoising_suffix)\n    os.makedirs(denoising_dir, exist_ok=True)\n    denoising_data = [p.name for p in (args.data / 'denoising').glob('*') if 'emov' not in p.name]\n    for split in ['train', 'valid', 'test']:\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'denoising' / 'emov' / f'{split}.tsv', km_path=args.data / 'denoising' / 'emov' / f'{split}.{args.km_ext}')\n        for EMOTION in EMOTIONS:\n            print('---')\n            print(split)\n            print(f'denoising: {EMOTION}')\n            (emotion_tsv, emotion_km) = ([], [])\n            for (tsv_line, km_line) in zip(tsv_lines, km_lines):\n                if EMOTION.lower() in tsv_line.lower():\n                    km_line = km_line if not args.dedup else dedup(km_line)\n                    emotion_tsv.append(tsv_line)\n                    emotion_km.append(km_line)\n            print(f'{len(emotion_km)} samples')\n            open(denoising_dir / f'files.{split}.{EMOTION}', 'w').writelines([root] + emotion_tsv)\n            open(denoising_dir / f'{split}.{EMOTION}', 'w').writelines(emotion_km)\n        for data in denoising_data:\n            with open(args.data / 'denoising' / data / f'{split}.{args.km_ext}', 'r') as f1:\n                with open(denoising_dir / f'{split}.{data}', 'w') as f2:\n                    f2.writelines([l if not args.dedup else dedup(l) for l in f1.readlines()])\n        (root, tsv_lines, km_lines) = load_tsv_km(tsv_path=args.data / 'translation' / f'{split}.tsv', km_path=args.data / 'translation' / f'{split}.{args.km_ext}')\n        for SRC_EMOTION in EMOTIONS:\n            TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n            for TRG_EMOTION in TRG_EMOTIONS:\n                print('---')\n                print(split)\n                print(f'src emotions: {SRC_EMOTION}\\ntrg emotions: {TRG_EMOTION}')\n                spkr2utts = defaultdict(lambda : defaultdict(list))\n                for (i, tsv_line) in enumerate(tsv_lines):\n                    speaker = tsv_line.split('/')[0]\n                    if args.cross_speaker:\n                        speaker = 'SAME'\n                    assert speaker in SPEAKERS, 'unknown speaker! make sure the .tsv contains EMOV data'\n                    utt_id = get_utt_id(tsv_line)\n                    spkr2utts[speaker][utt_id].append(i)\n                (src_tsv, trg_tsv, src_km, trg_km) = ([], [], [], [])\n                for (speaker, utt_ids) in spkr2utts.items():\n                    for (utt_id, indices) in utt_ids.items():\n                        pairs = [(x, y) for x in indices for y in indices]\n                        if SRC_EMOTION == TRG_EMOTION:\n                            pairs = [(x, y) for (x, y) in pairs if x == y]\n                        pairs = [(x, y) for (x, y) in pairs if get_emotion(tsv_lines[x]) == SRC_EMOTION and get_emotion(tsv_lines[y]) == TRG_EMOTION]\n                        for (idx1, idx2) in pairs:\n                            assert get_utt_id(tsv_lines[idx1]) == get_utt_id(tsv_lines[idx2])\n                            src_tsv.append(tsv_lines[idx1])\n                            trg_tsv.append(tsv_lines[idx2])\n                            km_line_idx1 = km_lines[idx1]\n                            km_line_idx2 = km_lines[idx2]\n                            km_line_idx1 = km_line_idx1 if not args.dedup else dedup(km_line_idx1)\n                            km_line_idx2 = km_line_idx2 if not args.dedup else dedup(km_line_idx2)\n                            src_km.append(km_line_idx1)\n                            trg_km.append(km_line_idx2)\n                assert len(src_tsv) == len(trg_tsv) == len(src_km) == len(trg_km)\n                print(f'{len(src_tsv)} pairs')\n                if len(src_tsv) == 0:\n                    raise Exception('ERROR: generated 0 pairs!')\n                if args.dry_run:\n                    continue\n                os.makedirs(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', exist_ok=True)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{SRC_EMOTION}', 'w').writelines([root] + src_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'files.{split}.{TRG_EMOTION}', 'w').writelines([root] + trg_tsv)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{SRC_EMOTION}', 'w').writelines(src_km)\n                open(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}' / f'{split}.{TRG_EMOTION}', 'w').writelines(trg_km)\n    for EMOTION in EMOTIONS + denoising_data:\n        denoising_preprocess(denoising_dir, EMOTION, args.dict)\n    os.system(f'cp {args.dict} {denoising_dir}/tokenized/dict.txt')\n    os.makedirs(translation_dir / 'tokenized', exist_ok=True)\n    for SRC_EMOTION in EMOTIONS:\n        TRG_EMOTIONS = EMOTIONS if args.autoencode else set(EMOTIONS) - set([SRC_EMOTION])\n        for TRG_EMOTION in TRG_EMOTIONS:\n            translation_preprocess(translation_dir / f'{SRC_EMOTION}-{TRG_EMOTION}', SRC_EMOTION, TRG_EMOTION, args.dict)\n    os.system(f'cp -rf {translation_dir}/**/tokenized/* {translation_dir}/tokenized')"
        ]
    }
]