[
    {
        "func_name": "_parse_input",
        "original": "def _parse_input(input_value):\n    (component_input, job_input) = (None, None)\n    if isinstance(input_value, Input):\n        component_input = Input(**input_value._to_dict())\n        input_type = input_value.type\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value._to_dict())\n    elif isinstance(input_value, dict):\n        input_type = input_value.get('type', None)\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value)\n        component_input = Input(**input_value)\n    elif isinstance(input_value, str):\n        component_input = ComponentTranslatableMixin._to_input_builder_function(input_value)\n        job_input = input_value\n    elif isinstance(input_value, (PipelineInput, NodeOutput)):\n        if input_value._data is None or isinstance(input_value._data, Output):\n            data = Input(type=input_value.type, mode=input_value.mode)\n        else:\n            data = input_value._data\n        (component_input, _) = _parse_input(data)\n        job_input = input_value\n    else:\n        msg = f'Unsupported input type: {type(input_value)}, only Input, dict, str, PipelineInput and NodeOutput are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_input, job_input)",
        "mutated": [
            "def _parse_input(input_value):\n    if False:\n        i = 10\n    (component_input, job_input) = (None, None)\n    if isinstance(input_value, Input):\n        component_input = Input(**input_value._to_dict())\n        input_type = input_value.type\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value._to_dict())\n    elif isinstance(input_value, dict):\n        input_type = input_value.get('type', None)\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value)\n        component_input = Input(**input_value)\n    elif isinstance(input_value, str):\n        component_input = ComponentTranslatableMixin._to_input_builder_function(input_value)\n        job_input = input_value\n    elif isinstance(input_value, (PipelineInput, NodeOutput)):\n        if input_value._data is None or isinstance(input_value._data, Output):\n            data = Input(type=input_value.type, mode=input_value.mode)\n        else:\n            data = input_value._data\n        (component_input, _) = _parse_input(data)\n        job_input = input_value\n    else:\n        msg = f'Unsupported input type: {type(input_value)}, only Input, dict, str, PipelineInput and NodeOutput are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_input, job_input)",
            "def _parse_input(input_value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (component_input, job_input) = (None, None)\n    if isinstance(input_value, Input):\n        component_input = Input(**input_value._to_dict())\n        input_type = input_value.type\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value._to_dict())\n    elif isinstance(input_value, dict):\n        input_type = input_value.get('type', None)\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value)\n        component_input = Input(**input_value)\n    elif isinstance(input_value, str):\n        component_input = ComponentTranslatableMixin._to_input_builder_function(input_value)\n        job_input = input_value\n    elif isinstance(input_value, (PipelineInput, NodeOutput)):\n        if input_value._data is None or isinstance(input_value._data, Output):\n            data = Input(type=input_value.type, mode=input_value.mode)\n        else:\n            data = input_value._data\n        (component_input, _) = _parse_input(data)\n        job_input = input_value\n    else:\n        msg = f'Unsupported input type: {type(input_value)}, only Input, dict, str, PipelineInput and NodeOutput are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_input, job_input)",
            "def _parse_input(input_value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (component_input, job_input) = (None, None)\n    if isinstance(input_value, Input):\n        component_input = Input(**input_value._to_dict())\n        input_type = input_value.type\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value._to_dict())\n    elif isinstance(input_value, dict):\n        input_type = input_value.get('type', None)\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value)\n        component_input = Input(**input_value)\n    elif isinstance(input_value, str):\n        component_input = ComponentTranslatableMixin._to_input_builder_function(input_value)\n        job_input = input_value\n    elif isinstance(input_value, (PipelineInput, NodeOutput)):\n        if input_value._data is None or isinstance(input_value._data, Output):\n            data = Input(type=input_value.type, mode=input_value.mode)\n        else:\n            data = input_value._data\n        (component_input, _) = _parse_input(data)\n        job_input = input_value\n    else:\n        msg = f'Unsupported input type: {type(input_value)}, only Input, dict, str, PipelineInput and NodeOutput are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_input, job_input)",
            "def _parse_input(input_value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (component_input, job_input) = (None, None)\n    if isinstance(input_value, Input):\n        component_input = Input(**input_value._to_dict())\n        input_type = input_value.type\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value._to_dict())\n    elif isinstance(input_value, dict):\n        input_type = input_value.get('type', None)\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value)\n        component_input = Input(**input_value)\n    elif isinstance(input_value, str):\n        component_input = ComponentTranslatableMixin._to_input_builder_function(input_value)\n        job_input = input_value\n    elif isinstance(input_value, (PipelineInput, NodeOutput)):\n        if input_value._data is None or isinstance(input_value._data, Output):\n            data = Input(type=input_value.type, mode=input_value.mode)\n        else:\n            data = input_value._data\n        (component_input, _) = _parse_input(data)\n        job_input = input_value\n    else:\n        msg = f'Unsupported input type: {type(input_value)}, only Input, dict, str, PipelineInput and NodeOutput are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_input, job_input)",
            "def _parse_input(input_value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (component_input, job_input) = (None, None)\n    if isinstance(input_value, Input):\n        component_input = Input(**input_value._to_dict())\n        input_type = input_value.type\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value._to_dict())\n    elif isinstance(input_value, dict):\n        input_type = input_value.get('type', None)\n        if input_type in SUPPORTED_INPUTS:\n            job_input = Input(**input_value)\n        component_input = Input(**input_value)\n    elif isinstance(input_value, str):\n        component_input = ComponentTranslatableMixin._to_input_builder_function(input_value)\n        job_input = input_value\n    elif isinstance(input_value, (PipelineInput, NodeOutput)):\n        if input_value._data is None or isinstance(input_value._data, Output):\n            data = Input(type=input_value.type, mode=input_value.mode)\n        else:\n            data = input_value._data\n        (component_input, _) = _parse_input(data)\n        job_input = input_value\n    else:\n        msg = f'Unsupported input type: {type(input_value)}, only Input, dict, str, PipelineInput and NodeOutput are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_input, job_input)"
        ]
    },
    {
        "func_name": "_parse_output",
        "original": "def _parse_output(output_value):\n    (component_output, job_output) = (None, None)\n    if isinstance(output_value, Output):\n        component_output = Output(**output_value._to_dict())\n        job_output = Output(**output_value._to_dict())\n    elif not output_value:\n        component_output = ComponentTranslatableMixin._to_output(output_value)\n        job_output = output_value\n    elif isinstance(output_value, dict):\n        job_output = Output(**output_value)\n        component_output = Output(**output_value)\n    elif isinstance(output_value, str):\n        job_output = output_value\n    else:\n        msg = f'Unsupported output type: {type(output_value)}, only Output and dict are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_output, job_output)",
        "mutated": [
            "def _parse_output(output_value):\n    if False:\n        i = 10\n    (component_output, job_output) = (None, None)\n    if isinstance(output_value, Output):\n        component_output = Output(**output_value._to_dict())\n        job_output = Output(**output_value._to_dict())\n    elif not output_value:\n        component_output = ComponentTranslatableMixin._to_output(output_value)\n        job_output = output_value\n    elif isinstance(output_value, dict):\n        job_output = Output(**output_value)\n        component_output = Output(**output_value)\n    elif isinstance(output_value, str):\n        job_output = output_value\n    else:\n        msg = f'Unsupported output type: {type(output_value)}, only Output and dict are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_output, job_output)",
            "def _parse_output(output_value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (component_output, job_output) = (None, None)\n    if isinstance(output_value, Output):\n        component_output = Output(**output_value._to_dict())\n        job_output = Output(**output_value._to_dict())\n    elif not output_value:\n        component_output = ComponentTranslatableMixin._to_output(output_value)\n        job_output = output_value\n    elif isinstance(output_value, dict):\n        job_output = Output(**output_value)\n        component_output = Output(**output_value)\n    elif isinstance(output_value, str):\n        job_output = output_value\n    else:\n        msg = f'Unsupported output type: {type(output_value)}, only Output and dict are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_output, job_output)",
            "def _parse_output(output_value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (component_output, job_output) = (None, None)\n    if isinstance(output_value, Output):\n        component_output = Output(**output_value._to_dict())\n        job_output = Output(**output_value._to_dict())\n    elif not output_value:\n        component_output = ComponentTranslatableMixin._to_output(output_value)\n        job_output = output_value\n    elif isinstance(output_value, dict):\n        job_output = Output(**output_value)\n        component_output = Output(**output_value)\n    elif isinstance(output_value, str):\n        job_output = output_value\n    else:\n        msg = f'Unsupported output type: {type(output_value)}, only Output and dict are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_output, job_output)",
            "def _parse_output(output_value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (component_output, job_output) = (None, None)\n    if isinstance(output_value, Output):\n        component_output = Output(**output_value._to_dict())\n        job_output = Output(**output_value._to_dict())\n    elif not output_value:\n        component_output = ComponentTranslatableMixin._to_output(output_value)\n        job_output = output_value\n    elif isinstance(output_value, dict):\n        job_output = Output(**output_value)\n        component_output = Output(**output_value)\n    elif isinstance(output_value, str):\n        job_output = output_value\n    else:\n        msg = f'Unsupported output type: {type(output_value)}, only Output and dict are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_output, job_output)",
            "def _parse_output(output_value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (component_output, job_output) = (None, None)\n    if isinstance(output_value, Output):\n        component_output = Output(**output_value._to_dict())\n        job_output = Output(**output_value._to_dict())\n    elif not output_value:\n        component_output = ComponentTranslatableMixin._to_output(output_value)\n        job_output = output_value\n    elif isinstance(output_value, dict):\n        job_output = Output(**output_value)\n        component_output = Output(**output_value)\n    elif isinstance(output_value, str):\n        job_output = output_value\n    else:\n        msg = f'Unsupported output type: {type(output_value)}, only Output and dict are supported.'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.JOB, error_type=ValidationErrorType.INVALID_VALUE)\n    return (component_output, job_output)"
        ]
    },
    {
        "func_name": "_parse_inputs_outputs",
        "original": "def _parse_inputs_outputs(io_dict: Dict, parse_func: Callable) -> Tuple[Dict, Dict]:\n    (component_io_dict, job_io_dict) = ({}, {})\n    if io_dict:\n        for (key, val) in io_dict.items():\n            (component_io, job_io) = parse_func(val)\n            component_io_dict[key] = component_io\n            job_io_dict[key] = job_io\n    return (component_io_dict, job_io_dict)",
        "mutated": [
            "def _parse_inputs_outputs(io_dict: Dict, parse_func: Callable) -> Tuple[Dict, Dict]:\n    if False:\n        i = 10\n    (component_io_dict, job_io_dict) = ({}, {})\n    if io_dict:\n        for (key, val) in io_dict.items():\n            (component_io, job_io) = parse_func(val)\n            component_io_dict[key] = component_io\n            job_io_dict[key] = job_io\n    return (component_io_dict, job_io_dict)",
            "def _parse_inputs_outputs(io_dict: Dict, parse_func: Callable) -> Tuple[Dict, Dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (component_io_dict, job_io_dict) = ({}, {})\n    if io_dict:\n        for (key, val) in io_dict.items():\n            (component_io, job_io) = parse_func(val)\n            component_io_dict[key] = component_io\n            job_io_dict[key] = job_io\n    return (component_io_dict, job_io_dict)",
            "def _parse_inputs_outputs(io_dict: Dict, parse_func: Callable) -> Tuple[Dict, Dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (component_io_dict, job_io_dict) = ({}, {})\n    if io_dict:\n        for (key, val) in io_dict.items():\n            (component_io, job_io) = parse_func(val)\n            component_io_dict[key] = component_io\n            job_io_dict[key] = job_io\n    return (component_io_dict, job_io_dict)",
            "def _parse_inputs_outputs(io_dict: Dict, parse_func: Callable) -> Tuple[Dict, Dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (component_io_dict, job_io_dict) = ({}, {})\n    if io_dict:\n        for (key, val) in io_dict.items():\n            (component_io, job_io) = parse_func(val)\n            component_io_dict[key] = component_io\n            job_io_dict[key] = job_io\n    return (component_io_dict, job_io_dict)",
            "def _parse_inputs_outputs(io_dict: Dict, parse_func: Callable) -> Tuple[Dict, Dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (component_io_dict, job_io_dict) = ({}, {})\n    if io_dict:\n        for (key, val) in io_dict.items():\n            (component_io, job_io) = parse_func(val)\n            component_io_dict[key] = component_io\n            job_io_dict[key] = job_io\n    return (component_io_dict, job_io_dict)"
        ]
    },
    {
        "func_name": "_build_data_index",
        "original": "def _build_data_index(io_dict: Union[Dict, DataIndex]):\n    if io_dict is None:\n        return io_dict\n    if isinstance(io_dict, DataIndex):\n        component_io = io_dict\n    elif isinstance(io_dict, dict):\n        component_io = DataIndex(**io_dict)\n    else:\n        msg = 'data_index only support dict and DataIndex'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.DATA, error_category=ErrorCategory.USER_ERROR, error_type=ValidationErrorType.INVALID_VALUE)\n    return component_io",
        "mutated": [
            "def _build_data_index(io_dict: Union[Dict, DataIndex]):\n    if False:\n        i = 10\n    if io_dict is None:\n        return io_dict\n    if isinstance(io_dict, DataIndex):\n        component_io = io_dict\n    elif isinstance(io_dict, dict):\n        component_io = DataIndex(**io_dict)\n    else:\n        msg = 'data_index only support dict and DataIndex'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.DATA, error_category=ErrorCategory.USER_ERROR, error_type=ValidationErrorType.INVALID_VALUE)\n    return component_io",
            "def _build_data_index(io_dict: Union[Dict, DataIndex]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if io_dict is None:\n        return io_dict\n    if isinstance(io_dict, DataIndex):\n        component_io = io_dict\n    elif isinstance(io_dict, dict):\n        component_io = DataIndex(**io_dict)\n    else:\n        msg = 'data_index only support dict and DataIndex'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.DATA, error_category=ErrorCategory.USER_ERROR, error_type=ValidationErrorType.INVALID_VALUE)\n    return component_io",
            "def _build_data_index(io_dict: Union[Dict, DataIndex]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if io_dict is None:\n        return io_dict\n    if isinstance(io_dict, DataIndex):\n        component_io = io_dict\n    elif isinstance(io_dict, dict):\n        component_io = DataIndex(**io_dict)\n    else:\n        msg = 'data_index only support dict and DataIndex'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.DATA, error_category=ErrorCategory.USER_ERROR, error_type=ValidationErrorType.INVALID_VALUE)\n    return component_io",
            "def _build_data_index(io_dict: Union[Dict, DataIndex]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if io_dict is None:\n        return io_dict\n    if isinstance(io_dict, DataIndex):\n        component_io = io_dict\n    elif isinstance(io_dict, dict):\n        component_io = DataIndex(**io_dict)\n    else:\n        msg = 'data_index only support dict and DataIndex'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.DATA, error_category=ErrorCategory.USER_ERROR, error_type=ValidationErrorType.INVALID_VALUE)\n    return component_io",
            "def _build_data_index(io_dict: Union[Dict, DataIndex]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if io_dict is None:\n        return io_dict\n    if isinstance(io_dict, DataIndex):\n        component_io = io_dict\n    elif isinstance(io_dict, dict):\n        component_io = DataIndex(**io_dict)\n    else:\n        msg = 'data_index only support dict and DataIndex'\n        raise ValidationException(message=msg, no_personal_data_message=msg, target=ErrorTarget.DATA, error_category=ErrorCategory.USER_ERROR, error_type=ValidationErrorType.INVALID_VALUE)\n    return component_io"
        ]
    },
    {
        "func_name": "index_data",
        "original": "@experimental\n@pipeline_node_decorator\ndef index_data(*, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, ml_client: Optional[Any]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None, **kwargs) -> PipelineJob:\n    \"\"\"\n    Create a PipelineJob object which can be used inside dsl.pipeline.\n\n    :keywork data_index: The data index configuration.\n    :type data_index: DataIndex\n    :keyword description: Description of the job.\n    :type description: str\n    :keyword tags: Tag dictionary. Tags can be added, removed, and updated.\n    :type tags: dict[str, str]\n    :keyword display_name: Display name of the job.\n    :type display_name: str\n    :keyword experiment_name: Name of the experiment the job will be created under.\n    :type experiment_name: str\n    :keyword compute: The compute resource the job runs on.\n    :type compute: str\n    :keyword serverless_instance_type: The instance type to use for serverless compute.\n    :type serverless_instance_type: Optional[str]\n    :keyword ml_client: The ml client to use for the job.\n    :type ml_client: Any\n    :keyword identity: Identity configuration for the job.\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\n    :keyword input_data_override: Input data override for the job.\n        Used to pipe output of step into DataIndex Job in a pipeline.\n    :type input_data_override: Optional[Input]\n    :return: A PipelineJob object.\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\n    \"\"\"\n    data_index = _build_data_index(data_index)\n    if data_index.index.type == DataIndexTypes.FAISS:\n        configured_component = data_index_faiss(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    elif data_index.index.type == DataIndexTypes.ACS:\n        if kwargs.get('incremental_update', False):\n            configured_component = data_index_incremental_update_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n        else:\n            configured_component = data_index_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    else:\n        raise ValueError(f'Unsupported index type: {data_index.index.type}')\n    configured_component.properties['azureml.mlIndexAssetName'] = data_index.name\n    configured_component.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    configured_component.properties['azureml.mlIndexAssetSource'] = 'Data Asset'\n    return configured_component",
        "mutated": [
            "@experimental\n@pipeline_node_decorator\ndef index_data(*, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, ml_client: Optional[Any]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n    '\\n    Create a PipelineJob object which can be used inside dsl.pipeline.\\n\\n    :keywork data_index: The data index configuration.\\n    :type data_index: DataIndex\\n    :keyword description: Description of the job.\\n    :type description: str\\n    :keyword tags: Tag dictionary. Tags can be added, removed, and updated.\\n    :type tags: dict[str, str]\\n    :keyword display_name: Display name of the job.\\n    :type display_name: str\\n    :keyword experiment_name: Name of the experiment the job will be created under.\\n    :type experiment_name: str\\n    :keyword compute: The compute resource the job runs on.\\n    :type compute: str\\n    :keyword serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :keyword ml_client: The ml client to use for the job.\\n    :type ml_client: Any\\n    :keyword identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :keyword input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :return: A PipelineJob object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    data_index = _build_data_index(data_index)\n    if data_index.index.type == DataIndexTypes.FAISS:\n        configured_component = data_index_faiss(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    elif data_index.index.type == DataIndexTypes.ACS:\n        if kwargs.get('incremental_update', False):\n            configured_component = data_index_incremental_update_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n        else:\n            configured_component = data_index_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    else:\n        raise ValueError(f'Unsupported index type: {data_index.index.type}')\n    configured_component.properties['azureml.mlIndexAssetName'] = data_index.name\n    configured_component.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    configured_component.properties['azureml.mlIndexAssetSource'] = 'Data Asset'\n    return configured_component",
            "@experimental\n@pipeline_node_decorator\ndef index_data(*, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, ml_client: Optional[Any]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Create a PipelineJob object which can be used inside dsl.pipeline.\\n\\n    :keywork data_index: The data index configuration.\\n    :type data_index: DataIndex\\n    :keyword description: Description of the job.\\n    :type description: str\\n    :keyword tags: Tag dictionary. Tags can be added, removed, and updated.\\n    :type tags: dict[str, str]\\n    :keyword display_name: Display name of the job.\\n    :type display_name: str\\n    :keyword experiment_name: Name of the experiment the job will be created under.\\n    :type experiment_name: str\\n    :keyword compute: The compute resource the job runs on.\\n    :type compute: str\\n    :keyword serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :keyword ml_client: The ml client to use for the job.\\n    :type ml_client: Any\\n    :keyword identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :keyword input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :return: A PipelineJob object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    data_index = _build_data_index(data_index)\n    if data_index.index.type == DataIndexTypes.FAISS:\n        configured_component = data_index_faiss(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    elif data_index.index.type == DataIndexTypes.ACS:\n        if kwargs.get('incremental_update', False):\n            configured_component = data_index_incremental_update_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n        else:\n            configured_component = data_index_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    else:\n        raise ValueError(f'Unsupported index type: {data_index.index.type}')\n    configured_component.properties['azureml.mlIndexAssetName'] = data_index.name\n    configured_component.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    configured_component.properties['azureml.mlIndexAssetSource'] = 'Data Asset'\n    return configured_component",
            "@experimental\n@pipeline_node_decorator\ndef index_data(*, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, ml_client: Optional[Any]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Create a PipelineJob object which can be used inside dsl.pipeline.\\n\\n    :keywork data_index: The data index configuration.\\n    :type data_index: DataIndex\\n    :keyword description: Description of the job.\\n    :type description: str\\n    :keyword tags: Tag dictionary. Tags can be added, removed, and updated.\\n    :type tags: dict[str, str]\\n    :keyword display_name: Display name of the job.\\n    :type display_name: str\\n    :keyword experiment_name: Name of the experiment the job will be created under.\\n    :type experiment_name: str\\n    :keyword compute: The compute resource the job runs on.\\n    :type compute: str\\n    :keyword serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :keyword ml_client: The ml client to use for the job.\\n    :type ml_client: Any\\n    :keyword identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :keyword input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :return: A PipelineJob object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    data_index = _build_data_index(data_index)\n    if data_index.index.type == DataIndexTypes.FAISS:\n        configured_component = data_index_faiss(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    elif data_index.index.type == DataIndexTypes.ACS:\n        if kwargs.get('incremental_update', False):\n            configured_component = data_index_incremental_update_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n        else:\n            configured_component = data_index_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    else:\n        raise ValueError(f'Unsupported index type: {data_index.index.type}')\n    configured_component.properties['azureml.mlIndexAssetName'] = data_index.name\n    configured_component.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    configured_component.properties['azureml.mlIndexAssetSource'] = 'Data Asset'\n    return configured_component",
            "@experimental\n@pipeline_node_decorator\ndef index_data(*, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, ml_client: Optional[Any]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Create a PipelineJob object which can be used inside dsl.pipeline.\\n\\n    :keywork data_index: The data index configuration.\\n    :type data_index: DataIndex\\n    :keyword description: Description of the job.\\n    :type description: str\\n    :keyword tags: Tag dictionary. Tags can be added, removed, and updated.\\n    :type tags: dict[str, str]\\n    :keyword display_name: Display name of the job.\\n    :type display_name: str\\n    :keyword experiment_name: Name of the experiment the job will be created under.\\n    :type experiment_name: str\\n    :keyword compute: The compute resource the job runs on.\\n    :type compute: str\\n    :keyword serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :keyword ml_client: The ml client to use for the job.\\n    :type ml_client: Any\\n    :keyword identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :keyword input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :return: A PipelineJob object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    data_index = _build_data_index(data_index)\n    if data_index.index.type == DataIndexTypes.FAISS:\n        configured_component = data_index_faiss(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    elif data_index.index.type == DataIndexTypes.ACS:\n        if kwargs.get('incremental_update', False):\n            configured_component = data_index_incremental_update_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n        else:\n            configured_component = data_index_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    else:\n        raise ValueError(f'Unsupported index type: {data_index.index.type}')\n    configured_component.properties['azureml.mlIndexAssetName'] = data_index.name\n    configured_component.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    configured_component.properties['azureml.mlIndexAssetSource'] = 'Data Asset'\n    return configured_component",
            "@experimental\n@pipeline_node_decorator\ndef index_data(*, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, ml_client: Optional[Any]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Create a PipelineJob object which can be used inside dsl.pipeline.\\n\\n    :keywork data_index: The data index configuration.\\n    :type data_index: DataIndex\\n    :keyword description: Description of the job.\\n    :type description: str\\n    :keyword tags: Tag dictionary. Tags can be added, removed, and updated.\\n    :type tags: dict[str, str]\\n    :keyword display_name: Display name of the job.\\n    :type display_name: str\\n    :keyword experiment_name: Name of the experiment the job will be created under.\\n    :type experiment_name: str\\n    :keyword compute: The compute resource the job runs on.\\n    :type compute: str\\n    :keyword serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :keyword ml_client: The ml client to use for the job.\\n    :type ml_client: Any\\n    :keyword identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :keyword input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :return: A PipelineJob object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    data_index = _build_data_index(data_index)\n    if data_index.index.type == DataIndexTypes.FAISS:\n        configured_component = data_index_faiss(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    elif data_index.index.type == DataIndexTypes.ACS:\n        if kwargs.get('incremental_update', False):\n            configured_component = data_index_incremental_update_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n        else:\n            configured_component = data_index_acs(ml_client, data_index, description, tags, name, display_name, experiment_name, compute, serverless_instance_type, identity, input_data_override)\n    else:\n        raise ValueError(f'Unsupported index type: {data_index.index.type}')\n    configured_component.properties['azureml.mlIndexAssetName'] = data_index.name\n    configured_component.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    configured_component.properties['azureml.mlIndexAssetSource'] = 'Data Asset'\n    return configured_component"
        ]
    },
    {
        "func_name": "data_index_acs_pipeline",
        "original": "@pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param chunk_overlap: The number of tokens to overlap between chunks.\n        :type chunk_overlap: int\n        :param input_glob: The glob pattern to use when searching for input data.\n        :type input_glob: str\n        :param citation_url: The URL to use when generating citations for the input data.\n        :type citation_url: str\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\n        :type citation_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n    crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(embeddings_container):\n        crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        crack_and_chunk_and_embed.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
        "mutated": [
            "@pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param chunk_overlap: The number of tokens to overlap between chunks.\\n        :type chunk_overlap: int\\n        :param input_glob: The glob pattern to use when searching for input data.\\n        :type input_glob: str\\n        :param citation_url: The URL to use when generating citations for the input data.\\n        :type citation_url: str\\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\\n        :type citation_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(embeddings_container):\n        crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        crack_and_chunk_and_embed.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param chunk_overlap: The number of tokens to overlap between chunks.\\n        :type chunk_overlap: int\\n        :param input_glob: The glob pattern to use when searching for input data.\\n        :type input_glob: str\\n        :param citation_url: The URL to use when generating citations for the input data.\\n        :type citation_url: str\\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\\n        :type citation_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(embeddings_container):\n        crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        crack_and_chunk_and_embed.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param chunk_overlap: The number of tokens to overlap between chunks.\\n        :type chunk_overlap: int\\n        :param input_glob: The glob pattern to use when searching for input data.\\n        :type input_glob: str\\n        :param citation_url: The URL to use when generating citations for the input data.\\n        :type citation_url: str\\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\\n        :type citation_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(embeddings_container):\n        crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        crack_and_chunk_and_embed.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param chunk_overlap: The number of tokens to overlap between chunks.\\n        :type chunk_overlap: int\\n        :param input_glob: The glob pattern to use when searching for input data.\\n        :type input_glob: str\\n        :param citation_url: The URL to use when generating citations for the input data.\\n        :type citation_url: str\\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\\n        :type citation_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(embeddings_container):\n        crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        crack_and_chunk_and_embed.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param chunk_overlap: The number of tokens to overlap between chunks.\\n        :type chunk_overlap: int\\n        :param input_glob: The glob pattern to use when searching for input data.\\n        :type input_glob: str\\n        :param citation_url: The URL to use when generating citations for the input data.\\n        :type citation_url: str\\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\\n        :type citation_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(embeddings_container):\n        crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        crack_and_chunk_and_embed.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}"
        ]
    },
    {
        "func_name": "data_index_incremental_update_acs",
        "original": "def data_index_incremental_update_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_and_embed_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK_AND_EMBED)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param chunk_overlap: The number of tokens to overlap between chunks.\n        :type chunk_overlap: int\n        :param input_glob: The glob pattern to use when searching for input data.\n        :type input_glob: str\n        :param citation_url: The URL to use when generating citations for the input data.\n        :type citation_url: str\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\n        :type citation_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(embeddings_container):\n            crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            crack_and_chunk_and_embed.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name, 'full_sync': True}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, input_glob=data_index.source.input_glob, chunk_size=data_index.source.chunk_size, chunk_overlap=data_index.source.chunk_overlap, citation_url=data_index.source.citation_url, citation_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, embeddings_model=build_model_protocol(data_index.embedding.model), aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None, acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection))\n    component.inputs['input_glob']._meta.optional = True\n    component.inputs['chunk_size']._meta.optional = True\n    component.inputs['chunk_overlap']._meta.optional = True\n    component.inputs['citation_url']._meta.optional = True\n    component.inputs['citation_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
        "mutated": [
            "def data_index_incremental_update_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_and_embed_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK_AND_EMBED)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param chunk_overlap: The number of tokens to overlap between chunks.\n        :type chunk_overlap: int\n        :param input_glob: The glob pattern to use when searching for input data.\n        :type input_glob: str\n        :param citation_url: The URL to use when generating citations for the input data.\n        :type citation_url: str\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\n        :type citation_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(embeddings_container):\n            crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            crack_and_chunk_and_embed.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name, 'full_sync': True}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, input_glob=data_index.source.input_glob, chunk_size=data_index.source.chunk_size, chunk_overlap=data_index.source.chunk_overlap, citation_url=data_index.source.citation_url, citation_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, embeddings_model=build_model_protocol(data_index.embedding.model), aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None, acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection))\n    component.inputs['input_glob']._meta.optional = True\n    component.inputs['chunk_size']._meta.optional = True\n    component.inputs['chunk_overlap']._meta.optional = True\n    component.inputs['citation_url']._meta.optional = True\n    component.inputs['citation_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_incremental_update_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_and_embed_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK_AND_EMBED)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param chunk_overlap: The number of tokens to overlap between chunks.\n        :type chunk_overlap: int\n        :param input_glob: The glob pattern to use when searching for input data.\n        :type input_glob: str\n        :param citation_url: The URL to use when generating citations for the input data.\n        :type citation_url: str\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\n        :type citation_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(embeddings_container):\n            crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            crack_and_chunk_and_embed.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name, 'full_sync': True}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, input_glob=data_index.source.input_glob, chunk_size=data_index.source.chunk_size, chunk_overlap=data_index.source.chunk_overlap, citation_url=data_index.source.citation_url, citation_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, embeddings_model=build_model_protocol(data_index.embedding.model), aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None, acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection))\n    component.inputs['input_glob']._meta.optional = True\n    component.inputs['chunk_size']._meta.optional = True\n    component.inputs['chunk_overlap']._meta.optional = True\n    component.inputs['citation_url']._meta.optional = True\n    component.inputs['citation_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_incremental_update_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_and_embed_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK_AND_EMBED)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param chunk_overlap: The number of tokens to overlap between chunks.\n        :type chunk_overlap: int\n        :param input_glob: The glob pattern to use when searching for input data.\n        :type input_glob: str\n        :param citation_url: The URL to use when generating citations for the input data.\n        :type citation_url: str\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\n        :type citation_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(embeddings_container):\n            crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            crack_and_chunk_and_embed.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name, 'full_sync': True}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, input_glob=data_index.source.input_glob, chunk_size=data_index.source.chunk_size, chunk_overlap=data_index.source.chunk_overlap, citation_url=data_index.source.citation_url, citation_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, embeddings_model=build_model_protocol(data_index.embedding.model), aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None, acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection))\n    component.inputs['input_glob']._meta.optional = True\n    component.inputs['chunk_size']._meta.optional = True\n    component.inputs['chunk_overlap']._meta.optional = True\n    component.inputs['citation_url']._meta.optional = True\n    component.inputs['citation_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_incremental_update_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_and_embed_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK_AND_EMBED)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param chunk_overlap: The number of tokens to overlap between chunks.\n        :type chunk_overlap: int\n        :param input_glob: The glob pattern to use when searching for input data.\n        :type input_glob: str\n        :param citation_url: The URL to use when generating citations for the input data.\n        :type citation_url: str\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\n        :type citation_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(embeddings_container):\n            crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            crack_and_chunk_and_embed.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name, 'full_sync': True}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, input_glob=data_index.source.input_glob, chunk_size=data_index.source.chunk_size, chunk_overlap=data_index.source.chunk_overlap, citation_url=data_index.source.citation_url, citation_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, embeddings_model=build_model_protocol(data_index.embedding.model), aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None, acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection))\n    component.inputs['input_glob']._meta.optional = True\n    component.inputs['chunk_size']._meta.optional = True\n    component.inputs['chunk_overlap']._meta.optional = True\n    component.inputs['citation_url']._meta.optional = True\n    component.inputs['citation_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_incremental_update_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_and_embed_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK_AND_EMBED)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_incremental_update_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS (Incremental Update)', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=768, chunk_overlap: int=0, input_glob: str='**/*', citation_url: str=None, citation_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param chunk_overlap: The number of tokens to overlap between chunks.\n        :type chunk_overlap: int\n        :param input_glob: The glob pattern to use when searching for input data.\n        :type input_glob: str\n        :param citation_url: The URL to use when generating citations for the input data.\n        :type citation_url: str\n        :param citation_replacement_regex: The regex to use when generating citations for the input data.\n        :type citation_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk_and_embed = crack_and_chunk_and_embed_component(input_data=input_data, input_glob=input_glob, chunk_size=chunk_size, chunk_overlap=chunk_overlap, citation_url=citation_url, citation_replacement_regex=citation_replacement_regex, embeddings_container=embeddings_container, embeddings_model=embeddings_model, embeddings_connection_id=aoai_connection_id)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk_and_embed, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(embeddings_container):\n            crack_and_chunk_and_embed.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            crack_and_chunk_and_embed.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=crack_and_chunk_and_embed.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name, 'full_sync': True}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, input_glob=data_index.source.input_glob, chunk_size=data_index.source.chunk_size, chunk_overlap=data_index.source.chunk_overlap, citation_url=data_index.source.citation_url, citation_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, embeddings_model=build_model_protocol(data_index.embedding.model), aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None, acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection))\n    component.inputs['input_glob']._meta.optional = True\n    component.inputs['chunk_size']._meta.optional = True\n    component.inputs['chunk_overlap']._meta.optional = True\n    component.inputs['citation_url']._meta.optional = True\n    component.inputs['citation_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component"
        ]
    },
    {
        "func_name": "data_index_faiss_pipeline",
        "original": "@pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    \"\"\"\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Faiss index.\n        :rtype: str.\n        \"\"\"\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n    if identity:\n        create_faiss_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
        "mutated": [
            "@pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n    '\\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Faiss index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n    if identity:\n        create_faiss_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Faiss index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n    if identity:\n        create_faiss_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Faiss index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n    if identity:\n        create_faiss_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Faiss index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n    if identity:\n        create_faiss_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Faiss index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n    if identity:\n        create_faiss_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}"
        ]
    },
    {
        "func_name": "data_index_faiss",
        "original": "def data_index_faiss(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    create_faiss_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CREATE_FAISS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Faiss index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n        if identity:\n            create_faiss_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    component = data_index_faiss_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
        "mutated": [
            "def data_index_faiss(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    create_faiss_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CREATE_FAISS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Faiss index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n        if identity:\n            create_faiss_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    component = data_index_faiss_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_faiss(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    create_faiss_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CREATE_FAISS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Faiss index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n        if identity:\n            create_faiss_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    component = data_index_faiss_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_faiss(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    create_faiss_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CREATE_FAISS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Faiss index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n        if identity:\n            create_faiss_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    component = data_index_faiss_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_faiss(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    create_faiss_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CREATE_FAISS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Faiss index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n        if identity:\n            create_faiss_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    component = data_index_faiss_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_faiss(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    create_faiss_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CREATE_FAISS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_faiss', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to Faiss', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_faiss_pipeline(input_data: Input, embeddings_model: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and create a Faiss index from them.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Faiss index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        create_faiss_index = create_faiss_index_component(embeddings=generate_embeddings.outputs.embeddings)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(create_faiss_index, instance_type=serverless_instance_type)\n        if identity:\n            create_faiss_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=create_faiss_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': create_faiss_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    component = data_index_faiss_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component"
        ]
    },
    {
        "func_name": "data_index_acs_pipeline",
        "original": "@pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
        "mutated": [
            "@pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}",
            "@pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\ndef data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\\n\\n        :param input_data: The input data to be indexed.\\n        :type input_data: Input\\n        :param embeddings_model: The embedding model to use when processing source data chunks.\\n        :type embeddings_model: str\\n        :param acs_config: The configuration for the Azure Cognitive Search index.\\n        :type acs_config: str\\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\\n        :type acs_connection_id: str\\n        :param chunk_size: The size of the chunks to break the input data into.\\n        :type chunk_size: int\\n        :param data_source_glob: The glob pattern to use when searching for input data.\\n        :type data_source_glob: str\\n        :param data_source_url: The URL to use when generating citations for the input data.\\n        :type data_source_url: str\\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\\n        :type document_path_replacement_regex: str\\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\\n        :type aoai_connection_id: str\\n        :param embeddings_container: The container to use when caching embeddings.\\n        :type embeddings_container: Input\\n        :return: The URI of the generated Azure Cognitive Search index.\\n        :rtype: str.\\n        '\n    crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n    if identity:\n        crack_and_chunk.identity = identity\n    generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n    if optional_pipeline_input_provided(aoai_connection_id):\n        generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n    if optional_pipeline_input_provided(embeddings_container):\n        generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n    if identity:\n        generate_embeddings.identity = identity\n    update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n    update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n    if identity:\n        update_acs_index.identity = identity\n    register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n    if compute is None or compute == 'serverless':\n        use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n    if identity:\n        register_mlindex_asset.identity = identity\n    return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}"
        ]
    },
    {
        "func_name": "data_index_acs",
        "original": "def data_index_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
        "mutated": [
            "def data_index_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component",
            "def data_index_acs(ml_client: Any, data_index: DataIndex, description: Optional[str]=None, tags: Optional[Dict]=None, name: Optional[str]=None, display_name: Optional[str]=None, experiment_name: Optional[str]=None, compute: Optional[str]=None, serverless_instance_type: Optional[str]=None, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, input_data_override: Optional[Input]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from azure.ai.generative.index._dataindex.data_index.models import build_model_protocol\n    from azure.ai.generative.index._dataindex.dsl._pipeline_decorator import pipeline\n    crack_and_chunk_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_CRACK_AND_CHUNK)\n    generate_embeddings_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_GENERATE_EMBEDDINGS)\n    update_acs_index_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_UPDATE_ACS_INDEX)\n    register_mlindex_asset_component = get_component_obj(ml_client, LLMRAGComponentUri.LLM_RAG_REGISTER_MLINDEX_ASSET)\n\n    @pipeline(name=name if name else 'data_index_acs', description=description, tags=tags, display_name=display_name if display_name else 'LLM - Data to ACS', experiment_name=experiment_name, compute=compute, get_component=True)\n    def data_index_acs_pipeline(input_data: Input, embeddings_model: str, acs_config: str, acs_connection_id: str, chunk_size: int=1024, data_source_glob: str=None, data_source_url: str=None, document_path_replacement_regex: str=None, aoai_connection_id: str=None, embeddings_container: Input=None):\n        \"\"\"\n        Generate embeddings for a `input_data` source and push them into an Azure Cognitive Search index.\n\n        :param input_data: The input data to be indexed.\n        :type input_data: Input\n        :param embeddings_model: The embedding model to use when processing source data chunks.\n        :type embeddings_model: str\n        :param acs_config: The configuration for the Azure Cognitive Search index.\n        :type acs_config: str\n        :param acs_connection_id: The connection ID for the Azure Cognitive Search index.\n        :type acs_connection_id: str\n        :param chunk_size: The size of the chunks to break the input data into.\n        :type chunk_size: int\n        :param data_source_glob: The glob pattern to use when searching for input data.\n        :type data_source_glob: str\n        :param data_source_url: The URL to use when generating citations for the input data.\n        :type data_source_url: str\n        :param document_path_replacement_regex: The regex to use when generating citations for the input data.\n        :type document_path_replacement_regex: str\n        :param aoai_connection_id: The connection ID for the Azure Open AI service.\n        :type aoai_connection_id: str\n        :param embeddings_container: The container to use when caching embeddings.\n        :type embeddings_container: Input\n        :return: The URI of the generated Azure Cognitive Search index.\n        :rtype: str.\n        \"\"\"\n        crack_and_chunk = crack_and_chunk_component(input_data=input_data, input_glob=data_source_glob, chunk_size=chunk_size, data_source_url=data_source_url, document_path_replacement_regex=document_path_replacement_regex)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(crack_and_chunk, instance_type=serverless_instance_type)\n        if identity:\n            crack_and_chunk.identity = identity\n        generate_embeddings = generate_embeddings_component(chunks_source=crack_and_chunk.outputs.output_chunks, embeddings_container=embeddings_container, embeddings_model=embeddings_model)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(generate_embeddings, instance_type=serverless_instance_type)\n        if optional_pipeline_input_provided(aoai_connection_id):\n            generate_embeddings.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n        if optional_pipeline_input_provided(embeddings_container):\n            generate_embeddings.outputs.embeddings = Output(type='uri_folder', path=f'{embeddings_container.path}/{{name}}')\n        if identity:\n            generate_embeddings.identity = identity\n        update_acs_index = update_acs_index_component(embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(update_acs_index, instance_type=serverless_instance_type)\n        update_acs_index.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_ACS'] = acs_connection_id\n        if identity:\n            update_acs_index.identity = identity\n        register_mlindex_asset = register_mlindex_asset_component(storage_uri=update_acs_index.outputs.index, asset_name=data_index.name)\n        if compute is None or compute == 'serverless':\n            use_automatic_compute(register_mlindex_asset, instance_type=serverless_instance_type)\n        if identity:\n            register_mlindex_asset.identity = identity\n        return {'mlindex_asset_uri': update_acs_index.outputs.index, 'mlindex_asset_id': register_mlindex_asset.outputs.asset_id}\n    if input_data_override is not None:\n        input_data = input_data_override\n    else:\n        input_data = Input(type=data_index.source.input_data.type, path=data_index.source.input_data.path)\n    acs_config = {'index_name': data_index.index.name if data_index.index.name is not None else data_index.name}\n    if data_index.index.config is not None:\n        acs_config.update(data_index.index.config)\n    component = data_index_acs_pipeline(input_data=input_data, embeddings_model=build_model_protocol(data_index.embedding.model), acs_config=json.dumps(acs_config), acs_connection_id=_resolve_connection_id(ml_client, data_index.index.connection), chunk_size=data_index.source.chunk_size, data_source_glob=data_index.source.input_glob, data_source_url=data_index.source.citation_url, document_path_replacement_regex=json.dumps(data_index.source.citation_url_replacement_regex._to_dict()) if data_index.source.citation_url_replacement_regex else None, aoai_connection_id=_resolve_connection_id(ml_client, data_index.embedding.connection), embeddings_container=Input(type=AssetTypes.URI_FOLDER, path=data_index.embedding.cache_path) if data_index.embedding.cache_path else None)\n    component.inputs['data_source_glob']._meta.optional = True\n    component.inputs['data_source_url']._meta.optional = True\n    component.inputs['document_path_replacement_regex']._meta.optional = True\n    component.inputs['aoai_connection_id']._meta.optional = True\n    component.inputs['embeddings_container']._meta.optional = True\n    if data_index.path:\n        component.outputs.mlindex_asset_uri = Output(type=AssetTypes.URI_FOLDER, path=data_index.path)\n    return component"
        ]
    },
    {
        "func_name": "optional_pipeline_input_provided",
        "original": "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n    \"\"\"\n    Checks if optional pipeline inputs are provided.\n\n    :param input: The pipeline input to check.\n    :type input: Optional[PipelineInput]\n    :return: True if the input is not None and has a value, False otherwise.\n    :rtype: bool.\n    \"\"\"\n    return input is not None and input._data is not None",
        "mutated": [
            "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n    if False:\n        i = 10\n    '\\n    Checks if optional pipeline inputs are provided.\\n\\n    :param input: The pipeline input to check.\\n    :type input: Optional[PipelineInput]\\n    :return: True if the input is not None and has a value, False otherwise.\\n    :rtype: bool.\\n    '\n    return input is not None and input._data is not None",
            "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Checks if optional pipeline inputs are provided.\\n\\n    :param input: The pipeline input to check.\\n    :type input: Optional[PipelineInput]\\n    :return: True if the input is not None and has a value, False otherwise.\\n    :rtype: bool.\\n    '\n    return input is not None and input._data is not None",
            "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Checks if optional pipeline inputs are provided.\\n\\n    :param input: The pipeline input to check.\\n    :type input: Optional[PipelineInput]\\n    :return: True if the input is not None and has a value, False otherwise.\\n    :rtype: bool.\\n    '\n    return input is not None and input._data is not None",
            "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Checks if optional pipeline inputs are provided.\\n\\n    :param input: The pipeline input to check.\\n    :type input: Optional[PipelineInput]\\n    :return: True if the input is not None and has a value, False otherwise.\\n    :rtype: bool.\\n    '\n    return input is not None and input._data is not None",
            "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Checks if optional pipeline inputs are provided.\\n\\n    :param input: The pipeline input to check.\\n    :type input: Optional[PipelineInput]\\n    :return: True if the input is not None and has a value, False otherwise.\\n    :rtype: bool.\\n    '\n    return input is not None and input._data is not None"
        ]
    },
    {
        "func_name": "use_automatic_compute",
        "original": "def use_automatic_compute(component, instance_count=1, instance_type=None):\n    \"\"\"\n    Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\n\n    This avoids the need to provision a compute cluster to run the component.\n    :param component: The component to configure.\n    :type component: Any\n    :param instance_count: The number of instances to use.\n    :type instance_count: int\n    :param instance_type: The type of instance to use.\n    :type instance_type: str\n    :return: The configured component.\n    :rtype: Any.\n    \"\"\"\n    component.set_resources(instance_count=instance_count, instance_type=instance_type, properties={'compute_specification': {'automatic': True}})\n    return component",
        "mutated": [
            "def use_automatic_compute(component, instance_count=1, instance_type=None):\n    if False:\n        i = 10\n    '\\n    Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\\n\\n    This avoids the need to provision a compute cluster to run the component.\\n    :param component: The component to configure.\\n    :type component: Any\\n    :param instance_count: The number of instances to use.\\n    :type instance_count: int\\n    :param instance_type: The type of instance to use.\\n    :type instance_type: str\\n    :return: The configured component.\\n    :rtype: Any.\\n    '\n    component.set_resources(instance_count=instance_count, instance_type=instance_type, properties={'compute_specification': {'automatic': True}})\n    return component",
            "def use_automatic_compute(component, instance_count=1, instance_type=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\\n\\n    This avoids the need to provision a compute cluster to run the component.\\n    :param component: The component to configure.\\n    :type component: Any\\n    :param instance_count: The number of instances to use.\\n    :type instance_count: int\\n    :param instance_type: The type of instance to use.\\n    :type instance_type: str\\n    :return: The configured component.\\n    :rtype: Any.\\n    '\n    component.set_resources(instance_count=instance_count, instance_type=instance_type, properties={'compute_specification': {'automatic': True}})\n    return component",
            "def use_automatic_compute(component, instance_count=1, instance_type=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\\n\\n    This avoids the need to provision a compute cluster to run the component.\\n    :param component: The component to configure.\\n    :type component: Any\\n    :param instance_count: The number of instances to use.\\n    :type instance_count: int\\n    :param instance_type: The type of instance to use.\\n    :type instance_type: str\\n    :return: The configured component.\\n    :rtype: Any.\\n    '\n    component.set_resources(instance_count=instance_count, instance_type=instance_type, properties={'compute_specification': {'automatic': True}})\n    return component",
            "def use_automatic_compute(component, instance_count=1, instance_type=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\\n\\n    This avoids the need to provision a compute cluster to run the component.\\n    :param component: The component to configure.\\n    :type component: Any\\n    :param instance_count: The number of instances to use.\\n    :type instance_count: int\\n    :param instance_type: The type of instance to use.\\n    :type instance_type: str\\n    :return: The configured component.\\n    :rtype: Any.\\n    '\n    component.set_resources(instance_count=instance_count, instance_type=instance_type, properties={'compute_specification': {'automatic': True}})\n    return component",
            "def use_automatic_compute(component, instance_count=1, instance_type=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\\n\\n    This avoids the need to provision a compute cluster to run the component.\\n    :param component: The component to configure.\\n    :type component: Any\\n    :param instance_count: The number of instances to use.\\n    :type instance_count: int\\n    :param instance_type: The type of instance to use.\\n    :type instance_type: str\\n    :return: The configured component.\\n    :rtype: Any.\\n    '\n    component.set_resources(instance_count=instance_count, instance_type=instance_type, properties={'compute_specification': {'automatic': True}})\n    return component"
        ]
    },
    {
        "func_name": "get_component_obj",
        "original": "def get_component_obj(ml_client, component_uri):\n    from azure.ai.ml import MLClient\n    if not isinstance(component_uri, str):\n        return component_uri\n    matches = re.match('azureml://registries/(?P<registry_name>.*)/components/(?P<component_name>.*)/(?P<identifier_type>.*)/(?P<identifier_name>.*)', component_uri)\n    if matches is None:\n        from azure.ai.ml import load_component\n        return load_component(source=component_uri)\n    registry_name = matches.group('registry_name')\n    registry_client = MLClient(subscription_id=ml_client.subscription_id, resource_group_name=ml_client.resource_group_name, credential=ml_client._credential, registry_name=registry_name)\n    component_obj = registry_client.components.get(matches.group('component_name'), **{matches.group('identifier_type').rstrip('s'): matches.group('identifier_name')})\n    return component_obj",
        "mutated": [
            "def get_component_obj(ml_client, component_uri):\n    if False:\n        i = 10\n    from azure.ai.ml import MLClient\n    if not isinstance(component_uri, str):\n        return component_uri\n    matches = re.match('azureml://registries/(?P<registry_name>.*)/components/(?P<component_name>.*)/(?P<identifier_type>.*)/(?P<identifier_name>.*)', component_uri)\n    if matches is None:\n        from azure.ai.ml import load_component\n        return load_component(source=component_uri)\n    registry_name = matches.group('registry_name')\n    registry_client = MLClient(subscription_id=ml_client.subscription_id, resource_group_name=ml_client.resource_group_name, credential=ml_client._credential, registry_name=registry_name)\n    component_obj = registry_client.components.get(matches.group('component_name'), **{matches.group('identifier_type').rstrip('s'): matches.group('identifier_name')})\n    return component_obj",
            "def get_component_obj(ml_client, component_uri):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from azure.ai.ml import MLClient\n    if not isinstance(component_uri, str):\n        return component_uri\n    matches = re.match('azureml://registries/(?P<registry_name>.*)/components/(?P<component_name>.*)/(?P<identifier_type>.*)/(?P<identifier_name>.*)', component_uri)\n    if matches is None:\n        from azure.ai.ml import load_component\n        return load_component(source=component_uri)\n    registry_name = matches.group('registry_name')\n    registry_client = MLClient(subscription_id=ml_client.subscription_id, resource_group_name=ml_client.resource_group_name, credential=ml_client._credential, registry_name=registry_name)\n    component_obj = registry_client.components.get(matches.group('component_name'), **{matches.group('identifier_type').rstrip('s'): matches.group('identifier_name')})\n    return component_obj",
            "def get_component_obj(ml_client, component_uri):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from azure.ai.ml import MLClient\n    if not isinstance(component_uri, str):\n        return component_uri\n    matches = re.match('azureml://registries/(?P<registry_name>.*)/components/(?P<component_name>.*)/(?P<identifier_type>.*)/(?P<identifier_name>.*)', component_uri)\n    if matches is None:\n        from azure.ai.ml import load_component\n        return load_component(source=component_uri)\n    registry_name = matches.group('registry_name')\n    registry_client = MLClient(subscription_id=ml_client.subscription_id, resource_group_name=ml_client.resource_group_name, credential=ml_client._credential, registry_name=registry_name)\n    component_obj = registry_client.components.get(matches.group('component_name'), **{matches.group('identifier_type').rstrip('s'): matches.group('identifier_name')})\n    return component_obj",
            "def get_component_obj(ml_client, component_uri):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from azure.ai.ml import MLClient\n    if not isinstance(component_uri, str):\n        return component_uri\n    matches = re.match('azureml://registries/(?P<registry_name>.*)/components/(?P<component_name>.*)/(?P<identifier_type>.*)/(?P<identifier_name>.*)', component_uri)\n    if matches is None:\n        from azure.ai.ml import load_component\n        return load_component(source=component_uri)\n    registry_name = matches.group('registry_name')\n    registry_client = MLClient(subscription_id=ml_client.subscription_id, resource_group_name=ml_client.resource_group_name, credential=ml_client._credential, registry_name=registry_name)\n    component_obj = registry_client.components.get(matches.group('component_name'), **{matches.group('identifier_type').rstrip('s'): matches.group('identifier_name')})\n    return component_obj",
            "def get_component_obj(ml_client, component_uri):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from azure.ai.ml import MLClient\n    if not isinstance(component_uri, str):\n        return component_uri\n    matches = re.match('azureml://registries/(?P<registry_name>.*)/components/(?P<component_name>.*)/(?P<identifier_type>.*)/(?P<identifier_name>.*)', component_uri)\n    if matches is None:\n        from azure.ai.ml import load_component\n        return load_component(source=component_uri)\n    registry_name = matches.group('registry_name')\n    registry_client = MLClient(subscription_id=ml_client.subscription_id, resource_group_name=ml_client.resource_group_name, credential=ml_client._credential, registry_name=registry_name)\n    component_obj = registry_client.components.get(matches.group('component_name'), **{matches.group('identifier_type').rstrip('s'): matches.group('identifier_name')})\n    return component_obj"
        ]
    },
    {
        "func_name": "_resolve_connection_id",
        "original": "def _resolve_connection_id(ml_client, connection: Optional[Union[str, WorkspaceConnection]]=None) -> Optional[str]:\n    if connection is None:\n        return None\n    if isinstance(connection, str):\n        short_form = re.match('azureml:(?P<connection_name>[^/]*)', connection)\n        if short_form:\n            connection_name = short_form.group('connection_name')\n        else:\n            long_form = re.match('(azureml:/)?/.*/connections/(?P<connection_name>[^/]*)', connection)\n            connection_name = long_form.group('connection_name') if long_form else connection\n        connection = ml_client.connections.get(connection_name)\n    elif hasattr(connection, '_workspace_connection'):\n        connection = connection._workspace_connection\n    return connection.id",
        "mutated": [
            "def _resolve_connection_id(ml_client, connection: Optional[Union[str, WorkspaceConnection]]=None) -> Optional[str]:\n    if False:\n        i = 10\n    if connection is None:\n        return None\n    if isinstance(connection, str):\n        short_form = re.match('azureml:(?P<connection_name>[^/]*)', connection)\n        if short_form:\n            connection_name = short_form.group('connection_name')\n        else:\n            long_form = re.match('(azureml:/)?/.*/connections/(?P<connection_name>[^/]*)', connection)\n            connection_name = long_form.group('connection_name') if long_form else connection\n        connection = ml_client.connections.get(connection_name)\n    elif hasattr(connection, '_workspace_connection'):\n        connection = connection._workspace_connection\n    return connection.id",
            "def _resolve_connection_id(ml_client, connection: Optional[Union[str, WorkspaceConnection]]=None) -> Optional[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if connection is None:\n        return None\n    if isinstance(connection, str):\n        short_form = re.match('azureml:(?P<connection_name>[^/]*)', connection)\n        if short_form:\n            connection_name = short_form.group('connection_name')\n        else:\n            long_form = re.match('(azureml:/)?/.*/connections/(?P<connection_name>[^/]*)', connection)\n            connection_name = long_form.group('connection_name') if long_form else connection\n        connection = ml_client.connections.get(connection_name)\n    elif hasattr(connection, '_workspace_connection'):\n        connection = connection._workspace_connection\n    return connection.id",
            "def _resolve_connection_id(ml_client, connection: Optional[Union[str, WorkspaceConnection]]=None) -> Optional[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if connection is None:\n        return None\n    if isinstance(connection, str):\n        short_form = re.match('azureml:(?P<connection_name>[^/]*)', connection)\n        if short_form:\n            connection_name = short_form.group('connection_name')\n        else:\n            long_form = re.match('(azureml:/)?/.*/connections/(?P<connection_name>[^/]*)', connection)\n            connection_name = long_form.group('connection_name') if long_form else connection\n        connection = ml_client.connections.get(connection_name)\n    elif hasattr(connection, '_workspace_connection'):\n        connection = connection._workspace_connection\n    return connection.id",
            "def _resolve_connection_id(ml_client, connection: Optional[Union[str, WorkspaceConnection]]=None) -> Optional[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if connection is None:\n        return None\n    if isinstance(connection, str):\n        short_form = re.match('azureml:(?P<connection_name>[^/]*)', connection)\n        if short_form:\n            connection_name = short_form.group('connection_name')\n        else:\n            long_form = re.match('(azureml:/)?/.*/connections/(?P<connection_name>[^/]*)', connection)\n            connection_name = long_form.group('connection_name') if long_form else connection\n        connection = ml_client.connections.get(connection_name)\n    elif hasattr(connection, '_workspace_connection'):\n        connection = connection._workspace_connection\n    return connection.id",
            "def _resolve_connection_id(ml_client, connection: Optional[Union[str, WorkspaceConnection]]=None) -> Optional[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if connection is None:\n        return None\n    if isinstance(connection, str):\n        short_form = re.match('azureml:(?P<connection_name>[^/]*)', connection)\n        if short_form:\n            connection_name = short_form.group('connection_name')\n        else:\n            long_form = re.match('(azureml:/)?/.*/connections/(?P<connection_name>[^/]*)', connection)\n            connection_name = long_form.group('connection_name') if long_form else connection\n        connection = ml_client.connections.get(connection_name)\n    elif hasattr(connection, '_workspace_connection'):\n        connection = connection._workspace_connection\n    return connection.id"
        ]
    }
]