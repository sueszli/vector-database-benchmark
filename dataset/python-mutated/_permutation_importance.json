[
    {
        "func_name": "_weights_scorer",
        "original": "def _weights_scorer(scorer, estimator, X, y, sample_weight):\n    if sample_weight is not None:\n        return scorer(estimator, X, y, sample_weight=sample_weight)\n    return scorer(estimator, X, y)",
        "mutated": [
            "def _weights_scorer(scorer, estimator, X, y, sample_weight):\n    if False:\n        i = 10\n    if sample_weight is not None:\n        return scorer(estimator, X, y, sample_weight=sample_weight)\n    return scorer(estimator, X, y)",
            "def _weights_scorer(scorer, estimator, X, y, sample_weight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if sample_weight is not None:\n        return scorer(estimator, X, y, sample_weight=sample_weight)\n    return scorer(estimator, X, y)",
            "def _weights_scorer(scorer, estimator, X, y, sample_weight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if sample_weight is not None:\n        return scorer(estimator, X, y, sample_weight=sample_weight)\n    return scorer(estimator, X, y)",
            "def _weights_scorer(scorer, estimator, X, y, sample_weight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if sample_weight is not None:\n        return scorer(estimator, X, y, sample_weight=sample_weight)\n    return scorer(estimator, X, y)",
            "def _weights_scorer(scorer, estimator, X, y, sample_weight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if sample_weight is not None:\n        return scorer(estimator, X, y, sample_weight=sample_weight)\n    return scorer(estimator, X, y)"
        ]
    },
    {
        "func_name": "_calculate_permutation_scores",
        "original": "def _calculate_permutation_scores(estimator, X, y, sample_weight, col_idx, random_state, n_repeats, scorer, max_samples):\n    \"\"\"Calculate score when `col_idx` is permuted.\"\"\"\n    random_state = check_random_state(random_state)\n    if max_samples < X.shape[0]:\n        row_indices = _generate_indices(random_state=random_state, bootstrap=False, n_population=X.shape[0], n_samples=max_samples)\n        X_permuted = _safe_indexing(X, row_indices, axis=0)\n        y = _safe_indexing(y, row_indices, axis=0)\n    else:\n        X_permuted = X.copy()\n    scores = []\n    shuffling_idx = np.arange(X_permuted.shape[0])\n    for _ in range(n_repeats):\n        random_state.shuffle(shuffling_idx)\n        if hasattr(X_permuted, 'iloc'):\n            col = X_permuted.iloc[shuffling_idx, col_idx]\n            col.index = X_permuted.index\n            X_permuted[X_permuted.columns[col_idx]] = col\n        else:\n            X_permuted[:, col_idx] = X_permuted[shuffling_idx, col_idx]\n        scores.append(_weights_scorer(scorer, estimator, X_permuted, y, sample_weight))\n    if isinstance(scores[0], dict):\n        scores = _aggregate_score_dicts(scores)\n    else:\n        scores = np.array(scores)\n    return scores",
        "mutated": [
            "def _calculate_permutation_scores(estimator, X, y, sample_weight, col_idx, random_state, n_repeats, scorer, max_samples):\n    if False:\n        i = 10\n    'Calculate score when `col_idx` is permuted.'\n    random_state = check_random_state(random_state)\n    if max_samples < X.shape[0]:\n        row_indices = _generate_indices(random_state=random_state, bootstrap=False, n_population=X.shape[0], n_samples=max_samples)\n        X_permuted = _safe_indexing(X, row_indices, axis=0)\n        y = _safe_indexing(y, row_indices, axis=0)\n    else:\n        X_permuted = X.copy()\n    scores = []\n    shuffling_idx = np.arange(X_permuted.shape[0])\n    for _ in range(n_repeats):\n        random_state.shuffle(shuffling_idx)\n        if hasattr(X_permuted, 'iloc'):\n            col = X_permuted.iloc[shuffling_idx, col_idx]\n            col.index = X_permuted.index\n            X_permuted[X_permuted.columns[col_idx]] = col\n        else:\n            X_permuted[:, col_idx] = X_permuted[shuffling_idx, col_idx]\n        scores.append(_weights_scorer(scorer, estimator, X_permuted, y, sample_weight))\n    if isinstance(scores[0], dict):\n        scores = _aggregate_score_dicts(scores)\n    else:\n        scores = np.array(scores)\n    return scores",
            "def _calculate_permutation_scores(estimator, X, y, sample_weight, col_idx, random_state, n_repeats, scorer, max_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate score when `col_idx` is permuted.'\n    random_state = check_random_state(random_state)\n    if max_samples < X.shape[0]:\n        row_indices = _generate_indices(random_state=random_state, bootstrap=False, n_population=X.shape[0], n_samples=max_samples)\n        X_permuted = _safe_indexing(X, row_indices, axis=0)\n        y = _safe_indexing(y, row_indices, axis=0)\n    else:\n        X_permuted = X.copy()\n    scores = []\n    shuffling_idx = np.arange(X_permuted.shape[0])\n    for _ in range(n_repeats):\n        random_state.shuffle(shuffling_idx)\n        if hasattr(X_permuted, 'iloc'):\n            col = X_permuted.iloc[shuffling_idx, col_idx]\n            col.index = X_permuted.index\n            X_permuted[X_permuted.columns[col_idx]] = col\n        else:\n            X_permuted[:, col_idx] = X_permuted[shuffling_idx, col_idx]\n        scores.append(_weights_scorer(scorer, estimator, X_permuted, y, sample_weight))\n    if isinstance(scores[0], dict):\n        scores = _aggregate_score_dicts(scores)\n    else:\n        scores = np.array(scores)\n    return scores",
            "def _calculate_permutation_scores(estimator, X, y, sample_weight, col_idx, random_state, n_repeats, scorer, max_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate score when `col_idx` is permuted.'\n    random_state = check_random_state(random_state)\n    if max_samples < X.shape[0]:\n        row_indices = _generate_indices(random_state=random_state, bootstrap=False, n_population=X.shape[0], n_samples=max_samples)\n        X_permuted = _safe_indexing(X, row_indices, axis=0)\n        y = _safe_indexing(y, row_indices, axis=0)\n    else:\n        X_permuted = X.copy()\n    scores = []\n    shuffling_idx = np.arange(X_permuted.shape[0])\n    for _ in range(n_repeats):\n        random_state.shuffle(shuffling_idx)\n        if hasattr(X_permuted, 'iloc'):\n            col = X_permuted.iloc[shuffling_idx, col_idx]\n            col.index = X_permuted.index\n            X_permuted[X_permuted.columns[col_idx]] = col\n        else:\n            X_permuted[:, col_idx] = X_permuted[shuffling_idx, col_idx]\n        scores.append(_weights_scorer(scorer, estimator, X_permuted, y, sample_weight))\n    if isinstance(scores[0], dict):\n        scores = _aggregate_score_dicts(scores)\n    else:\n        scores = np.array(scores)\n    return scores",
            "def _calculate_permutation_scores(estimator, X, y, sample_weight, col_idx, random_state, n_repeats, scorer, max_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate score when `col_idx` is permuted.'\n    random_state = check_random_state(random_state)\n    if max_samples < X.shape[0]:\n        row_indices = _generate_indices(random_state=random_state, bootstrap=False, n_population=X.shape[0], n_samples=max_samples)\n        X_permuted = _safe_indexing(X, row_indices, axis=0)\n        y = _safe_indexing(y, row_indices, axis=0)\n    else:\n        X_permuted = X.copy()\n    scores = []\n    shuffling_idx = np.arange(X_permuted.shape[0])\n    for _ in range(n_repeats):\n        random_state.shuffle(shuffling_idx)\n        if hasattr(X_permuted, 'iloc'):\n            col = X_permuted.iloc[shuffling_idx, col_idx]\n            col.index = X_permuted.index\n            X_permuted[X_permuted.columns[col_idx]] = col\n        else:\n            X_permuted[:, col_idx] = X_permuted[shuffling_idx, col_idx]\n        scores.append(_weights_scorer(scorer, estimator, X_permuted, y, sample_weight))\n    if isinstance(scores[0], dict):\n        scores = _aggregate_score_dicts(scores)\n    else:\n        scores = np.array(scores)\n    return scores",
            "def _calculate_permutation_scores(estimator, X, y, sample_weight, col_idx, random_state, n_repeats, scorer, max_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate score when `col_idx` is permuted.'\n    random_state = check_random_state(random_state)\n    if max_samples < X.shape[0]:\n        row_indices = _generate_indices(random_state=random_state, bootstrap=False, n_population=X.shape[0], n_samples=max_samples)\n        X_permuted = _safe_indexing(X, row_indices, axis=0)\n        y = _safe_indexing(y, row_indices, axis=0)\n    else:\n        X_permuted = X.copy()\n    scores = []\n    shuffling_idx = np.arange(X_permuted.shape[0])\n    for _ in range(n_repeats):\n        random_state.shuffle(shuffling_idx)\n        if hasattr(X_permuted, 'iloc'):\n            col = X_permuted.iloc[shuffling_idx, col_idx]\n            col.index = X_permuted.index\n            X_permuted[X_permuted.columns[col_idx]] = col\n        else:\n            X_permuted[:, col_idx] = X_permuted[shuffling_idx, col_idx]\n        scores.append(_weights_scorer(scorer, estimator, X_permuted, y, sample_weight))\n    if isinstance(scores[0], dict):\n        scores = _aggregate_score_dicts(scores)\n    else:\n        scores = np.array(scores)\n    return scores"
        ]
    },
    {
        "func_name": "_create_importances_bunch",
        "original": "def _create_importances_bunch(baseline_score, permuted_score):\n    \"\"\"Compute the importances as the decrease in score.\n\n    Parameters\n    ----------\n    baseline_score : ndarray of shape (n_features,)\n        The baseline score without permutation.\n    permuted_score : ndarray of shape (n_features, n_repeats)\n        The permuted scores for the `n` repetitions.\n\n    Returns\n    -------\n    importances : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n        importances_mean : ndarray, shape (n_features, )\n            Mean of feature importance over `n_repeats`.\n        importances_std : ndarray, shape (n_features, )\n            Standard deviation over `n_repeats`.\n        importances : ndarray, shape (n_features, n_repeats)\n            Raw permutation importance scores.\n    \"\"\"\n    importances = baseline_score - permuted_score\n    return Bunch(importances_mean=np.mean(importances, axis=1), importances_std=np.std(importances, axis=1), importances=importances)",
        "mutated": [
            "def _create_importances_bunch(baseline_score, permuted_score):\n    if False:\n        i = 10\n    'Compute the importances as the decrease in score.\\n\\n    Parameters\\n    ----------\\n    baseline_score : ndarray of shape (n_features,)\\n        The baseline score without permutation.\\n    permuted_score : ndarray of shape (n_features, n_repeats)\\n        The permuted scores for the `n` repetitions.\\n\\n    Returns\\n    -------\\n    importances : :class:`~sklearn.utils.Bunch`\\n        Dictionary-like object, with the following attributes.\\n        importances_mean : ndarray, shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray, shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray, shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n    '\n    importances = baseline_score - permuted_score\n    return Bunch(importances_mean=np.mean(importances, axis=1), importances_std=np.std(importances, axis=1), importances=importances)",
            "def _create_importances_bunch(baseline_score, permuted_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the importances as the decrease in score.\\n\\n    Parameters\\n    ----------\\n    baseline_score : ndarray of shape (n_features,)\\n        The baseline score without permutation.\\n    permuted_score : ndarray of shape (n_features, n_repeats)\\n        The permuted scores for the `n` repetitions.\\n\\n    Returns\\n    -------\\n    importances : :class:`~sklearn.utils.Bunch`\\n        Dictionary-like object, with the following attributes.\\n        importances_mean : ndarray, shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray, shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray, shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n    '\n    importances = baseline_score - permuted_score\n    return Bunch(importances_mean=np.mean(importances, axis=1), importances_std=np.std(importances, axis=1), importances=importances)",
            "def _create_importances_bunch(baseline_score, permuted_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the importances as the decrease in score.\\n\\n    Parameters\\n    ----------\\n    baseline_score : ndarray of shape (n_features,)\\n        The baseline score without permutation.\\n    permuted_score : ndarray of shape (n_features, n_repeats)\\n        The permuted scores for the `n` repetitions.\\n\\n    Returns\\n    -------\\n    importances : :class:`~sklearn.utils.Bunch`\\n        Dictionary-like object, with the following attributes.\\n        importances_mean : ndarray, shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray, shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray, shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n    '\n    importances = baseline_score - permuted_score\n    return Bunch(importances_mean=np.mean(importances, axis=1), importances_std=np.std(importances, axis=1), importances=importances)",
            "def _create_importances_bunch(baseline_score, permuted_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the importances as the decrease in score.\\n\\n    Parameters\\n    ----------\\n    baseline_score : ndarray of shape (n_features,)\\n        The baseline score without permutation.\\n    permuted_score : ndarray of shape (n_features, n_repeats)\\n        The permuted scores for the `n` repetitions.\\n\\n    Returns\\n    -------\\n    importances : :class:`~sklearn.utils.Bunch`\\n        Dictionary-like object, with the following attributes.\\n        importances_mean : ndarray, shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray, shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray, shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n    '\n    importances = baseline_score - permuted_score\n    return Bunch(importances_mean=np.mean(importances, axis=1), importances_std=np.std(importances, axis=1), importances=importances)",
            "def _create_importances_bunch(baseline_score, permuted_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the importances as the decrease in score.\\n\\n    Parameters\\n    ----------\\n    baseline_score : ndarray of shape (n_features,)\\n        The baseline score without permutation.\\n    permuted_score : ndarray of shape (n_features, n_repeats)\\n        The permuted scores for the `n` repetitions.\\n\\n    Returns\\n    -------\\n    importances : :class:`~sklearn.utils.Bunch`\\n        Dictionary-like object, with the following attributes.\\n        importances_mean : ndarray, shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray, shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray, shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n    '\n    importances = baseline_score - permuted_score\n    return Bunch(importances_mean=np.mean(importances, axis=1), importances_std=np.std(importances, axis=1), importances=importances)"
        ]
    },
    {
        "func_name": "permutation_importance",
        "original": "@validate_params({'estimator': [HasMethods(['fit'])], 'X': ['array-like'], 'y': ['array-like', None], 'scoring': [StrOptions(set(get_scorer_names())), callable, list, tuple, dict, None], 'n_repeats': [Interval(Integral, 1, None, closed='left')], 'n_jobs': [Integral, None], 'random_state': ['random_state'], 'sample_weight': ['array-like', None], 'max_samples': [Interval(Integral, 1, None, closed='left'), Interval(RealNotInt, 0, 1, closed='right')]}, prefer_skip_nested_validation=True)\ndef permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0):\n    \"\"\"Permutation importance for feature evaluation [BRE]_.\n\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\n    data set used to train the estimator or a hold-out set. The permutation\n    importance of a feature is calculated as follows. First, a baseline metric,\n    defined by :term:`scoring`, is evaluated on a (potentially different)\n    dataset defined by the `X`. Next, a feature column from the validation set\n    is permuted and the metric is evaluated again. The permutation importance\n    is defined to be the difference between the baseline metric and metric from\n    permutating the feature column.\n\n    Read more in the :ref:`User Guide <permutation_importance>`.\n\n    Parameters\n    ----------\n    estimator : object\n        An estimator that has already been :term:`fitted` and is compatible\n        with :term:`scorer`.\n\n    X : ndarray or DataFrame, shape (n_samples, n_features)\n        Data on which permutation importance will be computed.\n\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n        Targets for supervised or `None` for unsupervised.\n\n    scoring : str, callable, list, tuple, or dict, default=None\n        Scorer to use.\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        Passing multiple scores to `scoring` is more efficient than calling\n        `permutation_importance` for each of the scores as it reuses\n        predictions to avoid redundant computation.\n\n        If None, the estimator's default scorer is used.\n\n    n_repeats : int, default=5\n        Number of times to permute a feature.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel. The computation is done by computing\n        permutation score for each columns and parallelized over the columns.\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance, default=None\n        Pseudo-random number generator to control the permutations of each\n        feature.\n        Pass an int to get reproducible results across function calls.\n        See :term:`Glossary <random_state>`.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights used in scoring.\n\n        .. versionadded:: 0.24\n\n    max_samples : int or float, default=1.0\n        The number of samples to draw from X to compute feature importance\n        in each repeat (without replacement).\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n        - If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples\n          will be used.\n\n        While using this option may provide less accurate importance estimates,\n        it keeps the method tractable when evaluating feature importance on\n        large datasets. In combination with `n_repeats`, this allows to control\n        the computational speed vs statistical accuracy trade-off of this method.\n\n        .. versionadded:: 1.0\n\n    Returns\n    -------\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\n        Dictionary-like object, with the following attributes.\n\n        importances_mean : ndarray of shape (n_features, )\n            Mean of feature importance over `n_repeats`.\n        importances_std : ndarray of shape (n_features, )\n            Standard deviation over `n_repeats`.\n        importances : ndarray of shape (n_features, n_repeats)\n            Raw permutation importance scores.\n\n        If there are multiple scoring metrics in the scoring parameter\n        `result` is a dict with scorer names as keys (e.g. 'roc_auc') and\n        `Bunch` objects like above as values.\n\n    References\n    ----------\n    .. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n             2001. <10.1023/A:1010933404324>`\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.inspection import permutation_importance\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\n    >>> y = [1, 1, 1, 0, 0, 0]\n    >>> clf = LogisticRegression().fit(X, y)\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\n    ...                                 random_state=0)\n    >>> result.importances_mean\n    array([0.4666..., 0.       , 0.       ])\n    >>> result.importances_std\n    array([0.2211..., 0.       , 0.       ])\n    \"\"\"\n    if not hasattr(X, 'iloc'):\n        X = check_array(X, force_all_finite='allow-nan', dtype=None)\n    random_state = check_random_state(random_state)\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n    if not isinstance(max_samples, numbers.Integral):\n        max_samples = int(max_samples * X.shape[0])\n    elif max_samples > X.shape[0]:\n        raise ValueError('max_samples must be <= n_samples')\n    if callable(scoring):\n        scorer = scoring\n    elif scoring is None or isinstance(scoring, str):\n        scorer = check_scoring(estimator, scoring=scoring)\n    else:\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\n        scorer = _MultimetricScorer(scorers=scorers_dict)\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n    scores = Parallel(n_jobs=n_jobs)((delayed(_calculate_permutation_scores)(estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer, max_samples) for col_idx in range(X.shape[1])))\n    if isinstance(baseline_score, dict):\n        return {name: _create_importances_bunch(baseline_score[name], np.array([scores[col_idx][name] for col_idx in range(X.shape[1])])) for name in baseline_score}\n    else:\n        return _create_importances_bunch(baseline_score, np.array(scores))",
        "mutated": [
            "@validate_params({'estimator': [HasMethods(['fit'])], 'X': ['array-like'], 'y': ['array-like', None], 'scoring': [StrOptions(set(get_scorer_names())), callable, list, tuple, dict, None], 'n_repeats': [Interval(Integral, 1, None, closed='left')], 'n_jobs': [Integral, None], 'random_state': ['random_state'], 'sample_weight': ['array-like', None], 'max_samples': [Interval(Integral, 1, None, closed='left'), Interval(RealNotInt, 0, 1, closed='right')]}, prefer_skip_nested_validation=True)\ndef permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0):\n    if False:\n        i = 10\n    'Permutation importance for feature evaluation [BRE]_.\\n\\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\\n    data set used to train the estimator or a hold-out set. The permutation\\n    importance of a feature is calculated as follows. First, a baseline metric,\\n    defined by :term:`scoring`, is evaluated on a (potentially different)\\n    dataset defined by the `X`. Next, a feature column from the validation set\\n    is permuted and the metric is evaluated again. The permutation importance\\n    is defined to be the difference between the baseline metric and metric from\\n    permutating the feature column.\\n\\n    Read more in the :ref:`User Guide <permutation_importance>`.\\n\\n    Parameters\\n    ----------\\n    estimator : object\\n        An estimator that has already been :term:`fitted` and is compatible\\n        with :term:`scorer`.\\n\\n    X : ndarray or DataFrame, shape (n_samples, n_features)\\n        Data on which permutation importance will be computed.\\n\\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\\n        Targets for supervised or `None` for unsupervised.\\n\\n    scoring : str, callable, list, tuple, or dict, default=None\\n        Scorer to use.\\n        If `scoring` represents a single score, one can use:\\n\\n        - a single string (see :ref:`scoring_parameter`);\\n        - a callable (see :ref:`scoring`) that returns a single value.\\n\\n        If `scoring` represents multiple scores, one can use:\\n\\n        - a list or tuple of unique strings;\\n        - a callable returning a dictionary where the keys are the metric\\n          names and the values are the metric scores;\\n        - a dictionary with metric names as keys and callables a values.\\n\\n        Passing multiple scores to `scoring` is more efficient than calling\\n        `permutation_importance` for each of the scores as it reuses\\n        predictions to avoid redundant computation.\\n\\n        If None, the estimator\\'s default scorer is used.\\n\\n    n_repeats : int, default=5\\n        Number of times to permute a feature.\\n\\n    n_jobs : int or None, default=None\\n        Number of jobs to run in parallel. The computation is done by computing\\n        permutation score for each columns and parallelized over the columns.\\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    random_state : int, RandomState instance, default=None\\n        Pseudo-random number generator to control the permutations of each\\n        feature.\\n        Pass an int to get reproducible results across function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    sample_weight : array-like of shape (n_samples,), default=None\\n        Sample weights used in scoring.\\n\\n        .. versionadded:: 0.24\\n\\n    max_samples : int or float, default=1.0\\n        The number of samples to draw from X to compute feature importance\\n        in each repeat (without replacement).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n        - If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples\\n          will be used.\\n\\n        While using this option may provide less accurate importance estimates,\\n        it keeps the method tractable when evaluating feature importance on\\n        large datasets. In combination with `n_repeats`, this allows to control\\n        the computational speed vs statistical accuracy trade-off of this method.\\n\\n        .. versionadded:: 1.0\\n\\n    Returns\\n    -------\\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\\n        Dictionary-like object, with the following attributes.\\n\\n        importances_mean : ndarray of shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray of shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray of shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n\\n        If there are multiple scoring metrics in the scoring parameter\\n        `result` is a dict with scorer names as keys (e.g. \\'roc_auc\\') and\\n        `Bunch` objects like above as values.\\n\\n    References\\n    ----------\\n    .. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\\n             2001. <10.1023/A:1010933404324>`\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from sklearn.inspection import permutation_importance\\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\\n    >>> y = [1, 1, 1, 0, 0, 0]\\n    >>> clf = LogisticRegression().fit(X, y)\\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\\n    ...                                 random_state=0)\\n    >>> result.importances_mean\\n    array([0.4666..., 0.       , 0.       ])\\n    >>> result.importances_std\\n    array([0.2211..., 0.       , 0.       ])\\n    '\n    if not hasattr(X, 'iloc'):\n        X = check_array(X, force_all_finite='allow-nan', dtype=None)\n    random_state = check_random_state(random_state)\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n    if not isinstance(max_samples, numbers.Integral):\n        max_samples = int(max_samples * X.shape[0])\n    elif max_samples > X.shape[0]:\n        raise ValueError('max_samples must be <= n_samples')\n    if callable(scoring):\n        scorer = scoring\n    elif scoring is None or isinstance(scoring, str):\n        scorer = check_scoring(estimator, scoring=scoring)\n    else:\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\n        scorer = _MultimetricScorer(scorers=scorers_dict)\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n    scores = Parallel(n_jobs=n_jobs)((delayed(_calculate_permutation_scores)(estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer, max_samples) for col_idx in range(X.shape[1])))\n    if isinstance(baseline_score, dict):\n        return {name: _create_importances_bunch(baseline_score[name], np.array([scores[col_idx][name] for col_idx in range(X.shape[1])])) for name in baseline_score}\n    else:\n        return _create_importances_bunch(baseline_score, np.array(scores))",
            "@validate_params({'estimator': [HasMethods(['fit'])], 'X': ['array-like'], 'y': ['array-like', None], 'scoring': [StrOptions(set(get_scorer_names())), callable, list, tuple, dict, None], 'n_repeats': [Interval(Integral, 1, None, closed='left')], 'n_jobs': [Integral, None], 'random_state': ['random_state'], 'sample_weight': ['array-like', None], 'max_samples': [Interval(Integral, 1, None, closed='left'), Interval(RealNotInt, 0, 1, closed='right')]}, prefer_skip_nested_validation=True)\ndef permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Permutation importance for feature evaluation [BRE]_.\\n\\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\\n    data set used to train the estimator or a hold-out set. The permutation\\n    importance of a feature is calculated as follows. First, a baseline metric,\\n    defined by :term:`scoring`, is evaluated on a (potentially different)\\n    dataset defined by the `X`. Next, a feature column from the validation set\\n    is permuted and the metric is evaluated again. The permutation importance\\n    is defined to be the difference between the baseline metric and metric from\\n    permutating the feature column.\\n\\n    Read more in the :ref:`User Guide <permutation_importance>`.\\n\\n    Parameters\\n    ----------\\n    estimator : object\\n        An estimator that has already been :term:`fitted` and is compatible\\n        with :term:`scorer`.\\n\\n    X : ndarray or DataFrame, shape (n_samples, n_features)\\n        Data on which permutation importance will be computed.\\n\\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\\n        Targets for supervised or `None` for unsupervised.\\n\\n    scoring : str, callable, list, tuple, or dict, default=None\\n        Scorer to use.\\n        If `scoring` represents a single score, one can use:\\n\\n        - a single string (see :ref:`scoring_parameter`);\\n        - a callable (see :ref:`scoring`) that returns a single value.\\n\\n        If `scoring` represents multiple scores, one can use:\\n\\n        - a list or tuple of unique strings;\\n        - a callable returning a dictionary where the keys are the metric\\n          names and the values are the metric scores;\\n        - a dictionary with metric names as keys and callables a values.\\n\\n        Passing multiple scores to `scoring` is more efficient than calling\\n        `permutation_importance` for each of the scores as it reuses\\n        predictions to avoid redundant computation.\\n\\n        If None, the estimator\\'s default scorer is used.\\n\\n    n_repeats : int, default=5\\n        Number of times to permute a feature.\\n\\n    n_jobs : int or None, default=None\\n        Number of jobs to run in parallel. The computation is done by computing\\n        permutation score for each columns and parallelized over the columns.\\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    random_state : int, RandomState instance, default=None\\n        Pseudo-random number generator to control the permutations of each\\n        feature.\\n        Pass an int to get reproducible results across function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    sample_weight : array-like of shape (n_samples,), default=None\\n        Sample weights used in scoring.\\n\\n        .. versionadded:: 0.24\\n\\n    max_samples : int or float, default=1.0\\n        The number of samples to draw from X to compute feature importance\\n        in each repeat (without replacement).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n        - If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples\\n          will be used.\\n\\n        While using this option may provide less accurate importance estimates,\\n        it keeps the method tractable when evaluating feature importance on\\n        large datasets. In combination with `n_repeats`, this allows to control\\n        the computational speed vs statistical accuracy trade-off of this method.\\n\\n        .. versionadded:: 1.0\\n\\n    Returns\\n    -------\\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\\n        Dictionary-like object, with the following attributes.\\n\\n        importances_mean : ndarray of shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray of shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray of shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n\\n        If there are multiple scoring metrics in the scoring parameter\\n        `result` is a dict with scorer names as keys (e.g. \\'roc_auc\\') and\\n        `Bunch` objects like above as values.\\n\\n    References\\n    ----------\\n    .. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\\n             2001. <10.1023/A:1010933404324>`\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from sklearn.inspection import permutation_importance\\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\\n    >>> y = [1, 1, 1, 0, 0, 0]\\n    >>> clf = LogisticRegression().fit(X, y)\\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\\n    ...                                 random_state=0)\\n    >>> result.importances_mean\\n    array([0.4666..., 0.       , 0.       ])\\n    >>> result.importances_std\\n    array([0.2211..., 0.       , 0.       ])\\n    '\n    if not hasattr(X, 'iloc'):\n        X = check_array(X, force_all_finite='allow-nan', dtype=None)\n    random_state = check_random_state(random_state)\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n    if not isinstance(max_samples, numbers.Integral):\n        max_samples = int(max_samples * X.shape[0])\n    elif max_samples > X.shape[0]:\n        raise ValueError('max_samples must be <= n_samples')\n    if callable(scoring):\n        scorer = scoring\n    elif scoring is None or isinstance(scoring, str):\n        scorer = check_scoring(estimator, scoring=scoring)\n    else:\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\n        scorer = _MultimetricScorer(scorers=scorers_dict)\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n    scores = Parallel(n_jobs=n_jobs)((delayed(_calculate_permutation_scores)(estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer, max_samples) for col_idx in range(X.shape[1])))\n    if isinstance(baseline_score, dict):\n        return {name: _create_importances_bunch(baseline_score[name], np.array([scores[col_idx][name] for col_idx in range(X.shape[1])])) for name in baseline_score}\n    else:\n        return _create_importances_bunch(baseline_score, np.array(scores))",
            "@validate_params({'estimator': [HasMethods(['fit'])], 'X': ['array-like'], 'y': ['array-like', None], 'scoring': [StrOptions(set(get_scorer_names())), callable, list, tuple, dict, None], 'n_repeats': [Interval(Integral, 1, None, closed='left')], 'n_jobs': [Integral, None], 'random_state': ['random_state'], 'sample_weight': ['array-like', None], 'max_samples': [Interval(Integral, 1, None, closed='left'), Interval(RealNotInt, 0, 1, closed='right')]}, prefer_skip_nested_validation=True)\ndef permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Permutation importance for feature evaluation [BRE]_.\\n\\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\\n    data set used to train the estimator or a hold-out set. The permutation\\n    importance of a feature is calculated as follows. First, a baseline metric,\\n    defined by :term:`scoring`, is evaluated on a (potentially different)\\n    dataset defined by the `X`. Next, a feature column from the validation set\\n    is permuted and the metric is evaluated again. The permutation importance\\n    is defined to be the difference between the baseline metric and metric from\\n    permutating the feature column.\\n\\n    Read more in the :ref:`User Guide <permutation_importance>`.\\n\\n    Parameters\\n    ----------\\n    estimator : object\\n        An estimator that has already been :term:`fitted` and is compatible\\n        with :term:`scorer`.\\n\\n    X : ndarray or DataFrame, shape (n_samples, n_features)\\n        Data on which permutation importance will be computed.\\n\\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\\n        Targets for supervised or `None` for unsupervised.\\n\\n    scoring : str, callable, list, tuple, or dict, default=None\\n        Scorer to use.\\n        If `scoring` represents a single score, one can use:\\n\\n        - a single string (see :ref:`scoring_parameter`);\\n        - a callable (see :ref:`scoring`) that returns a single value.\\n\\n        If `scoring` represents multiple scores, one can use:\\n\\n        - a list or tuple of unique strings;\\n        - a callable returning a dictionary where the keys are the metric\\n          names and the values are the metric scores;\\n        - a dictionary with metric names as keys and callables a values.\\n\\n        Passing multiple scores to `scoring` is more efficient than calling\\n        `permutation_importance` for each of the scores as it reuses\\n        predictions to avoid redundant computation.\\n\\n        If None, the estimator\\'s default scorer is used.\\n\\n    n_repeats : int, default=5\\n        Number of times to permute a feature.\\n\\n    n_jobs : int or None, default=None\\n        Number of jobs to run in parallel. The computation is done by computing\\n        permutation score for each columns and parallelized over the columns.\\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    random_state : int, RandomState instance, default=None\\n        Pseudo-random number generator to control the permutations of each\\n        feature.\\n        Pass an int to get reproducible results across function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    sample_weight : array-like of shape (n_samples,), default=None\\n        Sample weights used in scoring.\\n\\n        .. versionadded:: 0.24\\n\\n    max_samples : int or float, default=1.0\\n        The number of samples to draw from X to compute feature importance\\n        in each repeat (without replacement).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n        - If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples\\n          will be used.\\n\\n        While using this option may provide less accurate importance estimates,\\n        it keeps the method tractable when evaluating feature importance on\\n        large datasets. In combination with `n_repeats`, this allows to control\\n        the computational speed vs statistical accuracy trade-off of this method.\\n\\n        .. versionadded:: 1.0\\n\\n    Returns\\n    -------\\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\\n        Dictionary-like object, with the following attributes.\\n\\n        importances_mean : ndarray of shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray of shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray of shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n\\n        If there are multiple scoring metrics in the scoring parameter\\n        `result` is a dict with scorer names as keys (e.g. \\'roc_auc\\') and\\n        `Bunch` objects like above as values.\\n\\n    References\\n    ----------\\n    .. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\\n             2001. <10.1023/A:1010933404324>`\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from sklearn.inspection import permutation_importance\\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\\n    >>> y = [1, 1, 1, 0, 0, 0]\\n    >>> clf = LogisticRegression().fit(X, y)\\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\\n    ...                                 random_state=0)\\n    >>> result.importances_mean\\n    array([0.4666..., 0.       , 0.       ])\\n    >>> result.importances_std\\n    array([0.2211..., 0.       , 0.       ])\\n    '\n    if not hasattr(X, 'iloc'):\n        X = check_array(X, force_all_finite='allow-nan', dtype=None)\n    random_state = check_random_state(random_state)\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n    if not isinstance(max_samples, numbers.Integral):\n        max_samples = int(max_samples * X.shape[0])\n    elif max_samples > X.shape[0]:\n        raise ValueError('max_samples must be <= n_samples')\n    if callable(scoring):\n        scorer = scoring\n    elif scoring is None or isinstance(scoring, str):\n        scorer = check_scoring(estimator, scoring=scoring)\n    else:\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\n        scorer = _MultimetricScorer(scorers=scorers_dict)\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n    scores = Parallel(n_jobs=n_jobs)((delayed(_calculate_permutation_scores)(estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer, max_samples) for col_idx in range(X.shape[1])))\n    if isinstance(baseline_score, dict):\n        return {name: _create_importances_bunch(baseline_score[name], np.array([scores[col_idx][name] for col_idx in range(X.shape[1])])) for name in baseline_score}\n    else:\n        return _create_importances_bunch(baseline_score, np.array(scores))",
            "@validate_params({'estimator': [HasMethods(['fit'])], 'X': ['array-like'], 'y': ['array-like', None], 'scoring': [StrOptions(set(get_scorer_names())), callable, list, tuple, dict, None], 'n_repeats': [Interval(Integral, 1, None, closed='left')], 'n_jobs': [Integral, None], 'random_state': ['random_state'], 'sample_weight': ['array-like', None], 'max_samples': [Interval(Integral, 1, None, closed='left'), Interval(RealNotInt, 0, 1, closed='right')]}, prefer_skip_nested_validation=True)\ndef permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Permutation importance for feature evaluation [BRE]_.\\n\\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\\n    data set used to train the estimator or a hold-out set. The permutation\\n    importance of a feature is calculated as follows. First, a baseline metric,\\n    defined by :term:`scoring`, is evaluated on a (potentially different)\\n    dataset defined by the `X`. Next, a feature column from the validation set\\n    is permuted and the metric is evaluated again. The permutation importance\\n    is defined to be the difference between the baseline metric and metric from\\n    permutating the feature column.\\n\\n    Read more in the :ref:`User Guide <permutation_importance>`.\\n\\n    Parameters\\n    ----------\\n    estimator : object\\n        An estimator that has already been :term:`fitted` and is compatible\\n        with :term:`scorer`.\\n\\n    X : ndarray or DataFrame, shape (n_samples, n_features)\\n        Data on which permutation importance will be computed.\\n\\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\\n        Targets for supervised or `None` for unsupervised.\\n\\n    scoring : str, callable, list, tuple, or dict, default=None\\n        Scorer to use.\\n        If `scoring` represents a single score, one can use:\\n\\n        - a single string (see :ref:`scoring_parameter`);\\n        - a callable (see :ref:`scoring`) that returns a single value.\\n\\n        If `scoring` represents multiple scores, one can use:\\n\\n        - a list or tuple of unique strings;\\n        - a callable returning a dictionary where the keys are the metric\\n          names and the values are the metric scores;\\n        - a dictionary with metric names as keys and callables a values.\\n\\n        Passing multiple scores to `scoring` is more efficient than calling\\n        `permutation_importance` for each of the scores as it reuses\\n        predictions to avoid redundant computation.\\n\\n        If None, the estimator\\'s default scorer is used.\\n\\n    n_repeats : int, default=5\\n        Number of times to permute a feature.\\n\\n    n_jobs : int or None, default=None\\n        Number of jobs to run in parallel. The computation is done by computing\\n        permutation score for each columns and parallelized over the columns.\\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    random_state : int, RandomState instance, default=None\\n        Pseudo-random number generator to control the permutations of each\\n        feature.\\n        Pass an int to get reproducible results across function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    sample_weight : array-like of shape (n_samples,), default=None\\n        Sample weights used in scoring.\\n\\n        .. versionadded:: 0.24\\n\\n    max_samples : int or float, default=1.0\\n        The number of samples to draw from X to compute feature importance\\n        in each repeat (without replacement).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n        - If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples\\n          will be used.\\n\\n        While using this option may provide less accurate importance estimates,\\n        it keeps the method tractable when evaluating feature importance on\\n        large datasets. In combination with `n_repeats`, this allows to control\\n        the computational speed vs statistical accuracy trade-off of this method.\\n\\n        .. versionadded:: 1.0\\n\\n    Returns\\n    -------\\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\\n        Dictionary-like object, with the following attributes.\\n\\n        importances_mean : ndarray of shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray of shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray of shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n\\n        If there are multiple scoring metrics in the scoring parameter\\n        `result` is a dict with scorer names as keys (e.g. \\'roc_auc\\') and\\n        `Bunch` objects like above as values.\\n\\n    References\\n    ----------\\n    .. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\\n             2001. <10.1023/A:1010933404324>`\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from sklearn.inspection import permutation_importance\\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\\n    >>> y = [1, 1, 1, 0, 0, 0]\\n    >>> clf = LogisticRegression().fit(X, y)\\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\\n    ...                                 random_state=0)\\n    >>> result.importances_mean\\n    array([0.4666..., 0.       , 0.       ])\\n    >>> result.importances_std\\n    array([0.2211..., 0.       , 0.       ])\\n    '\n    if not hasattr(X, 'iloc'):\n        X = check_array(X, force_all_finite='allow-nan', dtype=None)\n    random_state = check_random_state(random_state)\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n    if not isinstance(max_samples, numbers.Integral):\n        max_samples = int(max_samples * X.shape[0])\n    elif max_samples > X.shape[0]:\n        raise ValueError('max_samples must be <= n_samples')\n    if callable(scoring):\n        scorer = scoring\n    elif scoring is None or isinstance(scoring, str):\n        scorer = check_scoring(estimator, scoring=scoring)\n    else:\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\n        scorer = _MultimetricScorer(scorers=scorers_dict)\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n    scores = Parallel(n_jobs=n_jobs)((delayed(_calculate_permutation_scores)(estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer, max_samples) for col_idx in range(X.shape[1])))\n    if isinstance(baseline_score, dict):\n        return {name: _create_importances_bunch(baseline_score[name], np.array([scores[col_idx][name] for col_idx in range(X.shape[1])])) for name in baseline_score}\n    else:\n        return _create_importances_bunch(baseline_score, np.array(scores))",
            "@validate_params({'estimator': [HasMethods(['fit'])], 'X': ['array-like'], 'y': ['array-like', None], 'scoring': [StrOptions(set(get_scorer_names())), callable, list, tuple, dict, None], 'n_repeats': [Interval(Integral, 1, None, closed='left')], 'n_jobs': [Integral, None], 'random_state': ['random_state'], 'sample_weight': ['array-like', None], 'max_samples': [Interval(Integral, 1, None, closed='left'), Interval(RealNotInt, 0, 1, closed='right')]}, prefer_skip_nested_validation=True)\ndef permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Permutation importance for feature evaluation [BRE]_.\\n\\n    The :term:`estimator` is required to be a fitted estimator. `X` can be the\\n    data set used to train the estimator or a hold-out set. The permutation\\n    importance of a feature is calculated as follows. First, a baseline metric,\\n    defined by :term:`scoring`, is evaluated on a (potentially different)\\n    dataset defined by the `X`. Next, a feature column from the validation set\\n    is permuted and the metric is evaluated again. The permutation importance\\n    is defined to be the difference between the baseline metric and metric from\\n    permutating the feature column.\\n\\n    Read more in the :ref:`User Guide <permutation_importance>`.\\n\\n    Parameters\\n    ----------\\n    estimator : object\\n        An estimator that has already been :term:`fitted` and is compatible\\n        with :term:`scorer`.\\n\\n    X : ndarray or DataFrame, shape (n_samples, n_features)\\n        Data on which permutation importance will be computed.\\n\\n    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\\n        Targets for supervised or `None` for unsupervised.\\n\\n    scoring : str, callable, list, tuple, or dict, default=None\\n        Scorer to use.\\n        If `scoring` represents a single score, one can use:\\n\\n        - a single string (see :ref:`scoring_parameter`);\\n        - a callable (see :ref:`scoring`) that returns a single value.\\n\\n        If `scoring` represents multiple scores, one can use:\\n\\n        - a list or tuple of unique strings;\\n        - a callable returning a dictionary where the keys are the metric\\n          names and the values are the metric scores;\\n        - a dictionary with metric names as keys and callables a values.\\n\\n        Passing multiple scores to `scoring` is more efficient than calling\\n        `permutation_importance` for each of the scores as it reuses\\n        predictions to avoid redundant computation.\\n\\n        If None, the estimator\\'s default scorer is used.\\n\\n    n_repeats : int, default=5\\n        Number of times to permute a feature.\\n\\n    n_jobs : int or None, default=None\\n        Number of jobs to run in parallel. The computation is done by computing\\n        permutation score for each columns and parallelized over the columns.\\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\\n        for more details.\\n\\n    random_state : int, RandomState instance, default=None\\n        Pseudo-random number generator to control the permutations of each\\n        feature.\\n        Pass an int to get reproducible results across function calls.\\n        See :term:`Glossary <random_state>`.\\n\\n    sample_weight : array-like of shape (n_samples,), default=None\\n        Sample weights used in scoring.\\n\\n        .. versionadded:: 0.24\\n\\n    max_samples : int or float, default=1.0\\n        The number of samples to draw from X to compute feature importance\\n        in each repeat (without replacement).\\n\\n        - If int, then draw `max_samples` samples.\\n        - If float, then draw `max_samples * X.shape[0]` samples.\\n        - If `max_samples` is equal to `1.0` or `X.shape[0]`, all samples\\n          will be used.\\n\\n        While using this option may provide less accurate importance estimates,\\n        it keeps the method tractable when evaluating feature importance on\\n        large datasets. In combination with `n_repeats`, this allows to control\\n        the computational speed vs statistical accuracy trade-off of this method.\\n\\n        .. versionadded:: 1.0\\n\\n    Returns\\n    -------\\n    result : :class:`~sklearn.utils.Bunch` or dict of such instances\\n        Dictionary-like object, with the following attributes.\\n\\n        importances_mean : ndarray of shape (n_features, )\\n            Mean of feature importance over `n_repeats`.\\n        importances_std : ndarray of shape (n_features, )\\n            Standard deviation over `n_repeats`.\\n        importances : ndarray of shape (n_features, n_repeats)\\n            Raw permutation importance scores.\\n\\n        If there are multiple scoring metrics in the scoring parameter\\n        `result` is a dict with scorer names as keys (e.g. \\'roc_auc\\') and\\n        `Bunch` objects like above as values.\\n\\n    References\\n    ----------\\n    .. [BRE] :doi:`L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\\n             2001. <10.1023/A:1010933404324>`\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> from sklearn.inspection import permutation_importance\\n    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\\n    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\\n    >>> y = [1, 1, 1, 0, 0, 0]\\n    >>> clf = LogisticRegression().fit(X, y)\\n    >>> result = permutation_importance(clf, X, y, n_repeats=10,\\n    ...                                 random_state=0)\\n    >>> result.importances_mean\\n    array([0.4666..., 0.       , 0.       ])\\n    >>> result.importances_std\\n    array([0.2211..., 0.       , 0.       ])\\n    '\n    if not hasattr(X, 'iloc'):\n        X = check_array(X, force_all_finite='allow-nan', dtype=None)\n    random_state = check_random_state(random_state)\n    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n    if not isinstance(max_samples, numbers.Integral):\n        max_samples = int(max_samples * X.shape[0])\n    elif max_samples > X.shape[0]:\n        raise ValueError('max_samples must be <= n_samples')\n    if callable(scoring):\n        scorer = scoring\n    elif scoring is None or isinstance(scoring, str):\n        scorer = check_scoring(estimator, scoring=scoring)\n    else:\n        scorers_dict = _check_multimetric_scoring(estimator, scoring)\n        scorer = _MultimetricScorer(scorers=scorers_dict)\n    baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)\n    scores = Parallel(n_jobs=n_jobs)((delayed(_calculate_permutation_scores)(estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer, max_samples) for col_idx in range(X.shape[1])))\n    if isinstance(baseline_score, dict):\n        return {name: _create_importances_bunch(baseline_score[name], np.array([scores[col_idx][name] for col_idx in range(X.shape[1])])) for name in baseline_score}\n    else:\n        return _create_importances_bunch(baseline_score, np.array(scores))"
        ]
    }
]