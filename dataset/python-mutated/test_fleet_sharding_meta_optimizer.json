[
    {
        "func_name": "test_sharding_optimizer",
        "original": "def test_sharding_optimizer(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable is True]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_sharding_optimizer(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable is True]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable is True]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable is True]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable is True]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable is True]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_sharding_amp_optimizer",
        "original": "def test_sharding_amp_optimizer(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_sharding_amp_optimizer(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_amp_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_amp_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_amp_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_amp_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_sharding_recompute_optimizer",
        "original": "def test_sharding_recompute_optimizer(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_sharding_recompute_optimizer(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_recompute_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_recompute_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_recompute_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_recompute_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'mul', 'elementwise_add', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_sharding_amp_recompute_optimizer",
        "original": "def test_sharding_amp_recompute_optimizer(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_sharding_amp_recompute_optimizer(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_amp_recompute_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_amp_recompute_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_amp_recompute_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_amp_recompute_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'recompute')\n    self.set_strategy(strategy, 'amp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('subprog', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0', 'loss_scaling_0', 'num_bad_steps_0', 'num_good_steps_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_sharding_amp_asp_optimizer",
        "original": "def test_sharding_amp_asp_optimizer(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'asp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_2.b_0', 'num_good_steps_0', 'fc_2.w_0', 'loss_scaling_0', 'num_bad_steps_0', 'fc_2.w_0_velocity_0', 'fc_2.w_0.asp_mask', 'learning_rate_0', 'fc_1.b_0', 'fc_1.w_0.asp_mask', 'fc_0.w_0.asp_mask', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'elementwise_mul'])",
        "mutated": [
            "def test_sharding_amp_asp_optimizer(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'asp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_2.b_0', 'num_good_steps_0', 'fc_2.w_0', 'loss_scaling_0', 'num_bad_steps_0', 'fc_2.w_0_velocity_0', 'fc_2.w_0.asp_mask', 'learning_rate_0', 'fc_1.b_0', 'fc_1.w_0.asp_mask', 'fc_0.w_0.asp_mask', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'elementwise_mul'])",
            "def test_sharding_amp_asp_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'asp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_2.b_0', 'num_good_steps_0', 'fc_2.w_0', 'loss_scaling_0', 'num_bad_steps_0', 'fc_2.w_0_velocity_0', 'fc_2.w_0.asp_mask', 'learning_rate_0', 'fc_1.b_0', 'fc_1.w_0.asp_mask', 'fc_0.w_0.asp_mask', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'elementwise_mul'])",
            "def test_sharding_amp_asp_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'asp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_2.b_0', 'num_good_steps_0', 'fc_2.w_0', 'loss_scaling_0', 'num_bad_steps_0', 'fc_2.w_0_velocity_0', 'fc_2.w_0.asp_mask', 'learning_rate_0', 'fc_1.b_0', 'fc_1.w_0.asp_mask', 'fc_0.w_0.asp_mask', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'elementwise_mul'])",
            "def test_sharding_amp_asp_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'asp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_2.b_0', 'num_good_steps_0', 'fc_2.w_0', 'loss_scaling_0', 'num_bad_steps_0', 'fc_2.w_0_velocity_0', 'fc_2.w_0.asp_mask', 'learning_rate_0', 'fc_1.b_0', 'fc_1.w_0.asp_mask', 'fc_0.w_0.asp_mask', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'elementwise_mul'])",
            "def test_sharding_amp_asp_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'asp')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertIn('cast', ops)\n    self.assertIn('check_finite_and_unscale', ops)\n    self.assertEqual(set(parameters), {'fc_2.b_0', 'num_good_steps_0', 'fc_2.w_0', 'loss_scaling_0', 'num_bad_steps_0', 'fc_2.w_0_velocity_0', 'fc_2.w_0.asp_mask', 'learning_rate_0', 'fc_1.b_0', 'fc_1.w_0.asp_mask', 'fc_0.w_0.asp_mask', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0'})\n    self.assertEqual(ops, ['cast', 'cast', 'cast', 'fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'cast', 'cast', 'cast', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'elementwise_mul'])"
        ]
    },
    {
        "func_name": "test_sharding_weight_decay",
        "original": "def test_sharding_weight_decay(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    regularization = paddle.regularizer.L2Decay(0.0001)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, regularization=regularization)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_sharding_weight_decay(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    regularization = paddle.regularizer.L2Decay(0.0001)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, regularization=regularization)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_weight_decay(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    regularization = paddle.regularizer.L2Decay(0.0001)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, regularization=regularization)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_weight_decay(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    regularization = paddle.regularizer.L2Decay(0.0001)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, regularization=regularization)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_weight_decay(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    regularization = paddle.regularizer.L2Decay(0.0001)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, regularization=regularization)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_weight_decay(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    regularization = paddle.regularizer.L2Decay(0.0001)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, regularization=regularization)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_sharding_gradient_clip",
        "original": "def test_sharding_gradient_clip(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_sharding_gradient_clip(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_gradient_clip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_gradient_clip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_gradient_clip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_gradient_clip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    parameters = [x.name for x in train_prog.list_vars() if x.persistable]\n    ops = [op.type for op in avg_cost.block.ops]\n    vars = [x.name for x in train_prog.list_vars()]\n    self.assertIn('@BroadCast', ''.join(vars))\n    self.assertEqual(set(parameters), {'fc_1.b_0', 'fc_2.b_0', 'fc_2.w_0', 'fc_1.b_0_velocity_0', 'fc_2.b_0_velocity_0', 'fc_2.w_0_velocity_0', 'learning_rate_0'})\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_sharding_clone_for_test",
        "original": "def test_sharding_clone_for_test(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    sharding.utils.comm_analyse(train_prog)\n    test_prog = train_prog.clone(for_test=True)\n    sharding.utils.add_sync_comm(test_prog, 1)\n    ops = [op.type for op in test_prog.global_block().ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean'])",
        "mutated": [
            "def test_sharding_clone_for_test(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    sharding.utils.comm_analyse(train_prog)\n    test_prog = train_prog.clone(for_test=True)\n    sharding.utils.add_sync_comm(test_prog, 1)\n    ops = [op.type for op in test_prog.global_block().ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean'])",
            "def test_sharding_clone_for_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    sharding.utils.comm_analyse(train_prog)\n    test_prog = train_prog.clone(for_test=True)\n    sharding.utils.add_sync_comm(test_prog, 1)\n    ops = [op.type for op in test_prog.global_block().ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean'])",
            "def test_sharding_clone_for_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    sharding.utils.comm_analyse(train_prog)\n    test_prog = train_prog.clone(for_test=True)\n    sharding.utils.add_sync_comm(test_prog, 1)\n    ops = [op.type for op in test_prog.global_block().ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean'])",
            "def test_sharding_clone_for_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    sharding.utils.comm_analyse(train_prog)\n    test_prog = train_prog.clone(for_test=True)\n    sharding.utils.add_sync_comm(test_prog, 1)\n    ops = [op.type for op in test_prog.global_block().ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean'])",
            "def test_sharding_clone_for_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'sharding')\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    sharding.utils.comm_analyse(train_prog)\n    test_prog = train_prog.clone(for_test=True)\n    sharding.utils.add_sync_comm(test_prog, 1)\n    ops = [op.type for op in test_prog.global_block().ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean'])"
        ]
    },
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20"
        ]
    },
    {
        "func_name": "test_sharding_with_mp",
        "original": "def test_sharding_with_mp(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 1, 'mp_degree': 2}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_sharding_with_mp(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 1, 'mp_degree': 2}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_sharding_with_mp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 1, 'mp_degree': 2}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_sharding_with_mp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 1, 'mp_degree': 2}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_sharding_with_mp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 1, 'mp_degree': 2}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_sharding_with_mp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 1, 'mp_degree': 2}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_sharding_hybrid_dp",
        "original": "def test_sharding_hybrid_dp(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.sharding_configs['sharding_degree'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    ops = [op.type for op in main_prog_ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_sharding_hybrid_dp(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.sharding_configs['sharding_degree'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    ops = [op.type for op in main_prog_ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_hybrid_dp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.sharding_configs['sharding_degree'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    ops = [op.type for op in main_prog_ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_hybrid_dp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.sharding_configs['sharding_degree'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    ops = [op.type for op in main_prog_ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_hybrid_dp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.sharding_configs['sharding_degree'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    ops = [op.type for op in main_prog_ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])",
            "def test_sharding_hybrid_dp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.sharding_configs['sharding_degree'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    ops = [op.type for op in main_prog_ops]\n    self.assertEqual(ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_sharding_hybrid_dp_gm",
        "original": "def test_sharding_hybrid_dp_gm(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 4, 'mp_degree': 1}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    fw_bw_ops = [op.type for op in train_prog.blocks[0].ops]\n    opt_ops = [op.type for op in train_prog.blocks[2].ops]\n    self.assertEqual(fw_bw_ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'elementwise_add', 'elementwise_add', 'elementwise_add', 'increment', 'elementwise_mod', 'equal', 'conditional_block'])\n    self.assertEqual(opt_ops, ['c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'scale', 'scale', 'scale', 'momentum', 'momentum', 'momentum', 'fill_constant', 'fill_constant', 'fill_constant'])\n    scale_ = -1\n    for op in train_prog.blocks[2].ops:\n        if op.type == 'scale':\n            scale_ = float(op.desc.attr('scale'))\n            self.assertEqual(scale_, 0.25)",
        "mutated": [
            "def test_sharding_hybrid_dp_gm(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 4, 'mp_degree': 1}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    fw_bw_ops = [op.type for op in train_prog.blocks[0].ops]\n    opt_ops = [op.type for op in train_prog.blocks[2].ops]\n    self.assertEqual(fw_bw_ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'elementwise_add', 'elementwise_add', 'elementwise_add', 'increment', 'elementwise_mod', 'equal', 'conditional_block'])\n    self.assertEqual(opt_ops, ['c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'scale', 'scale', 'scale', 'momentum', 'momentum', 'momentum', 'fill_constant', 'fill_constant', 'fill_constant'])\n    scale_ = -1\n    for op in train_prog.blocks[2].ops:\n        if op.type == 'scale':\n            scale_ = float(op.desc.attr('scale'))\n            self.assertEqual(scale_, 0.25)",
            "def test_sharding_hybrid_dp_gm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 4, 'mp_degree': 1}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    fw_bw_ops = [op.type for op in train_prog.blocks[0].ops]\n    opt_ops = [op.type for op in train_prog.blocks[2].ops]\n    self.assertEqual(fw_bw_ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'elementwise_add', 'elementwise_add', 'elementwise_add', 'increment', 'elementwise_mod', 'equal', 'conditional_block'])\n    self.assertEqual(opt_ops, ['c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'scale', 'scale', 'scale', 'momentum', 'momentum', 'momentum', 'fill_constant', 'fill_constant', 'fill_constant'])\n    scale_ = -1\n    for op in train_prog.blocks[2].ops:\n        if op.type == 'scale':\n            scale_ = float(op.desc.attr('scale'))\n            self.assertEqual(scale_, 0.25)",
            "def test_sharding_hybrid_dp_gm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 4, 'mp_degree': 1}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    fw_bw_ops = [op.type for op in train_prog.blocks[0].ops]\n    opt_ops = [op.type for op in train_prog.blocks[2].ops]\n    self.assertEqual(fw_bw_ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'elementwise_add', 'elementwise_add', 'elementwise_add', 'increment', 'elementwise_mod', 'equal', 'conditional_block'])\n    self.assertEqual(opt_ops, ['c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'scale', 'scale', 'scale', 'momentum', 'momentum', 'momentum', 'fill_constant', 'fill_constant', 'fill_constant'])\n    scale_ = -1\n    for op in train_prog.blocks[2].ops:\n        if op.type == 'scale':\n            scale_ = float(op.desc.attr('scale'))\n            self.assertEqual(scale_, 0.25)",
            "def test_sharding_hybrid_dp_gm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 4, 'mp_degree': 1}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    fw_bw_ops = [op.type for op in train_prog.blocks[0].ops]\n    opt_ops = [op.type for op in train_prog.blocks[2].ops]\n    self.assertEqual(fw_bw_ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'elementwise_add', 'elementwise_add', 'elementwise_add', 'increment', 'elementwise_mod', 'equal', 'conditional_block'])\n    self.assertEqual(opt_ops, ['c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'scale', 'scale', 'scale', 'momentum', 'momentum', 'momentum', 'fill_constant', 'fill_constant', 'fill_constant'])\n    scale_ = -1\n    for op in train_prog.blocks[2].ops:\n        if op.type == 'scale':\n            scale_ = float(op.desc.attr('scale'))\n            self.assertEqual(scale_, 0.25)",
            "def test_sharding_hybrid_dp_gm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.2, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 4, 'mp_degree': 1}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])\n    fw_bw_ops = [op.type for op in train_prog.blocks[0].ops]\n    opt_ops = [op.type for op in train_prog.blocks[2].ops]\n    self.assertEqual(fw_bw_ops, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'elementwise_add', 'elementwise_add', 'elementwise_add', 'increment', 'elementwise_mod', 'equal', 'conditional_block'])\n    self.assertEqual(opt_ops, ['c_allreduce_sum', 'c_allreduce_sum', 'c_allreduce_sum', 'scale', 'scale', 'scale', 'momentum', 'momentum', 'momentum', 'fill_constant', 'fill_constant', 'fill_constant'])\n    scale_ = -1\n    for op in train_prog.blocks[2].ops:\n        if op.type == 'scale':\n            scale_ = float(op.desc.attr('scale'))\n            self.assertEqual(scale_, 0.25)"
        ]
    },
    {
        "func_name": "test_sharding_with_pp",
        "original": "def test_sharding_with_pp(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 4, 'mp_degree': 1, 'pp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    print(startup_prog_op_types)\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_sharding_with_pp(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 4, 'mp_degree': 1, 'pp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    print(startup_prog_op_types)\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_sharding_with_pp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 4, 'mp_degree': 1, 'pp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    print(startup_prog_op_types)\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_sharding_with_pp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 4, 'mp_degree': 1, 'pp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    print(startup_prog_op_types)\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_sharding_with_pp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 4, 'mp_degree': 1, 'pp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    print(startup_prog_op_types)\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_sharding_with_pp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'hybrid_dp': False, 'gradient_merge_acc_step': 4, 'mp_degree': 1, 'pp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    print(startup_prog_op_types)\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types, ['fill_constant', 'fill_constant', 'fill_constant', 'c_sync_calc_stream', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_sync_comm_stream', 'recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'c_sync_calc_stream', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_sharding_dp_with_allreduce_fuse",
        "original": "def test_sharding_dp_with_allreduce_fuse(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 2\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    main_prog_ops = train_prog.global_block().ops\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    assert 'c_allreduce_sum' in main_prog_op_types\n    assert 'coalesce_tensor' in main_prog_op_types\n    for op in main_prog_ops:\n        if op.type == 'c_allreduce_sum':\n            assert 'FusedGrad' in op.input_arg_names[0]",
        "mutated": [
            "def test_sharding_dp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 2\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    main_prog_ops = train_prog.global_block().ops\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    assert 'c_allreduce_sum' in main_prog_op_types\n    assert 'coalesce_tensor' in main_prog_op_types\n    for op in main_prog_ops:\n        if op.type == 'c_allreduce_sum':\n            assert 'FusedGrad' in op.input_arg_names[0]",
            "def test_sharding_dp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 2\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    main_prog_ops = train_prog.global_block().ops\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    assert 'c_allreduce_sum' in main_prog_op_types\n    assert 'coalesce_tensor' in main_prog_op_types\n    for op in main_prog_ops:\n        if op.type == 'c_allreduce_sum':\n            assert 'FusedGrad' in op.input_arg_names[0]",
            "def test_sharding_dp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 2\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    main_prog_ops = train_prog.global_block().ops\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    assert 'c_allreduce_sum' in main_prog_op_types\n    assert 'coalesce_tensor' in main_prog_op_types\n    for op in main_prog_ops:\n        if op.type == 'c_allreduce_sum':\n            assert 'FusedGrad' in op.input_arg_names[0]",
            "def test_sharding_dp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 2\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    main_prog_ops = train_prog.global_block().ops\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    assert 'c_allreduce_sum' in main_prog_op_types\n    assert 'coalesce_tensor' in main_prog_op_types\n    for op in main_prog_ops:\n        if op.type == 'c_allreduce_sum':\n            assert 'FusedGrad' in op.input_arg_names[0]",
            "def test_sharding_dp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, _) = self.net(train_prog, startup_prog)\n    strategy = paddle.distributed.fleet.DistributedStrategy()\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_segment_strategy': 'segment_broadcast_MB', 'segment_broadcast_MB': 0.1, 'segment_anchors': None, 'sharding_degree': 2, 'dp_degree': 2, 'hybrid_dp': True, 'gradient_merge_acc_step': 1, 'mp_degree': 1}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 2\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    main_prog_ops = train_prog.global_block().ops\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    assert 'c_allreduce_sum' in main_prog_op_types\n    assert 'coalesce_tensor' in main_prog_op_types\n    for op in main_prog_ops:\n        if op.type == 'c_allreduce_sum':\n            assert 'FusedGrad' in op.input_arg_names[0]"
        ]
    },
    {
        "func_name": "test_hybrid_with_mp_pp_amp_gclip",
        "original": "def test_hybrid_with_mp_pp_amp_gclip(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_hybrid_with_mp_pp_amp_gclip(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_mp_pp_amp_gclip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_mp_pp_amp_gclip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_mp_pp_amp_gclip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_mp_pp_amp_gclip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_mp_pp_amp_gclip_for_optimizer",
        "original": "def test_hybrid_with_mp_pp_amp_gclip_for_optimizer(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip, name='adamw')\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_hybrid_with_mp_pp_amp_gclip_for_optimizer(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip, name='adamw')\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_mp_pp_amp_gclip_for_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip, name='adamw')\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_mp_pp_amp_gclip_for_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip, name='adamw')\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_mp_pp_amp_gclip_for_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip, name='adamw')\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_mp_pp_amp_gclip_for_optimizer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 2, 'pp_degree': 2, 'dp_degree': 1}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip, name='adamw')\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['partial_recv', 'partial_allgather', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'partial_send', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw', 'adamw'])\n    self.assertIn('partial_recv', main_prog_op_types)\n    self.assertIn('partial_allgather', main_prog_op_types)\n    self.assertIn('partial_send', main_prog_op_types)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    self.assertEqual(main_prog_op_types.count('c_allreduce_sum'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.mp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            mp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(mp_group_waiting_ports, ['127.0.0.1:36003'])\n    sharding_group_waiting_port = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_pp_dp_amp_fp16allreduce",
        "original": "def test_hybrid_with_pp_dp_amp_fp16allreduce(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_hybrid_with_pp_dp_amp_fp16allreduce(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_sharding_pp_amp_fp16allreduce_in_optimize",
        "original": "def test_hybrid_with_sharding_pp_amp_fp16allreduce_in_optimize(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 1, 'pp_allreduce_in_optimize': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_hybrid_with_sharding_pp_amp_fp16allreduce_in_optimize(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 1, 'pp_allreduce_in_optimize': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_sharding_pp_amp_fp16allreduce_in_optimize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 1, 'pp_allreduce_in_optimize': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_sharding_pp_amp_fp16allreduce_in_optimize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 1, 'pp_allreduce_in_optimize': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_sharding_pp_amp_fp16allreduce_in_optimize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 1, 'pp_allreduce_in_optimize': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_sharding_pp_amp_fp16allreduce_in_optimize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'segment_broadcast_MB': 0.1, 'sharding_degree': 2, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 1, 'pp_allreduce_in_optimize': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 2)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.sharding_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            sharding_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(sharding_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_1':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast",
        "original": "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_offload",
        "original": "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_offload(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_offload': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'momentum', 'memcpy', 'momentum', 'cast', 'memcpy'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_offload(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_offload': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'momentum', 'memcpy', 'momentum', 'cast', 'memcpy'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_offload(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_offload': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'momentum', 'memcpy', 'momentum', 'cast', 'memcpy'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_offload(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_offload': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'momentum', 'memcpy', 'momentum', 'cast', 'memcpy'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_offload(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_offload': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'momentum', 'memcpy', 'momentum', 'cast', 'memcpy'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_offload(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_offload': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast', 'cast', 'memcpy', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'memcpy', 'momentum', 'cast', 'memcpy', 'momentum', 'memcpy', 'momentum', 'cast', 'memcpy'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast_with_gradient_fuse",
        "original": "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast_with_gradient_fuse(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'cast', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast_with_gradient_fuse(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'cast', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast_with_gradient_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'cast', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast_with_gradient_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'cast', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast_with_gradient_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'cast', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_fp16allreduce_optimize_cast_with_gradient_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2, 'optimize_cast': True}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fp16_allreduce = True\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'cast', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'cast', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_pp_dp_amp_with_gradient_fuse",
        "original": "def test_hybrid_with_pp_dp_amp_with_gradient_fuse(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])\n    self.assertEqual(main_prog_op_types.count('c_allreduce_max'), 1)\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_ports, ['127.0.0.1:36003'])\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_pp_dp_amp_with_gradient_fuse_and_avg_after_sum",
        "original": "def test_hybrid_with_pp_dp_amp_with_gradient_fuse_and_avg_after_sum(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_amp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_pp_dp_with_gradient_fuse_and_avg_after_sum",
        "original": "def test_hybrid_with_pp_dp_with_gradient_fuse_and_avg_after_sum(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_hybrid_with_pp_dp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_with_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])"
        ]
    },
    {
        "func_name": "test_hybrid_with_pp_dp_with_amp_no_dynamic_gradient_fuse_and_avg_after_sum",
        "original": "def test_hybrid_with_pp_dp_with_amp_no_dynamic_gradient_fuse_and_avg_after_sum(self):\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0'], 'use_dynamic_loss_scaling': False}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'scale', 'check_finite_and_unscale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
        "mutated": [
            "def test_hybrid_with_pp_dp_with_amp_no_dynamic_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0'], 'use_dynamic_loss_scaling': False}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'scale', 'check_finite_and_unscale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_with_amp_no_dynamic_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0'], 'use_dynamic_loss_scaling': False}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'scale', 'check_finite_and_unscale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_with_amp_no_dynamic_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0'], 'use_dynamic_loss_scaling': False}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'scale', 'check_finite_and_unscale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_with_amp_no_dynamic_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0'], 'use_dynamic_loss_scaling': False}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'scale', 'check_finite_and_unscale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])",
            "def test_hybrid_with_pp_dp_with_amp_no_dynamic_gradient_fuse_and_avg_after_sum(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (paddle.base.Program(), paddle.base.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'mp_degree': 1, 'pp_degree': 2, 'dp_degree': 2}\n    strategy.amp = True\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0'], 'use_dynamic_loss_scaling': False}\n    strategy.pipeline = True\n    strategy.pipeline_configs = {'schedule_mode': '1F1B', 'micro_batch_size': 2, 'accumulate_steps': 4}\n    strategy.gradient_scale_configs = {'scale_strategy': 'avg', 'scale_gradient': True}\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'cast', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'c_sync_comm_stream', 'scale', 'scale', 'check_finite_and_unscale', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum'])"
        ]
    }
]