[
    {
        "func_name": "stable_sort_residuals",
        "original": "def stable_sort_residuals(residuals: Tensor, ransidx: Tensor) -> Tuple[Tensor, Tensor]:\n    logres = torch.log(residuals + 1e-10)\n    minlogres = torch.min(logres)\n    maxlogres = torch.max(logres)\n    sorting_score = ransidx.unsqueeze(0).float() + 0.99 * (logres - minlogres) / (maxlogres - minlogres)\n    sorting_idxes = torch.argsort(sorting_score, dim=-1)\n    iters_range = torch.arange(residuals.shape[0], device=residuals.device)\n    return (residuals[iters_range.unsqueeze(-1), sorting_idxes], sorting_idxes)",
        "mutated": [
            "def stable_sort_residuals(residuals: Tensor, ransidx: Tensor) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n    logres = torch.log(residuals + 1e-10)\n    minlogres = torch.min(logres)\n    maxlogres = torch.max(logres)\n    sorting_score = ransidx.unsqueeze(0).float() + 0.99 * (logres - minlogres) / (maxlogres - minlogres)\n    sorting_idxes = torch.argsort(sorting_score, dim=-1)\n    iters_range = torch.arange(residuals.shape[0], device=residuals.device)\n    return (residuals[iters_range.unsqueeze(-1), sorting_idxes], sorting_idxes)",
            "def stable_sort_residuals(residuals: Tensor, ransidx: Tensor) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    logres = torch.log(residuals + 1e-10)\n    minlogres = torch.min(logres)\n    maxlogres = torch.max(logres)\n    sorting_score = ransidx.unsqueeze(0).float() + 0.99 * (logres - minlogres) / (maxlogres - minlogres)\n    sorting_idxes = torch.argsort(sorting_score, dim=-1)\n    iters_range = torch.arange(residuals.shape[0], device=residuals.device)\n    return (residuals[iters_range.unsqueeze(-1), sorting_idxes], sorting_idxes)",
            "def stable_sort_residuals(residuals: Tensor, ransidx: Tensor) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    logres = torch.log(residuals + 1e-10)\n    minlogres = torch.min(logres)\n    maxlogres = torch.max(logres)\n    sorting_score = ransidx.unsqueeze(0).float() + 0.99 * (logres - minlogres) / (maxlogres - minlogres)\n    sorting_idxes = torch.argsort(sorting_score, dim=-1)\n    iters_range = torch.arange(residuals.shape[0], device=residuals.device)\n    return (residuals[iters_range.unsqueeze(-1), sorting_idxes], sorting_idxes)",
            "def stable_sort_residuals(residuals: Tensor, ransidx: Tensor) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    logres = torch.log(residuals + 1e-10)\n    minlogres = torch.min(logres)\n    maxlogres = torch.max(logres)\n    sorting_score = ransidx.unsqueeze(0).float() + 0.99 * (logres - minlogres) / (maxlogres - minlogres)\n    sorting_idxes = torch.argsort(sorting_score, dim=-1)\n    iters_range = torch.arange(residuals.shape[0], device=residuals.device)\n    return (residuals[iters_range.unsqueeze(-1), sorting_idxes], sorting_idxes)",
            "def stable_sort_residuals(residuals: Tensor, ransidx: Tensor) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    logres = torch.log(residuals + 1e-10)\n    minlogres = torch.min(logres)\n    maxlogres = torch.max(logres)\n    sorting_score = ransidx.unsqueeze(0).float() + 0.99 * (logres - minlogres) / (maxlogres - minlogres)\n    sorting_idxes = torch.argsort(sorting_score, dim=-1)\n    iters_range = torch.arange(residuals.shape[0], device=residuals.device)\n    return (residuals[iters_range.unsqueeze(-1), sorting_idxes], sorting_idxes)"
        ]
    },
    {
        "func_name": "group_sum_and_cumsum",
        "original": "def group_sum_and_cumsum(scores_mat: Tensor, end_group_idx: Tensor, group_idx: Union[Tensor, slice, None]=None) -> Tuple[Tensor, Union[Tensor, None]]:\n    cumulative_scores = torch.cumsum(scores_mat, dim=1)\n    ending_cumusums = cumulative_scores[:, end_group_idx]\n    shifted_ending_cumusums = torch.cat([torch.zeros(size=(ending_cumusums.shape[0], 1), dtype=ending_cumusums.dtype, device=scores_mat.device), ending_cumusums[:, :-1]], dim=1)\n    grouped_sums = ending_cumusums - shifted_ending_cumusums\n    if group_idx is not None:\n        grouped_cumsums = cumulative_scores - shifted_ending_cumusums[:, group_idx]\n        return (grouped_sums, grouped_cumsums)\n    return (grouped_sums, None)",
        "mutated": [
            "def group_sum_and_cumsum(scores_mat: Tensor, end_group_idx: Tensor, group_idx: Union[Tensor, slice, None]=None) -> Tuple[Tensor, Union[Tensor, None]]:\n    if False:\n        i = 10\n    cumulative_scores = torch.cumsum(scores_mat, dim=1)\n    ending_cumusums = cumulative_scores[:, end_group_idx]\n    shifted_ending_cumusums = torch.cat([torch.zeros(size=(ending_cumusums.shape[0], 1), dtype=ending_cumusums.dtype, device=scores_mat.device), ending_cumusums[:, :-1]], dim=1)\n    grouped_sums = ending_cumusums - shifted_ending_cumusums\n    if group_idx is not None:\n        grouped_cumsums = cumulative_scores - shifted_ending_cumusums[:, group_idx]\n        return (grouped_sums, grouped_cumsums)\n    return (grouped_sums, None)",
            "def group_sum_and_cumsum(scores_mat: Tensor, end_group_idx: Tensor, group_idx: Union[Tensor, slice, None]=None) -> Tuple[Tensor, Union[Tensor, None]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cumulative_scores = torch.cumsum(scores_mat, dim=1)\n    ending_cumusums = cumulative_scores[:, end_group_idx]\n    shifted_ending_cumusums = torch.cat([torch.zeros(size=(ending_cumusums.shape[0], 1), dtype=ending_cumusums.dtype, device=scores_mat.device), ending_cumusums[:, :-1]], dim=1)\n    grouped_sums = ending_cumusums - shifted_ending_cumusums\n    if group_idx is not None:\n        grouped_cumsums = cumulative_scores - shifted_ending_cumusums[:, group_idx]\n        return (grouped_sums, grouped_cumsums)\n    return (grouped_sums, None)",
            "def group_sum_and_cumsum(scores_mat: Tensor, end_group_idx: Tensor, group_idx: Union[Tensor, slice, None]=None) -> Tuple[Tensor, Union[Tensor, None]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cumulative_scores = torch.cumsum(scores_mat, dim=1)\n    ending_cumusums = cumulative_scores[:, end_group_idx]\n    shifted_ending_cumusums = torch.cat([torch.zeros(size=(ending_cumusums.shape[0], 1), dtype=ending_cumusums.dtype, device=scores_mat.device), ending_cumusums[:, :-1]], dim=1)\n    grouped_sums = ending_cumusums - shifted_ending_cumusums\n    if group_idx is not None:\n        grouped_cumsums = cumulative_scores - shifted_ending_cumusums[:, group_idx]\n        return (grouped_sums, grouped_cumsums)\n    return (grouped_sums, None)",
            "def group_sum_and_cumsum(scores_mat: Tensor, end_group_idx: Tensor, group_idx: Union[Tensor, slice, None]=None) -> Tuple[Tensor, Union[Tensor, None]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cumulative_scores = torch.cumsum(scores_mat, dim=1)\n    ending_cumusums = cumulative_scores[:, end_group_idx]\n    shifted_ending_cumusums = torch.cat([torch.zeros(size=(ending_cumusums.shape[0], 1), dtype=ending_cumusums.dtype, device=scores_mat.device), ending_cumusums[:, :-1]], dim=1)\n    grouped_sums = ending_cumusums - shifted_ending_cumusums\n    if group_idx is not None:\n        grouped_cumsums = cumulative_scores - shifted_ending_cumusums[:, group_idx]\n        return (grouped_sums, grouped_cumsums)\n    return (grouped_sums, None)",
            "def group_sum_and_cumsum(scores_mat: Tensor, end_group_idx: Tensor, group_idx: Union[Tensor, slice, None]=None) -> Tuple[Tensor, Union[Tensor, None]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cumulative_scores = torch.cumsum(scores_mat, dim=1)\n    ending_cumusums = cumulative_scores[:, end_group_idx]\n    shifted_ending_cumusums = torch.cat([torch.zeros(size=(ending_cumusums.shape[0], 1), dtype=ending_cumusums.dtype, device=scores_mat.device), ending_cumusums[:, :-1]], dim=1)\n    grouped_sums = ending_cumusums - shifted_ending_cumusums\n    if group_idx is not None:\n        grouped_cumsums = cumulative_scores - shifted_ending_cumusums[:, group_idx]\n        return (grouped_sums, grouped_cumsums)\n    return (grouped_sums, None)"
        ]
    },
    {
        "func_name": "confidence_based_inlier_selection",
        "original": "def confidence_based_inlier_selection(residuals: Tensor, ransidx: Tensor, rdims: Tensor, idxoffsets: Tensor, dv: torch.device, min_confidence: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n    numransacs = rdims.shape[0]\n    numiters = residuals.shape[0]\n    (sorted_res, sorting_idxes) = stable_sort_residuals(residuals, ransidx)\n    sorted_res_sqr = sorted_res ** 2\n    too_perfect_fits = sorted_res_sqr <= 1e-08\n    end_rans_indexing = torch.cumsum(rdims, dim=0) - 1\n    (_, inv_indices, res_dup_counts) = torch.unique_consecutive(sorted_res_sqr.half().float(), dim=1, return_counts=True, return_inverse=True)\n    duplicates_per_sample = res_dup_counts[inv_indices]\n    inlier_weights = (1.0 / duplicates_per_sample).repeat(numiters, 1)\n    inlier_weights[too_perfect_fits] = 0.0\n    (balanced_rdims, weights_cumsums) = group_sum_and_cumsum(inlier_weights, end_rans_indexing, ransidx)\n    if not isinstance(weights_cumsums, Tensor):\n        raise TypeError('Expected the `weights_cumsums` to be a Tensor!')\n    progressive_inl_rates = weights_cumsums.float() / balanced_rdims.repeat_interleave(rdims, dim=1).float()\n    good_inl_mask = (sorted_res_sqr * min_confidence <= progressive_inl_rates) | too_perfect_fits\n    inlier_weights[~good_inl_mask] = 0.0\n    (inlier_counts_matrix, _) = group_sum_and_cumsum(inlier_weights, end_rans_indexing)\n    (inl_counts, inl_iters) = torch.max(inlier_counts_matrix.long(), dim=0)\n    relative_inl_idxes = arange_sequence(inl_counts)\n    inl_ransidx = torch.arange(numransacs, device=dv).repeat_interleave(inl_counts)\n    inl_sampleidx = sorting_idxes[inl_iters.repeat_interleave(inl_counts), idxoffsets[inl_ransidx] + relative_inl_idxes]\n    highest_accepted_sqr_residuals = sorted_res_sqr[inl_iters, idxoffsets + inl_counts - 1]\n    expected_extra_inl = balanced_rdims[inl_iters, torch.arange(numransacs, device=dv)].float() * highest_accepted_sqr_residuals\n    return (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_counts.float() / expected_extra_inl)",
        "mutated": [
            "def confidence_based_inlier_selection(residuals: Tensor, ransidx: Tensor, rdims: Tensor, idxoffsets: Tensor, dv: torch.device, min_confidence: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n    numransacs = rdims.shape[0]\n    numiters = residuals.shape[0]\n    (sorted_res, sorting_idxes) = stable_sort_residuals(residuals, ransidx)\n    sorted_res_sqr = sorted_res ** 2\n    too_perfect_fits = sorted_res_sqr <= 1e-08\n    end_rans_indexing = torch.cumsum(rdims, dim=0) - 1\n    (_, inv_indices, res_dup_counts) = torch.unique_consecutive(sorted_res_sqr.half().float(), dim=1, return_counts=True, return_inverse=True)\n    duplicates_per_sample = res_dup_counts[inv_indices]\n    inlier_weights = (1.0 / duplicates_per_sample).repeat(numiters, 1)\n    inlier_weights[too_perfect_fits] = 0.0\n    (balanced_rdims, weights_cumsums) = group_sum_and_cumsum(inlier_weights, end_rans_indexing, ransidx)\n    if not isinstance(weights_cumsums, Tensor):\n        raise TypeError('Expected the `weights_cumsums` to be a Tensor!')\n    progressive_inl_rates = weights_cumsums.float() / balanced_rdims.repeat_interleave(rdims, dim=1).float()\n    good_inl_mask = (sorted_res_sqr * min_confidence <= progressive_inl_rates) | too_perfect_fits\n    inlier_weights[~good_inl_mask] = 0.0\n    (inlier_counts_matrix, _) = group_sum_and_cumsum(inlier_weights, end_rans_indexing)\n    (inl_counts, inl_iters) = torch.max(inlier_counts_matrix.long(), dim=0)\n    relative_inl_idxes = arange_sequence(inl_counts)\n    inl_ransidx = torch.arange(numransacs, device=dv).repeat_interleave(inl_counts)\n    inl_sampleidx = sorting_idxes[inl_iters.repeat_interleave(inl_counts), idxoffsets[inl_ransidx] + relative_inl_idxes]\n    highest_accepted_sqr_residuals = sorted_res_sqr[inl_iters, idxoffsets + inl_counts - 1]\n    expected_extra_inl = balanced_rdims[inl_iters, torch.arange(numransacs, device=dv)].float() * highest_accepted_sqr_residuals\n    return (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_counts.float() / expected_extra_inl)",
            "def confidence_based_inlier_selection(residuals: Tensor, ransidx: Tensor, rdims: Tensor, idxoffsets: Tensor, dv: torch.device, min_confidence: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    numransacs = rdims.shape[0]\n    numiters = residuals.shape[0]\n    (sorted_res, sorting_idxes) = stable_sort_residuals(residuals, ransidx)\n    sorted_res_sqr = sorted_res ** 2\n    too_perfect_fits = sorted_res_sqr <= 1e-08\n    end_rans_indexing = torch.cumsum(rdims, dim=0) - 1\n    (_, inv_indices, res_dup_counts) = torch.unique_consecutive(sorted_res_sqr.half().float(), dim=1, return_counts=True, return_inverse=True)\n    duplicates_per_sample = res_dup_counts[inv_indices]\n    inlier_weights = (1.0 / duplicates_per_sample).repeat(numiters, 1)\n    inlier_weights[too_perfect_fits] = 0.0\n    (balanced_rdims, weights_cumsums) = group_sum_and_cumsum(inlier_weights, end_rans_indexing, ransidx)\n    if not isinstance(weights_cumsums, Tensor):\n        raise TypeError('Expected the `weights_cumsums` to be a Tensor!')\n    progressive_inl_rates = weights_cumsums.float() / balanced_rdims.repeat_interleave(rdims, dim=1).float()\n    good_inl_mask = (sorted_res_sqr * min_confidence <= progressive_inl_rates) | too_perfect_fits\n    inlier_weights[~good_inl_mask] = 0.0\n    (inlier_counts_matrix, _) = group_sum_and_cumsum(inlier_weights, end_rans_indexing)\n    (inl_counts, inl_iters) = torch.max(inlier_counts_matrix.long(), dim=0)\n    relative_inl_idxes = arange_sequence(inl_counts)\n    inl_ransidx = torch.arange(numransacs, device=dv).repeat_interleave(inl_counts)\n    inl_sampleidx = sorting_idxes[inl_iters.repeat_interleave(inl_counts), idxoffsets[inl_ransidx] + relative_inl_idxes]\n    highest_accepted_sqr_residuals = sorted_res_sqr[inl_iters, idxoffsets + inl_counts - 1]\n    expected_extra_inl = balanced_rdims[inl_iters, torch.arange(numransacs, device=dv)].float() * highest_accepted_sqr_residuals\n    return (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_counts.float() / expected_extra_inl)",
            "def confidence_based_inlier_selection(residuals: Tensor, ransidx: Tensor, rdims: Tensor, idxoffsets: Tensor, dv: torch.device, min_confidence: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    numransacs = rdims.shape[0]\n    numiters = residuals.shape[0]\n    (sorted_res, sorting_idxes) = stable_sort_residuals(residuals, ransidx)\n    sorted_res_sqr = sorted_res ** 2\n    too_perfect_fits = sorted_res_sqr <= 1e-08\n    end_rans_indexing = torch.cumsum(rdims, dim=0) - 1\n    (_, inv_indices, res_dup_counts) = torch.unique_consecutive(sorted_res_sqr.half().float(), dim=1, return_counts=True, return_inverse=True)\n    duplicates_per_sample = res_dup_counts[inv_indices]\n    inlier_weights = (1.0 / duplicates_per_sample).repeat(numiters, 1)\n    inlier_weights[too_perfect_fits] = 0.0\n    (balanced_rdims, weights_cumsums) = group_sum_and_cumsum(inlier_weights, end_rans_indexing, ransidx)\n    if not isinstance(weights_cumsums, Tensor):\n        raise TypeError('Expected the `weights_cumsums` to be a Tensor!')\n    progressive_inl_rates = weights_cumsums.float() / balanced_rdims.repeat_interleave(rdims, dim=1).float()\n    good_inl_mask = (sorted_res_sqr * min_confidence <= progressive_inl_rates) | too_perfect_fits\n    inlier_weights[~good_inl_mask] = 0.0\n    (inlier_counts_matrix, _) = group_sum_and_cumsum(inlier_weights, end_rans_indexing)\n    (inl_counts, inl_iters) = torch.max(inlier_counts_matrix.long(), dim=0)\n    relative_inl_idxes = arange_sequence(inl_counts)\n    inl_ransidx = torch.arange(numransacs, device=dv).repeat_interleave(inl_counts)\n    inl_sampleidx = sorting_idxes[inl_iters.repeat_interleave(inl_counts), idxoffsets[inl_ransidx] + relative_inl_idxes]\n    highest_accepted_sqr_residuals = sorted_res_sqr[inl_iters, idxoffsets + inl_counts - 1]\n    expected_extra_inl = balanced_rdims[inl_iters, torch.arange(numransacs, device=dv)].float() * highest_accepted_sqr_residuals\n    return (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_counts.float() / expected_extra_inl)",
            "def confidence_based_inlier_selection(residuals: Tensor, ransidx: Tensor, rdims: Tensor, idxoffsets: Tensor, dv: torch.device, min_confidence: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    numransacs = rdims.shape[0]\n    numiters = residuals.shape[0]\n    (sorted_res, sorting_idxes) = stable_sort_residuals(residuals, ransidx)\n    sorted_res_sqr = sorted_res ** 2\n    too_perfect_fits = sorted_res_sqr <= 1e-08\n    end_rans_indexing = torch.cumsum(rdims, dim=0) - 1\n    (_, inv_indices, res_dup_counts) = torch.unique_consecutive(sorted_res_sqr.half().float(), dim=1, return_counts=True, return_inverse=True)\n    duplicates_per_sample = res_dup_counts[inv_indices]\n    inlier_weights = (1.0 / duplicates_per_sample).repeat(numiters, 1)\n    inlier_weights[too_perfect_fits] = 0.0\n    (balanced_rdims, weights_cumsums) = group_sum_and_cumsum(inlier_weights, end_rans_indexing, ransidx)\n    if not isinstance(weights_cumsums, Tensor):\n        raise TypeError('Expected the `weights_cumsums` to be a Tensor!')\n    progressive_inl_rates = weights_cumsums.float() / balanced_rdims.repeat_interleave(rdims, dim=1).float()\n    good_inl_mask = (sorted_res_sqr * min_confidence <= progressive_inl_rates) | too_perfect_fits\n    inlier_weights[~good_inl_mask] = 0.0\n    (inlier_counts_matrix, _) = group_sum_and_cumsum(inlier_weights, end_rans_indexing)\n    (inl_counts, inl_iters) = torch.max(inlier_counts_matrix.long(), dim=0)\n    relative_inl_idxes = arange_sequence(inl_counts)\n    inl_ransidx = torch.arange(numransacs, device=dv).repeat_interleave(inl_counts)\n    inl_sampleidx = sorting_idxes[inl_iters.repeat_interleave(inl_counts), idxoffsets[inl_ransidx] + relative_inl_idxes]\n    highest_accepted_sqr_residuals = sorted_res_sqr[inl_iters, idxoffsets + inl_counts - 1]\n    expected_extra_inl = balanced_rdims[inl_iters, torch.arange(numransacs, device=dv)].float() * highest_accepted_sqr_residuals\n    return (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_counts.float() / expected_extra_inl)",
            "def confidence_based_inlier_selection(residuals: Tensor, ransidx: Tensor, rdims: Tensor, idxoffsets: Tensor, dv: torch.device, min_confidence: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    numransacs = rdims.shape[0]\n    numiters = residuals.shape[0]\n    (sorted_res, sorting_idxes) = stable_sort_residuals(residuals, ransidx)\n    sorted_res_sqr = sorted_res ** 2\n    too_perfect_fits = sorted_res_sqr <= 1e-08\n    end_rans_indexing = torch.cumsum(rdims, dim=0) - 1\n    (_, inv_indices, res_dup_counts) = torch.unique_consecutive(sorted_res_sqr.half().float(), dim=1, return_counts=True, return_inverse=True)\n    duplicates_per_sample = res_dup_counts[inv_indices]\n    inlier_weights = (1.0 / duplicates_per_sample).repeat(numiters, 1)\n    inlier_weights[too_perfect_fits] = 0.0\n    (balanced_rdims, weights_cumsums) = group_sum_and_cumsum(inlier_weights, end_rans_indexing, ransidx)\n    if not isinstance(weights_cumsums, Tensor):\n        raise TypeError('Expected the `weights_cumsums` to be a Tensor!')\n    progressive_inl_rates = weights_cumsums.float() / balanced_rdims.repeat_interleave(rdims, dim=1).float()\n    good_inl_mask = (sorted_res_sqr * min_confidence <= progressive_inl_rates) | too_perfect_fits\n    inlier_weights[~good_inl_mask] = 0.0\n    (inlier_counts_matrix, _) = group_sum_and_cumsum(inlier_weights, end_rans_indexing)\n    (inl_counts, inl_iters) = torch.max(inlier_counts_matrix.long(), dim=0)\n    relative_inl_idxes = arange_sequence(inl_counts)\n    inl_ransidx = torch.arange(numransacs, device=dv).repeat_interleave(inl_counts)\n    inl_sampleidx = sorting_idxes[inl_iters.repeat_interleave(inl_counts), idxoffsets[inl_ransidx] + relative_inl_idxes]\n    highest_accepted_sqr_residuals = sorted_res_sqr[inl_iters, idxoffsets + inl_counts - 1]\n    expected_extra_inl = balanced_rdims[inl_iters, torch.arange(numransacs, device=dv)].float() * highest_accepted_sqr_residuals\n    return (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_counts.float() / expected_extra_inl)"
        ]
    },
    {
        "func_name": "sample_padded_inliers",
        "original": "def sample_padded_inliers(xsamples: Tensor, ysamples: Tensor, inlier_counts: Tensor, inl_ransidx: Tensor, inl_sampleidx: Tensor, numransacs: int, dv: torch.device) -> Tuple[Tensor, Tensor]:\n    maxinliers = int(torch.max(inlier_counts).item())\n    dtype = xsamples.dtype\n    padded_inlier_x = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_y = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_x[inl_ransidx, piecewise_arange(inl_ransidx)] = xsamples[inl_sampleidx]\n    padded_inlier_y[inl_ransidx, piecewise_arange(inl_ransidx)] = ysamples[inl_sampleidx]\n    return (padded_inlier_x, padded_inlier_y)",
        "mutated": [
            "def sample_padded_inliers(xsamples: Tensor, ysamples: Tensor, inlier_counts: Tensor, inl_ransidx: Tensor, inl_sampleidx: Tensor, numransacs: int, dv: torch.device) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n    maxinliers = int(torch.max(inlier_counts).item())\n    dtype = xsamples.dtype\n    padded_inlier_x = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_y = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_x[inl_ransidx, piecewise_arange(inl_ransidx)] = xsamples[inl_sampleidx]\n    padded_inlier_y[inl_ransidx, piecewise_arange(inl_ransidx)] = ysamples[inl_sampleidx]\n    return (padded_inlier_x, padded_inlier_y)",
            "def sample_padded_inliers(xsamples: Tensor, ysamples: Tensor, inlier_counts: Tensor, inl_ransidx: Tensor, inl_sampleidx: Tensor, numransacs: int, dv: torch.device) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    maxinliers = int(torch.max(inlier_counts).item())\n    dtype = xsamples.dtype\n    padded_inlier_x = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_y = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_x[inl_ransidx, piecewise_arange(inl_ransidx)] = xsamples[inl_sampleidx]\n    padded_inlier_y[inl_ransidx, piecewise_arange(inl_ransidx)] = ysamples[inl_sampleidx]\n    return (padded_inlier_x, padded_inlier_y)",
            "def sample_padded_inliers(xsamples: Tensor, ysamples: Tensor, inlier_counts: Tensor, inl_ransidx: Tensor, inl_sampleidx: Tensor, numransacs: int, dv: torch.device) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    maxinliers = int(torch.max(inlier_counts).item())\n    dtype = xsamples.dtype\n    padded_inlier_x = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_y = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_x[inl_ransidx, piecewise_arange(inl_ransidx)] = xsamples[inl_sampleidx]\n    padded_inlier_y[inl_ransidx, piecewise_arange(inl_ransidx)] = ysamples[inl_sampleidx]\n    return (padded_inlier_x, padded_inlier_y)",
            "def sample_padded_inliers(xsamples: Tensor, ysamples: Tensor, inlier_counts: Tensor, inl_ransidx: Tensor, inl_sampleidx: Tensor, numransacs: int, dv: torch.device) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    maxinliers = int(torch.max(inlier_counts).item())\n    dtype = xsamples.dtype\n    padded_inlier_x = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_y = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_x[inl_ransidx, piecewise_arange(inl_ransidx)] = xsamples[inl_sampleidx]\n    padded_inlier_y[inl_ransidx, piecewise_arange(inl_ransidx)] = ysamples[inl_sampleidx]\n    return (padded_inlier_x, padded_inlier_y)",
            "def sample_padded_inliers(xsamples: Tensor, ysamples: Tensor, inlier_counts: Tensor, inl_ransidx: Tensor, inl_sampleidx: Tensor, numransacs: int, dv: torch.device) -> Tuple[Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    maxinliers = int(torch.max(inlier_counts).item())\n    dtype = xsamples.dtype\n    padded_inlier_x = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_y = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)\n    padded_inlier_x[inl_ransidx, piecewise_arange(inl_ransidx)] = xsamples[inl_sampleidx]\n    padded_inlier_y[inl_ransidx, piecewise_arange(inl_ransidx)] = ysamples[inl_sampleidx]\n    return (padded_inlier_x, padded_inlier_y)"
        ]
    },
    {
        "func_name": "ransac",
        "original": "def ransac(xsamples: Tensor, ysamples: Tensor, rdims: Tensor, config: Dict[str, Any], iters: int=128, refit: bool=True) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    DET_THR = config['detected_scale_rate_threshold']\n    MIN_CONFIDENCE = config['min_confidence']\n    dv: torch.device = config['device']\n    numransacs = rdims.shape[0]\n    ransidx = torch.arange(numransacs, device=dv).repeat_interleave(rdims)\n    idxoffsets = torch.cat([torch.tensor([0], device=dv), torch.cumsum(rdims[:-1], dim=0)], dim=0)\n    rand_samples_rel = draw_first_k_couples(iters, rdims, dv)\n    rand_samples_abs = rand_samples_rel + idxoffsets\n    sampled_x = torch.transpose(xsamples[rand_samples_abs], dim0=1, dim1=2)\n    sampled_y = torch.transpose(ysamples[rand_samples_abs], dim0=1, dim1=2)\n    affinities_fit = torch.transpose(batch_2x2_inv(sampled_x, check_dets=True) @ sampled_y, -1, -2)\n    if not refit:\n        (eigenvals, eigenvecs) = batch_2x2_ellipse(affinities_fit)\n        bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n        affinities_fit[bad_ones] = torch.eye(2, device=dv)\n    y_pred = (affinities_fit[:, ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals, ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    if len(inl_sampleidx) == 0:\n        refit = False\n    if not refit:\n        return (inl_sampleidx, affinities_fit[inl_iters, torch.arange(inl_iters.shape[0], device=dv)], inl_confidence, inl_counts)\n    (padded_inlier_x, padded_inlier_y) = sample_padded_inliers(xsamples, ysamples, inl_counts, inl_ransidx, inl_sampleidx, numransacs, dv)\n    refit_affinity = padded_inlier_y.transpose(-2, -1) @ padded_inlier_x @ batch_2x2_inv(padded_inlier_x.transpose(-2, -1) @ padded_inlier_x, check_dets=True)\n    (eigenvals, eigenvecs) = batch_2x2_ellipse(refit_affinity)\n    bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n    refit_affinity[bad_ones] = torch.eye(2, device=dv, dtype=refit_affinity.dtype)\n    y_pred = (refit_affinity[ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals.unsqueeze(0), ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    return (inl_sampleidx, refit_affinity, inl_confidence, inl_counts)",
        "mutated": [
            "def ransac(xsamples: Tensor, ysamples: Tensor, rdims: Tensor, config: Dict[str, Any], iters: int=128, refit: bool=True) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n    DET_THR = config['detected_scale_rate_threshold']\n    MIN_CONFIDENCE = config['min_confidence']\n    dv: torch.device = config['device']\n    numransacs = rdims.shape[0]\n    ransidx = torch.arange(numransacs, device=dv).repeat_interleave(rdims)\n    idxoffsets = torch.cat([torch.tensor([0], device=dv), torch.cumsum(rdims[:-1], dim=0)], dim=0)\n    rand_samples_rel = draw_first_k_couples(iters, rdims, dv)\n    rand_samples_abs = rand_samples_rel + idxoffsets\n    sampled_x = torch.transpose(xsamples[rand_samples_abs], dim0=1, dim1=2)\n    sampled_y = torch.transpose(ysamples[rand_samples_abs], dim0=1, dim1=2)\n    affinities_fit = torch.transpose(batch_2x2_inv(sampled_x, check_dets=True) @ sampled_y, -1, -2)\n    if not refit:\n        (eigenvals, eigenvecs) = batch_2x2_ellipse(affinities_fit)\n        bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n        affinities_fit[bad_ones] = torch.eye(2, device=dv)\n    y_pred = (affinities_fit[:, ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals, ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    if len(inl_sampleidx) == 0:\n        refit = False\n    if not refit:\n        return (inl_sampleidx, affinities_fit[inl_iters, torch.arange(inl_iters.shape[0], device=dv)], inl_confidence, inl_counts)\n    (padded_inlier_x, padded_inlier_y) = sample_padded_inliers(xsamples, ysamples, inl_counts, inl_ransidx, inl_sampleidx, numransacs, dv)\n    refit_affinity = padded_inlier_y.transpose(-2, -1) @ padded_inlier_x @ batch_2x2_inv(padded_inlier_x.transpose(-2, -1) @ padded_inlier_x, check_dets=True)\n    (eigenvals, eigenvecs) = batch_2x2_ellipse(refit_affinity)\n    bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n    refit_affinity[bad_ones] = torch.eye(2, device=dv, dtype=refit_affinity.dtype)\n    y_pred = (refit_affinity[ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals.unsqueeze(0), ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    return (inl_sampleidx, refit_affinity, inl_confidence, inl_counts)",
            "def ransac(xsamples: Tensor, ysamples: Tensor, rdims: Tensor, config: Dict[str, Any], iters: int=128, refit: bool=True) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    DET_THR = config['detected_scale_rate_threshold']\n    MIN_CONFIDENCE = config['min_confidence']\n    dv: torch.device = config['device']\n    numransacs = rdims.shape[0]\n    ransidx = torch.arange(numransacs, device=dv).repeat_interleave(rdims)\n    idxoffsets = torch.cat([torch.tensor([0], device=dv), torch.cumsum(rdims[:-1], dim=0)], dim=0)\n    rand_samples_rel = draw_first_k_couples(iters, rdims, dv)\n    rand_samples_abs = rand_samples_rel + idxoffsets\n    sampled_x = torch.transpose(xsamples[rand_samples_abs], dim0=1, dim1=2)\n    sampled_y = torch.transpose(ysamples[rand_samples_abs], dim0=1, dim1=2)\n    affinities_fit = torch.transpose(batch_2x2_inv(sampled_x, check_dets=True) @ sampled_y, -1, -2)\n    if not refit:\n        (eigenvals, eigenvecs) = batch_2x2_ellipse(affinities_fit)\n        bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n        affinities_fit[bad_ones] = torch.eye(2, device=dv)\n    y_pred = (affinities_fit[:, ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals, ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    if len(inl_sampleidx) == 0:\n        refit = False\n    if not refit:\n        return (inl_sampleidx, affinities_fit[inl_iters, torch.arange(inl_iters.shape[0], device=dv)], inl_confidence, inl_counts)\n    (padded_inlier_x, padded_inlier_y) = sample_padded_inliers(xsamples, ysamples, inl_counts, inl_ransidx, inl_sampleidx, numransacs, dv)\n    refit_affinity = padded_inlier_y.transpose(-2, -1) @ padded_inlier_x @ batch_2x2_inv(padded_inlier_x.transpose(-2, -1) @ padded_inlier_x, check_dets=True)\n    (eigenvals, eigenvecs) = batch_2x2_ellipse(refit_affinity)\n    bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n    refit_affinity[bad_ones] = torch.eye(2, device=dv, dtype=refit_affinity.dtype)\n    y_pred = (refit_affinity[ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals.unsqueeze(0), ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    return (inl_sampleidx, refit_affinity, inl_confidence, inl_counts)",
            "def ransac(xsamples: Tensor, ysamples: Tensor, rdims: Tensor, config: Dict[str, Any], iters: int=128, refit: bool=True) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    DET_THR = config['detected_scale_rate_threshold']\n    MIN_CONFIDENCE = config['min_confidence']\n    dv: torch.device = config['device']\n    numransacs = rdims.shape[0]\n    ransidx = torch.arange(numransacs, device=dv).repeat_interleave(rdims)\n    idxoffsets = torch.cat([torch.tensor([0], device=dv), torch.cumsum(rdims[:-1], dim=0)], dim=0)\n    rand_samples_rel = draw_first_k_couples(iters, rdims, dv)\n    rand_samples_abs = rand_samples_rel + idxoffsets\n    sampled_x = torch.transpose(xsamples[rand_samples_abs], dim0=1, dim1=2)\n    sampled_y = torch.transpose(ysamples[rand_samples_abs], dim0=1, dim1=2)\n    affinities_fit = torch.transpose(batch_2x2_inv(sampled_x, check_dets=True) @ sampled_y, -1, -2)\n    if not refit:\n        (eigenvals, eigenvecs) = batch_2x2_ellipse(affinities_fit)\n        bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n        affinities_fit[bad_ones] = torch.eye(2, device=dv)\n    y_pred = (affinities_fit[:, ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals, ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    if len(inl_sampleidx) == 0:\n        refit = False\n    if not refit:\n        return (inl_sampleidx, affinities_fit[inl_iters, torch.arange(inl_iters.shape[0], device=dv)], inl_confidence, inl_counts)\n    (padded_inlier_x, padded_inlier_y) = sample_padded_inliers(xsamples, ysamples, inl_counts, inl_ransidx, inl_sampleidx, numransacs, dv)\n    refit_affinity = padded_inlier_y.transpose(-2, -1) @ padded_inlier_x @ batch_2x2_inv(padded_inlier_x.transpose(-2, -1) @ padded_inlier_x, check_dets=True)\n    (eigenvals, eigenvecs) = batch_2x2_ellipse(refit_affinity)\n    bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n    refit_affinity[bad_ones] = torch.eye(2, device=dv, dtype=refit_affinity.dtype)\n    y_pred = (refit_affinity[ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals.unsqueeze(0), ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    return (inl_sampleidx, refit_affinity, inl_confidence, inl_counts)",
            "def ransac(xsamples: Tensor, ysamples: Tensor, rdims: Tensor, config: Dict[str, Any], iters: int=128, refit: bool=True) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    DET_THR = config['detected_scale_rate_threshold']\n    MIN_CONFIDENCE = config['min_confidence']\n    dv: torch.device = config['device']\n    numransacs = rdims.shape[0]\n    ransidx = torch.arange(numransacs, device=dv).repeat_interleave(rdims)\n    idxoffsets = torch.cat([torch.tensor([0], device=dv), torch.cumsum(rdims[:-1], dim=0)], dim=0)\n    rand_samples_rel = draw_first_k_couples(iters, rdims, dv)\n    rand_samples_abs = rand_samples_rel + idxoffsets\n    sampled_x = torch.transpose(xsamples[rand_samples_abs], dim0=1, dim1=2)\n    sampled_y = torch.transpose(ysamples[rand_samples_abs], dim0=1, dim1=2)\n    affinities_fit = torch.transpose(batch_2x2_inv(sampled_x, check_dets=True) @ sampled_y, -1, -2)\n    if not refit:\n        (eigenvals, eigenvecs) = batch_2x2_ellipse(affinities_fit)\n        bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n        affinities_fit[bad_ones] = torch.eye(2, device=dv)\n    y_pred = (affinities_fit[:, ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals, ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    if len(inl_sampleidx) == 0:\n        refit = False\n    if not refit:\n        return (inl_sampleidx, affinities_fit[inl_iters, torch.arange(inl_iters.shape[0], device=dv)], inl_confidence, inl_counts)\n    (padded_inlier_x, padded_inlier_y) = sample_padded_inliers(xsamples, ysamples, inl_counts, inl_ransidx, inl_sampleidx, numransacs, dv)\n    refit_affinity = padded_inlier_y.transpose(-2, -1) @ padded_inlier_x @ batch_2x2_inv(padded_inlier_x.transpose(-2, -1) @ padded_inlier_x, check_dets=True)\n    (eigenvals, eigenvecs) = batch_2x2_ellipse(refit_affinity)\n    bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n    refit_affinity[bad_ones] = torch.eye(2, device=dv, dtype=refit_affinity.dtype)\n    y_pred = (refit_affinity[ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals.unsqueeze(0), ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    return (inl_sampleidx, refit_affinity, inl_confidence, inl_counts)",
            "def ransac(xsamples: Tensor, ysamples: Tensor, rdims: Tensor, config: Dict[str, Any], iters: int=128, refit: bool=True) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    DET_THR = config['detected_scale_rate_threshold']\n    MIN_CONFIDENCE = config['min_confidence']\n    dv: torch.device = config['device']\n    numransacs = rdims.shape[0]\n    ransidx = torch.arange(numransacs, device=dv).repeat_interleave(rdims)\n    idxoffsets = torch.cat([torch.tensor([0], device=dv), torch.cumsum(rdims[:-1], dim=0)], dim=0)\n    rand_samples_rel = draw_first_k_couples(iters, rdims, dv)\n    rand_samples_abs = rand_samples_rel + idxoffsets\n    sampled_x = torch.transpose(xsamples[rand_samples_abs], dim0=1, dim1=2)\n    sampled_y = torch.transpose(ysamples[rand_samples_abs], dim0=1, dim1=2)\n    affinities_fit = torch.transpose(batch_2x2_inv(sampled_x, check_dets=True) @ sampled_y, -1, -2)\n    if not refit:\n        (eigenvals, eigenvecs) = batch_2x2_ellipse(affinities_fit)\n        bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n        affinities_fit[bad_ones] = torch.eye(2, device=dv)\n    y_pred = (affinities_fit[:, ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals, ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    if len(inl_sampleidx) == 0:\n        refit = False\n    if not refit:\n        return (inl_sampleidx, affinities_fit[inl_iters, torch.arange(inl_iters.shape[0], device=dv)], inl_confidence, inl_counts)\n    (padded_inlier_x, padded_inlier_y) = sample_padded_inliers(xsamples, ysamples, inl_counts, inl_ransidx, inl_sampleidx, numransacs, dv)\n    refit_affinity = padded_inlier_y.transpose(-2, -1) @ padded_inlier_x @ batch_2x2_inv(padded_inlier_x.transpose(-2, -1) @ padded_inlier_x, check_dets=True)\n    (eigenvals, eigenvecs) = batch_2x2_ellipse(refit_affinity)\n    bad_ones = (eigenvals[..., 1] < 1 / DET_THR ** 2) | (eigenvals[..., 0] > DET_THR ** 2)\n    refit_affinity[bad_ones] = torch.eye(2, device=dv, dtype=refit_affinity.dtype)\n    y_pred = (refit_affinity[ransidx] @ xsamples.unsqueeze(-1)).squeeze(-1)\n    residuals = torch.norm(y_pred - ysamples, dim=-1)\n    (inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_confidence) = confidence_based_inlier_selection(residuals.unsqueeze(0), ransidx, rdims, idxoffsets, dv=dv, min_confidence=MIN_CONFIDENCE)\n    return (inl_sampleidx, refit_affinity, inl_confidence, inl_counts)"
        ]
    }
]