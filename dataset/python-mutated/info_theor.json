[
    {
        "func_name": "bayesian_info_criterion",
        "original": "def bayesian_info_criterion(log_likelihood, n_params, n_samples):\n    \"\"\"Computes the Bayesian Information Criterion (BIC) given the log of the\n    likelihood function evaluated at the estimated (or analytically derived)\n    parameters, the number of parameters, and the number of samples.\n\n    The BIC is usually applied to decide whether increasing the number of free\n    parameters (hence, increasing the model complexity) yields significantly\n    better fittings. The decision is in favor of the model with the lowest\n    BIC.\n\n    BIC is given as\n\n    .. math::\n\n        \\\\mathrm{BIC} = k \\\\ln(n) - 2L,\n\n    in which :math:`n` is the sample size, :math:`k` is the number of free\n    parameters, and :math:`L` is the log likelihood function of the model\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\n    which L is maximized).\n\n    When comparing two models define\n    :math:`\\\\Delta \\\\mathrm{BIC} = \\\\mathrm{BIC}_h - \\\\mathrm{BIC}_l`, in which\n    :math:`\\\\mathrm{BIC}_h` is the higher BIC, and :math:`\\\\mathrm{BIC}_l` is\n    the lower BIC. The higher is :math:`\\\\Delta \\\\mathrm{BIC}` the stronger is\n    the evidence against the model with higher BIC.\n\n    The general rule of thumb is:\n\n    :math:`0 < \\\\Delta\\\\mathrm{BIC} \\\\leq 2`: weak evidence that model low is\n    better\n\n    :math:`2 < \\\\Delta\\\\mathrm{BIC} \\\\leq 6`: moderate evidence that model low is\n    better\n\n    :math:`6 < \\\\Delta\\\\mathrm{BIC} \\\\leq 10`: strong evidence that model low is\n    better\n\n    :math:`\\\\Delta\\\\mathrm{BIC} > 10`: very strong evidence that model low is\n    better\n\n    For a detailed explanation, see [1]_ - [5]_.\n\n    Parameters\n    ----------\n    log_likelihood : float\n        Logarithm of the likelihood function of the model evaluated at the\n        point of maxima (with respect to the parameter space).\n    n_params : int\n        Number of free parameters of the model, i.e., dimension of the\n        parameter space.\n    n_samples : int\n        Number of observations.\n\n    Returns\n    -------\n    bic : float\n        Bayesian Information Criterion.\n\n    Examples\n    --------\n    The following example was originally presented in [1]_. Consider a\n    Gaussian model (mu, sigma) and a t-Student model (mu, sigma, delta).\n    In addition, assume that the t model has presented a higher likelihood.\n    The question that the BIC is proposed to answer is: \"Is the increase in\n    likelihood due to larger number of parameters?\"\n\n    >>> from astropy.stats.info_theory import bayesian_info_criterion\n    >>> lnL_g = -176.4\n    >>> lnL_t = -173.0\n    >>> n_params_g = 2\n    >>> n_params_t = 3\n    >>> n_samples = 100\n    >>> bic_g = bayesian_info_criterion(lnL_g, n_params_g, n_samples)\n    >>> bic_t = bayesian_info_criterion(lnL_t, n_params_t, n_samples)\n    >>> bic_g - bic_t # doctest: +FLOAT_CMP\n    2.1948298140119391\n\n    Therefore, there exist a moderate evidence that the increasing in\n    likelihood for t-Student model is due to the larger number of parameters.\n\n    References\n    ----------\n    .. [1] Richards, D. Maximum Likelihood Estimation and the Bayesian\n       Information Criterion.\n       <https://hea-www.harvard.edu/astrostat/Stat310_0910/dr_20100323_mle.pdf>\n    .. [2] Wikipedia. Bayesian Information Criterion.\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\n    .. [3] Origin Lab. Comparing Two Fitting Functions.\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\n    .. [4] Liddle, A. R. Information Criteria for Astrophysical Model\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\n    .. [5] Liddle, A. R. How many cosmological parameters? 2008.\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\n    \"\"\"\n    return n_params * np.log(n_samples) - 2.0 * log_likelihood",
        "mutated": [
            "def bayesian_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n    'Computes the Bayesian Information Criterion (BIC) given the log of the\\n    likelihood function evaluated at the estimated (or analytically derived)\\n    parameters, the number of parameters, and the number of samples.\\n\\n    The BIC is usually applied to decide whether increasing the number of free\\n    parameters (hence, increasing the model complexity) yields significantly\\n    better fittings. The decision is in favor of the model with the lowest\\n    BIC.\\n\\n    BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = k \\\\ln(n) - 2L,\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    When comparing two models define\\n    :math:`\\\\Delta \\\\mathrm{BIC} = \\\\mathrm{BIC}_h - \\\\mathrm{BIC}_l`, in which\\n    :math:`\\\\mathrm{BIC}_h` is the higher BIC, and :math:`\\\\mathrm{BIC}_l` is\\n    the lower BIC. The higher is :math:`\\\\Delta \\\\mathrm{BIC}` the stronger is\\n    the evidence against the model with higher BIC.\\n\\n    The general rule of thumb is:\\n\\n    :math:`0 < \\\\Delta\\\\mathrm{BIC} \\\\leq 2`: weak evidence that model low is\\n    better\\n\\n    :math:`2 < \\\\Delta\\\\mathrm{BIC} \\\\leq 6`: moderate evidence that model low is\\n    better\\n\\n    :math:`6 < \\\\Delta\\\\mathrm{BIC} \\\\leq 10`: strong evidence that model low is\\n    better\\n\\n    :math:`\\\\Delta\\\\mathrm{BIC} > 10`: very strong evidence that model low is\\n    better\\n\\n    For a detailed explanation, see [1]_ - [5]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n        Bayesian Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [1]_. Consider a\\n    Gaussian model (mu, sigma) and a t-Student model (mu, sigma, delta).\\n    In addition, assume that the t model has presented a higher likelihood.\\n    The question that the BIC is proposed to answer is: \"Is the increase in\\n    likelihood due to larger number of parameters?\"\\n\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion\\n    >>> lnL_g = -176.4\\n    >>> lnL_t = -173.0\\n    >>> n_params_g = 2\\n    >>> n_params_t = 3\\n    >>> n_samples = 100\\n    >>> bic_g = bayesian_info_criterion(lnL_g, n_params_g, n_samples)\\n    >>> bic_t = bayesian_info_criterion(lnL_t, n_params_t, n_samples)\\n    >>> bic_g - bic_t # doctest: +FLOAT_CMP\\n    2.1948298140119391\\n\\n    Therefore, there exist a moderate evidence that the increasing in\\n    likelihood for t-Student model is due to the larger number of parameters.\\n\\n    References\\n    ----------\\n    .. [1] Richards, D. Maximum Likelihood Estimation and the Bayesian\\n       Information Criterion.\\n       <https://hea-www.harvard.edu/astrostat/Stat310_0910/dr_20100323_mle.pdf>\\n    .. [2] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [3] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [4] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [5] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    return n_params * np.log(n_samples) - 2.0 * log_likelihood",
            "def bayesian_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes the Bayesian Information Criterion (BIC) given the log of the\\n    likelihood function evaluated at the estimated (or analytically derived)\\n    parameters, the number of parameters, and the number of samples.\\n\\n    The BIC is usually applied to decide whether increasing the number of free\\n    parameters (hence, increasing the model complexity) yields significantly\\n    better fittings. The decision is in favor of the model with the lowest\\n    BIC.\\n\\n    BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = k \\\\ln(n) - 2L,\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    When comparing two models define\\n    :math:`\\\\Delta \\\\mathrm{BIC} = \\\\mathrm{BIC}_h - \\\\mathrm{BIC}_l`, in which\\n    :math:`\\\\mathrm{BIC}_h` is the higher BIC, and :math:`\\\\mathrm{BIC}_l` is\\n    the lower BIC. The higher is :math:`\\\\Delta \\\\mathrm{BIC}` the stronger is\\n    the evidence against the model with higher BIC.\\n\\n    The general rule of thumb is:\\n\\n    :math:`0 < \\\\Delta\\\\mathrm{BIC} \\\\leq 2`: weak evidence that model low is\\n    better\\n\\n    :math:`2 < \\\\Delta\\\\mathrm{BIC} \\\\leq 6`: moderate evidence that model low is\\n    better\\n\\n    :math:`6 < \\\\Delta\\\\mathrm{BIC} \\\\leq 10`: strong evidence that model low is\\n    better\\n\\n    :math:`\\\\Delta\\\\mathrm{BIC} > 10`: very strong evidence that model low is\\n    better\\n\\n    For a detailed explanation, see [1]_ - [5]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n        Bayesian Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [1]_. Consider a\\n    Gaussian model (mu, sigma) and a t-Student model (mu, sigma, delta).\\n    In addition, assume that the t model has presented a higher likelihood.\\n    The question that the BIC is proposed to answer is: \"Is the increase in\\n    likelihood due to larger number of parameters?\"\\n\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion\\n    >>> lnL_g = -176.4\\n    >>> lnL_t = -173.0\\n    >>> n_params_g = 2\\n    >>> n_params_t = 3\\n    >>> n_samples = 100\\n    >>> bic_g = bayesian_info_criterion(lnL_g, n_params_g, n_samples)\\n    >>> bic_t = bayesian_info_criterion(lnL_t, n_params_t, n_samples)\\n    >>> bic_g - bic_t # doctest: +FLOAT_CMP\\n    2.1948298140119391\\n\\n    Therefore, there exist a moderate evidence that the increasing in\\n    likelihood for t-Student model is due to the larger number of parameters.\\n\\n    References\\n    ----------\\n    .. [1] Richards, D. Maximum Likelihood Estimation and the Bayesian\\n       Information Criterion.\\n       <https://hea-www.harvard.edu/astrostat/Stat310_0910/dr_20100323_mle.pdf>\\n    .. [2] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [3] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [4] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [5] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    return n_params * np.log(n_samples) - 2.0 * log_likelihood",
            "def bayesian_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes the Bayesian Information Criterion (BIC) given the log of the\\n    likelihood function evaluated at the estimated (or analytically derived)\\n    parameters, the number of parameters, and the number of samples.\\n\\n    The BIC is usually applied to decide whether increasing the number of free\\n    parameters (hence, increasing the model complexity) yields significantly\\n    better fittings. The decision is in favor of the model with the lowest\\n    BIC.\\n\\n    BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = k \\\\ln(n) - 2L,\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    When comparing two models define\\n    :math:`\\\\Delta \\\\mathrm{BIC} = \\\\mathrm{BIC}_h - \\\\mathrm{BIC}_l`, in which\\n    :math:`\\\\mathrm{BIC}_h` is the higher BIC, and :math:`\\\\mathrm{BIC}_l` is\\n    the lower BIC. The higher is :math:`\\\\Delta \\\\mathrm{BIC}` the stronger is\\n    the evidence against the model with higher BIC.\\n\\n    The general rule of thumb is:\\n\\n    :math:`0 < \\\\Delta\\\\mathrm{BIC} \\\\leq 2`: weak evidence that model low is\\n    better\\n\\n    :math:`2 < \\\\Delta\\\\mathrm{BIC} \\\\leq 6`: moderate evidence that model low is\\n    better\\n\\n    :math:`6 < \\\\Delta\\\\mathrm{BIC} \\\\leq 10`: strong evidence that model low is\\n    better\\n\\n    :math:`\\\\Delta\\\\mathrm{BIC} > 10`: very strong evidence that model low is\\n    better\\n\\n    For a detailed explanation, see [1]_ - [5]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n        Bayesian Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [1]_. Consider a\\n    Gaussian model (mu, sigma) and a t-Student model (mu, sigma, delta).\\n    In addition, assume that the t model has presented a higher likelihood.\\n    The question that the BIC is proposed to answer is: \"Is the increase in\\n    likelihood due to larger number of parameters?\"\\n\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion\\n    >>> lnL_g = -176.4\\n    >>> lnL_t = -173.0\\n    >>> n_params_g = 2\\n    >>> n_params_t = 3\\n    >>> n_samples = 100\\n    >>> bic_g = bayesian_info_criterion(lnL_g, n_params_g, n_samples)\\n    >>> bic_t = bayesian_info_criterion(lnL_t, n_params_t, n_samples)\\n    >>> bic_g - bic_t # doctest: +FLOAT_CMP\\n    2.1948298140119391\\n\\n    Therefore, there exist a moderate evidence that the increasing in\\n    likelihood for t-Student model is due to the larger number of parameters.\\n\\n    References\\n    ----------\\n    .. [1] Richards, D. Maximum Likelihood Estimation and the Bayesian\\n       Information Criterion.\\n       <https://hea-www.harvard.edu/astrostat/Stat310_0910/dr_20100323_mle.pdf>\\n    .. [2] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [3] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [4] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [5] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    return n_params * np.log(n_samples) - 2.0 * log_likelihood",
            "def bayesian_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes the Bayesian Information Criterion (BIC) given the log of the\\n    likelihood function evaluated at the estimated (or analytically derived)\\n    parameters, the number of parameters, and the number of samples.\\n\\n    The BIC is usually applied to decide whether increasing the number of free\\n    parameters (hence, increasing the model complexity) yields significantly\\n    better fittings. The decision is in favor of the model with the lowest\\n    BIC.\\n\\n    BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = k \\\\ln(n) - 2L,\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    When comparing two models define\\n    :math:`\\\\Delta \\\\mathrm{BIC} = \\\\mathrm{BIC}_h - \\\\mathrm{BIC}_l`, in which\\n    :math:`\\\\mathrm{BIC}_h` is the higher BIC, and :math:`\\\\mathrm{BIC}_l` is\\n    the lower BIC. The higher is :math:`\\\\Delta \\\\mathrm{BIC}` the stronger is\\n    the evidence against the model with higher BIC.\\n\\n    The general rule of thumb is:\\n\\n    :math:`0 < \\\\Delta\\\\mathrm{BIC} \\\\leq 2`: weak evidence that model low is\\n    better\\n\\n    :math:`2 < \\\\Delta\\\\mathrm{BIC} \\\\leq 6`: moderate evidence that model low is\\n    better\\n\\n    :math:`6 < \\\\Delta\\\\mathrm{BIC} \\\\leq 10`: strong evidence that model low is\\n    better\\n\\n    :math:`\\\\Delta\\\\mathrm{BIC} > 10`: very strong evidence that model low is\\n    better\\n\\n    For a detailed explanation, see [1]_ - [5]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n        Bayesian Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [1]_. Consider a\\n    Gaussian model (mu, sigma) and a t-Student model (mu, sigma, delta).\\n    In addition, assume that the t model has presented a higher likelihood.\\n    The question that the BIC is proposed to answer is: \"Is the increase in\\n    likelihood due to larger number of parameters?\"\\n\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion\\n    >>> lnL_g = -176.4\\n    >>> lnL_t = -173.0\\n    >>> n_params_g = 2\\n    >>> n_params_t = 3\\n    >>> n_samples = 100\\n    >>> bic_g = bayesian_info_criterion(lnL_g, n_params_g, n_samples)\\n    >>> bic_t = bayesian_info_criterion(lnL_t, n_params_t, n_samples)\\n    >>> bic_g - bic_t # doctest: +FLOAT_CMP\\n    2.1948298140119391\\n\\n    Therefore, there exist a moderate evidence that the increasing in\\n    likelihood for t-Student model is due to the larger number of parameters.\\n\\n    References\\n    ----------\\n    .. [1] Richards, D. Maximum Likelihood Estimation and the Bayesian\\n       Information Criterion.\\n       <https://hea-www.harvard.edu/astrostat/Stat310_0910/dr_20100323_mle.pdf>\\n    .. [2] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [3] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [4] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [5] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    return n_params * np.log(n_samples) - 2.0 * log_likelihood",
            "def bayesian_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes the Bayesian Information Criterion (BIC) given the log of the\\n    likelihood function evaluated at the estimated (or analytically derived)\\n    parameters, the number of parameters, and the number of samples.\\n\\n    The BIC is usually applied to decide whether increasing the number of free\\n    parameters (hence, increasing the model complexity) yields significantly\\n    better fittings. The decision is in favor of the model with the lowest\\n    BIC.\\n\\n    BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = k \\\\ln(n) - 2L,\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    When comparing two models define\\n    :math:`\\\\Delta \\\\mathrm{BIC} = \\\\mathrm{BIC}_h - \\\\mathrm{BIC}_l`, in which\\n    :math:`\\\\mathrm{BIC}_h` is the higher BIC, and :math:`\\\\mathrm{BIC}_l` is\\n    the lower BIC. The higher is :math:`\\\\Delta \\\\mathrm{BIC}` the stronger is\\n    the evidence against the model with higher BIC.\\n\\n    The general rule of thumb is:\\n\\n    :math:`0 < \\\\Delta\\\\mathrm{BIC} \\\\leq 2`: weak evidence that model low is\\n    better\\n\\n    :math:`2 < \\\\Delta\\\\mathrm{BIC} \\\\leq 6`: moderate evidence that model low is\\n    better\\n\\n    :math:`6 < \\\\Delta\\\\mathrm{BIC} \\\\leq 10`: strong evidence that model low is\\n    better\\n\\n    :math:`\\\\Delta\\\\mathrm{BIC} > 10`: very strong evidence that model low is\\n    better\\n\\n    For a detailed explanation, see [1]_ - [5]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n        Bayesian Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [1]_. Consider a\\n    Gaussian model (mu, sigma) and a t-Student model (mu, sigma, delta).\\n    In addition, assume that the t model has presented a higher likelihood.\\n    The question that the BIC is proposed to answer is: \"Is the increase in\\n    likelihood due to larger number of parameters?\"\\n\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion\\n    >>> lnL_g = -176.4\\n    >>> lnL_t = -173.0\\n    >>> n_params_g = 2\\n    >>> n_params_t = 3\\n    >>> n_samples = 100\\n    >>> bic_g = bayesian_info_criterion(lnL_g, n_params_g, n_samples)\\n    >>> bic_t = bayesian_info_criterion(lnL_t, n_params_t, n_samples)\\n    >>> bic_g - bic_t # doctest: +FLOAT_CMP\\n    2.1948298140119391\\n\\n    Therefore, there exist a moderate evidence that the increasing in\\n    likelihood for t-Student model is due to the larger number of parameters.\\n\\n    References\\n    ----------\\n    .. [1] Richards, D. Maximum Likelihood Estimation and the Bayesian\\n       Information Criterion.\\n       <https://hea-www.harvard.edu/astrostat/Stat310_0910/dr_20100323_mle.pdf>\\n    .. [2] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [3] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [4] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [5] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    return n_params * np.log(n_samples) - 2.0 * log_likelihood"
        ]
    },
    {
        "func_name": "bayesian_info_criterion_lsq",
        "original": "def bayesian_info_criterion_lsq(ssr, n_params, n_samples):\n    \"\"\"\n    Computes the Bayesian Information Criterion (BIC) assuming that the\n    observations come from a Gaussian distribution.\n\n    In this case, BIC is given as\n\n    .. math::\n\n        \\\\mathrm{BIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + k\\\\ln(n)\n\n    in which :math:`n` is the sample size, :math:`k` is the number of free\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\n    between model and data.\n\n    This is applicable, for instance, when the parameters of a model are\n    estimated using the least squares statistic. See [1]_ and [2]_.\n\n    Parameters\n    ----------\n    ssr : float\n        Sum of squared residuals (SSR) between model and data.\n    n_params : int\n        Number of free parameters of the model, i.e., dimension of the\n        parameter space.\n    n_samples : int\n        Number of observations.\n\n    Returns\n    -------\n    bic : float\n\n    Examples\n    --------\n    Consider the simple 1-D fitting example presented in the Astropy\n    modeling webpage [3]_. There, two models (Box and Gaussian) were fitted to\n    a source flux using the least squares statistic. However, the fittings\n    themselves do not tell much about which model better represents this\n    hypothetical source. Therefore, we are going to apply to BIC in order to\n    decide in favor of a model.\n\n    >>> import numpy as np\n    >>> from astropy.modeling import models, fitting\n    >>> from astropy.stats.info_theory import bayesian_info_criterion_lsq\n    >>> # Generate fake data\n    >>> np.random.seed(0)\n    >>> x = np.linspace(-5., 5., 200)\n    >>> y = 3 * np.exp(-0.5 * (x - 1.3)**2 / 0.8**2)\n    >>> y += np.random.normal(0., 0.2, x.shape)\n    >>> # Fit the data using a Box model.\n    >>> # Bounds are not really needed but included here to demonstrate usage.\n    >>> t_init = models.Trapezoid1D(amplitude=1., x_0=0., width=1., slope=0.5,\n    ...                             bounds={\"x_0\": (-5., 5.)})\n    >>> fit_t = fitting.LevMarLSQFitter()\n    >>> t = fit_t(t_init, x, y)\n    >>> # Fit the data using a Gaussian\n    >>> g_init = models.Gaussian1D(amplitude=1., mean=0, stddev=1.)\n    >>> fit_g = fitting.LevMarLSQFitter()\n    >>> g = fit_g(g_init, x, y)\n    >>> # Compute the mean squared errors\n    >>> ssr_t = np.sum((t(x) - y)*(t(x) - y))\n    >>> ssr_g = np.sum((g(x) - y)*(g(x) - y))\n    >>> # Compute the bics\n    >>> bic_t = bayesian_info_criterion_lsq(ssr_t, 4, x.shape[0])\n    >>> bic_g = bayesian_info_criterion_lsq(ssr_g, 3, x.shape[0])\n    >>> bic_t - bic_g  # doctest: +SKIP\n    30.644474706065466\n\n    Hence, there is a very strong evidence that the Gaussian model has a\n    significantly better representation of the data than the Box model. This\n    is, obviously, expected since the true model is Gaussian.\n\n    References\n    ----------\n    .. [1] Wikipedia. Bayesian Information Criterion.\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\n    .. [3] Astropy Models and Fitting\n        <https://docs.astropy.org/en/stable/modeling>\n    \"\"\"\n    return bayesian_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
        "mutated": [
            "def bayesian_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n    '\\n    Computes the Bayesian Information Criterion (BIC) assuming that the\\n    observations come from a Gaussian distribution.\\n\\n    In this case, BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + k\\\\ln(n)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic. See [1]_ and [2]_.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n\\n    Examples\\n    --------\\n    Consider the simple 1-D fitting example presented in the Astropy\\n    modeling webpage [3]_. There, two models (Box and Gaussian) were fitted to\\n    a source flux using the least squares statistic. However, the fittings\\n    themselves do not tell much about which model better represents this\\n    hypothetical source. Therefore, we are going to apply to BIC in order to\\n    decide in favor of a model.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion_lsq\\n    >>> # Generate fake data\\n    >>> np.random.seed(0)\\n    >>> x = np.linspace(-5., 5., 200)\\n    >>> y = 3 * np.exp(-0.5 * (x - 1.3)**2 / 0.8**2)\\n    >>> y += np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit the data using a Box model.\\n    >>> # Bounds are not really needed but included here to demonstrate usage.\\n    >>> t_init = models.Trapezoid1D(amplitude=1., x_0=0., width=1., slope=0.5,\\n    ...                             bounds={\"x_0\": (-5., 5.)})\\n    >>> fit_t = fitting.LevMarLSQFitter()\\n    >>> t = fit_t(t_init, x, y)\\n    >>> # Fit the data using a Gaussian\\n    >>> g_init = models.Gaussian1D(amplitude=1., mean=0, stddev=1.)\\n    >>> fit_g = fitting.LevMarLSQFitter()\\n    >>> g = fit_g(g_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_t = np.sum((t(x) - y)*(t(x) - y))\\n    >>> ssr_g = np.sum((g(x) - y)*(g(x) - y))\\n    >>> # Compute the bics\\n    >>> bic_t = bayesian_info_criterion_lsq(ssr_t, 4, x.shape[0])\\n    >>> bic_g = bayesian_info_criterion_lsq(ssr_g, 3, x.shape[0])\\n    >>> bic_t - bic_g  # doctest: +SKIP\\n    30.644474706065466\\n\\n    Hence, there is a very strong evidence that the Gaussian model has a\\n    significantly better representation of the data than the Box model. This\\n    is, obviously, expected since the true model is Gaussian.\\n\\n    References\\n    ----------\\n    .. [1] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [3] Astropy Models and Fitting\\n        <https://docs.astropy.org/en/stable/modeling>\\n    '\n    return bayesian_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
            "def bayesian_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Computes the Bayesian Information Criterion (BIC) assuming that the\\n    observations come from a Gaussian distribution.\\n\\n    In this case, BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + k\\\\ln(n)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic. See [1]_ and [2]_.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n\\n    Examples\\n    --------\\n    Consider the simple 1-D fitting example presented in the Astropy\\n    modeling webpage [3]_. There, two models (Box and Gaussian) were fitted to\\n    a source flux using the least squares statistic. However, the fittings\\n    themselves do not tell much about which model better represents this\\n    hypothetical source. Therefore, we are going to apply to BIC in order to\\n    decide in favor of a model.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion_lsq\\n    >>> # Generate fake data\\n    >>> np.random.seed(0)\\n    >>> x = np.linspace(-5., 5., 200)\\n    >>> y = 3 * np.exp(-0.5 * (x - 1.3)**2 / 0.8**2)\\n    >>> y += np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit the data using a Box model.\\n    >>> # Bounds are not really needed but included here to demonstrate usage.\\n    >>> t_init = models.Trapezoid1D(amplitude=1., x_0=0., width=1., slope=0.5,\\n    ...                             bounds={\"x_0\": (-5., 5.)})\\n    >>> fit_t = fitting.LevMarLSQFitter()\\n    >>> t = fit_t(t_init, x, y)\\n    >>> # Fit the data using a Gaussian\\n    >>> g_init = models.Gaussian1D(amplitude=1., mean=0, stddev=1.)\\n    >>> fit_g = fitting.LevMarLSQFitter()\\n    >>> g = fit_g(g_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_t = np.sum((t(x) - y)*(t(x) - y))\\n    >>> ssr_g = np.sum((g(x) - y)*(g(x) - y))\\n    >>> # Compute the bics\\n    >>> bic_t = bayesian_info_criterion_lsq(ssr_t, 4, x.shape[0])\\n    >>> bic_g = bayesian_info_criterion_lsq(ssr_g, 3, x.shape[0])\\n    >>> bic_t - bic_g  # doctest: +SKIP\\n    30.644474706065466\\n\\n    Hence, there is a very strong evidence that the Gaussian model has a\\n    significantly better representation of the data than the Box model. This\\n    is, obviously, expected since the true model is Gaussian.\\n\\n    References\\n    ----------\\n    .. [1] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [3] Astropy Models and Fitting\\n        <https://docs.astropy.org/en/stable/modeling>\\n    '\n    return bayesian_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
            "def bayesian_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Computes the Bayesian Information Criterion (BIC) assuming that the\\n    observations come from a Gaussian distribution.\\n\\n    In this case, BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + k\\\\ln(n)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic. See [1]_ and [2]_.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n\\n    Examples\\n    --------\\n    Consider the simple 1-D fitting example presented in the Astropy\\n    modeling webpage [3]_. There, two models (Box and Gaussian) were fitted to\\n    a source flux using the least squares statistic. However, the fittings\\n    themselves do not tell much about which model better represents this\\n    hypothetical source. Therefore, we are going to apply to BIC in order to\\n    decide in favor of a model.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion_lsq\\n    >>> # Generate fake data\\n    >>> np.random.seed(0)\\n    >>> x = np.linspace(-5., 5., 200)\\n    >>> y = 3 * np.exp(-0.5 * (x - 1.3)**2 / 0.8**2)\\n    >>> y += np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit the data using a Box model.\\n    >>> # Bounds are not really needed but included here to demonstrate usage.\\n    >>> t_init = models.Trapezoid1D(amplitude=1., x_0=0., width=1., slope=0.5,\\n    ...                             bounds={\"x_0\": (-5., 5.)})\\n    >>> fit_t = fitting.LevMarLSQFitter()\\n    >>> t = fit_t(t_init, x, y)\\n    >>> # Fit the data using a Gaussian\\n    >>> g_init = models.Gaussian1D(amplitude=1., mean=0, stddev=1.)\\n    >>> fit_g = fitting.LevMarLSQFitter()\\n    >>> g = fit_g(g_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_t = np.sum((t(x) - y)*(t(x) - y))\\n    >>> ssr_g = np.sum((g(x) - y)*(g(x) - y))\\n    >>> # Compute the bics\\n    >>> bic_t = bayesian_info_criterion_lsq(ssr_t, 4, x.shape[0])\\n    >>> bic_g = bayesian_info_criterion_lsq(ssr_g, 3, x.shape[0])\\n    >>> bic_t - bic_g  # doctest: +SKIP\\n    30.644474706065466\\n\\n    Hence, there is a very strong evidence that the Gaussian model has a\\n    significantly better representation of the data than the Box model. This\\n    is, obviously, expected since the true model is Gaussian.\\n\\n    References\\n    ----------\\n    .. [1] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [3] Astropy Models and Fitting\\n        <https://docs.astropy.org/en/stable/modeling>\\n    '\n    return bayesian_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
            "def bayesian_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Computes the Bayesian Information Criterion (BIC) assuming that the\\n    observations come from a Gaussian distribution.\\n\\n    In this case, BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + k\\\\ln(n)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic. See [1]_ and [2]_.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n\\n    Examples\\n    --------\\n    Consider the simple 1-D fitting example presented in the Astropy\\n    modeling webpage [3]_. There, two models (Box and Gaussian) were fitted to\\n    a source flux using the least squares statistic. However, the fittings\\n    themselves do not tell much about which model better represents this\\n    hypothetical source. Therefore, we are going to apply to BIC in order to\\n    decide in favor of a model.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion_lsq\\n    >>> # Generate fake data\\n    >>> np.random.seed(0)\\n    >>> x = np.linspace(-5., 5., 200)\\n    >>> y = 3 * np.exp(-0.5 * (x - 1.3)**2 / 0.8**2)\\n    >>> y += np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit the data using a Box model.\\n    >>> # Bounds are not really needed but included here to demonstrate usage.\\n    >>> t_init = models.Trapezoid1D(amplitude=1., x_0=0., width=1., slope=0.5,\\n    ...                             bounds={\"x_0\": (-5., 5.)})\\n    >>> fit_t = fitting.LevMarLSQFitter()\\n    >>> t = fit_t(t_init, x, y)\\n    >>> # Fit the data using a Gaussian\\n    >>> g_init = models.Gaussian1D(amplitude=1., mean=0, stddev=1.)\\n    >>> fit_g = fitting.LevMarLSQFitter()\\n    >>> g = fit_g(g_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_t = np.sum((t(x) - y)*(t(x) - y))\\n    >>> ssr_g = np.sum((g(x) - y)*(g(x) - y))\\n    >>> # Compute the bics\\n    >>> bic_t = bayesian_info_criterion_lsq(ssr_t, 4, x.shape[0])\\n    >>> bic_g = bayesian_info_criterion_lsq(ssr_g, 3, x.shape[0])\\n    >>> bic_t - bic_g  # doctest: +SKIP\\n    30.644474706065466\\n\\n    Hence, there is a very strong evidence that the Gaussian model has a\\n    significantly better representation of the data than the Box model. This\\n    is, obviously, expected since the true model is Gaussian.\\n\\n    References\\n    ----------\\n    .. [1] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [3] Astropy Models and Fitting\\n        <https://docs.astropy.org/en/stable/modeling>\\n    '\n    return bayesian_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
            "def bayesian_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Computes the Bayesian Information Criterion (BIC) assuming that the\\n    observations come from a Gaussian distribution.\\n\\n    In this case, BIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{BIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + k\\\\ln(n)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic. See [1]_ and [2]_.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    bic : float\\n\\n    Examples\\n    --------\\n    Consider the simple 1-D fitting example presented in the Astropy\\n    modeling webpage [3]_. There, two models (Box and Gaussian) were fitted to\\n    a source flux using the least squares statistic. However, the fittings\\n    themselves do not tell much about which model better represents this\\n    hypothetical source. Therefore, we are going to apply to BIC in order to\\n    decide in favor of a model.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import bayesian_info_criterion_lsq\\n    >>> # Generate fake data\\n    >>> np.random.seed(0)\\n    >>> x = np.linspace(-5., 5., 200)\\n    >>> y = 3 * np.exp(-0.5 * (x - 1.3)**2 / 0.8**2)\\n    >>> y += np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit the data using a Box model.\\n    >>> # Bounds are not really needed but included here to demonstrate usage.\\n    >>> t_init = models.Trapezoid1D(amplitude=1., x_0=0., width=1., slope=0.5,\\n    ...                             bounds={\"x_0\": (-5., 5.)})\\n    >>> fit_t = fitting.LevMarLSQFitter()\\n    >>> t = fit_t(t_init, x, y)\\n    >>> # Fit the data using a Gaussian\\n    >>> g_init = models.Gaussian1D(amplitude=1., mean=0, stddev=1.)\\n    >>> fit_g = fitting.LevMarLSQFitter()\\n    >>> g = fit_g(g_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_t = np.sum((t(x) - y)*(t(x) - y))\\n    >>> ssr_g = np.sum((g(x) - y)*(g(x) - y))\\n    >>> # Compute the bics\\n    >>> bic_t = bayesian_info_criterion_lsq(ssr_t, 4, x.shape[0])\\n    >>> bic_g = bayesian_info_criterion_lsq(ssr_g, 3, x.shape[0])\\n    >>> bic_t - bic_g  # doctest: +SKIP\\n    30.644474706065466\\n\\n    Hence, there is a very strong evidence that the Gaussian model has a\\n    significantly better representation of the data than the Box model. This\\n    is, obviously, expected since the true model is Gaussian.\\n\\n    References\\n    ----------\\n    .. [1] Wikipedia. Bayesian Information Criterion.\\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [3] Astropy Models and Fitting\\n        <https://docs.astropy.org/en/stable/modeling>\\n    '\n    return bayesian_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)"
        ]
    },
    {
        "func_name": "akaike_info_criterion",
        "original": "def akaike_info_criterion(log_likelihood, n_params, n_samples):\n    \"\"\"\n    Computes the Akaike Information Criterion (AIC).\n\n    Like the Bayesian Information Criterion, the AIC is a measure of\n    relative fitting quality which is used for fitting evaluation and model\n    selection. The decision is in favor of the model with the lowest AIC.\n\n    AIC is given as\n\n    .. math::\n\n        \\\\mathrm{AIC} = 2(k - L)\n\n    in which :math:`n` is the sample size, :math:`k` is the number of free\n    parameters, and :math:`L` is the log likelihood function of the model\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\n    which L is maximized).\n\n    In case that the sample size is not \"large enough\" a correction is\n    applied, i.e.\n\n    .. math::\n\n        \\\\mathrm{AIC} = 2(k - L) + \\\\dfrac{2k(k+1)}{n - k - 1}\n\n    Rule of thumb [1]_:\n\n    :math:`\\\\Delta\\\\mathrm{AIC}_i = \\\\mathrm{AIC}_i - \\\\mathrm{AIC}_{min}`\n\n    :math:`\\\\Delta\\\\mathrm{AIC}_i < 2`: substantial support for model i\n\n    :math:`3 < \\\\Delta\\\\mathrm{AIC}_i < 7`: considerably less support for model i\n\n    :math:`\\\\Delta\\\\mathrm{AIC}_i > 10`: essentially none support for model i\n\n    in which :math:`\\\\mathrm{AIC}_{min}` stands for the lower AIC among the\n    models which are being compared.\n\n    For detailed explanations see [1]_-[6]_.\n\n    Parameters\n    ----------\n    log_likelihood : float\n        Logarithm of the likelihood function of the model evaluated at the\n        point of maxima (with respect to the parameter space).\n    n_params : int\n        Number of free parameters of the model, i.e., dimension of the\n        parameter space.\n    n_samples : int\n        Number of observations.\n\n    Returns\n    -------\n    aic : float\n        Akaike Information Criterion.\n\n    Examples\n    --------\n    The following example was originally presented in [2]_. Basically, two\n    models are being compared. One with six parameters (model 1) and another\n    with five parameters (model 2). Despite of the fact that model 2 has a\n    lower AIC, we could decide in favor of model 1 since the difference (in\n    AIC)  between them is only about 1.0.\n\n    >>> n_samples = 121\n    >>> lnL1 = -3.54\n    >>> n1_params = 6\n    >>> lnL2 = -4.17\n    >>> n2_params = 5\n    >>> aic1 = akaike_info_criterion(lnL1, n1_params, n_samples)\n    >>> aic2 = akaike_info_criterion(lnL2, n2_params, n_samples)\n    >>> aic1 - aic2 # doctest: +FLOAT_CMP\n    0.9551029748283746\n\n    Therefore, we can strongly support the model 1 with the advantage that\n    it has more free parameters.\n\n    References\n    ----------\n    .. [1] Cavanaugh, J. E.  Model Selection Lecture II: The Akaike\n       Information Criterion.\n       <http://machinelearning102.pbworks.com/w/file/fetch/47699383/ms_lec_2_ho.pdf>\n    .. [2] Mazerolle, M. J. Making sense out of Akaike's Information\n       Criterion (AIC): its use and interpretation in model selection and\n       inference from ecological data.\n    .. [3] Wikipedia. Akaike Information Criterion.\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\n    .. [4] Origin Lab. Comparing Two Fitting Functions.\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\n    .. [5] Liddle, A. R. Information Criteria for Astrophysical Model\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\n    .. [6] Liddle, A. R. How many cosmological parameters? 2008.\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\n    \"\"\"\n    if n_samples / float(n_params) >= 40.0:\n        aic = 2.0 * (n_params - log_likelihood)\n    else:\n        aic = 2.0 * (n_params - log_likelihood) + 2.0 * n_params * (n_params + 1.0) / (n_samples - n_params - 1.0)\n    return aic",
        "mutated": [
            "def akaike_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n    '\\n    Computes the Akaike Information Criterion (AIC).\\n\\n    Like the Bayesian Information Criterion, the AIC is a measure of\\n    relative fitting quality which is used for fitting evaluation and model\\n    selection. The decision is in favor of the model with the lowest AIC.\\n\\n    AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    In case that the sample size is not \"large enough\" a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L) + \\\\dfrac{2k(k+1)}{n - k - 1}\\n\\n    Rule of thumb [1]_:\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i = \\\\mathrm{AIC}_i - \\\\mathrm{AIC}_{min}`\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i < 2`: substantial support for model i\\n\\n    :math:`3 < \\\\Delta\\\\mathrm{AIC}_i < 7`: considerably less support for model i\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i > 10`: essentially none support for model i\\n\\n    in which :math:`\\\\mathrm{AIC}_{min}` stands for the lower AIC among the\\n    models which are being compared.\\n\\n    For detailed explanations see [1]_-[6]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [2]_. Basically, two\\n    models are being compared. One with six parameters (model 1) and another\\n    with five parameters (model 2). Despite of the fact that model 2 has a\\n    lower AIC, we could decide in favor of model 1 since the difference (in\\n    AIC)  between them is only about 1.0.\\n\\n    >>> n_samples = 121\\n    >>> lnL1 = -3.54\\n    >>> n1_params = 6\\n    >>> lnL2 = -4.17\\n    >>> n2_params = 5\\n    >>> aic1 = akaike_info_criterion(lnL1, n1_params, n_samples)\\n    >>> aic2 = akaike_info_criterion(lnL2, n2_params, n_samples)\\n    >>> aic1 - aic2 # doctest: +FLOAT_CMP\\n    0.9551029748283746\\n\\n    Therefore, we can strongly support the model 1 with the advantage that\\n    it has more free parameters.\\n\\n    References\\n    ----------\\n    .. [1] Cavanaugh, J. E.  Model Selection Lecture II: The Akaike\\n       Information Criterion.\\n       <http://machinelearning102.pbworks.com/w/file/fetch/47699383/ms_lec_2_ho.pdf>\\n    .. [2] Mazerolle, M. J. Making sense out of Akaike\\'s Information\\n       Criterion (AIC): its use and interpretation in model selection and\\n       inference from ecological data.\\n    .. [3] Wikipedia. Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [4] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [5] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [6] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    if n_samples / float(n_params) >= 40.0:\n        aic = 2.0 * (n_params - log_likelihood)\n    else:\n        aic = 2.0 * (n_params - log_likelihood) + 2.0 * n_params * (n_params + 1.0) / (n_samples - n_params - 1.0)\n    return aic",
            "def akaike_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Computes the Akaike Information Criterion (AIC).\\n\\n    Like the Bayesian Information Criterion, the AIC is a measure of\\n    relative fitting quality which is used for fitting evaluation and model\\n    selection. The decision is in favor of the model with the lowest AIC.\\n\\n    AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    In case that the sample size is not \"large enough\" a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L) + \\\\dfrac{2k(k+1)}{n - k - 1}\\n\\n    Rule of thumb [1]_:\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i = \\\\mathrm{AIC}_i - \\\\mathrm{AIC}_{min}`\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i < 2`: substantial support for model i\\n\\n    :math:`3 < \\\\Delta\\\\mathrm{AIC}_i < 7`: considerably less support for model i\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i > 10`: essentially none support for model i\\n\\n    in which :math:`\\\\mathrm{AIC}_{min}` stands for the lower AIC among the\\n    models which are being compared.\\n\\n    For detailed explanations see [1]_-[6]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [2]_. Basically, two\\n    models are being compared. One with six parameters (model 1) and another\\n    with five parameters (model 2). Despite of the fact that model 2 has a\\n    lower AIC, we could decide in favor of model 1 since the difference (in\\n    AIC)  between them is only about 1.0.\\n\\n    >>> n_samples = 121\\n    >>> lnL1 = -3.54\\n    >>> n1_params = 6\\n    >>> lnL2 = -4.17\\n    >>> n2_params = 5\\n    >>> aic1 = akaike_info_criterion(lnL1, n1_params, n_samples)\\n    >>> aic2 = akaike_info_criterion(lnL2, n2_params, n_samples)\\n    >>> aic1 - aic2 # doctest: +FLOAT_CMP\\n    0.9551029748283746\\n\\n    Therefore, we can strongly support the model 1 with the advantage that\\n    it has more free parameters.\\n\\n    References\\n    ----------\\n    .. [1] Cavanaugh, J. E.  Model Selection Lecture II: The Akaike\\n       Information Criterion.\\n       <http://machinelearning102.pbworks.com/w/file/fetch/47699383/ms_lec_2_ho.pdf>\\n    .. [2] Mazerolle, M. J. Making sense out of Akaike\\'s Information\\n       Criterion (AIC): its use and interpretation in model selection and\\n       inference from ecological data.\\n    .. [3] Wikipedia. Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [4] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [5] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [6] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    if n_samples / float(n_params) >= 40.0:\n        aic = 2.0 * (n_params - log_likelihood)\n    else:\n        aic = 2.0 * (n_params - log_likelihood) + 2.0 * n_params * (n_params + 1.0) / (n_samples - n_params - 1.0)\n    return aic",
            "def akaike_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Computes the Akaike Information Criterion (AIC).\\n\\n    Like the Bayesian Information Criterion, the AIC is a measure of\\n    relative fitting quality which is used for fitting evaluation and model\\n    selection. The decision is in favor of the model with the lowest AIC.\\n\\n    AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    In case that the sample size is not \"large enough\" a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L) + \\\\dfrac{2k(k+1)}{n - k - 1}\\n\\n    Rule of thumb [1]_:\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i = \\\\mathrm{AIC}_i - \\\\mathrm{AIC}_{min}`\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i < 2`: substantial support for model i\\n\\n    :math:`3 < \\\\Delta\\\\mathrm{AIC}_i < 7`: considerably less support for model i\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i > 10`: essentially none support for model i\\n\\n    in which :math:`\\\\mathrm{AIC}_{min}` stands for the lower AIC among the\\n    models which are being compared.\\n\\n    For detailed explanations see [1]_-[6]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [2]_. Basically, two\\n    models are being compared. One with six parameters (model 1) and another\\n    with five parameters (model 2). Despite of the fact that model 2 has a\\n    lower AIC, we could decide in favor of model 1 since the difference (in\\n    AIC)  between them is only about 1.0.\\n\\n    >>> n_samples = 121\\n    >>> lnL1 = -3.54\\n    >>> n1_params = 6\\n    >>> lnL2 = -4.17\\n    >>> n2_params = 5\\n    >>> aic1 = akaike_info_criterion(lnL1, n1_params, n_samples)\\n    >>> aic2 = akaike_info_criterion(lnL2, n2_params, n_samples)\\n    >>> aic1 - aic2 # doctest: +FLOAT_CMP\\n    0.9551029748283746\\n\\n    Therefore, we can strongly support the model 1 with the advantage that\\n    it has more free parameters.\\n\\n    References\\n    ----------\\n    .. [1] Cavanaugh, J. E.  Model Selection Lecture II: The Akaike\\n       Information Criterion.\\n       <http://machinelearning102.pbworks.com/w/file/fetch/47699383/ms_lec_2_ho.pdf>\\n    .. [2] Mazerolle, M. J. Making sense out of Akaike\\'s Information\\n       Criterion (AIC): its use and interpretation in model selection and\\n       inference from ecological data.\\n    .. [3] Wikipedia. Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [4] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [5] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [6] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    if n_samples / float(n_params) >= 40.0:\n        aic = 2.0 * (n_params - log_likelihood)\n    else:\n        aic = 2.0 * (n_params - log_likelihood) + 2.0 * n_params * (n_params + 1.0) / (n_samples - n_params - 1.0)\n    return aic",
            "def akaike_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Computes the Akaike Information Criterion (AIC).\\n\\n    Like the Bayesian Information Criterion, the AIC is a measure of\\n    relative fitting quality which is used for fitting evaluation and model\\n    selection. The decision is in favor of the model with the lowest AIC.\\n\\n    AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    In case that the sample size is not \"large enough\" a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L) + \\\\dfrac{2k(k+1)}{n - k - 1}\\n\\n    Rule of thumb [1]_:\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i = \\\\mathrm{AIC}_i - \\\\mathrm{AIC}_{min}`\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i < 2`: substantial support for model i\\n\\n    :math:`3 < \\\\Delta\\\\mathrm{AIC}_i < 7`: considerably less support for model i\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i > 10`: essentially none support for model i\\n\\n    in which :math:`\\\\mathrm{AIC}_{min}` stands for the lower AIC among the\\n    models which are being compared.\\n\\n    For detailed explanations see [1]_-[6]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [2]_. Basically, two\\n    models are being compared. One with six parameters (model 1) and another\\n    with five parameters (model 2). Despite of the fact that model 2 has a\\n    lower AIC, we could decide in favor of model 1 since the difference (in\\n    AIC)  between them is only about 1.0.\\n\\n    >>> n_samples = 121\\n    >>> lnL1 = -3.54\\n    >>> n1_params = 6\\n    >>> lnL2 = -4.17\\n    >>> n2_params = 5\\n    >>> aic1 = akaike_info_criterion(lnL1, n1_params, n_samples)\\n    >>> aic2 = akaike_info_criterion(lnL2, n2_params, n_samples)\\n    >>> aic1 - aic2 # doctest: +FLOAT_CMP\\n    0.9551029748283746\\n\\n    Therefore, we can strongly support the model 1 with the advantage that\\n    it has more free parameters.\\n\\n    References\\n    ----------\\n    .. [1] Cavanaugh, J. E.  Model Selection Lecture II: The Akaike\\n       Information Criterion.\\n       <http://machinelearning102.pbworks.com/w/file/fetch/47699383/ms_lec_2_ho.pdf>\\n    .. [2] Mazerolle, M. J. Making sense out of Akaike\\'s Information\\n       Criterion (AIC): its use and interpretation in model selection and\\n       inference from ecological data.\\n    .. [3] Wikipedia. Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [4] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [5] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [6] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    if n_samples / float(n_params) >= 40.0:\n        aic = 2.0 * (n_params - log_likelihood)\n    else:\n        aic = 2.0 * (n_params - log_likelihood) + 2.0 * n_params * (n_params + 1.0) / (n_samples - n_params - 1.0)\n    return aic",
            "def akaike_info_criterion(log_likelihood, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Computes the Akaike Information Criterion (AIC).\\n\\n    Like the Bayesian Information Criterion, the AIC is a measure of\\n    relative fitting quality which is used for fitting evaluation and model\\n    selection. The decision is in favor of the model with the lowest AIC.\\n\\n    AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L)\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters, and :math:`L` is the log likelihood function of the model\\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\\n    which L is maximized).\\n\\n    In case that the sample size is not \"large enough\" a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = 2(k - L) + \\\\dfrac{2k(k+1)}{n - k - 1}\\n\\n    Rule of thumb [1]_:\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i = \\\\mathrm{AIC}_i - \\\\mathrm{AIC}_{min}`\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i < 2`: substantial support for model i\\n\\n    :math:`3 < \\\\Delta\\\\mathrm{AIC}_i < 7`: considerably less support for model i\\n\\n    :math:`\\\\Delta\\\\mathrm{AIC}_i > 10`: essentially none support for model i\\n\\n    in which :math:`\\\\mathrm{AIC}_{min}` stands for the lower AIC among the\\n    models which are being compared.\\n\\n    For detailed explanations see [1]_-[6]_.\\n\\n    Parameters\\n    ----------\\n    log_likelihood : float\\n        Logarithm of the likelihood function of the model evaluated at the\\n        point of maxima (with respect to the parameter space).\\n    n_params : int\\n        Number of free parameters of the model, i.e., dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    The following example was originally presented in [2]_. Basically, two\\n    models are being compared. One with six parameters (model 1) and another\\n    with five parameters (model 2). Despite of the fact that model 2 has a\\n    lower AIC, we could decide in favor of model 1 since the difference (in\\n    AIC)  between them is only about 1.0.\\n\\n    >>> n_samples = 121\\n    >>> lnL1 = -3.54\\n    >>> n1_params = 6\\n    >>> lnL2 = -4.17\\n    >>> n2_params = 5\\n    >>> aic1 = akaike_info_criterion(lnL1, n1_params, n_samples)\\n    >>> aic2 = akaike_info_criterion(lnL2, n2_params, n_samples)\\n    >>> aic1 - aic2 # doctest: +FLOAT_CMP\\n    0.9551029748283746\\n\\n    Therefore, we can strongly support the model 1 with the advantage that\\n    it has more free parameters.\\n\\n    References\\n    ----------\\n    .. [1] Cavanaugh, J. E.  Model Selection Lecture II: The Akaike\\n       Information Criterion.\\n       <http://machinelearning102.pbworks.com/w/file/fetch/47699383/ms_lec_2_ho.pdf>\\n    .. [2] Mazerolle, M. J. Making sense out of Akaike\\'s Information\\n       Criterion (AIC): its use and interpretation in model selection and\\n       inference from ecological data.\\n    .. [3] Wikipedia. Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [4] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    .. [5] Liddle, A. R. Information Criteria for Astrophysical Model\\n       Selection. 2008. <https://arxiv.org/pdf/astro-ph/0701113v2.pdf>\\n    .. [6] Liddle, A. R. How many cosmological parameters? 2008.\\n       <https://arxiv.org/pdf/astro-ph/0401198v3.pdf>\\n    '\n    if n_samples / float(n_params) >= 40.0:\n        aic = 2.0 * (n_params - log_likelihood)\n    else:\n        aic = 2.0 * (n_params - log_likelihood) + 2.0 * n_params * (n_params + 1.0) / (n_samples - n_params - 1.0)\n    return aic"
        ]
    },
    {
        "func_name": "akaike_info_criterion_lsq",
        "original": "def akaike_info_criterion_lsq(ssr, n_params, n_samples):\n    \"\"\"\n    Computes the Akaike Information Criterion assuming that the observations\n    are Gaussian distributed.\n\n    In this case, AIC is given as\n\n    .. math::\n\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k\n\n    In case that the sample size is not \"large enough\", a correction is\n    applied, i.e.\n\n    .. math::\n\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k +\n                       \\\\dfrac{2k(k+1)}{n-k-1}\n\n\n    in which :math:`n` is the sample size, :math:`k` is the number of free\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\n    between model and data.\n\n    This is applicable, for instance, when the parameters of a model are\n    estimated using the least squares statistic.\n\n    Parameters\n    ----------\n    ssr : float\n        Sum of squared residuals (SSR) between model and data.\n    n_params : int\n        Number of free parameters of the model, i.e.,  the dimension of the\n        parameter space.\n    n_samples : int\n        Number of observations.\n\n    Returns\n    -------\n    aic : float\n        Akaike Information Criterion.\n\n    Examples\n    --------\n    This example is based on Astropy Modeling webpage, Compound models\n    section.\n\n    >>> import numpy as np\n    >>> from astropy.modeling import models, fitting\n    >>> from astropy.stats.info_theory import akaike_info_criterion_lsq\n    >>> np.random.seed(42)\n    >>> # Generate fake data\n    >>> g1 = models.Gaussian1D(.1, 0, 0.2) # changed this to noise level\n    >>> g2 = models.Gaussian1D(.1, 0.3, 0.2) # and added another Gaussian\n    >>> g3 = models.Gaussian1D(2.5, 0.5, 0.1)\n    >>> x = np.linspace(-1, 1, 200)\n    >>> y = g1(x) + g2(x) + g3(x) + np.random.normal(0., 0.2, x.shape)\n    >>> # Fit with three Gaussians\n    >>> g3_init = (models.Gaussian1D(.1, 0, 0.1)\n    ...            + models.Gaussian1D(.1, 0.2, 0.15)\n    ...            + models.Gaussian1D(2.4, .4, 0.1))\n    >>> fitter = fitting.LevMarLSQFitter()\n    >>> g3_fit = fitter(g3_init, x, y)\n    >>> # Fit with two Gaussians\n    >>> g2_init = (models.Gaussian1D(.1, 0, 0.1) +\n    ...            models.Gaussian1D(2, 0.5, 0.1))\n    >>> g2_fit = fitter(g2_init, x, y)\n    >>> # Fit with only one Gaussian\n    >>> g1_init = models.Gaussian1D(amplitude=2., mean=0.3, stddev=.5)\n    >>> g1_fit = fitter(g1_init, x, y)\n    >>> # Compute the mean squared errors\n    >>> ssr_g3 = np.sum((g3_fit(x) - y)**2.0)\n    >>> ssr_g2 = np.sum((g2_fit(x) - y)**2.0)\n    >>> ssr_g1 = np.sum((g1_fit(x) - y)**2.0)\n    >>> akaike_info_criterion_lsq(ssr_g3, 9, x.shape[0]) # doctest: +FLOAT_CMP\n    -634.5257517810961\n    >>> akaike_info_criterion_lsq(ssr_g2, 6, x.shape[0]) # doctest: +FLOAT_CMP\n    -662.83834510232043\n    >>> akaike_info_criterion_lsq(ssr_g1, 3, x.shape[0]) # doctest: +FLOAT_CMP\n    -647.47312032659499\n\n    Hence, from the AIC values, we would prefer to choose the model g2_fit.\n    However, we can considerably support the model g3_fit, since the\n    difference in AIC is about 2.4. We should reject the model g1_fit.\n\n    References\n    ----------\n    .. [1] Akaike Information Criterion.\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\n    \"\"\"\n    return akaike_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
        "mutated": [
            "def akaike_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n    '\\n    Computes the Akaike Information Criterion assuming that the observations\\n    are Gaussian distributed.\\n\\n    In this case, AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k\\n\\n    In case that the sample size is not \"large enough\", a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k +\\n                       \\\\dfrac{2k(k+1)}{n-k-1}\\n\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e.,  the dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    This example is based on Astropy Modeling webpage, Compound models\\n    section.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import akaike_info_criterion_lsq\\n    >>> np.random.seed(42)\\n    >>> # Generate fake data\\n    >>> g1 = models.Gaussian1D(.1, 0, 0.2) # changed this to noise level\\n    >>> g2 = models.Gaussian1D(.1, 0.3, 0.2) # and added another Gaussian\\n    >>> g3 = models.Gaussian1D(2.5, 0.5, 0.1)\\n    >>> x = np.linspace(-1, 1, 200)\\n    >>> y = g1(x) + g2(x) + g3(x) + np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit with three Gaussians\\n    >>> g3_init = (models.Gaussian1D(.1, 0, 0.1)\\n    ...            + models.Gaussian1D(.1, 0.2, 0.15)\\n    ...            + models.Gaussian1D(2.4, .4, 0.1))\\n    >>> fitter = fitting.LevMarLSQFitter()\\n    >>> g3_fit = fitter(g3_init, x, y)\\n    >>> # Fit with two Gaussians\\n    >>> g2_init = (models.Gaussian1D(.1, 0, 0.1) +\\n    ...            models.Gaussian1D(2, 0.5, 0.1))\\n    >>> g2_fit = fitter(g2_init, x, y)\\n    >>> # Fit with only one Gaussian\\n    >>> g1_init = models.Gaussian1D(amplitude=2., mean=0.3, stddev=.5)\\n    >>> g1_fit = fitter(g1_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_g3 = np.sum((g3_fit(x) - y)**2.0)\\n    >>> ssr_g2 = np.sum((g2_fit(x) - y)**2.0)\\n    >>> ssr_g1 = np.sum((g1_fit(x) - y)**2.0)\\n    >>> akaike_info_criterion_lsq(ssr_g3, 9, x.shape[0]) # doctest: +FLOAT_CMP\\n    -634.5257517810961\\n    >>> akaike_info_criterion_lsq(ssr_g2, 6, x.shape[0]) # doctest: +FLOAT_CMP\\n    -662.83834510232043\\n    >>> akaike_info_criterion_lsq(ssr_g1, 3, x.shape[0]) # doctest: +FLOAT_CMP\\n    -647.47312032659499\\n\\n    Hence, from the AIC values, we would prefer to choose the model g2_fit.\\n    However, we can considerably support the model g3_fit, since the\\n    difference in AIC is about 2.4. We should reject the model g1_fit.\\n\\n    References\\n    ----------\\n    .. [1] Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    '\n    return akaike_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
            "def akaike_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Computes the Akaike Information Criterion assuming that the observations\\n    are Gaussian distributed.\\n\\n    In this case, AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k\\n\\n    In case that the sample size is not \"large enough\", a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k +\\n                       \\\\dfrac{2k(k+1)}{n-k-1}\\n\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e.,  the dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    This example is based on Astropy Modeling webpage, Compound models\\n    section.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import akaike_info_criterion_lsq\\n    >>> np.random.seed(42)\\n    >>> # Generate fake data\\n    >>> g1 = models.Gaussian1D(.1, 0, 0.2) # changed this to noise level\\n    >>> g2 = models.Gaussian1D(.1, 0.3, 0.2) # and added another Gaussian\\n    >>> g3 = models.Gaussian1D(2.5, 0.5, 0.1)\\n    >>> x = np.linspace(-1, 1, 200)\\n    >>> y = g1(x) + g2(x) + g3(x) + np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit with three Gaussians\\n    >>> g3_init = (models.Gaussian1D(.1, 0, 0.1)\\n    ...            + models.Gaussian1D(.1, 0.2, 0.15)\\n    ...            + models.Gaussian1D(2.4, .4, 0.1))\\n    >>> fitter = fitting.LevMarLSQFitter()\\n    >>> g3_fit = fitter(g3_init, x, y)\\n    >>> # Fit with two Gaussians\\n    >>> g2_init = (models.Gaussian1D(.1, 0, 0.1) +\\n    ...            models.Gaussian1D(2, 0.5, 0.1))\\n    >>> g2_fit = fitter(g2_init, x, y)\\n    >>> # Fit with only one Gaussian\\n    >>> g1_init = models.Gaussian1D(amplitude=2., mean=0.3, stddev=.5)\\n    >>> g1_fit = fitter(g1_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_g3 = np.sum((g3_fit(x) - y)**2.0)\\n    >>> ssr_g2 = np.sum((g2_fit(x) - y)**2.0)\\n    >>> ssr_g1 = np.sum((g1_fit(x) - y)**2.0)\\n    >>> akaike_info_criterion_lsq(ssr_g3, 9, x.shape[0]) # doctest: +FLOAT_CMP\\n    -634.5257517810961\\n    >>> akaike_info_criterion_lsq(ssr_g2, 6, x.shape[0]) # doctest: +FLOAT_CMP\\n    -662.83834510232043\\n    >>> akaike_info_criterion_lsq(ssr_g1, 3, x.shape[0]) # doctest: +FLOAT_CMP\\n    -647.47312032659499\\n\\n    Hence, from the AIC values, we would prefer to choose the model g2_fit.\\n    However, we can considerably support the model g3_fit, since the\\n    difference in AIC is about 2.4. We should reject the model g1_fit.\\n\\n    References\\n    ----------\\n    .. [1] Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    '\n    return akaike_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
            "def akaike_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Computes the Akaike Information Criterion assuming that the observations\\n    are Gaussian distributed.\\n\\n    In this case, AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k\\n\\n    In case that the sample size is not \"large enough\", a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k +\\n                       \\\\dfrac{2k(k+1)}{n-k-1}\\n\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e.,  the dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    This example is based on Astropy Modeling webpage, Compound models\\n    section.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import akaike_info_criterion_lsq\\n    >>> np.random.seed(42)\\n    >>> # Generate fake data\\n    >>> g1 = models.Gaussian1D(.1, 0, 0.2) # changed this to noise level\\n    >>> g2 = models.Gaussian1D(.1, 0.3, 0.2) # and added another Gaussian\\n    >>> g3 = models.Gaussian1D(2.5, 0.5, 0.1)\\n    >>> x = np.linspace(-1, 1, 200)\\n    >>> y = g1(x) + g2(x) + g3(x) + np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit with three Gaussians\\n    >>> g3_init = (models.Gaussian1D(.1, 0, 0.1)\\n    ...            + models.Gaussian1D(.1, 0.2, 0.15)\\n    ...            + models.Gaussian1D(2.4, .4, 0.1))\\n    >>> fitter = fitting.LevMarLSQFitter()\\n    >>> g3_fit = fitter(g3_init, x, y)\\n    >>> # Fit with two Gaussians\\n    >>> g2_init = (models.Gaussian1D(.1, 0, 0.1) +\\n    ...            models.Gaussian1D(2, 0.5, 0.1))\\n    >>> g2_fit = fitter(g2_init, x, y)\\n    >>> # Fit with only one Gaussian\\n    >>> g1_init = models.Gaussian1D(amplitude=2., mean=0.3, stddev=.5)\\n    >>> g1_fit = fitter(g1_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_g3 = np.sum((g3_fit(x) - y)**2.0)\\n    >>> ssr_g2 = np.sum((g2_fit(x) - y)**2.0)\\n    >>> ssr_g1 = np.sum((g1_fit(x) - y)**2.0)\\n    >>> akaike_info_criterion_lsq(ssr_g3, 9, x.shape[0]) # doctest: +FLOAT_CMP\\n    -634.5257517810961\\n    >>> akaike_info_criterion_lsq(ssr_g2, 6, x.shape[0]) # doctest: +FLOAT_CMP\\n    -662.83834510232043\\n    >>> akaike_info_criterion_lsq(ssr_g1, 3, x.shape[0]) # doctest: +FLOAT_CMP\\n    -647.47312032659499\\n\\n    Hence, from the AIC values, we would prefer to choose the model g2_fit.\\n    However, we can considerably support the model g3_fit, since the\\n    difference in AIC is about 2.4. We should reject the model g1_fit.\\n\\n    References\\n    ----------\\n    .. [1] Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    '\n    return akaike_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
            "def akaike_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Computes the Akaike Information Criterion assuming that the observations\\n    are Gaussian distributed.\\n\\n    In this case, AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k\\n\\n    In case that the sample size is not \"large enough\", a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k +\\n                       \\\\dfrac{2k(k+1)}{n-k-1}\\n\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e.,  the dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    This example is based on Astropy Modeling webpage, Compound models\\n    section.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import akaike_info_criterion_lsq\\n    >>> np.random.seed(42)\\n    >>> # Generate fake data\\n    >>> g1 = models.Gaussian1D(.1, 0, 0.2) # changed this to noise level\\n    >>> g2 = models.Gaussian1D(.1, 0.3, 0.2) # and added another Gaussian\\n    >>> g3 = models.Gaussian1D(2.5, 0.5, 0.1)\\n    >>> x = np.linspace(-1, 1, 200)\\n    >>> y = g1(x) + g2(x) + g3(x) + np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit with three Gaussians\\n    >>> g3_init = (models.Gaussian1D(.1, 0, 0.1)\\n    ...            + models.Gaussian1D(.1, 0.2, 0.15)\\n    ...            + models.Gaussian1D(2.4, .4, 0.1))\\n    >>> fitter = fitting.LevMarLSQFitter()\\n    >>> g3_fit = fitter(g3_init, x, y)\\n    >>> # Fit with two Gaussians\\n    >>> g2_init = (models.Gaussian1D(.1, 0, 0.1) +\\n    ...            models.Gaussian1D(2, 0.5, 0.1))\\n    >>> g2_fit = fitter(g2_init, x, y)\\n    >>> # Fit with only one Gaussian\\n    >>> g1_init = models.Gaussian1D(amplitude=2., mean=0.3, stddev=.5)\\n    >>> g1_fit = fitter(g1_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_g3 = np.sum((g3_fit(x) - y)**2.0)\\n    >>> ssr_g2 = np.sum((g2_fit(x) - y)**2.0)\\n    >>> ssr_g1 = np.sum((g1_fit(x) - y)**2.0)\\n    >>> akaike_info_criterion_lsq(ssr_g3, 9, x.shape[0]) # doctest: +FLOAT_CMP\\n    -634.5257517810961\\n    >>> akaike_info_criterion_lsq(ssr_g2, 6, x.shape[0]) # doctest: +FLOAT_CMP\\n    -662.83834510232043\\n    >>> akaike_info_criterion_lsq(ssr_g1, 3, x.shape[0]) # doctest: +FLOAT_CMP\\n    -647.47312032659499\\n\\n    Hence, from the AIC values, we would prefer to choose the model g2_fit.\\n    However, we can considerably support the model g3_fit, since the\\n    difference in AIC is about 2.4. We should reject the model g1_fit.\\n\\n    References\\n    ----------\\n    .. [1] Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    '\n    return akaike_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)",
            "def akaike_info_criterion_lsq(ssr, n_params, n_samples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Computes the Akaike Information Criterion assuming that the observations\\n    are Gaussian distributed.\\n\\n    In this case, AIC is given as\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k\\n\\n    In case that the sample size is not \"large enough\", a correction is\\n    applied, i.e.\\n\\n    .. math::\\n\\n        \\\\mathrm{AIC} = n\\\\ln\\\\left(\\\\dfrac{\\\\mathrm{SSR}}{n}\\\\right) + 2k +\\n                       \\\\dfrac{2k(k+1)}{n-k-1}\\n\\n\\n    in which :math:`n` is the sample size, :math:`k` is the number of free\\n    parameters and :math:`\\\\mathrm{SSR}` stands for the sum of squared residuals\\n    between model and data.\\n\\n    This is applicable, for instance, when the parameters of a model are\\n    estimated using the least squares statistic.\\n\\n    Parameters\\n    ----------\\n    ssr : float\\n        Sum of squared residuals (SSR) between model and data.\\n    n_params : int\\n        Number of free parameters of the model, i.e.,  the dimension of the\\n        parameter space.\\n    n_samples : int\\n        Number of observations.\\n\\n    Returns\\n    -------\\n    aic : float\\n        Akaike Information Criterion.\\n\\n    Examples\\n    --------\\n    This example is based on Astropy Modeling webpage, Compound models\\n    section.\\n\\n    >>> import numpy as np\\n    >>> from astropy.modeling import models, fitting\\n    >>> from astropy.stats.info_theory import akaike_info_criterion_lsq\\n    >>> np.random.seed(42)\\n    >>> # Generate fake data\\n    >>> g1 = models.Gaussian1D(.1, 0, 0.2) # changed this to noise level\\n    >>> g2 = models.Gaussian1D(.1, 0.3, 0.2) # and added another Gaussian\\n    >>> g3 = models.Gaussian1D(2.5, 0.5, 0.1)\\n    >>> x = np.linspace(-1, 1, 200)\\n    >>> y = g1(x) + g2(x) + g3(x) + np.random.normal(0., 0.2, x.shape)\\n    >>> # Fit with three Gaussians\\n    >>> g3_init = (models.Gaussian1D(.1, 0, 0.1)\\n    ...            + models.Gaussian1D(.1, 0.2, 0.15)\\n    ...            + models.Gaussian1D(2.4, .4, 0.1))\\n    >>> fitter = fitting.LevMarLSQFitter()\\n    >>> g3_fit = fitter(g3_init, x, y)\\n    >>> # Fit with two Gaussians\\n    >>> g2_init = (models.Gaussian1D(.1, 0, 0.1) +\\n    ...            models.Gaussian1D(2, 0.5, 0.1))\\n    >>> g2_fit = fitter(g2_init, x, y)\\n    >>> # Fit with only one Gaussian\\n    >>> g1_init = models.Gaussian1D(amplitude=2., mean=0.3, stddev=.5)\\n    >>> g1_fit = fitter(g1_init, x, y)\\n    >>> # Compute the mean squared errors\\n    >>> ssr_g3 = np.sum((g3_fit(x) - y)**2.0)\\n    >>> ssr_g2 = np.sum((g2_fit(x) - y)**2.0)\\n    >>> ssr_g1 = np.sum((g1_fit(x) - y)**2.0)\\n    >>> akaike_info_criterion_lsq(ssr_g3, 9, x.shape[0]) # doctest: +FLOAT_CMP\\n    -634.5257517810961\\n    >>> akaike_info_criterion_lsq(ssr_g2, 6, x.shape[0]) # doctest: +FLOAT_CMP\\n    -662.83834510232043\\n    >>> akaike_info_criterion_lsq(ssr_g1, 3, x.shape[0]) # doctest: +FLOAT_CMP\\n    -647.47312032659499\\n\\n    Hence, from the AIC values, we would prefer to choose the model g2_fit.\\n    However, we can considerably support the model g3_fit, since the\\n    difference in AIC is about 2.4. We should reject the model g1_fit.\\n\\n    References\\n    ----------\\n    .. [1] Akaike Information Criterion.\\n       <https://en.wikipedia.org/wiki/Akaike_information_criterion>\\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\\n       <https://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\\n    '\n    return akaike_info_criterion(-0.5 * n_samples * np.log(ssr / n_samples), n_params, n_samples)"
        ]
    }
]