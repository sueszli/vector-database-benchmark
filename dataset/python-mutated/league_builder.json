[
    {
        "func_name": "__init__",
        "original": "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig):\n    \"\"\"Initializes a LeagueBuilder instance.\n\n        Args:\n            algo: The Algorithm object by which this league builder is used.\n                Algorithm calls `build_league()` after each training step.\n            algo_config: The (not yet validated) config to be\n                used on the Algorithm. Child classes of `LeagueBuilder`\n                should preprocess this to add e.g. multiagent settings\n                to this config.\n        \"\"\"\n    self.algo = algo\n    self.config = algo_config",
        "mutated": [
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig):\n    if False:\n        i = 10\n    'Initializes a LeagueBuilder instance.\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step.\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n        '\n    self.algo = algo\n    self.config = algo_config",
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes a LeagueBuilder instance.\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step.\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n        '\n    self.algo = algo\n    self.config = algo_config",
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes a LeagueBuilder instance.\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step.\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n        '\n    self.algo = algo\n    self.config = algo_config",
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes a LeagueBuilder instance.\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step.\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n        '\n    self.algo = algo\n    self.config = algo_config",
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes a LeagueBuilder instance.\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step.\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n        '\n    self.algo = algo\n    self.config = algo_config"
        ]
    },
    {
        "func_name": "build_league",
        "original": "def build_league(self, result: ResultDict) -> None:\n    \"\"\"Method containing league-building logic. Called after train step.\n\n        Args:\n            result: The most recent result dict with all necessary stats in\n                it (e.g. episode rewards) to perform league building\n                operations.\n        \"\"\"\n    raise NotImplementedError",
        "mutated": [
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n    'Method containing league-building logic. Called after train step.\\n\\n        Args:\\n            result: The most recent result dict with all necessary stats in\\n                it (e.g. episode rewards) to perform league building\\n                operations.\\n        '\n    raise NotImplementedError",
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Method containing league-building logic. Called after train step.\\n\\n        Args:\\n            result: The most recent result dict with all necessary stats in\\n                it (e.g. episode rewards) to perform league building\\n                operations.\\n        '\n    raise NotImplementedError",
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Method containing league-building logic. Called after train step.\\n\\n        Args:\\n            result: The most recent result dict with all necessary stats in\\n                it (e.g. episode rewards) to perform league building\\n                operations.\\n        '\n    raise NotImplementedError",
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Method containing league-building logic. Called after train step.\\n\\n        Args:\\n            result: The most recent result dict with all necessary stats in\\n                it (e.g. episode rewards) to perform league building\\n                operations.\\n        '\n    raise NotImplementedError",
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Method containing league-building logic. Called after train step.\\n\\n        Args:\\n            result: The most recent result dict with all necessary stats in\\n                it (e.g. episode rewards) to perform league building\\n                operations.\\n        '\n    raise NotImplementedError"
        ]
    },
    {
        "func_name": "__getstate__",
        "original": "def __getstate__(self) -> Dict[str, Any]:\n    \"\"\"Returns a state dict, mapping str keys to state variables.\n\n        Returns:\n            The current state dict of this LeagueBuilder.\n        \"\"\"\n    return {}",
        "mutated": [
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n    'Returns a state dict, mapping str keys to state variables.\\n\\n        Returns:\\n            The current state dict of this LeagueBuilder.\\n        '\n    return {}",
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a state dict, mapping str keys to state variables.\\n\\n        Returns:\\n            The current state dict of this LeagueBuilder.\\n        '\n    return {}",
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a state dict, mapping str keys to state variables.\\n\\n        Returns:\\n            The current state dict of this LeagueBuilder.\\n        '\n    return {}",
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a state dict, mapping str keys to state variables.\\n\\n        Returns:\\n            The current state dict of this LeagueBuilder.\\n        '\n    return {}",
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a state dict, mapping str keys to state variables.\\n\\n        Returns:\\n            The current state dict of this LeagueBuilder.\\n        '\n    return {}"
        ]
    },
    {
        "func_name": "build_league",
        "original": "def build_league(self, result: ResultDict) -> None:\n    pass",
        "mutated": [
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n    pass",
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "def build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig, num_random_policies: int=2, num_learning_league_exploiters: int=4, num_learning_main_exploiters: int=4, win_rate_threshold_for_new_snapshot: float=0.8, keep_new_snapshot_training_prob: float=0.0, prob_league_exploiter_match: float=0.33, prob_main_exploiter_match: float=0.33, prob_main_exploiter_playing_against_learning_main: float=0.5):\n    \"\"\"Initializes a AlphaStarLeagueBuilder instance.\n\n        The following match types are possible:\n        LE: A learning (not snapshot) league_exploiter vs any snapshot policy.\n        ME: A learning (not snapshot) main exploiter vs any main.\n        M: Main self-play (main vs main).\n\n        Args:\n            algo: The Algorithm object by which this league builder is used.\n                Algorithm calls `build_league()` after each training step to reconfigure\n                the league structure (e.g. to add/remove policies).\n            algo_config: The (not yet validated) config to be\n                used on the Algorithm. Child classes of `LeagueBuilder`\n                should preprocess this to add e.g. multiagent settings\n                to this config.\n            num_random_policies: The number of random policies to add to the\n                league. This must be an even number (including 0) as these\n                will be evenly distributed amongst league- and main- exploiters.\n            num_learning_league_exploiters: The number of initially learning\n                league-exploiters to create.\n            num_learning_main_exploiters: The number of initially learning\n                main-exploiters to create.\n            win_rate_threshold_for_new_snapshot: The win-rate to be achieved\n                for a learning policy to get snapshot'd (forked into `self` +\n                a new learning or non-learning copy of `self`).\n            keep_new_snapshot_training_prob: The probability with which a new\n                snapshot should keep training. Note that the policy from which\n                this snapshot is taken will continue to train regardless.\n            prob_league_exploiter_match: Probability of an episode to become a\n                league-exploiter vs snapshot match.\n            prob_main_exploiter_match: Probability of an episode to become a\n                main-exploiter vs main match.\n            prob_main_exploiter_playing_against_learning_main: Probability of\n                a main-exploiter vs (training!) main match.\n        \"\"\"\n    super().__init__(algo, algo_config)\n    self.win_rate_threshold_for_new_snapshot = win_rate_threshold_for_new_snapshot\n    self.keep_new_snapshot_training_prob = keep_new_snapshot_training_prob\n    self.prob_league_exploiter_match = prob_league_exploiter_match\n    self.prob_main_exploiter_match = prob_main_exploiter_match\n    self.prob_main_exploiter_playing_against_learning_main = prob_main_exploiter_playing_against_learning_main\n    self.win_rates: DefaultDict[PolicyID, float] = defaultdict(float)\n    assert num_random_policies % 2 == 0, \"ERROR: `num_random_policies` must be even number (we'll distribute these evenly amongst league- and main-exploiters)!\"\n    self.config._is_frozen = False\n    assert self.config.policies is None, 'ERROR: `config.policies` should be None (not pre-defined by user)! AlphaStarLeagueBuilder will construct this itself.'\n    policies = {}\n    self.main_policies = 1\n    self.league_exploiters = num_learning_league_exploiters + num_random_policies / 2\n    self.main_exploiters = num_learning_main_exploiters + num_random_policies / 2\n    policies['main_0'] = PolicySpec()\n    policies_to_train = ['main_0']\n    i = -1\n    for i in range(num_random_policies // 2):\n        policies[f'league_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n        policies[f'main_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n    for j in range(num_learning_league_exploiters):\n        pid = f'league_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    for j in range(num_learning_league_exploiters):\n        pid = f'main_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    self.config.policy_mapping_fn = lambda agent_id, episode, worker, **kw: 'main_0' if episode.episode_id % 2 == agent_id else 'main_exploiter_0'\n    self.config.policies = policies\n    self.config.policies_to_train = policies_to_train\n    self.config.freeze()",
        "mutated": [
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig, num_random_policies: int=2, num_learning_league_exploiters: int=4, num_learning_main_exploiters: int=4, win_rate_threshold_for_new_snapshot: float=0.8, keep_new_snapshot_training_prob: float=0.0, prob_league_exploiter_match: float=0.33, prob_main_exploiter_match: float=0.33, prob_main_exploiter_playing_against_learning_main: float=0.5):\n    if False:\n        i = 10\n    \"Initializes a AlphaStarLeagueBuilder instance.\\n\\n        The following match types are possible:\\n        LE: A learning (not snapshot) league_exploiter vs any snapshot policy.\\n        ME: A learning (not snapshot) main exploiter vs any main.\\n        M: Main self-play (main vs main).\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step to reconfigure\\n                the league structure (e.g. to add/remove policies).\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n            num_random_policies: The number of random policies to add to the\\n                league. This must be an even number (including 0) as these\\n                will be evenly distributed amongst league- and main- exploiters.\\n            num_learning_league_exploiters: The number of initially learning\\n                league-exploiters to create.\\n            num_learning_main_exploiters: The number of initially learning\\n                main-exploiters to create.\\n            win_rate_threshold_for_new_snapshot: The win-rate to be achieved\\n                for a learning policy to get snapshot'd (forked into `self` +\\n                a new learning or non-learning copy of `self`).\\n            keep_new_snapshot_training_prob: The probability with which a new\\n                snapshot should keep training. Note that the policy from which\\n                this snapshot is taken will continue to train regardless.\\n            prob_league_exploiter_match: Probability of an episode to become a\\n                league-exploiter vs snapshot match.\\n            prob_main_exploiter_match: Probability of an episode to become a\\n                main-exploiter vs main match.\\n            prob_main_exploiter_playing_against_learning_main: Probability of\\n                a main-exploiter vs (training!) main match.\\n        \"\n    super().__init__(algo, algo_config)\n    self.win_rate_threshold_for_new_snapshot = win_rate_threshold_for_new_snapshot\n    self.keep_new_snapshot_training_prob = keep_new_snapshot_training_prob\n    self.prob_league_exploiter_match = prob_league_exploiter_match\n    self.prob_main_exploiter_match = prob_main_exploiter_match\n    self.prob_main_exploiter_playing_against_learning_main = prob_main_exploiter_playing_against_learning_main\n    self.win_rates: DefaultDict[PolicyID, float] = defaultdict(float)\n    assert num_random_policies % 2 == 0, \"ERROR: `num_random_policies` must be even number (we'll distribute these evenly amongst league- and main-exploiters)!\"\n    self.config._is_frozen = False\n    assert self.config.policies is None, 'ERROR: `config.policies` should be None (not pre-defined by user)! AlphaStarLeagueBuilder will construct this itself.'\n    policies = {}\n    self.main_policies = 1\n    self.league_exploiters = num_learning_league_exploiters + num_random_policies / 2\n    self.main_exploiters = num_learning_main_exploiters + num_random_policies / 2\n    policies['main_0'] = PolicySpec()\n    policies_to_train = ['main_0']\n    i = -1\n    for i in range(num_random_policies // 2):\n        policies[f'league_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n        policies[f'main_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n    for j in range(num_learning_league_exploiters):\n        pid = f'league_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    for j in range(num_learning_league_exploiters):\n        pid = f'main_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    self.config.policy_mapping_fn = lambda agent_id, episode, worker, **kw: 'main_0' if episode.episode_id % 2 == agent_id else 'main_exploiter_0'\n    self.config.policies = policies\n    self.config.policies_to_train = policies_to_train\n    self.config.freeze()",
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig, num_random_policies: int=2, num_learning_league_exploiters: int=4, num_learning_main_exploiters: int=4, win_rate_threshold_for_new_snapshot: float=0.8, keep_new_snapshot_training_prob: float=0.0, prob_league_exploiter_match: float=0.33, prob_main_exploiter_match: float=0.33, prob_main_exploiter_playing_against_learning_main: float=0.5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Initializes a AlphaStarLeagueBuilder instance.\\n\\n        The following match types are possible:\\n        LE: A learning (not snapshot) league_exploiter vs any snapshot policy.\\n        ME: A learning (not snapshot) main exploiter vs any main.\\n        M: Main self-play (main vs main).\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step to reconfigure\\n                the league structure (e.g. to add/remove policies).\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n            num_random_policies: The number of random policies to add to the\\n                league. This must be an even number (including 0) as these\\n                will be evenly distributed amongst league- and main- exploiters.\\n            num_learning_league_exploiters: The number of initially learning\\n                league-exploiters to create.\\n            num_learning_main_exploiters: The number of initially learning\\n                main-exploiters to create.\\n            win_rate_threshold_for_new_snapshot: The win-rate to be achieved\\n                for a learning policy to get snapshot'd (forked into `self` +\\n                a new learning or non-learning copy of `self`).\\n            keep_new_snapshot_training_prob: The probability with which a new\\n                snapshot should keep training. Note that the policy from which\\n                this snapshot is taken will continue to train regardless.\\n            prob_league_exploiter_match: Probability of an episode to become a\\n                league-exploiter vs snapshot match.\\n            prob_main_exploiter_match: Probability of an episode to become a\\n                main-exploiter vs main match.\\n            prob_main_exploiter_playing_against_learning_main: Probability of\\n                a main-exploiter vs (training!) main match.\\n        \"\n    super().__init__(algo, algo_config)\n    self.win_rate_threshold_for_new_snapshot = win_rate_threshold_for_new_snapshot\n    self.keep_new_snapshot_training_prob = keep_new_snapshot_training_prob\n    self.prob_league_exploiter_match = prob_league_exploiter_match\n    self.prob_main_exploiter_match = prob_main_exploiter_match\n    self.prob_main_exploiter_playing_against_learning_main = prob_main_exploiter_playing_against_learning_main\n    self.win_rates: DefaultDict[PolicyID, float] = defaultdict(float)\n    assert num_random_policies % 2 == 0, \"ERROR: `num_random_policies` must be even number (we'll distribute these evenly amongst league- and main-exploiters)!\"\n    self.config._is_frozen = False\n    assert self.config.policies is None, 'ERROR: `config.policies` should be None (not pre-defined by user)! AlphaStarLeagueBuilder will construct this itself.'\n    policies = {}\n    self.main_policies = 1\n    self.league_exploiters = num_learning_league_exploiters + num_random_policies / 2\n    self.main_exploiters = num_learning_main_exploiters + num_random_policies / 2\n    policies['main_0'] = PolicySpec()\n    policies_to_train = ['main_0']\n    i = -1\n    for i in range(num_random_policies // 2):\n        policies[f'league_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n        policies[f'main_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n    for j in range(num_learning_league_exploiters):\n        pid = f'league_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    for j in range(num_learning_league_exploiters):\n        pid = f'main_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    self.config.policy_mapping_fn = lambda agent_id, episode, worker, **kw: 'main_0' if episode.episode_id % 2 == agent_id else 'main_exploiter_0'\n    self.config.policies = policies\n    self.config.policies_to_train = policies_to_train\n    self.config.freeze()",
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig, num_random_policies: int=2, num_learning_league_exploiters: int=4, num_learning_main_exploiters: int=4, win_rate_threshold_for_new_snapshot: float=0.8, keep_new_snapshot_training_prob: float=0.0, prob_league_exploiter_match: float=0.33, prob_main_exploiter_match: float=0.33, prob_main_exploiter_playing_against_learning_main: float=0.5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Initializes a AlphaStarLeagueBuilder instance.\\n\\n        The following match types are possible:\\n        LE: A learning (not snapshot) league_exploiter vs any snapshot policy.\\n        ME: A learning (not snapshot) main exploiter vs any main.\\n        M: Main self-play (main vs main).\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step to reconfigure\\n                the league structure (e.g. to add/remove policies).\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n            num_random_policies: The number of random policies to add to the\\n                league. This must be an even number (including 0) as these\\n                will be evenly distributed amongst league- and main- exploiters.\\n            num_learning_league_exploiters: The number of initially learning\\n                league-exploiters to create.\\n            num_learning_main_exploiters: The number of initially learning\\n                main-exploiters to create.\\n            win_rate_threshold_for_new_snapshot: The win-rate to be achieved\\n                for a learning policy to get snapshot'd (forked into `self` +\\n                a new learning or non-learning copy of `self`).\\n            keep_new_snapshot_training_prob: The probability with which a new\\n                snapshot should keep training. Note that the policy from which\\n                this snapshot is taken will continue to train regardless.\\n            prob_league_exploiter_match: Probability of an episode to become a\\n                league-exploiter vs snapshot match.\\n            prob_main_exploiter_match: Probability of an episode to become a\\n                main-exploiter vs main match.\\n            prob_main_exploiter_playing_against_learning_main: Probability of\\n                a main-exploiter vs (training!) main match.\\n        \"\n    super().__init__(algo, algo_config)\n    self.win_rate_threshold_for_new_snapshot = win_rate_threshold_for_new_snapshot\n    self.keep_new_snapshot_training_prob = keep_new_snapshot_training_prob\n    self.prob_league_exploiter_match = prob_league_exploiter_match\n    self.prob_main_exploiter_match = prob_main_exploiter_match\n    self.prob_main_exploiter_playing_against_learning_main = prob_main_exploiter_playing_against_learning_main\n    self.win_rates: DefaultDict[PolicyID, float] = defaultdict(float)\n    assert num_random_policies % 2 == 0, \"ERROR: `num_random_policies` must be even number (we'll distribute these evenly amongst league- and main-exploiters)!\"\n    self.config._is_frozen = False\n    assert self.config.policies is None, 'ERROR: `config.policies` should be None (not pre-defined by user)! AlphaStarLeagueBuilder will construct this itself.'\n    policies = {}\n    self.main_policies = 1\n    self.league_exploiters = num_learning_league_exploiters + num_random_policies / 2\n    self.main_exploiters = num_learning_main_exploiters + num_random_policies / 2\n    policies['main_0'] = PolicySpec()\n    policies_to_train = ['main_0']\n    i = -1\n    for i in range(num_random_policies // 2):\n        policies[f'league_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n        policies[f'main_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n    for j in range(num_learning_league_exploiters):\n        pid = f'league_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    for j in range(num_learning_league_exploiters):\n        pid = f'main_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    self.config.policy_mapping_fn = lambda agent_id, episode, worker, **kw: 'main_0' if episode.episode_id % 2 == agent_id else 'main_exploiter_0'\n    self.config.policies = policies\n    self.config.policies_to_train = policies_to_train\n    self.config.freeze()",
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig, num_random_policies: int=2, num_learning_league_exploiters: int=4, num_learning_main_exploiters: int=4, win_rate_threshold_for_new_snapshot: float=0.8, keep_new_snapshot_training_prob: float=0.0, prob_league_exploiter_match: float=0.33, prob_main_exploiter_match: float=0.33, prob_main_exploiter_playing_against_learning_main: float=0.5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Initializes a AlphaStarLeagueBuilder instance.\\n\\n        The following match types are possible:\\n        LE: A learning (not snapshot) league_exploiter vs any snapshot policy.\\n        ME: A learning (not snapshot) main exploiter vs any main.\\n        M: Main self-play (main vs main).\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step to reconfigure\\n                the league structure (e.g. to add/remove policies).\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n            num_random_policies: The number of random policies to add to the\\n                league. This must be an even number (including 0) as these\\n                will be evenly distributed amongst league- and main- exploiters.\\n            num_learning_league_exploiters: The number of initially learning\\n                league-exploiters to create.\\n            num_learning_main_exploiters: The number of initially learning\\n                main-exploiters to create.\\n            win_rate_threshold_for_new_snapshot: The win-rate to be achieved\\n                for a learning policy to get snapshot'd (forked into `self` +\\n                a new learning or non-learning copy of `self`).\\n            keep_new_snapshot_training_prob: The probability with which a new\\n                snapshot should keep training. Note that the policy from which\\n                this snapshot is taken will continue to train regardless.\\n            prob_league_exploiter_match: Probability of an episode to become a\\n                league-exploiter vs snapshot match.\\n            prob_main_exploiter_match: Probability of an episode to become a\\n                main-exploiter vs main match.\\n            prob_main_exploiter_playing_against_learning_main: Probability of\\n                a main-exploiter vs (training!) main match.\\n        \"\n    super().__init__(algo, algo_config)\n    self.win_rate_threshold_for_new_snapshot = win_rate_threshold_for_new_snapshot\n    self.keep_new_snapshot_training_prob = keep_new_snapshot_training_prob\n    self.prob_league_exploiter_match = prob_league_exploiter_match\n    self.prob_main_exploiter_match = prob_main_exploiter_match\n    self.prob_main_exploiter_playing_against_learning_main = prob_main_exploiter_playing_against_learning_main\n    self.win_rates: DefaultDict[PolicyID, float] = defaultdict(float)\n    assert num_random_policies % 2 == 0, \"ERROR: `num_random_policies` must be even number (we'll distribute these evenly amongst league- and main-exploiters)!\"\n    self.config._is_frozen = False\n    assert self.config.policies is None, 'ERROR: `config.policies` should be None (not pre-defined by user)! AlphaStarLeagueBuilder will construct this itself.'\n    policies = {}\n    self.main_policies = 1\n    self.league_exploiters = num_learning_league_exploiters + num_random_policies / 2\n    self.main_exploiters = num_learning_main_exploiters + num_random_policies / 2\n    policies['main_0'] = PolicySpec()\n    policies_to_train = ['main_0']\n    i = -1\n    for i in range(num_random_policies // 2):\n        policies[f'league_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n        policies[f'main_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n    for j in range(num_learning_league_exploiters):\n        pid = f'league_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    for j in range(num_learning_league_exploiters):\n        pid = f'main_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    self.config.policy_mapping_fn = lambda agent_id, episode, worker, **kw: 'main_0' if episode.episode_id % 2 == agent_id else 'main_exploiter_0'\n    self.config.policies = policies\n    self.config.policies_to_train = policies_to_train\n    self.config.freeze()",
            "def __init__(self, algo: Algorithm, algo_config: AlgorithmConfig, num_random_policies: int=2, num_learning_league_exploiters: int=4, num_learning_main_exploiters: int=4, win_rate_threshold_for_new_snapshot: float=0.8, keep_new_snapshot_training_prob: float=0.0, prob_league_exploiter_match: float=0.33, prob_main_exploiter_match: float=0.33, prob_main_exploiter_playing_against_learning_main: float=0.5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Initializes a AlphaStarLeagueBuilder instance.\\n\\n        The following match types are possible:\\n        LE: A learning (not snapshot) league_exploiter vs any snapshot policy.\\n        ME: A learning (not snapshot) main exploiter vs any main.\\n        M: Main self-play (main vs main).\\n\\n        Args:\\n            algo: The Algorithm object by which this league builder is used.\\n                Algorithm calls `build_league()` after each training step to reconfigure\\n                the league structure (e.g. to add/remove policies).\\n            algo_config: The (not yet validated) config to be\\n                used on the Algorithm. Child classes of `LeagueBuilder`\\n                should preprocess this to add e.g. multiagent settings\\n                to this config.\\n            num_random_policies: The number of random policies to add to the\\n                league. This must be an even number (including 0) as these\\n                will be evenly distributed amongst league- and main- exploiters.\\n            num_learning_league_exploiters: The number of initially learning\\n                league-exploiters to create.\\n            num_learning_main_exploiters: The number of initially learning\\n                main-exploiters to create.\\n            win_rate_threshold_for_new_snapshot: The win-rate to be achieved\\n                for a learning policy to get snapshot'd (forked into `self` +\\n                a new learning or non-learning copy of `self`).\\n            keep_new_snapshot_training_prob: The probability with which a new\\n                snapshot should keep training. Note that the policy from which\\n                this snapshot is taken will continue to train regardless.\\n            prob_league_exploiter_match: Probability of an episode to become a\\n                league-exploiter vs snapshot match.\\n            prob_main_exploiter_match: Probability of an episode to become a\\n                main-exploiter vs main match.\\n            prob_main_exploiter_playing_against_learning_main: Probability of\\n                a main-exploiter vs (training!) main match.\\n        \"\n    super().__init__(algo, algo_config)\n    self.win_rate_threshold_for_new_snapshot = win_rate_threshold_for_new_snapshot\n    self.keep_new_snapshot_training_prob = keep_new_snapshot_training_prob\n    self.prob_league_exploiter_match = prob_league_exploiter_match\n    self.prob_main_exploiter_match = prob_main_exploiter_match\n    self.prob_main_exploiter_playing_against_learning_main = prob_main_exploiter_playing_against_learning_main\n    self.win_rates: DefaultDict[PolicyID, float] = defaultdict(float)\n    assert num_random_policies % 2 == 0, \"ERROR: `num_random_policies` must be even number (we'll distribute these evenly amongst league- and main-exploiters)!\"\n    self.config._is_frozen = False\n    assert self.config.policies is None, 'ERROR: `config.policies` should be None (not pre-defined by user)! AlphaStarLeagueBuilder will construct this itself.'\n    policies = {}\n    self.main_policies = 1\n    self.league_exploiters = num_learning_league_exploiters + num_random_policies / 2\n    self.main_exploiters = num_learning_main_exploiters + num_random_policies / 2\n    policies['main_0'] = PolicySpec()\n    policies_to_train = ['main_0']\n    i = -1\n    for i in range(num_random_policies // 2):\n        policies[f'league_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n        policies[f'main_exploiter_{i}'] = PolicySpec(policy_class=RandomPolicy)\n    for j in range(num_learning_league_exploiters):\n        pid = f'league_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    for j in range(num_learning_league_exploiters):\n        pid = f'main_exploiter_{j + i + 1}'\n        policies[pid] = PolicySpec()\n        policies_to_train.append(pid)\n    self.config.policy_mapping_fn = lambda agent_id, episode, worker, **kw: 'main_0' if episode.episode_id % 2 == agent_id else 'main_exploiter_0'\n    self.config.policies = policies\n    self.config.policies_to_train = policies_to_train\n    self.config.freeze()"
        ]
    },
    {
        "func_name": "policy_mapping_fn",
        "original": "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n    type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n    if type_ == 'LE':\n        if episode.episode_id % 2 == agent_id:\n            league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n            return league_exploiter\n        else:\n            all_opponents = list(non_trainable_policies)\n            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n            opponent = np.random.choice(all_opponents, p=probs)\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n            return opponent\n    elif type_ == 'ME':\n        if episode.episode_id % 2 == agent_id:\n            main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n            return main_exploiter\n        else:\n            if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                main = 'main_0'\n                training = 'training'\n            else:\n                all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                main = np.random.choice(all_opponents, p=probs)\n                training = 'frozen'\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n            return main\n    else:\n        logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n        return 'main_0'",
        "mutated": [
            "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n    if False:\n        i = 10\n    type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n    if type_ == 'LE':\n        if episode.episode_id % 2 == agent_id:\n            league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n            return league_exploiter\n        else:\n            all_opponents = list(non_trainable_policies)\n            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n            opponent = np.random.choice(all_opponents, p=probs)\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n            return opponent\n    elif type_ == 'ME':\n        if episode.episode_id % 2 == agent_id:\n            main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n            return main_exploiter\n        else:\n            if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                main = 'main_0'\n                training = 'training'\n            else:\n                all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                main = np.random.choice(all_opponents, p=probs)\n                training = 'frozen'\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n            return main\n    else:\n        logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n        return 'main_0'",
            "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n    if type_ == 'LE':\n        if episode.episode_id % 2 == agent_id:\n            league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n            return league_exploiter\n        else:\n            all_opponents = list(non_trainable_policies)\n            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n            opponent = np.random.choice(all_opponents, p=probs)\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n            return opponent\n    elif type_ == 'ME':\n        if episode.episode_id % 2 == agent_id:\n            main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n            return main_exploiter\n        else:\n            if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                main = 'main_0'\n                training = 'training'\n            else:\n                all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                main = np.random.choice(all_opponents, p=probs)\n                training = 'frozen'\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n            return main\n    else:\n        logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n        return 'main_0'",
            "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n    if type_ == 'LE':\n        if episode.episode_id % 2 == agent_id:\n            league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n            return league_exploiter\n        else:\n            all_opponents = list(non_trainable_policies)\n            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n            opponent = np.random.choice(all_opponents, p=probs)\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n            return opponent\n    elif type_ == 'ME':\n        if episode.episode_id % 2 == agent_id:\n            main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n            return main_exploiter\n        else:\n            if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                main = 'main_0'\n                training = 'training'\n            else:\n                all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                main = np.random.choice(all_opponents, p=probs)\n                training = 'frozen'\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n            return main\n    else:\n        logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n        return 'main_0'",
            "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n    if type_ == 'LE':\n        if episode.episode_id % 2 == agent_id:\n            league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n            return league_exploiter\n        else:\n            all_opponents = list(non_trainable_policies)\n            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n            opponent = np.random.choice(all_opponents, p=probs)\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n            return opponent\n    elif type_ == 'ME':\n        if episode.episode_id % 2 == agent_id:\n            main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n            return main_exploiter\n        else:\n            if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                main = 'main_0'\n                training = 'training'\n            else:\n                all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                main = np.random.choice(all_opponents, p=probs)\n                training = 'frozen'\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n            return main\n    else:\n        logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n        return 'main_0'",
            "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n    if type_ == 'LE':\n        if episode.episode_id % 2 == agent_id:\n            league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n            return league_exploiter\n        else:\n            all_opponents = list(non_trainable_policies)\n            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n            opponent = np.random.choice(all_opponents, p=probs)\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n            return opponent\n    elif type_ == 'ME':\n        if episode.episode_id % 2 == agent_id:\n            main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n            return main_exploiter\n        else:\n            if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                main = 'main_0'\n                training = 'training'\n            else:\n                all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                main = np.random.choice(all_opponents, p=probs)\n                training = 'frozen'\n            logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n            return main\n    else:\n        logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n        return 'main_0'"
        ]
    },
    {
        "func_name": "build_league",
        "original": "@override(LeagueBuilder)\ndef build_league(self, result: ResultDict) -> None:\n    local_worker = self.algo.workers.local_worker()\n    if 'evaluation' in result:\n        hist_stats = result['evaluation']['hist_stats']\n    else:\n        hist_stats = result['hist_stats']\n    trainable_policies = local_worker.get_policies_to_train()\n    non_trainable_policies = set(local_worker.policy_map.keys()) - trainable_policies\n    logger.info(f'League building after iter {self.algo.iteration}:')\n    for (policy_id, rew) in hist_stats.items():\n        mo = re.match('^policy_(.+)_reward$', policy_id)\n        if mo is None:\n            continue\n        policy_id = mo.group(1)\n        won = 0\n        for r in rew:\n            if r > 0.0:\n                won += 1\n        win_rate = won / len(rew)\n        self.win_rates[policy_id] = win_rate\n        if policy_id not in trainable_policies:\n            continue\n        logger.info(f'\\t{policy_id} win-rate={win_rate} -> ')\n        if win_rate >= self.win_rate_threshold_for_new_snapshot:\n            is_main = re.match('^main(_\\\\d+)?$', policy_id)\n            keep_training_p = self.keep_new_snapshot_training_prob\n            keep_training = False if is_main else np.random.choice([True, False], p=[keep_training_p, 1.0 - keep_training_p])\n            if policy_id.startswith('league_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.league_exploiters}', policy_id)\n                self.league_exploiters += 1\n            elif policy_id.startswith('main_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_exploiters}', policy_id)\n                self.main_exploiters += 1\n            else:\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_policies}', policy_id)\n                self.main_policies += 1\n            if keep_training:\n                trainable_policies.add(new_pol_id)\n            else:\n                non_trainable_policies.add(new_pol_id)\n            logger.info(f'adding new opponents to the mix ({new_pol_id}; trainable={keep_training}).')\n            num_main_policies = self.main_policies\n            probs_match_types = [self.prob_league_exploiter_match, self.prob_main_exploiter_match, 1.0 - self.prob_league_exploiter_match - self.prob_main_exploiter_match]\n            prob_playing_learning_main = self.prob_main_exploiter_playing_against_learning_main\n\n            def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n                type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n                if type_ == 'LE':\n                    if episode.episode_id % 2 == agent_id:\n                        league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n                        return league_exploiter\n                    else:\n                        all_opponents = list(non_trainable_policies)\n                        probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                        opponent = np.random.choice(all_opponents, p=probs)\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n                        return opponent\n                elif type_ == 'ME':\n                    if episode.episode_id % 2 == agent_id:\n                        main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n                        return main_exploiter\n                    else:\n                        if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                            main = 'main_0'\n                            training = 'training'\n                        else:\n                            all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                            main = np.random.choice(all_opponents, p=probs)\n                            training = 'frozen'\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n                        return main\n                else:\n                    logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n                    return 'main_0'\n            state = self.algo.get_policy(policy_id).get_state()\n            self.algo.add_policy(policy_id=new_pol_id, policy_cls=type(self.algo.get_policy(policy_id)), policy_state=state, policy_mapping_fn=policy_mapping_fn, policies_to_train=trainable_policies)\n        else:\n            logger.info('not good enough; will keep learning ...')",
        "mutated": [
            "@override(LeagueBuilder)\ndef build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n    local_worker = self.algo.workers.local_worker()\n    if 'evaluation' in result:\n        hist_stats = result['evaluation']['hist_stats']\n    else:\n        hist_stats = result['hist_stats']\n    trainable_policies = local_worker.get_policies_to_train()\n    non_trainable_policies = set(local_worker.policy_map.keys()) - trainable_policies\n    logger.info(f'League building after iter {self.algo.iteration}:')\n    for (policy_id, rew) in hist_stats.items():\n        mo = re.match('^policy_(.+)_reward$', policy_id)\n        if mo is None:\n            continue\n        policy_id = mo.group(1)\n        won = 0\n        for r in rew:\n            if r > 0.0:\n                won += 1\n        win_rate = won / len(rew)\n        self.win_rates[policy_id] = win_rate\n        if policy_id not in trainable_policies:\n            continue\n        logger.info(f'\\t{policy_id} win-rate={win_rate} -> ')\n        if win_rate >= self.win_rate_threshold_for_new_snapshot:\n            is_main = re.match('^main(_\\\\d+)?$', policy_id)\n            keep_training_p = self.keep_new_snapshot_training_prob\n            keep_training = False if is_main else np.random.choice([True, False], p=[keep_training_p, 1.0 - keep_training_p])\n            if policy_id.startswith('league_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.league_exploiters}', policy_id)\n                self.league_exploiters += 1\n            elif policy_id.startswith('main_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_exploiters}', policy_id)\n                self.main_exploiters += 1\n            else:\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_policies}', policy_id)\n                self.main_policies += 1\n            if keep_training:\n                trainable_policies.add(new_pol_id)\n            else:\n                non_trainable_policies.add(new_pol_id)\n            logger.info(f'adding new opponents to the mix ({new_pol_id}; trainable={keep_training}).')\n            num_main_policies = self.main_policies\n            probs_match_types = [self.prob_league_exploiter_match, self.prob_main_exploiter_match, 1.0 - self.prob_league_exploiter_match - self.prob_main_exploiter_match]\n            prob_playing_learning_main = self.prob_main_exploiter_playing_against_learning_main\n\n            def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n                type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n                if type_ == 'LE':\n                    if episode.episode_id % 2 == agent_id:\n                        league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n                        return league_exploiter\n                    else:\n                        all_opponents = list(non_trainable_policies)\n                        probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                        opponent = np.random.choice(all_opponents, p=probs)\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n                        return opponent\n                elif type_ == 'ME':\n                    if episode.episode_id % 2 == agent_id:\n                        main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n                        return main_exploiter\n                    else:\n                        if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                            main = 'main_0'\n                            training = 'training'\n                        else:\n                            all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                            main = np.random.choice(all_opponents, p=probs)\n                            training = 'frozen'\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n                        return main\n                else:\n                    logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n                    return 'main_0'\n            state = self.algo.get_policy(policy_id).get_state()\n            self.algo.add_policy(policy_id=new_pol_id, policy_cls=type(self.algo.get_policy(policy_id)), policy_state=state, policy_mapping_fn=policy_mapping_fn, policies_to_train=trainable_policies)\n        else:\n            logger.info('not good enough; will keep learning ...')",
            "@override(LeagueBuilder)\ndef build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    local_worker = self.algo.workers.local_worker()\n    if 'evaluation' in result:\n        hist_stats = result['evaluation']['hist_stats']\n    else:\n        hist_stats = result['hist_stats']\n    trainable_policies = local_worker.get_policies_to_train()\n    non_trainable_policies = set(local_worker.policy_map.keys()) - trainable_policies\n    logger.info(f'League building after iter {self.algo.iteration}:')\n    for (policy_id, rew) in hist_stats.items():\n        mo = re.match('^policy_(.+)_reward$', policy_id)\n        if mo is None:\n            continue\n        policy_id = mo.group(1)\n        won = 0\n        for r in rew:\n            if r > 0.0:\n                won += 1\n        win_rate = won / len(rew)\n        self.win_rates[policy_id] = win_rate\n        if policy_id not in trainable_policies:\n            continue\n        logger.info(f'\\t{policy_id} win-rate={win_rate} -> ')\n        if win_rate >= self.win_rate_threshold_for_new_snapshot:\n            is_main = re.match('^main(_\\\\d+)?$', policy_id)\n            keep_training_p = self.keep_new_snapshot_training_prob\n            keep_training = False if is_main else np.random.choice([True, False], p=[keep_training_p, 1.0 - keep_training_p])\n            if policy_id.startswith('league_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.league_exploiters}', policy_id)\n                self.league_exploiters += 1\n            elif policy_id.startswith('main_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_exploiters}', policy_id)\n                self.main_exploiters += 1\n            else:\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_policies}', policy_id)\n                self.main_policies += 1\n            if keep_training:\n                trainable_policies.add(new_pol_id)\n            else:\n                non_trainable_policies.add(new_pol_id)\n            logger.info(f'adding new opponents to the mix ({new_pol_id}; trainable={keep_training}).')\n            num_main_policies = self.main_policies\n            probs_match_types = [self.prob_league_exploiter_match, self.prob_main_exploiter_match, 1.0 - self.prob_league_exploiter_match - self.prob_main_exploiter_match]\n            prob_playing_learning_main = self.prob_main_exploiter_playing_against_learning_main\n\n            def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n                type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n                if type_ == 'LE':\n                    if episode.episode_id % 2 == agent_id:\n                        league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n                        return league_exploiter\n                    else:\n                        all_opponents = list(non_trainable_policies)\n                        probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                        opponent = np.random.choice(all_opponents, p=probs)\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n                        return opponent\n                elif type_ == 'ME':\n                    if episode.episode_id % 2 == agent_id:\n                        main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n                        return main_exploiter\n                    else:\n                        if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                            main = 'main_0'\n                            training = 'training'\n                        else:\n                            all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                            main = np.random.choice(all_opponents, p=probs)\n                            training = 'frozen'\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n                        return main\n                else:\n                    logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n                    return 'main_0'\n            state = self.algo.get_policy(policy_id).get_state()\n            self.algo.add_policy(policy_id=new_pol_id, policy_cls=type(self.algo.get_policy(policy_id)), policy_state=state, policy_mapping_fn=policy_mapping_fn, policies_to_train=trainable_policies)\n        else:\n            logger.info('not good enough; will keep learning ...')",
            "@override(LeagueBuilder)\ndef build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    local_worker = self.algo.workers.local_worker()\n    if 'evaluation' in result:\n        hist_stats = result['evaluation']['hist_stats']\n    else:\n        hist_stats = result['hist_stats']\n    trainable_policies = local_worker.get_policies_to_train()\n    non_trainable_policies = set(local_worker.policy_map.keys()) - trainable_policies\n    logger.info(f'League building after iter {self.algo.iteration}:')\n    for (policy_id, rew) in hist_stats.items():\n        mo = re.match('^policy_(.+)_reward$', policy_id)\n        if mo is None:\n            continue\n        policy_id = mo.group(1)\n        won = 0\n        for r in rew:\n            if r > 0.0:\n                won += 1\n        win_rate = won / len(rew)\n        self.win_rates[policy_id] = win_rate\n        if policy_id not in trainable_policies:\n            continue\n        logger.info(f'\\t{policy_id} win-rate={win_rate} -> ')\n        if win_rate >= self.win_rate_threshold_for_new_snapshot:\n            is_main = re.match('^main(_\\\\d+)?$', policy_id)\n            keep_training_p = self.keep_new_snapshot_training_prob\n            keep_training = False if is_main else np.random.choice([True, False], p=[keep_training_p, 1.0 - keep_training_p])\n            if policy_id.startswith('league_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.league_exploiters}', policy_id)\n                self.league_exploiters += 1\n            elif policy_id.startswith('main_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_exploiters}', policy_id)\n                self.main_exploiters += 1\n            else:\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_policies}', policy_id)\n                self.main_policies += 1\n            if keep_training:\n                trainable_policies.add(new_pol_id)\n            else:\n                non_trainable_policies.add(new_pol_id)\n            logger.info(f'adding new opponents to the mix ({new_pol_id}; trainable={keep_training}).')\n            num_main_policies = self.main_policies\n            probs_match_types = [self.prob_league_exploiter_match, self.prob_main_exploiter_match, 1.0 - self.prob_league_exploiter_match - self.prob_main_exploiter_match]\n            prob_playing_learning_main = self.prob_main_exploiter_playing_against_learning_main\n\n            def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n                type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n                if type_ == 'LE':\n                    if episode.episode_id % 2 == agent_id:\n                        league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n                        return league_exploiter\n                    else:\n                        all_opponents = list(non_trainable_policies)\n                        probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                        opponent = np.random.choice(all_opponents, p=probs)\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n                        return opponent\n                elif type_ == 'ME':\n                    if episode.episode_id % 2 == agent_id:\n                        main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n                        return main_exploiter\n                    else:\n                        if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                            main = 'main_0'\n                            training = 'training'\n                        else:\n                            all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                            main = np.random.choice(all_opponents, p=probs)\n                            training = 'frozen'\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n                        return main\n                else:\n                    logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n                    return 'main_0'\n            state = self.algo.get_policy(policy_id).get_state()\n            self.algo.add_policy(policy_id=new_pol_id, policy_cls=type(self.algo.get_policy(policy_id)), policy_state=state, policy_mapping_fn=policy_mapping_fn, policies_to_train=trainable_policies)\n        else:\n            logger.info('not good enough; will keep learning ...')",
            "@override(LeagueBuilder)\ndef build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    local_worker = self.algo.workers.local_worker()\n    if 'evaluation' in result:\n        hist_stats = result['evaluation']['hist_stats']\n    else:\n        hist_stats = result['hist_stats']\n    trainable_policies = local_worker.get_policies_to_train()\n    non_trainable_policies = set(local_worker.policy_map.keys()) - trainable_policies\n    logger.info(f'League building after iter {self.algo.iteration}:')\n    for (policy_id, rew) in hist_stats.items():\n        mo = re.match('^policy_(.+)_reward$', policy_id)\n        if mo is None:\n            continue\n        policy_id = mo.group(1)\n        won = 0\n        for r in rew:\n            if r > 0.0:\n                won += 1\n        win_rate = won / len(rew)\n        self.win_rates[policy_id] = win_rate\n        if policy_id not in trainable_policies:\n            continue\n        logger.info(f'\\t{policy_id} win-rate={win_rate} -> ')\n        if win_rate >= self.win_rate_threshold_for_new_snapshot:\n            is_main = re.match('^main(_\\\\d+)?$', policy_id)\n            keep_training_p = self.keep_new_snapshot_training_prob\n            keep_training = False if is_main else np.random.choice([True, False], p=[keep_training_p, 1.0 - keep_training_p])\n            if policy_id.startswith('league_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.league_exploiters}', policy_id)\n                self.league_exploiters += 1\n            elif policy_id.startswith('main_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_exploiters}', policy_id)\n                self.main_exploiters += 1\n            else:\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_policies}', policy_id)\n                self.main_policies += 1\n            if keep_training:\n                trainable_policies.add(new_pol_id)\n            else:\n                non_trainable_policies.add(new_pol_id)\n            logger.info(f'adding new opponents to the mix ({new_pol_id}; trainable={keep_training}).')\n            num_main_policies = self.main_policies\n            probs_match_types = [self.prob_league_exploiter_match, self.prob_main_exploiter_match, 1.0 - self.prob_league_exploiter_match - self.prob_main_exploiter_match]\n            prob_playing_learning_main = self.prob_main_exploiter_playing_against_learning_main\n\n            def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n                type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n                if type_ == 'LE':\n                    if episode.episode_id % 2 == agent_id:\n                        league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n                        return league_exploiter\n                    else:\n                        all_opponents = list(non_trainable_policies)\n                        probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                        opponent = np.random.choice(all_opponents, p=probs)\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n                        return opponent\n                elif type_ == 'ME':\n                    if episode.episode_id % 2 == agent_id:\n                        main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n                        return main_exploiter\n                    else:\n                        if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                            main = 'main_0'\n                            training = 'training'\n                        else:\n                            all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                            main = np.random.choice(all_opponents, p=probs)\n                            training = 'frozen'\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n                        return main\n                else:\n                    logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n                    return 'main_0'\n            state = self.algo.get_policy(policy_id).get_state()\n            self.algo.add_policy(policy_id=new_pol_id, policy_cls=type(self.algo.get_policy(policy_id)), policy_state=state, policy_mapping_fn=policy_mapping_fn, policies_to_train=trainable_policies)\n        else:\n            logger.info('not good enough; will keep learning ...')",
            "@override(LeagueBuilder)\ndef build_league(self, result: ResultDict) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    local_worker = self.algo.workers.local_worker()\n    if 'evaluation' in result:\n        hist_stats = result['evaluation']['hist_stats']\n    else:\n        hist_stats = result['hist_stats']\n    trainable_policies = local_worker.get_policies_to_train()\n    non_trainable_policies = set(local_worker.policy_map.keys()) - trainable_policies\n    logger.info(f'League building after iter {self.algo.iteration}:')\n    for (policy_id, rew) in hist_stats.items():\n        mo = re.match('^policy_(.+)_reward$', policy_id)\n        if mo is None:\n            continue\n        policy_id = mo.group(1)\n        won = 0\n        for r in rew:\n            if r > 0.0:\n                won += 1\n        win_rate = won / len(rew)\n        self.win_rates[policy_id] = win_rate\n        if policy_id not in trainable_policies:\n            continue\n        logger.info(f'\\t{policy_id} win-rate={win_rate} -> ')\n        if win_rate >= self.win_rate_threshold_for_new_snapshot:\n            is_main = re.match('^main(_\\\\d+)?$', policy_id)\n            keep_training_p = self.keep_new_snapshot_training_prob\n            keep_training = False if is_main else np.random.choice([True, False], p=[keep_training_p, 1.0 - keep_training_p])\n            if policy_id.startswith('league_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.league_exploiters}', policy_id)\n                self.league_exploiters += 1\n            elif policy_id.startswith('main_ex'):\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_exploiters}', policy_id)\n                self.main_exploiters += 1\n            else:\n                new_pol_id = re.sub('_\\\\d+$', f'_{self.main_policies}', policy_id)\n                self.main_policies += 1\n            if keep_training:\n                trainable_policies.add(new_pol_id)\n            else:\n                non_trainable_policies.add(new_pol_id)\n            logger.info(f'adding new opponents to the mix ({new_pol_id}; trainable={keep_training}).')\n            num_main_policies = self.main_policies\n            probs_match_types = [self.prob_league_exploiter_match, self.prob_main_exploiter_match, 1.0 - self.prob_league_exploiter_match - self.prob_main_exploiter_match]\n            prob_playing_learning_main = self.prob_main_exploiter_playing_against_learning_main\n\n            def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n                type_ = np.random.choice(['LE', 'ME', 'M'], p=probs_match_types)\n                if type_ == 'LE':\n                    if episode.episode_id % 2 == agent_id:\n                        league_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('league_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {league_exploiter} (training)')\n                        return league_exploiter\n                    else:\n                        all_opponents = list(non_trainable_policies)\n                        probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                        opponent = np.random.choice(all_opponents, p=probs)\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {opponent} (frozen)')\n                        return opponent\n                elif type_ == 'ME':\n                    if episode.episode_id % 2 == agent_id:\n                        main_exploiter = np.random.choice([p for p in trainable_policies if p.startswith('main_ex')])\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main_exploiter} (training)')\n                        return main_exploiter\n                    else:\n                        if num_main_policies == 1 or np.random.random() < prob_playing_learning_main:\n                            main = 'main_0'\n                            training = 'training'\n                        else:\n                            all_opponents = [f'main_{p}' for p in list(range(1, num_main_policies))]\n                            probs = softmax([worker.global_vars['win_rates'][pid] for pid in all_opponents])\n                            main = np.random.choice(all_opponents, p=probs)\n                            training = 'frozen'\n                        logger.debug(f'Episode {episode.episode_id}: AgentID {agent_id} played by {main} ({training})')\n                        return main\n                else:\n                    logger.debug(f'Episode {episode.episode_id}: main_0 vs main_0')\n                    return 'main_0'\n            state = self.algo.get_policy(policy_id).get_state()\n            self.algo.add_policy(policy_id=new_pol_id, policy_cls=type(self.algo.get_policy(policy_id)), policy_state=state, policy_mapping_fn=policy_mapping_fn, policies_to_train=trainable_policies)\n        else:\n            logger.info('not good enough; will keep learning ...')"
        ]
    },
    {
        "func_name": "__getstate__",
        "original": "def __getstate__(self) -> Dict[str, Any]:\n    return {'win_rates': self.win_rates, 'main_policies': self.main_policies, 'league_exploiters': self.league_exploiters, 'main_exploiters': self.main_exploiters}",
        "mutated": [
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n    return {'win_rates': self.win_rates, 'main_policies': self.main_policies, 'league_exploiters': self.league_exploiters, 'main_exploiters': self.main_exploiters}",
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'win_rates': self.win_rates, 'main_policies': self.main_policies, 'league_exploiters': self.league_exploiters, 'main_exploiters': self.main_exploiters}",
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'win_rates': self.win_rates, 'main_policies': self.main_policies, 'league_exploiters': self.league_exploiters, 'main_exploiters': self.main_exploiters}",
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'win_rates': self.win_rates, 'main_policies': self.main_policies, 'league_exploiters': self.league_exploiters, 'main_exploiters': self.main_exploiters}",
            "def __getstate__(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'win_rates': self.win_rates, 'main_policies': self.main_policies, 'league_exploiters': self.league_exploiters, 'main_exploiters': self.main_exploiters}"
        ]
    },
    {
        "func_name": "__setstate__",
        "original": "def __setstate__(self, state) -> None:\n    self.win_rates = state['win_rates']\n    self.main_policies = state['main_policies']\n    self.league_exploiters = state['league_exploiters']\n    self.main_exploiters = state['main_exploiters']",
        "mutated": [
            "def __setstate__(self, state) -> None:\n    if False:\n        i = 10\n    self.win_rates = state['win_rates']\n    self.main_policies = state['main_policies']\n    self.league_exploiters = state['league_exploiters']\n    self.main_exploiters = state['main_exploiters']",
            "def __setstate__(self, state) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.win_rates = state['win_rates']\n    self.main_policies = state['main_policies']\n    self.league_exploiters = state['league_exploiters']\n    self.main_exploiters = state['main_exploiters']",
            "def __setstate__(self, state) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.win_rates = state['win_rates']\n    self.main_policies = state['main_policies']\n    self.league_exploiters = state['league_exploiters']\n    self.main_exploiters = state['main_exploiters']",
            "def __setstate__(self, state) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.win_rates = state['win_rates']\n    self.main_policies = state['main_policies']\n    self.league_exploiters = state['league_exploiters']\n    self.main_exploiters = state['main_exploiters']",
            "def __setstate__(self, state) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.win_rates = state['win_rates']\n    self.main_policies = state['main_policies']\n    self.league_exploiters = state['league_exploiters']\n    self.main_exploiters = state['main_exploiters']"
        ]
    }
]