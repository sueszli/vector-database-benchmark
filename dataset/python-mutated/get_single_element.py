"""Python wrappers for Datasets and Iterators."""
from tensorflow.python.types import data as data_types
from tensorflow.python.util import deprecation
from tensorflow.python.util.tf_export import tf_export

@deprecation.deprecated(None, 'Use `tf.data.Dataset.get_single_element()`.')
@tf_export('data.experimental.get_single_element')
def get_single_element(dataset):
    if False:
        for i in range(10):
            print('nop')
    'Returns the single element of the `dataset` as a nested structure of tensors.\n\n  The function enables you to use a `tf.data.Dataset` in a stateless\n  "tensor-in tensor-out" expression, without creating an iterator.\n  This facilitates the ease of data transformation on tensors using the\n  optimized `tf.data.Dataset` abstraction on top of them.\n\n  For example, lets consider a `preprocessing_fn` which would take as an\n  input the raw features and returns the processed feature along with\n  it\'s label.\n\n  ```python\n  def preprocessing_fn(raw_feature):\n    # ... the raw_feature is preprocessed as per the use-case\n    return feature\n\n  raw_features = ...  # input batch of BATCH_SIZE elements.\n  dataset = (tf.data.Dataset.from_tensor_slices(raw_features)\n             .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n             .batch(BATCH_SIZE))\n\n  processed_features = tf.data.experimental.get_single_element(dataset)\n  ```\n\n  In the above example, the `raw_features` tensor of length=BATCH_SIZE\n  was converted to a `tf.data.Dataset`. Next, each of the `raw_feature` was\n  mapped using the `preprocessing_fn` and the processed features were\n  grouped into a single batch. The final `dataset` contains only one element\n  which is a batch of all the processed features.\n\n  NOTE: The `dataset` should contain only one element.\n\n  Now, instead of creating an iterator for the `dataset` and retrieving the\n  batch of features, the `tf.data.experimental.get_single_element()` function\n  is used to skip the iterator creation process and directly output the batch\n  of features.\n\n  This can be particularly useful when your tensor transformations are\n  expressed as `tf.data.Dataset` operations, and you want to use those\n  transformations while serving your model.\n\n  # Keras\n\n  ```python\n\n  model = ... # A pre-built or custom model\n\n  class PreprocessingModel(tf.keras.Model):\n    def __init__(self, model):\n      super().__init__(self)\n      self.model = model\n\n    @tf.function(input_signature=[...])\n    def serving_fn(self, data):\n      ds = tf.data.Dataset.from_tensor_slices(data)\n      ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n      ds = ds.batch(batch_size=BATCH_SIZE)\n      return tf.argmax(\n        self.model(tf.data.experimental.get_single_element(ds)),\n        axis=-1\n      )\n\n  preprocessing_model = PreprocessingModel(model)\n  your_exported_model_dir = ... # save the model to this path.\n  tf.saved_model.save(preprocessing_model, your_exported_model_dir,\n                signatures={\'serving_default\': preprocessing_model.serving_fn})\n  ```\n\n  # Estimator\n\n  In the case of estimators, you need to generally define a `serving_input_fn`\n  which would require the features to be processed by the model while\n  inferencing.\n\n  ```python\n  def serving_input_fn():\n\n    raw_feature_spec = ... # Spec for the raw_features\n    input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n        raw_feature_spec, default_batch_size=None)\n    )\n    serving_input_receiver = input_fn()\n    raw_features = serving_input_receiver.features\n\n    def preprocessing_fn(raw_feature):\n      # ... the raw_feature is preprocessed as per the use-case\n      return feature\n\n    dataset = (tf.data.Dataset.from_tensor_slices(raw_features)\n              .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n              .batch(BATCH_SIZE))\n\n    processed_features = tf.data.experimental.get_single_element(dataset)\n\n    # Please note that the value of `BATCH_SIZE` should be equal to\n    # the size of the leading dimension of `raw_features`. This ensures\n    # that `dataset` has only element, which is a pre-requisite for\n    # using `tf.data.experimental.get_single_element(dataset)`.\n\n    return tf.estimator.export.ServingInputReceiver(\n        processed_features, serving_input_receiver.receiver_tensors)\n\n  estimator = ... # A pre-built or custom estimator\n  estimator.export_saved_model(your_exported_model_dir, serving_input_fn)\n  ```\n\n  Args:\n    dataset: A `tf.data.Dataset` object containing a single element.\n\n  Returns:\n    A nested structure of `tf.Tensor` objects, corresponding to the single\n    element of `dataset`.\n\n  Raises:\n    TypeError: if `dataset` is not a `tf.data.Dataset` object.\n    InvalidArgumentError: (at runtime) if `dataset` does not contain exactly\n      one element.\n  '
    if not isinstance(dataset, data_types.DatasetV2):
        raise TypeError(f'Invalid `dataset`. Expected a `tf.data.Dataset` object but got {type(dataset)}.')
    return dataset.get_single_element()