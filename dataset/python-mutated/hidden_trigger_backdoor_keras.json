[
    {
        "func_name": "__init__",
        "original": "def __init__(self, classifier: Union['KerasClassifier', 'TensorFlowV2Classifier'], target: np.ndarray, source: np.ndarray, feature_layer: Union[str, int], backdoor: PoisoningAttackBackdoor, eps: float=0.1, learning_rate: float=0.001, decay_coeff: float=0.95, decay_iter: Union[int, List[int]]=2000, stopping_threshold: float=10, max_iter: int=5000, batch_size: float=100, poison_percent: float=0.1, is_index: bool=False, verbose: bool=True, print_iter: int=100) -> None:\n    \"\"\"\n        Creates a new Hidden Trigger Backdoor poisoning attack for Keras and TensorflowV2.\n\n        :param classifier: A trained neural network classifier.\n        :param target: The target class/indices to poison. Triggers added to inputs not in the target class will\n                       result in misclassifications to the target class. If an int, it represents a label.\n                       Otherwise, it is an array of indices.\n        :param source: The class/indices which will have a trigger added to cause misclassification\n                       If an int, it represents a label. Otherwise, it is an array of indices.\n        :param feature_layer: The name of the feature representation layer.\n        :param backdoor: A PoisoningAttackBackdoor that adds a backdoor trigger to the input.\n        :param eps: Maximum perturbation that the attacker can introduce.\n        :param learning_rate: The learning rate of clean-label attack optimization.\n        :param decay_coeff: The decay coefficient of the learning rate.\n        :param decay_iter: The number of iterations before the learning rate decays\n        :param stopping_threshold: Stop iterations after loss is less than this threshold.\n        :param max_iter: The maximum number of iterations for the attack.\n        :param batch_size: The number of samples to draw per batch.\n        :param poison_percent: The percentage of the data to poison. This is ignored if indices are provided\n        :param is_index: If true, the source and target params are assumed to represent indices rather\n                         than a class label. poison_percent is ignored if true.\n        :param verbose: Show progress bars.\n        :print iter: The number of iterations to print the current loss progress.\n        \"\"\"\n    super().__init__(classifier=classifier)\n    self.target = target\n    self.source = source\n    self.feature_layer = feature_layer\n    self.backdoor = backdoor\n    self.eps = eps\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.decay_iter = decay_iter\n    self.stopping_threshold = stopping_threshold\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.poison_percent = poison_percent\n    self.is_index = is_index\n    self.verbose = verbose\n    self.print_iter = print_iter",
        "mutated": [
            "def __init__(self, classifier: Union['KerasClassifier', 'TensorFlowV2Classifier'], target: np.ndarray, source: np.ndarray, feature_layer: Union[str, int], backdoor: PoisoningAttackBackdoor, eps: float=0.1, learning_rate: float=0.001, decay_coeff: float=0.95, decay_iter: Union[int, List[int]]=2000, stopping_threshold: float=10, max_iter: int=5000, batch_size: float=100, poison_percent: float=0.1, is_index: bool=False, verbose: bool=True, print_iter: int=100) -> None:\n    if False:\n        i = 10\n    '\\n        Creates a new Hidden Trigger Backdoor poisoning attack for Keras and TensorflowV2.\\n\\n        :param classifier: A trained neural network classifier.\\n        :param target: The target class/indices to poison. Triggers added to inputs not in the target class will\\n                       result in misclassifications to the target class. If an int, it represents a label.\\n                       Otherwise, it is an array of indices.\\n        :param source: The class/indices which will have a trigger added to cause misclassification\\n                       If an int, it represents a label. Otherwise, it is an array of indices.\\n        :param feature_layer: The name of the feature representation layer.\\n        :param backdoor: A PoisoningAttackBackdoor that adds a backdoor trigger to the input.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param decay_iter: The number of iterations before the learning rate decays\\n        :param stopping_threshold: Stop iterations after loss is less than this threshold.\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param batch_size: The number of samples to draw per batch.\\n        :param poison_percent: The percentage of the data to poison. This is ignored if indices are provided\\n        :param is_index: If true, the source and target params are assumed to represent indices rather\\n                         than a class label. poison_percent is ignored if true.\\n        :param verbose: Show progress bars.\\n        :print iter: The number of iterations to print the current loss progress.\\n        '\n    super().__init__(classifier=classifier)\n    self.target = target\n    self.source = source\n    self.feature_layer = feature_layer\n    self.backdoor = backdoor\n    self.eps = eps\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.decay_iter = decay_iter\n    self.stopping_threshold = stopping_threshold\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.poison_percent = poison_percent\n    self.is_index = is_index\n    self.verbose = verbose\n    self.print_iter = print_iter",
            "def __init__(self, classifier: Union['KerasClassifier', 'TensorFlowV2Classifier'], target: np.ndarray, source: np.ndarray, feature_layer: Union[str, int], backdoor: PoisoningAttackBackdoor, eps: float=0.1, learning_rate: float=0.001, decay_coeff: float=0.95, decay_iter: Union[int, List[int]]=2000, stopping_threshold: float=10, max_iter: int=5000, batch_size: float=100, poison_percent: float=0.1, is_index: bool=False, verbose: bool=True, print_iter: int=100) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Creates a new Hidden Trigger Backdoor poisoning attack for Keras and TensorflowV2.\\n\\n        :param classifier: A trained neural network classifier.\\n        :param target: The target class/indices to poison. Triggers added to inputs not in the target class will\\n                       result in misclassifications to the target class. If an int, it represents a label.\\n                       Otherwise, it is an array of indices.\\n        :param source: The class/indices which will have a trigger added to cause misclassification\\n                       If an int, it represents a label. Otherwise, it is an array of indices.\\n        :param feature_layer: The name of the feature representation layer.\\n        :param backdoor: A PoisoningAttackBackdoor that adds a backdoor trigger to the input.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param decay_iter: The number of iterations before the learning rate decays\\n        :param stopping_threshold: Stop iterations after loss is less than this threshold.\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param batch_size: The number of samples to draw per batch.\\n        :param poison_percent: The percentage of the data to poison. This is ignored if indices are provided\\n        :param is_index: If true, the source and target params are assumed to represent indices rather\\n                         than a class label. poison_percent is ignored if true.\\n        :param verbose: Show progress bars.\\n        :print iter: The number of iterations to print the current loss progress.\\n        '\n    super().__init__(classifier=classifier)\n    self.target = target\n    self.source = source\n    self.feature_layer = feature_layer\n    self.backdoor = backdoor\n    self.eps = eps\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.decay_iter = decay_iter\n    self.stopping_threshold = stopping_threshold\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.poison_percent = poison_percent\n    self.is_index = is_index\n    self.verbose = verbose\n    self.print_iter = print_iter",
            "def __init__(self, classifier: Union['KerasClassifier', 'TensorFlowV2Classifier'], target: np.ndarray, source: np.ndarray, feature_layer: Union[str, int], backdoor: PoisoningAttackBackdoor, eps: float=0.1, learning_rate: float=0.001, decay_coeff: float=0.95, decay_iter: Union[int, List[int]]=2000, stopping_threshold: float=10, max_iter: int=5000, batch_size: float=100, poison_percent: float=0.1, is_index: bool=False, verbose: bool=True, print_iter: int=100) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Creates a new Hidden Trigger Backdoor poisoning attack for Keras and TensorflowV2.\\n\\n        :param classifier: A trained neural network classifier.\\n        :param target: The target class/indices to poison. Triggers added to inputs not in the target class will\\n                       result in misclassifications to the target class. If an int, it represents a label.\\n                       Otherwise, it is an array of indices.\\n        :param source: The class/indices which will have a trigger added to cause misclassification\\n                       If an int, it represents a label. Otherwise, it is an array of indices.\\n        :param feature_layer: The name of the feature representation layer.\\n        :param backdoor: A PoisoningAttackBackdoor that adds a backdoor trigger to the input.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param decay_iter: The number of iterations before the learning rate decays\\n        :param stopping_threshold: Stop iterations after loss is less than this threshold.\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param batch_size: The number of samples to draw per batch.\\n        :param poison_percent: The percentage of the data to poison. This is ignored if indices are provided\\n        :param is_index: If true, the source and target params are assumed to represent indices rather\\n                         than a class label. poison_percent is ignored if true.\\n        :param verbose: Show progress bars.\\n        :print iter: The number of iterations to print the current loss progress.\\n        '\n    super().__init__(classifier=classifier)\n    self.target = target\n    self.source = source\n    self.feature_layer = feature_layer\n    self.backdoor = backdoor\n    self.eps = eps\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.decay_iter = decay_iter\n    self.stopping_threshold = stopping_threshold\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.poison_percent = poison_percent\n    self.is_index = is_index\n    self.verbose = verbose\n    self.print_iter = print_iter",
            "def __init__(self, classifier: Union['KerasClassifier', 'TensorFlowV2Classifier'], target: np.ndarray, source: np.ndarray, feature_layer: Union[str, int], backdoor: PoisoningAttackBackdoor, eps: float=0.1, learning_rate: float=0.001, decay_coeff: float=0.95, decay_iter: Union[int, List[int]]=2000, stopping_threshold: float=10, max_iter: int=5000, batch_size: float=100, poison_percent: float=0.1, is_index: bool=False, verbose: bool=True, print_iter: int=100) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Creates a new Hidden Trigger Backdoor poisoning attack for Keras and TensorflowV2.\\n\\n        :param classifier: A trained neural network classifier.\\n        :param target: The target class/indices to poison. Triggers added to inputs not in the target class will\\n                       result in misclassifications to the target class. If an int, it represents a label.\\n                       Otherwise, it is an array of indices.\\n        :param source: The class/indices which will have a trigger added to cause misclassification\\n                       If an int, it represents a label. Otherwise, it is an array of indices.\\n        :param feature_layer: The name of the feature representation layer.\\n        :param backdoor: A PoisoningAttackBackdoor that adds a backdoor trigger to the input.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param decay_iter: The number of iterations before the learning rate decays\\n        :param stopping_threshold: Stop iterations after loss is less than this threshold.\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param batch_size: The number of samples to draw per batch.\\n        :param poison_percent: The percentage of the data to poison. This is ignored if indices are provided\\n        :param is_index: If true, the source and target params are assumed to represent indices rather\\n                         than a class label. poison_percent is ignored if true.\\n        :param verbose: Show progress bars.\\n        :print iter: The number of iterations to print the current loss progress.\\n        '\n    super().__init__(classifier=classifier)\n    self.target = target\n    self.source = source\n    self.feature_layer = feature_layer\n    self.backdoor = backdoor\n    self.eps = eps\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.decay_iter = decay_iter\n    self.stopping_threshold = stopping_threshold\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.poison_percent = poison_percent\n    self.is_index = is_index\n    self.verbose = verbose\n    self.print_iter = print_iter",
            "def __init__(self, classifier: Union['KerasClassifier', 'TensorFlowV2Classifier'], target: np.ndarray, source: np.ndarray, feature_layer: Union[str, int], backdoor: PoisoningAttackBackdoor, eps: float=0.1, learning_rate: float=0.001, decay_coeff: float=0.95, decay_iter: Union[int, List[int]]=2000, stopping_threshold: float=10, max_iter: int=5000, batch_size: float=100, poison_percent: float=0.1, is_index: bool=False, verbose: bool=True, print_iter: int=100) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Creates a new Hidden Trigger Backdoor poisoning attack for Keras and TensorflowV2.\\n\\n        :param classifier: A trained neural network classifier.\\n        :param target: The target class/indices to poison. Triggers added to inputs not in the target class will\\n                       result in misclassifications to the target class. If an int, it represents a label.\\n                       Otherwise, it is an array of indices.\\n        :param source: The class/indices which will have a trigger added to cause misclassification\\n                       If an int, it represents a label. Otherwise, it is an array of indices.\\n        :param feature_layer: The name of the feature representation layer.\\n        :param backdoor: A PoisoningAttackBackdoor that adds a backdoor trigger to the input.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param decay_iter: The number of iterations before the learning rate decays\\n        :param stopping_threshold: Stop iterations after loss is less than this threshold.\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param batch_size: The number of samples to draw per batch.\\n        :param poison_percent: The percentage of the data to poison. This is ignored if indices are provided\\n        :param is_index: If true, the source and target params are assumed to represent indices rather\\n                         than a class label. poison_percent is ignored if true.\\n        :param verbose: Show progress bars.\\n        :print iter: The number of iterations to print the current loss progress.\\n        '\n    super().__init__(classifier=classifier)\n    self.target = target\n    self.source = source\n    self.feature_layer = feature_layer\n    self.backdoor = backdoor\n    self.eps = eps\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.decay_iter = decay_iter\n    self.stopping_threshold = stopping_threshold\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.poison_percent = poison_percent\n    self.is_index = is_index\n    self.verbose = verbose\n    self.print_iter = print_iter"
        ]
    },
    {
        "func_name": "poison",
        "original": "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        Calls perturbation function on the dataset x and returns only the perturbed input and their\n        indices in the dataset.\n        :param x: An array in the shape NxWxHxC with the points to draw source and target samples from.\n                  Source indicates the class(es) that the backdoor would be added to to cause\n                  misclassification into the target label.\n                  Target indicates the class that the backdoor should cause misclassification into.\n        :param y: The labels of the provided samples. If none, we will use the classifier to label the\n                  data.\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\n        \"\"\"\n    import tensorflow as tf\n    from scipy.spatial import distance\n    if isinstance(self.estimator, KerasClassifier):\n        if not self.estimator.is_tensorflow:\n            import keras.backend as k\n        else:\n            import tensorflow.keras.backend as k\n    data = np.copy(x)\n    if y is None:\n        estimated_labels = self.estimator.predict(data)\n    else:\n        estimated_labels = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if not self.is_index:\n        poison_class = self.target\n        poison_indices = np.where(np.all(estimated_labels == poison_class, axis=1))[0]\n        num_poison = int(np.ceil(self.poison_percent * len(poison_indices)))\n        if num_poison == 0:\n            raise ValueError('No data points with target label found')\n        poison_indices = np.random.choice(poison_indices, num_poison, replace=False)\n    else:\n        poison_class = estimated_labels[self.target[0]]\n        poison_indices = self.target\n        if not np.all(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('The target indices do not share the same label')\n        num_poison = len(poison_indices)\n    if not self.is_index:\n        trigger_class = self.source\n        trigger_indices = np.where(np.all(estimated_labels == trigger_class, axis=1))[0]\n        num_trigger = min(len(trigger_indices), num_poison)\n        if num_trigger == 0:\n            raise ValueError('No data points with source label found')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n        trigger_indices = np.random.choice(trigger_indices, num_poison, replace=False)\n        num_trigger = len(trigger_indices)\n    else:\n        trigger_indices = self.source\n        num_trigger = len(trigger_indices)\n        if np.any(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('There is a source class that is labeled as the target indices')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n    logger.info('Number of poison inputs: %d', num_poison)\n    logger.info('Number of trigger inputs: %d', num_trigger)\n    batches = int(np.ceil(num_poison / float(self.batch_size)))\n    losses = LossMeter()\n    final_poison = np.copy(data[poison_indices])\n    original_images = np.copy(data[poison_indices])\n    for batch_id in trange(batches, desc='Hidden Trigger', disable=not self.verbose):\n        cur_index = self.batch_size * batch_id\n        offset = min(self.batch_size, num_poison - cur_index)\n        poison_batch_indices = poison_indices[cur_index:cur_index + offset]\n        trigger_batch_indices = trigger_indices[cur_index:cur_index + offset]\n        poison_samples = data[poison_batch_indices]\n        (trigger_samples, _) = self.backdoor.poison(data[trigger_batch_indices], self.target, broadcast=True)\n        feat1 = self.estimator.get_activations(trigger_samples, self.feature_layer)\n        feat1 = np.copy(feat1)\n        for i in range(self.max_iter):\n            if isinstance(self.decay_iter, int):\n                decay_exp = i // self.decay_iter\n            else:\n                max_index = [ii for (ii, _) in enumerate(self.decay_iter) if self.decay_iter[ii] <= i]\n                if len(max_index) == 0:\n                    decay_exp = 0\n                else:\n                    decay_exp = max(max_index) + 1\n            learning_rate = self.learning_rate * self.decay_coeff ** decay_exp\n            feat2 = self.estimator.get_activations(poison_samples, self.feature_layer)\n            feat11 = np.copy(feat1)\n            dist = distance.cdist(feat1, feat2, 'minkowski')\n            for _ in range(len(feat2)):\n                min_index = np.squeeze((dist == np.min(dist)).nonzero())\n                feat1[min_index[1]] = feat11[min_index[0]]\n                dist[min_index[0], min_index[1]] = 100000.0\n            loss = np.linalg.norm(feat1 - feat2) ** 2\n            losses.update(loss, len(trigger_samples))\n            if isinstance(self.estimator, KerasClassifier):\n                if not hasattr(self, '_custom_loss'):\n                    self._custom_loss = {}\n                    feat1_var = k.variable(feat1)\n                    self._custom_loss['feat_var'] = feat1_var\n                    output_tensor = self._get_keras_tensor()\n                    attack_loss = tf.math.square(tf.norm(feat1_var - output_tensor))\n                    attack_grad_f = k.gradients(attack_loss, self.estimator._input)[0]\n                    self._custom_loss['loss_function'] = k.function([self.estimator._input, k.learning_phase()], [attack_grad_f])\n                else:\n                    feat1_var = self._custom_loss['feat_var']\n                k.set_value(feat1_var, feat1)\n                preprocessed_poison_samples = self._apply_preprocessing(poison_samples)\n                attack_grad = self._custom_loss['loss_function']([preprocessed_poison_samples, 0])[0]\n            else:\n                poison_tensor = tf.convert_to_tensor(poison_samples)\n                with tf.GradientTape() as tape:\n                    tape.watch(poison_tensor)\n                    feat2_tensor = self.estimator.get_activations(poison_tensor, 9, 1, framework=True)\n                    attack_loss = tf.math.square(tf.norm(feat1 - feat2_tensor))\n                attack_grad = tape.gradient(attack_loss, poison_tensor).numpy()\n            poison_samples = poison_samples - learning_rate * attack_grad\n            pert = poison_samples - original_images[cur_index:cur_index + offset]\n            pert = np.clip(pert, -self.eps, self.eps)\n            poison_samples = pert + original_images[cur_index:cur_index + offset]\n            poison_samples = np.clip(poison_samples, *self.estimator.clip_values)\n            if i % self.print_iter == 0:\n                print(f'Batch: {batch_id} | i: {i:5d} |                         LR: {learning_rate:2.5f} |                         Loss Val: {losses.val:5.3f} | Loss Avg: {losses.avg:5.3f}')\n            if loss < self.stopping_threshold or i == self.max_iter - 1:\n                print(f'Max_Loss: {loss}')\n                final_poison[cur_index:cur_index + offset] = poison_samples\n                break\n    return (final_poison, poison_indices)",
        "mutated": [
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n    '\\n        Calls perturbation function on the dataset x and returns only the perturbed input and their\\n        indices in the dataset.\\n        :param x: An array in the shape NxWxHxC with the points to draw source and target samples from.\\n                  Source indicates the class(es) that the backdoor would be added to to cause\\n                  misclassification into the target label.\\n                  Target indicates the class that the backdoor should cause misclassification into.\\n        :param y: The labels of the provided samples. If none, we will use the classifier to label the\\n                  data.\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    import tensorflow as tf\n    from scipy.spatial import distance\n    if isinstance(self.estimator, KerasClassifier):\n        if not self.estimator.is_tensorflow:\n            import keras.backend as k\n        else:\n            import tensorflow.keras.backend as k\n    data = np.copy(x)\n    if y is None:\n        estimated_labels = self.estimator.predict(data)\n    else:\n        estimated_labels = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if not self.is_index:\n        poison_class = self.target\n        poison_indices = np.where(np.all(estimated_labels == poison_class, axis=1))[0]\n        num_poison = int(np.ceil(self.poison_percent * len(poison_indices)))\n        if num_poison == 0:\n            raise ValueError('No data points with target label found')\n        poison_indices = np.random.choice(poison_indices, num_poison, replace=False)\n    else:\n        poison_class = estimated_labels[self.target[0]]\n        poison_indices = self.target\n        if not np.all(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('The target indices do not share the same label')\n        num_poison = len(poison_indices)\n    if not self.is_index:\n        trigger_class = self.source\n        trigger_indices = np.where(np.all(estimated_labels == trigger_class, axis=1))[0]\n        num_trigger = min(len(trigger_indices), num_poison)\n        if num_trigger == 0:\n            raise ValueError('No data points with source label found')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n        trigger_indices = np.random.choice(trigger_indices, num_poison, replace=False)\n        num_trigger = len(trigger_indices)\n    else:\n        trigger_indices = self.source\n        num_trigger = len(trigger_indices)\n        if np.any(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('There is a source class that is labeled as the target indices')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n    logger.info('Number of poison inputs: %d', num_poison)\n    logger.info('Number of trigger inputs: %d', num_trigger)\n    batches = int(np.ceil(num_poison / float(self.batch_size)))\n    losses = LossMeter()\n    final_poison = np.copy(data[poison_indices])\n    original_images = np.copy(data[poison_indices])\n    for batch_id in trange(batches, desc='Hidden Trigger', disable=not self.verbose):\n        cur_index = self.batch_size * batch_id\n        offset = min(self.batch_size, num_poison - cur_index)\n        poison_batch_indices = poison_indices[cur_index:cur_index + offset]\n        trigger_batch_indices = trigger_indices[cur_index:cur_index + offset]\n        poison_samples = data[poison_batch_indices]\n        (trigger_samples, _) = self.backdoor.poison(data[trigger_batch_indices], self.target, broadcast=True)\n        feat1 = self.estimator.get_activations(trigger_samples, self.feature_layer)\n        feat1 = np.copy(feat1)\n        for i in range(self.max_iter):\n            if isinstance(self.decay_iter, int):\n                decay_exp = i // self.decay_iter\n            else:\n                max_index = [ii for (ii, _) in enumerate(self.decay_iter) if self.decay_iter[ii] <= i]\n                if len(max_index) == 0:\n                    decay_exp = 0\n                else:\n                    decay_exp = max(max_index) + 1\n            learning_rate = self.learning_rate * self.decay_coeff ** decay_exp\n            feat2 = self.estimator.get_activations(poison_samples, self.feature_layer)\n            feat11 = np.copy(feat1)\n            dist = distance.cdist(feat1, feat2, 'minkowski')\n            for _ in range(len(feat2)):\n                min_index = np.squeeze((dist == np.min(dist)).nonzero())\n                feat1[min_index[1]] = feat11[min_index[0]]\n                dist[min_index[0], min_index[1]] = 100000.0\n            loss = np.linalg.norm(feat1 - feat2) ** 2\n            losses.update(loss, len(trigger_samples))\n            if isinstance(self.estimator, KerasClassifier):\n                if not hasattr(self, '_custom_loss'):\n                    self._custom_loss = {}\n                    feat1_var = k.variable(feat1)\n                    self._custom_loss['feat_var'] = feat1_var\n                    output_tensor = self._get_keras_tensor()\n                    attack_loss = tf.math.square(tf.norm(feat1_var - output_tensor))\n                    attack_grad_f = k.gradients(attack_loss, self.estimator._input)[0]\n                    self._custom_loss['loss_function'] = k.function([self.estimator._input, k.learning_phase()], [attack_grad_f])\n                else:\n                    feat1_var = self._custom_loss['feat_var']\n                k.set_value(feat1_var, feat1)\n                preprocessed_poison_samples = self._apply_preprocessing(poison_samples)\n                attack_grad = self._custom_loss['loss_function']([preprocessed_poison_samples, 0])[0]\n            else:\n                poison_tensor = tf.convert_to_tensor(poison_samples)\n                with tf.GradientTape() as tape:\n                    tape.watch(poison_tensor)\n                    feat2_tensor = self.estimator.get_activations(poison_tensor, 9, 1, framework=True)\n                    attack_loss = tf.math.square(tf.norm(feat1 - feat2_tensor))\n                attack_grad = tape.gradient(attack_loss, poison_tensor).numpy()\n            poison_samples = poison_samples - learning_rate * attack_grad\n            pert = poison_samples - original_images[cur_index:cur_index + offset]\n            pert = np.clip(pert, -self.eps, self.eps)\n            poison_samples = pert + original_images[cur_index:cur_index + offset]\n            poison_samples = np.clip(poison_samples, *self.estimator.clip_values)\n            if i % self.print_iter == 0:\n                print(f'Batch: {batch_id} | i: {i:5d} |                         LR: {learning_rate:2.5f} |                         Loss Val: {losses.val:5.3f} | Loss Avg: {losses.avg:5.3f}')\n            if loss < self.stopping_threshold or i == self.max_iter - 1:\n                print(f'Max_Loss: {loss}')\n                final_poison[cur_index:cur_index + offset] = poison_samples\n                break\n    return (final_poison, poison_indices)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Calls perturbation function on the dataset x and returns only the perturbed input and their\\n        indices in the dataset.\\n        :param x: An array in the shape NxWxHxC with the points to draw source and target samples from.\\n                  Source indicates the class(es) that the backdoor would be added to to cause\\n                  misclassification into the target label.\\n                  Target indicates the class that the backdoor should cause misclassification into.\\n        :param y: The labels of the provided samples. If none, we will use the classifier to label the\\n                  data.\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    import tensorflow as tf\n    from scipy.spatial import distance\n    if isinstance(self.estimator, KerasClassifier):\n        if not self.estimator.is_tensorflow:\n            import keras.backend as k\n        else:\n            import tensorflow.keras.backend as k\n    data = np.copy(x)\n    if y is None:\n        estimated_labels = self.estimator.predict(data)\n    else:\n        estimated_labels = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if not self.is_index:\n        poison_class = self.target\n        poison_indices = np.where(np.all(estimated_labels == poison_class, axis=1))[0]\n        num_poison = int(np.ceil(self.poison_percent * len(poison_indices)))\n        if num_poison == 0:\n            raise ValueError('No data points with target label found')\n        poison_indices = np.random.choice(poison_indices, num_poison, replace=False)\n    else:\n        poison_class = estimated_labels[self.target[0]]\n        poison_indices = self.target\n        if not np.all(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('The target indices do not share the same label')\n        num_poison = len(poison_indices)\n    if not self.is_index:\n        trigger_class = self.source\n        trigger_indices = np.where(np.all(estimated_labels == trigger_class, axis=1))[0]\n        num_trigger = min(len(trigger_indices), num_poison)\n        if num_trigger == 0:\n            raise ValueError('No data points with source label found')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n        trigger_indices = np.random.choice(trigger_indices, num_poison, replace=False)\n        num_trigger = len(trigger_indices)\n    else:\n        trigger_indices = self.source\n        num_trigger = len(trigger_indices)\n        if np.any(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('There is a source class that is labeled as the target indices')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n    logger.info('Number of poison inputs: %d', num_poison)\n    logger.info('Number of trigger inputs: %d', num_trigger)\n    batches = int(np.ceil(num_poison / float(self.batch_size)))\n    losses = LossMeter()\n    final_poison = np.copy(data[poison_indices])\n    original_images = np.copy(data[poison_indices])\n    for batch_id in trange(batches, desc='Hidden Trigger', disable=not self.verbose):\n        cur_index = self.batch_size * batch_id\n        offset = min(self.batch_size, num_poison - cur_index)\n        poison_batch_indices = poison_indices[cur_index:cur_index + offset]\n        trigger_batch_indices = trigger_indices[cur_index:cur_index + offset]\n        poison_samples = data[poison_batch_indices]\n        (trigger_samples, _) = self.backdoor.poison(data[trigger_batch_indices], self.target, broadcast=True)\n        feat1 = self.estimator.get_activations(trigger_samples, self.feature_layer)\n        feat1 = np.copy(feat1)\n        for i in range(self.max_iter):\n            if isinstance(self.decay_iter, int):\n                decay_exp = i // self.decay_iter\n            else:\n                max_index = [ii for (ii, _) in enumerate(self.decay_iter) if self.decay_iter[ii] <= i]\n                if len(max_index) == 0:\n                    decay_exp = 0\n                else:\n                    decay_exp = max(max_index) + 1\n            learning_rate = self.learning_rate * self.decay_coeff ** decay_exp\n            feat2 = self.estimator.get_activations(poison_samples, self.feature_layer)\n            feat11 = np.copy(feat1)\n            dist = distance.cdist(feat1, feat2, 'minkowski')\n            for _ in range(len(feat2)):\n                min_index = np.squeeze((dist == np.min(dist)).nonzero())\n                feat1[min_index[1]] = feat11[min_index[0]]\n                dist[min_index[0], min_index[1]] = 100000.0\n            loss = np.linalg.norm(feat1 - feat2) ** 2\n            losses.update(loss, len(trigger_samples))\n            if isinstance(self.estimator, KerasClassifier):\n                if not hasattr(self, '_custom_loss'):\n                    self._custom_loss = {}\n                    feat1_var = k.variable(feat1)\n                    self._custom_loss['feat_var'] = feat1_var\n                    output_tensor = self._get_keras_tensor()\n                    attack_loss = tf.math.square(tf.norm(feat1_var - output_tensor))\n                    attack_grad_f = k.gradients(attack_loss, self.estimator._input)[0]\n                    self._custom_loss['loss_function'] = k.function([self.estimator._input, k.learning_phase()], [attack_grad_f])\n                else:\n                    feat1_var = self._custom_loss['feat_var']\n                k.set_value(feat1_var, feat1)\n                preprocessed_poison_samples = self._apply_preprocessing(poison_samples)\n                attack_grad = self._custom_loss['loss_function']([preprocessed_poison_samples, 0])[0]\n            else:\n                poison_tensor = tf.convert_to_tensor(poison_samples)\n                with tf.GradientTape() as tape:\n                    tape.watch(poison_tensor)\n                    feat2_tensor = self.estimator.get_activations(poison_tensor, 9, 1, framework=True)\n                    attack_loss = tf.math.square(tf.norm(feat1 - feat2_tensor))\n                attack_grad = tape.gradient(attack_loss, poison_tensor).numpy()\n            poison_samples = poison_samples - learning_rate * attack_grad\n            pert = poison_samples - original_images[cur_index:cur_index + offset]\n            pert = np.clip(pert, -self.eps, self.eps)\n            poison_samples = pert + original_images[cur_index:cur_index + offset]\n            poison_samples = np.clip(poison_samples, *self.estimator.clip_values)\n            if i % self.print_iter == 0:\n                print(f'Batch: {batch_id} | i: {i:5d} |                         LR: {learning_rate:2.5f} |                         Loss Val: {losses.val:5.3f} | Loss Avg: {losses.avg:5.3f}')\n            if loss < self.stopping_threshold or i == self.max_iter - 1:\n                print(f'Max_Loss: {loss}')\n                final_poison[cur_index:cur_index + offset] = poison_samples\n                break\n    return (final_poison, poison_indices)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Calls perturbation function on the dataset x and returns only the perturbed input and their\\n        indices in the dataset.\\n        :param x: An array in the shape NxWxHxC with the points to draw source and target samples from.\\n                  Source indicates the class(es) that the backdoor would be added to to cause\\n                  misclassification into the target label.\\n                  Target indicates the class that the backdoor should cause misclassification into.\\n        :param y: The labels of the provided samples. If none, we will use the classifier to label the\\n                  data.\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    import tensorflow as tf\n    from scipy.spatial import distance\n    if isinstance(self.estimator, KerasClassifier):\n        if not self.estimator.is_tensorflow:\n            import keras.backend as k\n        else:\n            import tensorflow.keras.backend as k\n    data = np.copy(x)\n    if y is None:\n        estimated_labels = self.estimator.predict(data)\n    else:\n        estimated_labels = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if not self.is_index:\n        poison_class = self.target\n        poison_indices = np.where(np.all(estimated_labels == poison_class, axis=1))[0]\n        num_poison = int(np.ceil(self.poison_percent * len(poison_indices)))\n        if num_poison == 0:\n            raise ValueError('No data points with target label found')\n        poison_indices = np.random.choice(poison_indices, num_poison, replace=False)\n    else:\n        poison_class = estimated_labels[self.target[0]]\n        poison_indices = self.target\n        if not np.all(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('The target indices do not share the same label')\n        num_poison = len(poison_indices)\n    if not self.is_index:\n        trigger_class = self.source\n        trigger_indices = np.where(np.all(estimated_labels == trigger_class, axis=1))[0]\n        num_trigger = min(len(trigger_indices), num_poison)\n        if num_trigger == 0:\n            raise ValueError('No data points with source label found')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n        trigger_indices = np.random.choice(trigger_indices, num_poison, replace=False)\n        num_trigger = len(trigger_indices)\n    else:\n        trigger_indices = self.source\n        num_trigger = len(trigger_indices)\n        if np.any(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('There is a source class that is labeled as the target indices')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n    logger.info('Number of poison inputs: %d', num_poison)\n    logger.info('Number of trigger inputs: %d', num_trigger)\n    batches = int(np.ceil(num_poison / float(self.batch_size)))\n    losses = LossMeter()\n    final_poison = np.copy(data[poison_indices])\n    original_images = np.copy(data[poison_indices])\n    for batch_id in trange(batches, desc='Hidden Trigger', disable=not self.verbose):\n        cur_index = self.batch_size * batch_id\n        offset = min(self.batch_size, num_poison - cur_index)\n        poison_batch_indices = poison_indices[cur_index:cur_index + offset]\n        trigger_batch_indices = trigger_indices[cur_index:cur_index + offset]\n        poison_samples = data[poison_batch_indices]\n        (trigger_samples, _) = self.backdoor.poison(data[trigger_batch_indices], self.target, broadcast=True)\n        feat1 = self.estimator.get_activations(trigger_samples, self.feature_layer)\n        feat1 = np.copy(feat1)\n        for i in range(self.max_iter):\n            if isinstance(self.decay_iter, int):\n                decay_exp = i // self.decay_iter\n            else:\n                max_index = [ii for (ii, _) in enumerate(self.decay_iter) if self.decay_iter[ii] <= i]\n                if len(max_index) == 0:\n                    decay_exp = 0\n                else:\n                    decay_exp = max(max_index) + 1\n            learning_rate = self.learning_rate * self.decay_coeff ** decay_exp\n            feat2 = self.estimator.get_activations(poison_samples, self.feature_layer)\n            feat11 = np.copy(feat1)\n            dist = distance.cdist(feat1, feat2, 'minkowski')\n            for _ in range(len(feat2)):\n                min_index = np.squeeze((dist == np.min(dist)).nonzero())\n                feat1[min_index[1]] = feat11[min_index[0]]\n                dist[min_index[0], min_index[1]] = 100000.0\n            loss = np.linalg.norm(feat1 - feat2) ** 2\n            losses.update(loss, len(trigger_samples))\n            if isinstance(self.estimator, KerasClassifier):\n                if not hasattr(self, '_custom_loss'):\n                    self._custom_loss = {}\n                    feat1_var = k.variable(feat1)\n                    self._custom_loss['feat_var'] = feat1_var\n                    output_tensor = self._get_keras_tensor()\n                    attack_loss = tf.math.square(tf.norm(feat1_var - output_tensor))\n                    attack_grad_f = k.gradients(attack_loss, self.estimator._input)[0]\n                    self._custom_loss['loss_function'] = k.function([self.estimator._input, k.learning_phase()], [attack_grad_f])\n                else:\n                    feat1_var = self._custom_loss['feat_var']\n                k.set_value(feat1_var, feat1)\n                preprocessed_poison_samples = self._apply_preprocessing(poison_samples)\n                attack_grad = self._custom_loss['loss_function']([preprocessed_poison_samples, 0])[0]\n            else:\n                poison_tensor = tf.convert_to_tensor(poison_samples)\n                with tf.GradientTape() as tape:\n                    tape.watch(poison_tensor)\n                    feat2_tensor = self.estimator.get_activations(poison_tensor, 9, 1, framework=True)\n                    attack_loss = tf.math.square(tf.norm(feat1 - feat2_tensor))\n                attack_grad = tape.gradient(attack_loss, poison_tensor).numpy()\n            poison_samples = poison_samples - learning_rate * attack_grad\n            pert = poison_samples - original_images[cur_index:cur_index + offset]\n            pert = np.clip(pert, -self.eps, self.eps)\n            poison_samples = pert + original_images[cur_index:cur_index + offset]\n            poison_samples = np.clip(poison_samples, *self.estimator.clip_values)\n            if i % self.print_iter == 0:\n                print(f'Batch: {batch_id} | i: {i:5d} |                         LR: {learning_rate:2.5f} |                         Loss Val: {losses.val:5.3f} | Loss Avg: {losses.avg:5.3f}')\n            if loss < self.stopping_threshold or i == self.max_iter - 1:\n                print(f'Max_Loss: {loss}')\n                final_poison[cur_index:cur_index + offset] = poison_samples\n                break\n    return (final_poison, poison_indices)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Calls perturbation function on the dataset x and returns only the perturbed input and their\\n        indices in the dataset.\\n        :param x: An array in the shape NxWxHxC with the points to draw source and target samples from.\\n                  Source indicates the class(es) that the backdoor would be added to to cause\\n                  misclassification into the target label.\\n                  Target indicates the class that the backdoor should cause misclassification into.\\n        :param y: The labels of the provided samples. If none, we will use the classifier to label the\\n                  data.\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    import tensorflow as tf\n    from scipy.spatial import distance\n    if isinstance(self.estimator, KerasClassifier):\n        if not self.estimator.is_tensorflow:\n            import keras.backend as k\n        else:\n            import tensorflow.keras.backend as k\n    data = np.copy(x)\n    if y is None:\n        estimated_labels = self.estimator.predict(data)\n    else:\n        estimated_labels = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if not self.is_index:\n        poison_class = self.target\n        poison_indices = np.where(np.all(estimated_labels == poison_class, axis=1))[0]\n        num_poison = int(np.ceil(self.poison_percent * len(poison_indices)))\n        if num_poison == 0:\n            raise ValueError('No data points with target label found')\n        poison_indices = np.random.choice(poison_indices, num_poison, replace=False)\n    else:\n        poison_class = estimated_labels[self.target[0]]\n        poison_indices = self.target\n        if not np.all(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('The target indices do not share the same label')\n        num_poison = len(poison_indices)\n    if not self.is_index:\n        trigger_class = self.source\n        trigger_indices = np.where(np.all(estimated_labels == trigger_class, axis=1))[0]\n        num_trigger = min(len(trigger_indices), num_poison)\n        if num_trigger == 0:\n            raise ValueError('No data points with source label found')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n        trigger_indices = np.random.choice(trigger_indices, num_poison, replace=False)\n        num_trigger = len(trigger_indices)\n    else:\n        trigger_indices = self.source\n        num_trigger = len(trigger_indices)\n        if np.any(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('There is a source class that is labeled as the target indices')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n    logger.info('Number of poison inputs: %d', num_poison)\n    logger.info('Number of trigger inputs: %d', num_trigger)\n    batches = int(np.ceil(num_poison / float(self.batch_size)))\n    losses = LossMeter()\n    final_poison = np.copy(data[poison_indices])\n    original_images = np.copy(data[poison_indices])\n    for batch_id in trange(batches, desc='Hidden Trigger', disable=not self.verbose):\n        cur_index = self.batch_size * batch_id\n        offset = min(self.batch_size, num_poison - cur_index)\n        poison_batch_indices = poison_indices[cur_index:cur_index + offset]\n        trigger_batch_indices = trigger_indices[cur_index:cur_index + offset]\n        poison_samples = data[poison_batch_indices]\n        (trigger_samples, _) = self.backdoor.poison(data[trigger_batch_indices], self.target, broadcast=True)\n        feat1 = self.estimator.get_activations(trigger_samples, self.feature_layer)\n        feat1 = np.copy(feat1)\n        for i in range(self.max_iter):\n            if isinstance(self.decay_iter, int):\n                decay_exp = i // self.decay_iter\n            else:\n                max_index = [ii for (ii, _) in enumerate(self.decay_iter) if self.decay_iter[ii] <= i]\n                if len(max_index) == 0:\n                    decay_exp = 0\n                else:\n                    decay_exp = max(max_index) + 1\n            learning_rate = self.learning_rate * self.decay_coeff ** decay_exp\n            feat2 = self.estimator.get_activations(poison_samples, self.feature_layer)\n            feat11 = np.copy(feat1)\n            dist = distance.cdist(feat1, feat2, 'minkowski')\n            for _ in range(len(feat2)):\n                min_index = np.squeeze((dist == np.min(dist)).nonzero())\n                feat1[min_index[1]] = feat11[min_index[0]]\n                dist[min_index[0], min_index[1]] = 100000.0\n            loss = np.linalg.norm(feat1 - feat2) ** 2\n            losses.update(loss, len(trigger_samples))\n            if isinstance(self.estimator, KerasClassifier):\n                if not hasattr(self, '_custom_loss'):\n                    self._custom_loss = {}\n                    feat1_var = k.variable(feat1)\n                    self._custom_loss['feat_var'] = feat1_var\n                    output_tensor = self._get_keras_tensor()\n                    attack_loss = tf.math.square(tf.norm(feat1_var - output_tensor))\n                    attack_grad_f = k.gradients(attack_loss, self.estimator._input)[0]\n                    self._custom_loss['loss_function'] = k.function([self.estimator._input, k.learning_phase()], [attack_grad_f])\n                else:\n                    feat1_var = self._custom_loss['feat_var']\n                k.set_value(feat1_var, feat1)\n                preprocessed_poison_samples = self._apply_preprocessing(poison_samples)\n                attack_grad = self._custom_loss['loss_function']([preprocessed_poison_samples, 0])[0]\n            else:\n                poison_tensor = tf.convert_to_tensor(poison_samples)\n                with tf.GradientTape() as tape:\n                    tape.watch(poison_tensor)\n                    feat2_tensor = self.estimator.get_activations(poison_tensor, 9, 1, framework=True)\n                    attack_loss = tf.math.square(tf.norm(feat1 - feat2_tensor))\n                attack_grad = tape.gradient(attack_loss, poison_tensor).numpy()\n            poison_samples = poison_samples - learning_rate * attack_grad\n            pert = poison_samples - original_images[cur_index:cur_index + offset]\n            pert = np.clip(pert, -self.eps, self.eps)\n            poison_samples = pert + original_images[cur_index:cur_index + offset]\n            poison_samples = np.clip(poison_samples, *self.estimator.clip_values)\n            if i % self.print_iter == 0:\n                print(f'Batch: {batch_id} | i: {i:5d} |                         LR: {learning_rate:2.5f} |                         Loss Val: {losses.val:5.3f} | Loss Avg: {losses.avg:5.3f}')\n            if loss < self.stopping_threshold or i == self.max_iter - 1:\n                print(f'Max_Loss: {loss}')\n                final_poison[cur_index:cur_index + offset] = poison_samples\n                break\n    return (final_poison, poison_indices)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Calls perturbation function on the dataset x and returns only the perturbed input and their\\n        indices in the dataset.\\n        :param x: An array in the shape NxWxHxC with the points to draw source and target samples from.\\n                  Source indicates the class(es) that the backdoor would be added to to cause\\n                  misclassification into the target label.\\n                  Target indicates the class that the backdoor should cause misclassification into.\\n        :param y: The labels of the provided samples. If none, we will use the classifier to label the\\n                  data.\\n        :return: An tuple holding the `(poisoning_examples, poisoning_labels)`.\\n        '\n    import tensorflow as tf\n    from scipy.spatial import distance\n    if isinstance(self.estimator, KerasClassifier):\n        if not self.estimator.is_tensorflow:\n            import keras.backend as k\n        else:\n            import tensorflow.keras.backend as k\n    data = np.copy(x)\n    if y is None:\n        estimated_labels = self.estimator.predict(data)\n    else:\n        estimated_labels = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if not self.is_index:\n        poison_class = self.target\n        poison_indices = np.where(np.all(estimated_labels == poison_class, axis=1))[0]\n        num_poison = int(np.ceil(self.poison_percent * len(poison_indices)))\n        if num_poison == 0:\n            raise ValueError('No data points with target label found')\n        poison_indices = np.random.choice(poison_indices, num_poison, replace=False)\n    else:\n        poison_class = estimated_labels[self.target[0]]\n        poison_indices = self.target\n        if not np.all(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('The target indices do not share the same label')\n        num_poison = len(poison_indices)\n    if not self.is_index:\n        trigger_class = self.source\n        trigger_indices = np.where(np.all(estimated_labels == trigger_class, axis=1))[0]\n        num_trigger = min(len(trigger_indices), num_poison)\n        if num_trigger == 0:\n            raise ValueError('No data points with source label found')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n        trigger_indices = np.random.choice(trigger_indices, num_poison, replace=False)\n        num_trigger = len(trigger_indices)\n    else:\n        trigger_indices = self.source\n        num_trigger = len(trigger_indices)\n        if np.any(np.all(estimated_labels[poison_indices] == poison_class, axis=1)):\n            raise ValueError('There is a source class that is labeled as the target indices')\n        if num_trigger < num_poison:\n            raise ValueError('There must be at least as many images with the source label as the target.')\n    logger.info('Number of poison inputs: %d', num_poison)\n    logger.info('Number of trigger inputs: %d', num_trigger)\n    batches = int(np.ceil(num_poison / float(self.batch_size)))\n    losses = LossMeter()\n    final_poison = np.copy(data[poison_indices])\n    original_images = np.copy(data[poison_indices])\n    for batch_id in trange(batches, desc='Hidden Trigger', disable=not self.verbose):\n        cur_index = self.batch_size * batch_id\n        offset = min(self.batch_size, num_poison - cur_index)\n        poison_batch_indices = poison_indices[cur_index:cur_index + offset]\n        trigger_batch_indices = trigger_indices[cur_index:cur_index + offset]\n        poison_samples = data[poison_batch_indices]\n        (trigger_samples, _) = self.backdoor.poison(data[trigger_batch_indices], self.target, broadcast=True)\n        feat1 = self.estimator.get_activations(trigger_samples, self.feature_layer)\n        feat1 = np.copy(feat1)\n        for i in range(self.max_iter):\n            if isinstance(self.decay_iter, int):\n                decay_exp = i // self.decay_iter\n            else:\n                max_index = [ii for (ii, _) in enumerate(self.decay_iter) if self.decay_iter[ii] <= i]\n                if len(max_index) == 0:\n                    decay_exp = 0\n                else:\n                    decay_exp = max(max_index) + 1\n            learning_rate = self.learning_rate * self.decay_coeff ** decay_exp\n            feat2 = self.estimator.get_activations(poison_samples, self.feature_layer)\n            feat11 = np.copy(feat1)\n            dist = distance.cdist(feat1, feat2, 'minkowski')\n            for _ in range(len(feat2)):\n                min_index = np.squeeze((dist == np.min(dist)).nonzero())\n                feat1[min_index[1]] = feat11[min_index[0]]\n                dist[min_index[0], min_index[1]] = 100000.0\n            loss = np.linalg.norm(feat1 - feat2) ** 2\n            losses.update(loss, len(trigger_samples))\n            if isinstance(self.estimator, KerasClassifier):\n                if not hasattr(self, '_custom_loss'):\n                    self._custom_loss = {}\n                    feat1_var = k.variable(feat1)\n                    self._custom_loss['feat_var'] = feat1_var\n                    output_tensor = self._get_keras_tensor()\n                    attack_loss = tf.math.square(tf.norm(feat1_var - output_tensor))\n                    attack_grad_f = k.gradients(attack_loss, self.estimator._input)[0]\n                    self._custom_loss['loss_function'] = k.function([self.estimator._input, k.learning_phase()], [attack_grad_f])\n                else:\n                    feat1_var = self._custom_loss['feat_var']\n                k.set_value(feat1_var, feat1)\n                preprocessed_poison_samples = self._apply_preprocessing(poison_samples)\n                attack_grad = self._custom_loss['loss_function']([preprocessed_poison_samples, 0])[0]\n            else:\n                poison_tensor = tf.convert_to_tensor(poison_samples)\n                with tf.GradientTape() as tape:\n                    tape.watch(poison_tensor)\n                    feat2_tensor = self.estimator.get_activations(poison_tensor, 9, 1, framework=True)\n                    attack_loss = tf.math.square(tf.norm(feat1 - feat2_tensor))\n                attack_grad = tape.gradient(attack_loss, poison_tensor).numpy()\n            poison_samples = poison_samples - learning_rate * attack_grad\n            pert = poison_samples - original_images[cur_index:cur_index + offset]\n            pert = np.clip(pert, -self.eps, self.eps)\n            poison_samples = pert + original_images[cur_index:cur_index + offset]\n            poison_samples = np.clip(poison_samples, *self.estimator.clip_values)\n            if i % self.print_iter == 0:\n                print(f'Batch: {batch_id} | i: {i:5d} |                         LR: {learning_rate:2.5f} |                         Loss Val: {losses.val:5.3f} | Loss Avg: {losses.avg:5.3f}')\n            if loss < self.stopping_threshold or i == self.max_iter - 1:\n                print(f'Max_Loss: {loss}')\n                final_poison[cur_index:cur_index + offset] = poison_samples\n                break\n    return (final_poison, poison_indices)"
        ]
    },
    {
        "func_name": "_get_keras_tensor",
        "original": "def _get_keras_tensor(self):\n    \"\"\"\n        Helper function to get the feature layer output tensor in the keras graph\n        :return: Output tensor\n        \"\"\"\n    if self.estimator._layer_names is None:\n        raise ValueError('No layer names identified.')\n    if isinstance(self.feature_layer, six.string_types):\n        if self.feature_layer not in self.estimator._layer_names:\n            raise ValueError(f'Layer name {self.feature_layer} is not part of the graph.')\n        layer_name = self.feature_layer\n    elif isinstance(self.feature_layer, int):\n        if self.feature_layer < 0 or self.feature_layer >= len(self.estimator._layer_names):\n            raise ValueError(f'Layer index {self.feature_layer} is outside of range [0 to {len(self.estimator._layer_names) - 1}]).')\n        layer_name = self.estimator._layer_names[self.feature_layer]\n    else:\n        raise TypeError('Layer must be of type `str` or `int`.')\n    keras_layer = self.estimator._model.get_layer(layer_name)\n    num_inbound_nodes = len(getattr(keras_layer, '_inbound_nodes', []))\n    if num_inbound_nodes > 1:\n        layer_output = keras_layer.get_output_at(0)\n    else:\n        layer_output = keras_layer.output\n    return layer_output",
        "mutated": [
            "def _get_keras_tensor(self):\n    if False:\n        i = 10\n    '\\n        Helper function to get the feature layer output tensor in the keras graph\\n        :return: Output tensor\\n        '\n    if self.estimator._layer_names is None:\n        raise ValueError('No layer names identified.')\n    if isinstance(self.feature_layer, six.string_types):\n        if self.feature_layer not in self.estimator._layer_names:\n            raise ValueError(f'Layer name {self.feature_layer} is not part of the graph.')\n        layer_name = self.feature_layer\n    elif isinstance(self.feature_layer, int):\n        if self.feature_layer < 0 or self.feature_layer >= len(self.estimator._layer_names):\n            raise ValueError(f'Layer index {self.feature_layer} is outside of range [0 to {len(self.estimator._layer_names) - 1}]).')\n        layer_name = self.estimator._layer_names[self.feature_layer]\n    else:\n        raise TypeError('Layer must be of type `str` or `int`.')\n    keras_layer = self.estimator._model.get_layer(layer_name)\n    num_inbound_nodes = len(getattr(keras_layer, '_inbound_nodes', []))\n    if num_inbound_nodes > 1:\n        layer_output = keras_layer.get_output_at(0)\n    else:\n        layer_output = keras_layer.output\n    return layer_output",
            "def _get_keras_tensor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Helper function to get the feature layer output tensor in the keras graph\\n        :return: Output tensor\\n        '\n    if self.estimator._layer_names is None:\n        raise ValueError('No layer names identified.')\n    if isinstance(self.feature_layer, six.string_types):\n        if self.feature_layer not in self.estimator._layer_names:\n            raise ValueError(f'Layer name {self.feature_layer} is not part of the graph.')\n        layer_name = self.feature_layer\n    elif isinstance(self.feature_layer, int):\n        if self.feature_layer < 0 or self.feature_layer >= len(self.estimator._layer_names):\n            raise ValueError(f'Layer index {self.feature_layer} is outside of range [0 to {len(self.estimator._layer_names) - 1}]).')\n        layer_name = self.estimator._layer_names[self.feature_layer]\n    else:\n        raise TypeError('Layer must be of type `str` or `int`.')\n    keras_layer = self.estimator._model.get_layer(layer_name)\n    num_inbound_nodes = len(getattr(keras_layer, '_inbound_nodes', []))\n    if num_inbound_nodes > 1:\n        layer_output = keras_layer.get_output_at(0)\n    else:\n        layer_output = keras_layer.output\n    return layer_output",
            "def _get_keras_tensor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Helper function to get the feature layer output tensor in the keras graph\\n        :return: Output tensor\\n        '\n    if self.estimator._layer_names is None:\n        raise ValueError('No layer names identified.')\n    if isinstance(self.feature_layer, six.string_types):\n        if self.feature_layer not in self.estimator._layer_names:\n            raise ValueError(f'Layer name {self.feature_layer} is not part of the graph.')\n        layer_name = self.feature_layer\n    elif isinstance(self.feature_layer, int):\n        if self.feature_layer < 0 or self.feature_layer >= len(self.estimator._layer_names):\n            raise ValueError(f'Layer index {self.feature_layer} is outside of range [0 to {len(self.estimator._layer_names) - 1}]).')\n        layer_name = self.estimator._layer_names[self.feature_layer]\n    else:\n        raise TypeError('Layer must be of type `str` or `int`.')\n    keras_layer = self.estimator._model.get_layer(layer_name)\n    num_inbound_nodes = len(getattr(keras_layer, '_inbound_nodes', []))\n    if num_inbound_nodes > 1:\n        layer_output = keras_layer.get_output_at(0)\n    else:\n        layer_output = keras_layer.output\n    return layer_output",
            "def _get_keras_tensor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Helper function to get the feature layer output tensor in the keras graph\\n        :return: Output tensor\\n        '\n    if self.estimator._layer_names is None:\n        raise ValueError('No layer names identified.')\n    if isinstance(self.feature_layer, six.string_types):\n        if self.feature_layer not in self.estimator._layer_names:\n            raise ValueError(f'Layer name {self.feature_layer} is not part of the graph.')\n        layer_name = self.feature_layer\n    elif isinstance(self.feature_layer, int):\n        if self.feature_layer < 0 or self.feature_layer >= len(self.estimator._layer_names):\n            raise ValueError(f'Layer index {self.feature_layer} is outside of range [0 to {len(self.estimator._layer_names) - 1}]).')\n        layer_name = self.estimator._layer_names[self.feature_layer]\n    else:\n        raise TypeError('Layer must be of type `str` or `int`.')\n    keras_layer = self.estimator._model.get_layer(layer_name)\n    num_inbound_nodes = len(getattr(keras_layer, '_inbound_nodes', []))\n    if num_inbound_nodes > 1:\n        layer_output = keras_layer.get_output_at(0)\n    else:\n        layer_output = keras_layer.output\n    return layer_output",
            "def _get_keras_tensor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Helper function to get the feature layer output tensor in the keras graph\\n        :return: Output tensor\\n        '\n    if self.estimator._layer_names is None:\n        raise ValueError('No layer names identified.')\n    if isinstance(self.feature_layer, six.string_types):\n        if self.feature_layer not in self.estimator._layer_names:\n            raise ValueError(f'Layer name {self.feature_layer} is not part of the graph.')\n        layer_name = self.feature_layer\n    elif isinstance(self.feature_layer, int):\n        if self.feature_layer < 0 or self.feature_layer >= len(self.estimator._layer_names):\n            raise ValueError(f'Layer index {self.feature_layer} is outside of range [0 to {len(self.estimator._layer_names) - 1}]).')\n        layer_name = self.estimator._layer_names[self.feature_layer]\n    else:\n        raise TypeError('Layer must be of type `str` or `int`.')\n    keras_layer = self.estimator._model.get_layer(layer_name)\n    num_inbound_nodes = len(getattr(keras_layer, '_inbound_nodes', []))\n    if num_inbound_nodes > 1:\n        layer_output = keras_layer.get_output_at(0)\n    else:\n        layer_output = keras_layer.output\n    return layer_output"
        ]
    },
    {
        "func_name": "_apply_preprocessing",
        "original": "def _apply_preprocessing(self, x: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Helper function to preprocess the input for use with computing the loss gradient.\n        :param x: The input to preprocess\n        :return: Preprocessed input\n        \"\"\"\n    if x.shape == self.estimator.input_shape:\n        x_expanded = np.expand_dims(x, 0)\n    else:\n        x_expanded = x\n    (x_preprocessed, _) = self.estimator._apply_preprocessing(x=x_expanded, y=None, fit=False)\n    return x_preprocessed",
        "mutated": [
            "def _apply_preprocessing(self, x: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Helper function to preprocess the input for use with computing the loss gradient.\\n        :param x: The input to preprocess\\n        :return: Preprocessed input\\n        '\n    if x.shape == self.estimator.input_shape:\n        x_expanded = np.expand_dims(x, 0)\n    else:\n        x_expanded = x\n    (x_preprocessed, _) = self.estimator._apply_preprocessing(x=x_expanded, y=None, fit=False)\n    return x_preprocessed",
            "def _apply_preprocessing(self, x: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Helper function to preprocess the input for use with computing the loss gradient.\\n        :param x: The input to preprocess\\n        :return: Preprocessed input\\n        '\n    if x.shape == self.estimator.input_shape:\n        x_expanded = np.expand_dims(x, 0)\n    else:\n        x_expanded = x\n    (x_preprocessed, _) = self.estimator._apply_preprocessing(x=x_expanded, y=None, fit=False)\n    return x_preprocessed",
            "def _apply_preprocessing(self, x: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Helper function to preprocess the input for use with computing the loss gradient.\\n        :param x: The input to preprocess\\n        :return: Preprocessed input\\n        '\n    if x.shape == self.estimator.input_shape:\n        x_expanded = np.expand_dims(x, 0)\n    else:\n        x_expanded = x\n    (x_preprocessed, _) = self.estimator._apply_preprocessing(x=x_expanded, y=None, fit=False)\n    return x_preprocessed",
            "def _apply_preprocessing(self, x: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Helper function to preprocess the input for use with computing the loss gradient.\\n        :param x: The input to preprocess\\n        :return: Preprocessed input\\n        '\n    if x.shape == self.estimator.input_shape:\n        x_expanded = np.expand_dims(x, 0)\n    else:\n        x_expanded = x\n    (x_preprocessed, _) = self.estimator._apply_preprocessing(x=x_expanded, y=None, fit=False)\n    return x_preprocessed",
            "def _apply_preprocessing(self, x: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Helper function to preprocess the input for use with computing the loss gradient.\\n        :param x: The input to preprocess\\n        :return: Preprocessed input\\n        '\n    if x.shape == self.estimator.input_shape:\n        x_expanded = np.expand_dims(x, 0)\n    else:\n        x_expanded = x\n    (x_preprocessed, _) = self.estimator._apply_preprocessing(x=x_expanded, y=None, fit=False)\n    return x_preprocessed"
        ]
    }
]