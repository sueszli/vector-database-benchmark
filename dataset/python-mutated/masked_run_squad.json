[
    {
        "func_name": "set_seed",
        "original": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
        "mutated": [
            "def set_seed(args):\n    if False:\n        i = 10\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
            "def set_seed(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
            "def set_seed(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
            "def set_seed(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)",
            "def set_seed(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)"
        ]
    },
    {
        "func_name": "schedule_threshold",
        "original": "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
        "mutated": [
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)",
            "def schedule_threshold(step: int, total_step: int, warmup_steps: int, initial_threshold: float, final_threshold: float, initial_warmup: int, final_warmup: int, final_lambda: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if step <= initial_warmup * warmup_steps:\n        threshold = initial_threshold\n    elif step > total_step - final_warmup * warmup_steps:\n        threshold = final_threshold\n    else:\n        spars_warmup_steps = initial_warmup * warmup_steps\n        spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n        mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n        threshold = final_threshold + (initial_threshold - final_threshold) * mul_coeff ** 3\n    regu_lambda = final_lambda * threshold / final_threshold\n    return (threshold, regu_lambda)"
        ]
    },
    {
        "func_name": "regularization",
        "original": "def regularization(model: nn.Module, mode: str):\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
        "mutated": [
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter",
            "def regularization(model: nn.Module, mode: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (regu, counter) = (0, 0)\n    for (name, param) in model.named_parameters():\n        if 'mask_scores' in name:\n            if mode == 'l1':\n                regu += torch.norm(torch.sigmoid(param), p=1) / param.numel()\n            elif mode == 'l0':\n                regu += torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum() / param.numel()\n            else:\n                ValueError(\"Don't know this mode.\")\n            counter += 1\n    return regu / counter"
        ]
    },
    {
        "func_name": "to_list",
        "original": "def to_list(tensor):\n    return tensor.detach().cpu().tolist()",
        "mutated": [
            "def to_list(tensor):\n    if False:\n        i = 10\n    return tensor.detach().cpu().tolist()",
            "def to_list(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return tensor.detach().cpu().tolist()",
            "def to_list(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return tensor.detach().cpu().tolist()",
            "def to_list(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return tensor.detach().cpu().tolist()",
            "def to_list(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return tensor.detach().cpu().tolist()"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(args, train_dataset, model, tokenizer, teacher=None):\n    \"\"\"Train the model\"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 1\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            checkpoint_suffix = args.model_name_or_path.split('-')[-1].split('/')[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n            logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n            logger.info('  Continuing training from epoch %d', epochs_trained)\n            logger.info('  Continuing training from global step %d', global_step)\n            logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info('  Starting fine-tuning.')\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2], 'start_positions': batch[3], 'end_positions': batch[4]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[5], 'p_mask': batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({'is_impossible': batch[7]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, start_logits_stu, end_logits_stu) = outputs\n            if teacher is not None:\n                with torch.no_grad():\n                    (start_logits_tea, end_logits_tea) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_start = nn.functional.kl_div(input=nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(start_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_end = nn.functional.kl_div(input=nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(end_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_logits = (loss_start + loss_end) / 2.0\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        if 'pooler' in name:\n                            continue\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n                    learning_rate_scalar = scheduler.get_lr()\n                    tb_writer.add_scalar('lr', learning_rate_scalar[0], global_step)\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            tb_writer.add_scalar(f'lr/{idx + 1}', lr, global_step)\n                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    if teacher is not None:\n                        tb_writer.add_scalar('loss/distil', loss_logits.item(), global_step)\n                    if args.regularization is not None:\n                        tb_writer.add_scalar('loss/regularization', regu_.item(), global_step)\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - regu_lambda * regu_.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        elif teacher is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        else:\n                            tb_writer.add_scalar('loss/instant_ce', loss.item() - regu_lambda * regu_.item(), global_step)\n                    logging_loss = tr_loss\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
        "mutated": [
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 1\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            checkpoint_suffix = args.model_name_or_path.split('-')[-1].split('/')[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n            logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n            logger.info('  Continuing training from epoch %d', epochs_trained)\n            logger.info('  Continuing training from global step %d', global_step)\n            logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info('  Starting fine-tuning.')\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2], 'start_positions': batch[3], 'end_positions': batch[4]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[5], 'p_mask': batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({'is_impossible': batch[7]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, start_logits_stu, end_logits_stu) = outputs\n            if teacher is not None:\n                with torch.no_grad():\n                    (start_logits_tea, end_logits_tea) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_start = nn.functional.kl_div(input=nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(start_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_end = nn.functional.kl_div(input=nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(end_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_logits = (loss_start + loss_end) / 2.0\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        if 'pooler' in name:\n                            continue\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n                    learning_rate_scalar = scheduler.get_lr()\n                    tb_writer.add_scalar('lr', learning_rate_scalar[0], global_step)\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            tb_writer.add_scalar(f'lr/{idx + 1}', lr, global_step)\n                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    if teacher is not None:\n                        tb_writer.add_scalar('loss/distil', loss_logits.item(), global_step)\n                    if args.regularization is not None:\n                        tb_writer.add_scalar('loss/regularization', regu_.item(), global_step)\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - regu_lambda * regu_.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        elif teacher is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        else:\n                            tb_writer.add_scalar('loss/instant_ce', loss.item() - regu_lambda * regu_.item(), global_step)\n                    logging_loss = tr_loss\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 1\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            checkpoint_suffix = args.model_name_or_path.split('-')[-1].split('/')[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n            logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n            logger.info('  Continuing training from epoch %d', epochs_trained)\n            logger.info('  Continuing training from global step %d', global_step)\n            logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info('  Starting fine-tuning.')\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2], 'start_positions': batch[3], 'end_positions': batch[4]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[5], 'p_mask': batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({'is_impossible': batch[7]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, start_logits_stu, end_logits_stu) = outputs\n            if teacher is not None:\n                with torch.no_grad():\n                    (start_logits_tea, end_logits_tea) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_start = nn.functional.kl_div(input=nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(start_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_end = nn.functional.kl_div(input=nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(end_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_logits = (loss_start + loss_end) / 2.0\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        if 'pooler' in name:\n                            continue\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n                    learning_rate_scalar = scheduler.get_lr()\n                    tb_writer.add_scalar('lr', learning_rate_scalar[0], global_step)\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            tb_writer.add_scalar(f'lr/{idx + 1}', lr, global_step)\n                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    if teacher is not None:\n                        tb_writer.add_scalar('loss/distil', loss_logits.item(), global_step)\n                    if args.regularization is not None:\n                        tb_writer.add_scalar('loss/regularization', regu_.item(), global_step)\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - regu_lambda * regu_.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        elif teacher is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        else:\n                            tb_writer.add_scalar('loss/instant_ce', loss.item() - regu_lambda * regu_.item(), global_step)\n                    logging_loss = tr_loss\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 1\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            checkpoint_suffix = args.model_name_or_path.split('-')[-1].split('/')[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n            logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n            logger.info('  Continuing training from epoch %d', epochs_trained)\n            logger.info('  Continuing training from global step %d', global_step)\n            logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info('  Starting fine-tuning.')\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2], 'start_positions': batch[3], 'end_positions': batch[4]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[5], 'p_mask': batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({'is_impossible': batch[7]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, start_logits_stu, end_logits_stu) = outputs\n            if teacher is not None:\n                with torch.no_grad():\n                    (start_logits_tea, end_logits_tea) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_start = nn.functional.kl_div(input=nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(start_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_end = nn.functional.kl_div(input=nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(end_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_logits = (loss_start + loss_end) / 2.0\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        if 'pooler' in name:\n                            continue\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n                    learning_rate_scalar = scheduler.get_lr()\n                    tb_writer.add_scalar('lr', learning_rate_scalar[0], global_step)\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            tb_writer.add_scalar(f'lr/{idx + 1}', lr, global_step)\n                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    if teacher is not None:\n                        tb_writer.add_scalar('loss/distil', loss_logits.item(), global_step)\n                    if args.regularization is not None:\n                        tb_writer.add_scalar('loss/regularization', regu_.item(), global_step)\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - regu_lambda * regu_.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        elif teacher is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        else:\n                            tb_writer.add_scalar('loss/instant_ce', loss.item() - regu_lambda * regu_.item(), global_step)\n                    logging_loss = tr_loss\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 1\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            checkpoint_suffix = args.model_name_or_path.split('-')[-1].split('/')[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n            logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n            logger.info('  Continuing training from epoch %d', epochs_trained)\n            logger.info('  Continuing training from global step %d', global_step)\n            logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info('  Starting fine-tuning.')\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2], 'start_positions': batch[3], 'end_positions': batch[4]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[5], 'p_mask': batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({'is_impossible': batch[7]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, start_logits_stu, end_logits_stu) = outputs\n            if teacher is not None:\n                with torch.no_grad():\n                    (start_logits_tea, end_logits_tea) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_start = nn.functional.kl_div(input=nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(start_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_end = nn.functional.kl_div(input=nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(end_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_logits = (loss_start + loss_end) / 2.0\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        if 'pooler' in name:\n                            continue\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n                    learning_rate_scalar = scheduler.get_lr()\n                    tb_writer.add_scalar('lr', learning_rate_scalar[0], global_step)\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            tb_writer.add_scalar(f'lr/{idx + 1}', lr, global_step)\n                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    if teacher is not None:\n                        tb_writer.add_scalar('loss/distil', loss_logits.item(), global_step)\n                    if args.regularization is not None:\n                        tb_writer.add_scalar('loss/regularization', regu_.item(), global_step)\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - regu_lambda * regu_.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        elif teacher is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        else:\n                            tb_writer.add_scalar('loss/instant_ce', loss.item() - regu_lambda * regu_.item(), global_step)\n                    logging_loss = tr_loss\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)",
            "def train(args, train_dataset, model, tokenizer, teacher=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Train the model'\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if 'mask_score' in n and p.requires_grad], 'lr': args.mask_scores_learning_rate}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and (not any((nd in n for nd in no_decay)))], 'lr': args.learning_rate, 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if 'mask_score' not in n and p.requires_grad and any((nd in n for nd in no_decay))], 'lr': args.learning_rate, 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n    if os.path.isfile(os.path.join(args.model_name_or_path, 'optimizer.pt')) and os.path.isfile(os.path.join(args.model_name_or_path, 'scheduler.pt')):\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'optimizer.pt')))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, 'scheduler.pt')))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n        (model, optimizer) = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    if args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    logger.info('***** Running training *****')\n    logger.info('  Num examples = %d', len(train_dataset))\n    logger.info('  Num Epochs = %d', args.num_train_epochs)\n    logger.info('  Instantaneous batch size per GPU = %d', args.per_gpu_train_batch_size)\n    logger.info('  Total train batch size (w. parallel, distributed & accumulation) = %d', args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info('  Gradient Accumulation steps = %d', args.gradient_accumulation_steps)\n    logger.info('  Total optimization steps = %d', t_total)\n    if teacher is not None:\n        logger.info('  Training with distillation')\n    global_step = 1\n    if args.global_topk:\n        threshold_mem = None\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    if os.path.exists(args.model_name_or_path):\n        try:\n            checkpoint_suffix = args.model_name_or_path.split('-')[-1].split('/')[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n            logger.info('  Continuing training from checkpoint, will skip to saved global_step')\n            logger.info('  Continuing training from epoch %d', epochs_trained)\n            logger.info('  Continuing training from global step %d', global_step)\n            logger.info('  Will skip the first %d steps in the first epoch', steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info('  Starting fine-tuning.')\n    (tr_loss, logging_loss) = (0.0, 0.0)\n    model.zero_grad()\n    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc='Epoch', disable=args.local_rank not in [-1, 0])\n    set_seed(args)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])\n        for (step, batch) in enumerate(epoch_iterator):\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple((t.to(args.device) for t in batch))\n            (threshold, regu_lambda) = schedule_threshold(step=global_step, total_step=t_total, warmup_steps=args.warmup_steps, final_threshold=args.final_threshold, initial_threshold=args.initial_threshold, final_warmup=args.final_warmup, initial_warmup=args.initial_warmup, final_lambda=args.final_lambda)\n            if args.global_topk:\n                if threshold == 1.0:\n                    threshold = -100.0\n                elif threshold_mem is None or global_step % args.global_topk_frequency_compute == 0:\n                    concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                    n = concat.numel()\n                    kth = max(n - (int(n * threshold) + 1), 1)\n                    threshold_mem = concat.kthvalue(kth).values.item()\n                    threshold = threshold_mem\n                else:\n                    threshold = threshold_mem\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2], 'start_positions': batch[3], 'end_positions': batch[4]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[5], 'p_mask': batch[6]})\n                if args.version_2_with_negative:\n                    inputs.update({'is_impossible': batch[7]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = threshold\n            outputs = model(**inputs)\n            (loss, start_logits_stu, end_logits_stu) = outputs\n            if teacher is not None:\n                with torch.no_grad():\n                    (start_logits_tea, end_logits_tea) = teacher(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n                loss_start = nn.functional.kl_div(input=nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(start_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_end = nn.functional.kl_div(input=nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1), target=nn.functional.softmax(end_logits_tea / args.temperature, dim=-1), reduction='batchmean') * args.temperature ** 2\n                loss_logits = (loss_start + loss_end) / 2.0\n                loss = args.alpha_distil * loss_logits + args.alpha_ce * loss\n            if args.regularization is not None:\n                regu_ = regularization(model=model, mode=args.regularization)\n                loss = loss + regu_lambda * regu_\n            if args.n_gpu > 1:\n                loss = loss.mean()\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    tb_writer.add_scalar('threshold', threshold, global_step)\n                    for (name, param) in model.named_parameters():\n                        if not param.requires_grad:\n                            continue\n                        tb_writer.add_scalar('parameter_mean/' + name, param.data.mean(), global_step)\n                        tb_writer.add_scalar('parameter_std/' + name, param.data.std(), global_step)\n                        tb_writer.add_scalar('parameter_min/' + name, param.data.min(), global_step)\n                        tb_writer.add_scalar('parameter_max/' + name, param.data.max(), global_step)\n                        if 'pooler' in name:\n                            continue\n                        tb_writer.add_scalar('grad_mean/' + name, param.grad.data.mean(), global_step)\n                        tb_writer.add_scalar('grad_std/' + name, param.grad.data.std(), global_step)\n                        if args.regularization is not None and 'mask_scores' in name:\n                            if args.regularization == 'l1':\n                                perc = (torch.sigmoid(param) > threshold).sum().item() / param.numel()\n                            elif args.regularization == 'l0':\n                                perc = torch.sigmoid(param - 2 / 3 * np.log(0.1 / 1.1)).sum().item() / param.numel()\n                            tb_writer.add_scalar('retained_weights_perc/' + name, perc, global_step)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and (global_step % args.logging_steps == 0):\n                    if args.local_rank == -1 and args.evaluate_during_training:\n                        results = evaluate(args, model, tokenizer)\n                        for (key, value) in results.items():\n                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n                    learning_rate_scalar = scheduler.get_lr()\n                    tb_writer.add_scalar('lr', learning_rate_scalar[0], global_step)\n                    if len(learning_rate_scalar) > 1:\n                        for (idx, lr) in enumerate(learning_rate_scalar[1:]):\n                            tb_writer.add_scalar(f'lr/{idx + 1}', lr, global_step)\n                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    if teacher is not None:\n                        tb_writer.add_scalar('loss/distil', loss_logits.item(), global_step)\n                    if args.regularization is not None:\n                        tb_writer.add_scalar('loss/regularization', regu_.item(), global_step)\n                    if teacher is not None or args.regularization is not None:\n                        if teacher is not None and args.regularization is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - regu_lambda * regu_.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        elif teacher is not None:\n                            tb_writer.add_scalar('loss/instant_ce', (loss.item() - args.alpha_distil * loss_logits.item()) / args.alpha_ce, global_step)\n                        else:\n                            tb_writer.add_scalar('loss/instant_ce', loss.item() - regu_lambda * regu_.item(), global_step)\n                    logging_loss = tr_loss\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and (global_step % args.save_steps == 0):\n                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, 'module') else model\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n                    logger.info('Saving model checkpoint to %s', output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))\n                    logger.info('Saving optimizer and scheduler states to %s', output_dir)\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n    return (global_step, tr_loss / global_step)"
        ]
    },
    {
        "func_name": "evaluate",
        "original": "def evaluate(args, model, tokenizer, prefix=''):\n    (dataset, examples, features) = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n    if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n        model = nn.DataParallel(model)\n    logger.info('***** Running evaluation {} *****'.format(prefix))\n    logger.info('  Num examples = %d', len(dataset))\n    logger.info('  Batch size = %d', args.eval_batch_size)\n    all_results = []\n    start_time = timeit.default_timer()\n    if args.global_topk:\n        threshold_mem = None\n    for batch in tqdm(eval_dataloader, desc='Evaluating'):\n        model.eval()\n        batch = tuple((t.to(args.device) for t in batch))\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            example_indices = batch[3]\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[4], 'p_mask': batch[5]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = args.final_threshold\n                if args.global_topk:\n                    if threshold_mem is None:\n                        concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                        n = concat.numel()\n                        kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                        threshold_mem = concat.kthvalue(kth).values.item()\n                    inputs['threshold'] = threshold_mem\n            outputs = model(**inputs)\n        for (i, example_index) in enumerate(example_indices):\n            eval_feature = features[example_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            output = [to_list(output[i]) for output in outputs]\n            if len(output) >= 5:\n                start_logits = output[0]\n                start_top_index = output[1]\n                end_logits = output[2]\n                end_top_index = output[3]\n                cls_logits = output[4]\n                result = SquadResult(unique_id, start_logits, end_logits, start_top_index=start_top_index, end_top_index=end_top_index, cls_logits=cls_logits)\n            else:\n                (start_logits, end_logits) = output\n                result = SquadResult(unique_id, start_logits, end_logits)\n            all_results.append(result)\n    evalTime = timeit.default_timer() - start_time\n    logger.info('  Evaluation done in total %f secs (%f sec per example)', evalTime, evalTime / len(dataset))\n    output_prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(prefix))\n    output_nbest_file = os.path.join(args.output_dir, 'nbest_predictions_{}.json'.format(prefix))\n    if args.version_2_with_negative:\n        output_null_log_odds_file = os.path.join(args.output_dir, 'null_odds_{}.json'.format(prefix))\n    else:\n        output_null_log_odds_file = None\n    if args.model_type in ['xlnet', 'xlm']:\n        start_n_top = model.config.start_n_top if hasattr(model, 'config') else model.module.config.start_n_top\n        end_n_top = model.config.end_n_top if hasattr(model, 'config') else model.module.config.end_n_top\n        predictions = compute_predictions_log_probs(examples, features, all_results, args.n_best_size, args.max_answer_length, output_prediction_file, output_nbest_file, output_null_log_odds_file, start_n_top, end_n_top, args.version_2_with_negative, tokenizer, args.verbose_logging)\n    else:\n        predictions = compute_predictions_logits(examples, features, all_results, args.n_best_size, args.max_answer_length, args.do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, args.verbose_logging, args.version_2_with_negative, args.null_score_diff_threshold, tokenizer)\n    results = squad_evaluate(examples, predictions)\n    return results",
        "mutated": [
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n    (dataset, examples, features) = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n    if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n        model = nn.DataParallel(model)\n    logger.info('***** Running evaluation {} *****'.format(prefix))\n    logger.info('  Num examples = %d', len(dataset))\n    logger.info('  Batch size = %d', args.eval_batch_size)\n    all_results = []\n    start_time = timeit.default_timer()\n    if args.global_topk:\n        threshold_mem = None\n    for batch in tqdm(eval_dataloader, desc='Evaluating'):\n        model.eval()\n        batch = tuple((t.to(args.device) for t in batch))\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            example_indices = batch[3]\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[4], 'p_mask': batch[5]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = args.final_threshold\n                if args.global_topk:\n                    if threshold_mem is None:\n                        concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                        n = concat.numel()\n                        kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                        threshold_mem = concat.kthvalue(kth).values.item()\n                    inputs['threshold'] = threshold_mem\n            outputs = model(**inputs)\n        for (i, example_index) in enumerate(example_indices):\n            eval_feature = features[example_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            output = [to_list(output[i]) for output in outputs]\n            if len(output) >= 5:\n                start_logits = output[0]\n                start_top_index = output[1]\n                end_logits = output[2]\n                end_top_index = output[3]\n                cls_logits = output[4]\n                result = SquadResult(unique_id, start_logits, end_logits, start_top_index=start_top_index, end_top_index=end_top_index, cls_logits=cls_logits)\n            else:\n                (start_logits, end_logits) = output\n                result = SquadResult(unique_id, start_logits, end_logits)\n            all_results.append(result)\n    evalTime = timeit.default_timer() - start_time\n    logger.info('  Evaluation done in total %f secs (%f sec per example)', evalTime, evalTime / len(dataset))\n    output_prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(prefix))\n    output_nbest_file = os.path.join(args.output_dir, 'nbest_predictions_{}.json'.format(prefix))\n    if args.version_2_with_negative:\n        output_null_log_odds_file = os.path.join(args.output_dir, 'null_odds_{}.json'.format(prefix))\n    else:\n        output_null_log_odds_file = None\n    if args.model_type in ['xlnet', 'xlm']:\n        start_n_top = model.config.start_n_top if hasattr(model, 'config') else model.module.config.start_n_top\n        end_n_top = model.config.end_n_top if hasattr(model, 'config') else model.module.config.end_n_top\n        predictions = compute_predictions_log_probs(examples, features, all_results, args.n_best_size, args.max_answer_length, output_prediction_file, output_nbest_file, output_null_log_odds_file, start_n_top, end_n_top, args.version_2_with_negative, tokenizer, args.verbose_logging)\n    else:\n        predictions = compute_predictions_logits(examples, features, all_results, args.n_best_size, args.max_answer_length, args.do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, args.verbose_logging, args.version_2_with_negative, args.null_score_diff_threshold, tokenizer)\n    results = squad_evaluate(examples, predictions)\n    return results",
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (dataset, examples, features) = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n    if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n        model = nn.DataParallel(model)\n    logger.info('***** Running evaluation {} *****'.format(prefix))\n    logger.info('  Num examples = %d', len(dataset))\n    logger.info('  Batch size = %d', args.eval_batch_size)\n    all_results = []\n    start_time = timeit.default_timer()\n    if args.global_topk:\n        threshold_mem = None\n    for batch in tqdm(eval_dataloader, desc='Evaluating'):\n        model.eval()\n        batch = tuple((t.to(args.device) for t in batch))\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            example_indices = batch[3]\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[4], 'p_mask': batch[5]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = args.final_threshold\n                if args.global_topk:\n                    if threshold_mem is None:\n                        concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                        n = concat.numel()\n                        kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                        threshold_mem = concat.kthvalue(kth).values.item()\n                    inputs['threshold'] = threshold_mem\n            outputs = model(**inputs)\n        for (i, example_index) in enumerate(example_indices):\n            eval_feature = features[example_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            output = [to_list(output[i]) for output in outputs]\n            if len(output) >= 5:\n                start_logits = output[0]\n                start_top_index = output[1]\n                end_logits = output[2]\n                end_top_index = output[3]\n                cls_logits = output[4]\n                result = SquadResult(unique_id, start_logits, end_logits, start_top_index=start_top_index, end_top_index=end_top_index, cls_logits=cls_logits)\n            else:\n                (start_logits, end_logits) = output\n                result = SquadResult(unique_id, start_logits, end_logits)\n            all_results.append(result)\n    evalTime = timeit.default_timer() - start_time\n    logger.info('  Evaluation done in total %f secs (%f sec per example)', evalTime, evalTime / len(dataset))\n    output_prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(prefix))\n    output_nbest_file = os.path.join(args.output_dir, 'nbest_predictions_{}.json'.format(prefix))\n    if args.version_2_with_negative:\n        output_null_log_odds_file = os.path.join(args.output_dir, 'null_odds_{}.json'.format(prefix))\n    else:\n        output_null_log_odds_file = None\n    if args.model_type in ['xlnet', 'xlm']:\n        start_n_top = model.config.start_n_top if hasattr(model, 'config') else model.module.config.start_n_top\n        end_n_top = model.config.end_n_top if hasattr(model, 'config') else model.module.config.end_n_top\n        predictions = compute_predictions_log_probs(examples, features, all_results, args.n_best_size, args.max_answer_length, output_prediction_file, output_nbest_file, output_null_log_odds_file, start_n_top, end_n_top, args.version_2_with_negative, tokenizer, args.verbose_logging)\n    else:\n        predictions = compute_predictions_logits(examples, features, all_results, args.n_best_size, args.max_answer_length, args.do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, args.verbose_logging, args.version_2_with_negative, args.null_score_diff_threshold, tokenizer)\n    results = squad_evaluate(examples, predictions)\n    return results",
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (dataset, examples, features) = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n    if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n        model = nn.DataParallel(model)\n    logger.info('***** Running evaluation {} *****'.format(prefix))\n    logger.info('  Num examples = %d', len(dataset))\n    logger.info('  Batch size = %d', args.eval_batch_size)\n    all_results = []\n    start_time = timeit.default_timer()\n    if args.global_topk:\n        threshold_mem = None\n    for batch in tqdm(eval_dataloader, desc='Evaluating'):\n        model.eval()\n        batch = tuple((t.to(args.device) for t in batch))\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            example_indices = batch[3]\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[4], 'p_mask': batch[5]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = args.final_threshold\n                if args.global_topk:\n                    if threshold_mem is None:\n                        concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                        n = concat.numel()\n                        kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                        threshold_mem = concat.kthvalue(kth).values.item()\n                    inputs['threshold'] = threshold_mem\n            outputs = model(**inputs)\n        for (i, example_index) in enumerate(example_indices):\n            eval_feature = features[example_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            output = [to_list(output[i]) for output in outputs]\n            if len(output) >= 5:\n                start_logits = output[0]\n                start_top_index = output[1]\n                end_logits = output[2]\n                end_top_index = output[3]\n                cls_logits = output[4]\n                result = SquadResult(unique_id, start_logits, end_logits, start_top_index=start_top_index, end_top_index=end_top_index, cls_logits=cls_logits)\n            else:\n                (start_logits, end_logits) = output\n                result = SquadResult(unique_id, start_logits, end_logits)\n            all_results.append(result)\n    evalTime = timeit.default_timer() - start_time\n    logger.info('  Evaluation done in total %f secs (%f sec per example)', evalTime, evalTime / len(dataset))\n    output_prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(prefix))\n    output_nbest_file = os.path.join(args.output_dir, 'nbest_predictions_{}.json'.format(prefix))\n    if args.version_2_with_negative:\n        output_null_log_odds_file = os.path.join(args.output_dir, 'null_odds_{}.json'.format(prefix))\n    else:\n        output_null_log_odds_file = None\n    if args.model_type in ['xlnet', 'xlm']:\n        start_n_top = model.config.start_n_top if hasattr(model, 'config') else model.module.config.start_n_top\n        end_n_top = model.config.end_n_top if hasattr(model, 'config') else model.module.config.end_n_top\n        predictions = compute_predictions_log_probs(examples, features, all_results, args.n_best_size, args.max_answer_length, output_prediction_file, output_nbest_file, output_null_log_odds_file, start_n_top, end_n_top, args.version_2_with_negative, tokenizer, args.verbose_logging)\n    else:\n        predictions = compute_predictions_logits(examples, features, all_results, args.n_best_size, args.max_answer_length, args.do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, args.verbose_logging, args.version_2_with_negative, args.null_score_diff_threshold, tokenizer)\n    results = squad_evaluate(examples, predictions)\n    return results",
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (dataset, examples, features) = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n    if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n        model = nn.DataParallel(model)\n    logger.info('***** Running evaluation {} *****'.format(prefix))\n    logger.info('  Num examples = %d', len(dataset))\n    logger.info('  Batch size = %d', args.eval_batch_size)\n    all_results = []\n    start_time = timeit.default_timer()\n    if args.global_topk:\n        threshold_mem = None\n    for batch in tqdm(eval_dataloader, desc='Evaluating'):\n        model.eval()\n        batch = tuple((t.to(args.device) for t in batch))\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            example_indices = batch[3]\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[4], 'p_mask': batch[5]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = args.final_threshold\n                if args.global_topk:\n                    if threshold_mem is None:\n                        concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                        n = concat.numel()\n                        kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                        threshold_mem = concat.kthvalue(kth).values.item()\n                    inputs['threshold'] = threshold_mem\n            outputs = model(**inputs)\n        for (i, example_index) in enumerate(example_indices):\n            eval_feature = features[example_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            output = [to_list(output[i]) for output in outputs]\n            if len(output) >= 5:\n                start_logits = output[0]\n                start_top_index = output[1]\n                end_logits = output[2]\n                end_top_index = output[3]\n                cls_logits = output[4]\n                result = SquadResult(unique_id, start_logits, end_logits, start_top_index=start_top_index, end_top_index=end_top_index, cls_logits=cls_logits)\n            else:\n                (start_logits, end_logits) = output\n                result = SquadResult(unique_id, start_logits, end_logits)\n            all_results.append(result)\n    evalTime = timeit.default_timer() - start_time\n    logger.info('  Evaluation done in total %f secs (%f sec per example)', evalTime, evalTime / len(dataset))\n    output_prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(prefix))\n    output_nbest_file = os.path.join(args.output_dir, 'nbest_predictions_{}.json'.format(prefix))\n    if args.version_2_with_negative:\n        output_null_log_odds_file = os.path.join(args.output_dir, 'null_odds_{}.json'.format(prefix))\n    else:\n        output_null_log_odds_file = None\n    if args.model_type in ['xlnet', 'xlm']:\n        start_n_top = model.config.start_n_top if hasattr(model, 'config') else model.module.config.start_n_top\n        end_n_top = model.config.end_n_top if hasattr(model, 'config') else model.module.config.end_n_top\n        predictions = compute_predictions_log_probs(examples, features, all_results, args.n_best_size, args.max_answer_length, output_prediction_file, output_nbest_file, output_null_log_odds_file, start_n_top, end_n_top, args.version_2_with_negative, tokenizer, args.verbose_logging)\n    else:\n        predictions = compute_predictions_logits(examples, features, all_results, args.n_best_size, args.max_answer_length, args.do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, args.verbose_logging, args.version_2_with_negative, args.null_score_diff_threshold, tokenizer)\n    results = squad_evaluate(examples, predictions)\n    return results",
            "def evaluate(args, model, tokenizer, prefix=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (dataset, examples, features) = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n    if args.n_gpu > 1 and (not isinstance(model, nn.DataParallel)):\n        model = nn.DataParallel(model)\n    logger.info('***** Running evaluation {} *****'.format(prefix))\n    logger.info('  Num examples = %d', len(dataset))\n    logger.info('  Batch size = %d', args.eval_batch_size)\n    all_results = []\n    start_time = timeit.default_timer()\n    if args.global_topk:\n        threshold_mem = None\n    for batch in tqdm(eval_dataloader, desc='Evaluating'):\n        model.eval()\n        batch = tuple((t.to(args.device) for t in batch))\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2]}\n            if args.model_type in ['xlm', 'roberta', 'distilbert', 'camembert']:\n                del inputs['token_type_ids']\n            example_indices = batch[3]\n            if args.model_type in ['xlnet', 'xlm']:\n                inputs.update({'cls_index': batch[4], 'p_mask': batch[5]})\n                if hasattr(model, 'config') and hasattr(model.config, 'lang2id'):\n                    inputs.update({'langs': (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)})\n            if 'masked' in args.model_type:\n                inputs['threshold'] = args.final_threshold\n                if args.global_topk:\n                    if threshold_mem is None:\n                        concat = torch.cat([param.view(-1) for (name, param) in model.named_parameters() if 'mask_scores' in name])\n                        n = concat.numel()\n                        kth = max(n - (int(n * args.final_threshold) + 1), 1)\n                        threshold_mem = concat.kthvalue(kth).values.item()\n                    inputs['threshold'] = threshold_mem\n            outputs = model(**inputs)\n        for (i, example_index) in enumerate(example_indices):\n            eval_feature = features[example_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            output = [to_list(output[i]) for output in outputs]\n            if len(output) >= 5:\n                start_logits = output[0]\n                start_top_index = output[1]\n                end_logits = output[2]\n                end_top_index = output[3]\n                cls_logits = output[4]\n                result = SquadResult(unique_id, start_logits, end_logits, start_top_index=start_top_index, end_top_index=end_top_index, cls_logits=cls_logits)\n            else:\n                (start_logits, end_logits) = output\n                result = SquadResult(unique_id, start_logits, end_logits)\n            all_results.append(result)\n    evalTime = timeit.default_timer() - start_time\n    logger.info('  Evaluation done in total %f secs (%f sec per example)', evalTime, evalTime / len(dataset))\n    output_prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(prefix))\n    output_nbest_file = os.path.join(args.output_dir, 'nbest_predictions_{}.json'.format(prefix))\n    if args.version_2_with_negative:\n        output_null_log_odds_file = os.path.join(args.output_dir, 'null_odds_{}.json'.format(prefix))\n    else:\n        output_null_log_odds_file = None\n    if args.model_type in ['xlnet', 'xlm']:\n        start_n_top = model.config.start_n_top if hasattr(model, 'config') else model.module.config.start_n_top\n        end_n_top = model.config.end_n_top if hasattr(model, 'config') else model.module.config.end_n_top\n        predictions = compute_predictions_log_probs(examples, features, all_results, args.n_best_size, args.max_answer_length, output_prediction_file, output_nbest_file, output_null_log_odds_file, start_n_top, end_n_top, args.version_2_with_negative, tokenizer, args.verbose_logging)\n    else:\n        predictions = compute_predictions_logits(examples, features, all_results, args.n_best_size, args.max_answer_length, args.do_lower_case, output_prediction_file, output_nbest_file, output_null_log_odds_file, args.verbose_logging, args.version_2_with_negative, args.null_score_diff_threshold, tokenizer)\n    results = squad_evaluate(examples, predictions)\n    return results"
        ]
    },
    {
        "func_name": "load_and_cache_examples",
        "original": "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    input_dir = args.data_dir if args.data_dir else '.'\n    cached_features_file = os.path.join(input_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', args.tokenizer_name if args.tokenizer_name else list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), list(filter(None, args.predict_file.split('/'))).pop() if evaluate else list(filter(None, args.train_file.split('/'))).pop()))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features_and_dataset = torch.load(cached_features_file)\n        (features, dataset, examples) = (features_and_dataset['features'], features_and_dataset['dataset'], features_and_dataset['examples'])\n    else:\n        logger.info('Creating features from dataset file at %s', input_dir)\n        if not args.data_dir and (evaluate and (not args.predict_file) or (not evaluate and (not args.train_file))):\n            try:\n                import tensorflow_datasets as tfds\n            except ImportError:\n                raise ImportError('If not data_dir is specified, tensorflow_datasets needs to be installed.')\n            if args.version_2_with_negative:\n                logger.warning('tensorflow_datasets does not handle version 2 of SQuAD.')\n            tfds_examples = tfds.load('squad')\n            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n        else:\n            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n            if evaluate:\n                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n            else:\n                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n        (features, dataset) = squad_convert_examples_to_features(examples=examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=not evaluate, return_dataset='pt', threads=args.threads)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save({'features': features, 'dataset': dataset, 'examples': examples}, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    if output_examples:\n        return (dataset, examples, features)\n    return dataset",
        "mutated": [
            "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n    if False:\n        i = 10\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    input_dir = args.data_dir if args.data_dir else '.'\n    cached_features_file = os.path.join(input_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', args.tokenizer_name if args.tokenizer_name else list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), list(filter(None, args.predict_file.split('/'))).pop() if evaluate else list(filter(None, args.train_file.split('/'))).pop()))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features_and_dataset = torch.load(cached_features_file)\n        (features, dataset, examples) = (features_and_dataset['features'], features_and_dataset['dataset'], features_and_dataset['examples'])\n    else:\n        logger.info('Creating features from dataset file at %s', input_dir)\n        if not args.data_dir and (evaluate and (not args.predict_file) or (not evaluate and (not args.train_file))):\n            try:\n                import tensorflow_datasets as tfds\n            except ImportError:\n                raise ImportError('If not data_dir is specified, tensorflow_datasets needs to be installed.')\n            if args.version_2_with_negative:\n                logger.warning('tensorflow_datasets does not handle version 2 of SQuAD.')\n            tfds_examples = tfds.load('squad')\n            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n        else:\n            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n            if evaluate:\n                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n            else:\n                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n        (features, dataset) = squad_convert_examples_to_features(examples=examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=not evaluate, return_dataset='pt', threads=args.threads)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save({'features': features, 'dataset': dataset, 'examples': examples}, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    if output_examples:\n        return (dataset, examples, features)\n    return dataset",
            "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    input_dir = args.data_dir if args.data_dir else '.'\n    cached_features_file = os.path.join(input_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', args.tokenizer_name if args.tokenizer_name else list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), list(filter(None, args.predict_file.split('/'))).pop() if evaluate else list(filter(None, args.train_file.split('/'))).pop()))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features_and_dataset = torch.load(cached_features_file)\n        (features, dataset, examples) = (features_and_dataset['features'], features_and_dataset['dataset'], features_and_dataset['examples'])\n    else:\n        logger.info('Creating features from dataset file at %s', input_dir)\n        if not args.data_dir and (evaluate and (not args.predict_file) or (not evaluate and (not args.train_file))):\n            try:\n                import tensorflow_datasets as tfds\n            except ImportError:\n                raise ImportError('If not data_dir is specified, tensorflow_datasets needs to be installed.')\n            if args.version_2_with_negative:\n                logger.warning('tensorflow_datasets does not handle version 2 of SQuAD.')\n            tfds_examples = tfds.load('squad')\n            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n        else:\n            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n            if evaluate:\n                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n            else:\n                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n        (features, dataset) = squad_convert_examples_to_features(examples=examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=not evaluate, return_dataset='pt', threads=args.threads)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save({'features': features, 'dataset': dataset, 'examples': examples}, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    if output_examples:\n        return (dataset, examples, features)\n    return dataset",
            "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    input_dir = args.data_dir if args.data_dir else '.'\n    cached_features_file = os.path.join(input_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', args.tokenizer_name if args.tokenizer_name else list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), list(filter(None, args.predict_file.split('/'))).pop() if evaluate else list(filter(None, args.train_file.split('/'))).pop()))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features_and_dataset = torch.load(cached_features_file)\n        (features, dataset, examples) = (features_and_dataset['features'], features_and_dataset['dataset'], features_and_dataset['examples'])\n    else:\n        logger.info('Creating features from dataset file at %s', input_dir)\n        if not args.data_dir and (evaluate and (not args.predict_file) or (not evaluate and (not args.train_file))):\n            try:\n                import tensorflow_datasets as tfds\n            except ImportError:\n                raise ImportError('If not data_dir is specified, tensorflow_datasets needs to be installed.')\n            if args.version_2_with_negative:\n                logger.warning('tensorflow_datasets does not handle version 2 of SQuAD.')\n            tfds_examples = tfds.load('squad')\n            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n        else:\n            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n            if evaluate:\n                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n            else:\n                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n        (features, dataset) = squad_convert_examples_to_features(examples=examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=not evaluate, return_dataset='pt', threads=args.threads)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save({'features': features, 'dataset': dataset, 'examples': examples}, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    if output_examples:\n        return (dataset, examples, features)\n    return dataset",
            "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    input_dir = args.data_dir if args.data_dir else '.'\n    cached_features_file = os.path.join(input_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', args.tokenizer_name if args.tokenizer_name else list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), list(filter(None, args.predict_file.split('/'))).pop() if evaluate else list(filter(None, args.train_file.split('/'))).pop()))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features_and_dataset = torch.load(cached_features_file)\n        (features, dataset, examples) = (features_and_dataset['features'], features_and_dataset['dataset'], features_and_dataset['examples'])\n    else:\n        logger.info('Creating features from dataset file at %s', input_dir)\n        if not args.data_dir and (evaluate and (not args.predict_file) or (not evaluate and (not args.train_file))):\n            try:\n                import tensorflow_datasets as tfds\n            except ImportError:\n                raise ImportError('If not data_dir is specified, tensorflow_datasets needs to be installed.')\n            if args.version_2_with_negative:\n                logger.warning('tensorflow_datasets does not handle version 2 of SQuAD.')\n            tfds_examples = tfds.load('squad')\n            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n        else:\n            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n            if evaluate:\n                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n            else:\n                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n        (features, dataset) = squad_convert_examples_to_features(examples=examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=not evaluate, return_dataset='pt', threads=args.threads)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save({'features': features, 'dataset': dataset, 'examples': examples}, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    if output_examples:\n        return (dataset, examples, features)\n    return dataset",
            "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if args.local_rank not in [-1, 0] and (not evaluate):\n        torch.distributed.barrier()\n    input_dir = args.data_dir if args.data_dir else '.'\n    cached_features_file = os.path.join(input_dir, 'cached_{}_{}_{}_{}'.format('dev' if evaluate else 'train', args.tokenizer_name if args.tokenizer_name else list(filter(None, args.model_name_or_path.split('/'))).pop(), str(args.max_seq_length), list(filter(None, args.predict_file.split('/'))).pop() if evaluate else list(filter(None, args.train_file.split('/'))).pop()))\n    if os.path.exists(cached_features_file) and (not args.overwrite_cache):\n        logger.info('Loading features from cached file %s', cached_features_file)\n        features_and_dataset = torch.load(cached_features_file)\n        (features, dataset, examples) = (features_and_dataset['features'], features_and_dataset['dataset'], features_and_dataset['examples'])\n    else:\n        logger.info('Creating features from dataset file at %s', input_dir)\n        if not args.data_dir and (evaluate and (not args.predict_file) or (not evaluate and (not args.train_file))):\n            try:\n                import tensorflow_datasets as tfds\n            except ImportError:\n                raise ImportError('If not data_dir is specified, tensorflow_datasets needs to be installed.')\n            if args.version_2_with_negative:\n                logger.warning('tensorflow_datasets does not handle version 2 of SQuAD.')\n            tfds_examples = tfds.load('squad')\n            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n        else:\n            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n            if evaluate:\n                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n            else:\n                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n        (features, dataset) = squad_convert_examples_to_features(examples=examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=not evaluate, return_dataset='pt', threads=args.threads)\n        if args.local_rank in [-1, 0]:\n            logger.info('Saving features into cached file %s', cached_features_file)\n            torch.save({'features': features, 'dataset': dataset, 'examples': examples}, cached_features_file)\n    if args.local_rank == 0 and (not evaluate):\n        torch.distributed.barrier()\n    if output_examples:\n        return (dataset, examples, features)\n    return dataset"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model checkpoints and predictions will be written.')\n    parser.add_argument('--data_dir', default=None, type=str, help='The input data dir. Should contain the .json files for the task.' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--train_file', default=None, type=str, help='The input training file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--predict_file', default=None, type=str, help='The input evaluation file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, the SQuAD examples contain some that do not have an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='If null_score - best_non_null is greater than the threshold predict null.')\n    parser.add_argument('--max_seq_length', default=384, type=int, help='The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.')\n    parser.add_argument('--doc_stride', default=128, type=int, help='When splitting up a long document into chunks, how much stride to take between chunks.')\n    parser.add_argument('--max_query_length', default=64, type=int, help='The maximum number of tokens for the question. Questions longer than this will be truncated to this length.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already SQuAD fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--n_best_size', default=20, type=int, help='The total number of n-best predictions to generate in the nbest_predictions.json output file.')\n    parser.add_argument('--max_answer_length', default=30, type=int, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--verbose_logging', action='store_true', help='If true, all of the warnings related to data processing will be printed. A number of warnings are expected for a normal SQuAD evaluation.')\n    parser.add_argument('--lang_id', default=0, type=int, help='language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)')\n    parser.add_argument('--logging_steps', type=int, default=500, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=500, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--threads', type=int, default=1, help='multiple threads for converting example to features')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n        logger.warning(\"WARNING - You've set a doc stride which may be superior to the document length in some examples. This could result in errors when building features from the examples. Please reduce the doc stride or increase the maximum length to ensure the features are correctly built.\")\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError('Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.'.format(args.output_dir))\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case, cache_dir=args.cache_dir if args.cache_dir else None)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.fp16:\n        try:\n            import apex\n            apex.amp.register_half_function(torch, 'einsum')\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        if args.do_train:\n            logger.info('Loading checkpoints saved during training for evaluation')\n            checkpoints = [args.output_dir]\n            if args.eval_all_checkpoints:\n                checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        else:\n            logger.info('Loading checkpoint %s for evaluation', args.model_name_or_path)\n            checkpoints = [args.model_name_or_path]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=global_step)\n            result = {k + ('_{}'.format(global_step) if global_step else ''): v for (k, v) in result.items()}\n            results.update(result)\n    logger.info('Results: {}'.format(results))\n    predict_file = list(filter(None, args.predict_file.split('/'))).pop()\n    if not os.path.exists(os.path.join(args.output_dir, predict_file)):\n        os.makedirs(os.path.join(args.output_dir, predict_file))\n    output_eval_file = os.path.join(args.output_dir, predict_file, 'eval_results.txt')\n    with open(output_eval_file, 'w') as writer:\n        for key in sorted(results.keys()):\n            writer.write('%s = %s\\n' % (key, str(results[key])))\n    return results",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model checkpoints and predictions will be written.')\n    parser.add_argument('--data_dir', default=None, type=str, help='The input data dir. Should contain the .json files for the task.' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--train_file', default=None, type=str, help='The input training file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--predict_file', default=None, type=str, help='The input evaluation file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, the SQuAD examples contain some that do not have an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='If null_score - best_non_null is greater than the threshold predict null.')\n    parser.add_argument('--max_seq_length', default=384, type=int, help='The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.')\n    parser.add_argument('--doc_stride', default=128, type=int, help='When splitting up a long document into chunks, how much stride to take between chunks.')\n    parser.add_argument('--max_query_length', default=64, type=int, help='The maximum number of tokens for the question. Questions longer than this will be truncated to this length.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already SQuAD fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--n_best_size', default=20, type=int, help='The total number of n-best predictions to generate in the nbest_predictions.json output file.')\n    parser.add_argument('--max_answer_length', default=30, type=int, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--verbose_logging', action='store_true', help='If true, all of the warnings related to data processing will be printed. A number of warnings are expected for a normal SQuAD evaluation.')\n    parser.add_argument('--lang_id', default=0, type=int, help='language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)')\n    parser.add_argument('--logging_steps', type=int, default=500, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=500, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--threads', type=int, default=1, help='multiple threads for converting example to features')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n        logger.warning(\"WARNING - You've set a doc stride which may be superior to the document length in some examples. This could result in errors when building features from the examples. Please reduce the doc stride or increase the maximum length to ensure the features are correctly built.\")\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError('Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.'.format(args.output_dir))\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case, cache_dir=args.cache_dir if args.cache_dir else None)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.fp16:\n        try:\n            import apex\n            apex.amp.register_half_function(torch, 'einsum')\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        if args.do_train:\n            logger.info('Loading checkpoints saved during training for evaluation')\n            checkpoints = [args.output_dir]\n            if args.eval_all_checkpoints:\n                checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        else:\n            logger.info('Loading checkpoint %s for evaluation', args.model_name_or_path)\n            checkpoints = [args.model_name_or_path]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=global_step)\n            result = {k + ('_{}'.format(global_step) if global_step else ''): v for (k, v) in result.items()}\n            results.update(result)\n    logger.info('Results: {}'.format(results))\n    predict_file = list(filter(None, args.predict_file.split('/'))).pop()\n    if not os.path.exists(os.path.join(args.output_dir, predict_file)):\n        os.makedirs(os.path.join(args.output_dir, predict_file))\n    output_eval_file = os.path.join(args.output_dir, predict_file, 'eval_results.txt')\n    with open(output_eval_file, 'w') as writer:\n        for key in sorted(results.keys()):\n            writer.write('%s = %s\\n' % (key, str(results[key])))\n    return results",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model checkpoints and predictions will be written.')\n    parser.add_argument('--data_dir', default=None, type=str, help='The input data dir. Should contain the .json files for the task.' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--train_file', default=None, type=str, help='The input training file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--predict_file', default=None, type=str, help='The input evaluation file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, the SQuAD examples contain some that do not have an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='If null_score - best_non_null is greater than the threshold predict null.')\n    parser.add_argument('--max_seq_length', default=384, type=int, help='The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.')\n    parser.add_argument('--doc_stride', default=128, type=int, help='When splitting up a long document into chunks, how much stride to take between chunks.')\n    parser.add_argument('--max_query_length', default=64, type=int, help='The maximum number of tokens for the question. Questions longer than this will be truncated to this length.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already SQuAD fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--n_best_size', default=20, type=int, help='The total number of n-best predictions to generate in the nbest_predictions.json output file.')\n    parser.add_argument('--max_answer_length', default=30, type=int, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--verbose_logging', action='store_true', help='If true, all of the warnings related to data processing will be printed. A number of warnings are expected for a normal SQuAD evaluation.')\n    parser.add_argument('--lang_id', default=0, type=int, help='language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)')\n    parser.add_argument('--logging_steps', type=int, default=500, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=500, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--threads', type=int, default=1, help='multiple threads for converting example to features')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n        logger.warning(\"WARNING - You've set a doc stride which may be superior to the document length in some examples. This could result in errors when building features from the examples. Please reduce the doc stride or increase the maximum length to ensure the features are correctly built.\")\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError('Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.'.format(args.output_dir))\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case, cache_dir=args.cache_dir if args.cache_dir else None)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.fp16:\n        try:\n            import apex\n            apex.amp.register_half_function(torch, 'einsum')\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        if args.do_train:\n            logger.info('Loading checkpoints saved during training for evaluation')\n            checkpoints = [args.output_dir]\n            if args.eval_all_checkpoints:\n                checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        else:\n            logger.info('Loading checkpoint %s for evaluation', args.model_name_or_path)\n            checkpoints = [args.model_name_or_path]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=global_step)\n            result = {k + ('_{}'.format(global_step) if global_step else ''): v for (k, v) in result.items()}\n            results.update(result)\n    logger.info('Results: {}'.format(results))\n    predict_file = list(filter(None, args.predict_file.split('/'))).pop()\n    if not os.path.exists(os.path.join(args.output_dir, predict_file)):\n        os.makedirs(os.path.join(args.output_dir, predict_file))\n    output_eval_file = os.path.join(args.output_dir, predict_file, 'eval_results.txt')\n    with open(output_eval_file, 'w') as writer:\n        for key in sorted(results.keys()):\n            writer.write('%s = %s\\n' % (key, str(results[key])))\n    return results",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model checkpoints and predictions will be written.')\n    parser.add_argument('--data_dir', default=None, type=str, help='The input data dir. Should contain the .json files for the task.' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--train_file', default=None, type=str, help='The input training file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--predict_file', default=None, type=str, help='The input evaluation file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, the SQuAD examples contain some that do not have an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='If null_score - best_non_null is greater than the threshold predict null.')\n    parser.add_argument('--max_seq_length', default=384, type=int, help='The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.')\n    parser.add_argument('--doc_stride', default=128, type=int, help='When splitting up a long document into chunks, how much stride to take between chunks.')\n    parser.add_argument('--max_query_length', default=64, type=int, help='The maximum number of tokens for the question. Questions longer than this will be truncated to this length.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already SQuAD fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--n_best_size', default=20, type=int, help='The total number of n-best predictions to generate in the nbest_predictions.json output file.')\n    parser.add_argument('--max_answer_length', default=30, type=int, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--verbose_logging', action='store_true', help='If true, all of the warnings related to data processing will be printed. A number of warnings are expected for a normal SQuAD evaluation.')\n    parser.add_argument('--lang_id', default=0, type=int, help='language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)')\n    parser.add_argument('--logging_steps', type=int, default=500, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=500, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--threads', type=int, default=1, help='multiple threads for converting example to features')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n        logger.warning(\"WARNING - You've set a doc stride which may be superior to the document length in some examples. This could result in errors when building features from the examples. Please reduce the doc stride or increase the maximum length to ensure the features are correctly built.\")\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError('Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.'.format(args.output_dir))\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case, cache_dir=args.cache_dir if args.cache_dir else None)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.fp16:\n        try:\n            import apex\n            apex.amp.register_half_function(torch, 'einsum')\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        if args.do_train:\n            logger.info('Loading checkpoints saved during training for evaluation')\n            checkpoints = [args.output_dir]\n            if args.eval_all_checkpoints:\n                checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        else:\n            logger.info('Loading checkpoint %s for evaluation', args.model_name_or_path)\n            checkpoints = [args.model_name_or_path]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=global_step)\n            result = {k + ('_{}'.format(global_step) if global_step else ''): v for (k, v) in result.items()}\n            results.update(result)\n    logger.info('Results: {}'.format(results))\n    predict_file = list(filter(None, args.predict_file.split('/'))).pop()\n    if not os.path.exists(os.path.join(args.output_dir, predict_file)):\n        os.makedirs(os.path.join(args.output_dir, predict_file))\n    output_eval_file = os.path.join(args.output_dir, predict_file, 'eval_results.txt')\n    with open(output_eval_file, 'w') as writer:\n        for key in sorted(results.keys()):\n            writer.write('%s = %s\\n' % (key, str(results[key])))\n    return results",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model checkpoints and predictions will be written.')\n    parser.add_argument('--data_dir', default=None, type=str, help='The input data dir. Should contain the .json files for the task.' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--train_file', default=None, type=str, help='The input training file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--predict_file', default=None, type=str, help='The input evaluation file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, the SQuAD examples contain some that do not have an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='If null_score - best_non_null is greater than the threshold predict null.')\n    parser.add_argument('--max_seq_length', default=384, type=int, help='The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.')\n    parser.add_argument('--doc_stride', default=128, type=int, help='When splitting up a long document into chunks, how much stride to take between chunks.')\n    parser.add_argument('--max_query_length', default=64, type=int, help='The maximum number of tokens for the question. Questions longer than this will be truncated to this length.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already SQuAD fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--n_best_size', default=20, type=int, help='The total number of n-best predictions to generate in the nbest_predictions.json output file.')\n    parser.add_argument('--max_answer_length', default=30, type=int, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--verbose_logging', action='store_true', help='If true, all of the warnings related to data processing will be printed. A number of warnings are expected for a normal SQuAD evaluation.')\n    parser.add_argument('--lang_id', default=0, type=int, help='language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)')\n    parser.add_argument('--logging_steps', type=int, default=500, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=500, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--threads', type=int, default=1, help='multiple threads for converting example to features')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n        logger.warning(\"WARNING - You've set a doc stride which may be superior to the document length in some examples. This could result in errors when building features from the examples. Please reduce the doc stride or increase the maximum length to ensure the features are correctly built.\")\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError('Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.'.format(args.output_dir))\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case, cache_dir=args.cache_dir if args.cache_dir else None)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.fp16:\n        try:\n            import apex\n            apex.amp.register_half_function(torch, 'einsum')\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        if args.do_train:\n            logger.info('Loading checkpoints saved during training for evaluation')\n            checkpoints = [args.output_dir]\n            if args.eval_all_checkpoints:\n                checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        else:\n            logger.info('Loading checkpoint %s for evaluation', args.model_name_or_path)\n            checkpoints = [args.model_name_or_path]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=global_step)\n            result = {k + ('_{}'.format(global_step) if global_step else ''): v for (k, v) in result.items()}\n            results.update(result)\n    logger.info('Results: {}'.format(results))\n    predict_file = list(filter(None, args.predict_file.split('/'))).pop()\n    if not os.path.exists(os.path.join(args.output_dir, predict_file)):\n        os.makedirs(os.path.join(args.output_dir, predict_file))\n    output_eval_file = os.path.join(args.output_dir, predict_file, 'eval_results.txt')\n    with open(output_eval_file, 'w') as writer:\n        for key in sorted(results.keys()):\n            writer.write('%s = %s\\n' % (key, str(results[key])))\n    return results",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', default=None, type=str, required=True, help='Model type selected in the list: ' + ', '.join(MODEL_CLASSES.keys()))\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model checkpoints and predictions will be written.')\n    parser.add_argument('--data_dir', default=None, type=str, help='The input data dir. Should contain the .json files for the task.' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--train_file', default=None, type=str, help='The input training file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--predict_file', default=None, type=str, help='The input evaluation file. If a data dir is specified, will look for the file there' + 'If no data dir or train/predict files are specified, will run with tensorflow_datasets.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--cache_dir', default='', type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, the SQuAD examples contain some that do not have an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='If null_score - best_non_null is greater than the threshold predict null.')\n    parser.add_argument('--max_seq_length', default=384, type=int, help='The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.')\n    parser.add_argument('--doc_stride', default=128, type=int, help='When splitting up a long document into chunks, how much stride to take between chunks.')\n    parser.add_argument('--max_query_length', default=64, type=int, help='The maximum number of tokens for the question. Questions longer than this will be truncated to this length.')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_eval', action='store_true', help='Whether to run eval on the dev set.')\n    parser.add_argument('--evaluate_during_training', action='store_true', help='Run evaluation during training at each logging step.')\n    parser.add_argument('--do_lower_case', action='store_true', help='Set this flag if you are using an uncased model.')\n    parser.add_argument('--per_gpu_train_batch_size', default=8, type=int, help='Batch size per GPU/CPU for training.')\n    parser.add_argument('--per_gpu_eval_batch_size', default=8, type=int, help='Batch size per GPU/CPU for evaluation.')\n    parser.add_argument('--learning_rate', default=5e-05, type=float, help='The initial learning rate for Adam.')\n    parser.add_argument('--mask_scores_learning_rate', default=0.01, type=float, help='The Adam initial learning rate of the mask scores.')\n    parser.add_argument('--initial_threshold', default=1.0, type=float, help='Initial value of the threshold (for scheduling).')\n    parser.add_argument('--final_threshold', default=0.7, type=float, help='Final value of the threshold (for scheduling).')\n    parser.add_argument('--initial_warmup', default=1, type=int, help='Run `initial_warmup` * `warmup_steps` steps of threshold warmup during which threshold stays at its `initial_threshold` value (sparsity schedule).')\n    parser.add_argument('--final_warmup', default=2, type=int, help='Run `final_warmup` * `warmup_steps` steps of threshold cool-down during which threshold stays at its final_threshold value (sparsity schedule).')\n    parser.add_argument('--pruning_method', default='topK', type=str, help='Pruning Method (l0 = L0 regularization, magnitude = Magnitude pruning, topK = Movement pruning, sigmoied_threshold = Soft movement pruning).')\n    parser.add_argument('--mask_init', default='constant', type=str, help='Initialization method for the mask scores. Choices: constant, uniform, kaiming.')\n    parser.add_argument('--mask_scale', default=0.0, type=float, help='Initialization parameter for the chosen initialization method.')\n    parser.add_argument('--regularization', default=None, help='Add L0 or L1 regularization to the mask scores.')\n    parser.add_argument('--final_lambda', default=0.0, type=float, help='Regularization intensity (used in conjunction with `regularization`.')\n    parser.add_argument('--global_topk', action='store_true', help='Global TopK on the Scores.')\n    parser.add_argument('--global_topk_frequency_compute', default=25, type=int, help='Frequency at which we compute the TopK global threshold.')\n    parser.add_argument('--teacher_type', default=None, type=str, help='Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for distillation.')\n    parser.add_argument('--teacher_name_or_path', default=None, type=str, help='Path to the already SQuAD fine-tuned teacher model. Only for distillation.')\n    parser.add_argument('--alpha_ce', default=0.5, type=float, help='Cross entropy loss linear weight. Only for distillation.')\n    parser.add_argument('--alpha_distil', default=0.5, type=float, help='Distillation loss linear weight. Only for distillation.')\n    parser.add_argument('--temperature', default=2.0, type=float, help='Distillation temperature. Only for distillation.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--weight_decay', default=0.0, type=float, help='Weight decay if we apply some.')\n    parser.add_argument('--adam_epsilon', default=1e-08, type=float, help='Epsilon for Adam optimizer.')\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')\n    parser.add_argument('--num_train_epochs', default=3.0, type=float, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_steps', default=-1, type=int, help='If > 0: set total number of training steps to perform. Override num_train_epochs.')\n    parser.add_argument('--warmup_steps', default=0, type=int, help='Linear warmup over warmup_steps.')\n    parser.add_argument('--n_best_size', default=20, type=int, help='The total number of n-best predictions to generate in the nbest_predictions.json output file.')\n    parser.add_argument('--max_answer_length', default=30, type=int, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--verbose_logging', action='store_true', help='If true, all of the warnings related to data processing will be printed. A number of warnings are expected for a normal SQuAD evaluation.')\n    parser.add_argument('--lang_id', default=0, type=int, help='language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)')\n    parser.add_argument('--logging_steps', type=int, default=500, help='Log every X updates steps.')\n    parser.add_argument('--save_steps', type=int, default=500, help='Save checkpoint every X updates steps.')\n    parser.add_argument('--eval_all_checkpoints', action='store_true', help='Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Overwrite the content of the output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--seed', type=int, default=42, help='random seed for initialization')\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--fp16', action='store_true', help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit')\n    parser.add_argument('--fp16_opt_level', type=str, default='O1', help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html\")\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--threads', type=int, default=1, help='multiple threads for converting example to features')\n    args = parser.parse_args()\n    if args.regularization == 'null':\n        args.regularization = None\n    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n        logger.warning(\"WARNING - You've set a doc stride which may be superior to the document length in some examples. This could result in errors when building features from the examples. Please reduce the doc stride or increase the maximum length to ensure the features are correctly built.\")\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and (not args.overwrite_output_dir):\n        raise ValueError('Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.'.format(args.output_dir))\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        torch.distributed.init_process_group(backend='nccl')\n        args.n_gpu = 1\n    args.device = device\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning('Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s', args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    set_seed(args)\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()\n    args.model_type = args.model_type.lower()\n    (config_class, model_class, tokenizer_class) = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None, pruning_method=args.pruning_method, mask_init=args.mask_init, mask_scale=args.mask_scale)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case, cache_dir=args.cache_dir if args.cache_dir else None)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.teacher_type is not None:\n        assert args.teacher_name_or_path is not None\n        assert args.alpha_distil > 0.0\n        assert args.alpha_distil + args.alpha_ce > 0.0\n        (teacher_config_class, teacher_model_class, _) = MODEL_CLASSES[args.teacher_type]\n        teacher_config = teacher_config_class.from_pretrained(args.teacher_name_or_path)\n        teacher = teacher_model_class.from_pretrained(args.teacher_name_or_path, from_tf=False, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None)\n        teacher.to(args.device)\n    else:\n        teacher = None\n    if args.local_rank == 0:\n        torch.distributed.barrier()\n    model.to(args.device)\n    logger.info('Training/evaluation parameters %s', args)\n    if args.fp16:\n        try:\n            import apex\n            apex.amp.register_half_function(torch, 'einsum')\n        except ImportError:\n            raise ImportError('Please install apex from https://www.github.com/nvidia/apex to use fp16 training.')\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n        (global_step, tr_loss) = train(args, train_dataset, model, tokenizer, teacher=teacher)\n        logger.info(' global_step = %s, average loss = %s', global_step, tr_loss)\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        logger.info('Saving model checkpoint to %s', args.output_dir)\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        model.to(args.device)\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        if args.do_train:\n            logger.info('Loading checkpoints saved during training for evaluation')\n            checkpoints = [args.output_dir]\n            if args.eval_all_checkpoints:\n                checkpoints = [os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True))]\n        else:\n            logger.info('Loading checkpoint %s for evaluation', args.model_name_or_path)\n            checkpoints = [args.model_name_or_path]\n        logger.info('Evaluate the following checkpoints: %s', checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else ''\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=global_step)\n            result = {k + ('_{}'.format(global_step) if global_step else ''): v for (k, v) in result.items()}\n            results.update(result)\n    logger.info('Results: {}'.format(results))\n    predict_file = list(filter(None, args.predict_file.split('/'))).pop()\n    if not os.path.exists(os.path.join(args.output_dir, predict_file)):\n        os.makedirs(os.path.join(args.output_dir, predict_file))\n    output_eval_file = os.path.join(args.output_dir, predict_file, 'eval_results.txt')\n    with open(output_eval_file, 'w') as writer:\n        for key in sorted(results.keys()):\n            writer.write('%s = %s\\n' % (key, str(results[key])))\n    return results"
        ]
    }
]