[
    {
        "func_name": "fit",
        "original": "def fit(self, data_dictionary: Dict[str, Any], dk: FreqaiDataKitchen, **kwargs):\n    \"\"\"\n        User customizable fit method\n        :param data_dictionary: dict = common data dictionary containing all train/test\n            features/labels/weights.\n        :param dk: FreqaiDatakitchen = data kitchen for current pair.\n        :return:\n        model Any = trained model to be used for inference in dry/live/backtesting\n        \"\"\"\n    train_df = data_dictionary['train_features']\n    total_timesteps = self.freqai_info['rl_config']['train_cycles'] * len(train_df)\n    policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=self.net_arch)\n    if self.activate_tensorboard:\n        tb_path = Path(dk.full_path / 'tensorboard' / dk.pair.split('/')[0])\n    else:\n        tb_path = None\n    if dk.pair not in self.dd.model_dictionary or not self.continual_learning:\n        model = self.MODELCLASS(self.policy_type, self.train_env, policy_kwargs=policy_kwargs, tensorboard_log=tb_path, **self.freqai_info.get('model_training_parameters', {}))\n    else:\n        logger.info('Continual training activated - starting training from previously trained agent.')\n        model = self.dd.model_dictionary[dk.pair]\n        model.set_env(self.train_env)\n    callbacks: List[Any] = [self.eval_callback, self.tensorboard_callback]\n    progressbar_callback: Optional[ProgressBarCallback] = None\n    if self.rl_config.get('progress_bar', False):\n        progressbar_callback = ProgressBarCallback()\n        callbacks.insert(0, progressbar_callback)\n    try:\n        model.learn(total_timesteps=int(total_timesteps), callback=callbacks)\n    finally:\n        if progressbar_callback:\n            progressbar_callback.on_training_end()\n    if Path(dk.data_path / 'best_model.zip').is_file():\n        logger.info('Callback found a best model.')\n        best_model = self.MODELCLASS.load(dk.data_path / 'best_model')\n        return best_model\n    logger.info(\"Couldn't find best model, using final model instead.\")\n    return model",
        "mutated": [
            "def fit(self, data_dictionary: Dict[str, Any], dk: FreqaiDataKitchen, **kwargs):\n    if False:\n        i = 10\n    '\\n        User customizable fit method\\n        :param data_dictionary: dict = common data dictionary containing all train/test\\n            features/labels/weights.\\n        :param dk: FreqaiDatakitchen = data kitchen for current pair.\\n        :return:\\n        model Any = trained model to be used for inference in dry/live/backtesting\\n        '\n    train_df = data_dictionary['train_features']\n    total_timesteps = self.freqai_info['rl_config']['train_cycles'] * len(train_df)\n    policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=self.net_arch)\n    if self.activate_tensorboard:\n        tb_path = Path(dk.full_path / 'tensorboard' / dk.pair.split('/')[0])\n    else:\n        tb_path = None\n    if dk.pair not in self.dd.model_dictionary or not self.continual_learning:\n        model = self.MODELCLASS(self.policy_type, self.train_env, policy_kwargs=policy_kwargs, tensorboard_log=tb_path, **self.freqai_info.get('model_training_parameters', {}))\n    else:\n        logger.info('Continual training activated - starting training from previously trained agent.')\n        model = self.dd.model_dictionary[dk.pair]\n        model.set_env(self.train_env)\n    callbacks: List[Any] = [self.eval_callback, self.tensorboard_callback]\n    progressbar_callback: Optional[ProgressBarCallback] = None\n    if self.rl_config.get('progress_bar', False):\n        progressbar_callback = ProgressBarCallback()\n        callbacks.insert(0, progressbar_callback)\n    try:\n        model.learn(total_timesteps=int(total_timesteps), callback=callbacks)\n    finally:\n        if progressbar_callback:\n            progressbar_callback.on_training_end()\n    if Path(dk.data_path / 'best_model.zip').is_file():\n        logger.info('Callback found a best model.')\n        best_model = self.MODELCLASS.load(dk.data_path / 'best_model')\n        return best_model\n    logger.info(\"Couldn't find best model, using final model instead.\")\n    return model",
            "def fit(self, data_dictionary: Dict[str, Any], dk: FreqaiDataKitchen, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        User customizable fit method\\n        :param data_dictionary: dict = common data dictionary containing all train/test\\n            features/labels/weights.\\n        :param dk: FreqaiDatakitchen = data kitchen for current pair.\\n        :return:\\n        model Any = trained model to be used for inference in dry/live/backtesting\\n        '\n    train_df = data_dictionary['train_features']\n    total_timesteps = self.freqai_info['rl_config']['train_cycles'] * len(train_df)\n    policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=self.net_arch)\n    if self.activate_tensorboard:\n        tb_path = Path(dk.full_path / 'tensorboard' / dk.pair.split('/')[0])\n    else:\n        tb_path = None\n    if dk.pair not in self.dd.model_dictionary or not self.continual_learning:\n        model = self.MODELCLASS(self.policy_type, self.train_env, policy_kwargs=policy_kwargs, tensorboard_log=tb_path, **self.freqai_info.get('model_training_parameters', {}))\n    else:\n        logger.info('Continual training activated - starting training from previously trained agent.')\n        model = self.dd.model_dictionary[dk.pair]\n        model.set_env(self.train_env)\n    callbacks: List[Any] = [self.eval_callback, self.tensorboard_callback]\n    progressbar_callback: Optional[ProgressBarCallback] = None\n    if self.rl_config.get('progress_bar', False):\n        progressbar_callback = ProgressBarCallback()\n        callbacks.insert(0, progressbar_callback)\n    try:\n        model.learn(total_timesteps=int(total_timesteps), callback=callbacks)\n    finally:\n        if progressbar_callback:\n            progressbar_callback.on_training_end()\n    if Path(dk.data_path / 'best_model.zip').is_file():\n        logger.info('Callback found a best model.')\n        best_model = self.MODELCLASS.load(dk.data_path / 'best_model')\n        return best_model\n    logger.info(\"Couldn't find best model, using final model instead.\")\n    return model",
            "def fit(self, data_dictionary: Dict[str, Any], dk: FreqaiDataKitchen, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        User customizable fit method\\n        :param data_dictionary: dict = common data dictionary containing all train/test\\n            features/labels/weights.\\n        :param dk: FreqaiDatakitchen = data kitchen for current pair.\\n        :return:\\n        model Any = trained model to be used for inference in dry/live/backtesting\\n        '\n    train_df = data_dictionary['train_features']\n    total_timesteps = self.freqai_info['rl_config']['train_cycles'] * len(train_df)\n    policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=self.net_arch)\n    if self.activate_tensorboard:\n        tb_path = Path(dk.full_path / 'tensorboard' / dk.pair.split('/')[0])\n    else:\n        tb_path = None\n    if dk.pair not in self.dd.model_dictionary or not self.continual_learning:\n        model = self.MODELCLASS(self.policy_type, self.train_env, policy_kwargs=policy_kwargs, tensorboard_log=tb_path, **self.freqai_info.get('model_training_parameters', {}))\n    else:\n        logger.info('Continual training activated - starting training from previously trained agent.')\n        model = self.dd.model_dictionary[dk.pair]\n        model.set_env(self.train_env)\n    callbacks: List[Any] = [self.eval_callback, self.tensorboard_callback]\n    progressbar_callback: Optional[ProgressBarCallback] = None\n    if self.rl_config.get('progress_bar', False):\n        progressbar_callback = ProgressBarCallback()\n        callbacks.insert(0, progressbar_callback)\n    try:\n        model.learn(total_timesteps=int(total_timesteps), callback=callbacks)\n    finally:\n        if progressbar_callback:\n            progressbar_callback.on_training_end()\n    if Path(dk.data_path / 'best_model.zip').is_file():\n        logger.info('Callback found a best model.')\n        best_model = self.MODELCLASS.load(dk.data_path / 'best_model')\n        return best_model\n    logger.info(\"Couldn't find best model, using final model instead.\")\n    return model",
            "def fit(self, data_dictionary: Dict[str, Any], dk: FreqaiDataKitchen, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        User customizable fit method\\n        :param data_dictionary: dict = common data dictionary containing all train/test\\n            features/labels/weights.\\n        :param dk: FreqaiDatakitchen = data kitchen for current pair.\\n        :return:\\n        model Any = trained model to be used for inference in dry/live/backtesting\\n        '\n    train_df = data_dictionary['train_features']\n    total_timesteps = self.freqai_info['rl_config']['train_cycles'] * len(train_df)\n    policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=self.net_arch)\n    if self.activate_tensorboard:\n        tb_path = Path(dk.full_path / 'tensorboard' / dk.pair.split('/')[0])\n    else:\n        tb_path = None\n    if dk.pair not in self.dd.model_dictionary or not self.continual_learning:\n        model = self.MODELCLASS(self.policy_type, self.train_env, policy_kwargs=policy_kwargs, tensorboard_log=tb_path, **self.freqai_info.get('model_training_parameters', {}))\n    else:\n        logger.info('Continual training activated - starting training from previously trained agent.')\n        model = self.dd.model_dictionary[dk.pair]\n        model.set_env(self.train_env)\n    callbacks: List[Any] = [self.eval_callback, self.tensorboard_callback]\n    progressbar_callback: Optional[ProgressBarCallback] = None\n    if self.rl_config.get('progress_bar', False):\n        progressbar_callback = ProgressBarCallback()\n        callbacks.insert(0, progressbar_callback)\n    try:\n        model.learn(total_timesteps=int(total_timesteps), callback=callbacks)\n    finally:\n        if progressbar_callback:\n            progressbar_callback.on_training_end()\n    if Path(dk.data_path / 'best_model.zip').is_file():\n        logger.info('Callback found a best model.')\n        best_model = self.MODELCLASS.load(dk.data_path / 'best_model')\n        return best_model\n    logger.info(\"Couldn't find best model, using final model instead.\")\n    return model",
            "def fit(self, data_dictionary: Dict[str, Any], dk: FreqaiDataKitchen, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        User customizable fit method\\n        :param data_dictionary: dict = common data dictionary containing all train/test\\n            features/labels/weights.\\n        :param dk: FreqaiDatakitchen = data kitchen for current pair.\\n        :return:\\n        model Any = trained model to be used for inference in dry/live/backtesting\\n        '\n    train_df = data_dictionary['train_features']\n    total_timesteps = self.freqai_info['rl_config']['train_cycles'] * len(train_df)\n    policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=self.net_arch)\n    if self.activate_tensorboard:\n        tb_path = Path(dk.full_path / 'tensorboard' / dk.pair.split('/')[0])\n    else:\n        tb_path = None\n    if dk.pair not in self.dd.model_dictionary or not self.continual_learning:\n        model = self.MODELCLASS(self.policy_type, self.train_env, policy_kwargs=policy_kwargs, tensorboard_log=tb_path, **self.freqai_info.get('model_training_parameters', {}))\n    else:\n        logger.info('Continual training activated - starting training from previously trained agent.')\n        model = self.dd.model_dictionary[dk.pair]\n        model.set_env(self.train_env)\n    callbacks: List[Any] = [self.eval_callback, self.tensorboard_callback]\n    progressbar_callback: Optional[ProgressBarCallback] = None\n    if self.rl_config.get('progress_bar', False):\n        progressbar_callback = ProgressBarCallback()\n        callbacks.insert(0, progressbar_callback)\n    try:\n        model.learn(total_timesteps=int(total_timesteps), callback=callbacks)\n    finally:\n        if progressbar_callback:\n            progressbar_callback.on_training_end()\n    if Path(dk.data_path / 'best_model.zip').is_file():\n        logger.info('Callback found a best model.')\n        best_model = self.MODELCLASS.load(dk.data_path / 'best_model')\n        return best_model\n    logger.info(\"Couldn't find best model, using final model instead.\")\n    return model"
        ]
    },
    {
        "func_name": "calculate_reward",
        "original": "def calculate_reward(self, action: int) -> float:\n    \"\"\"\n            An example reward function. This is the one function that users will likely\n            wish to inject their own creativity into.\n\n                        Warning!\n            This is function is a showcase of functionality designed to show as many possible\n            environment control features as possible. It is also designed to run quickly\n            on small computers. This is a benchmark, it is *not* for live production.\n\n            :param action: int = The action made by the agent for the current candle.\n            :return:\n            float = the reward to give to the agent for current step (used for optimization\n                of weights in NN)\n            \"\"\"\n    if not self._is_valid(action):\n        self.tensorboard_log('invalid', category='actions')\n        return -2\n    pnl = self.get_unrealized_profit()\n    factor = 100.0\n    if action == Actions.Long_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Short_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Neutral.value and self._position == Positions.Neutral:\n        return -1\n    max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n    trade_duration = self._current_tick - self._last_trade_tick\n    if trade_duration <= max_trade_duration:\n        factor *= 1.5\n    elif trade_duration > max_trade_duration:\n        factor *= 0.5\n    if self._position in (Positions.Short, Positions.Long) and action == Actions.Neutral.value:\n        return -1 * trade_duration / max_trade_duration\n    if action == Actions.Long_exit.value and self._position == Positions.Long:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    if action == Actions.Short_exit.value and self._position == Positions.Short:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    return 0.0",
        "mutated": [
            "def calculate_reward(self, action: int) -> float:\n    if False:\n        i = 10\n    '\\n            An example reward function. This is the one function that users will likely\\n            wish to inject their own creativity into.\\n\\n                        Warning!\\n            This is function is a showcase of functionality designed to show as many possible\\n            environment control features as possible. It is also designed to run quickly\\n            on small computers. This is a benchmark, it is *not* for live production.\\n\\n            :param action: int = The action made by the agent for the current candle.\\n            :return:\\n            float = the reward to give to the agent for current step (used for optimization\\n                of weights in NN)\\n            '\n    if not self._is_valid(action):\n        self.tensorboard_log('invalid', category='actions')\n        return -2\n    pnl = self.get_unrealized_profit()\n    factor = 100.0\n    if action == Actions.Long_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Short_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Neutral.value and self._position == Positions.Neutral:\n        return -1\n    max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n    trade_duration = self._current_tick - self._last_trade_tick\n    if trade_duration <= max_trade_duration:\n        factor *= 1.5\n    elif trade_duration > max_trade_duration:\n        factor *= 0.5\n    if self._position in (Positions.Short, Positions.Long) and action == Actions.Neutral.value:\n        return -1 * trade_duration / max_trade_duration\n    if action == Actions.Long_exit.value and self._position == Positions.Long:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    if action == Actions.Short_exit.value and self._position == Positions.Short:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    return 0.0",
            "def calculate_reward(self, action: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n            An example reward function. This is the one function that users will likely\\n            wish to inject their own creativity into.\\n\\n                        Warning!\\n            This is function is a showcase of functionality designed to show as many possible\\n            environment control features as possible. It is also designed to run quickly\\n            on small computers. This is a benchmark, it is *not* for live production.\\n\\n            :param action: int = The action made by the agent for the current candle.\\n            :return:\\n            float = the reward to give to the agent for current step (used for optimization\\n                of weights in NN)\\n            '\n    if not self._is_valid(action):\n        self.tensorboard_log('invalid', category='actions')\n        return -2\n    pnl = self.get_unrealized_profit()\n    factor = 100.0\n    if action == Actions.Long_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Short_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Neutral.value and self._position == Positions.Neutral:\n        return -1\n    max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n    trade_duration = self._current_tick - self._last_trade_tick\n    if trade_duration <= max_trade_duration:\n        factor *= 1.5\n    elif trade_duration > max_trade_duration:\n        factor *= 0.5\n    if self._position in (Positions.Short, Positions.Long) and action == Actions.Neutral.value:\n        return -1 * trade_duration / max_trade_duration\n    if action == Actions.Long_exit.value and self._position == Positions.Long:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    if action == Actions.Short_exit.value and self._position == Positions.Short:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    return 0.0",
            "def calculate_reward(self, action: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n            An example reward function. This is the one function that users will likely\\n            wish to inject their own creativity into.\\n\\n                        Warning!\\n            This is function is a showcase of functionality designed to show as many possible\\n            environment control features as possible. It is also designed to run quickly\\n            on small computers. This is a benchmark, it is *not* for live production.\\n\\n            :param action: int = The action made by the agent for the current candle.\\n            :return:\\n            float = the reward to give to the agent for current step (used for optimization\\n                of weights in NN)\\n            '\n    if not self._is_valid(action):\n        self.tensorboard_log('invalid', category='actions')\n        return -2\n    pnl = self.get_unrealized_profit()\n    factor = 100.0\n    if action == Actions.Long_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Short_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Neutral.value and self._position == Positions.Neutral:\n        return -1\n    max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n    trade_duration = self._current_tick - self._last_trade_tick\n    if trade_duration <= max_trade_duration:\n        factor *= 1.5\n    elif trade_duration > max_trade_duration:\n        factor *= 0.5\n    if self._position in (Positions.Short, Positions.Long) and action == Actions.Neutral.value:\n        return -1 * trade_duration / max_trade_duration\n    if action == Actions.Long_exit.value and self._position == Positions.Long:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    if action == Actions.Short_exit.value and self._position == Positions.Short:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    return 0.0",
            "def calculate_reward(self, action: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n            An example reward function. This is the one function that users will likely\\n            wish to inject their own creativity into.\\n\\n                        Warning!\\n            This is function is a showcase of functionality designed to show as many possible\\n            environment control features as possible. It is also designed to run quickly\\n            on small computers. This is a benchmark, it is *not* for live production.\\n\\n            :param action: int = The action made by the agent for the current candle.\\n            :return:\\n            float = the reward to give to the agent for current step (used for optimization\\n                of weights in NN)\\n            '\n    if not self._is_valid(action):\n        self.tensorboard_log('invalid', category='actions')\n        return -2\n    pnl = self.get_unrealized_profit()\n    factor = 100.0\n    if action == Actions.Long_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Short_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Neutral.value and self._position == Positions.Neutral:\n        return -1\n    max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n    trade_duration = self._current_tick - self._last_trade_tick\n    if trade_duration <= max_trade_duration:\n        factor *= 1.5\n    elif trade_duration > max_trade_duration:\n        factor *= 0.5\n    if self._position in (Positions.Short, Positions.Long) and action == Actions.Neutral.value:\n        return -1 * trade_duration / max_trade_duration\n    if action == Actions.Long_exit.value and self._position == Positions.Long:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    if action == Actions.Short_exit.value and self._position == Positions.Short:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    return 0.0",
            "def calculate_reward(self, action: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n            An example reward function. This is the one function that users will likely\\n            wish to inject their own creativity into.\\n\\n                        Warning!\\n            This is function is a showcase of functionality designed to show as many possible\\n            environment control features as possible. It is also designed to run quickly\\n            on small computers. This is a benchmark, it is *not* for live production.\\n\\n            :param action: int = The action made by the agent for the current candle.\\n            :return:\\n            float = the reward to give to the agent for current step (used for optimization\\n                of weights in NN)\\n            '\n    if not self._is_valid(action):\n        self.tensorboard_log('invalid', category='actions')\n        return -2\n    pnl = self.get_unrealized_profit()\n    factor = 100.0\n    if action == Actions.Long_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Short_enter.value and self._position == Positions.Neutral:\n        return 25\n    if action == Actions.Neutral.value and self._position == Positions.Neutral:\n        return -1\n    max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n    trade_duration = self._current_tick - self._last_trade_tick\n    if trade_duration <= max_trade_duration:\n        factor *= 1.5\n    elif trade_duration > max_trade_duration:\n        factor *= 0.5\n    if self._position in (Positions.Short, Positions.Long) and action == Actions.Neutral.value:\n        return -1 * trade_duration / max_trade_duration\n    if action == Actions.Long_exit.value and self._position == Positions.Long:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    if action == Actions.Short_exit.value and self._position == Positions.Short:\n        if pnl > self.profit_aim * self.rr:\n            factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n        return float(pnl * factor)\n    return 0.0"
        ]
    }
]