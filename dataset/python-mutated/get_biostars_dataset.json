[
    {
        "func_name": "get_biostars_dataset",
        "original": "def get_biostars_dataset(start_idx=9557161, accept_threshold=1000000, sleep=0.1, folder='biostars'):\n    \"\"\"\n    Download BioStarts data set from the official API using GET requests\n\n    Args:\n        start_idx (int): The identifier (UID) of the post to retrieve; 9557161 was the last post included in the dataset\n        accept_threshold (int): stop if this many posts with \"has_accepted\" true are retrieved\n        sleep (float): Amount of time to sleep between requests\n        folder (string): folder to store responses as JSON files\n    Returns:\n        Nothing. Content is saved to individual JSON files for each post.\n    \"\"\"\n    headers = {'Content-Type': 'application/json'}\n    has_accepted_count = 0\n    pbar = tqdm(range(start_idx, 0, -1), desc='Running ...')\n    for idx in pbar:\n        url = f'https://www.biostars.org/api/post/{idx}'\n        file = os.path.join(folder, f'{idx}.json')\n        if os.path.isfile(file):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                if data.get('has_accepted'):\n                    has_accepted_count += 1\n            print(f'MSG: {file} exists. Skipping; Current accepted: {has_accepted_count}')\n            continue\n        r = requests.get(url, headers=headers)\n        if r.status_code == 200:\n            data = r.json()\n            if data.get('has_accepted'):\n                has_accepted_count += 1\n            with open(file, 'w') as f:\n                json.dump(data, f)\n        else:\n            print('ERROR: Retrieving data: ', idx)\n        time.sleep(sleep)\n        if has_accepted_count == accept_threshold:\n            print(f'{accept_threshold} entries with has_accepted found. Stopping.')\n            break\n        pbar.set_description(f'Item: {idx}; Accepted {has_accepted_count}')",
        "mutated": [
            "def get_biostars_dataset(start_idx=9557161, accept_threshold=1000000, sleep=0.1, folder='biostars'):\n    if False:\n        i = 10\n    '\\n    Download BioStarts data set from the official API using GET requests\\n\\n    Args:\\n        start_idx (int): The identifier (UID) of the post to retrieve; 9557161 was the last post included in the dataset\\n        accept_threshold (int): stop if this many posts with \"has_accepted\" true are retrieved\\n        sleep (float): Amount of time to sleep between requests\\n        folder (string): folder to store responses as JSON files\\n    Returns:\\n        Nothing. Content is saved to individual JSON files for each post.\\n    '\n    headers = {'Content-Type': 'application/json'}\n    has_accepted_count = 0\n    pbar = tqdm(range(start_idx, 0, -1), desc='Running ...')\n    for idx in pbar:\n        url = f'https://www.biostars.org/api/post/{idx}'\n        file = os.path.join(folder, f'{idx}.json')\n        if os.path.isfile(file):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                if data.get('has_accepted'):\n                    has_accepted_count += 1\n            print(f'MSG: {file} exists. Skipping; Current accepted: {has_accepted_count}')\n            continue\n        r = requests.get(url, headers=headers)\n        if r.status_code == 200:\n            data = r.json()\n            if data.get('has_accepted'):\n                has_accepted_count += 1\n            with open(file, 'w') as f:\n                json.dump(data, f)\n        else:\n            print('ERROR: Retrieving data: ', idx)\n        time.sleep(sleep)\n        if has_accepted_count == accept_threshold:\n            print(f'{accept_threshold} entries with has_accepted found. Stopping.')\n            break\n        pbar.set_description(f'Item: {idx}; Accepted {has_accepted_count}')",
            "def get_biostars_dataset(start_idx=9557161, accept_threshold=1000000, sleep=0.1, folder='biostars'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Download BioStarts data set from the official API using GET requests\\n\\n    Args:\\n        start_idx (int): The identifier (UID) of the post to retrieve; 9557161 was the last post included in the dataset\\n        accept_threshold (int): stop if this many posts with \"has_accepted\" true are retrieved\\n        sleep (float): Amount of time to sleep between requests\\n        folder (string): folder to store responses as JSON files\\n    Returns:\\n        Nothing. Content is saved to individual JSON files for each post.\\n    '\n    headers = {'Content-Type': 'application/json'}\n    has_accepted_count = 0\n    pbar = tqdm(range(start_idx, 0, -1), desc='Running ...')\n    for idx in pbar:\n        url = f'https://www.biostars.org/api/post/{idx}'\n        file = os.path.join(folder, f'{idx}.json')\n        if os.path.isfile(file):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                if data.get('has_accepted'):\n                    has_accepted_count += 1\n            print(f'MSG: {file} exists. Skipping; Current accepted: {has_accepted_count}')\n            continue\n        r = requests.get(url, headers=headers)\n        if r.status_code == 200:\n            data = r.json()\n            if data.get('has_accepted'):\n                has_accepted_count += 1\n            with open(file, 'w') as f:\n                json.dump(data, f)\n        else:\n            print('ERROR: Retrieving data: ', idx)\n        time.sleep(sleep)\n        if has_accepted_count == accept_threshold:\n            print(f'{accept_threshold} entries with has_accepted found. Stopping.')\n            break\n        pbar.set_description(f'Item: {idx}; Accepted {has_accepted_count}')",
            "def get_biostars_dataset(start_idx=9557161, accept_threshold=1000000, sleep=0.1, folder='biostars'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Download BioStarts data set from the official API using GET requests\\n\\n    Args:\\n        start_idx (int): The identifier (UID) of the post to retrieve; 9557161 was the last post included in the dataset\\n        accept_threshold (int): stop if this many posts with \"has_accepted\" true are retrieved\\n        sleep (float): Amount of time to sleep between requests\\n        folder (string): folder to store responses as JSON files\\n    Returns:\\n        Nothing. Content is saved to individual JSON files for each post.\\n    '\n    headers = {'Content-Type': 'application/json'}\n    has_accepted_count = 0\n    pbar = tqdm(range(start_idx, 0, -1), desc='Running ...')\n    for idx in pbar:\n        url = f'https://www.biostars.org/api/post/{idx}'\n        file = os.path.join(folder, f'{idx}.json')\n        if os.path.isfile(file):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                if data.get('has_accepted'):\n                    has_accepted_count += 1\n            print(f'MSG: {file} exists. Skipping; Current accepted: {has_accepted_count}')\n            continue\n        r = requests.get(url, headers=headers)\n        if r.status_code == 200:\n            data = r.json()\n            if data.get('has_accepted'):\n                has_accepted_count += 1\n            with open(file, 'w') as f:\n                json.dump(data, f)\n        else:\n            print('ERROR: Retrieving data: ', idx)\n        time.sleep(sleep)\n        if has_accepted_count == accept_threshold:\n            print(f'{accept_threshold} entries with has_accepted found. Stopping.')\n            break\n        pbar.set_description(f'Item: {idx}; Accepted {has_accepted_count}')",
            "def get_biostars_dataset(start_idx=9557161, accept_threshold=1000000, sleep=0.1, folder='biostars'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Download BioStarts data set from the official API using GET requests\\n\\n    Args:\\n        start_idx (int): The identifier (UID) of the post to retrieve; 9557161 was the last post included in the dataset\\n        accept_threshold (int): stop if this many posts with \"has_accepted\" true are retrieved\\n        sleep (float): Amount of time to sleep between requests\\n        folder (string): folder to store responses as JSON files\\n    Returns:\\n        Nothing. Content is saved to individual JSON files for each post.\\n    '\n    headers = {'Content-Type': 'application/json'}\n    has_accepted_count = 0\n    pbar = tqdm(range(start_idx, 0, -1), desc='Running ...')\n    for idx in pbar:\n        url = f'https://www.biostars.org/api/post/{idx}'\n        file = os.path.join(folder, f'{idx}.json')\n        if os.path.isfile(file):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                if data.get('has_accepted'):\n                    has_accepted_count += 1\n            print(f'MSG: {file} exists. Skipping; Current accepted: {has_accepted_count}')\n            continue\n        r = requests.get(url, headers=headers)\n        if r.status_code == 200:\n            data = r.json()\n            if data.get('has_accepted'):\n                has_accepted_count += 1\n            with open(file, 'w') as f:\n                json.dump(data, f)\n        else:\n            print('ERROR: Retrieving data: ', idx)\n        time.sleep(sleep)\n        if has_accepted_count == accept_threshold:\n            print(f'{accept_threshold} entries with has_accepted found. Stopping.')\n            break\n        pbar.set_description(f'Item: {idx}; Accepted {has_accepted_count}')",
            "def get_biostars_dataset(start_idx=9557161, accept_threshold=1000000, sleep=0.1, folder='biostars'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Download BioStarts data set from the official API using GET requests\\n\\n    Args:\\n        start_idx (int): The identifier (UID) of the post to retrieve; 9557161 was the last post included in the dataset\\n        accept_threshold (int): stop if this many posts with \"has_accepted\" true are retrieved\\n        sleep (float): Amount of time to sleep between requests\\n        folder (string): folder to store responses as JSON files\\n    Returns:\\n        Nothing. Content is saved to individual JSON files for each post.\\n    '\n    headers = {'Content-Type': 'application/json'}\n    has_accepted_count = 0\n    pbar = tqdm(range(start_idx, 0, -1), desc='Running ...')\n    for idx in pbar:\n        url = f'https://www.biostars.org/api/post/{idx}'\n        file = os.path.join(folder, f'{idx}.json')\n        if os.path.isfile(file):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                if data.get('has_accepted'):\n                    has_accepted_count += 1\n            print(f'MSG: {file} exists. Skipping; Current accepted: {has_accepted_count}')\n            continue\n        r = requests.get(url, headers=headers)\n        if r.status_code == 200:\n            data = r.json()\n            if data.get('has_accepted'):\n                has_accepted_count += 1\n            with open(file, 'w') as f:\n                json.dump(data, f)\n        else:\n            print('ERROR: Retrieving data: ', idx)\n        time.sleep(sleep)\n        if has_accepted_count == accept_threshold:\n            print(f'{accept_threshold} entries with has_accepted found. Stopping.')\n            break\n        pbar.set_description(f'Item: {idx}; Accepted {has_accepted_count}')"
        ]
    },
    {
        "func_name": "extract_accepted_data",
        "original": "def extract_accepted_data(folder='biostars', merged_json_file=None):\n    \"\"\"\n    Extract questions paired with their accepted answers\n\n    Args:\n        folder (string): folder to store responses as JSON files\n        merged_json_file (string): A JSON file with individual post content (from get_biostars_dataset()) merged as a JSON array of objects can be provided\n\n    Returns:\n        Nothing. Content is saved to the file: biostars_qa.parquet\n    \"\"\"\n    if merged_json_file is None:\n        json_files = [file for file in os.listdir(folder) if file.endswith('.json')]\n        all_entries = []\n        for file in tqdm(json_files, desc='Get All Entries'):\n            with open(os.path.join(folder, file), 'r') as f:\n                data = json.load(f)\n                all_entries.append(data)\n        with open(merged_json_file, 'w') as f:\n            json.dump(all_entries, f, indent=2)\n    df = pd.read_json(merged_json_file)\n    questions_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Question')]\n    answers_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Answer')]\n    matched_uids = []\n    for input_str in tqdm(answers_df['url'], desc='Find Matched Answers'):\n        match_obj = re.match('https://www.biostars.org/p/(\\\\d+)/#(\\\\d+)', input_str)\n        question_id = match_obj.group(1)\n        answer_id = match_obj.group(2)\n        output_dict = {'question': question_id, 'answer': answer_id}\n        matched_uids.append(output_dict)\n    matched_qa = []\n    for match in tqdm(matched_uids, desc='Get Matched Answers'):\n        entry = {}\n        entry_obj = questions_df[questions_df['uid'] == int(match['question'])]\n        if entry_obj.empty:\n            continue\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['INSTRUCTION'] = entry_dict['content']\n        entry['SOURCE'] = 'biostars'\n        entry['METADATA'] = f\"\"\"{{\"uid\": {entry_dict['uid']}, \"view_count\": {entry_dict['view_count']}, \"vote_count\": {entry_dict['vote_count']}}}\"\"\"\n        entry_obj = answers_df[answers_df['uid'] == int(match['answer'])]\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['RESPONSE'] = entry_dict['content']\n        sorted_entry = {k: entry[k] for k in ['INSTRUCTION', 'RESPONSE', 'SOURCE', 'METADATA']}\n        matched_qa.append(sorted_entry)\n    with open('matched_biostars_qa.json', 'w') as f:\n        json.dump(matched_qa, f, indent=2)\n    len(matched_qa)\n    tmp = pd.read_json('matched_biostars_qa.json')\n    tmp.to_parquet('biostars_qa.parquet', row_group_size=100, engine='pyarrow')",
        "mutated": [
            "def extract_accepted_data(folder='biostars', merged_json_file=None):\n    if False:\n        i = 10\n    '\\n    Extract questions paired with their accepted answers\\n\\n    Args:\\n        folder (string): folder to store responses as JSON files\\n        merged_json_file (string): A JSON file with individual post content (from get_biostars_dataset()) merged as a JSON array of objects can be provided\\n\\n    Returns:\\n        Nothing. Content is saved to the file: biostars_qa.parquet\\n    '\n    if merged_json_file is None:\n        json_files = [file for file in os.listdir(folder) if file.endswith('.json')]\n        all_entries = []\n        for file in tqdm(json_files, desc='Get All Entries'):\n            with open(os.path.join(folder, file), 'r') as f:\n                data = json.load(f)\n                all_entries.append(data)\n        with open(merged_json_file, 'w') as f:\n            json.dump(all_entries, f, indent=2)\n    df = pd.read_json(merged_json_file)\n    questions_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Question')]\n    answers_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Answer')]\n    matched_uids = []\n    for input_str in tqdm(answers_df['url'], desc='Find Matched Answers'):\n        match_obj = re.match('https://www.biostars.org/p/(\\\\d+)/#(\\\\d+)', input_str)\n        question_id = match_obj.group(1)\n        answer_id = match_obj.group(2)\n        output_dict = {'question': question_id, 'answer': answer_id}\n        matched_uids.append(output_dict)\n    matched_qa = []\n    for match in tqdm(matched_uids, desc='Get Matched Answers'):\n        entry = {}\n        entry_obj = questions_df[questions_df['uid'] == int(match['question'])]\n        if entry_obj.empty:\n            continue\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['INSTRUCTION'] = entry_dict['content']\n        entry['SOURCE'] = 'biostars'\n        entry['METADATA'] = f\"\"\"{{\"uid\": {entry_dict['uid']}, \"view_count\": {entry_dict['view_count']}, \"vote_count\": {entry_dict['vote_count']}}}\"\"\"\n        entry_obj = answers_df[answers_df['uid'] == int(match['answer'])]\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['RESPONSE'] = entry_dict['content']\n        sorted_entry = {k: entry[k] for k in ['INSTRUCTION', 'RESPONSE', 'SOURCE', 'METADATA']}\n        matched_qa.append(sorted_entry)\n    with open('matched_biostars_qa.json', 'w') as f:\n        json.dump(matched_qa, f, indent=2)\n    len(matched_qa)\n    tmp = pd.read_json('matched_biostars_qa.json')\n    tmp.to_parquet('biostars_qa.parquet', row_group_size=100, engine='pyarrow')",
            "def extract_accepted_data(folder='biostars', merged_json_file=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Extract questions paired with their accepted answers\\n\\n    Args:\\n        folder (string): folder to store responses as JSON files\\n        merged_json_file (string): A JSON file with individual post content (from get_biostars_dataset()) merged as a JSON array of objects can be provided\\n\\n    Returns:\\n        Nothing. Content is saved to the file: biostars_qa.parquet\\n    '\n    if merged_json_file is None:\n        json_files = [file for file in os.listdir(folder) if file.endswith('.json')]\n        all_entries = []\n        for file in tqdm(json_files, desc='Get All Entries'):\n            with open(os.path.join(folder, file), 'r') as f:\n                data = json.load(f)\n                all_entries.append(data)\n        with open(merged_json_file, 'w') as f:\n            json.dump(all_entries, f, indent=2)\n    df = pd.read_json(merged_json_file)\n    questions_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Question')]\n    answers_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Answer')]\n    matched_uids = []\n    for input_str in tqdm(answers_df['url'], desc='Find Matched Answers'):\n        match_obj = re.match('https://www.biostars.org/p/(\\\\d+)/#(\\\\d+)', input_str)\n        question_id = match_obj.group(1)\n        answer_id = match_obj.group(2)\n        output_dict = {'question': question_id, 'answer': answer_id}\n        matched_uids.append(output_dict)\n    matched_qa = []\n    for match in tqdm(matched_uids, desc='Get Matched Answers'):\n        entry = {}\n        entry_obj = questions_df[questions_df['uid'] == int(match['question'])]\n        if entry_obj.empty:\n            continue\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['INSTRUCTION'] = entry_dict['content']\n        entry['SOURCE'] = 'biostars'\n        entry['METADATA'] = f\"\"\"{{\"uid\": {entry_dict['uid']}, \"view_count\": {entry_dict['view_count']}, \"vote_count\": {entry_dict['vote_count']}}}\"\"\"\n        entry_obj = answers_df[answers_df['uid'] == int(match['answer'])]\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['RESPONSE'] = entry_dict['content']\n        sorted_entry = {k: entry[k] for k in ['INSTRUCTION', 'RESPONSE', 'SOURCE', 'METADATA']}\n        matched_qa.append(sorted_entry)\n    with open('matched_biostars_qa.json', 'w') as f:\n        json.dump(matched_qa, f, indent=2)\n    len(matched_qa)\n    tmp = pd.read_json('matched_biostars_qa.json')\n    tmp.to_parquet('biostars_qa.parquet', row_group_size=100, engine='pyarrow')",
            "def extract_accepted_data(folder='biostars', merged_json_file=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Extract questions paired with their accepted answers\\n\\n    Args:\\n        folder (string): folder to store responses as JSON files\\n        merged_json_file (string): A JSON file with individual post content (from get_biostars_dataset()) merged as a JSON array of objects can be provided\\n\\n    Returns:\\n        Nothing. Content is saved to the file: biostars_qa.parquet\\n    '\n    if merged_json_file is None:\n        json_files = [file for file in os.listdir(folder) if file.endswith('.json')]\n        all_entries = []\n        for file in tqdm(json_files, desc='Get All Entries'):\n            with open(os.path.join(folder, file), 'r') as f:\n                data = json.load(f)\n                all_entries.append(data)\n        with open(merged_json_file, 'w') as f:\n            json.dump(all_entries, f, indent=2)\n    df = pd.read_json(merged_json_file)\n    questions_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Question')]\n    answers_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Answer')]\n    matched_uids = []\n    for input_str in tqdm(answers_df['url'], desc='Find Matched Answers'):\n        match_obj = re.match('https://www.biostars.org/p/(\\\\d+)/#(\\\\d+)', input_str)\n        question_id = match_obj.group(1)\n        answer_id = match_obj.group(2)\n        output_dict = {'question': question_id, 'answer': answer_id}\n        matched_uids.append(output_dict)\n    matched_qa = []\n    for match in tqdm(matched_uids, desc='Get Matched Answers'):\n        entry = {}\n        entry_obj = questions_df[questions_df['uid'] == int(match['question'])]\n        if entry_obj.empty:\n            continue\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['INSTRUCTION'] = entry_dict['content']\n        entry['SOURCE'] = 'biostars'\n        entry['METADATA'] = f\"\"\"{{\"uid\": {entry_dict['uid']}, \"view_count\": {entry_dict['view_count']}, \"vote_count\": {entry_dict['vote_count']}}}\"\"\"\n        entry_obj = answers_df[answers_df['uid'] == int(match['answer'])]\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['RESPONSE'] = entry_dict['content']\n        sorted_entry = {k: entry[k] for k in ['INSTRUCTION', 'RESPONSE', 'SOURCE', 'METADATA']}\n        matched_qa.append(sorted_entry)\n    with open('matched_biostars_qa.json', 'w') as f:\n        json.dump(matched_qa, f, indent=2)\n    len(matched_qa)\n    tmp = pd.read_json('matched_biostars_qa.json')\n    tmp.to_parquet('biostars_qa.parquet', row_group_size=100, engine='pyarrow')",
            "def extract_accepted_data(folder='biostars', merged_json_file=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Extract questions paired with their accepted answers\\n\\n    Args:\\n        folder (string): folder to store responses as JSON files\\n        merged_json_file (string): A JSON file with individual post content (from get_biostars_dataset()) merged as a JSON array of objects can be provided\\n\\n    Returns:\\n        Nothing. Content is saved to the file: biostars_qa.parquet\\n    '\n    if merged_json_file is None:\n        json_files = [file for file in os.listdir(folder) if file.endswith('.json')]\n        all_entries = []\n        for file in tqdm(json_files, desc='Get All Entries'):\n            with open(os.path.join(folder, file), 'r') as f:\n                data = json.load(f)\n                all_entries.append(data)\n        with open(merged_json_file, 'w') as f:\n            json.dump(all_entries, f, indent=2)\n    df = pd.read_json(merged_json_file)\n    questions_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Question')]\n    answers_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Answer')]\n    matched_uids = []\n    for input_str in tqdm(answers_df['url'], desc='Find Matched Answers'):\n        match_obj = re.match('https://www.biostars.org/p/(\\\\d+)/#(\\\\d+)', input_str)\n        question_id = match_obj.group(1)\n        answer_id = match_obj.group(2)\n        output_dict = {'question': question_id, 'answer': answer_id}\n        matched_uids.append(output_dict)\n    matched_qa = []\n    for match in tqdm(matched_uids, desc='Get Matched Answers'):\n        entry = {}\n        entry_obj = questions_df[questions_df['uid'] == int(match['question'])]\n        if entry_obj.empty:\n            continue\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['INSTRUCTION'] = entry_dict['content']\n        entry['SOURCE'] = 'biostars'\n        entry['METADATA'] = f\"\"\"{{\"uid\": {entry_dict['uid']}, \"view_count\": {entry_dict['view_count']}, \"vote_count\": {entry_dict['vote_count']}}}\"\"\"\n        entry_obj = answers_df[answers_df['uid'] == int(match['answer'])]\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['RESPONSE'] = entry_dict['content']\n        sorted_entry = {k: entry[k] for k in ['INSTRUCTION', 'RESPONSE', 'SOURCE', 'METADATA']}\n        matched_qa.append(sorted_entry)\n    with open('matched_biostars_qa.json', 'w') as f:\n        json.dump(matched_qa, f, indent=2)\n    len(matched_qa)\n    tmp = pd.read_json('matched_biostars_qa.json')\n    tmp.to_parquet('biostars_qa.parquet', row_group_size=100, engine='pyarrow')",
            "def extract_accepted_data(folder='biostars', merged_json_file=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Extract questions paired with their accepted answers\\n\\n    Args:\\n        folder (string): folder to store responses as JSON files\\n        merged_json_file (string): A JSON file with individual post content (from get_biostars_dataset()) merged as a JSON array of objects can be provided\\n\\n    Returns:\\n        Nothing. Content is saved to the file: biostars_qa.parquet\\n    '\n    if merged_json_file is None:\n        json_files = [file for file in os.listdir(folder) if file.endswith('.json')]\n        all_entries = []\n        for file in tqdm(json_files, desc='Get All Entries'):\n            with open(os.path.join(folder, file), 'r') as f:\n                data = json.load(f)\n                all_entries.append(data)\n        with open(merged_json_file, 'w') as f:\n            json.dump(all_entries, f, indent=2)\n    df = pd.read_json(merged_json_file)\n    questions_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Question')]\n    answers_df = df[df['has_accepted'] & (df['vote_count'] > 0) & (df['type'] == 'Answer')]\n    matched_uids = []\n    for input_str in tqdm(answers_df['url'], desc='Find Matched Answers'):\n        match_obj = re.match('https://www.biostars.org/p/(\\\\d+)/#(\\\\d+)', input_str)\n        question_id = match_obj.group(1)\n        answer_id = match_obj.group(2)\n        output_dict = {'question': question_id, 'answer': answer_id}\n        matched_uids.append(output_dict)\n    matched_qa = []\n    for match in tqdm(matched_uids, desc='Get Matched Answers'):\n        entry = {}\n        entry_obj = questions_df[questions_df['uid'] == int(match['question'])]\n        if entry_obj.empty:\n            continue\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['INSTRUCTION'] = entry_dict['content']\n        entry['SOURCE'] = 'biostars'\n        entry['METADATA'] = f\"\"\"{{\"uid\": {entry_dict['uid']}, \"view_count\": {entry_dict['view_count']}, \"vote_count\": {entry_dict['vote_count']}}}\"\"\"\n        entry_obj = answers_df[answers_df['uid'] == int(match['answer'])]\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry['RESPONSE'] = entry_dict['content']\n        sorted_entry = {k: entry[k] for k in ['INSTRUCTION', 'RESPONSE', 'SOURCE', 'METADATA']}\n        matched_qa.append(sorted_entry)\n    with open('matched_biostars_qa.json', 'w') as f:\n        json.dump(matched_qa, f, indent=2)\n    len(matched_qa)\n    tmp = pd.read_json('matched_biostars_qa.json')\n    tmp.to_parquet('biostars_qa.parquet', row_group_size=100, engine='pyarrow')"
        ]
    }
]