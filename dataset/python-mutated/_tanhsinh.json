[
    {
        "func_name": "pre_func_eval",
        "original": "def pre_func_eval(work):\n    work.h = h0 / 2 ** work.n\n    (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n    (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n    xj = work.xj.copy()\n    xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n    xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n    xj[work.ainf] *= -1\n    return xj",
        "mutated": [
            "def pre_func_eval(work):\n    if False:\n        i = 10\n    work.h = h0 / 2 ** work.n\n    (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n    (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n    xj = work.xj.copy()\n    xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n    xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n    xj[work.ainf] *= -1\n    return xj",
            "def pre_func_eval(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    work.h = h0 / 2 ** work.n\n    (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n    (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n    xj = work.xj.copy()\n    xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n    xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n    xj[work.ainf] *= -1\n    return xj",
            "def pre_func_eval(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    work.h = h0 / 2 ** work.n\n    (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n    (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n    xj = work.xj.copy()\n    xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n    xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n    xj[work.ainf] *= -1\n    return xj",
            "def pre_func_eval(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    work.h = h0 / 2 ** work.n\n    (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n    (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n    xj = work.xj.copy()\n    xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n    xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n    xj[work.ainf] *= -1\n    return xj",
            "def pre_func_eval(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    work.h = h0 / 2 ** work.n\n    (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n    (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n    xj = work.xj.copy()\n    xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n    xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n    xj[work.ainf] *= -1\n    return xj"
        ]
    },
    {
        "func_name": "post_func_eval",
        "original": "def post_func_eval(x, fj, work):\n    if work.log:\n        fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n        fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n    else:\n        fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n        fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n    (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n    if work.Sk.shape[-1]:\n        Snm1 = work.Sk[:, -1]\n        Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n    work.fjwj = fjwj\n    work.Sn = Sn",
        "mutated": [
            "def post_func_eval(x, fj, work):\n    if False:\n        i = 10\n    if work.log:\n        fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n        fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n    else:\n        fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n        fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n    (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n    if work.Sk.shape[-1]:\n        Snm1 = work.Sk[:, -1]\n        Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n    work.fjwj = fjwj\n    work.Sn = Sn",
            "def post_func_eval(x, fj, work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if work.log:\n        fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n        fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n    else:\n        fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n        fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n    (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n    if work.Sk.shape[-1]:\n        Snm1 = work.Sk[:, -1]\n        Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n    work.fjwj = fjwj\n    work.Sn = Sn",
            "def post_func_eval(x, fj, work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if work.log:\n        fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n        fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n    else:\n        fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n        fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n    (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n    if work.Sk.shape[-1]:\n        Snm1 = work.Sk[:, -1]\n        Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n    work.fjwj = fjwj\n    work.Sn = Sn",
            "def post_func_eval(x, fj, work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if work.log:\n        fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n        fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n    else:\n        fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n        fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n    (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n    if work.Sk.shape[-1]:\n        Snm1 = work.Sk[:, -1]\n        Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n    work.fjwj = fjwj\n    work.Sn = Sn",
            "def post_func_eval(x, fj, work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if work.log:\n        fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n        fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n    else:\n        fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n        fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n    (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n    if work.Sk.shape[-1]:\n        Snm1 = work.Sk[:, -1]\n        Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n    work.fjwj = fjwj\n    work.Sn = Sn"
        ]
    },
    {
        "func_name": "check_termination",
        "original": "def check_termination(work):\n    \"\"\"Terminate due to convergence or encountering non-finite values\"\"\"\n    stop = np.zeros(work.Sn.shape, dtype=bool)\n    if work.nit == 0:\n        i = (work.a == work.b).ravel()\n        zero = -np.inf if log else 0\n        work.Sn[i] = zero\n        work.aerr[i] = zero\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    else:\n        (work.rerr, work.aerr) = _estimate_error(work)\n        i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    if log:\n        i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n    else:\n        i = ~np.isfinite(work.Sn) & ~stop\n    work.status[i] = _EVALUEERR\n    stop[i] = True\n    return stop",
        "mutated": [
            "def check_termination(work):\n    if False:\n        i = 10\n    'Terminate due to convergence or encountering non-finite values'\n    stop = np.zeros(work.Sn.shape, dtype=bool)\n    if work.nit == 0:\n        i = (work.a == work.b).ravel()\n        zero = -np.inf if log else 0\n        work.Sn[i] = zero\n        work.aerr[i] = zero\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    else:\n        (work.rerr, work.aerr) = _estimate_error(work)\n        i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    if log:\n        i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n    else:\n        i = ~np.isfinite(work.Sn) & ~stop\n    work.status[i] = _EVALUEERR\n    stop[i] = True\n    return stop",
            "def check_termination(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Terminate due to convergence or encountering non-finite values'\n    stop = np.zeros(work.Sn.shape, dtype=bool)\n    if work.nit == 0:\n        i = (work.a == work.b).ravel()\n        zero = -np.inf if log else 0\n        work.Sn[i] = zero\n        work.aerr[i] = zero\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    else:\n        (work.rerr, work.aerr) = _estimate_error(work)\n        i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    if log:\n        i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n    else:\n        i = ~np.isfinite(work.Sn) & ~stop\n    work.status[i] = _EVALUEERR\n    stop[i] = True\n    return stop",
            "def check_termination(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Terminate due to convergence or encountering non-finite values'\n    stop = np.zeros(work.Sn.shape, dtype=bool)\n    if work.nit == 0:\n        i = (work.a == work.b).ravel()\n        zero = -np.inf if log else 0\n        work.Sn[i] = zero\n        work.aerr[i] = zero\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    else:\n        (work.rerr, work.aerr) = _estimate_error(work)\n        i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    if log:\n        i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n    else:\n        i = ~np.isfinite(work.Sn) & ~stop\n    work.status[i] = _EVALUEERR\n    stop[i] = True\n    return stop",
            "def check_termination(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Terminate due to convergence or encountering non-finite values'\n    stop = np.zeros(work.Sn.shape, dtype=bool)\n    if work.nit == 0:\n        i = (work.a == work.b).ravel()\n        zero = -np.inf if log else 0\n        work.Sn[i] = zero\n        work.aerr[i] = zero\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    else:\n        (work.rerr, work.aerr) = _estimate_error(work)\n        i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    if log:\n        i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n    else:\n        i = ~np.isfinite(work.Sn) & ~stop\n    work.status[i] = _EVALUEERR\n    stop[i] = True\n    return stop",
            "def check_termination(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Terminate due to convergence or encountering non-finite values'\n    stop = np.zeros(work.Sn.shape, dtype=bool)\n    if work.nit == 0:\n        i = (work.a == work.b).ravel()\n        zero = -np.inf if log else 0\n        work.Sn[i] = zero\n        work.aerr[i] = zero\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    else:\n        (work.rerr, work.aerr) = _estimate_error(work)\n        i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n        work.status[i] = _ECONVERGED\n        stop[i] = True\n    if log:\n        i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n    else:\n        i = ~np.isfinite(work.Sn) & ~stop\n    work.status[i] = _EVALUEERR\n    stop[i] = True\n    return stop"
        ]
    },
    {
        "func_name": "post_termination_check",
        "original": "def post_termination_check(work):\n    work.n += 1\n    work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n    return",
        "mutated": [
            "def post_termination_check(work):\n    if False:\n        i = 10\n    work.n += 1\n    work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n    return",
            "def post_termination_check(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    work.n += 1\n    work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n    return",
            "def post_termination_check(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    work.n += 1\n    work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n    return",
            "def post_termination_check(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    work.n += 1\n    work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n    return",
            "def post_termination_check(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    work.n += 1\n    work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n    return"
        ]
    },
    {
        "func_name": "customize_result",
        "original": "def customize_result(res, shape):\n    if log and np.any(negative):\n        pi = res['integral'].dtype.type(np.pi)\n        j = np.complex64(1j)\n        res['integral'] = res['integral'] + negative * pi * j\n    else:\n        res['integral'][negative] *= -1\n    res['maxlevel'] = minlevel + res['nit'] - 1\n    res['maxlevel'][res['nit'] == 0] = -1\n    del res['nit']\n    return shape",
        "mutated": [
            "def customize_result(res, shape):\n    if False:\n        i = 10\n    if log and np.any(negative):\n        pi = res['integral'].dtype.type(np.pi)\n        j = np.complex64(1j)\n        res['integral'] = res['integral'] + negative * pi * j\n    else:\n        res['integral'][negative] *= -1\n    res['maxlevel'] = minlevel + res['nit'] - 1\n    res['maxlevel'][res['nit'] == 0] = -1\n    del res['nit']\n    return shape",
            "def customize_result(res, shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if log and np.any(negative):\n        pi = res['integral'].dtype.type(np.pi)\n        j = np.complex64(1j)\n        res['integral'] = res['integral'] + negative * pi * j\n    else:\n        res['integral'][negative] *= -1\n    res['maxlevel'] = minlevel + res['nit'] - 1\n    res['maxlevel'][res['nit'] == 0] = -1\n    del res['nit']\n    return shape",
            "def customize_result(res, shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if log and np.any(negative):\n        pi = res['integral'].dtype.type(np.pi)\n        j = np.complex64(1j)\n        res['integral'] = res['integral'] + negative * pi * j\n    else:\n        res['integral'][negative] *= -1\n    res['maxlevel'] = minlevel + res['nit'] - 1\n    res['maxlevel'][res['nit'] == 0] = -1\n    del res['nit']\n    return shape",
            "def customize_result(res, shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if log and np.any(negative):\n        pi = res['integral'].dtype.type(np.pi)\n        j = np.complex64(1j)\n        res['integral'] = res['integral'] + negative * pi * j\n    else:\n        res['integral'][negative] *= -1\n    res['maxlevel'] = minlevel + res['nit'] - 1\n    res['maxlevel'][res['nit'] == 0] = -1\n    del res['nit']\n    return shape",
            "def customize_result(res, shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if log and np.any(negative):\n        pi = res['integral'].dtype.type(np.pi)\n        j = np.complex64(1j)\n        res['integral'] = res['integral'] + negative * pi * j\n    else:\n        res['integral'][negative] *= -1\n    res['maxlevel'] = minlevel + res['nit'] - 1\n    res['maxlevel'][res['nit'] == 0] = -1\n    del res['nit']\n    return shape"
        ]
    },
    {
        "func_name": "_tanhsinh",
        "original": "def _tanhsinh(f, a, b, *, args=(), log=False, maxfun=None, maxlevel=None, minlevel=2, atol=None, rtol=None, callback=None):\n    \"\"\"Evaluate a convergent integral numerically using tanh-sinh quadrature.\n\n    In practice, tanh-sinh quadrature achieves quadratic convergence for\n    many integrands: the number of accurate *digits* scales roughly linearly\n    with the number of function evaluations [1]_.\n\n    Either or both of the limits of integration may be infinite, and\n    singularities at the endpoints are acceptable. Divergent integrals and\n    integrands with non-finite derivatives or singularities within an interval\n    are out of scope, but the latter may be evaluated be calling `_tanhsinh` on\n    each sub-interval separately.\n\n    Parameters\n    ----------\n    f : callable\n        The function to be integrated. The signature must be::\n            func(x: ndarray, *args) -> ndarray\n         where each element of ``x`` is a finite real and ``args`` is a tuple,\n         which may contain an arbitrary number of arrays that are broadcastable\n         with `x`. ``func`` must be an elementwise function: each element\n         ``func(x)[i]`` must equal ``func(x[i])`` for all indices ``i``.\n         If ``func`` returns a value with complex dtype when evaluated at\n         either endpoint, subsequent arguments ``x`` will have complex dtype\n         (but zero imaginary part).\n    a, b : array_like\n        Real lower and upper limits of integration. Must be broadcastable.\n        Elements may be infinite.\n    args : tuple, optional\n        Additional positional arguments to be passed to `func`. Must be arrays\n        broadcastable with `a` and `b`. If the callable to be integrated\n        requires arguments that are not broadcastable with `a` and `b`, wrap\n        that callable with `f`. See Examples.\n    log : bool, default: False\n        Setting to True indicates that `f` returns the log of the integrand\n        and that `atol` and `rtol` are expressed as the logs of the absolute\n        and relative errors. In this case, the result object will contain the\n        log of the integral and error. This is useful for integrands for which\n        numerical underflow or overflow would lead to inaccuracies.\n        When ``log=True``, the integrand (the exponential of `f`) must be real,\n        but it may be negative, in which case the log of the integrand is a\n        complex number with an imaginary part that is an odd multiple of \u03c0.\n    maxlevel : int, default: 10\n        The maximum refinement level of the algorithm.\n\n        At the zeroth level, `f` is called once, performing 16 function\n        evaluations. At each subsequent level, `f` is called once more,\n        approximately doubling the number of function evaluations that have\n        been performed. Accordingly, for many integrands, each successive level\n        will double the number of accurate digits in the result (up to the\n        limits of floating point precision).\n\n        The algorithm will terminate after completing level `maxlevel` or after\n        another termination condition is satisfied, whichever comes first.\n    minlevel : int, default: 2\n        The level at which to begin iteration (default: 2). This does not\n        change the total number of function evaluations or the abscissae at\n        which the function is evaluated; it changes only the *number of times*\n        `f` is called. If ``minlevel=k``, then the integrand is evaluated at\n        all abscissae from levels ``0`` through ``k`` in a single call.\n        Note that if `minlevel` exceeds `maxlevel`, the provided `minlevel` is\n        ignored, and `minlevel` is set equal to `maxlevel`.\n    atol, rtol : float, optional\n        Absolute termination tolerance (default: 0) and relative termination\n        tolerance (default: ``eps**0.75``, where ``eps`` is the precision of\n        the result dtype), respectively. The error estimate is as\n        described in [1]_ Section 5. While not theoretically rigorous or\n        conservative, it is said to work well in practice. Must be non-negative\n        and finite if `log` is False, and must be expressed as the log of a\n        non-negative and finite number if `log` is True.\n    callback : callable, optional\n        An optional user-supplied function to be called before the first\n        iteration and after each iteration.\n        Called as ``callback(res)``, where ``res`` is an ``OptimizeResult``\n        similar to that returned by `_differentiate` (but containing the\n        current iterate's values of all variables). If `callback` raises a\n        ``StopIteration``, the algorithm will terminate immediately and\n        `_tanhsinh` will return a result object.\n\n    Returns\n    -------\n    res : OptimizeResult\n        An instance of `scipy.optimize.OptimizeResult` with the following\n        attributes. (The descriptions are written as though the values will be\n        scalars; however, if `func` returns an array, the outputs will be\n        arrays of the same shape.)\n        success : bool\n            ``True`` when the algorithm terminated successfully (status ``0``).\n        status : int\n            An integer representing the exit status of the algorithm.\n            ``0`` : The algorithm converged to the specified tolerances.\n            ``-1`` : (unused)\n            ``-2`` : The maximum number of iterations was reached.\n            ``-3`` : A non-finite value was encountered.\n            ``-4`` : Iteration was terminated by `callback`.\n            ``1`` : The algorithm is proceeding normally (in `callback` only).\n        integral : float\n            An estimate of the integral\n        error : float\n            An estimate of the error. Only available if level two or higher\n            has been completed; otherwise NaN.\n        nit : int\n            The number of iterations performed.\n        nfev : int\n            The number of points at which `func` was evaluated.\n\n    See Also\n    --------\n    quad, quadrature\n\n    Notes\n    -----\n    Implements the algorithm as described in [1]_ with minor adaptations for\n    finite-precision arithmetic, including some described by [2]_ and [3]_. The\n    tanh-sinh scheme was originally introduced in [4]_.\n\n    Due floating-point error in the abscissae, the function may be evaluated\n    at the endpoints of the interval during iterations. The values returned by\n    the function at the endpoints will be ignored.\n\n    References\n    ----------\n    [1] Bailey, David H., Karthik Jeyabalan, and Xiaoye S. Li. \"A comparison of\n        three high-precision quadrature schemes.\" Experimental Mathematics 14.3\n        (2005): 317-329.\n    [2] Vanherck, Joren, Bart Sor\u00e9e, and Wim Magnus. \"Tanh-sinh quadrature for\n        single and multiple integration using floating-point arithmetic.\"\n        arXiv preprint arXiv:2007.15057 (2020).\n    [3] van Engelen, Robert A.  \"Improving the Double Exponential Quadrature\n        Tanh-Sinh, Sinh-Sinh and Exp-Sinh Formulas.\"\n        https://www.genivia.com/files/qthsh.pdf\n    [4] Takahasi, Hidetosi, and Masatake Mori. \"Double exponential formulas for\n        numerical integration.\" Publications of the Research Institute for\n        Mathematical Sciences 9.3 (1974): 721-741.\n\n    Example\n    -------\n    Evaluate the Gaussian integral:\n\n    >>> import numpy as np\n    >>> from scipy.integrate._tanhsinh import _tanhsinh\n    >>> def f(x):\n    ...     return np.exp(-x**2)\n    >>> res = _tanhsinh(f, -np.inf, np.inf)\n    >>> res.integral  # true value is np.sqrt(np.pi), 1.7724538509055159\n     1.7724538509055159\n    >>> res.error  # actual error is 0\n    4.0007963937534104e-16\n\n    The value of the Gaussian function (bell curve) is nearly zero for\n    arguments sufficiently far from zero, so the value of the integral\n    over a finite interval is nearly the same.\n\n    >>> _tanhsinh(f, -20, 20).integral\n    1.772453850905518\n\n    However, with unfavorable integration limits, the integration scheme\n    may not be able to find the important region.\n\n    >>> _tanhsinh(f, -np.inf, 1000).integral\n    4.500490856620352\n\n    In such cases, or when there are singularities within the interval,\n    break the integral into parts with endpoints at the important points.\n\n    >>> _tanhsinh(f, -np.inf, 0).integral + _tanhsinh(f, 0, 1000).integral\n    1.772453850905404\n\n    For integration involving very large or very small magnitudes, use\n    log-integration. (For illustrative purposes, the following example shows a\n    case in which both regular and log-integration work, but for more extreme\n    limits of integration, log-integration would avoid the underflow\n    experienced when evaluating the integral normally.)\n\n    >>> res = _tanhsinh(f, 20, 30, rtol=1e-10)\n    >>> res.integral, res.error\n    4.7819613911309014e-176, 4.670364401645202e-187\n    >>> def log_f(x):\n    ...     return -x**2\n    >>> np.exp(res.integral), np.exp(res.error)\n    4.7819613911306924e-176, 4.670364401645093e-187\n\n    The limits of integration and elements of `args` may be broadcastable\n    arrays, and integration is performed elementwise.\n\n    >>> from scipy import stats\n    >>> dist = stats.gausshyper(13.8, 3.12, 2.51, 5.18)\n    >>> a, b = dist.support()\n    >>> x = np.linspace(a, b, 100)\n    >>> res = _tanhsinh(dist.pdf, a, x)\n    >>> ref = dist.cdf(x)\n    >>> np.allclose(res.integral, ref)\n\n    \"\"\"\n    tmp = (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)\n    tmp = _tanhsinh_iv(*tmp)\n    (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback) = tmp\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        c = ((a.ravel() + b.ravel()) / 2).reshape(a.shape)\n        c[np.isinf(a)] = b[np.isinf(a)]\n        c[np.isinf(b)] = a[np.isinf(b)]\n        c[np.isnan(c)] = 0\n        tmp = _scalar_optimization_initialize(f, (c,), args, complex_ok=True)\n    (xs, fs, args, shape, dtype) = tmp\n    a = np.broadcast_to(a, shape).astype(dtype).ravel()\n    b = np.broadcast_to(b, shape).astype(dtype).ravel()\n    (a, b, a0, negative, abinf, ainf, binf) = _transform_integrals(a, b)\n    (nit, nfev) = (0, 1)\n    zero = -np.inf if log else 0\n    pi = dtype.type(np.pi)\n    maxiter = maxlevel - minlevel + 1\n    eps = np.finfo(dtype).eps\n    if rtol is None:\n        rtol = 0.75 * np.log(eps) if log else eps ** 0.75\n    Sn = np.full(shape, zero, dtype=dtype).ravel()\n    Sn[np.isnan(a) | np.isnan(b) | np.isnan(fs[0])] = np.nan\n    Sk = np.empty_like(Sn).reshape(-1, 1)[:, 0:0]\n    aerr = np.full(shape, np.nan, dtype=dtype).ravel()\n    status = np.full(shape, _EINPROGRESS, dtype=int).ravel()\n    h0 = np.real(_get_base_step(dtype=dtype))\n    xr0 = np.full(shape, -np.inf, dtype=dtype).ravel()\n    fr0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wr0 = np.zeros(shape, dtype=dtype).ravel()\n    xl0 = np.full(shape, np.inf, dtype=dtype).ravel()\n    fl0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wl0 = np.zeros(shape, dtype=dtype).ravel()\n    d4 = np.zeros(shape, dtype=dtype).ravel()\n    work = OptimizeResult(Sn=Sn, Sk=Sk, aerr=aerr, h=h0, log=log, dtype=dtype, pi=pi, eps=eps, a=a.reshape(-1, 1), b=b.reshape(-1, 1), n=minlevel, nit=nit, nfev=nfev, status=status, xr0=xr0, fr0=fr0, wr0=wr0, xl0=xl0, fl0=fl0, wl0=wl0, d4=d4, ainf=ainf, binf=binf, abinf=abinf, a0=a0.reshape(-1, 1))\n    res_work_pairs = [('status', 'status'), ('integral', 'Sn'), ('error', 'aerr'), ('nit', 'nit'), ('nfev', 'nfev')]\n\n    def pre_func_eval(work):\n        work.h = h0 / 2 ** work.n\n        (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n        (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n        xj = work.xj.copy()\n        xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n        xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n        xj[work.ainf] *= -1\n        return xj\n\n    def post_func_eval(x, fj, work):\n        if work.log:\n            fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n            fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n        else:\n            fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n            fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n        (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n        if work.Sk.shape[-1]:\n            Snm1 = work.Sk[:, -1]\n            Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n        work.fjwj = fjwj\n        work.Sn = Sn\n\n    def check_termination(work):\n        \"\"\"Terminate due to convergence or encountering non-finite values\"\"\"\n        stop = np.zeros(work.Sn.shape, dtype=bool)\n        if work.nit == 0:\n            i = (work.a == work.b).ravel()\n            zero = -np.inf if log else 0\n            work.Sn[i] = zero\n            work.aerr[i] = zero\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        else:\n            (work.rerr, work.aerr) = _estimate_error(work)\n            i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        if log:\n            i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n        else:\n            i = ~np.isfinite(work.Sn) & ~stop\n        work.status[i] = _EVALUEERR\n        stop[i] = True\n        return stop\n\n    def post_termination_check(work):\n        work.n += 1\n        work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n        return\n\n    def customize_result(res, shape):\n        if log and np.any(negative):\n            pi = res['integral'].dtype.type(np.pi)\n            j = np.complex64(1j)\n            res['integral'] = res['integral'] + negative * pi * j\n        else:\n            res['integral'][negative] *= -1\n        res['maxlevel'] = minlevel + res['nit'] - 1\n        res['maxlevel'][res['nit'] == 0] = -1\n        del res['nit']\n        return shape\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        res = _scalar_optimization_loop(work, callback, shape, maxiter, f, args, dtype, pre_func_eval, post_func_eval, check_termination, post_termination_check, customize_result, res_work_pairs)\n    return res",
        "mutated": [
            "def _tanhsinh(f, a, b, *, args=(), log=False, maxfun=None, maxlevel=None, minlevel=2, atol=None, rtol=None, callback=None):\n    if False:\n        i = 10\n    'Evaluate a convergent integral numerically using tanh-sinh quadrature.\\n\\n    In practice, tanh-sinh quadrature achieves quadratic convergence for\\n    many integrands: the number of accurate *digits* scales roughly linearly\\n    with the number of function evaluations [1]_.\\n\\n    Either or both of the limits of integration may be infinite, and\\n    singularities at the endpoints are acceptable. Divergent integrals and\\n    integrands with non-finite derivatives or singularities within an interval\\n    are out of scope, but the latter may be evaluated be calling `_tanhsinh` on\\n    each sub-interval separately.\\n\\n    Parameters\\n    ----------\\n    f : callable\\n        The function to be integrated. The signature must be::\\n            func(x: ndarray, *args) -> ndarray\\n         where each element of ``x`` is a finite real and ``args`` is a tuple,\\n         which may contain an arbitrary number of arrays that are broadcastable\\n         with `x`. ``func`` must be an elementwise function: each element\\n         ``func(x)[i]`` must equal ``func(x[i])`` for all indices ``i``.\\n         If ``func`` returns a value with complex dtype when evaluated at\\n         either endpoint, subsequent arguments ``x`` will have complex dtype\\n         (but zero imaginary part).\\n    a, b : array_like\\n        Real lower and upper limits of integration. Must be broadcastable.\\n        Elements may be infinite.\\n    args : tuple, optional\\n        Additional positional arguments to be passed to `func`. Must be arrays\\n        broadcastable with `a` and `b`. If the callable to be integrated\\n        requires arguments that are not broadcastable with `a` and `b`, wrap\\n        that callable with `f`. See Examples.\\n    log : bool, default: False\\n        Setting to True indicates that `f` returns the log of the integrand\\n        and that `atol` and `rtol` are expressed as the logs of the absolute\\n        and relative errors. In this case, the result object will contain the\\n        log of the integral and error. This is useful for integrands for which\\n        numerical underflow or overflow would lead to inaccuracies.\\n        When ``log=True``, the integrand (the exponential of `f`) must be real,\\n        but it may be negative, in which case the log of the integrand is a\\n        complex number with an imaginary part that is an odd multiple of \u03c0.\\n    maxlevel : int, default: 10\\n        The maximum refinement level of the algorithm.\\n\\n        At the zeroth level, `f` is called once, performing 16 function\\n        evaluations. At each subsequent level, `f` is called once more,\\n        approximately doubling the number of function evaluations that have\\n        been performed. Accordingly, for many integrands, each successive level\\n        will double the number of accurate digits in the result (up to the\\n        limits of floating point precision).\\n\\n        The algorithm will terminate after completing level `maxlevel` or after\\n        another termination condition is satisfied, whichever comes first.\\n    minlevel : int, default: 2\\n        The level at which to begin iteration (default: 2). This does not\\n        change the total number of function evaluations or the abscissae at\\n        which the function is evaluated; it changes only the *number of times*\\n        `f` is called. If ``minlevel=k``, then the integrand is evaluated at\\n        all abscissae from levels ``0`` through ``k`` in a single call.\\n        Note that if `minlevel` exceeds `maxlevel`, the provided `minlevel` is\\n        ignored, and `minlevel` is set equal to `maxlevel`.\\n    atol, rtol : float, optional\\n        Absolute termination tolerance (default: 0) and relative termination\\n        tolerance (default: ``eps**0.75``, where ``eps`` is the precision of\\n        the result dtype), respectively. The error estimate is as\\n        described in [1]_ Section 5. While not theoretically rigorous or\\n        conservative, it is said to work well in practice. Must be non-negative\\n        and finite if `log` is False, and must be expressed as the log of a\\n        non-negative and finite number if `log` is True.\\n    callback : callable, optional\\n        An optional user-supplied function to be called before the first\\n        iteration and after each iteration.\\n        Called as ``callback(res)``, where ``res`` is an ``OptimizeResult``\\n        similar to that returned by `_differentiate` (but containing the\\n        current iterate\\'s values of all variables). If `callback` raises a\\n        ``StopIteration``, the algorithm will terminate immediately and\\n        `_tanhsinh` will return a result object.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        An instance of `scipy.optimize.OptimizeResult` with the following\\n        attributes. (The descriptions are written as though the values will be\\n        scalars; however, if `func` returns an array, the outputs will be\\n        arrays of the same shape.)\\n        success : bool\\n            ``True`` when the algorithm terminated successfully (status ``0``).\\n        status : int\\n            An integer representing the exit status of the algorithm.\\n            ``0`` : The algorithm converged to the specified tolerances.\\n            ``-1`` : (unused)\\n            ``-2`` : The maximum number of iterations was reached.\\n            ``-3`` : A non-finite value was encountered.\\n            ``-4`` : Iteration was terminated by `callback`.\\n            ``1`` : The algorithm is proceeding normally (in `callback` only).\\n        integral : float\\n            An estimate of the integral\\n        error : float\\n            An estimate of the error. Only available if level two or higher\\n            has been completed; otherwise NaN.\\n        nit : int\\n            The number of iterations performed.\\n        nfev : int\\n            The number of points at which `func` was evaluated.\\n\\n    See Also\\n    --------\\n    quad, quadrature\\n\\n    Notes\\n    -----\\n    Implements the algorithm as described in [1]_ with minor adaptations for\\n    finite-precision arithmetic, including some described by [2]_ and [3]_. The\\n    tanh-sinh scheme was originally introduced in [4]_.\\n\\n    Due floating-point error in the abscissae, the function may be evaluated\\n    at the endpoints of the interval during iterations. The values returned by\\n    the function at the endpoints will be ignored.\\n\\n    References\\n    ----------\\n    [1] Bailey, David H., Karthik Jeyabalan, and Xiaoye S. Li. \"A comparison of\\n        three high-precision quadrature schemes.\" Experimental Mathematics 14.3\\n        (2005): 317-329.\\n    [2] Vanherck, Joren, Bart Sor\u00e9e, and Wim Magnus. \"Tanh-sinh quadrature for\\n        single and multiple integration using floating-point arithmetic.\"\\n        arXiv preprint arXiv:2007.15057 (2020).\\n    [3] van Engelen, Robert A.  \"Improving the Double Exponential Quadrature\\n        Tanh-Sinh, Sinh-Sinh and Exp-Sinh Formulas.\"\\n        https://www.genivia.com/files/qthsh.pdf\\n    [4] Takahasi, Hidetosi, and Masatake Mori. \"Double exponential formulas for\\n        numerical integration.\" Publications of the Research Institute for\\n        Mathematical Sciences 9.3 (1974): 721-741.\\n\\n    Example\\n    -------\\n    Evaluate the Gaussian integral:\\n\\n    >>> import numpy as np\\n    >>> from scipy.integrate._tanhsinh import _tanhsinh\\n    >>> def f(x):\\n    ...     return np.exp(-x**2)\\n    >>> res = _tanhsinh(f, -np.inf, np.inf)\\n    >>> res.integral  # true value is np.sqrt(np.pi), 1.7724538509055159\\n     1.7724538509055159\\n    >>> res.error  # actual error is 0\\n    4.0007963937534104e-16\\n\\n    The value of the Gaussian function (bell curve) is nearly zero for\\n    arguments sufficiently far from zero, so the value of the integral\\n    over a finite interval is nearly the same.\\n\\n    >>> _tanhsinh(f, -20, 20).integral\\n    1.772453850905518\\n\\n    However, with unfavorable integration limits, the integration scheme\\n    may not be able to find the important region.\\n\\n    >>> _tanhsinh(f, -np.inf, 1000).integral\\n    4.500490856620352\\n\\n    In such cases, or when there are singularities within the interval,\\n    break the integral into parts with endpoints at the important points.\\n\\n    >>> _tanhsinh(f, -np.inf, 0).integral + _tanhsinh(f, 0, 1000).integral\\n    1.772453850905404\\n\\n    For integration involving very large or very small magnitudes, use\\n    log-integration. (For illustrative purposes, the following example shows a\\n    case in which both regular and log-integration work, but for more extreme\\n    limits of integration, log-integration would avoid the underflow\\n    experienced when evaluating the integral normally.)\\n\\n    >>> res = _tanhsinh(f, 20, 30, rtol=1e-10)\\n    >>> res.integral, res.error\\n    4.7819613911309014e-176, 4.670364401645202e-187\\n    >>> def log_f(x):\\n    ...     return -x**2\\n    >>> np.exp(res.integral), np.exp(res.error)\\n    4.7819613911306924e-176, 4.670364401645093e-187\\n\\n    The limits of integration and elements of `args` may be broadcastable\\n    arrays, and integration is performed elementwise.\\n\\n    >>> from scipy import stats\\n    >>> dist = stats.gausshyper(13.8, 3.12, 2.51, 5.18)\\n    >>> a, b = dist.support()\\n    >>> x = np.linspace(a, b, 100)\\n    >>> res = _tanhsinh(dist.pdf, a, x)\\n    >>> ref = dist.cdf(x)\\n    >>> np.allclose(res.integral, ref)\\n\\n    '\n    tmp = (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)\n    tmp = _tanhsinh_iv(*tmp)\n    (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback) = tmp\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        c = ((a.ravel() + b.ravel()) / 2).reshape(a.shape)\n        c[np.isinf(a)] = b[np.isinf(a)]\n        c[np.isinf(b)] = a[np.isinf(b)]\n        c[np.isnan(c)] = 0\n        tmp = _scalar_optimization_initialize(f, (c,), args, complex_ok=True)\n    (xs, fs, args, shape, dtype) = tmp\n    a = np.broadcast_to(a, shape).astype(dtype).ravel()\n    b = np.broadcast_to(b, shape).astype(dtype).ravel()\n    (a, b, a0, negative, abinf, ainf, binf) = _transform_integrals(a, b)\n    (nit, nfev) = (0, 1)\n    zero = -np.inf if log else 0\n    pi = dtype.type(np.pi)\n    maxiter = maxlevel - minlevel + 1\n    eps = np.finfo(dtype).eps\n    if rtol is None:\n        rtol = 0.75 * np.log(eps) if log else eps ** 0.75\n    Sn = np.full(shape, zero, dtype=dtype).ravel()\n    Sn[np.isnan(a) | np.isnan(b) | np.isnan(fs[0])] = np.nan\n    Sk = np.empty_like(Sn).reshape(-1, 1)[:, 0:0]\n    aerr = np.full(shape, np.nan, dtype=dtype).ravel()\n    status = np.full(shape, _EINPROGRESS, dtype=int).ravel()\n    h0 = np.real(_get_base_step(dtype=dtype))\n    xr0 = np.full(shape, -np.inf, dtype=dtype).ravel()\n    fr0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wr0 = np.zeros(shape, dtype=dtype).ravel()\n    xl0 = np.full(shape, np.inf, dtype=dtype).ravel()\n    fl0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wl0 = np.zeros(shape, dtype=dtype).ravel()\n    d4 = np.zeros(shape, dtype=dtype).ravel()\n    work = OptimizeResult(Sn=Sn, Sk=Sk, aerr=aerr, h=h0, log=log, dtype=dtype, pi=pi, eps=eps, a=a.reshape(-1, 1), b=b.reshape(-1, 1), n=minlevel, nit=nit, nfev=nfev, status=status, xr0=xr0, fr0=fr0, wr0=wr0, xl0=xl0, fl0=fl0, wl0=wl0, d4=d4, ainf=ainf, binf=binf, abinf=abinf, a0=a0.reshape(-1, 1))\n    res_work_pairs = [('status', 'status'), ('integral', 'Sn'), ('error', 'aerr'), ('nit', 'nit'), ('nfev', 'nfev')]\n\n    def pre_func_eval(work):\n        work.h = h0 / 2 ** work.n\n        (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n        (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n        xj = work.xj.copy()\n        xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n        xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n        xj[work.ainf] *= -1\n        return xj\n\n    def post_func_eval(x, fj, work):\n        if work.log:\n            fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n            fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n        else:\n            fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n            fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n        (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n        if work.Sk.shape[-1]:\n            Snm1 = work.Sk[:, -1]\n            Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n        work.fjwj = fjwj\n        work.Sn = Sn\n\n    def check_termination(work):\n        \"\"\"Terminate due to convergence or encountering non-finite values\"\"\"\n        stop = np.zeros(work.Sn.shape, dtype=bool)\n        if work.nit == 0:\n            i = (work.a == work.b).ravel()\n            zero = -np.inf if log else 0\n            work.Sn[i] = zero\n            work.aerr[i] = zero\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        else:\n            (work.rerr, work.aerr) = _estimate_error(work)\n            i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        if log:\n            i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n        else:\n            i = ~np.isfinite(work.Sn) & ~stop\n        work.status[i] = _EVALUEERR\n        stop[i] = True\n        return stop\n\n    def post_termination_check(work):\n        work.n += 1\n        work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n        return\n\n    def customize_result(res, shape):\n        if log and np.any(negative):\n            pi = res['integral'].dtype.type(np.pi)\n            j = np.complex64(1j)\n            res['integral'] = res['integral'] + negative * pi * j\n        else:\n            res['integral'][negative] *= -1\n        res['maxlevel'] = minlevel + res['nit'] - 1\n        res['maxlevel'][res['nit'] == 0] = -1\n        del res['nit']\n        return shape\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        res = _scalar_optimization_loop(work, callback, shape, maxiter, f, args, dtype, pre_func_eval, post_func_eval, check_termination, post_termination_check, customize_result, res_work_pairs)\n    return res",
            "def _tanhsinh(f, a, b, *, args=(), log=False, maxfun=None, maxlevel=None, minlevel=2, atol=None, rtol=None, callback=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Evaluate a convergent integral numerically using tanh-sinh quadrature.\\n\\n    In practice, tanh-sinh quadrature achieves quadratic convergence for\\n    many integrands: the number of accurate *digits* scales roughly linearly\\n    with the number of function evaluations [1]_.\\n\\n    Either or both of the limits of integration may be infinite, and\\n    singularities at the endpoints are acceptable. Divergent integrals and\\n    integrands with non-finite derivatives or singularities within an interval\\n    are out of scope, but the latter may be evaluated be calling `_tanhsinh` on\\n    each sub-interval separately.\\n\\n    Parameters\\n    ----------\\n    f : callable\\n        The function to be integrated. The signature must be::\\n            func(x: ndarray, *args) -> ndarray\\n         where each element of ``x`` is a finite real and ``args`` is a tuple,\\n         which may contain an arbitrary number of arrays that are broadcastable\\n         with `x`. ``func`` must be an elementwise function: each element\\n         ``func(x)[i]`` must equal ``func(x[i])`` for all indices ``i``.\\n         If ``func`` returns a value with complex dtype when evaluated at\\n         either endpoint, subsequent arguments ``x`` will have complex dtype\\n         (but zero imaginary part).\\n    a, b : array_like\\n        Real lower and upper limits of integration. Must be broadcastable.\\n        Elements may be infinite.\\n    args : tuple, optional\\n        Additional positional arguments to be passed to `func`. Must be arrays\\n        broadcastable with `a` and `b`. If the callable to be integrated\\n        requires arguments that are not broadcastable with `a` and `b`, wrap\\n        that callable with `f`. See Examples.\\n    log : bool, default: False\\n        Setting to True indicates that `f` returns the log of the integrand\\n        and that `atol` and `rtol` are expressed as the logs of the absolute\\n        and relative errors. In this case, the result object will contain the\\n        log of the integral and error. This is useful for integrands for which\\n        numerical underflow or overflow would lead to inaccuracies.\\n        When ``log=True``, the integrand (the exponential of `f`) must be real,\\n        but it may be negative, in which case the log of the integrand is a\\n        complex number with an imaginary part that is an odd multiple of \u03c0.\\n    maxlevel : int, default: 10\\n        The maximum refinement level of the algorithm.\\n\\n        At the zeroth level, `f` is called once, performing 16 function\\n        evaluations. At each subsequent level, `f` is called once more,\\n        approximately doubling the number of function evaluations that have\\n        been performed. Accordingly, for many integrands, each successive level\\n        will double the number of accurate digits in the result (up to the\\n        limits of floating point precision).\\n\\n        The algorithm will terminate after completing level `maxlevel` or after\\n        another termination condition is satisfied, whichever comes first.\\n    minlevel : int, default: 2\\n        The level at which to begin iteration (default: 2). This does not\\n        change the total number of function evaluations or the abscissae at\\n        which the function is evaluated; it changes only the *number of times*\\n        `f` is called. If ``minlevel=k``, then the integrand is evaluated at\\n        all abscissae from levels ``0`` through ``k`` in a single call.\\n        Note that if `minlevel` exceeds `maxlevel`, the provided `minlevel` is\\n        ignored, and `minlevel` is set equal to `maxlevel`.\\n    atol, rtol : float, optional\\n        Absolute termination tolerance (default: 0) and relative termination\\n        tolerance (default: ``eps**0.75``, where ``eps`` is the precision of\\n        the result dtype), respectively. The error estimate is as\\n        described in [1]_ Section 5. While not theoretically rigorous or\\n        conservative, it is said to work well in practice. Must be non-negative\\n        and finite if `log` is False, and must be expressed as the log of a\\n        non-negative and finite number if `log` is True.\\n    callback : callable, optional\\n        An optional user-supplied function to be called before the first\\n        iteration and after each iteration.\\n        Called as ``callback(res)``, where ``res`` is an ``OptimizeResult``\\n        similar to that returned by `_differentiate` (but containing the\\n        current iterate\\'s values of all variables). If `callback` raises a\\n        ``StopIteration``, the algorithm will terminate immediately and\\n        `_tanhsinh` will return a result object.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        An instance of `scipy.optimize.OptimizeResult` with the following\\n        attributes. (The descriptions are written as though the values will be\\n        scalars; however, if `func` returns an array, the outputs will be\\n        arrays of the same shape.)\\n        success : bool\\n            ``True`` when the algorithm terminated successfully (status ``0``).\\n        status : int\\n            An integer representing the exit status of the algorithm.\\n            ``0`` : The algorithm converged to the specified tolerances.\\n            ``-1`` : (unused)\\n            ``-2`` : The maximum number of iterations was reached.\\n            ``-3`` : A non-finite value was encountered.\\n            ``-4`` : Iteration was terminated by `callback`.\\n            ``1`` : The algorithm is proceeding normally (in `callback` only).\\n        integral : float\\n            An estimate of the integral\\n        error : float\\n            An estimate of the error. Only available if level two or higher\\n            has been completed; otherwise NaN.\\n        nit : int\\n            The number of iterations performed.\\n        nfev : int\\n            The number of points at which `func` was evaluated.\\n\\n    See Also\\n    --------\\n    quad, quadrature\\n\\n    Notes\\n    -----\\n    Implements the algorithm as described in [1]_ with minor adaptations for\\n    finite-precision arithmetic, including some described by [2]_ and [3]_. The\\n    tanh-sinh scheme was originally introduced in [4]_.\\n\\n    Due floating-point error in the abscissae, the function may be evaluated\\n    at the endpoints of the interval during iterations. The values returned by\\n    the function at the endpoints will be ignored.\\n\\n    References\\n    ----------\\n    [1] Bailey, David H., Karthik Jeyabalan, and Xiaoye S. Li. \"A comparison of\\n        three high-precision quadrature schemes.\" Experimental Mathematics 14.3\\n        (2005): 317-329.\\n    [2] Vanherck, Joren, Bart Sor\u00e9e, and Wim Magnus. \"Tanh-sinh quadrature for\\n        single and multiple integration using floating-point arithmetic.\"\\n        arXiv preprint arXiv:2007.15057 (2020).\\n    [3] van Engelen, Robert A.  \"Improving the Double Exponential Quadrature\\n        Tanh-Sinh, Sinh-Sinh and Exp-Sinh Formulas.\"\\n        https://www.genivia.com/files/qthsh.pdf\\n    [4] Takahasi, Hidetosi, and Masatake Mori. \"Double exponential formulas for\\n        numerical integration.\" Publications of the Research Institute for\\n        Mathematical Sciences 9.3 (1974): 721-741.\\n\\n    Example\\n    -------\\n    Evaluate the Gaussian integral:\\n\\n    >>> import numpy as np\\n    >>> from scipy.integrate._tanhsinh import _tanhsinh\\n    >>> def f(x):\\n    ...     return np.exp(-x**2)\\n    >>> res = _tanhsinh(f, -np.inf, np.inf)\\n    >>> res.integral  # true value is np.sqrt(np.pi), 1.7724538509055159\\n     1.7724538509055159\\n    >>> res.error  # actual error is 0\\n    4.0007963937534104e-16\\n\\n    The value of the Gaussian function (bell curve) is nearly zero for\\n    arguments sufficiently far from zero, so the value of the integral\\n    over a finite interval is nearly the same.\\n\\n    >>> _tanhsinh(f, -20, 20).integral\\n    1.772453850905518\\n\\n    However, with unfavorable integration limits, the integration scheme\\n    may not be able to find the important region.\\n\\n    >>> _tanhsinh(f, -np.inf, 1000).integral\\n    4.500490856620352\\n\\n    In such cases, or when there are singularities within the interval,\\n    break the integral into parts with endpoints at the important points.\\n\\n    >>> _tanhsinh(f, -np.inf, 0).integral + _tanhsinh(f, 0, 1000).integral\\n    1.772453850905404\\n\\n    For integration involving very large or very small magnitudes, use\\n    log-integration. (For illustrative purposes, the following example shows a\\n    case in which both regular and log-integration work, but for more extreme\\n    limits of integration, log-integration would avoid the underflow\\n    experienced when evaluating the integral normally.)\\n\\n    >>> res = _tanhsinh(f, 20, 30, rtol=1e-10)\\n    >>> res.integral, res.error\\n    4.7819613911309014e-176, 4.670364401645202e-187\\n    >>> def log_f(x):\\n    ...     return -x**2\\n    >>> np.exp(res.integral), np.exp(res.error)\\n    4.7819613911306924e-176, 4.670364401645093e-187\\n\\n    The limits of integration and elements of `args` may be broadcastable\\n    arrays, and integration is performed elementwise.\\n\\n    >>> from scipy import stats\\n    >>> dist = stats.gausshyper(13.8, 3.12, 2.51, 5.18)\\n    >>> a, b = dist.support()\\n    >>> x = np.linspace(a, b, 100)\\n    >>> res = _tanhsinh(dist.pdf, a, x)\\n    >>> ref = dist.cdf(x)\\n    >>> np.allclose(res.integral, ref)\\n\\n    '\n    tmp = (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)\n    tmp = _tanhsinh_iv(*tmp)\n    (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback) = tmp\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        c = ((a.ravel() + b.ravel()) / 2).reshape(a.shape)\n        c[np.isinf(a)] = b[np.isinf(a)]\n        c[np.isinf(b)] = a[np.isinf(b)]\n        c[np.isnan(c)] = 0\n        tmp = _scalar_optimization_initialize(f, (c,), args, complex_ok=True)\n    (xs, fs, args, shape, dtype) = tmp\n    a = np.broadcast_to(a, shape).astype(dtype).ravel()\n    b = np.broadcast_to(b, shape).astype(dtype).ravel()\n    (a, b, a0, negative, abinf, ainf, binf) = _transform_integrals(a, b)\n    (nit, nfev) = (0, 1)\n    zero = -np.inf if log else 0\n    pi = dtype.type(np.pi)\n    maxiter = maxlevel - minlevel + 1\n    eps = np.finfo(dtype).eps\n    if rtol is None:\n        rtol = 0.75 * np.log(eps) if log else eps ** 0.75\n    Sn = np.full(shape, zero, dtype=dtype).ravel()\n    Sn[np.isnan(a) | np.isnan(b) | np.isnan(fs[0])] = np.nan\n    Sk = np.empty_like(Sn).reshape(-1, 1)[:, 0:0]\n    aerr = np.full(shape, np.nan, dtype=dtype).ravel()\n    status = np.full(shape, _EINPROGRESS, dtype=int).ravel()\n    h0 = np.real(_get_base_step(dtype=dtype))\n    xr0 = np.full(shape, -np.inf, dtype=dtype).ravel()\n    fr0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wr0 = np.zeros(shape, dtype=dtype).ravel()\n    xl0 = np.full(shape, np.inf, dtype=dtype).ravel()\n    fl0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wl0 = np.zeros(shape, dtype=dtype).ravel()\n    d4 = np.zeros(shape, dtype=dtype).ravel()\n    work = OptimizeResult(Sn=Sn, Sk=Sk, aerr=aerr, h=h0, log=log, dtype=dtype, pi=pi, eps=eps, a=a.reshape(-1, 1), b=b.reshape(-1, 1), n=minlevel, nit=nit, nfev=nfev, status=status, xr0=xr0, fr0=fr0, wr0=wr0, xl0=xl0, fl0=fl0, wl0=wl0, d4=d4, ainf=ainf, binf=binf, abinf=abinf, a0=a0.reshape(-1, 1))\n    res_work_pairs = [('status', 'status'), ('integral', 'Sn'), ('error', 'aerr'), ('nit', 'nit'), ('nfev', 'nfev')]\n\n    def pre_func_eval(work):\n        work.h = h0 / 2 ** work.n\n        (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n        (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n        xj = work.xj.copy()\n        xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n        xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n        xj[work.ainf] *= -1\n        return xj\n\n    def post_func_eval(x, fj, work):\n        if work.log:\n            fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n            fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n        else:\n            fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n            fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n        (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n        if work.Sk.shape[-1]:\n            Snm1 = work.Sk[:, -1]\n            Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n        work.fjwj = fjwj\n        work.Sn = Sn\n\n    def check_termination(work):\n        \"\"\"Terminate due to convergence or encountering non-finite values\"\"\"\n        stop = np.zeros(work.Sn.shape, dtype=bool)\n        if work.nit == 0:\n            i = (work.a == work.b).ravel()\n            zero = -np.inf if log else 0\n            work.Sn[i] = zero\n            work.aerr[i] = zero\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        else:\n            (work.rerr, work.aerr) = _estimate_error(work)\n            i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        if log:\n            i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n        else:\n            i = ~np.isfinite(work.Sn) & ~stop\n        work.status[i] = _EVALUEERR\n        stop[i] = True\n        return stop\n\n    def post_termination_check(work):\n        work.n += 1\n        work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n        return\n\n    def customize_result(res, shape):\n        if log and np.any(negative):\n            pi = res['integral'].dtype.type(np.pi)\n            j = np.complex64(1j)\n            res['integral'] = res['integral'] + negative * pi * j\n        else:\n            res['integral'][negative] *= -1\n        res['maxlevel'] = minlevel + res['nit'] - 1\n        res['maxlevel'][res['nit'] == 0] = -1\n        del res['nit']\n        return shape\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        res = _scalar_optimization_loop(work, callback, shape, maxiter, f, args, dtype, pre_func_eval, post_func_eval, check_termination, post_termination_check, customize_result, res_work_pairs)\n    return res",
            "def _tanhsinh(f, a, b, *, args=(), log=False, maxfun=None, maxlevel=None, minlevel=2, atol=None, rtol=None, callback=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Evaluate a convergent integral numerically using tanh-sinh quadrature.\\n\\n    In practice, tanh-sinh quadrature achieves quadratic convergence for\\n    many integrands: the number of accurate *digits* scales roughly linearly\\n    with the number of function evaluations [1]_.\\n\\n    Either or both of the limits of integration may be infinite, and\\n    singularities at the endpoints are acceptable. Divergent integrals and\\n    integrands with non-finite derivatives or singularities within an interval\\n    are out of scope, but the latter may be evaluated be calling `_tanhsinh` on\\n    each sub-interval separately.\\n\\n    Parameters\\n    ----------\\n    f : callable\\n        The function to be integrated. The signature must be::\\n            func(x: ndarray, *args) -> ndarray\\n         where each element of ``x`` is a finite real and ``args`` is a tuple,\\n         which may contain an arbitrary number of arrays that are broadcastable\\n         with `x`. ``func`` must be an elementwise function: each element\\n         ``func(x)[i]`` must equal ``func(x[i])`` for all indices ``i``.\\n         If ``func`` returns a value with complex dtype when evaluated at\\n         either endpoint, subsequent arguments ``x`` will have complex dtype\\n         (but zero imaginary part).\\n    a, b : array_like\\n        Real lower and upper limits of integration. Must be broadcastable.\\n        Elements may be infinite.\\n    args : tuple, optional\\n        Additional positional arguments to be passed to `func`. Must be arrays\\n        broadcastable with `a` and `b`. If the callable to be integrated\\n        requires arguments that are not broadcastable with `a` and `b`, wrap\\n        that callable with `f`. See Examples.\\n    log : bool, default: False\\n        Setting to True indicates that `f` returns the log of the integrand\\n        and that `atol` and `rtol` are expressed as the logs of the absolute\\n        and relative errors. In this case, the result object will contain the\\n        log of the integral and error. This is useful for integrands for which\\n        numerical underflow or overflow would lead to inaccuracies.\\n        When ``log=True``, the integrand (the exponential of `f`) must be real,\\n        but it may be negative, in which case the log of the integrand is a\\n        complex number with an imaginary part that is an odd multiple of \u03c0.\\n    maxlevel : int, default: 10\\n        The maximum refinement level of the algorithm.\\n\\n        At the zeroth level, `f` is called once, performing 16 function\\n        evaluations. At each subsequent level, `f` is called once more,\\n        approximately doubling the number of function evaluations that have\\n        been performed. Accordingly, for many integrands, each successive level\\n        will double the number of accurate digits in the result (up to the\\n        limits of floating point precision).\\n\\n        The algorithm will terminate after completing level `maxlevel` or after\\n        another termination condition is satisfied, whichever comes first.\\n    minlevel : int, default: 2\\n        The level at which to begin iteration (default: 2). This does not\\n        change the total number of function evaluations or the abscissae at\\n        which the function is evaluated; it changes only the *number of times*\\n        `f` is called. If ``minlevel=k``, then the integrand is evaluated at\\n        all abscissae from levels ``0`` through ``k`` in a single call.\\n        Note that if `minlevel` exceeds `maxlevel`, the provided `minlevel` is\\n        ignored, and `minlevel` is set equal to `maxlevel`.\\n    atol, rtol : float, optional\\n        Absolute termination tolerance (default: 0) and relative termination\\n        tolerance (default: ``eps**0.75``, where ``eps`` is the precision of\\n        the result dtype), respectively. The error estimate is as\\n        described in [1]_ Section 5. While not theoretically rigorous or\\n        conservative, it is said to work well in practice. Must be non-negative\\n        and finite if `log` is False, and must be expressed as the log of a\\n        non-negative and finite number if `log` is True.\\n    callback : callable, optional\\n        An optional user-supplied function to be called before the first\\n        iteration and after each iteration.\\n        Called as ``callback(res)``, where ``res`` is an ``OptimizeResult``\\n        similar to that returned by `_differentiate` (but containing the\\n        current iterate\\'s values of all variables). If `callback` raises a\\n        ``StopIteration``, the algorithm will terminate immediately and\\n        `_tanhsinh` will return a result object.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        An instance of `scipy.optimize.OptimizeResult` with the following\\n        attributes. (The descriptions are written as though the values will be\\n        scalars; however, if `func` returns an array, the outputs will be\\n        arrays of the same shape.)\\n        success : bool\\n            ``True`` when the algorithm terminated successfully (status ``0``).\\n        status : int\\n            An integer representing the exit status of the algorithm.\\n            ``0`` : The algorithm converged to the specified tolerances.\\n            ``-1`` : (unused)\\n            ``-2`` : The maximum number of iterations was reached.\\n            ``-3`` : A non-finite value was encountered.\\n            ``-4`` : Iteration was terminated by `callback`.\\n            ``1`` : The algorithm is proceeding normally (in `callback` only).\\n        integral : float\\n            An estimate of the integral\\n        error : float\\n            An estimate of the error. Only available if level two or higher\\n            has been completed; otherwise NaN.\\n        nit : int\\n            The number of iterations performed.\\n        nfev : int\\n            The number of points at which `func` was evaluated.\\n\\n    See Also\\n    --------\\n    quad, quadrature\\n\\n    Notes\\n    -----\\n    Implements the algorithm as described in [1]_ with minor adaptations for\\n    finite-precision arithmetic, including some described by [2]_ and [3]_. The\\n    tanh-sinh scheme was originally introduced in [4]_.\\n\\n    Due floating-point error in the abscissae, the function may be evaluated\\n    at the endpoints of the interval during iterations. The values returned by\\n    the function at the endpoints will be ignored.\\n\\n    References\\n    ----------\\n    [1] Bailey, David H., Karthik Jeyabalan, and Xiaoye S. Li. \"A comparison of\\n        three high-precision quadrature schemes.\" Experimental Mathematics 14.3\\n        (2005): 317-329.\\n    [2] Vanherck, Joren, Bart Sor\u00e9e, and Wim Magnus. \"Tanh-sinh quadrature for\\n        single and multiple integration using floating-point arithmetic.\"\\n        arXiv preprint arXiv:2007.15057 (2020).\\n    [3] van Engelen, Robert A.  \"Improving the Double Exponential Quadrature\\n        Tanh-Sinh, Sinh-Sinh and Exp-Sinh Formulas.\"\\n        https://www.genivia.com/files/qthsh.pdf\\n    [4] Takahasi, Hidetosi, and Masatake Mori. \"Double exponential formulas for\\n        numerical integration.\" Publications of the Research Institute for\\n        Mathematical Sciences 9.3 (1974): 721-741.\\n\\n    Example\\n    -------\\n    Evaluate the Gaussian integral:\\n\\n    >>> import numpy as np\\n    >>> from scipy.integrate._tanhsinh import _tanhsinh\\n    >>> def f(x):\\n    ...     return np.exp(-x**2)\\n    >>> res = _tanhsinh(f, -np.inf, np.inf)\\n    >>> res.integral  # true value is np.sqrt(np.pi), 1.7724538509055159\\n     1.7724538509055159\\n    >>> res.error  # actual error is 0\\n    4.0007963937534104e-16\\n\\n    The value of the Gaussian function (bell curve) is nearly zero for\\n    arguments sufficiently far from zero, so the value of the integral\\n    over a finite interval is nearly the same.\\n\\n    >>> _tanhsinh(f, -20, 20).integral\\n    1.772453850905518\\n\\n    However, with unfavorable integration limits, the integration scheme\\n    may not be able to find the important region.\\n\\n    >>> _tanhsinh(f, -np.inf, 1000).integral\\n    4.500490856620352\\n\\n    In such cases, or when there are singularities within the interval,\\n    break the integral into parts with endpoints at the important points.\\n\\n    >>> _tanhsinh(f, -np.inf, 0).integral + _tanhsinh(f, 0, 1000).integral\\n    1.772453850905404\\n\\n    For integration involving very large or very small magnitudes, use\\n    log-integration. (For illustrative purposes, the following example shows a\\n    case in which both regular and log-integration work, but for more extreme\\n    limits of integration, log-integration would avoid the underflow\\n    experienced when evaluating the integral normally.)\\n\\n    >>> res = _tanhsinh(f, 20, 30, rtol=1e-10)\\n    >>> res.integral, res.error\\n    4.7819613911309014e-176, 4.670364401645202e-187\\n    >>> def log_f(x):\\n    ...     return -x**2\\n    >>> np.exp(res.integral), np.exp(res.error)\\n    4.7819613911306924e-176, 4.670364401645093e-187\\n\\n    The limits of integration and elements of `args` may be broadcastable\\n    arrays, and integration is performed elementwise.\\n\\n    >>> from scipy import stats\\n    >>> dist = stats.gausshyper(13.8, 3.12, 2.51, 5.18)\\n    >>> a, b = dist.support()\\n    >>> x = np.linspace(a, b, 100)\\n    >>> res = _tanhsinh(dist.pdf, a, x)\\n    >>> ref = dist.cdf(x)\\n    >>> np.allclose(res.integral, ref)\\n\\n    '\n    tmp = (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)\n    tmp = _tanhsinh_iv(*tmp)\n    (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback) = tmp\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        c = ((a.ravel() + b.ravel()) / 2).reshape(a.shape)\n        c[np.isinf(a)] = b[np.isinf(a)]\n        c[np.isinf(b)] = a[np.isinf(b)]\n        c[np.isnan(c)] = 0\n        tmp = _scalar_optimization_initialize(f, (c,), args, complex_ok=True)\n    (xs, fs, args, shape, dtype) = tmp\n    a = np.broadcast_to(a, shape).astype(dtype).ravel()\n    b = np.broadcast_to(b, shape).astype(dtype).ravel()\n    (a, b, a0, negative, abinf, ainf, binf) = _transform_integrals(a, b)\n    (nit, nfev) = (0, 1)\n    zero = -np.inf if log else 0\n    pi = dtype.type(np.pi)\n    maxiter = maxlevel - minlevel + 1\n    eps = np.finfo(dtype).eps\n    if rtol is None:\n        rtol = 0.75 * np.log(eps) if log else eps ** 0.75\n    Sn = np.full(shape, zero, dtype=dtype).ravel()\n    Sn[np.isnan(a) | np.isnan(b) | np.isnan(fs[0])] = np.nan\n    Sk = np.empty_like(Sn).reshape(-1, 1)[:, 0:0]\n    aerr = np.full(shape, np.nan, dtype=dtype).ravel()\n    status = np.full(shape, _EINPROGRESS, dtype=int).ravel()\n    h0 = np.real(_get_base_step(dtype=dtype))\n    xr0 = np.full(shape, -np.inf, dtype=dtype).ravel()\n    fr0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wr0 = np.zeros(shape, dtype=dtype).ravel()\n    xl0 = np.full(shape, np.inf, dtype=dtype).ravel()\n    fl0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wl0 = np.zeros(shape, dtype=dtype).ravel()\n    d4 = np.zeros(shape, dtype=dtype).ravel()\n    work = OptimizeResult(Sn=Sn, Sk=Sk, aerr=aerr, h=h0, log=log, dtype=dtype, pi=pi, eps=eps, a=a.reshape(-1, 1), b=b.reshape(-1, 1), n=minlevel, nit=nit, nfev=nfev, status=status, xr0=xr0, fr0=fr0, wr0=wr0, xl0=xl0, fl0=fl0, wl0=wl0, d4=d4, ainf=ainf, binf=binf, abinf=abinf, a0=a0.reshape(-1, 1))\n    res_work_pairs = [('status', 'status'), ('integral', 'Sn'), ('error', 'aerr'), ('nit', 'nit'), ('nfev', 'nfev')]\n\n    def pre_func_eval(work):\n        work.h = h0 / 2 ** work.n\n        (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n        (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n        xj = work.xj.copy()\n        xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n        xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n        xj[work.ainf] *= -1\n        return xj\n\n    def post_func_eval(x, fj, work):\n        if work.log:\n            fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n            fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n        else:\n            fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n            fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n        (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n        if work.Sk.shape[-1]:\n            Snm1 = work.Sk[:, -1]\n            Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n        work.fjwj = fjwj\n        work.Sn = Sn\n\n    def check_termination(work):\n        \"\"\"Terminate due to convergence or encountering non-finite values\"\"\"\n        stop = np.zeros(work.Sn.shape, dtype=bool)\n        if work.nit == 0:\n            i = (work.a == work.b).ravel()\n            zero = -np.inf if log else 0\n            work.Sn[i] = zero\n            work.aerr[i] = zero\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        else:\n            (work.rerr, work.aerr) = _estimate_error(work)\n            i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        if log:\n            i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n        else:\n            i = ~np.isfinite(work.Sn) & ~stop\n        work.status[i] = _EVALUEERR\n        stop[i] = True\n        return stop\n\n    def post_termination_check(work):\n        work.n += 1\n        work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n        return\n\n    def customize_result(res, shape):\n        if log and np.any(negative):\n            pi = res['integral'].dtype.type(np.pi)\n            j = np.complex64(1j)\n            res['integral'] = res['integral'] + negative * pi * j\n        else:\n            res['integral'][negative] *= -1\n        res['maxlevel'] = minlevel + res['nit'] - 1\n        res['maxlevel'][res['nit'] == 0] = -1\n        del res['nit']\n        return shape\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        res = _scalar_optimization_loop(work, callback, shape, maxiter, f, args, dtype, pre_func_eval, post_func_eval, check_termination, post_termination_check, customize_result, res_work_pairs)\n    return res",
            "def _tanhsinh(f, a, b, *, args=(), log=False, maxfun=None, maxlevel=None, minlevel=2, atol=None, rtol=None, callback=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Evaluate a convergent integral numerically using tanh-sinh quadrature.\\n\\n    In practice, tanh-sinh quadrature achieves quadratic convergence for\\n    many integrands: the number of accurate *digits* scales roughly linearly\\n    with the number of function evaluations [1]_.\\n\\n    Either or both of the limits of integration may be infinite, and\\n    singularities at the endpoints are acceptable. Divergent integrals and\\n    integrands with non-finite derivatives or singularities within an interval\\n    are out of scope, but the latter may be evaluated be calling `_tanhsinh` on\\n    each sub-interval separately.\\n\\n    Parameters\\n    ----------\\n    f : callable\\n        The function to be integrated. The signature must be::\\n            func(x: ndarray, *args) -> ndarray\\n         where each element of ``x`` is a finite real and ``args`` is a tuple,\\n         which may contain an arbitrary number of arrays that are broadcastable\\n         with `x`. ``func`` must be an elementwise function: each element\\n         ``func(x)[i]`` must equal ``func(x[i])`` for all indices ``i``.\\n         If ``func`` returns a value with complex dtype when evaluated at\\n         either endpoint, subsequent arguments ``x`` will have complex dtype\\n         (but zero imaginary part).\\n    a, b : array_like\\n        Real lower and upper limits of integration. Must be broadcastable.\\n        Elements may be infinite.\\n    args : tuple, optional\\n        Additional positional arguments to be passed to `func`. Must be arrays\\n        broadcastable with `a` and `b`. If the callable to be integrated\\n        requires arguments that are not broadcastable with `a` and `b`, wrap\\n        that callable with `f`. See Examples.\\n    log : bool, default: False\\n        Setting to True indicates that `f` returns the log of the integrand\\n        and that `atol` and `rtol` are expressed as the logs of the absolute\\n        and relative errors. In this case, the result object will contain the\\n        log of the integral and error. This is useful for integrands for which\\n        numerical underflow or overflow would lead to inaccuracies.\\n        When ``log=True``, the integrand (the exponential of `f`) must be real,\\n        but it may be negative, in which case the log of the integrand is a\\n        complex number with an imaginary part that is an odd multiple of \u03c0.\\n    maxlevel : int, default: 10\\n        The maximum refinement level of the algorithm.\\n\\n        At the zeroth level, `f` is called once, performing 16 function\\n        evaluations. At each subsequent level, `f` is called once more,\\n        approximately doubling the number of function evaluations that have\\n        been performed. Accordingly, for many integrands, each successive level\\n        will double the number of accurate digits in the result (up to the\\n        limits of floating point precision).\\n\\n        The algorithm will terminate after completing level `maxlevel` or after\\n        another termination condition is satisfied, whichever comes first.\\n    minlevel : int, default: 2\\n        The level at which to begin iteration (default: 2). This does not\\n        change the total number of function evaluations or the abscissae at\\n        which the function is evaluated; it changes only the *number of times*\\n        `f` is called. If ``minlevel=k``, then the integrand is evaluated at\\n        all abscissae from levels ``0`` through ``k`` in a single call.\\n        Note that if `minlevel` exceeds `maxlevel`, the provided `minlevel` is\\n        ignored, and `minlevel` is set equal to `maxlevel`.\\n    atol, rtol : float, optional\\n        Absolute termination tolerance (default: 0) and relative termination\\n        tolerance (default: ``eps**0.75``, where ``eps`` is the precision of\\n        the result dtype), respectively. The error estimate is as\\n        described in [1]_ Section 5. While not theoretically rigorous or\\n        conservative, it is said to work well in practice. Must be non-negative\\n        and finite if `log` is False, and must be expressed as the log of a\\n        non-negative and finite number if `log` is True.\\n    callback : callable, optional\\n        An optional user-supplied function to be called before the first\\n        iteration and after each iteration.\\n        Called as ``callback(res)``, where ``res`` is an ``OptimizeResult``\\n        similar to that returned by `_differentiate` (but containing the\\n        current iterate\\'s values of all variables). If `callback` raises a\\n        ``StopIteration``, the algorithm will terminate immediately and\\n        `_tanhsinh` will return a result object.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        An instance of `scipy.optimize.OptimizeResult` with the following\\n        attributes. (The descriptions are written as though the values will be\\n        scalars; however, if `func` returns an array, the outputs will be\\n        arrays of the same shape.)\\n        success : bool\\n            ``True`` when the algorithm terminated successfully (status ``0``).\\n        status : int\\n            An integer representing the exit status of the algorithm.\\n            ``0`` : The algorithm converged to the specified tolerances.\\n            ``-1`` : (unused)\\n            ``-2`` : The maximum number of iterations was reached.\\n            ``-3`` : A non-finite value was encountered.\\n            ``-4`` : Iteration was terminated by `callback`.\\n            ``1`` : The algorithm is proceeding normally (in `callback` only).\\n        integral : float\\n            An estimate of the integral\\n        error : float\\n            An estimate of the error. Only available if level two or higher\\n            has been completed; otherwise NaN.\\n        nit : int\\n            The number of iterations performed.\\n        nfev : int\\n            The number of points at which `func` was evaluated.\\n\\n    See Also\\n    --------\\n    quad, quadrature\\n\\n    Notes\\n    -----\\n    Implements the algorithm as described in [1]_ with minor adaptations for\\n    finite-precision arithmetic, including some described by [2]_ and [3]_. The\\n    tanh-sinh scheme was originally introduced in [4]_.\\n\\n    Due floating-point error in the abscissae, the function may be evaluated\\n    at the endpoints of the interval during iterations. The values returned by\\n    the function at the endpoints will be ignored.\\n\\n    References\\n    ----------\\n    [1] Bailey, David H., Karthik Jeyabalan, and Xiaoye S. Li. \"A comparison of\\n        three high-precision quadrature schemes.\" Experimental Mathematics 14.3\\n        (2005): 317-329.\\n    [2] Vanherck, Joren, Bart Sor\u00e9e, and Wim Magnus. \"Tanh-sinh quadrature for\\n        single and multiple integration using floating-point arithmetic.\"\\n        arXiv preprint arXiv:2007.15057 (2020).\\n    [3] van Engelen, Robert A.  \"Improving the Double Exponential Quadrature\\n        Tanh-Sinh, Sinh-Sinh and Exp-Sinh Formulas.\"\\n        https://www.genivia.com/files/qthsh.pdf\\n    [4] Takahasi, Hidetosi, and Masatake Mori. \"Double exponential formulas for\\n        numerical integration.\" Publications of the Research Institute for\\n        Mathematical Sciences 9.3 (1974): 721-741.\\n\\n    Example\\n    -------\\n    Evaluate the Gaussian integral:\\n\\n    >>> import numpy as np\\n    >>> from scipy.integrate._tanhsinh import _tanhsinh\\n    >>> def f(x):\\n    ...     return np.exp(-x**2)\\n    >>> res = _tanhsinh(f, -np.inf, np.inf)\\n    >>> res.integral  # true value is np.sqrt(np.pi), 1.7724538509055159\\n     1.7724538509055159\\n    >>> res.error  # actual error is 0\\n    4.0007963937534104e-16\\n\\n    The value of the Gaussian function (bell curve) is nearly zero for\\n    arguments sufficiently far from zero, so the value of the integral\\n    over a finite interval is nearly the same.\\n\\n    >>> _tanhsinh(f, -20, 20).integral\\n    1.772453850905518\\n\\n    However, with unfavorable integration limits, the integration scheme\\n    may not be able to find the important region.\\n\\n    >>> _tanhsinh(f, -np.inf, 1000).integral\\n    4.500490856620352\\n\\n    In such cases, or when there are singularities within the interval,\\n    break the integral into parts with endpoints at the important points.\\n\\n    >>> _tanhsinh(f, -np.inf, 0).integral + _tanhsinh(f, 0, 1000).integral\\n    1.772453850905404\\n\\n    For integration involving very large or very small magnitudes, use\\n    log-integration. (For illustrative purposes, the following example shows a\\n    case in which both regular and log-integration work, but for more extreme\\n    limits of integration, log-integration would avoid the underflow\\n    experienced when evaluating the integral normally.)\\n\\n    >>> res = _tanhsinh(f, 20, 30, rtol=1e-10)\\n    >>> res.integral, res.error\\n    4.7819613911309014e-176, 4.670364401645202e-187\\n    >>> def log_f(x):\\n    ...     return -x**2\\n    >>> np.exp(res.integral), np.exp(res.error)\\n    4.7819613911306924e-176, 4.670364401645093e-187\\n\\n    The limits of integration and elements of `args` may be broadcastable\\n    arrays, and integration is performed elementwise.\\n\\n    >>> from scipy import stats\\n    >>> dist = stats.gausshyper(13.8, 3.12, 2.51, 5.18)\\n    >>> a, b = dist.support()\\n    >>> x = np.linspace(a, b, 100)\\n    >>> res = _tanhsinh(dist.pdf, a, x)\\n    >>> ref = dist.cdf(x)\\n    >>> np.allclose(res.integral, ref)\\n\\n    '\n    tmp = (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)\n    tmp = _tanhsinh_iv(*tmp)\n    (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback) = tmp\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        c = ((a.ravel() + b.ravel()) / 2).reshape(a.shape)\n        c[np.isinf(a)] = b[np.isinf(a)]\n        c[np.isinf(b)] = a[np.isinf(b)]\n        c[np.isnan(c)] = 0\n        tmp = _scalar_optimization_initialize(f, (c,), args, complex_ok=True)\n    (xs, fs, args, shape, dtype) = tmp\n    a = np.broadcast_to(a, shape).astype(dtype).ravel()\n    b = np.broadcast_to(b, shape).astype(dtype).ravel()\n    (a, b, a0, negative, abinf, ainf, binf) = _transform_integrals(a, b)\n    (nit, nfev) = (0, 1)\n    zero = -np.inf if log else 0\n    pi = dtype.type(np.pi)\n    maxiter = maxlevel - minlevel + 1\n    eps = np.finfo(dtype).eps\n    if rtol is None:\n        rtol = 0.75 * np.log(eps) if log else eps ** 0.75\n    Sn = np.full(shape, zero, dtype=dtype).ravel()\n    Sn[np.isnan(a) | np.isnan(b) | np.isnan(fs[0])] = np.nan\n    Sk = np.empty_like(Sn).reshape(-1, 1)[:, 0:0]\n    aerr = np.full(shape, np.nan, dtype=dtype).ravel()\n    status = np.full(shape, _EINPROGRESS, dtype=int).ravel()\n    h0 = np.real(_get_base_step(dtype=dtype))\n    xr0 = np.full(shape, -np.inf, dtype=dtype).ravel()\n    fr0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wr0 = np.zeros(shape, dtype=dtype).ravel()\n    xl0 = np.full(shape, np.inf, dtype=dtype).ravel()\n    fl0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wl0 = np.zeros(shape, dtype=dtype).ravel()\n    d4 = np.zeros(shape, dtype=dtype).ravel()\n    work = OptimizeResult(Sn=Sn, Sk=Sk, aerr=aerr, h=h0, log=log, dtype=dtype, pi=pi, eps=eps, a=a.reshape(-1, 1), b=b.reshape(-1, 1), n=minlevel, nit=nit, nfev=nfev, status=status, xr0=xr0, fr0=fr0, wr0=wr0, xl0=xl0, fl0=fl0, wl0=wl0, d4=d4, ainf=ainf, binf=binf, abinf=abinf, a0=a0.reshape(-1, 1))\n    res_work_pairs = [('status', 'status'), ('integral', 'Sn'), ('error', 'aerr'), ('nit', 'nit'), ('nfev', 'nfev')]\n\n    def pre_func_eval(work):\n        work.h = h0 / 2 ** work.n\n        (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n        (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n        xj = work.xj.copy()\n        xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n        xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n        xj[work.ainf] *= -1\n        return xj\n\n    def post_func_eval(x, fj, work):\n        if work.log:\n            fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n            fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n        else:\n            fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n            fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n        (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n        if work.Sk.shape[-1]:\n            Snm1 = work.Sk[:, -1]\n            Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n        work.fjwj = fjwj\n        work.Sn = Sn\n\n    def check_termination(work):\n        \"\"\"Terminate due to convergence or encountering non-finite values\"\"\"\n        stop = np.zeros(work.Sn.shape, dtype=bool)\n        if work.nit == 0:\n            i = (work.a == work.b).ravel()\n            zero = -np.inf if log else 0\n            work.Sn[i] = zero\n            work.aerr[i] = zero\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        else:\n            (work.rerr, work.aerr) = _estimate_error(work)\n            i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        if log:\n            i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n        else:\n            i = ~np.isfinite(work.Sn) & ~stop\n        work.status[i] = _EVALUEERR\n        stop[i] = True\n        return stop\n\n    def post_termination_check(work):\n        work.n += 1\n        work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n        return\n\n    def customize_result(res, shape):\n        if log and np.any(negative):\n            pi = res['integral'].dtype.type(np.pi)\n            j = np.complex64(1j)\n            res['integral'] = res['integral'] + negative * pi * j\n        else:\n            res['integral'][negative] *= -1\n        res['maxlevel'] = minlevel + res['nit'] - 1\n        res['maxlevel'][res['nit'] == 0] = -1\n        del res['nit']\n        return shape\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        res = _scalar_optimization_loop(work, callback, shape, maxiter, f, args, dtype, pre_func_eval, post_func_eval, check_termination, post_termination_check, customize_result, res_work_pairs)\n    return res",
            "def _tanhsinh(f, a, b, *, args=(), log=False, maxfun=None, maxlevel=None, minlevel=2, atol=None, rtol=None, callback=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Evaluate a convergent integral numerically using tanh-sinh quadrature.\\n\\n    In practice, tanh-sinh quadrature achieves quadratic convergence for\\n    many integrands: the number of accurate *digits* scales roughly linearly\\n    with the number of function evaluations [1]_.\\n\\n    Either or both of the limits of integration may be infinite, and\\n    singularities at the endpoints are acceptable. Divergent integrals and\\n    integrands with non-finite derivatives or singularities within an interval\\n    are out of scope, but the latter may be evaluated be calling `_tanhsinh` on\\n    each sub-interval separately.\\n\\n    Parameters\\n    ----------\\n    f : callable\\n        The function to be integrated. The signature must be::\\n            func(x: ndarray, *args) -> ndarray\\n         where each element of ``x`` is a finite real and ``args`` is a tuple,\\n         which may contain an arbitrary number of arrays that are broadcastable\\n         with `x`. ``func`` must be an elementwise function: each element\\n         ``func(x)[i]`` must equal ``func(x[i])`` for all indices ``i``.\\n         If ``func`` returns a value with complex dtype when evaluated at\\n         either endpoint, subsequent arguments ``x`` will have complex dtype\\n         (but zero imaginary part).\\n    a, b : array_like\\n        Real lower and upper limits of integration. Must be broadcastable.\\n        Elements may be infinite.\\n    args : tuple, optional\\n        Additional positional arguments to be passed to `func`. Must be arrays\\n        broadcastable with `a` and `b`. If the callable to be integrated\\n        requires arguments that are not broadcastable with `a` and `b`, wrap\\n        that callable with `f`. See Examples.\\n    log : bool, default: False\\n        Setting to True indicates that `f` returns the log of the integrand\\n        and that `atol` and `rtol` are expressed as the logs of the absolute\\n        and relative errors. In this case, the result object will contain the\\n        log of the integral and error. This is useful for integrands for which\\n        numerical underflow or overflow would lead to inaccuracies.\\n        When ``log=True``, the integrand (the exponential of `f`) must be real,\\n        but it may be negative, in which case the log of the integrand is a\\n        complex number with an imaginary part that is an odd multiple of \u03c0.\\n    maxlevel : int, default: 10\\n        The maximum refinement level of the algorithm.\\n\\n        At the zeroth level, `f` is called once, performing 16 function\\n        evaluations. At each subsequent level, `f` is called once more,\\n        approximately doubling the number of function evaluations that have\\n        been performed. Accordingly, for many integrands, each successive level\\n        will double the number of accurate digits in the result (up to the\\n        limits of floating point precision).\\n\\n        The algorithm will terminate after completing level `maxlevel` or after\\n        another termination condition is satisfied, whichever comes first.\\n    minlevel : int, default: 2\\n        The level at which to begin iteration (default: 2). This does not\\n        change the total number of function evaluations or the abscissae at\\n        which the function is evaluated; it changes only the *number of times*\\n        `f` is called. If ``minlevel=k``, then the integrand is evaluated at\\n        all abscissae from levels ``0`` through ``k`` in a single call.\\n        Note that if `minlevel` exceeds `maxlevel`, the provided `minlevel` is\\n        ignored, and `minlevel` is set equal to `maxlevel`.\\n    atol, rtol : float, optional\\n        Absolute termination tolerance (default: 0) and relative termination\\n        tolerance (default: ``eps**0.75``, where ``eps`` is the precision of\\n        the result dtype), respectively. The error estimate is as\\n        described in [1]_ Section 5. While not theoretically rigorous or\\n        conservative, it is said to work well in practice. Must be non-negative\\n        and finite if `log` is False, and must be expressed as the log of a\\n        non-negative and finite number if `log` is True.\\n    callback : callable, optional\\n        An optional user-supplied function to be called before the first\\n        iteration and after each iteration.\\n        Called as ``callback(res)``, where ``res`` is an ``OptimizeResult``\\n        similar to that returned by `_differentiate` (but containing the\\n        current iterate\\'s values of all variables). If `callback` raises a\\n        ``StopIteration``, the algorithm will terminate immediately and\\n        `_tanhsinh` will return a result object.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        An instance of `scipy.optimize.OptimizeResult` with the following\\n        attributes. (The descriptions are written as though the values will be\\n        scalars; however, if `func` returns an array, the outputs will be\\n        arrays of the same shape.)\\n        success : bool\\n            ``True`` when the algorithm terminated successfully (status ``0``).\\n        status : int\\n            An integer representing the exit status of the algorithm.\\n            ``0`` : The algorithm converged to the specified tolerances.\\n            ``-1`` : (unused)\\n            ``-2`` : The maximum number of iterations was reached.\\n            ``-3`` : A non-finite value was encountered.\\n            ``-4`` : Iteration was terminated by `callback`.\\n            ``1`` : The algorithm is proceeding normally (in `callback` only).\\n        integral : float\\n            An estimate of the integral\\n        error : float\\n            An estimate of the error. Only available if level two or higher\\n            has been completed; otherwise NaN.\\n        nit : int\\n            The number of iterations performed.\\n        nfev : int\\n            The number of points at which `func` was evaluated.\\n\\n    See Also\\n    --------\\n    quad, quadrature\\n\\n    Notes\\n    -----\\n    Implements the algorithm as described in [1]_ with minor adaptations for\\n    finite-precision arithmetic, including some described by [2]_ and [3]_. The\\n    tanh-sinh scheme was originally introduced in [4]_.\\n\\n    Due floating-point error in the abscissae, the function may be evaluated\\n    at the endpoints of the interval during iterations. The values returned by\\n    the function at the endpoints will be ignored.\\n\\n    References\\n    ----------\\n    [1] Bailey, David H., Karthik Jeyabalan, and Xiaoye S. Li. \"A comparison of\\n        three high-precision quadrature schemes.\" Experimental Mathematics 14.3\\n        (2005): 317-329.\\n    [2] Vanherck, Joren, Bart Sor\u00e9e, and Wim Magnus. \"Tanh-sinh quadrature for\\n        single and multiple integration using floating-point arithmetic.\"\\n        arXiv preprint arXiv:2007.15057 (2020).\\n    [3] van Engelen, Robert A.  \"Improving the Double Exponential Quadrature\\n        Tanh-Sinh, Sinh-Sinh and Exp-Sinh Formulas.\"\\n        https://www.genivia.com/files/qthsh.pdf\\n    [4] Takahasi, Hidetosi, and Masatake Mori. \"Double exponential formulas for\\n        numerical integration.\" Publications of the Research Institute for\\n        Mathematical Sciences 9.3 (1974): 721-741.\\n\\n    Example\\n    -------\\n    Evaluate the Gaussian integral:\\n\\n    >>> import numpy as np\\n    >>> from scipy.integrate._tanhsinh import _tanhsinh\\n    >>> def f(x):\\n    ...     return np.exp(-x**2)\\n    >>> res = _tanhsinh(f, -np.inf, np.inf)\\n    >>> res.integral  # true value is np.sqrt(np.pi), 1.7724538509055159\\n     1.7724538509055159\\n    >>> res.error  # actual error is 0\\n    4.0007963937534104e-16\\n\\n    The value of the Gaussian function (bell curve) is nearly zero for\\n    arguments sufficiently far from zero, so the value of the integral\\n    over a finite interval is nearly the same.\\n\\n    >>> _tanhsinh(f, -20, 20).integral\\n    1.772453850905518\\n\\n    However, with unfavorable integration limits, the integration scheme\\n    may not be able to find the important region.\\n\\n    >>> _tanhsinh(f, -np.inf, 1000).integral\\n    4.500490856620352\\n\\n    In such cases, or when there are singularities within the interval,\\n    break the integral into parts with endpoints at the important points.\\n\\n    >>> _tanhsinh(f, -np.inf, 0).integral + _tanhsinh(f, 0, 1000).integral\\n    1.772453850905404\\n\\n    For integration involving very large or very small magnitudes, use\\n    log-integration. (For illustrative purposes, the following example shows a\\n    case in which both regular and log-integration work, but for more extreme\\n    limits of integration, log-integration would avoid the underflow\\n    experienced when evaluating the integral normally.)\\n\\n    >>> res = _tanhsinh(f, 20, 30, rtol=1e-10)\\n    >>> res.integral, res.error\\n    4.7819613911309014e-176, 4.670364401645202e-187\\n    >>> def log_f(x):\\n    ...     return -x**2\\n    >>> np.exp(res.integral), np.exp(res.error)\\n    4.7819613911306924e-176, 4.670364401645093e-187\\n\\n    The limits of integration and elements of `args` may be broadcastable\\n    arrays, and integration is performed elementwise.\\n\\n    >>> from scipy import stats\\n    >>> dist = stats.gausshyper(13.8, 3.12, 2.51, 5.18)\\n    >>> a, b = dist.support()\\n    >>> x = np.linspace(a, b, 100)\\n    >>> res = _tanhsinh(dist.pdf, a, x)\\n    >>> ref = dist.cdf(x)\\n    >>> np.allclose(res.integral, ref)\\n\\n    '\n    tmp = (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)\n    tmp = _tanhsinh_iv(*tmp)\n    (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback) = tmp\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        c = ((a.ravel() + b.ravel()) / 2).reshape(a.shape)\n        c[np.isinf(a)] = b[np.isinf(a)]\n        c[np.isinf(b)] = a[np.isinf(b)]\n        c[np.isnan(c)] = 0\n        tmp = _scalar_optimization_initialize(f, (c,), args, complex_ok=True)\n    (xs, fs, args, shape, dtype) = tmp\n    a = np.broadcast_to(a, shape).astype(dtype).ravel()\n    b = np.broadcast_to(b, shape).astype(dtype).ravel()\n    (a, b, a0, negative, abinf, ainf, binf) = _transform_integrals(a, b)\n    (nit, nfev) = (0, 1)\n    zero = -np.inf if log else 0\n    pi = dtype.type(np.pi)\n    maxiter = maxlevel - minlevel + 1\n    eps = np.finfo(dtype).eps\n    if rtol is None:\n        rtol = 0.75 * np.log(eps) if log else eps ** 0.75\n    Sn = np.full(shape, zero, dtype=dtype).ravel()\n    Sn[np.isnan(a) | np.isnan(b) | np.isnan(fs[0])] = np.nan\n    Sk = np.empty_like(Sn).reshape(-1, 1)[:, 0:0]\n    aerr = np.full(shape, np.nan, dtype=dtype).ravel()\n    status = np.full(shape, _EINPROGRESS, dtype=int).ravel()\n    h0 = np.real(_get_base_step(dtype=dtype))\n    xr0 = np.full(shape, -np.inf, dtype=dtype).ravel()\n    fr0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wr0 = np.zeros(shape, dtype=dtype).ravel()\n    xl0 = np.full(shape, np.inf, dtype=dtype).ravel()\n    fl0 = np.full(shape, np.nan, dtype=dtype).ravel()\n    wl0 = np.zeros(shape, dtype=dtype).ravel()\n    d4 = np.zeros(shape, dtype=dtype).ravel()\n    work = OptimizeResult(Sn=Sn, Sk=Sk, aerr=aerr, h=h0, log=log, dtype=dtype, pi=pi, eps=eps, a=a.reshape(-1, 1), b=b.reshape(-1, 1), n=minlevel, nit=nit, nfev=nfev, status=status, xr0=xr0, fr0=fr0, wr0=wr0, xl0=xl0, fl0=fl0, wl0=wl0, d4=d4, ainf=ainf, binf=binf, abinf=abinf, a0=a0.reshape(-1, 1))\n    res_work_pairs = [('status', 'status'), ('integral', 'Sn'), ('error', 'aerr'), ('nit', 'nit'), ('nfev', 'nfev')]\n\n    def pre_func_eval(work):\n        work.h = h0 / 2 ** work.n\n        (xjc, wj) = _get_pairs(work.n, h0, dtype=work.dtype, inclusive=work.n == minlevel)\n        (work.xj, work.wj) = _transform_to_limits(xjc, wj, work.a, work.b)\n        xj = work.xj.copy()\n        xj[work.abinf] = xj[work.abinf] / (1 - xj[work.abinf] ** 2)\n        xj[work.binf] = 1 / xj[work.binf] - 1 + work.a0[work.binf]\n        xj[work.ainf] *= -1\n        return xj\n\n    def post_func_eval(x, fj, work):\n        if work.log:\n            fj[work.abinf] += np.log(1 + work.xj[work.abinf] ** 2) - 2 * np.log(1 - work.xj[work.abinf] ** 2)\n            fj[work.binf] -= 2 * np.log(work.xj[work.binf])\n        else:\n            fj[work.abinf] *= (1 + work.xj[work.abinf] ** 2) / (1 - work.xj[work.abinf] ** 2) ** 2\n            fj[work.binf] *= work.xj[work.binf] ** (-2.0)\n        (fjwj, Sn) = _euler_maclaurin_sum(fj, work)\n        if work.Sk.shape[-1]:\n            Snm1 = work.Sk[:, -1]\n            Sn = special.logsumexp([Snm1 - np.log(2), Sn], axis=0) if log else Snm1 / 2 + Sn\n        work.fjwj = fjwj\n        work.Sn = Sn\n\n    def check_termination(work):\n        \"\"\"Terminate due to convergence or encountering non-finite values\"\"\"\n        stop = np.zeros(work.Sn.shape, dtype=bool)\n        if work.nit == 0:\n            i = (work.a == work.b).ravel()\n            zero = -np.inf if log else 0\n            work.Sn[i] = zero\n            work.aerr[i] = zero\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        else:\n            (work.rerr, work.aerr) = _estimate_error(work)\n            i = (work.rerr < rtol) | (work.rerr + np.real(work.Sn) < atol) if log else (work.rerr < rtol) | (work.rerr * abs(work.Sn) < atol)\n            work.status[i] = _ECONVERGED\n            stop[i] = True\n        if log:\n            i = (np.isposinf(np.real(work.Sn)) | np.isnan(work.Sn)) & ~stop\n        else:\n            i = ~np.isfinite(work.Sn) & ~stop\n        work.status[i] = _EVALUEERR\n        stop[i] = True\n        return stop\n\n    def post_termination_check(work):\n        work.n += 1\n        work.Sk = np.concatenate((work.Sk, work.Sn[:, np.newaxis]), axis=-1)\n        return\n\n    def customize_result(res, shape):\n        if log and np.any(negative):\n            pi = res['integral'].dtype.type(np.pi)\n            j = np.complex64(1j)\n            res['integral'] = res['integral'] + negative * pi * j\n        else:\n            res['integral'][negative] *= -1\n        res['maxlevel'] = minlevel + res['nit'] - 1\n        res['maxlevel'][res['nit'] == 0] = -1\n        del res['nit']\n        return shape\n    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n        res = _scalar_optimization_loop(work, callback, shape, maxiter, f, args, dtype, pre_func_eval, post_func_eval, check_termination, post_termination_check, customize_result, res_work_pairs)\n    return res"
        ]
    },
    {
        "func_name": "_get_base_step",
        "original": "def _get_base_step(dtype=np.float64):\n    fmin = 4 * np.finfo(dtype).tiny\n    tmax = np.arcsinh(np.log(2 / fmin - 1) / np.pi)\n    h0 = tmax / _N_BASE_STEPS\n    return h0.astype(dtype)",
        "mutated": [
            "def _get_base_step(dtype=np.float64):\n    if False:\n        i = 10\n    fmin = 4 * np.finfo(dtype).tiny\n    tmax = np.arcsinh(np.log(2 / fmin - 1) / np.pi)\n    h0 = tmax / _N_BASE_STEPS\n    return h0.astype(dtype)",
            "def _get_base_step(dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    fmin = 4 * np.finfo(dtype).tiny\n    tmax = np.arcsinh(np.log(2 / fmin - 1) / np.pi)\n    h0 = tmax / _N_BASE_STEPS\n    return h0.astype(dtype)",
            "def _get_base_step(dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    fmin = 4 * np.finfo(dtype).tiny\n    tmax = np.arcsinh(np.log(2 / fmin - 1) / np.pi)\n    h0 = tmax / _N_BASE_STEPS\n    return h0.astype(dtype)",
            "def _get_base_step(dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    fmin = 4 * np.finfo(dtype).tiny\n    tmax = np.arcsinh(np.log(2 / fmin - 1) / np.pi)\n    h0 = tmax / _N_BASE_STEPS\n    return h0.astype(dtype)",
            "def _get_base_step(dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    fmin = 4 * np.finfo(dtype).tiny\n    tmax = np.arcsinh(np.log(2 / fmin - 1) / np.pi)\n    h0 = tmax / _N_BASE_STEPS\n    return h0.astype(dtype)"
        ]
    },
    {
        "func_name": "_compute_pair",
        "original": "def _compute_pair(k, h0):\n    h = h0 / 2 ** k\n    max = _N_BASE_STEPS * 2 ** k\n    j = np.arange(max + 1) if k == 0 else np.arange(1, max + 1, 2)\n    jh = j * h\n    pi_2 = np.pi / 2\n    u1 = pi_2 * np.cosh(jh)\n    u2 = pi_2 * np.sinh(jh)\n    wj = u1 / np.cosh(u2) ** 2\n    xjc = 1 / (np.exp(u2) * np.cosh(u2))\n    wj[0] = wj[0] / 2 if k == 0 else wj[0]\n    return (xjc, wj)",
        "mutated": [
            "def _compute_pair(k, h0):\n    if False:\n        i = 10\n    h = h0 / 2 ** k\n    max = _N_BASE_STEPS * 2 ** k\n    j = np.arange(max + 1) if k == 0 else np.arange(1, max + 1, 2)\n    jh = j * h\n    pi_2 = np.pi / 2\n    u1 = pi_2 * np.cosh(jh)\n    u2 = pi_2 * np.sinh(jh)\n    wj = u1 / np.cosh(u2) ** 2\n    xjc = 1 / (np.exp(u2) * np.cosh(u2))\n    wj[0] = wj[0] / 2 if k == 0 else wj[0]\n    return (xjc, wj)",
            "def _compute_pair(k, h0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    h = h0 / 2 ** k\n    max = _N_BASE_STEPS * 2 ** k\n    j = np.arange(max + 1) if k == 0 else np.arange(1, max + 1, 2)\n    jh = j * h\n    pi_2 = np.pi / 2\n    u1 = pi_2 * np.cosh(jh)\n    u2 = pi_2 * np.sinh(jh)\n    wj = u1 / np.cosh(u2) ** 2\n    xjc = 1 / (np.exp(u2) * np.cosh(u2))\n    wj[0] = wj[0] / 2 if k == 0 else wj[0]\n    return (xjc, wj)",
            "def _compute_pair(k, h0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    h = h0 / 2 ** k\n    max = _N_BASE_STEPS * 2 ** k\n    j = np.arange(max + 1) if k == 0 else np.arange(1, max + 1, 2)\n    jh = j * h\n    pi_2 = np.pi / 2\n    u1 = pi_2 * np.cosh(jh)\n    u2 = pi_2 * np.sinh(jh)\n    wj = u1 / np.cosh(u2) ** 2\n    xjc = 1 / (np.exp(u2) * np.cosh(u2))\n    wj[0] = wj[0] / 2 if k == 0 else wj[0]\n    return (xjc, wj)",
            "def _compute_pair(k, h0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    h = h0 / 2 ** k\n    max = _N_BASE_STEPS * 2 ** k\n    j = np.arange(max + 1) if k == 0 else np.arange(1, max + 1, 2)\n    jh = j * h\n    pi_2 = np.pi / 2\n    u1 = pi_2 * np.cosh(jh)\n    u2 = pi_2 * np.sinh(jh)\n    wj = u1 / np.cosh(u2) ** 2\n    xjc = 1 / (np.exp(u2) * np.cosh(u2))\n    wj[0] = wj[0] / 2 if k == 0 else wj[0]\n    return (xjc, wj)",
            "def _compute_pair(k, h0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    h = h0 / 2 ** k\n    max = _N_BASE_STEPS * 2 ** k\n    j = np.arange(max + 1) if k == 0 else np.arange(1, max + 1, 2)\n    jh = j * h\n    pi_2 = np.pi / 2\n    u1 = pi_2 * np.cosh(jh)\n    u2 = pi_2 * np.sinh(jh)\n    wj = u1 / np.cosh(u2) ** 2\n    xjc = 1 / (np.exp(u2) * np.cosh(u2))\n    wj[0] = wj[0] / 2 if k == 0 else wj[0]\n    return (xjc, wj)"
        ]
    },
    {
        "func_name": "_pair_cache",
        "original": "def _pair_cache(k, h0):\n    if h0 != _pair_cache.h0:\n        _pair_cache.xjc = np.empty(0)\n        _pair_cache.wj = np.empty(0)\n        _pair_cache.indices = [0]\n    xjcs = [_pair_cache.xjc]\n    wjs = [_pair_cache.wj]\n    for i in range(len(_pair_cache.indices) - 1, k + 1):\n        (xjc, wj) = _compute_pair(i, h0)\n        xjcs.append(xjc)\n        wjs.append(wj)\n        _pair_cache.indices.append(_pair_cache.indices[-1] + len(xjc))\n    _pair_cache.xjc = np.concatenate(xjcs)\n    _pair_cache.wj = np.concatenate(wjs)\n    _pair_cache.h0 = h0",
        "mutated": [
            "def _pair_cache(k, h0):\n    if False:\n        i = 10\n    if h0 != _pair_cache.h0:\n        _pair_cache.xjc = np.empty(0)\n        _pair_cache.wj = np.empty(0)\n        _pair_cache.indices = [0]\n    xjcs = [_pair_cache.xjc]\n    wjs = [_pair_cache.wj]\n    for i in range(len(_pair_cache.indices) - 1, k + 1):\n        (xjc, wj) = _compute_pair(i, h0)\n        xjcs.append(xjc)\n        wjs.append(wj)\n        _pair_cache.indices.append(_pair_cache.indices[-1] + len(xjc))\n    _pair_cache.xjc = np.concatenate(xjcs)\n    _pair_cache.wj = np.concatenate(wjs)\n    _pair_cache.h0 = h0",
            "def _pair_cache(k, h0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if h0 != _pair_cache.h0:\n        _pair_cache.xjc = np.empty(0)\n        _pair_cache.wj = np.empty(0)\n        _pair_cache.indices = [0]\n    xjcs = [_pair_cache.xjc]\n    wjs = [_pair_cache.wj]\n    for i in range(len(_pair_cache.indices) - 1, k + 1):\n        (xjc, wj) = _compute_pair(i, h0)\n        xjcs.append(xjc)\n        wjs.append(wj)\n        _pair_cache.indices.append(_pair_cache.indices[-1] + len(xjc))\n    _pair_cache.xjc = np.concatenate(xjcs)\n    _pair_cache.wj = np.concatenate(wjs)\n    _pair_cache.h0 = h0",
            "def _pair_cache(k, h0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if h0 != _pair_cache.h0:\n        _pair_cache.xjc = np.empty(0)\n        _pair_cache.wj = np.empty(0)\n        _pair_cache.indices = [0]\n    xjcs = [_pair_cache.xjc]\n    wjs = [_pair_cache.wj]\n    for i in range(len(_pair_cache.indices) - 1, k + 1):\n        (xjc, wj) = _compute_pair(i, h0)\n        xjcs.append(xjc)\n        wjs.append(wj)\n        _pair_cache.indices.append(_pair_cache.indices[-1] + len(xjc))\n    _pair_cache.xjc = np.concatenate(xjcs)\n    _pair_cache.wj = np.concatenate(wjs)\n    _pair_cache.h0 = h0",
            "def _pair_cache(k, h0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if h0 != _pair_cache.h0:\n        _pair_cache.xjc = np.empty(0)\n        _pair_cache.wj = np.empty(0)\n        _pair_cache.indices = [0]\n    xjcs = [_pair_cache.xjc]\n    wjs = [_pair_cache.wj]\n    for i in range(len(_pair_cache.indices) - 1, k + 1):\n        (xjc, wj) = _compute_pair(i, h0)\n        xjcs.append(xjc)\n        wjs.append(wj)\n        _pair_cache.indices.append(_pair_cache.indices[-1] + len(xjc))\n    _pair_cache.xjc = np.concatenate(xjcs)\n    _pair_cache.wj = np.concatenate(wjs)\n    _pair_cache.h0 = h0",
            "def _pair_cache(k, h0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if h0 != _pair_cache.h0:\n        _pair_cache.xjc = np.empty(0)\n        _pair_cache.wj = np.empty(0)\n        _pair_cache.indices = [0]\n    xjcs = [_pair_cache.xjc]\n    wjs = [_pair_cache.wj]\n    for i in range(len(_pair_cache.indices) - 1, k + 1):\n        (xjc, wj) = _compute_pair(i, h0)\n        xjcs.append(xjc)\n        wjs.append(wj)\n        _pair_cache.indices.append(_pair_cache.indices[-1] + len(xjc))\n    _pair_cache.xjc = np.concatenate(xjcs)\n    _pair_cache.wj = np.concatenate(wjs)\n    _pair_cache.h0 = h0"
        ]
    },
    {
        "func_name": "_get_pairs",
        "original": "def _get_pairs(k, h0, inclusive=False, dtype=np.float64):\n    if len(_pair_cache.indices) <= k + 2 or h0 != _pair_cache.h0:\n        _pair_cache(k, h0)\n    xjc = _pair_cache.xjc\n    wj = _pair_cache.wj\n    indices = _pair_cache.indices\n    start = 0 if inclusive else indices[k]\n    end = indices[k + 1]\n    return (xjc[start:end].astype(dtype), wj[start:end].astype(dtype))",
        "mutated": [
            "def _get_pairs(k, h0, inclusive=False, dtype=np.float64):\n    if False:\n        i = 10\n    if len(_pair_cache.indices) <= k + 2 or h0 != _pair_cache.h0:\n        _pair_cache(k, h0)\n    xjc = _pair_cache.xjc\n    wj = _pair_cache.wj\n    indices = _pair_cache.indices\n    start = 0 if inclusive else indices[k]\n    end = indices[k + 1]\n    return (xjc[start:end].astype(dtype), wj[start:end].astype(dtype))",
            "def _get_pairs(k, h0, inclusive=False, dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if len(_pair_cache.indices) <= k + 2 or h0 != _pair_cache.h0:\n        _pair_cache(k, h0)\n    xjc = _pair_cache.xjc\n    wj = _pair_cache.wj\n    indices = _pair_cache.indices\n    start = 0 if inclusive else indices[k]\n    end = indices[k + 1]\n    return (xjc[start:end].astype(dtype), wj[start:end].astype(dtype))",
            "def _get_pairs(k, h0, inclusive=False, dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if len(_pair_cache.indices) <= k + 2 or h0 != _pair_cache.h0:\n        _pair_cache(k, h0)\n    xjc = _pair_cache.xjc\n    wj = _pair_cache.wj\n    indices = _pair_cache.indices\n    start = 0 if inclusive else indices[k]\n    end = indices[k + 1]\n    return (xjc[start:end].astype(dtype), wj[start:end].astype(dtype))",
            "def _get_pairs(k, h0, inclusive=False, dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if len(_pair_cache.indices) <= k + 2 or h0 != _pair_cache.h0:\n        _pair_cache(k, h0)\n    xjc = _pair_cache.xjc\n    wj = _pair_cache.wj\n    indices = _pair_cache.indices\n    start = 0 if inclusive else indices[k]\n    end = indices[k + 1]\n    return (xjc[start:end].astype(dtype), wj[start:end].astype(dtype))",
            "def _get_pairs(k, h0, inclusive=False, dtype=np.float64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if len(_pair_cache.indices) <= k + 2 or h0 != _pair_cache.h0:\n        _pair_cache(k, h0)\n    xjc = _pair_cache.xjc\n    wj = _pair_cache.wj\n    indices = _pair_cache.indices\n    start = 0 if inclusive else indices[k]\n    end = indices[k + 1]\n    return (xjc[start:end].astype(dtype), wj[start:end].astype(dtype))"
        ]
    },
    {
        "func_name": "_transform_to_limits",
        "original": "def _transform_to_limits(xjc, wj, a, b):\n    alpha = (b - a) / 2\n    xj = np.concatenate((-alpha * xjc + b, alpha * xjc + a), axis=-1)\n    wj = wj * alpha\n    wj = np.concatenate((wj, wj), axis=-1)\n    invalid = (xj <= a) | (xj >= b)\n    wj[invalid] = 0\n    return (xj, wj)",
        "mutated": [
            "def _transform_to_limits(xjc, wj, a, b):\n    if False:\n        i = 10\n    alpha = (b - a) / 2\n    xj = np.concatenate((-alpha * xjc + b, alpha * xjc + a), axis=-1)\n    wj = wj * alpha\n    wj = np.concatenate((wj, wj), axis=-1)\n    invalid = (xj <= a) | (xj >= b)\n    wj[invalid] = 0\n    return (xj, wj)",
            "def _transform_to_limits(xjc, wj, a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    alpha = (b - a) / 2\n    xj = np.concatenate((-alpha * xjc + b, alpha * xjc + a), axis=-1)\n    wj = wj * alpha\n    wj = np.concatenate((wj, wj), axis=-1)\n    invalid = (xj <= a) | (xj >= b)\n    wj[invalid] = 0\n    return (xj, wj)",
            "def _transform_to_limits(xjc, wj, a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    alpha = (b - a) / 2\n    xj = np.concatenate((-alpha * xjc + b, alpha * xjc + a), axis=-1)\n    wj = wj * alpha\n    wj = np.concatenate((wj, wj), axis=-1)\n    invalid = (xj <= a) | (xj >= b)\n    wj[invalid] = 0\n    return (xj, wj)",
            "def _transform_to_limits(xjc, wj, a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    alpha = (b - a) / 2\n    xj = np.concatenate((-alpha * xjc + b, alpha * xjc + a), axis=-1)\n    wj = wj * alpha\n    wj = np.concatenate((wj, wj), axis=-1)\n    invalid = (xj <= a) | (xj >= b)\n    wj[invalid] = 0\n    return (xj, wj)",
            "def _transform_to_limits(xjc, wj, a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    alpha = (b - a) / 2\n    xj = np.concatenate((-alpha * xjc + b, alpha * xjc + a), axis=-1)\n    wj = wj * alpha\n    wj = np.concatenate((wj, wj), axis=-1)\n    invalid = (xj <= a) | (xj >= b)\n    wj[invalid] = 0\n    return (xj, wj)"
        ]
    },
    {
        "func_name": "_euler_maclaurin_sum",
        "original": "def _euler_maclaurin_sum(fj, work):\n    (xr0, fr0, wr0) = (work.xr0, work.fr0, work.wr0)\n    (xl0, fl0, wl0) = (work.xl0, work.fl0, work.wl0)\n    (xj, fj, wj) = (work.xj.T, fj.T, work.wj.T)\n    (n_x, n_active) = xj.shape\n    (xr, xl) = xj.reshape(2, n_x // 2, n_active).copy()\n    (fr, fl) = fj.reshape(2, n_x // 2, n_active)\n    (wr, wl) = wj.reshape(2, n_x // 2, n_active)\n    invalid_r = ~np.isfinite(fr) | (wr == 0)\n    invalid_l = ~np.isfinite(fl) | (wl == 0)\n    xr[invalid_r] = -np.inf\n    ir = np.argmax(xr, axis=0, keepdims=True)\n    xr_max = np.take_along_axis(xr, ir, axis=0)[0]\n    fr_max = np.take_along_axis(fr, ir, axis=0)[0]\n    wr_max = np.take_along_axis(wr, ir, axis=0)[0]\n    j = xr_max > xr0\n    xr0[j] = xr_max[j]\n    fr0[j] = fr_max[j]\n    wr0[j] = wr_max[j]\n    xl[invalid_l] = np.inf\n    il = np.argmin(xl, axis=0, keepdims=True)\n    xl_min = np.take_along_axis(xl, il, axis=0)[0]\n    fl_min = np.take_along_axis(fl, il, axis=0)[0]\n    wl_min = np.take_along_axis(wl, il, axis=0)[0]\n    j = xl_min < xl0\n    xl0[j] = xl_min[j]\n    fl0[j] = fl_min[j]\n    wl0[j] = wl_min[j]\n    fj = fj.T\n    flwl0 = fl0 + np.log(wl0) if work.log else fl0 * wl0\n    frwr0 = fr0 + np.log(wr0) if work.log else fr0 * wr0\n    magnitude = np.real if work.log else np.abs\n    work.d4 = np.maximum(magnitude(flwl0), magnitude(frwr0))\n    fr0b = np.broadcast_to(fr0[np.newaxis, :], fr.shape)\n    fl0b = np.broadcast_to(fl0[np.newaxis, :], fl.shape)\n    fr[invalid_r] = fr0b[invalid_r]\n    fl[invalid_l] = fl0b[invalid_l]\n    fjwj = fj + np.log(work.wj) if work.log else fj * work.wj\n    Sn = special.logsumexp(fjwj + np.log(work.h), axis=-1) if work.log else np.sum(fjwj, axis=-1) * work.h\n    (work.xr0, work.fr0, work.wr0) = (xr0, fr0, wr0)\n    (work.xl0, work.fl0, work.wl0) = (xl0, fl0, wl0)\n    return (fjwj, Sn)",
        "mutated": [
            "def _euler_maclaurin_sum(fj, work):\n    if False:\n        i = 10\n    (xr0, fr0, wr0) = (work.xr0, work.fr0, work.wr0)\n    (xl0, fl0, wl0) = (work.xl0, work.fl0, work.wl0)\n    (xj, fj, wj) = (work.xj.T, fj.T, work.wj.T)\n    (n_x, n_active) = xj.shape\n    (xr, xl) = xj.reshape(2, n_x // 2, n_active).copy()\n    (fr, fl) = fj.reshape(2, n_x // 2, n_active)\n    (wr, wl) = wj.reshape(2, n_x // 2, n_active)\n    invalid_r = ~np.isfinite(fr) | (wr == 0)\n    invalid_l = ~np.isfinite(fl) | (wl == 0)\n    xr[invalid_r] = -np.inf\n    ir = np.argmax(xr, axis=0, keepdims=True)\n    xr_max = np.take_along_axis(xr, ir, axis=0)[0]\n    fr_max = np.take_along_axis(fr, ir, axis=0)[0]\n    wr_max = np.take_along_axis(wr, ir, axis=0)[0]\n    j = xr_max > xr0\n    xr0[j] = xr_max[j]\n    fr0[j] = fr_max[j]\n    wr0[j] = wr_max[j]\n    xl[invalid_l] = np.inf\n    il = np.argmin(xl, axis=0, keepdims=True)\n    xl_min = np.take_along_axis(xl, il, axis=0)[0]\n    fl_min = np.take_along_axis(fl, il, axis=0)[0]\n    wl_min = np.take_along_axis(wl, il, axis=0)[0]\n    j = xl_min < xl0\n    xl0[j] = xl_min[j]\n    fl0[j] = fl_min[j]\n    wl0[j] = wl_min[j]\n    fj = fj.T\n    flwl0 = fl0 + np.log(wl0) if work.log else fl0 * wl0\n    frwr0 = fr0 + np.log(wr0) if work.log else fr0 * wr0\n    magnitude = np.real if work.log else np.abs\n    work.d4 = np.maximum(magnitude(flwl0), magnitude(frwr0))\n    fr0b = np.broadcast_to(fr0[np.newaxis, :], fr.shape)\n    fl0b = np.broadcast_to(fl0[np.newaxis, :], fl.shape)\n    fr[invalid_r] = fr0b[invalid_r]\n    fl[invalid_l] = fl0b[invalid_l]\n    fjwj = fj + np.log(work.wj) if work.log else fj * work.wj\n    Sn = special.logsumexp(fjwj + np.log(work.h), axis=-1) if work.log else np.sum(fjwj, axis=-1) * work.h\n    (work.xr0, work.fr0, work.wr0) = (xr0, fr0, wr0)\n    (work.xl0, work.fl0, work.wl0) = (xl0, fl0, wl0)\n    return (fjwj, Sn)",
            "def _euler_maclaurin_sum(fj, work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (xr0, fr0, wr0) = (work.xr0, work.fr0, work.wr0)\n    (xl0, fl0, wl0) = (work.xl0, work.fl0, work.wl0)\n    (xj, fj, wj) = (work.xj.T, fj.T, work.wj.T)\n    (n_x, n_active) = xj.shape\n    (xr, xl) = xj.reshape(2, n_x // 2, n_active).copy()\n    (fr, fl) = fj.reshape(2, n_x // 2, n_active)\n    (wr, wl) = wj.reshape(2, n_x // 2, n_active)\n    invalid_r = ~np.isfinite(fr) | (wr == 0)\n    invalid_l = ~np.isfinite(fl) | (wl == 0)\n    xr[invalid_r] = -np.inf\n    ir = np.argmax(xr, axis=0, keepdims=True)\n    xr_max = np.take_along_axis(xr, ir, axis=0)[0]\n    fr_max = np.take_along_axis(fr, ir, axis=0)[0]\n    wr_max = np.take_along_axis(wr, ir, axis=0)[0]\n    j = xr_max > xr0\n    xr0[j] = xr_max[j]\n    fr0[j] = fr_max[j]\n    wr0[j] = wr_max[j]\n    xl[invalid_l] = np.inf\n    il = np.argmin(xl, axis=0, keepdims=True)\n    xl_min = np.take_along_axis(xl, il, axis=0)[0]\n    fl_min = np.take_along_axis(fl, il, axis=0)[0]\n    wl_min = np.take_along_axis(wl, il, axis=0)[0]\n    j = xl_min < xl0\n    xl0[j] = xl_min[j]\n    fl0[j] = fl_min[j]\n    wl0[j] = wl_min[j]\n    fj = fj.T\n    flwl0 = fl0 + np.log(wl0) if work.log else fl0 * wl0\n    frwr0 = fr0 + np.log(wr0) if work.log else fr0 * wr0\n    magnitude = np.real if work.log else np.abs\n    work.d4 = np.maximum(magnitude(flwl0), magnitude(frwr0))\n    fr0b = np.broadcast_to(fr0[np.newaxis, :], fr.shape)\n    fl0b = np.broadcast_to(fl0[np.newaxis, :], fl.shape)\n    fr[invalid_r] = fr0b[invalid_r]\n    fl[invalid_l] = fl0b[invalid_l]\n    fjwj = fj + np.log(work.wj) if work.log else fj * work.wj\n    Sn = special.logsumexp(fjwj + np.log(work.h), axis=-1) if work.log else np.sum(fjwj, axis=-1) * work.h\n    (work.xr0, work.fr0, work.wr0) = (xr0, fr0, wr0)\n    (work.xl0, work.fl0, work.wl0) = (xl0, fl0, wl0)\n    return (fjwj, Sn)",
            "def _euler_maclaurin_sum(fj, work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (xr0, fr0, wr0) = (work.xr0, work.fr0, work.wr0)\n    (xl0, fl0, wl0) = (work.xl0, work.fl0, work.wl0)\n    (xj, fj, wj) = (work.xj.T, fj.T, work.wj.T)\n    (n_x, n_active) = xj.shape\n    (xr, xl) = xj.reshape(2, n_x // 2, n_active).copy()\n    (fr, fl) = fj.reshape(2, n_x // 2, n_active)\n    (wr, wl) = wj.reshape(2, n_x // 2, n_active)\n    invalid_r = ~np.isfinite(fr) | (wr == 0)\n    invalid_l = ~np.isfinite(fl) | (wl == 0)\n    xr[invalid_r] = -np.inf\n    ir = np.argmax(xr, axis=0, keepdims=True)\n    xr_max = np.take_along_axis(xr, ir, axis=0)[0]\n    fr_max = np.take_along_axis(fr, ir, axis=0)[0]\n    wr_max = np.take_along_axis(wr, ir, axis=0)[0]\n    j = xr_max > xr0\n    xr0[j] = xr_max[j]\n    fr0[j] = fr_max[j]\n    wr0[j] = wr_max[j]\n    xl[invalid_l] = np.inf\n    il = np.argmin(xl, axis=0, keepdims=True)\n    xl_min = np.take_along_axis(xl, il, axis=0)[0]\n    fl_min = np.take_along_axis(fl, il, axis=0)[0]\n    wl_min = np.take_along_axis(wl, il, axis=0)[0]\n    j = xl_min < xl0\n    xl0[j] = xl_min[j]\n    fl0[j] = fl_min[j]\n    wl0[j] = wl_min[j]\n    fj = fj.T\n    flwl0 = fl0 + np.log(wl0) if work.log else fl0 * wl0\n    frwr0 = fr0 + np.log(wr0) if work.log else fr0 * wr0\n    magnitude = np.real if work.log else np.abs\n    work.d4 = np.maximum(magnitude(flwl0), magnitude(frwr0))\n    fr0b = np.broadcast_to(fr0[np.newaxis, :], fr.shape)\n    fl0b = np.broadcast_to(fl0[np.newaxis, :], fl.shape)\n    fr[invalid_r] = fr0b[invalid_r]\n    fl[invalid_l] = fl0b[invalid_l]\n    fjwj = fj + np.log(work.wj) if work.log else fj * work.wj\n    Sn = special.logsumexp(fjwj + np.log(work.h), axis=-1) if work.log else np.sum(fjwj, axis=-1) * work.h\n    (work.xr0, work.fr0, work.wr0) = (xr0, fr0, wr0)\n    (work.xl0, work.fl0, work.wl0) = (xl0, fl0, wl0)\n    return (fjwj, Sn)",
            "def _euler_maclaurin_sum(fj, work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (xr0, fr0, wr0) = (work.xr0, work.fr0, work.wr0)\n    (xl0, fl0, wl0) = (work.xl0, work.fl0, work.wl0)\n    (xj, fj, wj) = (work.xj.T, fj.T, work.wj.T)\n    (n_x, n_active) = xj.shape\n    (xr, xl) = xj.reshape(2, n_x // 2, n_active).copy()\n    (fr, fl) = fj.reshape(2, n_x // 2, n_active)\n    (wr, wl) = wj.reshape(2, n_x // 2, n_active)\n    invalid_r = ~np.isfinite(fr) | (wr == 0)\n    invalid_l = ~np.isfinite(fl) | (wl == 0)\n    xr[invalid_r] = -np.inf\n    ir = np.argmax(xr, axis=0, keepdims=True)\n    xr_max = np.take_along_axis(xr, ir, axis=0)[0]\n    fr_max = np.take_along_axis(fr, ir, axis=0)[0]\n    wr_max = np.take_along_axis(wr, ir, axis=0)[0]\n    j = xr_max > xr0\n    xr0[j] = xr_max[j]\n    fr0[j] = fr_max[j]\n    wr0[j] = wr_max[j]\n    xl[invalid_l] = np.inf\n    il = np.argmin(xl, axis=0, keepdims=True)\n    xl_min = np.take_along_axis(xl, il, axis=0)[0]\n    fl_min = np.take_along_axis(fl, il, axis=0)[0]\n    wl_min = np.take_along_axis(wl, il, axis=0)[0]\n    j = xl_min < xl0\n    xl0[j] = xl_min[j]\n    fl0[j] = fl_min[j]\n    wl0[j] = wl_min[j]\n    fj = fj.T\n    flwl0 = fl0 + np.log(wl0) if work.log else fl0 * wl0\n    frwr0 = fr0 + np.log(wr0) if work.log else fr0 * wr0\n    magnitude = np.real if work.log else np.abs\n    work.d4 = np.maximum(magnitude(flwl0), magnitude(frwr0))\n    fr0b = np.broadcast_to(fr0[np.newaxis, :], fr.shape)\n    fl0b = np.broadcast_to(fl0[np.newaxis, :], fl.shape)\n    fr[invalid_r] = fr0b[invalid_r]\n    fl[invalid_l] = fl0b[invalid_l]\n    fjwj = fj + np.log(work.wj) if work.log else fj * work.wj\n    Sn = special.logsumexp(fjwj + np.log(work.h), axis=-1) if work.log else np.sum(fjwj, axis=-1) * work.h\n    (work.xr0, work.fr0, work.wr0) = (xr0, fr0, wr0)\n    (work.xl0, work.fl0, work.wl0) = (xl0, fl0, wl0)\n    return (fjwj, Sn)",
            "def _euler_maclaurin_sum(fj, work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (xr0, fr0, wr0) = (work.xr0, work.fr0, work.wr0)\n    (xl0, fl0, wl0) = (work.xl0, work.fl0, work.wl0)\n    (xj, fj, wj) = (work.xj.T, fj.T, work.wj.T)\n    (n_x, n_active) = xj.shape\n    (xr, xl) = xj.reshape(2, n_x // 2, n_active).copy()\n    (fr, fl) = fj.reshape(2, n_x // 2, n_active)\n    (wr, wl) = wj.reshape(2, n_x // 2, n_active)\n    invalid_r = ~np.isfinite(fr) | (wr == 0)\n    invalid_l = ~np.isfinite(fl) | (wl == 0)\n    xr[invalid_r] = -np.inf\n    ir = np.argmax(xr, axis=0, keepdims=True)\n    xr_max = np.take_along_axis(xr, ir, axis=0)[0]\n    fr_max = np.take_along_axis(fr, ir, axis=0)[0]\n    wr_max = np.take_along_axis(wr, ir, axis=0)[0]\n    j = xr_max > xr0\n    xr0[j] = xr_max[j]\n    fr0[j] = fr_max[j]\n    wr0[j] = wr_max[j]\n    xl[invalid_l] = np.inf\n    il = np.argmin(xl, axis=0, keepdims=True)\n    xl_min = np.take_along_axis(xl, il, axis=0)[0]\n    fl_min = np.take_along_axis(fl, il, axis=0)[0]\n    wl_min = np.take_along_axis(wl, il, axis=0)[0]\n    j = xl_min < xl0\n    xl0[j] = xl_min[j]\n    fl0[j] = fl_min[j]\n    wl0[j] = wl_min[j]\n    fj = fj.T\n    flwl0 = fl0 + np.log(wl0) if work.log else fl0 * wl0\n    frwr0 = fr0 + np.log(wr0) if work.log else fr0 * wr0\n    magnitude = np.real if work.log else np.abs\n    work.d4 = np.maximum(magnitude(flwl0), magnitude(frwr0))\n    fr0b = np.broadcast_to(fr0[np.newaxis, :], fr.shape)\n    fl0b = np.broadcast_to(fl0[np.newaxis, :], fl.shape)\n    fr[invalid_r] = fr0b[invalid_r]\n    fl[invalid_l] = fl0b[invalid_l]\n    fjwj = fj + np.log(work.wj) if work.log else fj * work.wj\n    Sn = special.logsumexp(fjwj + np.log(work.h), axis=-1) if work.log else np.sum(fjwj, axis=-1) * work.h\n    (work.xr0, work.fr0, work.wr0) = (xr0, fr0, wr0)\n    (work.xl0, work.fl0, work.wl0) = (xl0, fl0, wl0)\n    return (fjwj, Sn)"
        ]
    },
    {
        "func_name": "_estimate_error",
        "original": "def _estimate_error(work):\n    if work.n == 0 or work.nit == 0:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    indices = _pair_cache.indices\n    n_active = len(work.Sn)\n    axis_kwargs = dict(axis=-1, keepdims=True)\n    if work.Sk.shape[-1] == 0:\n        h = 2 * work.h\n        n_x = indices[work.n]\n        fjwj_rl = work.fjwj.reshape(n_active, 2, -1)\n        fjwj = fjwj_rl[:, :, :n_x].reshape(n_active, 2 * n_x)\n        Snm1 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm1, work.Sk), axis=-1)\n    if work.n == 1:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    if work.Sk.shape[-1] < 2:\n        h = 4 * work.h\n        n_x = indices[work.n - 1]\n        fjwj_rl = work.fjwj.reshape(len(work.Sn), 2, -1)\n        fjwj = fjwj_rl[..., :n_x].reshape(n_active, 2 * n_x)\n        Snm2 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm2, work.Sk), axis=-1)\n    Snm2 = work.Sk[..., -2]\n    Snm1 = work.Sk[..., -1]\n    e1 = work.eps\n    if work.log:\n        log_e1 = np.log(e1)\n        d1 = np.real(special.logsumexp([work.Sn, Snm1 + work.pi * 1j], axis=0))\n        d2 = np.real(special.logsumexp([work.Sn, Snm2 + work.pi * 1j], axis=0))\n        d3 = log_e1 + np.max(np.real(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** 2 / d2, 2 * d1, d3, d4], axis=0)\n        rerr = np.maximum(log_e1, aerr - np.real(work.Sn))\n    else:\n        d1 = np.abs(work.Sn - Snm1)\n        d2 = np.abs(work.Sn - Snm2)\n        d3 = e1 * np.max(np.abs(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** (np.log(d1) / np.log(d2)), d1 ** 2, d3, d4], axis=0)\n        rerr = np.maximum(e1, aerr / np.abs(work.Sn))\n    return (rerr, aerr.reshape(work.Sn.shape))",
        "mutated": [
            "def _estimate_error(work):\n    if False:\n        i = 10\n    if work.n == 0 or work.nit == 0:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    indices = _pair_cache.indices\n    n_active = len(work.Sn)\n    axis_kwargs = dict(axis=-1, keepdims=True)\n    if work.Sk.shape[-1] == 0:\n        h = 2 * work.h\n        n_x = indices[work.n]\n        fjwj_rl = work.fjwj.reshape(n_active, 2, -1)\n        fjwj = fjwj_rl[:, :, :n_x].reshape(n_active, 2 * n_x)\n        Snm1 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm1, work.Sk), axis=-1)\n    if work.n == 1:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    if work.Sk.shape[-1] < 2:\n        h = 4 * work.h\n        n_x = indices[work.n - 1]\n        fjwj_rl = work.fjwj.reshape(len(work.Sn), 2, -1)\n        fjwj = fjwj_rl[..., :n_x].reshape(n_active, 2 * n_x)\n        Snm2 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm2, work.Sk), axis=-1)\n    Snm2 = work.Sk[..., -2]\n    Snm1 = work.Sk[..., -1]\n    e1 = work.eps\n    if work.log:\n        log_e1 = np.log(e1)\n        d1 = np.real(special.logsumexp([work.Sn, Snm1 + work.pi * 1j], axis=0))\n        d2 = np.real(special.logsumexp([work.Sn, Snm2 + work.pi * 1j], axis=0))\n        d3 = log_e1 + np.max(np.real(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** 2 / d2, 2 * d1, d3, d4], axis=0)\n        rerr = np.maximum(log_e1, aerr - np.real(work.Sn))\n    else:\n        d1 = np.abs(work.Sn - Snm1)\n        d2 = np.abs(work.Sn - Snm2)\n        d3 = e1 * np.max(np.abs(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** (np.log(d1) / np.log(d2)), d1 ** 2, d3, d4], axis=0)\n        rerr = np.maximum(e1, aerr / np.abs(work.Sn))\n    return (rerr, aerr.reshape(work.Sn.shape))",
            "def _estimate_error(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if work.n == 0 or work.nit == 0:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    indices = _pair_cache.indices\n    n_active = len(work.Sn)\n    axis_kwargs = dict(axis=-1, keepdims=True)\n    if work.Sk.shape[-1] == 0:\n        h = 2 * work.h\n        n_x = indices[work.n]\n        fjwj_rl = work.fjwj.reshape(n_active, 2, -1)\n        fjwj = fjwj_rl[:, :, :n_x].reshape(n_active, 2 * n_x)\n        Snm1 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm1, work.Sk), axis=-1)\n    if work.n == 1:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    if work.Sk.shape[-1] < 2:\n        h = 4 * work.h\n        n_x = indices[work.n - 1]\n        fjwj_rl = work.fjwj.reshape(len(work.Sn), 2, -1)\n        fjwj = fjwj_rl[..., :n_x].reshape(n_active, 2 * n_x)\n        Snm2 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm2, work.Sk), axis=-1)\n    Snm2 = work.Sk[..., -2]\n    Snm1 = work.Sk[..., -1]\n    e1 = work.eps\n    if work.log:\n        log_e1 = np.log(e1)\n        d1 = np.real(special.logsumexp([work.Sn, Snm1 + work.pi * 1j], axis=0))\n        d2 = np.real(special.logsumexp([work.Sn, Snm2 + work.pi * 1j], axis=0))\n        d3 = log_e1 + np.max(np.real(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** 2 / d2, 2 * d1, d3, d4], axis=0)\n        rerr = np.maximum(log_e1, aerr - np.real(work.Sn))\n    else:\n        d1 = np.abs(work.Sn - Snm1)\n        d2 = np.abs(work.Sn - Snm2)\n        d3 = e1 * np.max(np.abs(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** (np.log(d1) / np.log(d2)), d1 ** 2, d3, d4], axis=0)\n        rerr = np.maximum(e1, aerr / np.abs(work.Sn))\n    return (rerr, aerr.reshape(work.Sn.shape))",
            "def _estimate_error(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if work.n == 0 or work.nit == 0:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    indices = _pair_cache.indices\n    n_active = len(work.Sn)\n    axis_kwargs = dict(axis=-1, keepdims=True)\n    if work.Sk.shape[-1] == 0:\n        h = 2 * work.h\n        n_x = indices[work.n]\n        fjwj_rl = work.fjwj.reshape(n_active, 2, -1)\n        fjwj = fjwj_rl[:, :, :n_x].reshape(n_active, 2 * n_x)\n        Snm1 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm1, work.Sk), axis=-1)\n    if work.n == 1:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    if work.Sk.shape[-1] < 2:\n        h = 4 * work.h\n        n_x = indices[work.n - 1]\n        fjwj_rl = work.fjwj.reshape(len(work.Sn), 2, -1)\n        fjwj = fjwj_rl[..., :n_x].reshape(n_active, 2 * n_x)\n        Snm2 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm2, work.Sk), axis=-1)\n    Snm2 = work.Sk[..., -2]\n    Snm1 = work.Sk[..., -1]\n    e1 = work.eps\n    if work.log:\n        log_e1 = np.log(e1)\n        d1 = np.real(special.logsumexp([work.Sn, Snm1 + work.pi * 1j], axis=0))\n        d2 = np.real(special.logsumexp([work.Sn, Snm2 + work.pi * 1j], axis=0))\n        d3 = log_e1 + np.max(np.real(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** 2 / d2, 2 * d1, d3, d4], axis=0)\n        rerr = np.maximum(log_e1, aerr - np.real(work.Sn))\n    else:\n        d1 = np.abs(work.Sn - Snm1)\n        d2 = np.abs(work.Sn - Snm2)\n        d3 = e1 * np.max(np.abs(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** (np.log(d1) / np.log(d2)), d1 ** 2, d3, d4], axis=0)\n        rerr = np.maximum(e1, aerr / np.abs(work.Sn))\n    return (rerr, aerr.reshape(work.Sn.shape))",
            "def _estimate_error(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if work.n == 0 or work.nit == 0:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    indices = _pair_cache.indices\n    n_active = len(work.Sn)\n    axis_kwargs = dict(axis=-1, keepdims=True)\n    if work.Sk.shape[-1] == 0:\n        h = 2 * work.h\n        n_x = indices[work.n]\n        fjwj_rl = work.fjwj.reshape(n_active, 2, -1)\n        fjwj = fjwj_rl[:, :, :n_x].reshape(n_active, 2 * n_x)\n        Snm1 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm1, work.Sk), axis=-1)\n    if work.n == 1:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    if work.Sk.shape[-1] < 2:\n        h = 4 * work.h\n        n_x = indices[work.n - 1]\n        fjwj_rl = work.fjwj.reshape(len(work.Sn), 2, -1)\n        fjwj = fjwj_rl[..., :n_x].reshape(n_active, 2 * n_x)\n        Snm2 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm2, work.Sk), axis=-1)\n    Snm2 = work.Sk[..., -2]\n    Snm1 = work.Sk[..., -1]\n    e1 = work.eps\n    if work.log:\n        log_e1 = np.log(e1)\n        d1 = np.real(special.logsumexp([work.Sn, Snm1 + work.pi * 1j], axis=0))\n        d2 = np.real(special.logsumexp([work.Sn, Snm2 + work.pi * 1j], axis=0))\n        d3 = log_e1 + np.max(np.real(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** 2 / d2, 2 * d1, d3, d4], axis=0)\n        rerr = np.maximum(log_e1, aerr - np.real(work.Sn))\n    else:\n        d1 = np.abs(work.Sn - Snm1)\n        d2 = np.abs(work.Sn - Snm2)\n        d3 = e1 * np.max(np.abs(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** (np.log(d1) / np.log(d2)), d1 ** 2, d3, d4], axis=0)\n        rerr = np.maximum(e1, aerr / np.abs(work.Sn))\n    return (rerr, aerr.reshape(work.Sn.shape))",
            "def _estimate_error(work):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if work.n == 0 or work.nit == 0:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    indices = _pair_cache.indices\n    n_active = len(work.Sn)\n    axis_kwargs = dict(axis=-1, keepdims=True)\n    if work.Sk.shape[-1] == 0:\n        h = 2 * work.h\n        n_x = indices[work.n]\n        fjwj_rl = work.fjwj.reshape(n_active, 2, -1)\n        fjwj = fjwj_rl[:, :, :n_x].reshape(n_active, 2 * n_x)\n        Snm1 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm1, work.Sk), axis=-1)\n    if work.n == 1:\n        nan = np.full_like(work.Sn, np.nan)\n        return (nan, nan)\n    if work.Sk.shape[-1] < 2:\n        h = 4 * work.h\n        n_x = indices[work.n - 1]\n        fjwj_rl = work.fjwj.reshape(len(work.Sn), 2, -1)\n        fjwj = fjwj_rl[..., :n_x].reshape(n_active, 2 * n_x)\n        Snm2 = special.logsumexp(fjwj, **axis_kwargs) + np.log(h) if work.log else np.sum(fjwj, **axis_kwargs) * h\n        work.Sk = np.concatenate((Snm2, work.Sk), axis=-1)\n    Snm2 = work.Sk[..., -2]\n    Snm1 = work.Sk[..., -1]\n    e1 = work.eps\n    if work.log:\n        log_e1 = np.log(e1)\n        d1 = np.real(special.logsumexp([work.Sn, Snm1 + work.pi * 1j], axis=0))\n        d2 = np.real(special.logsumexp([work.Sn, Snm2 + work.pi * 1j], axis=0))\n        d3 = log_e1 + np.max(np.real(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** 2 / d2, 2 * d1, d3, d4], axis=0)\n        rerr = np.maximum(log_e1, aerr - np.real(work.Sn))\n    else:\n        d1 = np.abs(work.Sn - Snm1)\n        d2 = np.abs(work.Sn - Snm2)\n        d3 = e1 * np.max(np.abs(work.fjwj), axis=-1)\n        d4 = work.d4\n        aerr = np.max([d1 ** (np.log(d1) / np.log(d2)), d1 ** 2, d3, d4], axis=0)\n        rerr = np.maximum(e1, aerr / np.abs(work.Sn))\n    return (rerr, aerr.reshape(work.Sn.shape))"
        ]
    },
    {
        "func_name": "_transform_integrals",
        "original": "def _transform_integrals(a, b):\n    negative = b < a\n    (a[negative], b[negative]) = (b[negative], a[negative])\n    abinf = np.isinf(a) & np.isinf(b)\n    (a[abinf], b[abinf]) = (-1, 1)\n    ainf = np.isinf(a)\n    (a[ainf], b[ainf]) = (-b[ainf], -a[ainf])\n    binf = np.isinf(b)\n    a0 = a.copy()\n    (a[binf], b[binf]) = (0, 1)\n    return (a, b, a0, negative, abinf, ainf, binf)",
        "mutated": [
            "def _transform_integrals(a, b):\n    if False:\n        i = 10\n    negative = b < a\n    (a[negative], b[negative]) = (b[negative], a[negative])\n    abinf = np.isinf(a) & np.isinf(b)\n    (a[abinf], b[abinf]) = (-1, 1)\n    ainf = np.isinf(a)\n    (a[ainf], b[ainf]) = (-b[ainf], -a[ainf])\n    binf = np.isinf(b)\n    a0 = a.copy()\n    (a[binf], b[binf]) = (0, 1)\n    return (a, b, a0, negative, abinf, ainf, binf)",
            "def _transform_integrals(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    negative = b < a\n    (a[negative], b[negative]) = (b[negative], a[negative])\n    abinf = np.isinf(a) & np.isinf(b)\n    (a[abinf], b[abinf]) = (-1, 1)\n    ainf = np.isinf(a)\n    (a[ainf], b[ainf]) = (-b[ainf], -a[ainf])\n    binf = np.isinf(b)\n    a0 = a.copy()\n    (a[binf], b[binf]) = (0, 1)\n    return (a, b, a0, negative, abinf, ainf, binf)",
            "def _transform_integrals(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    negative = b < a\n    (a[negative], b[negative]) = (b[negative], a[negative])\n    abinf = np.isinf(a) & np.isinf(b)\n    (a[abinf], b[abinf]) = (-1, 1)\n    ainf = np.isinf(a)\n    (a[ainf], b[ainf]) = (-b[ainf], -a[ainf])\n    binf = np.isinf(b)\n    a0 = a.copy()\n    (a[binf], b[binf]) = (0, 1)\n    return (a, b, a0, negative, abinf, ainf, binf)",
            "def _transform_integrals(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    negative = b < a\n    (a[negative], b[negative]) = (b[negative], a[negative])\n    abinf = np.isinf(a) & np.isinf(b)\n    (a[abinf], b[abinf]) = (-1, 1)\n    ainf = np.isinf(a)\n    (a[ainf], b[ainf]) = (-b[ainf], -a[ainf])\n    binf = np.isinf(b)\n    a0 = a.copy()\n    (a[binf], b[binf]) = (0, 1)\n    return (a, b, a0, negative, abinf, ainf, binf)",
            "def _transform_integrals(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    negative = b < a\n    (a[negative], b[negative]) = (b[negative], a[negative])\n    abinf = np.isinf(a) & np.isinf(b)\n    (a[abinf], b[abinf]) = (-1, 1)\n    ainf = np.isinf(a)\n    (a[ainf], b[ainf]) = (-b[ainf], -a[ainf])\n    binf = np.isinf(b)\n    a0 = a.copy()\n    (a[binf], b[binf]) = (0, 1)\n    return (a, b, a0, negative, abinf, ainf, binf)"
        ]
    },
    {
        "func_name": "_tanhsinh_iv",
        "original": "def _tanhsinh_iv(f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback):\n    message = '`f` must be callable.'\n    if not callable(f):\n        raise ValueError(message)\n    message = 'All elements of `a` and `b` must be real numbers.'\n    (a, b) = np.broadcast_arrays(a, b)\n    if np.any(np.iscomplex(a)) or np.any(np.iscomplex(b)):\n        raise ValueError(message)\n    message = '`log` must be True or False.'\n    if log not in {True, False}:\n        raise ValueError(message)\n    log = bool(log)\n    if atol is None:\n        atol = -np.inf if log else 0\n    rtol_temp = rtol if rtol is not None else 0.0\n    params = np.asarray([atol, rtol_temp, 0.0])\n    message = '`atol` and `rtol` must be real numbers.'\n    if not np.issubdtype(params.dtype, np.floating):\n        raise ValueError(message)\n    if log:\n        message = '`atol` and `rtol` may not be positive infinity.'\n        if np.any(np.isposinf(params)):\n            raise ValueError(message)\n    else:\n        message = '`atol` and `rtol` must be non-negative and finite.'\n        if np.any(params < 0) or np.any(np.isinf(params)):\n            raise ValueError(message)\n    atol = params[0]\n    rtol = rtol if rtol is None else params[1]\n    BIGINT = float(2 ** 62)\n    if maxfun is None and maxlevel is None:\n        maxlevel = 10\n    maxfun = BIGINT if maxfun is None else maxfun\n    maxlevel = BIGINT if maxlevel is None else maxlevel\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be integers.'\n    params = np.asarray([maxfun, maxlevel, minlevel])\n    if not (np.issubdtype(params.dtype, np.number) and np.all(np.isreal(params)) and np.all(params.astype(np.int64) == params)):\n        raise ValueError(message)\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be non-negative.'\n    if np.any(params < 0):\n        raise ValueError(message)\n    (maxfun, maxlevel, minlevel) = params.astype(np.int64)\n    minlevel = min(minlevel, maxlevel)\n    if not np.iterable(args):\n        args = (args,)\n    if callback is not None and (not callable(callback)):\n        raise ValueError('`callback` must be callable.')\n    return (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)",
        "mutated": [
            "def _tanhsinh_iv(f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback):\n    if False:\n        i = 10\n    message = '`f` must be callable.'\n    if not callable(f):\n        raise ValueError(message)\n    message = 'All elements of `a` and `b` must be real numbers.'\n    (a, b) = np.broadcast_arrays(a, b)\n    if np.any(np.iscomplex(a)) or np.any(np.iscomplex(b)):\n        raise ValueError(message)\n    message = '`log` must be True or False.'\n    if log not in {True, False}:\n        raise ValueError(message)\n    log = bool(log)\n    if atol is None:\n        atol = -np.inf if log else 0\n    rtol_temp = rtol if rtol is not None else 0.0\n    params = np.asarray([atol, rtol_temp, 0.0])\n    message = '`atol` and `rtol` must be real numbers.'\n    if not np.issubdtype(params.dtype, np.floating):\n        raise ValueError(message)\n    if log:\n        message = '`atol` and `rtol` may not be positive infinity.'\n        if np.any(np.isposinf(params)):\n            raise ValueError(message)\n    else:\n        message = '`atol` and `rtol` must be non-negative and finite.'\n        if np.any(params < 0) or np.any(np.isinf(params)):\n            raise ValueError(message)\n    atol = params[0]\n    rtol = rtol if rtol is None else params[1]\n    BIGINT = float(2 ** 62)\n    if maxfun is None and maxlevel is None:\n        maxlevel = 10\n    maxfun = BIGINT if maxfun is None else maxfun\n    maxlevel = BIGINT if maxlevel is None else maxlevel\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be integers.'\n    params = np.asarray([maxfun, maxlevel, minlevel])\n    if not (np.issubdtype(params.dtype, np.number) and np.all(np.isreal(params)) and np.all(params.astype(np.int64) == params)):\n        raise ValueError(message)\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be non-negative.'\n    if np.any(params < 0):\n        raise ValueError(message)\n    (maxfun, maxlevel, minlevel) = params.astype(np.int64)\n    minlevel = min(minlevel, maxlevel)\n    if not np.iterable(args):\n        args = (args,)\n    if callback is not None and (not callable(callback)):\n        raise ValueError('`callback` must be callable.')\n    return (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)",
            "def _tanhsinh_iv(f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    message = '`f` must be callable.'\n    if not callable(f):\n        raise ValueError(message)\n    message = 'All elements of `a` and `b` must be real numbers.'\n    (a, b) = np.broadcast_arrays(a, b)\n    if np.any(np.iscomplex(a)) or np.any(np.iscomplex(b)):\n        raise ValueError(message)\n    message = '`log` must be True or False.'\n    if log not in {True, False}:\n        raise ValueError(message)\n    log = bool(log)\n    if atol is None:\n        atol = -np.inf if log else 0\n    rtol_temp = rtol if rtol is not None else 0.0\n    params = np.asarray([atol, rtol_temp, 0.0])\n    message = '`atol` and `rtol` must be real numbers.'\n    if not np.issubdtype(params.dtype, np.floating):\n        raise ValueError(message)\n    if log:\n        message = '`atol` and `rtol` may not be positive infinity.'\n        if np.any(np.isposinf(params)):\n            raise ValueError(message)\n    else:\n        message = '`atol` and `rtol` must be non-negative and finite.'\n        if np.any(params < 0) or np.any(np.isinf(params)):\n            raise ValueError(message)\n    atol = params[0]\n    rtol = rtol if rtol is None else params[1]\n    BIGINT = float(2 ** 62)\n    if maxfun is None and maxlevel is None:\n        maxlevel = 10\n    maxfun = BIGINT if maxfun is None else maxfun\n    maxlevel = BIGINT if maxlevel is None else maxlevel\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be integers.'\n    params = np.asarray([maxfun, maxlevel, minlevel])\n    if not (np.issubdtype(params.dtype, np.number) and np.all(np.isreal(params)) and np.all(params.astype(np.int64) == params)):\n        raise ValueError(message)\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be non-negative.'\n    if np.any(params < 0):\n        raise ValueError(message)\n    (maxfun, maxlevel, minlevel) = params.astype(np.int64)\n    minlevel = min(minlevel, maxlevel)\n    if not np.iterable(args):\n        args = (args,)\n    if callback is not None and (not callable(callback)):\n        raise ValueError('`callback` must be callable.')\n    return (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)",
            "def _tanhsinh_iv(f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    message = '`f` must be callable.'\n    if not callable(f):\n        raise ValueError(message)\n    message = 'All elements of `a` and `b` must be real numbers.'\n    (a, b) = np.broadcast_arrays(a, b)\n    if np.any(np.iscomplex(a)) or np.any(np.iscomplex(b)):\n        raise ValueError(message)\n    message = '`log` must be True or False.'\n    if log not in {True, False}:\n        raise ValueError(message)\n    log = bool(log)\n    if atol is None:\n        atol = -np.inf if log else 0\n    rtol_temp = rtol if rtol is not None else 0.0\n    params = np.asarray([atol, rtol_temp, 0.0])\n    message = '`atol` and `rtol` must be real numbers.'\n    if not np.issubdtype(params.dtype, np.floating):\n        raise ValueError(message)\n    if log:\n        message = '`atol` and `rtol` may not be positive infinity.'\n        if np.any(np.isposinf(params)):\n            raise ValueError(message)\n    else:\n        message = '`atol` and `rtol` must be non-negative and finite.'\n        if np.any(params < 0) or np.any(np.isinf(params)):\n            raise ValueError(message)\n    atol = params[0]\n    rtol = rtol if rtol is None else params[1]\n    BIGINT = float(2 ** 62)\n    if maxfun is None and maxlevel is None:\n        maxlevel = 10\n    maxfun = BIGINT if maxfun is None else maxfun\n    maxlevel = BIGINT if maxlevel is None else maxlevel\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be integers.'\n    params = np.asarray([maxfun, maxlevel, minlevel])\n    if not (np.issubdtype(params.dtype, np.number) and np.all(np.isreal(params)) and np.all(params.astype(np.int64) == params)):\n        raise ValueError(message)\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be non-negative.'\n    if np.any(params < 0):\n        raise ValueError(message)\n    (maxfun, maxlevel, minlevel) = params.astype(np.int64)\n    minlevel = min(minlevel, maxlevel)\n    if not np.iterable(args):\n        args = (args,)\n    if callback is not None and (not callable(callback)):\n        raise ValueError('`callback` must be callable.')\n    return (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)",
            "def _tanhsinh_iv(f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    message = '`f` must be callable.'\n    if not callable(f):\n        raise ValueError(message)\n    message = 'All elements of `a` and `b` must be real numbers.'\n    (a, b) = np.broadcast_arrays(a, b)\n    if np.any(np.iscomplex(a)) or np.any(np.iscomplex(b)):\n        raise ValueError(message)\n    message = '`log` must be True or False.'\n    if log not in {True, False}:\n        raise ValueError(message)\n    log = bool(log)\n    if atol is None:\n        atol = -np.inf if log else 0\n    rtol_temp = rtol if rtol is not None else 0.0\n    params = np.asarray([atol, rtol_temp, 0.0])\n    message = '`atol` and `rtol` must be real numbers.'\n    if not np.issubdtype(params.dtype, np.floating):\n        raise ValueError(message)\n    if log:\n        message = '`atol` and `rtol` may not be positive infinity.'\n        if np.any(np.isposinf(params)):\n            raise ValueError(message)\n    else:\n        message = '`atol` and `rtol` must be non-negative and finite.'\n        if np.any(params < 0) or np.any(np.isinf(params)):\n            raise ValueError(message)\n    atol = params[0]\n    rtol = rtol if rtol is None else params[1]\n    BIGINT = float(2 ** 62)\n    if maxfun is None and maxlevel is None:\n        maxlevel = 10\n    maxfun = BIGINT if maxfun is None else maxfun\n    maxlevel = BIGINT if maxlevel is None else maxlevel\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be integers.'\n    params = np.asarray([maxfun, maxlevel, minlevel])\n    if not (np.issubdtype(params.dtype, np.number) and np.all(np.isreal(params)) and np.all(params.astype(np.int64) == params)):\n        raise ValueError(message)\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be non-negative.'\n    if np.any(params < 0):\n        raise ValueError(message)\n    (maxfun, maxlevel, minlevel) = params.astype(np.int64)\n    minlevel = min(minlevel, maxlevel)\n    if not np.iterable(args):\n        args = (args,)\n    if callback is not None and (not callable(callback)):\n        raise ValueError('`callback` must be callable.')\n    return (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)",
            "def _tanhsinh_iv(f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    message = '`f` must be callable.'\n    if not callable(f):\n        raise ValueError(message)\n    message = 'All elements of `a` and `b` must be real numbers.'\n    (a, b) = np.broadcast_arrays(a, b)\n    if np.any(np.iscomplex(a)) or np.any(np.iscomplex(b)):\n        raise ValueError(message)\n    message = '`log` must be True or False.'\n    if log not in {True, False}:\n        raise ValueError(message)\n    log = bool(log)\n    if atol is None:\n        atol = -np.inf if log else 0\n    rtol_temp = rtol if rtol is not None else 0.0\n    params = np.asarray([atol, rtol_temp, 0.0])\n    message = '`atol` and `rtol` must be real numbers.'\n    if not np.issubdtype(params.dtype, np.floating):\n        raise ValueError(message)\n    if log:\n        message = '`atol` and `rtol` may not be positive infinity.'\n        if np.any(np.isposinf(params)):\n            raise ValueError(message)\n    else:\n        message = '`atol` and `rtol` must be non-negative and finite.'\n        if np.any(params < 0) or np.any(np.isinf(params)):\n            raise ValueError(message)\n    atol = params[0]\n    rtol = rtol if rtol is None else params[1]\n    BIGINT = float(2 ** 62)\n    if maxfun is None and maxlevel is None:\n        maxlevel = 10\n    maxfun = BIGINT if maxfun is None else maxfun\n    maxlevel = BIGINT if maxlevel is None else maxlevel\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be integers.'\n    params = np.asarray([maxfun, maxlevel, minlevel])\n    if not (np.issubdtype(params.dtype, np.number) and np.all(np.isreal(params)) and np.all(params.astype(np.int64) == params)):\n        raise ValueError(message)\n    message = '`maxfun`, `maxlevel`, and `minlevel` must be non-negative.'\n    if np.any(params < 0):\n        raise ValueError(message)\n    (maxfun, maxlevel, minlevel) = params.astype(np.int64)\n    minlevel = min(minlevel, maxlevel)\n    if not np.iterable(args):\n        args = (args,)\n    if callback is not None and (not callable(callback)):\n        raise ValueError('`callback` must be callable.')\n    return (f, a, b, log, maxfun, maxlevel, minlevel, atol, rtol, args, callback)"
        ]
    }
]