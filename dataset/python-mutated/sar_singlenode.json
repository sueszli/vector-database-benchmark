[
    {
        "func_name": "__init__",
        "original": "def __init__(self, col_user=constants.DEFAULT_USER_COL, col_item=constants.DEFAULT_ITEM_COL, col_rating=constants.DEFAULT_RATING_COL, col_timestamp=constants.DEFAULT_TIMESTAMP_COL, col_prediction=constants.DEFAULT_PREDICTION_COL, similarity_type=SIM_JACCARD, time_decay_coefficient=30, time_now=None, timedecay_formula=False, threshold=1, normalize=False):\n    \"\"\"Initialize model parameters\n\n        Args:\n            col_user (str): user column name\n            col_item (str): item column name\n            col_rating (str): rating column name\n            col_timestamp (str): timestamp column name\n            col_prediction (str): prediction column name\n            similarity_type (str): ['cooccurrence', 'cosine', 'inclusion index', 'jaccard',\n              'lexicographers mutual information', 'lift', 'mutual information'] option for\n              computing item-item similarity\n            time_decay_coefficient (float): number of days till ratings are decayed by 1/2\n            time_now (int | None): current time for time decay calculation\n            timedecay_formula (bool): flag to apply time decay\n            threshold (int): item-item co-occurrences below this threshold will be removed\n            normalize (bool): option for normalizing predictions to scale of original ratings\n        \"\"\"\n    self.col_rating = col_rating\n    self.col_item = col_item\n    self.col_user = col_user\n    self.col_timestamp = col_timestamp\n    self.col_prediction = col_prediction\n    available_similarity_types = [SIM_COOCCUR, SIM_COSINE, SIM_INCLUSION_INDEX, SIM_JACCARD, SIM_LIFT, SIM_MUTUAL_INFORMATION, SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION]\n    if similarity_type not in available_similarity_types:\n        raise ValueError('Similarity type must be one of [\"' + '\" | \"'.join(available_similarity_types) + '\"]')\n    self.similarity_type = similarity_type\n    self.time_decay_half_life = time_decay_coefficient * 24 * 60 * 60\n    self.time_decay_flag = timedecay_formula\n    self.time_now = time_now\n    self.threshold = threshold\n    self.user_affinity = None\n    self.item_similarity = None\n    self.item_frequencies = None\n    self.user_frequencies = None\n    if self.threshold <= 0:\n        raise ValueError('Threshold cannot be < 1')\n    self.normalize = normalize\n    self.col_unity_rating = '_unity_rating'\n    self.unity_user_affinity = None\n    self.col_item_id = '_indexed_items'\n    self.col_user_id = '_indexed_users'\n    self.n_users = None\n    self.n_items = None\n    self.rating_min = None\n    self.rating_max = None\n    self.user2index = None\n    self.item2index = None\n    self.index2item = None\n    self.index2user = None",
        "mutated": [
            "def __init__(self, col_user=constants.DEFAULT_USER_COL, col_item=constants.DEFAULT_ITEM_COL, col_rating=constants.DEFAULT_RATING_COL, col_timestamp=constants.DEFAULT_TIMESTAMP_COL, col_prediction=constants.DEFAULT_PREDICTION_COL, similarity_type=SIM_JACCARD, time_decay_coefficient=30, time_now=None, timedecay_formula=False, threshold=1, normalize=False):\n    if False:\n        i = 10\n    \"Initialize model parameters\\n\\n        Args:\\n            col_user (str): user column name\\n            col_item (str): item column name\\n            col_rating (str): rating column name\\n            col_timestamp (str): timestamp column name\\n            col_prediction (str): prediction column name\\n            similarity_type (str): ['cooccurrence', 'cosine', 'inclusion index', 'jaccard',\\n              'lexicographers mutual information', 'lift', 'mutual information'] option for\\n              computing item-item similarity\\n            time_decay_coefficient (float): number of days till ratings are decayed by 1/2\\n            time_now (int | None): current time for time decay calculation\\n            timedecay_formula (bool): flag to apply time decay\\n            threshold (int): item-item co-occurrences below this threshold will be removed\\n            normalize (bool): option for normalizing predictions to scale of original ratings\\n        \"\n    self.col_rating = col_rating\n    self.col_item = col_item\n    self.col_user = col_user\n    self.col_timestamp = col_timestamp\n    self.col_prediction = col_prediction\n    available_similarity_types = [SIM_COOCCUR, SIM_COSINE, SIM_INCLUSION_INDEX, SIM_JACCARD, SIM_LIFT, SIM_MUTUAL_INFORMATION, SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION]\n    if similarity_type not in available_similarity_types:\n        raise ValueError('Similarity type must be one of [\"' + '\" | \"'.join(available_similarity_types) + '\"]')\n    self.similarity_type = similarity_type\n    self.time_decay_half_life = time_decay_coefficient * 24 * 60 * 60\n    self.time_decay_flag = timedecay_formula\n    self.time_now = time_now\n    self.threshold = threshold\n    self.user_affinity = None\n    self.item_similarity = None\n    self.item_frequencies = None\n    self.user_frequencies = None\n    if self.threshold <= 0:\n        raise ValueError('Threshold cannot be < 1')\n    self.normalize = normalize\n    self.col_unity_rating = '_unity_rating'\n    self.unity_user_affinity = None\n    self.col_item_id = '_indexed_items'\n    self.col_user_id = '_indexed_users'\n    self.n_users = None\n    self.n_items = None\n    self.rating_min = None\n    self.rating_max = None\n    self.user2index = None\n    self.item2index = None\n    self.index2item = None\n    self.index2user = None",
            "def __init__(self, col_user=constants.DEFAULT_USER_COL, col_item=constants.DEFAULT_ITEM_COL, col_rating=constants.DEFAULT_RATING_COL, col_timestamp=constants.DEFAULT_TIMESTAMP_COL, col_prediction=constants.DEFAULT_PREDICTION_COL, similarity_type=SIM_JACCARD, time_decay_coefficient=30, time_now=None, timedecay_formula=False, threshold=1, normalize=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Initialize model parameters\\n\\n        Args:\\n            col_user (str): user column name\\n            col_item (str): item column name\\n            col_rating (str): rating column name\\n            col_timestamp (str): timestamp column name\\n            col_prediction (str): prediction column name\\n            similarity_type (str): ['cooccurrence', 'cosine', 'inclusion index', 'jaccard',\\n              'lexicographers mutual information', 'lift', 'mutual information'] option for\\n              computing item-item similarity\\n            time_decay_coefficient (float): number of days till ratings are decayed by 1/2\\n            time_now (int | None): current time for time decay calculation\\n            timedecay_formula (bool): flag to apply time decay\\n            threshold (int): item-item co-occurrences below this threshold will be removed\\n            normalize (bool): option for normalizing predictions to scale of original ratings\\n        \"\n    self.col_rating = col_rating\n    self.col_item = col_item\n    self.col_user = col_user\n    self.col_timestamp = col_timestamp\n    self.col_prediction = col_prediction\n    available_similarity_types = [SIM_COOCCUR, SIM_COSINE, SIM_INCLUSION_INDEX, SIM_JACCARD, SIM_LIFT, SIM_MUTUAL_INFORMATION, SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION]\n    if similarity_type not in available_similarity_types:\n        raise ValueError('Similarity type must be one of [\"' + '\" | \"'.join(available_similarity_types) + '\"]')\n    self.similarity_type = similarity_type\n    self.time_decay_half_life = time_decay_coefficient * 24 * 60 * 60\n    self.time_decay_flag = timedecay_formula\n    self.time_now = time_now\n    self.threshold = threshold\n    self.user_affinity = None\n    self.item_similarity = None\n    self.item_frequencies = None\n    self.user_frequencies = None\n    if self.threshold <= 0:\n        raise ValueError('Threshold cannot be < 1')\n    self.normalize = normalize\n    self.col_unity_rating = '_unity_rating'\n    self.unity_user_affinity = None\n    self.col_item_id = '_indexed_items'\n    self.col_user_id = '_indexed_users'\n    self.n_users = None\n    self.n_items = None\n    self.rating_min = None\n    self.rating_max = None\n    self.user2index = None\n    self.item2index = None\n    self.index2item = None\n    self.index2user = None",
            "def __init__(self, col_user=constants.DEFAULT_USER_COL, col_item=constants.DEFAULT_ITEM_COL, col_rating=constants.DEFAULT_RATING_COL, col_timestamp=constants.DEFAULT_TIMESTAMP_COL, col_prediction=constants.DEFAULT_PREDICTION_COL, similarity_type=SIM_JACCARD, time_decay_coefficient=30, time_now=None, timedecay_formula=False, threshold=1, normalize=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Initialize model parameters\\n\\n        Args:\\n            col_user (str): user column name\\n            col_item (str): item column name\\n            col_rating (str): rating column name\\n            col_timestamp (str): timestamp column name\\n            col_prediction (str): prediction column name\\n            similarity_type (str): ['cooccurrence', 'cosine', 'inclusion index', 'jaccard',\\n              'lexicographers mutual information', 'lift', 'mutual information'] option for\\n              computing item-item similarity\\n            time_decay_coefficient (float): number of days till ratings are decayed by 1/2\\n            time_now (int | None): current time for time decay calculation\\n            timedecay_formula (bool): flag to apply time decay\\n            threshold (int): item-item co-occurrences below this threshold will be removed\\n            normalize (bool): option for normalizing predictions to scale of original ratings\\n        \"\n    self.col_rating = col_rating\n    self.col_item = col_item\n    self.col_user = col_user\n    self.col_timestamp = col_timestamp\n    self.col_prediction = col_prediction\n    available_similarity_types = [SIM_COOCCUR, SIM_COSINE, SIM_INCLUSION_INDEX, SIM_JACCARD, SIM_LIFT, SIM_MUTUAL_INFORMATION, SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION]\n    if similarity_type not in available_similarity_types:\n        raise ValueError('Similarity type must be one of [\"' + '\" | \"'.join(available_similarity_types) + '\"]')\n    self.similarity_type = similarity_type\n    self.time_decay_half_life = time_decay_coefficient * 24 * 60 * 60\n    self.time_decay_flag = timedecay_formula\n    self.time_now = time_now\n    self.threshold = threshold\n    self.user_affinity = None\n    self.item_similarity = None\n    self.item_frequencies = None\n    self.user_frequencies = None\n    if self.threshold <= 0:\n        raise ValueError('Threshold cannot be < 1')\n    self.normalize = normalize\n    self.col_unity_rating = '_unity_rating'\n    self.unity_user_affinity = None\n    self.col_item_id = '_indexed_items'\n    self.col_user_id = '_indexed_users'\n    self.n_users = None\n    self.n_items = None\n    self.rating_min = None\n    self.rating_max = None\n    self.user2index = None\n    self.item2index = None\n    self.index2item = None\n    self.index2user = None",
            "def __init__(self, col_user=constants.DEFAULT_USER_COL, col_item=constants.DEFAULT_ITEM_COL, col_rating=constants.DEFAULT_RATING_COL, col_timestamp=constants.DEFAULT_TIMESTAMP_COL, col_prediction=constants.DEFAULT_PREDICTION_COL, similarity_type=SIM_JACCARD, time_decay_coefficient=30, time_now=None, timedecay_formula=False, threshold=1, normalize=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Initialize model parameters\\n\\n        Args:\\n            col_user (str): user column name\\n            col_item (str): item column name\\n            col_rating (str): rating column name\\n            col_timestamp (str): timestamp column name\\n            col_prediction (str): prediction column name\\n            similarity_type (str): ['cooccurrence', 'cosine', 'inclusion index', 'jaccard',\\n              'lexicographers mutual information', 'lift', 'mutual information'] option for\\n              computing item-item similarity\\n            time_decay_coefficient (float): number of days till ratings are decayed by 1/2\\n            time_now (int | None): current time for time decay calculation\\n            timedecay_formula (bool): flag to apply time decay\\n            threshold (int): item-item co-occurrences below this threshold will be removed\\n            normalize (bool): option for normalizing predictions to scale of original ratings\\n        \"\n    self.col_rating = col_rating\n    self.col_item = col_item\n    self.col_user = col_user\n    self.col_timestamp = col_timestamp\n    self.col_prediction = col_prediction\n    available_similarity_types = [SIM_COOCCUR, SIM_COSINE, SIM_INCLUSION_INDEX, SIM_JACCARD, SIM_LIFT, SIM_MUTUAL_INFORMATION, SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION]\n    if similarity_type not in available_similarity_types:\n        raise ValueError('Similarity type must be one of [\"' + '\" | \"'.join(available_similarity_types) + '\"]')\n    self.similarity_type = similarity_type\n    self.time_decay_half_life = time_decay_coefficient * 24 * 60 * 60\n    self.time_decay_flag = timedecay_formula\n    self.time_now = time_now\n    self.threshold = threshold\n    self.user_affinity = None\n    self.item_similarity = None\n    self.item_frequencies = None\n    self.user_frequencies = None\n    if self.threshold <= 0:\n        raise ValueError('Threshold cannot be < 1')\n    self.normalize = normalize\n    self.col_unity_rating = '_unity_rating'\n    self.unity_user_affinity = None\n    self.col_item_id = '_indexed_items'\n    self.col_user_id = '_indexed_users'\n    self.n_users = None\n    self.n_items = None\n    self.rating_min = None\n    self.rating_max = None\n    self.user2index = None\n    self.item2index = None\n    self.index2item = None\n    self.index2user = None",
            "def __init__(self, col_user=constants.DEFAULT_USER_COL, col_item=constants.DEFAULT_ITEM_COL, col_rating=constants.DEFAULT_RATING_COL, col_timestamp=constants.DEFAULT_TIMESTAMP_COL, col_prediction=constants.DEFAULT_PREDICTION_COL, similarity_type=SIM_JACCARD, time_decay_coefficient=30, time_now=None, timedecay_formula=False, threshold=1, normalize=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Initialize model parameters\\n\\n        Args:\\n            col_user (str): user column name\\n            col_item (str): item column name\\n            col_rating (str): rating column name\\n            col_timestamp (str): timestamp column name\\n            col_prediction (str): prediction column name\\n            similarity_type (str): ['cooccurrence', 'cosine', 'inclusion index', 'jaccard',\\n              'lexicographers mutual information', 'lift', 'mutual information'] option for\\n              computing item-item similarity\\n            time_decay_coefficient (float): number of days till ratings are decayed by 1/2\\n            time_now (int | None): current time for time decay calculation\\n            timedecay_formula (bool): flag to apply time decay\\n            threshold (int): item-item co-occurrences below this threshold will be removed\\n            normalize (bool): option for normalizing predictions to scale of original ratings\\n        \"\n    self.col_rating = col_rating\n    self.col_item = col_item\n    self.col_user = col_user\n    self.col_timestamp = col_timestamp\n    self.col_prediction = col_prediction\n    available_similarity_types = [SIM_COOCCUR, SIM_COSINE, SIM_INCLUSION_INDEX, SIM_JACCARD, SIM_LIFT, SIM_MUTUAL_INFORMATION, SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION]\n    if similarity_type not in available_similarity_types:\n        raise ValueError('Similarity type must be one of [\"' + '\" | \"'.join(available_similarity_types) + '\"]')\n    self.similarity_type = similarity_type\n    self.time_decay_half_life = time_decay_coefficient * 24 * 60 * 60\n    self.time_decay_flag = timedecay_formula\n    self.time_now = time_now\n    self.threshold = threshold\n    self.user_affinity = None\n    self.item_similarity = None\n    self.item_frequencies = None\n    self.user_frequencies = None\n    if self.threshold <= 0:\n        raise ValueError('Threshold cannot be < 1')\n    self.normalize = normalize\n    self.col_unity_rating = '_unity_rating'\n    self.unity_user_affinity = None\n    self.col_item_id = '_indexed_items'\n    self.col_user_id = '_indexed_users'\n    self.n_users = None\n    self.n_items = None\n    self.rating_min = None\n    self.rating_max = None\n    self.user2index = None\n    self.item2index = None\n    self.index2item = None\n    self.index2user = None"
        ]
    },
    {
        "func_name": "compute_affinity_matrix",
        "original": "def compute_affinity_matrix(self, df, rating_col):\n    \"\"\"Affinity matrix.\n\n        The user-affinity matrix can be constructed by treating the users and items as\n        indices in a sparse matrix, and the events as the data. Here, we're treating\n        the ratings as the event weights.  We convert between different sparse-matrix\n        formats to de-duplicate user-item pairs, otherwise they will get added up.\n\n        Args:\n            df (pandas.DataFrame): Indexed df of users and items\n            rating_col (str): Name of column to use for ratings\n\n        Returns:\n            sparse.csr: Affinity matrix in Compressed Sparse Row (CSR) format.\n        \"\"\"\n    return sparse.coo_matrix((df[rating_col], (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()",
        "mutated": [
            "def compute_affinity_matrix(self, df, rating_col):\n    if False:\n        i = 10\n    \"Affinity matrix.\\n\\n        The user-affinity matrix can be constructed by treating the users and items as\\n        indices in a sparse matrix, and the events as the data. Here, we're treating\\n        the ratings as the event weights.  We convert between different sparse-matrix\\n        formats to de-duplicate user-item pairs, otherwise they will get added up.\\n\\n        Args:\\n            df (pandas.DataFrame): Indexed df of users and items\\n            rating_col (str): Name of column to use for ratings\\n\\n        Returns:\\n            sparse.csr: Affinity matrix in Compressed Sparse Row (CSR) format.\\n        \"\n    return sparse.coo_matrix((df[rating_col], (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()",
            "def compute_affinity_matrix(self, df, rating_col):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Affinity matrix.\\n\\n        The user-affinity matrix can be constructed by treating the users and items as\\n        indices in a sparse matrix, and the events as the data. Here, we're treating\\n        the ratings as the event weights.  We convert between different sparse-matrix\\n        formats to de-duplicate user-item pairs, otherwise they will get added up.\\n\\n        Args:\\n            df (pandas.DataFrame): Indexed df of users and items\\n            rating_col (str): Name of column to use for ratings\\n\\n        Returns:\\n            sparse.csr: Affinity matrix in Compressed Sparse Row (CSR) format.\\n        \"\n    return sparse.coo_matrix((df[rating_col], (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()",
            "def compute_affinity_matrix(self, df, rating_col):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Affinity matrix.\\n\\n        The user-affinity matrix can be constructed by treating the users and items as\\n        indices in a sparse matrix, and the events as the data. Here, we're treating\\n        the ratings as the event weights.  We convert between different sparse-matrix\\n        formats to de-duplicate user-item pairs, otherwise they will get added up.\\n\\n        Args:\\n            df (pandas.DataFrame): Indexed df of users and items\\n            rating_col (str): Name of column to use for ratings\\n\\n        Returns:\\n            sparse.csr: Affinity matrix in Compressed Sparse Row (CSR) format.\\n        \"\n    return sparse.coo_matrix((df[rating_col], (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()",
            "def compute_affinity_matrix(self, df, rating_col):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Affinity matrix.\\n\\n        The user-affinity matrix can be constructed by treating the users and items as\\n        indices in a sparse matrix, and the events as the data. Here, we're treating\\n        the ratings as the event weights.  We convert between different sparse-matrix\\n        formats to de-duplicate user-item pairs, otherwise they will get added up.\\n\\n        Args:\\n            df (pandas.DataFrame): Indexed df of users and items\\n            rating_col (str): Name of column to use for ratings\\n\\n        Returns:\\n            sparse.csr: Affinity matrix in Compressed Sparse Row (CSR) format.\\n        \"\n    return sparse.coo_matrix((df[rating_col], (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()",
            "def compute_affinity_matrix(self, df, rating_col):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Affinity matrix.\\n\\n        The user-affinity matrix can be constructed by treating the users and items as\\n        indices in a sparse matrix, and the events as the data. Here, we're treating\\n        the ratings as the event weights.  We convert between different sparse-matrix\\n        formats to de-duplicate user-item pairs, otherwise they will get added up.\\n\\n        Args:\\n            df (pandas.DataFrame): Indexed df of users and items\\n            rating_col (str): Name of column to use for ratings\\n\\n        Returns:\\n            sparse.csr: Affinity matrix in Compressed Sparse Row (CSR) format.\\n        \"\n    return sparse.coo_matrix((df[rating_col], (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()"
        ]
    },
    {
        "func_name": "compute_time_decay",
        "original": "def compute_time_decay(self, df, decay_column):\n    \"\"\"Compute time decay on provided column.\n\n        Args:\n            df (pandas.DataFrame): DataFrame of users and items\n            decay_column (str): column to decay\n\n        Returns:\n            pandas.DataFrame: with column decayed\n        \"\"\"\n    if self.time_now is None:\n        self.time_now = df[self.col_timestamp].max()\n    df[decay_column] *= exponential_decay(value=df[self.col_timestamp], max_val=self.time_now, half_life=self.time_decay_half_life)\n    return df.groupby([self.col_user, self.col_item]).sum().reset_index()",
        "mutated": [
            "def compute_time_decay(self, df, decay_column):\n    if False:\n        i = 10\n    'Compute time decay on provided column.\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n            decay_column (str): column to decay\\n\\n        Returns:\\n            pandas.DataFrame: with column decayed\\n        '\n    if self.time_now is None:\n        self.time_now = df[self.col_timestamp].max()\n    df[decay_column] *= exponential_decay(value=df[self.col_timestamp], max_val=self.time_now, half_life=self.time_decay_half_life)\n    return df.groupby([self.col_user, self.col_item]).sum().reset_index()",
            "def compute_time_decay(self, df, decay_column):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute time decay on provided column.\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n            decay_column (str): column to decay\\n\\n        Returns:\\n            pandas.DataFrame: with column decayed\\n        '\n    if self.time_now is None:\n        self.time_now = df[self.col_timestamp].max()\n    df[decay_column] *= exponential_decay(value=df[self.col_timestamp], max_val=self.time_now, half_life=self.time_decay_half_life)\n    return df.groupby([self.col_user, self.col_item]).sum().reset_index()",
            "def compute_time_decay(self, df, decay_column):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute time decay on provided column.\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n            decay_column (str): column to decay\\n\\n        Returns:\\n            pandas.DataFrame: with column decayed\\n        '\n    if self.time_now is None:\n        self.time_now = df[self.col_timestamp].max()\n    df[decay_column] *= exponential_decay(value=df[self.col_timestamp], max_val=self.time_now, half_life=self.time_decay_half_life)\n    return df.groupby([self.col_user, self.col_item]).sum().reset_index()",
            "def compute_time_decay(self, df, decay_column):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute time decay on provided column.\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n            decay_column (str): column to decay\\n\\n        Returns:\\n            pandas.DataFrame: with column decayed\\n        '\n    if self.time_now is None:\n        self.time_now = df[self.col_timestamp].max()\n    df[decay_column] *= exponential_decay(value=df[self.col_timestamp], max_val=self.time_now, half_life=self.time_decay_half_life)\n    return df.groupby([self.col_user, self.col_item]).sum().reset_index()",
            "def compute_time_decay(self, df, decay_column):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute time decay on provided column.\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n            decay_column (str): column to decay\\n\\n        Returns:\\n            pandas.DataFrame: with column decayed\\n        '\n    if self.time_now is None:\n        self.time_now = df[self.col_timestamp].max()\n    df[decay_column] *= exponential_decay(value=df[self.col_timestamp], max_val=self.time_now, half_life=self.time_decay_half_life)\n    return df.groupby([self.col_user, self.col_item]).sum().reset_index()"
        ]
    },
    {
        "func_name": "compute_cooccurrence_matrix",
        "original": "def compute_cooccurrence_matrix(self, df):\n    \"\"\"Co-occurrence matrix.\n\n        The co-occurrence matrix is defined as :math:`C = U^T * U`\n\n        where U is the user_affinity matrix with 1's as values (instead of ratings).\n\n        Args:\n            df (pandas.DataFrame): DataFrame of users and items\n\n        Returns:\n            numpy.ndarray: Co-occurrence matrix\n        \"\"\"\n    user_item_hits = sparse.coo_matrix((np.repeat(1, df.shape[0]), (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()\n    item_cooccurrence = user_item_hits.transpose().dot(user_item_hits)\n    item_cooccurrence = item_cooccurrence.multiply(item_cooccurrence >= self.threshold)\n    return item_cooccurrence.astype(df[self.col_rating].dtype)",
        "mutated": [
            "def compute_cooccurrence_matrix(self, df):\n    if False:\n        i = 10\n    \"Co-occurrence matrix.\\n\\n        The co-occurrence matrix is defined as :math:`C = U^T * U`\\n\\n        where U is the user_affinity matrix with 1's as values (instead of ratings).\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n\\n        Returns:\\n            numpy.ndarray: Co-occurrence matrix\\n        \"\n    user_item_hits = sparse.coo_matrix((np.repeat(1, df.shape[0]), (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()\n    item_cooccurrence = user_item_hits.transpose().dot(user_item_hits)\n    item_cooccurrence = item_cooccurrence.multiply(item_cooccurrence >= self.threshold)\n    return item_cooccurrence.astype(df[self.col_rating].dtype)",
            "def compute_cooccurrence_matrix(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Co-occurrence matrix.\\n\\n        The co-occurrence matrix is defined as :math:`C = U^T * U`\\n\\n        where U is the user_affinity matrix with 1's as values (instead of ratings).\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n\\n        Returns:\\n            numpy.ndarray: Co-occurrence matrix\\n        \"\n    user_item_hits = sparse.coo_matrix((np.repeat(1, df.shape[0]), (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()\n    item_cooccurrence = user_item_hits.transpose().dot(user_item_hits)\n    item_cooccurrence = item_cooccurrence.multiply(item_cooccurrence >= self.threshold)\n    return item_cooccurrence.astype(df[self.col_rating].dtype)",
            "def compute_cooccurrence_matrix(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Co-occurrence matrix.\\n\\n        The co-occurrence matrix is defined as :math:`C = U^T * U`\\n\\n        where U is the user_affinity matrix with 1's as values (instead of ratings).\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n\\n        Returns:\\n            numpy.ndarray: Co-occurrence matrix\\n        \"\n    user_item_hits = sparse.coo_matrix((np.repeat(1, df.shape[0]), (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()\n    item_cooccurrence = user_item_hits.transpose().dot(user_item_hits)\n    item_cooccurrence = item_cooccurrence.multiply(item_cooccurrence >= self.threshold)\n    return item_cooccurrence.astype(df[self.col_rating].dtype)",
            "def compute_cooccurrence_matrix(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Co-occurrence matrix.\\n\\n        The co-occurrence matrix is defined as :math:`C = U^T * U`\\n\\n        where U is the user_affinity matrix with 1's as values (instead of ratings).\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n\\n        Returns:\\n            numpy.ndarray: Co-occurrence matrix\\n        \"\n    user_item_hits = sparse.coo_matrix((np.repeat(1, df.shape[0]), (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()\n    item_cooccurrence = user_item_hits.transpose().dot(user_item_hits)\n    item_cooccurrence = item_cooccurrence.multiply(item_cooccurrence >= self.threshold)\n    return item_cooccurrence.astype(df[self.col_rating].dtype)",
            "def compute_cooccurrence_matrix(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Co-occurrence matrix.\\n\\n        The co-occurrence matrix is defined as :math:`C = U^T * U`\\n\\n        where U is the user_affinity matrix with 1's as values (instead of ratings).\\n\\n        Args:\\n            df (pandas.DataFrame): DataFrame of users and items\\n\\n        Returns:\\n            numpy.ndarray: Co-occurrence matrix\\n        \"\n    user_item_hits = sparse.coo_matrix((np.repeat(1, df.shape[0]), (df[self.col_user_id], df[self.col_item_id])), shape=(self.n_users, self.n_items)).tocsr()\n    item_cooccurrence = user_item_hits.transpose().dot(user_item_hits)\n    item_cooccurrence = item_cooccurrence.multiply(item_cooccurrence >= self.threshold)\n    return item_cooccurrence.astype(df[self.col_rating].dtype)"
        ]
    },
    {
        "func_name": "set_index",
        "original": "def set_index(self, df):\n    \"\"\"Generate continuous indices for users and items to reduce memory usage.\n\n        Args:\n            df (pandas.DataFrame): dataframe with user and item ids\n        \"\"\"\n    self.index2item = dict(enumerate(df[self.col_item].unique()))\n    self.index2user = dict(enumerate(df[self.col_user].unique()))\n    self.item2index = {v: k for (k, v) in self.index2item.items()}\n    self.user2index = {v: k for (k, v) in self.index2user.items()}\n    self.n_users = len(self.user2index)\n    self.n_items = len(self.index2item)",
        "mutated": [
            "def set_index(self, df):\n    if False:\n        i = 10\n    'Generate continuous indices for users and items to reduce memory usage.\\n\\n        Args:\\n            df (pandas.DataFrame): dataframe with user and item ids\\n        '\n    self.index2item = dict(enumerate(df[self.col_item].unique()))\n    self.index2user = dict(enumerate(df[self.col_user].unique()))\n    self.item2index = {v: k for (k, v) in self.index2item.items()}\n    self.user2index = {v: k for (k, v) in self.index2user.items()}\n    self.n_users = len(self.user2index)\n    self.n_items = len(self.index2item)",
            "def set_index(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generate continuous indices for users and items to reduce memory usage.\\n\\n        Args:\\n            df (pandas.DataFrame): dataframe with user and item ids\\n        '\n    self.index2item = dict(enumerate(df[self.col_item].unique()))\n    self.index2user = dict(enumerate(df[self.col_user].unique()))\n    self.item2index = {v: k for (k, v) in self.index2item.items()}\n    self.user2index = {v: k for (k, v) in self.index2user.items()}\n    self.n_users = len(self.user2index)\n    self.n_items = len(self.index2item)",
            "def set_index(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generate continuous indices for users and items to reduce memory usage.\\n\\n        Args:\\n            df (pandas.DataFrame): dataframe with user and item ids\\n        '\n    self.index2item = dict(enumerate(df[self.col_item].unique()))\n    self.index2user = dict(enumerate(df[self.col_user].unique()))\n    self.item2index = {v: k for (k, v) in self.index2item.items()}\n    self.user2index = {v: k for (k, v) in self.index2user.items()}\n    self.n_users = len(self.user2index)\n    self.n_items = len(self.index2item)",
            "def set_index(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generate continuous indices for users and items to reduce memory usage.\\n\\n        Args:\\n            df (pandas.DataFrame): dataframe with user and item ids\\n        '\n    self.index2item = dict(enumerate(df[self.col_item].unique()))\n    self.index2user = dict(enumerate(df[self.col_user].unique()))\n    self.item2index = {v: k for (k, v) in self.index2item.items()}\n    self.user2index = {v: k for (k, v) in self.index2user.items()}\n    self.n_users = len(self.user2index)\n    self.n_items = len(self.index2item)",
            "def set_index(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generate continuous indices for users and items to reduce memory usage.\\n\\n        Args:\\n            df (pandas.DataFrame): dataframe with user and item ids\\n        '\n    self.index2item = dict(enumerate(df[self.col_item].unique()))\n    self.index2user = dict(enumerate(df[self.col_user].unique()))\n    self.item2index = {v: k for (k, v) in self.index2item.items()}\n    self.user2index = {v: k for (k, v) in self.index2user.items()}\n    self.n_users = len(self.user2index)\n    self.n_items = len(self.index2item)"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, df):\n    \"\"\"Main fit method for SAR.\n\n        .. note::\n\n        Please make sure that `df` has no duplicates.\n\n        Args:\n            df (pandas.DataFrame): User item rating dataframe (without duplicates).\n        \"\"\"\n    select_columns = [self.col_user, self.col_item, self.col_rating]\n    if self.time_decay_flag:\n        select_columns += [self.col_timestamp]\n    if df[select_columns].duplicated().any():\n        raise ValueError('There should not be duplicates in the dataframe')\n    if self.index2item is None:\n        self.set_index(df)\n    logger.info('Collecting user affinity matrix')\n    if not np.issubdtype(df[self.col_rating].dtype, np.number):\n        raise TypeError('Rating column data type must be numeric')\n    temp_df = df[select_columns].copy()\n    if self.time_decay_flag:\n        logger.info('Calculating time-decayed affinities')\n        temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_rating)\n    logger.info('Creating index columns')\n    temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].apply(lambda item: self.item2index.get(item, np.NaN))\n    temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].apply(lambda user: self.user2index.get(user, np.NaN))\n    if self.normalize:\n        self.rating_min = temp_df[self.col_rating].min()\n        self.rating_max = temp_df[self.col_rating].max()\n        logger.info('Calculating normalization factors')\n        temp_df[self.col_unity_rating] = 1.0\n        if self.time_decay_flag:\n            temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_unity_rating)\n        self.unity_user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_unity_rating)\n    logger.info('Building user affinity sparse matrix')\n    self.user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_rating)\n    logger.info('Calculating item co-occurrence')\n    item_cooccurrence = self.compute_cooccurrence_matrix(df=temp_df)\n    del temp_df\n    self.item_frequencies = item_cooccurrence.diagonal()\n    logger.info('Calculating item similarity')\n    if self.similarity_type == SIM_COOCCUR:\n        logger.info('Using co-occurrence based similarity')\n        self.item_similarity = item_cooccurrence\n    elif self.similarity_type == SIM_COSINE:\n        logger.info('Using cosine similarity')\n        self.item_similarity = cosine_similarity(item_cooccurrence)\n    elif self.similarity_type == SIM_INCLUSION_INDEX:\n        logger.info('Using inclusion index')\n        self.item_similarity = inclusion_index(item_cooccurrence)\n    elif self.similarity_type == SIM_JACCARD:\n        logger.info('Using jaccard based similarity')\n        self.item_similarity = jaccard(item_cooccurrence)\n    elif self.similarity_type == SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION:\n        logger.info('Using lexicographers mutual information similarity')\n        self.item_similarity = lexicographers_mutual_information(item_cooccurrence)\n    elif self.similarity_type == SIM_LIFT:\n        logger.info('Using lift based similarity')\n        self.item_similarity = lift(item_cooccurrence)\n    elif self.similarity_type == SIM_MUTUAL_INFORMATION:\n        logger.info('Using mutual information similarity')\n        self.item_similarity = mutual_information(item_cooccurrence)\n    else:\n        raise ValueError('Unknown similarity type: {}'.format(self.similarity_type))\n    del item_cooccurrence\n    logger.info('Done training')",
        "mutated": [
            "def fit(self, df):\n    if False:\n        i = 10\n    'Main fit method for SAR.\\n\\n        .. note::\\n\\n        Please make sure that `df` has no duplicates.\\n\\n        Args:\\n            df (pandas.DataFrame): User item rating dataframe (without duplicates).\\n        '\n    select_columns = [self.col_user, self.col_item, self.col_rating]\n    if self.time_decay_flag:\n        select_columns += [self.col_timestamp]\n    if df[select_columns].duplicated().any():\n        raise ValueError('There should not be duplicates in the dataframe')\n    if self.index2item is None:\n        self.set_index(df)\n    logger.info('Collecting user affinity matrix')\n    if not np.issubdtype(df[self.col_rating].dtype, np.number):\n        raise TypeError('Rating column data type must be numeric')\n    temp_df = df[select_columns].copy()\n    if self.time_decay_flag:\n        logger.info('Calculating time-decayed affinities')\n        temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_rating)\n    logger.info('Creating index columns')\n    temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].apply(lambda item: self.item2index.get(item, np.NaN))\n    temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].apply(lambda user: self.user2index.get(user, np.NaN))\n    if self.normalize:\n        self.rating_min = temp_df[self.col_rating].min()\n        self.rating_max = temp_df[self.col_rating].max()\n        logger.info('Calculating normalization factors')\n        temp_df[self.col_unity_rating] = 1.0\n        if self.time_decay_flag:\n            temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_unity_rating)\n        self.unity_user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_unity_rating)\n    logger.info('Building user affinity sparse matrix')\n    self.user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_rating)\n    logger.info('Calculating item co-occurrence')\n    item_cooccurrence = self.compute_cooccurrence_matrix(df=temp_df)\n    del temp_df\n    self.item_frequencies = item_cooccurrence.diagonal()\n    logger.info('Calculating item similarity')\n    if self.similarity_type == SIM_COOCCUR:\n        logger.info('Using co-occurrence based similarity')\n        self.item_similarity = item_cooccurrence\n    elif self.similarity_type == SIM_COSINE:\n        logger.info('Using cosine similarity')\n        self.item_similarity = cosine_similarity(item_cooccurrence)\n    elif self.similarity_type == SIM_INCLUSION_INDEX:\n        logger.info('Using inclusion index')\n        self.item_similarity = inclusion_index(item_cooccurrence)\n    elif self.similarity_type == SIM_JACCARD:\n        logger.info('Using jaccard based similarity')\n        self.item_similarity = jaccard(item_cooccurrence)\n    elif self.similarity_type == SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION:\n        logger.info('Using lexicographers mutual information similarity')\n        self.item_similarity = lexicographers_mutual_information(item_cooccurrence)\n    elif self.similarity_type == SIM_LIFT:\n        logger.info('Using lift based similarity')\n        self.item_similarity = lift(item_cooccurrence)\n    elif self.similarity_type == SIM_MUTUAL_INFORMATION:\n        logger.info('Using mutual information similarity')\n        self.item_similarity = mutual_information(item_cooccurrence)\n    else:\n        raise ValueError('Unknown similarity type: {}'.format(self.similarity_type))\n    del item_cooccurrence\n    logger.info('Done training')",
            "def fit(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Main fit method for SAR.\\n\\n        .. note::\\n\\n        Please make sure that `df` has no duplicates.\\n\\n        Args:\\n            df (pandas.DataFrame): User item rating dataframe (without duplicates).\\n        '\n    select_columns = [self.col_user, self.col_item, self.col_rating]\n    if self.time_decay_flag:\n        select_columns += [self.col_timestamp]\n    if df[select_columns].duplicated().any():\n        raise ValueError('There should not be duplicates in the dataframe')\n    if self.index2item is None:\n        self.set_index(df)\n    logger.info('Collecting user affinity matrix')\n    if not np.issubdtype(df[self.col_rating].dtype, np.number):\n        raise TypeError('Rating column data type must be numeric')\n    temp_df = df[select_columns].copy()\n    if self.time_decay_flag:\n        logger.info('Calculating time-decayed affinities')\n        temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_rating)\n    logger.info('Creating index columns')\n    temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].apply(lambda item: self.item2index.get(item, np.NaN))\n    temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].apply(lambda user: self.user2index.get(user, np.NaN))\n    if self.normalize:\n        self.rating_min = temp_df[self.col_rating].min()\n        self.rating_max = temp_df[self.col_rating].max()\n        logger.info('Calculating normalization factors')\n        temp_df[self.col_unity_rating] = 1.0\n        if self.time_decay_flag:\n            temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_unity_rating)\n        self.unity_user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_unity_rating)\n    logger.info('Building user affinity sparse matrix')\n    self.user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_rating)\n    logger.info('Calculating item co-occurrence')\n    item_cooccurrence = self.compute_cooccurrence_matrix(df=temp_df)\n    del temp_df\n    self.item_frequencies = item_cooccurrence.diagonal()\n    logger.info('Calculating item similarity')\n    if self.similarity_type == SIM_COOCCUR:\n        logger.info('Using co-occurrence based similarity')\n        self.item_similarity = item_cooccurrence\n    elif self.similarity_type == SIM_COSINE:\n        logger.info('Using cosine similarity')\n        self.item_similarity = cosine_similarity(item_cooccurrence)\n    elif self.similarity_type == SIM_INCLUSION_INDEX:\n        logger.info('Using inclusion index')\n        self.item_similarity = inclusion_index(item_cooccurrence)\n    elif self.similarity_type == SIM_JACCARD:\n        logger.info('Using jaccard based similarity')\n        self.item_similarity = jaccard(item_cooccurrence)\n    elif self.similarity_type == SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION:\n        logger.info('Using lexicographers mutual information similarity')\n        self.item_similarity = lexicographers_mutual_information(item_cooccurrence)\n    elif self.similarity_type == SIM_LIFT:\n        logger.info('Using lift based similarity')\n        self.item_similarity = lift(item_cooccurrence)\n    elif self.similarity_type == SIM_MUTUAL_INFORMATION:\n        logger.info('Using mutual information similarity')\n        self.item_similarity = mutual_information(item_cooccurrence)\n    else:\n        raise ValueError('Unknown similarity type: {}'.format(self.similarity_type))\n    del item_cooccurrence\n    logger.info('Done training')",
            "def fit(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Main fit method for SAR.\\n\\n        .. note::\\n\\n        Please make sure that `df` has no duplicates.\\n\\n        Args:\\n            df (pandas.DataFrame): User item rating dataframe (without duplicates).\\n        '\n    select_columns = [self.col_user, self.col_item, self.col_rating]\n    if self.time_decay_flag:\n        select_columns += [self.col_timestamp]\n    if df[select_columns].duplicated().any():\n        raise ValueError('There should not be duplicates in the dataframe')\n    if self.index2item is None:\n        self.set_index(df)\n    logger.info('Collecting user affinity matrix')\n    if not np.issubdtype(df[self.col_rating].dtype, np.number):\n        raise TypeError('Rating column data type must be numeric')\n    temp_df = df[select_columns].copy()\n    if self.time_decay_flag:\n        logger.info('Calculating time-decayed affinities')\n        temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_rating)\n    logger.info('Creating index columns')\n    temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].apply(lambda item: self.item2index.get(item, np.NaN))\n    temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].apply(lambda user: self.user2index.get(user, np.NaN))\n    if self.normalize:\n        self.rating_min = temp_df[self.col_rating].min()\n        self.rating_max = temp_df[self.col_rating].max()\n        logger.info('Calculating normalization factors')\n        temp_df[self.col_unity_rating] = 1.0\n        if self.time_decay_flag:\n            temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_unity_rating)\n        self.unity_user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_unity_rating)\n    logger.info('Building user affinity sparse matrix')\n    self.user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_rating)\n    logger.info('Calculating item co-occurrence')\n    item_cooccurrence = self.compute_cooccurrence_matrix(df=temp_df)\n    del temp_df\n    self.item_frequencies = item_cooccurrence.diagonal()\n    logger.info('Calculating item similarity')\n    if self.similarity_type == SIM_COOCCUR:\n        logger.info('Using co-occurrence based similarity')\n        self.item_similarity = item_cooccurrence\n    elif self.similarity_type == SIM_COSINE:\n        logger.info('Using cosine similarity')\n        self.item_similarity = cosine_similarity(item_cooccurrence)\n    elif self.similarity_type == SIM_INCLUSION_INDEX:\n        logger.info('Using inclusion index')\n        self.item_similarity = inclusion_index(item_cooccurrence)\n    elif self.similarity_type == SIM_JACCARD:\n        logger.info('Using jaccard based similarity')\n        self.item_similarity = jaccard(item_cooccurrence)\n    elif self.similarity_type == SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION:\n        logger.info('Using lexicographers mutual information similarity')\n        self.item_similarity = lexicographers_mutual_information(item_cooccurrence)\n    elif self.similarity_type == SIM_LIFT:\n        logger.info('Using lift based similarity')\n        self.item_similarity = lift(item_cooccurrence)\n    elif self.similarity_type == SIM_MUTUAL_INFORMATION:\n        logger.info('Using mutual information similarity')\n        self.item_similarity = mutual_information(item_cooccurrence)\n    else:\n        raise ValueError('Unknown similarity type: {}'.format(self.similarity_type))\n    del item_cooccurrence\n    logger.info('Done training')",
            "def fit(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Main fit method for SAR.\\n\\n        .. note::\\n\\n        Please make sure that `df` has no duplicates.\\n\\n        Args:\\n            df (pandas.DataFrame): User item rating dataframe (without duplicates).\\n        '\n    select_columns = [self.col_user, self.col_item, self.col_rating]\n    if self.time_decay_flag:\n        select_columns += [self.col_timestamp]\n    if df[select_columns].duplicated().any():\n        raise ValueError('There should not be duplicates in the dataframe')\n    if self.index2item is None:\n        self.set_index(df)\n    logger.info('Collecting user affinity matrix')\n    if not np.issubdtype(df[self.col_rating].dtype, np.number):\n        raise TypeError('Rating column data type must be numeric')\n    temp_df = df[select_columns].copy()\n    if self.time_decay_flag:\n        logger.info('Calculating time-decayed affinities')\n        temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_rating)\n    logger.info('Creating index columns')\n    temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].apply(lambda item: self.item2index.get(item, np.NaN))\n    temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].apply(lambda user: self.user2index.get(user, np.NaN))\n    if self.normalize:\n        self.rating_min = temp_df[self.col_rating].min()\n        self.rating_max = temp_df[self.col_rating].max()\n        logger.info('Calculating normalization factors')\n        temp_df[self.col_unity_rating] = 1.0\n        if self.time_decay_flag:\n            temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_unity_rating)\n        self.unity_user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_unity_rating)\n    logger.info('Building user affinity sparse matrix')\n    self.user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_rating)\n    logger.info('Calculating item co-occurrence')\n    item_cooccurrence = self.compute_cooccurrence_matrix(df=temp_df)\n    del temp_df\n    self.item_frequencies = item_cooccurrence.diagonal()\n    logger.info('Calculating item similarity')\n    if self.similarity_type == SIM_COOCCUR:\n        logger.info('Using co-occurrence based similarity')\n        self.item_similarity = item_cooccurrence\n    elif self.similarity_type == SIM_COSINE:\n        logger.info('Using cosine similarity')\n        self.item_similarity = cosine_similarity(item_cooccurrence)\n    elif self.similarity_type == SIM_INCLUSION_INDEX:\n        logger.info('Using inclusion index')\n        self.item_similarity = inclusion_index(item_cooccurrence)\n    elif self.similarity_type == SIM_JACCARD:\n        logger.info('Using jaccard based similarity')\n        self.item_similarity = jaccard(item_cooccurrence)\n    elif self.similarity_type == SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION:\n        logger.info('Using lexicographers mutual information similarity')\n        self.item_similarity = lexicographers_mutual_information(item_cooccurrence)\n    elif self.similarity_type == SIM_LIFT:\n        logger.info('Using lift based similarity')\n        self.item_similarity = lift(item_cooccurrence)\n    elif self.similarity_type == SIM_MUTUAL_INFORMATION:\n        logger.info('Using mutual information similarity')\n        self.item_similarity = mutual_information(item_cooccurrence)\n    else:\n        raise ValueError('Unknown similarity type: {}'.format(self.similarity_type))\n    del item_cooccurrence\n    logger.info('Done training')",
            "def fit(self, df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Main fit method for SAR.\\n\\n        .. note::\\n\\n        Please make sure that `df` has no duplicates.\\n\\n        Args:\\n            df (pandas.DataFrame): User item rating dataframe (without duplicates).\\n        '\n    select_columns = [self.col_user, self.col_item, self.col_rating]\n    if self.time_decay_flag:\n        select_columns += [self.col_timestamp]\n    if df[select_columns].duplicated().any():\n        raise ValueError('There should not be duplicates in the dataframe')\n    if self.index2item is None:\n        self.set_index(df)\n    logger.info('Collecting user affinity matrix')\n    if not np.issubdtype(df[self.col_rating].dtype, np.number):\n        raise TypeError('Rating column data type must be numeric')\n    temp_df = df[select_columns].copy()\n    if self.time_decay_flag:\n        logger.info('Calculating time-decayed affinities')\n        temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_rating)\n    logger.info('Creating index columns')\n    temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].apply(lambda item: self.item2index.get(item, np.NaN))\n    temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].apply(lambda user: self.user2index.get(user, np.NaN))\n    if self.normalize:\n        self.rating_min = temp_df[self.col_rating].min()\n        self.rating_max = temp_df[self.col_rating].max()\n        logger.info('Calculating normalization factors')\n        temp_df[self.col_unity_rating] = 1.0\n        if self.time_decay_flag:\n            temp_df = self.compute_time_decay(df=temp_df, decay_column=self.col_unity_rating)\n        self.unity_user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_unity_rating)\n    logger.info('Building user affinity sparse matrix')\n    self.user_affinity = self.compute_affinity_matrix(df=temp_df, rating_col=self.col_rating)\n    logger.info('Calculating item co-occurrence')\n    item_cooccurrence = self.compute_cooccurrence_matrix(df=temp_df)\n    del temp_df\n    self.item_frequencies = item_cooccurrence.diagonal()\n    logger.info('Calculating item similarity')\n    if self.similarity_type == SIM_COOCCUR:\n        logger.info('Using co-occurrence based similarity')\n        self.item_similarity = item_cooccurrence\n    elif self.similarity_type == SIM_COSINE:\n        logger.info('Using cosine similarity')\n        self.item_similarity = cosine_similarity(item_cooccurrence)\n    elif self.similarity_type == SIM_INCLUSION_INDEX:\n        logger.info('Using inclusion index')\n        self.item_similarity = inclusion_index(item_cooccurrence)\n    elif self.similarity_type == SIM_JACCARD:\n        logger.info('Using jaccard based similarity')\n        self.item_similarity = jaccard(item_cooccurrence)\n    elif self.similarity_type == SIM_LEXICOGRAPHERS_MUTUAL_INFORMATION:\n        logger.info('Using lexicographers mutual information similarity')\n        self.item_similarity = lexicographers_mutual_information(item_cooccurrence)\n    elif self.similarity_type == SIM_LIFT:\n        logger.info('Using lift based similarity')\n        self.item_similarity = lift(item_cooccurrence)\n    elif self.similarity_type == SIM_MUTUAL_INFORMATION:\n        logger.info('Using mutual information similarity')\n        self.item_similarity = mutual_information(item_cooccurrence)\n    else:\n        raise ValueError('Unknown similarity type: {}'.format(self.similarity_type))\n    del item_cooccurrence\n    logger.info('Done training')"
        ]
    },
    {
        "func_name": "score",
        "original": "def score(self, test, remove_seen=False):\n    \"\"\"Score all items for test users.\n\n        Args:\n            test (pandas.DataFrame): user to test\n            remove_seen (bool): flag to remove items seen in training from recommendation\n\n        Returns:\n            numpy.ndarray: Value of interest of all items for the users.\n        \"\"\"\n    user_ids = list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].unique()))\n    if any(np.isnan(user_ids)):\n        raise ValueError('SAR cannot score users that are not in the training set')\n    logger.info('Calculating recommendation scores')\n    test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n    if isinstance(test_scores, sparse.spmatrix):\n        test_scores = test_scores.toarray()\n    if self.normalize:\n        counts = self.unity_user_affinity[user_ids, :].dot(self.item_similarity)\n        user_min_scores = np.tile(counts.min(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_min\n        user_max_scores = np.tile(counts.max(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_max\n        test_scores = rescale(test_scores, self.rating_min, self.rating_max, user_min_scores, user_max_scores)\n    if remove_seen:\n        logger.info('Removing seen items')\n        test_scores += self.user_affinity[user_ids, :] * -np.inf\n    return test_scores",
        "mutated": [
            "def score(self, test, remove_seen=False):\n    if False:\n        i = 10\n    'Score all items for test users.\\n\\n        Args:\\n            test (pandas.DataFrame): user to test\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            numpy.ndarray: Value of interest of all items for the users.\\n        '\n    user_ids = list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].unique()))\n    if any(np.isnan(user_ids)):\n        raise ValueError('SAR cannot score users that are not in the training set')\n    logger.info('Calculating recommendation scores')\n    test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n    if isinstance(test_scores, sparse.spmatrix):\n        test_scores = test_scores.toarray()\n    if self.normalize:\n        counts = self.unity_user_affinity[user_ids, :].dot(self.item_similarity)\n        user_min_scores = np.tile(counts.min(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_min\n        user_max_scores = np.tile(counts.max(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_max\n        test_scores = rescale(test_scores, self.rating_min, self.rating_max, user_min_scores, user_max_scores)\n    if remove_seen:\n        logger.info('Removing seen items')\n        test_scores += self.user_affinity[user_ids, :] * -np.inf\n    return test_scores",
            "def score(self, test, remove_seen=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Score all items for test users.\\n\\n        Args:\\n            test (pandas.DataFrame): user to test\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            numpy.ndarray: Value of interest of all items for the users.\\n        '\n    user_ids = list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].unique()))\n    if any(np.isnan(user_ids)):\n        raise ValueError('SAR cannot score users that are not in the training set')\n    logger.info('Calculating recommendation scores')\n    test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n    if isinstance(test_scores, sparse.spmatrix):\n        test_scores = test_scores.toarray()\n    if self.normalize:\n        counts = self.unity_user_affinity[user_ids, :].dot(self.item_similarity)\n        user_min_scores = np.tile(counts.min(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_min\n        user_max_scores = np.tile(counts.max(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_max\n        test_scores = rescale(test_scores, self.rating_min, self.rating_max, user_min_scores, user_max_scores)\n    if remove_seen:\n        logger.info('Removing seen items')\n        test_scores += self.user_affinity[user_ids, :] * -np.inf\n    return test_scores",
            "def score(self, test, remove_seen=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Score all items for test users.\\n\\n        Args:\\n            test (pandas.DataFrame): user to test\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            numpy.ndarray: Value of interest of all items for the users.\\n        '\n    user_ids = list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].unique()))\n    if any(np.isnan(user_ids)):\n        raise ValueError('SAR cannot score users that are not in the training set')\n    logger.info('Calculating recommendation scores')\n    test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n    if isinstance(test_scores, sparse.spmatrix):\n        test_scores = test_scores.toarray()\n    if self.normalize:\n        counts = self.unity_user_affinity[user_ids, :].dot(self.item_similarity)\n        user_min_scores = np.tile(counts.min(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_min\n        user_max_scores = np.tile(counts.max(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_max\n        test_scores = rescale(test_scores, self.rating_min, self.rating_max, user_min_scores, user_max_scores)\n    if remove_seen:\n        logger.info('Removing seen items')\n        test_scores += self.user_affinity[user_ids, :] * -np.inf\n    return test_scores",
            "def score(self, test, remove_seen=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Score all items for test users.\\n\\n        Args:\\n            test (pandas.DataFrame): user to test\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            numpy.ndarray: Value of interest of all items for the users.\\n        '\n    user_ids = list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].unique()))\n    if any(np.isnan(user_ids)):\n        raise ValueError('SAR cannot score users that are not in the training set')\n    logger.info('Calculating recommendation scores')\n    test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n    if isinstance(test_scores, sparse.spmatrix):\n        test_scores = test_scores.toarray()\n    if self.normalize:\n        counts = self.unity_user_affinity[user_ids, :].dot(self.item_similarity)\n        user_min_scores = np.tile(counts.min(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_min\n        user_max_scores = np.tile(counts.max(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_max\n        test_scores = rescale(test_scores, self.rating_min, self.rating_max, user_min_scores, user_max_scores)\n    if remove_seen:\n        logger.info('Removing seen items')\n        test_scores += self.user_affinity[user_ids, :] * -np.inf\n    return test_scores",
            "def score(self, test, remove_seen=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Score all items for test users.\\n\\n        Args:\\n            test (pandas.DataFrame): user to test\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            numpy.ndarray: Value of interest of all items for the users.\\n        '\n    user_ids = list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].unique()))\n    if any(np.isnan(user_ids)):\n        raise ValueError('SAR cannot score users that are not in the training set')\n    logger.info('Calculating recommendation scores')\n    test_scores = self.user_affinity[user_ids, :].dot(self.item_similarity)\n    if isinstance(test_scores, sparse.spmatrix):\n        test_scores = test_scores.toarray()\n    if self.normalize:\n        counts = self.unity_user_affinity[user_ids, :].dot(self.item_similarity)\n        user_min_scores = np.tile(counts.min(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_min\n        user_max_scores = np.tile(counts.max(axis=1)[:, np.newaxis], test_scores.shape[1]) * self.rating_max\n        test_scores = rescale(test_scores, self.rating_min, self.rating_max, user_min_scores, user_max_scores)\n    if remove_seen:\n        logger.info('Removing seen items')\n        test_scores += self.user_affinity[user_ids, :] * -np.inf\n    return test_scores"
        ]
    },
    {
        "func_name": "get_popularity_based_topk",
        "original": "def get_popularity_based_topk(self, top_k=10, sort_top_k=True, items=True):\n    \"\"\"Get top K most frequently occurring items across all users.\n\n        Args:\n            top_k (int): number of top items to recommend.\n            sort_top_k (bool): flag to sort top k results.\n            items (bool): if false, return most frequent users instead\n\n        Returns:\n            pandas.DataFrame: top k most popular items.\n        \"\"\"\n    if items:\n        frequencies = self.item_frequencies\n        col = self.col_item\n        idx = self.index2item\n    else:\n        if self.user_frequencies is None:\n            self.user_frequencies = self.user_affinity.getnnz(axis=1).astype('int64')\n        frequencies = self.user_frequencies\n        col = self.col_user\n        idx = self.index2user\n    test_scores = np.array([frequencies])\n    logger.info('Getting top K')\n    (top_components, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    return pd.DataFrame({col: [idx[item] for item in top_components.flatten()], self.col_prediction: top_scores.flatten()})",
        "mutated": [
            "def get_popularity_based_topk(self, top_k=10, sort_top_k=True, items=True):\n    if False:\n        i = 10\n    'Get top K most frequently occurring items across all users.\\n\\n        Args:\\n            top_k (int): number of top items to recommend.\\n            sort_top_k (bool): flag to sort top k results.\\n            items (bool): if false, return most frequent users instead\\n\\n        Returns:\\n            pandas.DataFrame: top k most popular items.\\n        '\n    if items:\n        frequencies = self.item_frequencies\n        col = self.col_item\n        idx = self.index2item\n    else:\n        if self.user_frequencies is None:\n            self.user_frequencies = self.user_affinity.getnnz(axis=1).astype('int64')\n        frequencies = self.user_frequencies\n        col = self.col_user\n        idx = self.index2user\n    test_scores = np.array([frequencies])\n    logger.info('Getting top K')\n    (top_components, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    return pd.DataFrame({col: [idx[item] for item in top_components.flatten()], self.col_prediction: top_scores.flatten()})",
            "def get_popularity_based_topk(self, top_k=10, sort_top_k=True, items=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get top K most frequently occurring items across all users.\\n\\n        Args:\\n            top_k (int): number of top items to recommend.\\n            sort_top_k (bool): flag to sort top k results.\\n            items (bool): if false, return most frequent users instead\\n\\n        Returns:\\n            pandas.DataFrame: top k most popular items.\\n        '\n    if items:\n        frequencies = self.item_frequencies\n        col = self.col_item\n        idx = self.index2item\n    else:\n        if self.user_frequencies is None:\n            self.user_frequencies = self.user_affinity.getnnz(axis=1).astype('int64')\n        frequencies = self.user_frequencies\n        col = self.col_user\n        idx = self.index2user\n    test_scores = np.array([frequencies])\n    logger.info('Getting top K')\n    (top_components, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    return pd.DataFrame({col: [idx[item] for item in top_components.flatten()], self.col_prediction: top_scores.flatten()})",
            "def get_popularity_based_topk(self, top_k=10, sort_top_k=True, items=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get top K most frequently occurring items across all users.\\n\\n        Args:\\n            top_k (int): number of top items to recommend.\\n            sort_top_k (bool): flag to sort top k results.\\n            items (bool): if false, return most frequent users instead\\n\\n        Returns:\\n            pandas.DataFrame: top k most popular items.\\n        '\n    if items:\n        frequencies = self.item_frequencies\n        col = self.col_item\n        idx = self.index2item\n    else:\n        if self.user_frequencies is None:\n            self.user_frequencies = self.user_affinity.getnnz(axis=1).astype('int64')\n        frequencies = self.user_frequencies\n        col = self.col_user\n        idx = self.index2user\n    test_scores = np.array([frequencies])\n    logger.info('Getting top K')\n    (top_components, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    return pd.DataFrame({col: [idx[item] for item in top_components.flatten()], self.col_prediction: top_scores.flatten()})",
            "def get_popularity_based_topk(self, top_k=10, sort_top_k=True, items=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get top K most frequently occurring items across all users.\\n\\n        Args:\\n            top_k (int): number of top items to recommend.\\n            sort_top_k (bool): flag to sort top k results.\\n            items (bool): if false, return most frequent users instead\\n\\n        Returns:\\n            pandas.DataFrame: top k most popular items.\\n        '\n    if items:\n        frequencies = self.item_frequencies\n        col = self.col_item\n        idx = self.index2item\n    else:\n        if self.user_frequencies is None:\n            self.user_frequencies = self.user_affinity.getnnz(axis=1).astype('int64')\n        frequencies = self.user_frequencies\n        col = self.col_user\n        idx = self.index2user\n    test_scores = np.array([frequencies])\n    logger.info('Getting top K')\n    (top_components, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    return pd.DataFrame({col: [idx[item] for item in top_components.flatten()], self.col_prediction: top_scores.flatten()})",
            "def get_popularity_based_topk(self, top_k=10, sort_top_k=True, items=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get top K most frequently occurring items across all users.\\n\\n        Args:\\n            top_k (int): number of top items to recommend.\\n            sort_top_k (bool): flag to sort top k results.\\n            items (bool): if false, return most frequent users instead\\n\\n        Returns:\\n            pandas.DataFrame: top k most popular items.\\n        '\n    if items:\n        frequencies = self.item_frequencies\n        col = self.col_item\n        idx = self.index2item\n    else:\n        if self.user_frequencies is None:\n            self.user_frequencies = self.user_affinity.getnnz(axis=1).astype('int64')\n        frequencies = self.user_frequencies\n        col = self.col_user\n        idx = self.index2user\n    test_scores = np.array([frequencies])\n    logger.info('Getting top K')\n    (top_components, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    return pd.DataFrame({col: [idx[item] for item in top_components.flatten()], self.col_prediction: top_scores.flatten()})"
        ]
    },
    {
        "func_name": "get_item_based_topk",
        "original": "def get_item_based_topk(self, items, top_k=10, sort_top_k=True):\n    \"\"\"Get top K similar items to provided seed items based on similarity metric defined.\n        This method will take a set of items and use them to recommend the most similar items to that set\n        based on the similarity matrix fit during training.\n        This allows recommendations for cold-users (unseen during training), note - the model is not updated.\n\n        The following options are possible based on information provided in the items input:\n        1. Single user or seed of items: only item column (ratings are assumed to be 1)\n        2. Single user or seed of items w/ ratings: item column and rating column\n        3. Separate users or seeds of items: item and user column (user ids are only used to separate item sets)\n        4. Separate users or seeds of items with ratings: item, user and rating columns provided\n\n        Args:\n            items (pandas.DataFrame): DataFrame with item, user (optional), and rating (optional) columns\n            top_k (int): number of top items to recommend\n            sort_top_k (bool): flag to sort top k results\n\n        Returns:\n            pandas.DataFrame: sorted top k recommendation items\n        \"\"\"\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), items[self.col_item].values)))\n    if self.col_rating in items.columns:\n        ratings = items[self.col_rating]\n    else:\n        ratings = pd.Series(np.ones_like(item_ids))\n    if self.col_user in items.columns:\n        test_users = items[self.col_user]\n        user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}\n        user_ids = test_users.map(user2index)\n    else:\n        test_users = pd.Series(np.zeros_like(item_ids))\n        user_ids = test_users\n    n_users = user_ids.drop_duplicates().shape[0]\n    pseudo_affinity = sparse.coo_matrix((ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)).tocsr()\n    test_scores = pseudo_affinity.dot(self.item_similarity)\n    test_scores[user_ids, item_ids] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test_users.drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
        "mutated": [
            "def get_item_based_topk(self, items, top_k=10, sort_top_k=True):\n    if False:\n        i = 10\n    'Get top K similar items to provided seed items based on similarity metric defined.\\n        This method will take a set of items and use them to recommend the most similar items to that set\\n        based on the similarity matrix fit during training.\\n        This allows recommendations for cold-users (unseen during training), note - the model is not updated.\\n\\n        The following options are possible based on information provided in the items input:\\n        1. Single user or seed of items: only item column (ratings are assumed to be 1)\\n        2. Single user or seed of items w/ ratings: item column and rating column\\n        3. Separate users or seeds of items: item and user column (user ids are only used to separate item sets)\\n        4. Separate users or seeds of items with ratings: item, user and rating columns provided\\n\\n        Args:\\n            items (pandas.DataFrame): DataFrame with item, user (optional), and rating (optional) columns\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: sorted top k recommendation items\\n        '\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), items[self.col_item].values)))\n    if self.col_rating in items.columns:\n        ratings = items[self.col_rating]\n    else:\n        ratings = pd.Series(np.ones_like(item_ids))\n    if self.col_user in items.columns:\n        test_users = items[self.col_user]\n        user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}\n        user_ids = test_users.map(user2index)\n    else:\n        test_users = pd.Series(np.zeros_like(item_ids))\n        user_ids = test_users\n    n_users = user_ids.drop_duplicates().shape[0]\n    pseudo_affinity = sparse.coo_matrix((ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)).tocsr()\n    test_scores = pseudo_affinity.dot(self.item_similarity)\n    test_scores[user_ids, item_ids] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test_users.drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def get_item_based_topk(self, items, top_k=10, sort_top_k=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get top K similar items to provided seed items based on similarity metric defined.\\n        This method will take a set of items and use them to recommend the most similar items to that set\\n        based on the similarity matrix fit during training.\\n        This allows recommendations for cold-users (unseen during training), note - the model is not updated.\\n\\n        The following options are possible based on information provided in the items input:\\n        1. Single user or seed of items: only item column (ratings are assumed to be 1)\\n        2. Single user or seed of items w/ ratings: item column and rating column\\n        3. Separate users or seeds of items: item and user column (user ids are only used to separate item sets)\\n        4. Separate users or seeds of items with ratings: item, user and rating columns provided\\n\\n        Args:\\n            items (pandas.DataFrame): DataFrame with item, user (optional), and rating (optional) columns\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: sorted top k recommendation items\\n        '\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), items[self.col_item].values)))\n    if self.col_rating in items.columns:\n        ratings = items[self.col_rating]\n    else:\n        ratings = pd.Series(np.ones_like(item_ids))\n    if self.col_user in items.columns:\n        test_users = items[self.col_user]\n        user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}\n        user_ids = test_users.map(user2index)\n    else:\n        test_users = pd.Series(np.zeros_like(item_ids))\n        user_ids = test_users\n    n_users = user_ids.drop_duplicates().shape[0]\n    pseudo_affinity = sparse.coo_matrix((ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)).tocsr()\n    test_scores = pseudo_affinity.dot(self.item_similarity)\n    test_scores[user_ids, item_ids] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test_users.drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def get_item_based_topk(self, items, top_k=10, sort_top_k=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get top K similar items to provided seed items based on similarity metric defined.\\n        This method will take a set of items and use them to recommend the most similar items to that set\\n        based on the similarity matrix fit during training.\\n        This allows recommendations for cold-users (unseen during training), note - the model is not updated.\\n\\n        The following options are possible based on information provided in the items input:\\n        1. Single user or seed of items: only item column (ratings are assumed to be 1)\\n        2. Single user or seed of items w/ ratings: item column and rating column\\n        3. Separate users or seeds of items: item and user column (user ids are only used to separate item sets)\\n        4. Separate users or seeds of items with ratings: item, user and rating columns provided\\n\\n        Args:\\n            items (pandas.DataFrame): DataFrame with item, user (optional), and rating (optional) columns\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: sorted top k recommendation items\\n        '\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), items[self.col_item].values)))\n    if self.col_rating in items.columns:\n        ratings = items[self.col_rating]\n    else:\n        ratings = pd.Series(np.ones_like(item_ids))\n    if self.col_user in items.columns:\n        test_users = items[self.col_user]\n        user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}\n        user_ids = test_users.map(user2index)\n    else:\n        test_users = pd.Series(np.zeros_like(item_ids))\n        user_ids = test_users\n    n_users = user_ids.drop_duplicates().shape[0]\n    pseudo_affinity = sparse.coo_matrix((ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)).tocsr()\n    test_scores = pseudo_affinity.dot(self.item_similarity)\n    test_scores[user_ids, item_ids] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test_users.drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def get_item_based_topk(self, items, top_k=10, sort_top_k=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get top K similar items to provided seed items based on similarity metric defined.\\n        This method will take a set of items and use them to recommend the most similar items to that set\\n        based on the similarity matrix fit during training.\\n        This allows recommendations for cold-users (unseen during training), note - the model is not updated.\\n\\n        The following options are possible based on information provided in the items input:\\n        1. Single user or seed of items: only item column (ratings are assumed to be 1)\\n        2. Single user or seed of items w/ ratings: item column and rating column\\n        3. Separate users or seeds of items: item and user column (user ids are only used to separate item sets)\\n        4. Separate users or seeds of items with ratings: item, user and rating columns provided\\n\\n        Args:\\n            items (pandas.DataFrame): DataFrame with item, user (optional), and rating (optional) columns\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: sorted top k recommendation items\\n        '\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), items[self.col_item].values)))\n    if self.col_rating in items.columns:\n        ratings = items[self.col_rating]\n    else:\n        ratings = pd.Series(np.ones_like(item_ids))\n    if self.col_user in items.columns:\n        test_users = items[self.col_user]\n        user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}\n        user_ids = test_users.map(user2index)\n    else:\n        test_users = pd.Series(np.zeros_like(item_ids))\n        user_ids = test_users\n    n_users = user_ids.drop_duplicates().shape[0]\n    pseudo_affinity = sparse.coo_matrix((ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)).tocsr()\n    test_scores = pseudo_affinity.dot(self.item_similarity)\n    test_scores[user_ids, item_ids] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test_users.drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def get_item_based_topk(self, items, top_k=10, sort_top_k=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get top K similar items to provided seed items based on similarity metric defined.\\n        This method will take a set of items and use them to recommend the most similar items to that set\\n        based on the similarity matrix fit during training.\\n        This allows recommendations for cold-users (unseen during training), note - the model is not updated.\\n\\n        The following options are possible based on information provided in the items input:\\n        1. Single user or seed of items: only item column (ratings are assumed to be 1)\\n        2. Single user or seed of items w/ ratings: item column and rating column\\n        3. Separate users or seeds of items: item and user column (user ids are only used to separate item sets)\\n        4. Separate users or seeds of items with ratings: item, user and rating columns provided\\n\\n        Args:\\n            items (pandas.DataFrame): DataFrame with item, user (optional), and rating (optional) columns\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: sorted top k recommendation items\\n        '\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), items[self.col_item].values)))\n    if self.col_rating in items.columns:\n        ratings = items[self.col_rating]\n    else:\n        ratings = pd.Series(np.ones_like(item_ids))\n    if self.col_user in items.columns:\n        test_users = items[self.col_user]\n        user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}\n        user_ids = test_users.map(user2index)\n    else:\n        test_users = pd.Series(np.zeros_like(item_ids))\n        user_ids = test_users\n    n_users = user_ids.drop_duplicates().shape[0]\n    pseudo_affinity = sparse.coo_matrix((ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)).tocsr()\n    test_scores = pseudo_affinity.dot(self.item_similarity)\n    test_scores[user_ids, item_ids] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test_users.drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()"
        ]
    },
    {
        "func_name": "get_topk_most_similar_users",
        "original": "def get_topk_most_similar_users(self, user, top_k, sort_top_k=True):\n    \"\"\"Based on user affinity towards items, calculate the most similar users to the given user.\n\n        Args:\n            user (int): user to retrieve most similar users for\n            top_k (int): number of top items to recommend\n            sort_top_k (bool): flag to sort top k results\n\n        Returns:\n            pandas.DataFrame: top k most similar users and their scores\n        \"\"\"\n    user_idx = self.user2index[user]\n    similarities = self.user_affinity[user_idx].dot(self.user_affinity.T).toarray()\n    similarities[0, user_idx] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=similarities, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: [self.index2user[user] for user in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
        "mutated": [
            "def get_topk_most_similar_users(self, user, top_k, sort_top_k=True):\n    if False:\n        i = 10\n    'Based on user affinity towards items, calculate the most similar users to the given user.\\n\\n        Args:\\n            user (int): user to retrieve most similar users for\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: top k most similar users and their scores\\n        '\n    user_idx = self.user2index[user]\n    similarities = self.user_affinity[user_idx].dot(self.user_affinity.T).toarray()\n    similarities[0, user_idx] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=similarities, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: [self.index2user[user] for user in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def get_topk_most_similar_users(self, user, top_k, sort_top_k=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Based on user affinity towards items, calculate the most similar users to the given user.\\n\\n        Args:\\n            user (int): user to retrieve most similar users for\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: top k most similar users and their scores\\n        '\n    user_idx = self.user2index[user]\n    similarities = self.user_affinity[user_idx].dot(self.user_affinity.T).toarray()\n    similarities[0, user_idx] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=similarities, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: [self.index2user[user] for user in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def get_topk_most_similar_users(self, user, top_k, sort_top_k=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Based on user affinity towards items, calculate the most similar users to the given user.\\n\\n        Args:\\n            user (int): user to retrieve most similar users for\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: top k most similar users and their scores\\n        '\n    user_idx = self.user2index[user]\n    similarities = self.user_affinity[user_idx].dot(self.user_affinity.T).toarray()\n    similarities[0, user_idx] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=similarities, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: [self.index2user[user] for user in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def get_topk_most_similar_users(self, user, top_k, sort_top_k=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Based on user affinity towards items, calculate the most similar users to the given user.\\n\\n        Args:\\n            user (int): user to retrieve most similar users for\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: top k most similar users and their scores\\n        '\n    user_idx = self.user2index[user]\n    similarities = self.user_affinity[user_idx].dot(self.user_affinity.T).toarray()\n    similarities[0, user_idx] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=similarities, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: [self.index2user[user] for user in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def get_topk_most_similar_users(self, user, top_k, sort_top_k=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Based on user affinity towards items, calculate the most similar users to the given user.\\n\\n        Args:\\n            user (int): user to retrieve most similar users for\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n\\n        Returns:\\n            pandas.DataFrame: top k most similar users and their scores\\n        '\n    user_idx = self.user2index[user]\n    similarities = self.user_affinity[user_idx].dot(self.user_affinity.T).toarray()\n    similarities[0, user_idx] = -np.inf\n    (top_items, top_scores) = get_top_k_scored_items(scores=similarities, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: [self.index2user[user] for user in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()"
        ]
    },
    {
        "func_name": "recommend_k_items",
        "original": "def recommend_k_items(self, test, top_k=10, sort_top_k=True, remove_seen=False):\n    \"\"\"Recommend top K items for all users which are in the test set\n\n        Args:\n            test (pandas.DataFrame): users to test\n            top_k (int): number of top items to recommend\n            sort_top_k (bool): flag to sort top k results\n            remove_seen (bool): flag to remove items seen in training from recommendation\n\n        Returns:\n            pandas.DataFrame: top k recommendation items for each user\n        \"\"\"\n    test_scores = self.score(test, remove_seen=remove_seen)\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test[self.col_user].drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
        "mutated": [
            "def recommend_k_items(self, test, top_k=10, sort_top_k=True, remove_seen=False):\n    if False:\n        i = 10\n    'Recommend top K items for all users which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): users to test\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            pandas.DataFrame: top k recommendation items for each user\\n        '\n    test_scores = self.score(test, remove_seen=remove_seen)\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test[self.col_user].drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def recommend_k_items(self, test, top_k=10, sort_top_k=True, remove_seen=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Recommend top K items for all users which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): users to test\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            pandas.DataFrame: top k recommendation items for each user\\n        '\n    test_scores = self.score(test, remove_seen=remove_seen)\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test[self.col_user].drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def recommend_k_items(self, test, top_k=10, sort_top_k=True, remove_seen=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Recommend top K items for all users which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): users to test\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            pandas.DataFrame: top k recommendation items for each user\\n        '\n    test_scores = self.score(test, remove_seen=remove_seen)\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test[self.col_user].drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def recommend_k_items(self, test, top_k=10, sort_top_k=True, remove_seen=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Recommend top K items for all users which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): users to test\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            pandas.DataFrame: top k recommendation items for each user\\n        '\n    test_scores = self.score(test, remove_seen=remove_seen)\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test[self.col_user].drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()",
            "def recommend_k_items(self, test, top_k=10, sort_top_k=True, remove_seen=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Recommend top K items for all users which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): users to test\\n            top_k (int): number of top items to recommend\\n            sort_top_k (bool): flag to sort top k results\\n            remove_seen (bool): flag to remove items seen in training from recommendation\\n\\n        Returns:\\n            pandas.DataFrame: top k recommendation items for each user\\n        '\n    test_scores = self.score(test, remove_seen=remove_seen)\n    (top_items, top_scores) = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)\n    df = pd.DataFrame({self.col_user: np.repeat(test[self.col_user].drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})\n    return df.replace(-np.inf, np.nan).dropna()"
        ]
    },
    {
        "func_name": "predict",
        "original": "def predict(self, test):\n    \"\"\"Output SAR scores for only the users-items pairs which are in the test set\n\n        Args:\n            test (pandas.DataFrame): DataFrame that contains users and items to test\n\n        Returns:\n            pandas.DataFrame: DataFrame contains the prediction results\n        \"\"\"\n    test_scores = self.score(test)\n    user_ids = np.asarray(list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].values)))\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), test[self.col_item].values)))\n    nans = np.isnan(item_ids)\n    if any(nans):\n        logger.warning('Items found in test not seen during training, new items will have score of 0')\n        test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)\n        item_ids[nans] = self.n_items\n        item_ids = item_ids.astype('int64')\n    df = pd.DataFrame({self.col_user: test[self.col_user].values, self.col_item: test[self.col_item].values, self.col_prediction: test_scores[user_ids, item_ids]})\n    return df",
        "mutated": [
            "def predict(self, test):\n    if False:\n        i = 10\n    'Output SAR scores for only the users-items pairs which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): DataFrame that contains users and items to test\\n\\n        Returns:\\n            pandas.DataFrame: DataFrame contains the prediction results\\n        '\n    test_scores = self.score(test)\n    user_ids = np.asarray(list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].values)))\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), test[self.col_item].values)))\n    nans = np.isnan(item_ids)\n    if any(nans):\n        logger.warning('Items found in test not seen during training, new items will have score of 0')\n        test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)\n        item_ids[nans] = self.n_items\n        item_ids = item_ids.astype('int64')\n    df = pd.DataFrame({self.col_user: test[self.col_user].values, self.col_item: test[self.col_item].values, self.col_prediction: test_scores[user_ids, item_ids]})\n    return df",
            "def predict(self, test):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Output SAR scores for only the users-items pairs which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): DataFrame that contains users and items to test\\n\\n        Returns:\\n            pandas.DataFrame: DataFrame contains the prediction results\\n        '\n    test_scores = self.score(test)\n    user_ids = np.asarray(list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].values)))\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), test[self.col_item].values)))\n    nans = np.isnan(item_ids)\n    if any(nans):\n        logger.warning('Items found in test not seen during training, new items will have score of 0')\n        test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)\n        item_ids[nans] = self.n_items\n        item_ids = item_ids.astype('int64')\n    df = pd.DataFrame({self.col_user: test[self.col_user].values, self.col_item: test[self.col_item].values, self.col_prediction: test_scores[user_ids, item_ids]})\n    return df",
            "def predict(self, test):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Output SAR scores for only the users-items pairs which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): DataFrame that contains users and items to test\\n\\n        Returns:\\n            pandas.DataFrame: DataFrame contains the prediction results\\n        '\n    test_scores = self.score(test)\n    user_ids = np.asarray(list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].values)))\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), test[self.col_item].values)))\n    nans = np.isnan(item_ids)\n    if any(nans):\n        logger.warning('Items found in test not seen during training, new items will have score of 0')\n        test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)\n        item_ids[nans] = self.n_items\n        item_ids = item_ids.astype('int64')\n    df = pd.DataFrame({self.col_user: test[self.col_user].values, self.col_item: test[self.col_item].values, self.col_prediction: test_scores[user_ids, item_ids]})\n    return df",
            "def predict(self, test):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Output SAR scores for only the users-items pairs which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): DataFrame that contains users and items to test\\n\\n        Returns:\\n            pandas.DataFrame: DataFrame contains the prediction results\\n        '\n    test_scores = self.score(test)\n    user_ids = np.asarray(list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].values)))\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), test[self.col_item].values)))\n    nans = np.isnan(item_ids)\n    if any(nans):\n        logger.warning('Items found in test not seen during training, new items will have score of 0')\n        test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)\n        item_ids[nans] = self.n_items\n        item_ids = item_ids.astype('int64')\n    df = pd.DataFrame({self.col_user: test[self.col_user].values, self.col_item: test[self.col_item].values, self.col_prediction: test_scores[user_ids, item_ids]})\n    return df",
            "def predict(self, test):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Output SAR scores for only the users-items pairs which are in the test set\\n\\n        Args:\\n            test (pandas.DataFrame): DataFrame that contains users and items to test\\n\\n        Returns:\\n            pandas.DataFrame: DataFrame contains the prediction results\\n        '\n    test_scores = self.score(test)\n    user_ids = np.asarray(list(map(lambda user: self.user2index.get(user, np.NaN), test[self.col_user].values)))\n    item_ids = np.asarray(list(map(lambda item: self.item2index.get(item, np.NaN), test[self.col_item].values)))\n    nans = np.isnan(item_ids)\n    if any(nans):\n        logger.warning('Items found in test not seen during training, new items will have score of 0')\n        test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)\n        item_ids[nans] = self.n_items\n        item_ids = item_ids.astype('int64')\n    df = pd.DataFrame({self.col_user: test[self.col_user].values, self.col_item: test[self.col_item].values, self.col_prediction: test_scores[user_ids, item_ids]})\n    return df"
        ]
    }
]